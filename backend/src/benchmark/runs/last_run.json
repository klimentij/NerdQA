{
  "average_metrics": {
    "precision@10": 0.04,
    "recall@10": 0.0003125,
    "f1@10": 0.00062015503875969,
    "rouge_1": 0.03116429595023674,
    "rouge_2": 0.01093275049397283,
    "rouge_l": 0.017358144176045677,
    "text_f1": 0.07924281856356394
  },
  "papers": [
    {
      "id": "https://openalex.org/W4392203343",
      "limited_meta": {
        "title": "A Comprehensive Survey on Deep Graph Representation Learning",
        "publication_date": "2024-02-27",
        "cited_by_count": 55,
        "url": ""
      },
      "text": "A Comprehensive Survey on Deep Graph Representation\nLearning\nWEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University,\nChina\nZIYUE QIAO, The Hong Kong University of Science and Technology, China\nYIFANG QIN and JIANHAO SHEN, Peking University, China\nFANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA\nJUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China\nYIFAN WANG, University of International Business and Economics, China\nXIAO LUO\u2217, University of California, Los Angeles, USA\nMING ZHANG\u2217, Peking University, China\nGraph representation learning aims to effectively encode high-dimensional sparse graph-structured data into\nlow-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields,\nincluding machine learning and data mining. Classic graph embedding methods follow the basic idea that the\nembedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby\npreserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i)\ntraditional methods have limited model capacity which limits the learning performance; (ii) existing techniques\ntypically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii)\nrepresentation learning and downstream tasks are dependent on each other which should be jointly enhanced.\nWith the remarkable success of deep learning, deep graph representation learning has shown great potential\nand advantages over shallow (traditional) methods, there exist a large number of deep graph representation\nlearning techniques have been proposed in the past decade, especially graph neural networks. In this survey,\nwe conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a\nnew taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential\ncomponents of graph representation learning and categorize existing approaches by the ways of graph neural\nnetwork architectures and the most recent advanced learning paradigms. Moreover, this survey also provides\nthe practical and promising applications of deep graph representation learning. Last but not least, we state\nnew perspectives and suggest challenging directions which deserve further investigations in the future.\nCCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations.\n\u2217Corresponding authors.\nAuthors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn;\nZequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871;\nZiyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453;\nYifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang\nSun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei\nYang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking\nUniversity, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics,\nBeijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang,\nmzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0004-5411/2024/2-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\narXiv:2304.05055v3 [cs.LG] 28 Feb 2024\n2 W. Ju, et al.\nAdditional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural\nNetwork, Survey\nACM Reference Format:\nWei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun,\nZhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024.\nA Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nGraphs have recently emerged as a powerful tool for representing a variety of structured and\ncomplex data, including social networks, traffic networks, information systems, knowledge graphs,\nprotein-protein interaction networks, and physical interaction networks. As a kind of general form\nof data organization, graph structures are capable of naturally expressing the intrinsic relationship\nof these data, and thus can characterize plenty of non-Euclidean structures that are crucial in\na variety of disciplines and domains due to their flexible adaptability. For example, to encode a\nsocial network as a graph, nodes on the graph are used to represent individual users, and edges are\nused to represent the relationship between two individuals, such as friends. In the field of biology,\nnodes can be used to represent proteins, and edges can be used to represent biological interactions\nbetween various proteins, such as the dynamic interactions between proteins. Thus, by analyzing\nand mining the graph-structured data, we can understand the deep meaning hidden behind the\ndata, and further discover valuable knowledge, so as to benefit society and human beings.\nIn the last decade years, a wide range of machine learning algorithms have been developed for\ngraph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410]\nusually break down graphs into different atomic substructures and then use kernel functions\nto measure the similarity between all pairs of them. Although graph kernels could provide a\nperspective on modeling graph topology, these approaches often generate substructures or feature\nrepresentations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer\nfrom high computational complexity, and therefore have weak scalability and subpar performance.\nIn the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have ever\u0002increasing emerged, which attempt to encode the structural information of the graph (usually a\nhigh-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to\npreserve the topology information and attribute information in the embedding space as much\nas possible, so that the learned graph embeddings can be naturally integrated into traditional\nmachine learning algorithms. Compared to previous works which use feature engineering in the\npre-processing phase to extract graph structural features, current graph embedding algorithms are\nconducted in a data-driven way leveraging machine learning algorithms (such as neural networks)\nto encode the structural information of the graph. Specifically, existing graph embedding methods\ncan be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354]\nthat factorize the matrix to learn node embedding which preserves the graph property; (ii) deep\nlearning based methods [155, 362, 443, 460] that apply deep learning techniques specifically de\u0002signed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either\nmaximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these\nmethods typically depend on shallow architectures, and fail to exploit the potential and capacity of\ndeep neural networks, resulting in sub-optimal representation quality and learning performance.\nInspired by the recent remarkable success of deep neural networks, a range of deep learning\nalgorithms has been developed for graph-structured data learning. The core of these methods is to\ngenerate effective node and graph representations using graph neural networks (GNNs), followed\nby a goal-oriented learning paradigm. In this way, the derived representations can be adaptively\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 3\ncoupled with a variety of downstream tasks and applications. Following this line of thought, in\nthis paper, we propose a new taxonomy to classify the existing graph representation learning\nalgorithms, i.e., graph neural network architectures, learning paradigms, and various promising\napplications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the\nstudies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer.\nFor the learning paradigms, we explore three advanced types namely supervised/semi-supervised\nlearning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate\nthe effectiveness of the learned graph representations, we provide several promising applications\nto build tight connections between representation learning and downstream tasks, such as social\nanalysis, molecular property prediction and generation, recommender systems, and traffic analysis.\nLast but not least, we present some perspectives for thought and suggest challenging directions\nthat deserve further study in the future.\nDifferences between this survey and existing ones. Up to now, there exist some other overview\npapers focusing on different perspectives of graph representation learning[17, 50, 53, 57, 227, 499,\n502, 577, 601, 603] that are closely related to ours. However, there are very few comprehensive\nreviews have summarized deep graph representation learning simultaneously from the perspective\nof diverse GNN architectures and corresponding up-to-date learning paradigms. Therefore, we\nhere clearly state their distinctions from our survey as follows. There have been several surveys\non classic graph embedding[42, 151], these works categorize graph embedding methods based on\ndifferent training objectives. Wang et al. [468] goes further and provides a comprehensive review of\nexisting heterogeneous graph embedding approaches. With the rapid development of deep learning,\nthere are a handful of surveys along this line. For example, Wu et al. [499] and Zhang et al. [577]\nmainly focus on several classical and representative GNN architectures without exploring deep\ngraph representation learning from a view of the most recent advanced learning paradigms such as\ngraph self-supervised learning and graph structure learning. Xia et al. [502] and Chami et al. [50]\njointly summarize the studies of graph embeddings and GNNs. Zhou et al. [601] explores different\ntypes of computational modules for GNNs. One recent survey under review [227] categorizes the\nexisting works in graph representation learning from both static and dynamic graphs. However,\nthese taxonomies emphasize the basic GNN methods but pay insufficient attention to the learning\nparadigms, and provide few discussions of the most promising applications, such as recommender\nsystems as well as molecular property prediction and generation. To the best of our knowledge, the\nmost relevant survey published formally is [603], which presents a review of GNN architectures\nand roughly discusses the corresponding applications. Nevertheless, this survey merely covers\nmethods up to the year of 2020, missing the latest developments in the past three years.\nTherefore, it is highly desired to summarize the representative GNN methods, the most recent\nadvanced learning paradigms, and promising applications into one unified and comprehensive\nframework. Moreover, we strongly believe this survey with a new taxonomy of literature and more\nthan 600 studies will strengthen future research on deep graph representation learning.\nContribution of this survey. The goal of this survey is to systematically review the literature\non the advances of deep graph representation learning and discuss further directions. It aims\nto help the researchers and practitioners who are interested in this area, and support them in\nunderstanding the panorama and the latest developments of deep graph representation learning.\nThe key contributions of this survey are summarized as follows:\n\u2022 Systematic Taxonomy. We propose a systematic taxonomy to organize the existing deep\ngraph representation learning approaches based on the ways of GNN architectures and the\nmost recent advanced learning paradigms via providing some representative branches of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n4 W. Ju, et al.\nGraph Self-Supervised Learning\nSemi-Supervised Learning on Graphs\nGraph Structure Learning\nLearning Paradigms\nGraph-Related Applications\nMolecular Generation\nMolecular Property Prediction\nSocial Analysis\nRecommender Systems\nTra c Analysis\nFuture Directions\nGraph Neural Network Architectures\nGraph Kernel Neural Networks\nGraph Pooling\nGraph Convolutions\nGraph Transformer\nGraph Representations\nGraph Data\nOptimized Graph Representations\nFig. 1. The architecture of this paper.\nmethods. Moreover, several promising applications are presented to illustrate the superiority\nand potential of graph representation learning.\n\u2022 Comprehensive Review. For each branch of this survey, we review the essential components\nand provide detailed descriptions of representative algorithms, and systematically summarize\nthe characteristics to make the overview comparison.\n\u2022 Future Directions. Based on the properties of existing deep graph representation learning\nalgorithms, we discuss the limitations and challenges of current methods and propose the\npotential as well as promising research directions deserving of future investigations.\n2 Background\nIn this section, we first briefly introduce some definitions in deep graph representation learning that\nneed to be clarified, and then we explain the reasons why we need graph representation learning.\n2.1 Problem Definition\nDefinition: Graph. Given a graph \ud835\udc3a = (\ud835\udc49 , \ud835\udc38, X), where \ud835\udc49 = {\ud835\udc631, \u00b7 \u00b7 \u00b7 , \ud835\udc63|\ud835\udc49 | } is the set of nodes,\n\ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52|\ud835\udc49 | } is the set of edges, and the edge \ud835\udc52 = (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38 represent the connection\nrelationship between nodes \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57in the graph. X \u2208 R\n|\ud835\udc49 |\u00d7\ud835\udc40 is the node feature matrix with\n\ud835\udc40 being the dimension of each node feature. The adjacency matrix of a graph can be defined as\nA \u2208 R\n|\ud835\udc49 |\u00d7 |\ud835\udc49 |\n, where A\ud835\udc56\ud835\udc57 = 1 if (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38, otherwise A\ud835\udc56\ud835\udc57 = 0.\nThe adjacency matrix can be regarded as the structural representation of the graph-structured\ndata, in which each row of the adjacency matrix A represents the connection relationship between\nthe corresponding node of the row and all other nodes, which can be regarded as a discrete repre\u0002sentation of the node. However, in real-life circumstances, the adjacency matrix A corresponding\nto \ud835\udc3a is a highly sparse matrix, and if A is used directly as node representations, it will be seriously\naffected by impractical storage demands and computational overhead. The storage space of the\nadjacency matrix A is |\ud835\udc49 |\u00d7 |\ud835\udc49 |, which is usually unacceptable when the total number of nodes grows\nto the order of millions. At the same time, the value of most dimensions in the node representation\nis 0. The sparsity will make subsequent machine learning tasks very difficult.\nGraph representation learning is a bridge between the original input data and the task objectives\nin the graph. The fundamental idea of the graph representation learning algorithm is first to learn\nthe embedded representations of nodes or the entire graph from the input graph structure data and\nthen apply these embedded representations to downstream related tasks, such as node classification,\ngraph classification, link prediction, community detection, and visualization, etc. Specifically, it\naims to learn low-dimensional, dense distributed embedding representations for nodes in the graph.\nFormally, the goal of graph representation learning is to learn its embedding vector representation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 5\nTable 1. Summary of traditional graph embedding methods.\nType Method Similarity measure Loss function (\ud835\udc3f)\nMatrix Factorization\nLLE [390] general |\ud835\udc67\ud835\udc56 \u2212\n\u00cd\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56 \ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57\n|\n2\nLE [11] general \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d,s.t.\ud835\udc4d\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c\nGF [4] \ud835\udc34\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nGraRep [46] \ud835\udc34\ud835\udc56,\ud835\udc57, \ud835\udc342\n\ud835\udc56,\ud835\udc57, ..., \ud835\udc34\ud835\udc58\n\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56\n, \ud835\udc67\ud835\udc57\u27e9|2\nHOPE [354] general |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nRandom Walk\nDeepWalk [362] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nNode2vec [155] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nHARP [59] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nLINE [443] Two-order Similarities Corresponding Loss\nNon-GNN Deep SDNE [460] Two-order Proximities Corresponding Loss\nDNGR [47] Two-order Proximities Corresponding Loss\n\ud835\udc45\ud835\udc63 \u2208 R\n\ud835\udc51\nfor each node \ud835\udc63 \u2208 \ud835\udc49 , where the dimension \ud835\udc51 of the vector is much smaller than the total\nnumber of nodes |\ud835\udc49 | in the graph.\n2.2 Traditional Graph Embedding\nTraditional graph embedding learning methods, as part of dimensionality reduction techniques,\naimed to embed graph data into a lower-dimensional vector space with the idea that connected nodes\nin the graph should still be closer to each other in this lower-dimensional space, thereby preserving\nthe structural information between nodes in the graph. Influenced by classical dimensionality\nreduction techniques, early graph embedding methods are primarily inspired by classic matrix\nfactorization techniques [25] and multi-dimensional scaling [245]. The following three sections\ndescribe these methods in more detail, distinguishing among matrix factorization-based methods,\nrandom walks-based methods and other non-GNN deep methods. In Table 1, we summarize different\ncategories of traditional graph embedding methods.\n2.2.1 Matrix factorization-based methods Matrix factorization-based methods are the early en\u0002deavors in graph embedding learning. These approaches can be outlined in a two-step process.\nIn the initial step, a proximity-based matrix is constructed for the graph, where each element of\nthe matrix represents the proximity measure between two nodes in the graph. Subsequently, a\ndimensionality reduction technique is employed on this matrix in the second step to generate the\nnode embeddings.\nLocally Linear Embedding (LLE) [390]. LLE assumes that node representations are sampled from\nthe same manifold space, and any node in the graph and its neighboring nodes are located in a\nlocal region of that manifold space. Therefore, node representations can be obtained by linearly\ncombining them with their neighboring nodes. LLE first constructs a local reconstruction weight\nmatrix, \ud835\udc4a\ud835\udc56\ud835\udc57 , for nodes in the graph to linearly combine neighboring nodes. By computing the\ndistance between the linear combination and the central node, the problem is reduced to solving\nfor matrix eigenvalues to learn low-dimensional vector representations for nodes. The objective\nfunction is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56 \u2212\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56\n\ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57|\n2\n, (1)\nwhere \ud835\udc67\ud835\udc56 represents the low-dimensional representation of the \ud835\udc56-th node, and \ud835\udc41\ud835\udc56is the set of\nneighboring nodes for the central node \ud835\udc56.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n6 W. Ju, et al.\nLaplacian Eigenmaps (LE) [11]. LE believes that nodes directly connected in graph data should\nbe kept as close as possible in the embedding space. Specifically, it achieves this by defining the\ndistance between connected nodes in the embedding space using the square of the Euclidean\ndistance. It transforms the final optimization objective into the computation of the Laplacian\nmatrix\u2019s eigenvectors. The objective function is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57\n|\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|\n2\ud835\udc4a\ud835\udc56\ud835\udc57 = \ud835\udc4d\ud835\udc47\n\ud835\udc3f\ud835\udc4d, s.t. \ud835\udc4d\n\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c, (2)\nwhere \ud835\udc4a\ud835\udc56\ud835\udc57 represents the connection weight between nodes \ud835\udc56 and \ud835\udc57 in the graph. After linear\ntransformation, the optimization of \ud835\udf19 (\ud835\udc4d) can be reformulated as \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d, where \ud835\udc3f = \ud835\udc37 \u2212\ud835\udc4a is the\nconstructed graph Laplacian matrix, and \ud835\udc37 is a symmetric matrix.\nGraph Factorization (GF) [4]. The matrix eigenvector-based methods mentioned before consider\nthe similarity between nodes throughout the entire graph, which can result in excellent node\nfeature representations. However, with the ever-growing scale of real-world graph data, computing\nmatrix eigenvectors for large graphs can be computationally expensive and memory-intensive.\nGF introduces a graph embedding method with a time complexity of \ud835\udc42(|\ud835\udc38|) by factorizing the\nadjacency matrix of the graph. The objective function is as follows:\n\ud835\udf19 (\ud835\udc4d, \ud835\udf06) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57 \u2208\ud835\udc38\n|\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2+\n\ud835\udf06\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56|\n2\n, (3)\nwhere \ud835\udf06 is a regularization coefficient, and \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9 represents the corresponding inner-product\noperation. Moreover, these inner-product methods also contain GraRep [46] and HOPE [354], which\nconsider higher-order and general node similarity respectively.\n2.2.2 Random walk-based methods Random walk-based methods have also attracted a lot of\nattention in graph embedding learning. The basic idea of these methods is to create random walks\namong nodes in the graph to capture its structural characteristics. Thus, nodes tend to have similar\nembedding if they co-occur on short random walks. Compared to fixed proximity measures in\ntraditional matrix factorization-based methods, these approaches use co-occurrence in a random\nwalk as a measure of node similarity, which is more flexible and has demonstrated promising\nperformance across various applications.\nDeepWalk [362]. DeepWalk analogizes nodes in a graph to words in text. It uses random walks\non the graph to generate numerous node sequences \ud835\udc46 = {\ud835\udc631, . . . , \ud835\udc63|\ud835\udc60 | }, treating these sequences as\nsentences, and then inputting them into the Word2vec [343], which aims to maximize the probability\nof node context given the target node \ud835\udc63\ud835\udc56. It can be written as:\n1\n|\ud835\udc46 |\n\u2211\ufe01\n|\ud835\udc46 |\n\ud835\udc56=1\n\u2211\ufe01\n\u2212\ud835\udc61 \u2264\ud835\udc57\u2264\ud835\udc61,\ud835\udc57\u22600\nlog \ud835\udc5d(\ud835\udc63\ud835\udc56+\ud835\udc57|\ud835\udc63\ud835\udc56), (4)\nwhere \ud835\udc61 is the context window size. Compared to matrix factorization-based methods, DeepWalk\nexhibits extremely low time complexity and is suitable for large-scale graph representation learning.\nHowever, DeepWalk only considers local information between nodes in the graph, making it\nchallenging to find the optimal random walk sampling sequences.\nNode2vec [155]. Based on DeepWalk, Node2vec utilizes parameters \ud835\udc5d and \ud835\udc5e to guide the random\nwalk. Parameter \ud835\udc5d allows the algorithm to revisit previously traversed nodes \ud835\udc61, with smaller\nvalues of \ud835\udc5d increasing the likelihood of returning to \ud835\udc61. Parameter \ud835\udc5e facilitates both inward and\noutward exploration; when \ud835\udc5e > 1, the algorithm tends to visit nodes closer to \ud835\udc61; while for \ud835\udc5e  1. And the first-order similarity can\nbe defined as:\n\ud835\udc3f2 =\n\u2211\ufe01\n(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57 ) \u2208\ud835\udc38\n\ud835\udc34\ud835\udc56\ud835\udc57 |\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|, (8)\nwhere \ud835\udc67\ud835\udc56is the learned representation of node \ud835\udc63\ud835\udc56.\nDeep Neural Graph Representations (DNGR) [47]. Similar to SDNE, DNGR utilizes pointwise\nmutual information between two nodes co-occurring in random walks instead of the adjacency\nmatrix values.\n2.3 Why study deep graph representation learning\nWith the rapid development of deep learning techniques, deep neural networks such as convolu\u0002tional neural networks and recurrent neural networks have made breakthroughs in the fields of\ncomputer vision, natural language processing, and speech recognition. They can well abstract the\nsemantic information of images, natural languages, and speeches. However, current deep learning\ntechniques fail to handle more complex and irregular graph-structured data. To effectively analyze\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n8 W. Ju, et al.\nand model this kind of non-Euclidean structure data, many graph representation learning algo\u0002rithms have emerged in recent years, including graph embedding and graph neural networks. At\npresent, compared with Euclidean-style data such as images, natural language, and speech, graph\u0002structured data is high-dimensional, complex, and irregular. Therefore, the graph representation\nlearning algorithm is a rather powerful tool for studying graph-structured data. To encode complex\ngraph-structured data, deep graph representation learning needs to meet several characteristics: (1)\ntopological properties: Graph representations need to capture the complex topological information\nof the graph, such as the relationship between nodes and nodes, and other substructure information,\nsuch as subgraphs, motif, etc; (2) feature attributes: It is necessary for graph representations to de\u0002scribe high-dimensional attribute features in the graph, including the attributes of nodes and edges\nthemselves; (3) scalability: Because different real graph data have different characteristics, graph\nrepresentation learning algorithms should be able to efficiently learn its embedding representation\non different graph structure data, making it universal and transferable.\n3 Graph Convolutions\nGraph convolutions have become the basic building blocks in many deep graph representation\nlearning algorithms and graph neural networks developed recently. In this section, we provide a\ncomprehensive review of graph convolutions, which generally fall into two categories: spectral\ngraph convolutions and spatial graph convolutions. Based on the solid mathematical foundations\nof Graph Signal Processing (GSP) [164, 396, 414], spectral graph convolutions seek to capture the\npatterns of the graph in the frequency domain. On the other hand, spatial graph convolutions\ninherit the idea of message passing from Recurrent Graph Neural Networks (RecGNNs), and they\ncompute node features by aggregating the features of their neighbors. Thus, the computation graph\nof a node is derived from the local graph structure around it, and the graph topology is naturally\nincorporated into the way node features are computed. In this section, we first introduce spectral\ngraph convolutions and then spatial graph convolutions, followed by a brief summary. In Table 2,\nwe summarize a number of graph convolutions proposed in recent years.\n3.1 Spectral Graph Convolutions\nWith the success of Convolutional Neural Networks (CNNs) in computer vision [244], efforts have\nbeen made to transfer the idea of convolution to the graph domain. However, this is not an easy task\nbecause of the non-Euclidean nature of graphical data. Graph signal processing (GSP) [164, 396, 414]\ndefines the Fourier Transform on graphs and thus provides a solid theoretical foundation of spectral\ngraph convolutions.\nIn graph signal processing, a graph signal refers to a set of scalars associated with every node\nin the graph, i.e. \ud835\udc53 (\ud835\udc63), \u2200\ud835\udc63 \u2208 \ud835\udc49 , and it can be written in the \ud835\udc5b-dimensional vector form x \u2208 R\n\ud835\udc5b\n,\nwhere \ud835\udc5b is the number of nodes in the graph. Another core concept of graph signal processing\nis the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian), defined as\nL = I \u2212 D\n\u22121/2AD\u22121/2\n, where I is the identity matrix, D is the degree matrix (i.e. a diagonal matrix\nD\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc57 A\ud835\udc56\ud835\udc57), and A is the adjacency matrix. In the typical setting of graph signal processing, the\ngraph \ud835\udc3a is undirected. Therefore, L is real symmetric and positive semi-definite. This guarantees\nthe eigen decomposition of the graph Laplacian: L = U\u039bU\ud835\udc47, where U = [u0, u1, ..., u\ud835\udc5b\u22121] is the\neigenvectors of the graph Laplacian and the diagonal elements of \u039b = diag(\ud835\udf060, \ud835\udf061, ..., \ud835\udf06\ud835\udc5b\u22121) are the\neigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal x is defined as x\u02dc = U\n\ud835\udc47 x,\nwhere x\u02dc is the graph frequencies of x. Correspondingly, the Inverse Graph Fourier Transform can\nbe written as x = Ux\u02dc.\nWith GFT and the Convolution Theorem, the graph convolution of a graph signal x and a filter\ng can be defined as g \u2217\ud835\udc3a x = U(U\n\ud835\udc47 g \u2299 U\ud835\udc47 x). To simplify this, let g\ud835\udf03 = diag(U\ud835\udc47 \ud835\udc54), the graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 9\nTable 2. Summary of graph convolution methods.\nMethod Category Aggregation Time Complexity\nSpectral CNN [39] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nHenaff et al. [172] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nChebNet [83] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGCN [230] Spectral / Spatial Weighted Average \ud835\udc42(\ud835\udc5a)\nCayleyNet [254] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGraphSAGE [163] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nGAT [452] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nDGCNN [477] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nLanzcosNet [280] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n2\n)\nSGC [493] Spatial Graph Convolution Weighted Average \ud835\udc42(\ud835\udc5a)\nGWNN [512] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGIN [518] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nGraphAIR [179] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nPNA [77] Spatial Graph Convolution Multiple \ud835\udc42(\ud835\udc5a)\nS\n2GC [606] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGNNML3 [21] Spatial / Spectral - \ud835\udc42(\ud835\udc5a)\nMSGNN [170] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nEGC [437] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nAPPNP [138] Spatial Graph Convolution (Approximate) Personalized Pagerank \ud835\udc42(\ud835\udc5a)\nGCNII [61] Spatial Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGATv2 [38] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nconvolution can be written as:\ng \u2217\ud835\udc3a x = Ug\ud835\udf03U\n\ud835\udc47\nx, (9)\nwhich is the general form of most spectral graph convolutions. The key of spectral graph convolu\u0002tions is to parameterize and learn the filter g\ud835\udf03 .\nSpectral Convolutional Neural Network (Spectral CNN) [39] sets graph filter as a learnable diagonal\nmatrix W. The convolution operation can be written as y = UWU\ud835\udc47 x. In practice, multi-channel\nsignals and activation functions are common, and the graph convolution can be written as\nY:,\ud835\udc57 = \ud835\udf0e\nU\n\u2211\ufe01\ud835\udc50\ud835\udc56\ud835\udc5b\n\ud835\udc56=1\nW\ud835\udc56,\ud835\udc57U\n\ud835\udc47 X:,\ud835\udc56!\n, \ud835\udc57 = 1, 2, ..., \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61, (10)\nwhere \ud835\udc50\ud835\udc56\ud835\udc5b is the number of input channel, \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 is the number of output channel, X is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc56\ud835\udc5b\nmatrix representing the input signal, Y is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix denoting the output signal, W\ud835\udc56,\ud835\udc57 is a\nparameterized diagonal matrix, and \ud835\udf0e(\u00b7) is the activation function. For mathematical convenience\nwe sometimes use single-channel versions of graph convolutions omitting activation functions,\nand the multi-channel versions are similar to Eq. 10.\nSpectral CNN has several limitations. Firstly, the filters are basis-dependent, which means that\nthey cannot be generalized across graphs. Secondly, the algorithm requires eigen decomposition,\nwhich is computationally expensive. Thirdly, it has no guarantee of spatial localization of filters.\nTo make filters spatially localized, Henaff et al. [172] propose to use a smooth spectral transfer\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n10 W. Ju, et al.\nfunction \u0398(\u039b) to parameterize the filter, and the convolution operation can be written as:\ny = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx. (11)\nChebyshev Spectral Convolutional Neural Network (ChebNet) [83] extends this idea by using\ntruncated Chebyshev polynomials to approximate the spectral transfer function. The Chebyshev\npolynomial is defined as \ud835\udc470 (\ud835\udc65) = 1, \ud835\udc471 (\ud835\udc65) = \ud835\udc65, \ud835\udc47\ud835\udc58 (\ud835\udc65) = 2\ud835\udc65\ud835\udc47\ud835\udc58\u22121 (\ud835\udc65) \u2212 \ud835\udc47\ud835\udc58\u22122 (\ud835\udc65), and the spectral\ntransfer function \ud835\udc39 (\u039b) is approximated to the order of \ud835\udc3e \u2212 1 as\n\ud835\udc39 (\u039b) =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc ), (12)\nwhere the model parameters \ud835\udf03\ud835\udc58, \ud835\udc58 \u2208 {0, 1, ..., \ud835\udc3e \u2212 1} are the Chebyshev coefficients, and \u039b\u02dc =\n2\u039b/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be written\nas:\ng \u2217\ud835\udc3a x = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx = U\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc )U\n\ud835\udc47\nx =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (L\u02dc)x, (13)\nwhere L\u02dc = 2L/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I.\nGraph Convolutional Network (GCN) [230] is proposed as the localized first-order approximation\nof ChebNet. Assuming \ud835\udc3e = 2 and \ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 = 2, Eq. 13 can be simplified as:\ng \u2217\ud835\udc3a x = \ud835\udf030x + \ud835\udf031 (L \u2212 I)x = \ud835\udf030x \u2212 \ud835\udf031D\n\u22121/2AD\u22121/2\nx. (14)\nTo further constraint the number of parameters, we assume \ud835\udf03 = \ud835\udf030 = \u2212\ud835\udf031, which gives a simpler\nform of graph convolution:\ng \u2217G x = \ud835\udf03 (I + D\n\u22121/2AD\u22121/2\n)x. (15)\nAs I + D\n\u22121/2AD\u22121/2 now has the eigenvalues in the range of [0, 2] and repeatedly multiplying\nthis matrix can lead to numerical instabilities, GCN empirically proposes a renormalization trick to\nsolve this problem by using D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2instead, where A\u02dc = A + I and D\u02dc\n\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc56 A\u02dc\n\ud835\udc56\ud835\udc57 .\nAllowing multi-channel signals and adding activation functions, the more common formula in\nliterature is:\nY = \ud835\udf0e( (D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2)X\u0398), (16)\nwhere X, Y have the same shape as in Eq. 10 and \u0398 is a \ud835\udc50\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix as model\u2019s parameters.\nApart from the aforementioned methods, other spectral graph convolutions have been proposed.\nLevie et al. [254] propose CayleyNets that utilize Cayley Polynomials to equip the filters with\nthe ability to detect narrow frequency bands. Liao et al. [280] propose LanczosNets that employ\nthe Lanczos algorithm to construct a low-rank approximation of graph Laplacian to improve the\ncomputation efficiency of graph convolutions. The proposed model is able to efficiently utilize the\nmulti-scale information in the graph data. Instead of using Graph Fourier Transform, Xu et al. [512]\npropose a Graph Wavelet Neural Network (GWNN) that uses graph wavelet transform to avoid\nmatrix eigendecomposition. Moreover, graph wavelets are sparse and localized, which provides\ngood interpretations for the convolution operation. Zhu and Koniusz [606] derive a Simple Spectral\nGraph Convolution (S2GC) from a modified Markov Diffusion Kernel, which achieves a trade-off\nbetween low-pass and high-pass filter bands.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 11\n3.2 Spatial Graph Convolutions\nInspired by the convolution on Euclidean data (e.g. images and texts), which applies data trans\u0002formation on a small region, spatial graph convolutions compute the central node\u2019s feature via\ntransforming and aggregating its neighbors\u2019 features. In this way, the graph structure is naturally\nembedded in the computation graph of node features. Moreover, the idea of sending one node\u2019s\nfeature to another node is similar to the message passing used in recurrent graph neural networks.\nIn the following, we will introduce several seminal spatial graph convolutions as well as some\nrecently proposed promising methods.\nSpatial graph convolutions generally follow a three-step paradigm: message generation, feature\naggregation and feature update. This can be mathematically written as:\ny\ud835\udc56 = UPDATE x\ud835\udc56, AGGREGATE {MESSAGE x\ud835\udc56, x\ud835\udc57, e\ud835\udc56\ud835\udc57\u0001, \ud835\udc57 \u2208 N (\ud835\udc56)}\u0001\u0001 , (17)\nwhere x\ud835\udc56 and y\ud835\udc56is the input and output feature vector of node \ud835\udc56, e\ud835\udc56\ud835\udc57 is the feature vector of the edge\n(or more generally, the relationship) between node \ud835\udc56 and its neighbor node \ud835\udc57, and N (\ud835\udc56) denote the\nneighbor of node \ud835\udc56, which could be more generally defined.\nIn the previous subsection, we show the spectral interpretation of GCN [230]. The model also\nhas its spatial interpretation, which can be mathematically written as:\ny\ud835\udc56 = \u0398\n\ud835\udc47 \u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\u222a\ud835\udc56\n1\n\u221a\ufe03\n\u02c6\ud835\udc51\ud835\udc56\u02c6\ud835\udc51\ud835\udc57\nx\ud835\udc57, (18)\nwhere \u02c6\ud835\udc51\ud835\udc56 and \u02c6\ud835\udc51\ud835\udc57is the \ud835\udc56-th and \ud835\udc57-th row sums of A\u02c6 in Eq. 16. For each node, the model takes a\nweighted sum of its neighbors\u2019 features as well as its own features and applies a linear transformation\nto obtain the result. In practice, multiple GCN layers are often stacked together with non-linear\nfunctions after convolution to encode complex and hierarchical features. Nonetheless, Wu et al.\n[493] show that the model still achieves competitive results without non-linearity.\nAlthough GCN as well as other spectral graph convolutions achieve competitive results on a\nnumber of benchmarks, these methods assume the presence of all nodes in the graph and fall in the\ncategory of transductive learning. Hamilton et al. [163] propose GraphSAGE that performs graph\nconvolutions in inductive settings, when there are new nodes during inference (e.g. newcomers\nin the social network). For each node, the model samples its \ud835\udc3e-hop neighbors and uses \ud835\udc3e graph\nconvolutions to aggregate their features hierarchically. Furthermore, the use of sampling also\nreduces the computation when a node has too many neighbors.\nThe attention mechanism has been successfully used in natural language processing [451], com\u0002puter vision [295] and multi-modal tasks [62, 168, 552, 591]. Graph Attention Networks (GAT) [452]\nintroduces the idea of attention to graphs. The attention mechanism uses an adaptive, feature\u0002dependent weight (i.e. attention coefficient) to aggregate a set of features, which can be mathemati\u0002cally written as:\n\ud835\udefc\ud835\udc56,\ud835\udc57 =\nexp LeakyReLU a\n\ud835\udc47\n[\u0398x\ud835\udc56||\u0398x\ud835\udc57]\n\u0001\u0001\n\u00cd\n\ud835\udc58 \u2208N (\ud835\udc56)\u222a{\ud835\udc56 } exp\nLeakyReLU a\n\ud835\udc47 [\u0398x\ud835\udc56\n||\u0398x\ud835\udc57]\n\u0001\u0001 , (19)\nwhere \ud835\udefc\ud835\udc56,\ud835\udc57 is the attention coefficient, a and \u0398 are model parameters, and [\u00b7||\u00b7] means concatenation.\nAfter the \ud835\udefcs are obtained, the new features are computed as a weighted sum of input node features,\nwhich is:\ny\ud835\udc56 = \ud835\udefc\ud835\udc56,\ud835\udc56\u0398x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\n\ud835\udefc\ud835\udc56,\ud835\udc57\u0398x\ud835\udc57. (20)\nXu et al. [518] explore the representational limitations of graph neural networks. What they\ndiscover is that message passing networks like GCN [230] and GraphSAGE [163] are incapable of\ndistinguishing certain graph structures. To improve the representational power of graph neural\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n12 W. Ju, et al.\nnetworks, they propose the Graph Isomorphism Network (GIN) that gives an adjustable weight to\nthe central node feature, which can be mathematically written as:\ny\ud835\udc56 = MLP \u00a9\n\u00ad\n\u00ab\n(1 + \ud835\udf16)x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\nx\ud835\udc57\n\u00aa\n\u00ae\n\u00ac\n, (21)\nwhere \ud835\udf16 is a learnable parameter.\nMore recently, efforts have been made to improve the representational power of graph neural\nnetworks. For example, Hu et al. [179] propose GraphAIR that explicitly models the neighborhood\ninteraction to better capture complex non-linear features. Specifically, they use the Hadamard\nproduct between pairs of nodes in the neighborhood to model the quadratic terms of neighborhood\ninteraction. Balcilar et al. [21] propose GNNML3 that breaks the limits of the first-order Weisfeiler\u0002Lehman test (1-WL) and reaches the third-order WL test (3-WL) experimentally. They also show\nthat the Hadamard product is required for the model to have more representational power than the\nfirst-order Weisfeiler-Lehman test. Other elements in spatial graph convolutions are widely studied.\nFor example, Corso et al. [77] explore the aggregation operation in GNN and proposes Principal\nNeighbourhood Aggregation (PNA) that uses multiple aggregators with degree-scalers. Tailor et al.\n[437] explore the anisotropism and isotropism in the message passing process of graph neural\nnetworks, and proposes Efficient Graph Convolution (EGC) that achieves promising results with\nreduced memory consumption due to isotropism. In order to increase the size of the neighborhood\nof a node, Gasteiger et al. [138] propose personalized propagation of neural predictions (PPNP) and\nits approximation using power iteration (APPNP). To increase the depth of graph neural networks,\nChen et al. [61] propose GCNII that uses initial residual and identity mapping to mitigate the over\u0002smoothing problem. Brody et al. [38] propose GATv2 that uses dynamic attention and improves\nthe expressive power of GAT [452].\n3.3 Summary\nThis section introduces graph convolutions. We provide the summary as follows:\n\u2022 Techniques. Graph convolutions mainly fall into two types, i.e. spectral graph convolu\u0002tions and spatial graph convolutions. Spectral graph convolutions have solid mathematical\nfoundations of Graph Signal Processing and therefore their operations have theoretical in\u0002terpretations. Spatial graph convolutions are inspired by Recurrent Graph Neural Networks\nand their computation is simple and straightforward, as their computation graph is derived\nfrom the local graph structure. Generally, spatial graph convolutions are more common in\napplications.\n\u2022 Challenges and Limitations. Despite the great success of graph convolutions, their perfor\u0002mance is unsatisfactory in more complicated applications. On the one hand, the performance\nof graph convolutions relies heavily on the construction of the graph. Different constructions\nof the graph might result in different performances of graph convolutions. On the other\nhand, graph convolutions are prone to over-smoothing when constructing very deep neural\nnetworks.\n\u2022 Future Works. In the future, we expect that more powerful graph convolutions will be\ndeveloped to mitigate the problem of over-smoothing and we also hope that techniques and\nmethodologies in Graph Structure Learning (GSL) can help learn more meaningful graph\nstructure to benefit the performance of graph convolutions.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 13\n4 Graph Kernel Neural Networks\nGraph kernels (GKs) are historically the most widely used technique on graph analyzing and\nrepresentation tasks [137, 243, 423, 601]. However, traditional graph kernels rely on hand-crafted\npatterns or domain knowledge on specific tasks[242, 410]. Over the years, an amount of research has\nbeen conducted on graph kernel neural networks (GKNNs), which has yielded promising results.\nResearchers have explored various aspects of GKNNs, including their theoretical foundations,\nalgorithmic design, and practical applications. These efforts have led to the development of a wide\nrange of GKNN-based models and methods that can be used for graph analysis and representation\ntasks, such as node classification [113, 222, 298, 534], link prediction [54, 300, 497, 525], and graph\nclustering [243, 299].\nThe success of GKNNs can be attributed to their ability to leverage the strengths of both graph\nkernels and neural networks [221, 299, 497]. By using kernel functions to measure similarity\nbetween graphs, GKNNs can capture the structural properties of graphs, while the use of neural\nnetworks enables them to learn more complex and abstract representations of graphs [56, 558].\nThis combination of techniques allows GKNNs to achieve state-of-the-art performance on a wide\nrange of graph-related tasks [216, 243, 479].\nIn this section, we begin with introducing the most representative traditional graph kernels.\nThen we summarize the basic framework for combining GNNs and graph kernels. Finally, we\ncategorize the popular graph kernel Neural networks into several categories and compare their\ndifferences.\n4.1 Graph Kernels\nGraph kernels generally evaluate pairwise similarity between nodes or graphs by decomposing\nthem into basic structural units. Random walks [223], subtrees [409], shortest paths [32] and\ngraphlets [410] are representative categories.\nGiven two graphs \ud835\udc3a1 = (\ud835\udc491, \ud835\udc381, \ud835\udc4b1) and \ud835\udc3a2 = (\ud835\udc492, \ud835\udc382, \ud835\udc4b2), a graph kernel function \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2)\nmeasures the similarity between \ud835\udc3a1 and \ud835\udc3a2 through the following formula:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 \ud835\udc59\ud835\udc3a1(\ud835\udc621),\ud835\udc59\ud835\udc3a2(\ud835\udc622)\n\u0001\n, (22)\nwhere \ud835\udc59\ud835\udc3a (\ud835\udc62) denotes a set of local substructures centered at node \ud835\udc62 in graph \ud835\udc3a, and \ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 is a base\nkernel measuring the similarity between the two sets of substructures. For simplicity, we may\nrewrite Eq. 22 as:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 (\ud835\udc621, \ud835\udc622), (23)\nthe uppercase letter \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) is denoted as graph kernels, \ud835\udf05(\ud835\udc621, \ud835\udc622) is denoted as node kernels,\nand lowercase \ud835\udc58 (\ud835\udc65, \ud835\udc66) is denoted as general kernel functions.\nThe kernel mapping of a kernel \ud835\udf13 maps a data point into its corresponding Reproducing Kernel\nHilbert Space (RKHS) H. Specifically, given a kernel \ud835\udc58\u2217 (\u00b7, \u00b7), its kernel mapping\ud835\udf13\u2217 can be formalized\nas,\n\u2200\ud835\udc651, \ud835\udc652, \ud835\udc58\u2217 (\ud835\udc651, \ud835\udc652) = \u27e8\ud835\udf13\u2217 (\ud835\udc651),\ud835\udf13\u2217 (\ud835\udc652)\u27e9H\u2217, (24)\nwhere H\u2217 is the RKHS of \ud835\udc58\u2217 (\u00b7, \u00b7).\nWe introduce several representative and popular graph kernels below.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n14 W. Ju, et al.\nWalk and Path Kernels. A \ud835\udc59-walk kernel \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 compares all length \ud835\udc59 walks starting from each\nnode in two graphs \ud835\udc3a1,\ud835\udc3a2,\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc641\u2208W\ud835\udc59(\ud835\udc3a1,\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc642\u2208W\ud835\udc59(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc4b1 (\ud835\udc641), \ud835\udc4b2 (\ud835\udc642)),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622).\n(25)\nSubstituting W with P is able to get the \ud835\udc59-path kernel.\nSubtree Kernels. The WL subtree kernel is the most popular one in subtree kernels. It is a finite\u0002depth kernel variant of the 1-WL test. The WL subtree kernel with depth \ud835\udc59, \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f compares all\nsubtrees with depth \u2264 \ud835\udc59 rooted at each node.\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc611\u2208 T\ud835\udc56(\ud835\udc3a1,\ud835\udc622 )\n\u2211\ufe01\n\ud835\udc612\u2208 T\ud835\udc56(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc611, \ud835\udc612),\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc59\n\ud835\udc56=0\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2),\n(26)\nwhere \ud835\udc61 \u2208 T (\ud835\udc56)(\ud835\udc3a, \ud835\udc62) denotes a subtree of depth \ud835\udc56 rooted at \ud835\udc62 in \ud835\udc3a.\n4.2 General Framework of GKNNs\nIn this section, we summarize the general framework of GKNNs. For the first step, a kernel that\nmeasures similarities of heterogeneous features from heterogeneous nodes and edges (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc622) and\n(\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622) should be defined. Take the inner product of neighbor tensors as an example, its neighbor\nkernel is defined as follows,\n\ud835\udf05( (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc621), (\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622)) = \u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc621), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc622)\u27e9.\nBased on the neighbor kernel, a kernel with two \ud835\udc59-hop neighborhoods for central node \ud835\udc621 and \ud835\udc622\ncan be defined as \ud835\udc3e\n(\ud835\udc59)\n(\ud835\udc621, \ud835\udc622) =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \ud835\udc59 = 0\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u2211\ufe01\n\ud835\udc631\u2208\ud835\udc41 (\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc632\u2208\ud835\udc41 (\ud835\udc622 )\n\ud835\udc3e\n(\ud835\udc59\u22121)\n(\ud835\udc631, \ud835\udc632) \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc631), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc632\n)\u27e9 \ud835\udc59 > 0\n, (27)\nBy regarding the lower-hop kernel \ud835\udf05\n(\ud835\udc59\u22121)\n(\ud835\udc621, \ud835\udc622), as the inner product of the (\ud835\udc59 \u2212 1)-th hidden\nrepresentations of \ud835\udc621 and \ud835\udc622. Furthermore, by recursively applying the neighborhood kernel, the\n\ud835\udc59-hop graph kernel can be derived as\n\ud835\udc3e\n\ud835\udc59\n(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc981\u2208W\ud835\udc59(\ud835\udc3a1 )\n\u2211\ufe01\n\ud835\udc982\u2208W\ud835\udc59(\ud835\udc3a2 )\n\u00d6\n\ud835\udc59\u22121\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n1\n), \ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n2\n)\u27e9 \u00d7\u00d6\n\ud835\udc59\u22122\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n1\n,\ud835\udc98\n(\ud835\udc56+1)\n1\n), \ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n2\n,\ud835\udc98\n(\ud835\udc56+1)\n2\n)\u27e9!,\n(28)\nwhere W\ud835\udc59(\ud835\udc3a) denotes the set of all walk sequences with length \ud835\udc59 in graph \ud835\udc3a, and \ud835\udc98\n(\ud835\udc56)\n1\ndenotes the\n\ud835\udc56-th node in sequence \ud835\udc981.\nAs shown in Eq. 24, kernel methods implicitly perform projections from original data spaces\nto their RKHS H. Hence, as GNNs also project nodes or graphs into vector spaces, connections\nhave been established between GKs and GNNs through the kernel mappings. And several works\nconducted research on the connections [253, 491], and found some foundation conclusions. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 15\nthe basic rule introduced in [253] as an example, the proposed graph kernel in Eq. 22 can be derived\nas the general formulas,\n\u210e\n(0)\n(\ud835\udc63) =\ud835\udc7e\n(0)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63),\n\u210e\n(\ud835\udc59)\n(\ud835\udc63) =\ud835\udc7e\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63) \u2299 \u2211\ufe01\n\ud835\udc62\u2208\ud835\udc41 (\ud835\udc63)\n(\ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\u210e\n(\ud835\udc59\u22121)\n(\ud835\udc62) \u2299 \ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc38 (\ud835\udc52\ud835\udc62,\ud835\udc63 )\n\ud835\udc53 (\ud835\udc52\ud835\udc62,\ud835\udc63 )), 1  1-WL\nGGT [102] \u2713 \u2713 structure only \u2713\nGTSA [241] \u2713 \u2713 \u2713 \u2713\nHGT [183] \u2713 \u2713\nG2SHGT [536] \u2713 \u2713 \u2713\nHINormer [335] \u2713 \u2713 \u2713\nGRUGT [41] \u2713 \u2713 \u2713\nGRIT [324] \u2713 \u2713 \u2713\nGraphormer-GD [560] \u2713 \u2713 \u2713\nGraphormer [540] \u2713 \u2713 \u2713 \u2713\nGSGT [191] \u2713 \u2713 \u2713\nTMDG [145] \u2713 \u2713 \u2713\nGraph-BERT [567] \u2713 \u2713 \u2713\nLRGT [498] \u2713 \u2713\nSAT [56] \u2713 \u2713 \u2713\n6.1 Transformer\nTransformer [451] was first applied to model machine translation, but two of the key mechanisms\nadopted in this work, attention operation and positional encoding, are highly compatible with the\ngraph modeling problem.\nTo be specific, we denote the input of attention layer in Transformer as X = [x0, x1, . . . , x\ud835\udc5b\u22121],\nx\ud835\udc56 \u2208 R\n\ud835\udc51\n, where \ud835\udc5b is the length of input sequence and \ud835\udc51 is the dimension of each input embedding\nx\ud835\udc56. Then the core operation of calculating new embedding x\u02c6\ud835\udc56 for each x\ud835\udc56in attention layer can be\nstreamlined as:\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57) = NORM\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\nQ\n\u210e\n(x\ud835\udc56)\nTK\u210e\n(x\ud835\udc58 )),\nx\n\u210e\n\ud835\udc56 =\n\u2211\ufe01\nx\ud835\udc57 \u2208X\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57)V\u210e(x\ud835\udc57),\nx\u02c6\ud835\udc56 = MERGE(x\n1\n\ud835\udc56\n, x\n2\n\ud835\udc56\n, . . . , x\n\ud835\udc3b\n\ud835\udc56\n),\n(57)\nwhere \u210e \u2208 {0, 1, . . . , \ud835\udc3b \u2212 1} represents the attention head number. Q\n\u210e\n, K\u210eand V\u210eare projection\nfunctions mapping a vector to the query space, key space and value space respectively. s\n\u210e\n(x\ud835\udc56\n, x\ud835\udc57) is\nscore function measuring the similarity between x\ud835\udc56 and x\ud835\udc57. NORM is the normalization operation\nensuring \u00cd\nx\ud835\udc57 \u2208X s\n\u210e\n(x\ud835\udc56, x\ud835\udc57) \u2261 1 to propel the stability of the output generated by a stack of attention\nlayers, it is usually performed as scaled softmax: NORM(\u00b7) = SoftMax(\u00b7/\u221a\ud835\udc51). And MERGE function\nis designed to combine the information extracted from multiple attention heads. Here, we omit\nfurther implementation details that do not affect our understanding of attention operation.\nThe attention process cannot encode the position information of each x\ud835\udc56, which is essential in\nmachine translation problems. So positional encoding is introduced to remedy this deficiency, and\nit\u2019s calculated as:\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57\n= sin(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), X\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57+1\n= cos(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), (58)\nwhere \ud835\udc56 is the position and \ud835\udc57 is the dimension. The positional encoding is added to the input before\nit is fed to the Transformer.\n6.2 Overview\nFrom the simplified process shown in Eq. 57, we can see that the core of the attention operation is\nto accomplish information transfer based on the similarity between the source and the target to be\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 25\nupdated. It\u2019s quite similar to the message-passing process on a fully-connected graph. However,\nthe direct application of this architecture to arbitrary graphs does not make use of structural\ninformation, so it may lead to poor performance when graph topology is important. On the other\nhand, the definition of positional encoding in graphs is not a trivial problem because the order or\ncoordinates of graph nodes are underdefined.\nAccording to these two challenges, Transformer-based methods for graph representation learning\ncan be classified into two major categories, one considering graph structure during the attention\nprocess, and the other encoding the topological information of the graph into initial node features.\nWe name the first one as Attention Modification and the second one as Encoding Enhancement. A\nsummarization is provided in Table 5. In the following discussion, if both methods are used in\none paper, we will list them in different subsections, and we will ignore the multi-head trick in\nattention operation.\n6.3 Attention Modification\nThis group of works attempts to modify the full attention operation to capture structure information.\nThe most prevalent approach is changing the score function, which is denoted as s(\u00b7, \u00b7) in Eq. 57.\nGGT [102] constrains each node feature can only attend to neighbors and enables the model to\nrepresent edge feature information by rewrite s(\u00b7, \u00b7) as:\ns\u02dc1 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\ud835\udc44x\ud835\udc56)\nT\n(W\ud835\udc3ex\ud835\udc57 \u2299 W\ud835\udc38e\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\u2212 \u221e, otherwise\n,\ns1 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc1 (x\ud835\udc56, x\ud835\udc58 )),\n(59)\nwhere \u2299 is Hadamard product and W\ud835\udc44,\ud835\udc3e,\ud835\udc38 represents trainable parameter matrix. This approach\nis not efficient yet to model long-distance dependencies since only 1st-neighbors are considered.\nThough it adopts Laplacian eigenvectors to gather global information (see Section 6.4), but only long\u0002distance structure information is remedied while the node and edge features are not. GTSA [241]\nimproves this approach by combining the original graph and the full graph. Specifically, it extends\ns1 (\u00b7, \u00b7) to:\ns\u02dc2 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\n\ud835\udc44\n1\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n1\nx\ud835\udc57 \u2299 W\ud835\udc38\n1\ne\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n(W\n\ud835\udc44\n0\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n0\nx\ud835\udc57 \u2299 W\ud835\udc38\n0\ne\ud835\udc57\ud835\udc56), otherwise\n,\ns2 (x\ud835\udc56, x\ud835\udc57) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\n\uf8f3\n1\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2208\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\ud835\udf06\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2209\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), otherwise\n,\n(60)\nwhere \ud835\udf06 is a hyperparameter representing the strength of the full connection.\nSome works try to reduce information-mixing problems [55] in heterogeneous graphs. HGT [183]\ndisentangles the attention of different node types and edge types by adopting additional attention\nheads. It defines W\ud835\udf0f (\ud835\udc63)\n\ud835\udc44,\ud835\udc3e,\ud835\udc49 for each node type \ud835\udf0f (\ud835\udc63) and W\n\ud835\udf19 (\ud835\udc52 )\n\ud835\udc38\nfor each edge type \ud835\udf19 (\ud835\udc52), \ud835\udf0f (\u00b7) and\n\ud835\udf19 (\u00b7) are type indicating function. G2SHGT [536] defines four types of subgraphs, fully-connected,\nconnected, default and reverse, to capture global, undirected, forward and backward information\nrespectively. Each subgraph is homogeneous, so it can reduce interactions between different classes.\nPath features between nodes are always treated as inductive bias added to the original score\nfunction. Let SP\ud835\udc56\ud835\udc57 = (\ud835\udc521, \ud835\udc522, . . . , \ud835\udc52\ud835\udc41 ) denote the shortest path between node pair (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57). GRUGT [41]\nuses GRU [74] to encode forward and backward features as: r\ud835\udc56\ud835\udc57 = GRU(SP\ud835\udc56\ud835\udc57), r\ud835\udc57\ud835\udc56 = GRU(SP\ud835\udc57\ud835\udc56).\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n26 W. Ju, et al.\nThen, the final attention score is calculated by adding up four components:\ns\u02dc3 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56, (61)\nfrom front to back, which represent content-based score, source-dependent bias, target-dependent\nbias and universal bias respectively. Graphormer [540] uses both path length and path embedding\nto introduce structural bias as:\ns\u02dc4 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 /\n\u221a\n\ud835\udc51 + \ud835\udc4f\ud835\udc41 + \ud835\udc50\ud835\udc56\ud835\udc57,\n\ud835\udc50\ud835\udc56\ud835\udc57 =\n1\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc58=1\n(e\ud835\udc58 )\nTw\ud835\udc38\n\ud835\udc58\n,\ns4 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc4 (x\ud835\udc56, x\ud835\udc58 )),\n(62)\nwhere \ud835\udc4f\ud835\udc41 is a trainable scalar indexed by \ud835\udc41, the length of SP\ud835\udc56\ud835\udc57 . e\ud835\udc58 is the embedding of the the\nedge \ud835\udc52\ud835\udc58 , and w\ud835\udc38\n\ud835\udc58\n\u2208 R\n\ud835\udc51\nis the \ud835\udc58-th edge parameter. If SP\ud835\udc56\ud835\udc57 does not exist, then \ud835\udc4f\ud835\udc41 and \ud835\udc50\ud835\udc56\ud835\udc57 are set to\nbe special values. GRIT [324] utilizes relative random walk probabilities as an inductive bias to\nencode relative path information. Graphormer-GD [560] also incorporates relative distance as bias,\nand rigorously proves that this bias is crucial for determining the biconnectivity of a graph.\n6.4 Encoding Enhancement\nThis kind of method intends to enhance initial node representations to enable the Transformer to\nencode structure information. They can be further divided into two categories, position-analogy\nmethods and structure-aware methods.\n6.4.1 Position-analogy methods In Euclidean space, the Laplacian operator corresponds to the\ndivergence of the gradient, whose eigenfunctions are sine/cosine functions. For the graph, the\nLaplacian operator is the Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.\nHence, inspired by Eq. 58, position-analogy methods utilize Laplacian eigenvectors to simulate\npositional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as they are the equivalents of sine/cosine functions.\nLaplacian eigenvectors can be calculated via the eigendecomposition of normalized graph Lapla\u0002cian matrix L\u02dc\n:\nL\u02dc \u225c I \u2212 D\n\u22121/2AD\u22121/2 = U\u039bUT\n, (63)\nwhere A is the adjacency matrix, D is the degree matrix, U = [u1, u2, . . . , u\ud835\udc5b\u22121] are eigenvectors\nand \u039b = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udf060, \ud835\udf061, . . . , \ud835\udf06\ud835\udc5b\u22121) are eigenvalues. With U and \u039b, GGT [102] uses eigenvectors of the\nk smallest non-trivial eigenvalues to denote the intermediate embedding X\n\ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\n, and maps it\nto d-dimensional space and gets the position encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\n. This process can be formalized\nas:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 = argmin\ud835\udc58({\ud835\udf06\ud835\udc56|0 \u2264 \ud835\udc56  0}),\nX\n\ud835\udc5a\ud835\udc56\ud835\udc51 = [u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc650\n, u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc651, . . . , u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc58\u22121]\nT\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = X\ud835\udc5a\ud835\udc56\ud835\udc51W\ud835\udc58\u00d7\ud835\udc51\n,\n(64)\nwhere \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 is the subscript of the selected eigenvectors. GTSA [241] puts eigenvector u\ud835\udc56 on\nthe frequency axis at \ud835\udf06\ud835\udc56 and uses sequence modeling methods to generate positional encoding.\nSpecifically, it extends X\n\ud835\udc5a\ud835\udc56\ud835\udc51 in Eq. 64 to X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\u00d72 by concatenating each value in eigenvectors\nwith corresponding eigenvalue, and then positional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\nare generated as:\nX\n\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 = X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51W2\u00d7\ud835\udc51\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = SumPooling(Transformer(X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61), dim = 1).\n(65)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 27\nHere, X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 R\n\ud835\udc5b\u00d7\ud835\udc58\u00d7\ud835\udc51\nis equivalent to the input matrix in sequence modeling with shape\n(\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e, \ud835\udc51\ud835\udc56\ud835\udc5a), and can be naturally processed by Transformer. Since the Laplacian\neigenvectors can be complex-valued for directed graph, GSGT [191] proposes to utilize SVD of\nadjacency matrix A, which is denoted as A = U\u03a3VT, and uses the largest \ud835\udc58 singular values \u03a3\ud835\udc58 and\nassociated left and right singular vectors U\ud835\udc58 and V\nT\n\ud835\udc58\nto output X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as X\ud835\udc5d\ud835\udc5c\ud835\udc60 = [U\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n\u2225V\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n],\nwhere \u2225 is the concatenation operation. In addition to SVD, TMDG [145] processes directed graphs\nby utilizing the Magnetic Laplacian. All these methods above randomly flip the signs of eigenvectors\nor singular vectors during the training phase to promote the invariance of the models to the sign\nambiguity.\n6.4.2 Structure-aware methods In contrast to position-analogy methods, structure-aware methods\ndo not attempt to mathematically rigorously simulate sequence positional encoding. They use some\nadditional mechanisms to directly calculate structure-related encoding.\nSome approaches compute extra encoding X\n\ud835\udc4e\ud835\udc51\ud835\udc51 and add it to the initial node representation.\nGraphormer [540] proposes to leverage node centrality as an additional signal to address the\nimportance of each node. Concretely, x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nis determined by the in-degree deg\u2212\n\ud835\udc56\nand outdegree deg+\n\ud835\udc56\n:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56 = P\n\u2212\n(deg\u2212\n\ud835\udc56\n) + P+(deg+\n\ud835\udc56\n), (66)\nwhere P\n\u2212\nand P\n+\nare learnable embedding function. Graph-BERT [567] employs Weisfeiler-Lehman\nalgorithm to label node \ud835\udc63\ud835\udc56 to a number WL(\ud835\udc63\ud835\udc56) \u2208 N and defines x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nas:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57 = sin(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n), x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57+1 = cos(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n). (67)\nThe other approaches try to leverage GNNs to initialize inputs to the Transformer. LRGT [498]\napplies GNN to get intermediate vectors as X\n\u2032 = GNN(X), and passes the concatenation of X\u2032\nand\na special vector xCLS to Transformer layer as: X\u02c6 = Transformer( [X\n\u2032\n\u2225xCLS]). Then x\u02c6CLS can be used\nas the representation of the entire graph for downstream tasks. This method cannot break the 1-WL\nbottleneck because it uses GCN [230] and GIN [518] as graph encoders in the first step, which\nare intrinsically limited by 1-WL test. SAT [56] improves this deficiency by using subgraph-GNN\nNGNN [569] for initialization, and achieves outstanding performance.\n6.5 Summary\nThis section introduces Transformer-based approaches for graph representation learning and we\nprovide the summary as follows:\n\u2022 Techniques. Graph Transformer methods modify two fundamental techniques in Trans\u0002former, attention operation and positional encoding, to enhance its ability to encode graph\ndata. Typically, they introduce fully connected attention to model long-distance relationships,\nutilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck, and separate\npoints and edges belonging to different classes to avoid over-mixing problems.\n\u2022 Challenges and Limitations. Though Graph Transformers achieve encouraging perfor\u0002mance, they still face two major challenges. The first challenge is the computational cost of\nthe quadratic attention mechanism and shortest path calculation. These operations require\nsignificant computing resources and can be a bottleneck, particularly for large graphs. The\nsecond is the reliance of Transformer-based models on large amounts of data for stable perfor\u0002mance. It poses a challenge when dealing with problems that lack sufficient data, especially\nfor few-shot and zero-shot settings.\n\u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further\nexplored. Additionally, there are some works using pre-training and fine-tuning frameworks\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n28 W. Ju, et al.\nto balance performance and complexity in downstream tasks [540], this may be a promising\nsolution to address the aforementioned two challenges.\n7 Semi-supervised Learning on Graphs\nWe have investigated various architectures of graph neural networks in which the parameters should\nbe tuned by a learning objective. The most prevalent optimization approach is supervised learning on\ngraph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention\nin the data mining community. In detail, these methods attempt to combine graph representation\nlearning with current semi-supervised techniques including pseudo-labeling, consistency learning,\nknowledge distillation and active learning. These works can be further subdivided into node-level\nrepresentation learning and graph-level representation learning. We would introduce both parts in\ndetail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6.\n7.1 Node Representation Learning\nTypically, node representation learning follows the concept of transductive learning, which has\naccess to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised\nloss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is\nusually adopted for optimization. In formulation,\nL\ud835\udc41 \ud835\udc46\ud835\udc3f = \u2212\n1\n|Y\ud835\udc3f |\n\u2211\ufe01\n\ud835\udc56\u2208Y\ud835\udc3f\ny\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (68)\nwhere Y\ud835\udc3f denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that\ncan be used to offer semantic information. To fully utilize these nodes, a range of methods attempt\nto combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a\nfundamental semi-supervised technique that uses the classifier to produce the label distribution of\nunlabeled examples and then adds appropriately labeled examples to the training set [265, 604].\nAnother line of semi-supervised learning is consistency regularization [247] that requires two\nexamples to have identical predictions under perturbation. This regularization is based on the\nassumption that each instance has a distinct label that is resistant to random perturbations [118, 357].\nThen, we show several representative works in detail.\nCooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-label\u0002based GNN approach for semi-supervised node classification. It employs two GNN classifiers to\njointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows:\n\ud835\udc36\ud835\udc49 (p\ud835\udc56) = p\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (69)\nwhere p\ud835\udc56 denotes the output label distribution. Then it selects the pseudo-labels with high confidence\ngenerated from one model to supervise the optimization of the other model. In particular, the\nobjective for unlabeled nodes is written as follows:\nL\ud835\udc36\ud835\udc5c\ud835\udc3a\ud835\udc41 \ud835\udc52\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc56\u2208V\ud835\udc48\n1\ud835\udc36\ud835\udc49 (p\ud835\udc56 )>\ud835\udf0fy\u02c6\n\ud835\udc47\n\ud835\udc56\n\ud835\udc59\ud835\udc5c\ud835\udc54q\ud835\udc56, (70)\nwhere y\u02c6\ud835\udc56 denotes the one-hot formulation of the pseudo-label \ud835\udc66\u02c6\ud835\udc56 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65p\ud835\udc56 and q\ud835\udc56 denotes\nthe label distribution predicted by the other classifier. \ud835\udf0f is a pre-defined temperature coefficient.\nThis cross supervision has been demonstrated effective in [64, 312] to prevent the provision of\nbiased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information\nfrom a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still\ngenerate the same prediction. In this way, CoGNet can illustrate the entire optimization process to\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 29\nTable 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered\nas a specific kind of consistency learning.\nApproach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning\nNode-level\nCoGNet [265] \u2713\nDSGCN [604] \u2713\nGRAND [118] \u2713\nAugGCR [357] \u2713\nHCPL [309] \u2713\nGraph-level\nSEAL [264] \u2713 \u2713\nInfoGraph [431] \u2713 \u2713\nDSGC [527] \u2713\nASGN [166] \u2713 \u2713\nTGNN [218] \u2713\nKGNN [221] \u2713\nHGMI [262] \u2713 \u2713\nASGNN [508] \u2713 \u2713\nDualGraph [310] \u2713 \u2713\nGLA [556] \u2713\nSS [507] \u2713\nenhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in\nsemi-supervised node classification, which can generate dynamics thresholds for reliable nodes.\nDynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive\nmanner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller\nweights to samples with lower confidence with the additional consideration of class balance. The\nweight is formulated as:\n\ud835\udf14\ud835\udc56 =\n1\n\ud835\udc5b\ud835\udc50\n\ud835\udc56\nmax (RELU (p\ud835\udc56 \u2212 \ud835\udefd \u00b7 1)) , (71)\nwhere \ud835\udc5b\ud835\udc50\n\ud835\udc56 denotes the number of unlabeled samples assigned to the class \ud835\udc50\n\ud835\udc56\n. This technique will\ndecrease the impact of wrong pseudo-labels during iterative training.\nGraph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learning\u0002based method. It first adds a variety of perturbations to the input graph to generate a list of\ngraph views. Each graph view \ud835\udc3a\n\ud835\udc5f\nis sent to a GNN classifier to produce a prediction matrix\nP\n\ud835\udc5f = [p\ud835\udc5f\n1\n, \u00b7 \u00b7 \u00b7 , p\n\ud835\udc5f\n\ud835\udc41\n]. Then it summarizes these matrices as:\nP =\n1\n\ud835\udc45\nP\n\ud835\udc5f\n. (72)\nTo provide more discriminative information and ensure that the matrix is row-normalized,\nGRAND sharpens the summarized label matrix into P\n\ud835\udc46\ud835\udc34 as:\nP\n\ud835\udc46\ud835\udc34\n\ud835\udc56\ud835\udc57 =\nP\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\n\u00cd\n\ud835\udc57\n\u2032=0 P\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\u2032\n, (73)\nwhere \ud835\udc47 is a given temperature parameter. Finally, consistency learning is performed by comparing\nthe sharpened summarized matrix with the matrix of each graph view. Formally, the objective is:\nL\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 =\n1\n\ud835\udc45\n\u2211\ufe01\n\ud835\udc45\n\ud835\udc5f=1\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n||P\n\ud835\udc46\ud835\udc34\n\ud835\udc56 \u2212 P\ud835\udc56\n||, (74)\nhere L\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 serves as a regularization which is combined with the standard supervised loss.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n30 W. Ju, et al.\nAugmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with\nthe generation of augmented graphs by random dropout and mixup of different order features. To\nenhance the model generalization, it borrows the idea of meta-learning to partition the training data,\nwhich improves the quality of graph augmentation. In addition, it utilizes consistency regularization\nto enhance the semi-supervised node classification.\n7.2 Graph Representation Learning\nThe objective of graph classification is to predict the property of the whole graph example.\nAssuming that the training set comprises \ud835\udc41\n\ud835\udc59\nand \ud835\udc41\n\ud835\udc62 graph samples G\ud835\udc59 = {\ud835\udc3a1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59\n} and\nG\n\ud835\udc62 = {\ud835\udc3a\ud835\udc41\n\ud835\udc59 +1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59 +\ud835\udc41\ud835\udc62\n}, the graph-level supervised loss for labeled data can be expressed as\nfollows:\nL\ud835\udc3a\ud835\udc46\ud835\udc3f = \u2212\n1\n|G\ud835\udc62 |\n\u2211\ufe01\n\ud835\udc3a\ud835\udc57 \u2208 G\ud835\udc3f\ny\n\ud835\udc57\ud835\udc47\n\ud835\udc59\ud835\udc5c\ud835\udc54p\n\ud835\udc57\n, (75)\nwhere y\n\ud835\udc57 denotes the one-hot label vector for the \ud835\udc57-th sample while p\ud835\udc57 denotes the predicted\ndistribution of \ud835\udc3a\n\ud835\udc57\n. When \ud835\udc41\n\ud835\udc62 = 0, this objective can be utilized to optimize supervised methods.\nHowever, due to the shortage of labels in graph data, supervised methods cannot reach exceptional\nperformance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph\nclassification has been developed extensively. These approaches can be categorized into pseudo\u0002labeling-based methods, knowledge distillation-based methods and contrastive learning-based\nmethods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph\nexamples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually\nutilize a teacher-student architecture, where the teacher model conducts graph representation\nlearning without label information to extract generalized knowledge while the student model\nfocuses on the downstream task. Due to the restricted number of labeled instances, the student\nmodel transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of\nthis topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning.\nTypically, these methods extract topological information from two perspectives (i.e., different\nperturbation strategies and graph encoders), and maximize the similarity of their representations\ncompared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique\nto improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166,\n508]. Then, we review these methods in detail.\nSEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node\nin a hierarchical graph. It builds two graph classifiers which generate graph representations and\nconduct semi-supervised graph classification respectively. SEAL employs a self-attention module\nto encode each graph into a graph-level representation, and then conducts message passing from a\ngraph level for final classification. SEAL can also be combined with cautious iteration and active\niteration. The former merely utilizes partial graph samples to optimize the parameters in the first\nclassifier due to the potential erroneous pseudo-labels. The second combines active learning with\nthe model, which increases the annotation efficiency in semi-supervised scenarios.\nInfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the\nsimilarity between summarized graph representations and their node representations. In particular,\nit generates node representations using the message passing mechanism and summarizes these\nnode representations into a graph representation. Let \u03a6(\u00b7, \u00b7) denote a discriminator to distinguish\nwhether a node belongs to the graph, and we have:\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 31\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e =\n| G\ud835\udc59|+| G\ud835\udc62 \u2211\ufe01|\n\ud835\udc57=1\n\u2211\ufe01\n\ud835\udc56\u2208 G\ud835\udc57\nh\n\u2212 sp \u0010\u2212\u03a6\n\u0010\nh\n\ud835\udc57\n\ud835\udc56\n, z\n\ud835\udc57\n\u0011 \u0011 i \u2212\n1\n|\ud835\udc41\n\ud835\udc57\n\ud835\udc56\n|\n\u2211\ufe01\n\ud835\udc56\n\u2032\ud835\udc57\n\u2032\n\u2208\ud835\udc41\n\ud835\udc57\n\ud835\udc56\nh\nsp \u0010\u03a6\n\u0010\nh\n\ud835\udc57\n\u2032\n\ud835\udc56\n\u2032\n, z\n\ud835\udc57\n\u0011 \u0011 i , (76)\nwhere sp(\u00b7) denotes the softplus function. \ud835\udc41\n\ud835\udc57\n\ud835\udc56\ndenotes the negative node set where nodes are not\nin \ud835\udc3a\n\ud835\udc57\n. This mutual information maximization formulation is originally developed for unsupervised\nlearning and it can be simply extended for semi-supervised graph classification. In particular,\nInfoGraph utilizes a teacher-student architecture that compares the representation across the\nteacher and student networks. The contrastive learning objective serves as a regularization by\ncombining with supervised loss.\nDual Space Graph Contrastive Learning (DSGC) [527]. DSGC is a representative contrastive\nlearning-based method. It utilizes two graph encoders. The first is a standard GNN encoder in the\nEuclidean space and the second is the hyperbolic GNN encoder. The hyperbolic GNN encoder first\nconverts graph embeddings into hyperbolic space and then measures the distance based on the\nlength of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic\nspace. Assuming the two GNNs are named as \ud835\udc531 (\u00b7) and \ud835\udc532 (\u00b7), the positive pair is denoted as:\nz\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n= exp\ud835\udc50\no\n(\ud835\udc531 (\ud835\udc3a\n\ud835\udc57\n)),\nz\n\ud835\udc57\n\ud835\udc3b\n= exp\ud835\udc50\no\n\ud835\udc532 (\ud835\udc3a\n\ud835\udc57\n)\n\u0001\n.\n(77)\nThen it selects one labeled sample and \ud835\udc41\ud835\udc35 unlabeled sample \ud835\udc3a\n\ud835\udc57\nfor graph contrastive learning in\nthe hyperbolic space. In formulation,\nL\ud835\udc37\ud835\udc46\ud835\udc3a\ud835\udc36 = \u2212 log e\n\ud835\udc51\n\ud835\udc3b\n(h\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f\ne\n\ud835\udc51\ud835\udc3b (z\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f +\n\u00cd\ud835\udc41\n\ud835\udc56=1\ne\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc3b\n\u0011\n/\ud835\udf0f\n\u2212\n\ud835\udf06\ud835\udc62\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56=1\nlog e\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\ne\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n+ e\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n,\n(78)\nwhere z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\nand z\n\ud835\udc56\n\ud835\udc3b\ndenote the embeddings for labeled graph sample\ud835\udc3a\n\ud835\udc56\nand \ud835\udc51\n\ud835\udc3b (\u00b7) denotes a distance\nmetric in the hyperbolic space. This contrastive learning objective maximizes the similarity between\nembeddings learned from two encoders compared with other samples. Finally, the contrastive\nlearning objective can be combined with the supervised loss to achieve effective semi-supervised\ncontrastive learning.\nActive Semi-supervised Graph Neural Network (ASGN) [166]. ASGN utilizes a teacher-student\narchitecture with the teacher model focusing on representation learning and the student model\ntargeting at molecular property prediction. In the teacher model, ASGN first employs a message\npassing neural network to learn node representations under the reconstruction task and then\nborrows the idea of balanced clustering to learn graph-level representations in a self-supervised\nfashion. In the student model, ASGN utilizes label information to monitor the model training based\non the weights of the teacher model. In addition, active learning is also used to minimize the\nannotation cost while maintaining sufficient performance. Typically, the teacher model seeks to\nprovide discriminative graph-level representations without labels, which transfer knowledge to the\nstudent model to overcome the potential overfitting in the presence of label scarcity.\nTwin Graph Neural Networks (TGNN) [218]. TGNN also uses two graph neural networks to\ngive different perspectives to learn graph representations. Differently, it adopts a graph kernel\nneural network to learn graph-level representations in virtue of random walk kernels. Rather than\ndirectly enforcing representation from two modules to be similar, TGNN exchanges information\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n32 W. Ju, et al.\nby contrasting the similarity structure of the two modules. In particular, it constructs a list of\nanchor graphs, \ud835\udc3a\n\ud835\udc4e1\n,\ud835\udc3a\ud835\udc4e2, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc4e\ud835\udc40 , and utilizes two graph encoders to produce their embeddings,\ni.e., {\ud835\udc67\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n, {\ud835\udc64\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n. Then it calculates the similarity distribution between each unlabeled\ngraph and anchor graphs for two modules. Formally,\n\ud835\udc5d\n\ud835\udc57\n\ud835\udc5a =\nexp cos \ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (\ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n, (79)\n\ud835\udc5e\n\ud835\udc57\n\ud835\udc5a =\nexp cos w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n. (80)\nThen, TGNN minimizes the distance between distributions from different modules as follows:\nL\ud835\udc47\ud835\udc3a\ud835\udc41 \ud835\udc41 =\n1\nG\ud835\udc48\n\u2211\ufe01\n\ud835\udc3a \ud835\udc57 \u2208 G\ud835\udc62\n1\n2\n\ud835\udc37KL\np\n\ud835\udc57\n\u2225q\n\ud835\udc57\n\u0001\n+ \ud835\udc37KL\nq\n\ud835\udc57\n\u2225p\n\ud835\udc57\n\u0001\u0001 , (81)\nwhich serves as a regularization term to combine with the supervised loss.\n7.3 Summary\nThis section introduces semi-supervised learning for graph representation learning and we provide\nthe summary as follows:\n\u2022 Techniques. Classic node classification aims to conduct transductive learning on graphs\nwith access to unlabeled data, which is a natural semi-supervised problem. Semi-supervised\ngraph classification aims to relieve the requirement of abundant labeled graphs. Here, a\nvariety of semi-supervised methods have been put forward to achieve better performance\nunder the label scarcity. Typically, they try to integrate semi-supervised techniques such as\nactive learning, pseudo-labeling, consistency learning, and consistency learning with graph\nrepresentation learning.\n\u2022 Challenges and Limitations. Despite their great success, the performance of these methods\nis still unsatisfactory, especially in graph-level representation learning. For example, DSGC\ncan only achieve an accuracy of 57% in a binary classification dataset REDDIT-BINARY. Even\nworse, label scarcity is often accompanied by unbalanced datasets and potential domain\nshifts, which provides more challenges from real-world applications.\n\u2022 Future Works. In the future, we expect that these methods can be applied to different\nproblems such as molecular property predictions. There are also works to extend graph\nrepresentation learning in more realistic scenarios like few-shot learning [51, 326]. A higher\naccuracy is always anticipated for more advanced and effective semi-supervised techniques.\n8 Graph Self-supervised Learning\nBesides supervised or semi-supervised methods, self-supervised learning (SSL) also has shown its\npowerful capability in data mining and representation embedding in recent years. In this section,\nwe investigated Graph Neural Networks based on SSL and provided a detailed introduction to a\nfew typical models. Graph SSL methods usually have a unified pipeline, which includes pretext\ntasks and downstream tasks. Pretext tasks help the model encoder to learn better representation,\nas a premise of better performance in downstream tasks. So a delicate design of pretext task is\ncrucial for Graph SSL. We would firstly introduce the overall framework of Graph SSL in Section 8.1,\nthen introduce the two kinds of pretext task design, generation-based methods and contrast-based\nmethods respectively in Section 8.2 and 8.3. A summarisation is provided in Table 7.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 33\nTable 7. Summary of methods for self-supervised Learning on Graphs. \"PT\", \"CT\" and \"UFE\" mean \"Pre\u0002training\", \"Collaborative Train\" and \"Unsupervised Feature Extracting\" respectively.\nApproach Augmentation Scheme Training Scheme Generation Target Objective Function\nGeneration-based\nGraph Completion [544] Feature Mask PT/CT Node Feature -\nAttributeMask [208] Feature Mask PT/CT PCA Node Feature -\nAttrMasking [181] Feature Mask PT Node/Edge Feature -\nMGAE [459] No Augmentation CT Node Feature -\nGAE [231] Feature Noise UFE Adjacency Matrix -\nContrast-based\nDeepWalk [362] Random Walk UFE - SkipGram\nLINE [443] Random Walk UFE - Jensen-Shannon\nGCC [375] Random Walk PT/URL - InfoNCE\nSimGCL [547] Embedding Noise UFE - InfoNCE\nSimGRACE [503] Model Noise UFE - InfoNCE\nGCA [612]\nFeature Masking &\nStructure Adjustment URL - InfoNCE\nBGRL [152]\nFeature Masking &\nStructure Adjustment URL - BYOL\n8.1 Overall framework\nConsider a featured graph G, we denote a graph encoder \ud835\udc53 to learn the representation of the\ngraph, and a pretext decoder \ud835\udc54 with specific architecture in different pretext tasks. Then the pretext\nself-supervised learning loss can be formulated as:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = \ud835\udc38G\u223cD [L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G)], (82)\nwhere D denotes the distribution of featured graph G. By minimizing L\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59 , we can learn encoder\n\ud835\udc53 with capacity to produce high-quality embedding. As for downstream tasks, we denote a graph\ndecoder \ud835\udc51 which transforms the output of graph encoder \ud835\udc53 into model prediction. The loss of\ndownstream tasks can be formulated as:\nL\ud835\udc60\ud835\udc62\ud835\udc5d = L\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66), (83)\nwhere \ud835\udc66 is the ground truth in downstream tasks. We can obverse that L\ud835\udc60\ud835\udc62\ud835\udc5d is a typical supervised\nloss. To ensure the model achieves wise graph representation extraction and optimistic prediction\nperformance, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d have to be minimized simultaneously. We introduce 3 different ways\nto minimize the two loss functions:\nPre-training. This strategy has two steps. In pre-training step, the L\ud835\udc60\ud835\udc60\ud835\udc59 is minimized to get \ud835\udc54\n\u2217\nand \ud835\udc53\n\u2217\n:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D). (84)\nThen the parameter of \ud835\udc53\n\u2217\nis kept to continue training in pretext supervised learning progress.\nThe supervised loss is minimized to get the final parameters of \ud835\udc53 and \ud835\udc51.\nmin\n\ud835\udc51,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc51, \ud835\udc53 |\ud835\udc530=\ud835\udc53\n\u2217 , G;\ud835\udc66). (85)\nCollaborative Train. In this strategy, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d are optimized simultaneously. A hyper\u0002parameter \ud835\udefc is used to balance the contribution of pretext task loss and downstream task loss.\nThe overall minimization strategy is like the traditional supervised strategy with a pretext task\nregularization:\nmin\n\ud835\udc54,\ud835\udc53 ,\ud835\udc51\n[L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G) + \ud835\udefcL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66)]. (86)\nUnsupervised Feature Extracting. This strategy is similar to the Pre-training and Fine-tuning\nstrategy in the first step to minimize pretext task loss L\ud835\udc60\ud835\udc60\ud835\udc59 and get \ud835\udc53\n\u2217\n. However, when minimizing\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n34 W. Ju, et al.\ndownstream loss L\ud835\udc60\ud835\udc62\ud835\udc5d , the encoder \ud835\udc53\n\u2217\nis fixed. Also, the training graph data are on the same dataset,\nwhich differs from the Pre-training and Fine-tuning strategy. The formulation is defined as:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D), (87)\nmin\n\ud835\udc51\nL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 \u2217, G;\ud835\udc66). (88)\n8.2 Generation-based pretext task design\nIf a model with an encoder-decoder structure can reproduce certain graph features from an in\u0002complete or perturbed graph, it indicates the encoder has the ability to extract useful graph\nrepresentation. This motivation is derived from Autoencoder [174] which originally learns on\nimage dataset. In such a case, Eq. 84 can be rewritten as:\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54(\ud835\udc53 (G)) \u02c6 , G), (89)\nwhere \ud835\udc53 (\u00b7) and \ud835\udc54(\u00b7) stand for the representation encoder and rebuilding decoder. However, feature\ninformation and structure information are both important compositions suitable to be rebuilt for\ngraph datasets. So generation-based pretext can be divided into two categories: feature rebuilding\nand structure rebuilding. We introduce several outstanding models in the following part.\nGraph Completion [544] is one of the representative methods of feature rebuilding. They mask\nsome node features to generate an incomplete graph. Then the pretext task is set as predicting the\nremoved node features. As shown in Eq. 90, this method can be formulated as a special case of\nEq. 90, letting G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and replacing G \u2212\u2192 \ud835\udc4b. The loss function is often Mean Squared Error or\nCross Entropy, depending on whether the feature is continuous or binary.\nmin\n\ud835\udc54,\ud835\udc53\nMSE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , X). (90)\nOther works make some changes to the feature settings. For example, AttrMasking [181] aims\nto rebuild both node representation and edge representation, AttributeMask [208] preprocess \ud835\udc4b\nfirstly by PCA to reduce the complexity of rebuilding features.\nOn the other hand, MGAE [459] modifies the original graph by adding noise in node representa\u0002tion, motivated by denoising autoencoder [454]. As shown in Eq. 90, we can also consider MGAE as\nan implement of Eq. 84 where G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and G \u2212\u2192 \ud835\udc4b. \ud835\udc4b\u02c6 stands for perturbed node representation.\nSince the noise is independent and random, the encoder is more robust to feature input.\nmin\n\ud835\udc54,\ud835\udc53\nBCE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , A). (91)\nAs for structure rebuilding methods, GAE [231] is the simplest instance, which can be regarded as\nan implement of Eq. 84 where G\u02c6 = G and G \u2212\u2192 \ud835\udc34. \ud835\udc34 is the adjacency matrix of the graph. Similar to\nfeature rebuilding methods, GAE compresses raw node representation vectors into low-dimensional\nembedding with its encoder. Then the adjacency matrix is rebuilt by computing node embedding\nsimilarity. The loss function is set to the error between the ground-truth adjacency matrix and\nthe recovered one, to help the model rebuild the correct graph structure. Other feature rebuilding\nmethods [558] and structure rebuilding methods [440, 487] are also increasingly being developed\nacross numerous related publications.\n8.3 Contrast-Based pretext task design\nThe mutual information maximization principle, which implements self-supervising by predicting\nthe similarity between the two augmented views, forms the foundation of contrast-based approaches.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 35\nSince mutual information represents the degree of correlation between two samples, we can\nmaximize it in augmented pairs and minimize it in random-selected pairs.\nThe contrast-based graph SSL taxonomy can be formulated as Eq. 92. The discriminator that\ncalculates the similarity of sample pairs is indicated by pretext decoder \ud835\udc54. G\n(1)\nand G\n(2)\nare two\nvariants of \ud835\udc3a that have been augmented. Since graph contrastive learning methods differ from each\nother in 1) view generation, 2) MI estimation method we introduce this methodology in these two\nperspectives.\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54[\ud835\udc53 (G\u02c6(1)), \ud835\udc53 (G\u02c6(2))]). (92)\nThe domain of contrastive-based graph SSL is witnessing an expanding body of work in a\ngrowing number of methods [176, 197, 531, 587] and applications [114, 136, 504].\n8.3.1 View generation. The traditional pipeline of contrastive learning-based models first involves\naugmenting the graph using well-crafted empirical methods, and then maximizing the consistency\nbetween different augmentations. Drawing from methods in the computer vision domain and\nconsidering the non-Euclidean structure of graph data, typical graph augmentation methods aim\nto modify the graph topologically or representationally.\nGiven graph G = (\ud835\udc34, \ud835\udc4b), the topologically augmentation methods usually modify the adjacency\nmatrix \ud835\udc34, which can be formulated as:\n\ud835\udc34\u02c6 = \ud835\udcaf\ud835\udc34 (\ud835\udc34), (93)\nwhere \ud835\udcaf\ud835\udc34 (\u00b7) is the transform function of adjacency matrix. Topology augmentation methods\nhave many variants, in which the most popular one is edge modification, given by \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\ud835\udc43 \u25e6 \ud835\udc34 + \ud835\udc44 \u25e6 (1 \u2212 \ud835\udc34). \ud835\udc43 and \ud835\udc44 are two matrices representing edge dropping and adding respectively.\nAnother method, graph diffusion, connect nodes with their k-hop neighbors with specific weight,\ndefined as: \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\u00cd\u221e\n\ud835\udc58=0\n\ud835\udefc\ud835\udc58\ud835\udc47\n\ud835\udc58\n. where \ud835\udefc and\ud835\udc47 are coefficient and transition matrix. Graph diffusion\nmethod can integrate broad topological information with local structure.\nOn the other hand, the representative augmentation modifies the node representation directly,\nwhich can be formulated as:\n\ud835\udc4b\u02c6 = \ud835\udcaf\ud835\udc4b (\ud835\udc4b), (94)\nusually \ud835\udcaf\ud835\udc4b (\u00b7) can be a simple masking operater, a.k.a. \ud835\udcaf\ud835\udc4b (\ud835\udc4b) = \ud835\udc40 \u25e6 \ud835\udc4b and \ud835\udc40 \u2208 {0, 1}\n\ud835\udc41 \u00d7\ud835\udc37 . Based\non such mask strategy, some methods propose ways to improve performance. GCA [612] preserves\ncritical nodes while giving less significant nodes a larger masking probability, where significance is\ndetermined by node centrality.\nAs introduced before, the paradigm of augmentation has been proven to be effective in contrastive\nlearning view generation. However, given the variety of graph data, it is challenging to maintain\nsemantics properly during augmentations. To preserve the valuable nature of specific graph datasets,\nThere are currently three mainly used methods: picking by trial-and-errors, trying laborious search,\nor seeking domain-specific information as guidance [214, 308, 311]. Such complicated augmentation\nmethods constrain the effectiveness and widespread application of graph contrastive learning. So\nmany newest works question the necessity of augmentation and seek other contrastive view\ngeneration methods.\nSimGCL [547] is one of the outstanding works challenging the effectiveness of graph augmenta\u0002tion. The author finds that noise can be a substitution to augmentation to produce graph views\nin specific tasks such as recommendation. After doing an ablation study about augmentation and\nInfoNCE [510], they find that the InfoNCE loss, not the augmentation of the graph, is what makes\nthe difference. It can be further explained by the importance of distribution uniformity. Contrastive\nlearning enhances model representation ability by intensifying two characteristics: The alignment\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n36 W. Ju, et al.\nof features from positive samples and the uniformity of the normalized feature distribution. SimGCL\ndirectly adds random noises to node embeddings as augmentation, to control the uniformity of the\nrepresentation distribution more effectively:\ne\n(1)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(1)\n\u2217 \ud835\udf0f\n(1)\n\ud835\udc56\n, e\n(2)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(2)\n\u2217 \ud835\udf0f\n(2)\n\ud835\udc56\n,\n\ud835\udf16 \u223c N (0, \ud835\udf0e2),\n(95)\nwhere e\ud835\udc56is a node representation in embedding space, \ud835\udf0f\n(1)\n\ud835\udc56\nand \ud835\udf0f\n(2)\n\ud835\udc56\nare two random sampled unit\nvector. The experiment results indicate that SimGCL performs better than its graph augmentation\u0002based competitors, while training time is significantly decreased.\nSimGRACE [503] is another graph contrastive learning framework without data augmentation.\nMotivated by the observation that despite encoder disruption, graph data can effectively maintain\ntheir semantics, SimGRACE takes GNN with its modified version as an encoder to produce two\ncontrastive embedding views by the same graph input. For GNN encoder \ud835\udc53 (\u00b7; \ud835\udf03), the two contrastive\nembedding views e, e\n\u2032\ncan be computed by:\ne\n(1) = \ud835\udc53 (G; \ud835\udf03), e(2) = \ud835\udc53 (G; \ud835\udf03 + \ud835\udf16 \u00b7 \u0394\ud835\udf03),\n\u0394\ud835\udf03\ud835\udc59 \u223c N (0, \ud835\udf0e2\n\ud835\udc59\n),\n(96)\nwhere \u0394\ud835\udf03\ud835\udc59 represents GNN parameter perturbation \u0394\ud835\udf03 in the \ud835\udc59th layer. SimGRACE can improve\nalignment and uniformity simultaneously, proving its capacity to produce high-quality embedding.\n8.3.2 MI estimation method. The mutual information \ud835\udc3c(\ud835\udc65, \ud835\udc66) measures the information that x and y\nshare, given a pair of random variables (\ud835\udc65, \ud835\udc66). As discussed before, mutual information is a significant\ncomponent of the contrast-based method by formulating the loss function. Mathematically rigorous\nMI is defined on the probability space, we can formulate mutual information between a pair of\ninstances (\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) as:\n\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \ud835\udc37\ud835\udc3e\ud835\udc3f (\ud835\udc5d(\ud835\udc65, \ud835\udc66)||\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66))\n= \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc5d(\ud835\udc65, \ud835\udc66)\n\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66)\n].\n(97)\nHowever, directly computing Eq. 97 is quite difficult, so we introduce several different types of\nestimation for MI:\nInfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimator. Given a\npositive sample \ud835\udc66 and several negative sample \ud835\udc66\n\u2032\n\ud835\udc56\n, a noise-contrastive estimator can be formulated\nas [611][375]:\nL = \u2212\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \u2212\ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66)\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66) +\n\u00cd\n\ud835\udc56\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66\u2032\n\ud835\udc56\n)\n], (98)\nusually the kernal function \ud835\udc54(\u00b7) can be cosine similarity or dot product.\nTriplet Loss. Intuitively, we can aim to create a distinct separation in the degree of similarity,\nensuring that positive samples are closer together and negative samples are further apart by a\ncertain distance. So we can define the loss function in the following manner [204]:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [max(\ud835\udc54(\ud835\udc65, \ud835\udc66) \u2212 \ud835\udc54(\ud835\udc65, \ud835\udc66\u2032) + \ud835\udf16, 0)], (99)\nwhere \ud835\udf16 is a hyperparameter. This function is straightforward to compute.\nBYOL Loss. Estimation without negative samples is investigated by BYOL [152]. The estimator\nis Asymmetrically structured:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [2 \u2212 2\n\ud835\udc54(\ud835\udc65) \u00b7 \ud835\udc66\n\u2225\ud835\udc54(\ud835\udc65) \u2225 \u2225\ud835\udc66\u2225\n], (100)\nnote that encoder \ud835\udc54 should keep the dimension of input and output the same.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 37\n8.4 Summary\nThis section introduces graph self-supervised learning and we provide the summary as follows:\n\u2022 Techniques. Differing from classic supervised and semi-supervised learning, self-supervised\nlearning increases a model\u2019s generalization ability and robustness while decreasing reliance\non labels. Graph SSL utilizes pretext tasks to extract inherent information from representation\ndistributions. Typical Graph SSL methods can be divided into generation-based and contrast\u0002based approaches. Generation-based methods learn an encoder with the ability to reconstruct\na graph as precisely as possible, motivated by the principles of Autoencoder. Contrast-based\nmethods, which have recently attracted significant interest, involve learning an encoder to\nminimize mutual information between relevant instances and maximize mutual information\nbetween unrelated instances.\n\u2022 Challenges and Limitations. Although graph SSL has achieved superior performance in\nmany tasks, its theoretical basis is not so solid. Many well-known methods are validated only\nthrough experiments, without providing theoretical explanations or mathematical proofs. It\nis imperative to establish a strong theoretical foundation for graph SSL.\n\u2022 Future Works. In the future we expect more graph ssl methods designed essentially by\ntheoretical proof, without dedicated designed augment process or pretext tasks by intuition.\nThis will bring us more definite mathematical properties and a less ambiguous empirical\nsense. Also, graphs are a prevalent form of data representation across diverse domains, yet\nobtaining manual labels can be prohibitively expensive. Expanding the applications of graph\nSSL to broader fields is a promising avenue for future research.\n9 Graph Structure Learning\nGraph structure determines how node features propagate and affect each other, playing a crucial\nrole in graph representation learning. In some scenarios the provided graph is incomplete, noisy, or\neven has no structure information at all. Recent research also finds that graph adversarial attacks\n(i.e., modifying a small number of node features or edges), can degrade learned representations\nsignificantly. These issues motivate graph structure learning (GSL), which aims to learn a new\ngraph structure to produce optimal graph representations. According to how edge connectivity is\nmodeled, there are three different approaches in GSL, namely metric-based approaches, model-based\napproaches, and direct approaches. Besides edge modeling, regularization is also a common trick to\nmake the learned graph satisfy some desired properties. We first present the basic framework and\nregularization methods for GSL in Sec. 9.1 and Sec. 9.2, respectively, and then introduce different\ncategories of GSL in Sec. 9.3, 9.4 and 9.5. We summarize GSL approaches in Table 8.\n9.1 Overall Framework\nWe denote a graph by G = (A, X), where A \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is the adjacency matrix and X \u2208 R\ud835\udc41 \u00d7\ud835\udc40 is\nthe node feature matrix with \ud835\udc40 being the dimension of each node feature. A graph encoder \ud835\udc53\ud835\udf03\nlearns to represent the graph based on node features and graph structure for task-specific objective\nL\ud835\udc61 (\ud835\udc53\ud835\udf03 (A, X)). In the GSL setting, there is also a graph structure learner which aims to build a\nnew graph adjacency matrix A\n\u2217\nto optimize the learned representation. Besides the task-specific\nobjective, a regularization term can be added to constrain the learned structure. So the overall\nobjective function of GSL can be formulated as\nmin\n\ud835\udf03,A\u2217\nL = L\ud835\udc61 (\ud835\udc53\ud835\udf03 (A\n\u2217\n, X)) + \ud835\udf06L\ud835\udc5f (A\n\u2217\n, A, X), (101)\nwhere L\ud835\udc61is the task-specific objective, L\ud835\udc5fis the regularization term and \ud835\udf06 is a hyperparameter for\nthe weight of regularization.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n38 W. Ju, et al.\nTable 8. Summary of graph structure learning methods.\nMethod Structure Learning Regularization\nSparsity Low-rank Smoothness\nMetric-based\nAGCN [267] Mahalanobis distance\nGRCN [546] Inner product \u2713\nCAGCN [613] Inner product \u2713\nGNNGUARD [571] Cosine similarity\nIDGL [65] Cosine similarity \u2713 \u2713 \u2713\nHGSL [586] Cosine similarity \u2713\nGDC [139] Graph diffusion \u2713\nModel-based\nGLN [364] Recurrent blocks\nGLCN [199] One-layer neural network \u2713 \u2713\nNeuralSparse [595] Multi-layer neural network \u2713\nGAT [452] Self-attention\nGaAN [566] Gated attention\nhGAO [132] Hard attention \u2713\nVIB-GSL [433] Dot-product attention \u2713\nMAGNA [461] Graph attention diffusion\nDirect\nGLNN [135] MAP estimation \u2713 \u2713\nPro-GNN [210] Direct optimization \u2713 \u2713 \u2713\nGSML [458] Bilevel optimization \u2713\nLSD-GNN [124] Bilevel optimization\nBGCNN [573] Bayesion optimization\nVGCN [104] Stochastic variational inference\n9.2 Regularization\nThe goal of regularization is to constrain the learned graph to satisfy some properties by adding\nsome penalties to the learned structure. The most common properties used in GSL are sparsity, low\nlank, and smoothness.\n9.2.1 Sparsity Noise or adversarial attacks will introduce redundant edges into graphs and degrade\nthe quality of graph representation. An effective technique to remove unnecessary edges is sparsity\nregularization, i.e., adding a penalty on the number of nonzero entries of the adjacency matrix\n(\u21130-norm) [458, 546, 586, 595]:\nL\ud835\udc60\ud835\udc5d = \u2225A\u22250, (102)\nhowever, \u21130-norm is not differentiable so optimizing it is difficult, and in many cases \u21131-norm\nis used instead as a convex relaxation. Other methods to impose sparsity include pruning and\ndiscretization [124, 613]. These processes are also called postprocessing since they usually happen\nafter the adjacency matrix is learned. Pruning removes part of the edges according to some crite\u0002ria [613]. For example, edges with weights lower than a threshold, or those not in the top-K edges\nof nodes or graphs. Discretization is applied to generate graph structure by sampling from some\ndistribution [124]. Compared to directly learning edge weights, sampling enjoys the advantage\nof controlling the generated graph, but has issues during optimizing since sampling itself is dis\u0002crete and hard to optimize. Reparameterization and Gumbel-softmax are two useful techniques to\novercome such issues, and are widely adopted in GSL.\n9.2.2 Low Rank In real-world graphs, similar nodes are likely to group together and form commu\u0002nities, which should lead to a low-rank adjacency matrix. Recent work also finds that adversarial\nattacks tend to increase the rank of the adjacency matrix quickly [65, 210]. Therefore, low-rank\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 39\nregularization is also a useful tool to make graph representation learning more robust:\nL\ud835\udc59\ud835\udc5f = \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc58 (A). (103)\nIt is hard to minimize matrix rank directly. A common technique is to optimize the nuclear norm,\nwhich is a convex envelope of the matrix rank:\nL\ud835\udc5b\ud835\udc50 = \u2225A\u2225\u2217 =\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56\n\ud835\udf0e\ud835\udc56, (104)\nwhere \ud835\udf0e\ud835\udc56 are singular values of A. Entezari et al. replaces the learned adjacency matrix with rank-r\napproximation by singular value decomposition (SVD) to achieve robust graph learning against\nadversarial attacks.\n9.2.3 Smoothness A common assumption is that connected nodes share similar features, or in other\nwords, the graph is \u201csmooth\u201d as the difference between local neighbors is small [65, 135, 199, 210].\nThe following metric is a natural way to measure graph smoothness:\nL\ud835\udc60\ud835\udc5a =\n1\n2\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56,\ud835\udc57=1\n\ud835\udc34\ud835\udc56\ud835\udc57 (\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n2 = \ud835\udc61\ud835\udc5f(X\u22a4\n(D \u2212 A)X) = \ud835\udc61\ud835\udc5f(X\n\u22a4\nLX), (105)\nwhere D is the degree matrix of A and L = D \u2212 A is called graph Laplacian. A variant is to use the\nnormalized graph Laplacian bL = D\n\u2212\n1\n2 LD\u2212\n1\n2 .\n9.3 Metric-based Methods\nMetric-based methods measure the similarity between nodes as the edge weights. They follow\nthe basic assumption that similar nodes tend to have connections with each other. We show some\nrepresentative works\nAdaptive Graph Convolutional Neural Networks (AGCN) [267]. AGCN learns a task-driven adaptive\ngraph during training to enable a more generalized and flexible graph representation model. After\nparameterizing the distance metric between nodes, AGCN is able to adapt graph topology to the\ngiven task. It proposes a generalized Mahalanobis distance between two nodes with the following\nformula:\nD(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) =\n\u221a\ufe03\n(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n\u22a4\ud835\udc40(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57), (106)\nwhere \ud835\udc40 = \ud835\udc4a\ud835\udc51\ud835\udc4a \u22a4\n\ud835\udc51\nand \ud835\udc4a\ud835\udc51 is the trainable weights to minimize task-specific objective. Then the\nGaussian kernel is used to obtain the adjacency matrix:\nG\ud835\udc56\ud835\udc57 = exp(\u2212D(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57)/(2\ud835\udf0e\n2\n)), (107)\n\ud835\udc34\u02c6 = \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52 (G). (108)\nGraph-Revised Convolutional Network (GRCN) [546]. GRCN uses a graph revision module to\npredict missing edges and revise edge weights through joint optimization on downstream tasks. It\nfirst learns the node embedding with GCN and then calculates pair-wise node similarity with the\ndot product as the kernel function.\n\ud835\udc4d = \ud835\udc3a\ud835\udc36\ud835\udc41\ud835\udc54 (\ud835\udc34, \ud835\udc4b), (109)\n\ud835\udc46\ud835\udc56\ud835\udc57 =\n\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\n. (110)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n40 W. Ju, et al.\nThe revised adjacency matrix is the residual summation of the original adjacency matrix\ud835\udc34\u02c6 = \ud835\udc34+\ud835\udc46.\nGRCN also applies a sparsification technique on the similarity matrix \ud835\udc46 to reduce computation cost:\n\ud835\udc46\n(\ud835\udc3e)\n\ud835\udc56\ud835\udc57 =\n\u001a\n\ud835\udc46\ud835\udc56\ud835\udc57, \ud835\udc46\ud835\udc56\ud835\udc57 \u2208 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n0, \ud835\udc46\ud835\udc56\ud835\udc57 \u2209 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n. (111)\nThreshold pruning is also a common strategy for sparsification. For example, CAGCN [613] uses\ndot product to measure node similarity, and refines the graph structure by removing edges between\nnodes whose similarity is less than a threshold \ud835\udf0f\ud835\udc5f and adding edges between nodes whose similarity\nis greater than another threshold \ud835\udf0f\ud835\udc4e.\nDefending Graph Neural Networks against Adversarial Attacks (GNNGuard) [571]. GNNGuard\nmeasures similarity between a node \ud835\udc62 and its neighbor \ud835\udc63 in the \ud835\udc58-th layer by cosine similarity and\nnormalizes node similarity at the node level within the neighborhood as follows:\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\u210e\n\ud835\udc58\n\ud835\udc62 \u2299 \u210e\n\ud835\udc58\n\ud835\udc63\n\u2225\u210e\n\ud835\udc58\n\ud835\udc62 \u22252 \u2225\u210e\n\ud835\udc58\n\ud835\udc63 \u22252\n, (112)\n\ud835\udefc\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63/\n\u2211\ufe01\n\ud835\udc63\u2208N\ud835\udc62\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u00d7 \ud835\udc41\u02c6 \ud835\udc58\ud835\udc62\n/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 \u2260 \ud835\udc63\n1/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 = \ud835\udc63\n, (113)\nwhere N\ud835\udc62 denotes the neighborhood of node \ud835\udc62 and \ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 =\n\u00cd\n\ud835\udc63\u2208N\ud835\udc62\n\u2225\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u22250. To stabilize GNN training,\nit also proposes a layer-wise graph memory by keeping part of the information from the previous\nlayer in the current layer. Similar to GNNGuard, IDGL [65] uses multi-head cosine similarity and\nmask edges with node similarity smaller than a non-negative threshold, and HGSL [586] generalizes\nthis idea to heterogeneous graphs.\nGraph Diffusion Convolution (GDC) [139]. GDC replaces the original adjacency matrix with\ngeneralized graph diffusion matrix S:\nS =\n\u2211\ufe01\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58T\n\ud835\udc58\n, (114)\nwhere \ud835\udf03\ud835\udc58 is the weighting coefficient and T is the generalized transition matrix. To ensure conver\u0002gence, GDC further requires that \u00cd\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58 = 1 and the eigenvalues of T lie in [0, 1]. The random\nwalk transition matrix T\ud835\udc5f \ud835\udc64 = AD\u22121and the symmetric transition matrix T\ud835\udc60\ud835\udc66\ud835\udc5a = D\n\u22121/2AD\u22121/2\nare\ntwo examples. This new graph structure allows graph convolution to aggregate information from a\nlarger neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying noise.\nHowever, in most cases graph diffusion will result in a dense adjacency matrix \ud835\udc46, so sparsification\ntechnology like top-k filtering and threshold filtering will be applied to graph diffusion. Following\nGDC, there are some other graph diffusion proposed. For example, AdaCAD [281] proposes Class\u0002Attentive Diffusion, which further considers node features and aggregates nodes probably of the\nsame class among K-hop neighbors. Adaptive diffusion convolution (ADC) [585] learns the optimal\nneighborhood size via optimizing a bi-level problem.\n9.4 Model-based Methods\nModel-based methods parameterize edge weights with more complex models like deep neural\nnetworks. Compared to metric-based methods, model-based methods offer greater flexibility and\nexpressive power.\nGraph Learning Network (GLN) [364]. GLN proposes a recurrent block to first produce interme\u0002diate node embeddings and then merge them with adjacency information as the output of this\nlayer to predict the adjacency matrix for the next layer. Specifically, it uses convolutional graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 41\noperations to extract node features, and creates a local-context embedding based on node features\nand the current adjacency matrix:\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc56=1\n\ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\n), (115)\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 = \ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc48\n(\ud835\udc59)\n), (116)\nwhere \ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\nand \ud835\udc48\n(\ud835\udc59)\nare the learnable weights. GLN then predicts the next adjacency matrix as\nfollows:\n\ud835\udc34\n(\ud835\udc59+1) = \ud835\udf0e\ud835\udc59 (\ud835\udc40(\ud835\udc59)\n\ud835\udefc\ud835\udc59 (\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59)\ud835\udc40\n(\ud835\udc59) \u22a4\n). (117)\nSimilarly, GLCN [199] models graph structure with a softmax layer over the inner product\nbetween the difference of node features and a learnable vector. NeuralSparse [595] uses a multi\u0002layer neural network to generate a learnable distribution from which a sparse graph structure is\nsampled. PTDNet [305] prunes graph edges with a multi-layer neural network and penalizes the\nnumber of non-zero elements to encourage sparsity.\nGraph Attention Networks (GAT) [452]. Besides constructing a new graph to guide the message\npassing and aggregation process of GNNs, many recent researchers also leverage the attention\nmechanism to adaptively model the relationship between nodes. GAT is the first work to introduce\nthe self-attention strategy into graph learning. In each attention layer, the attention weight between\ntwo nodes is calculated as the Softmax output on the combination of linear and non-linear transform\nof node features:\n\ud835\udc52\ud835\udc56\ud835\udc57 = \ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57), (118)\n\ud835\udefc\ud835\udc56\ud835\udc57 =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc57)\n\u00cd\n\ud835\udc58 \u2208N\ud835\udc56\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc58 )\n, (119)\nwhere N\ud835\udc56 denotes the neighborhood of node \ud835\udc56,W is learnable linear transform and \ud835\udc4e is pre-defined\nattention function. In the original implementation of GAT, \ud835\udc4e is a single-layer neural network with\nLeakyReLU:\n\ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57) = LeakyReLU(\u00aea\n\u22a4\n[W\u00ae\u210e\ud835\udc56||W\u00ae\u210e\ud835\udc57]). (120)\nThe attention weights are then used to guide the message-passing phase of GNNs:\n\u00ae\u210e\n\u2032\n\ud835\udc56 = \ud835\udf0e(\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n\ud835\udefc\ud835\udc56\ud835\udc57W\u00ae\u210e\ud835\udc57), (121)\nwhere \ud835\udf0e is a nonlinear function. It is beneficial to concatenate multiple heads of attention to\u0002gether to get a more stable and generalizable model, so-called multi-head attention. The attention\nmechanism serves as a soft graph structure learner which captures important connections within\nnode neighborhoods. Following GAT, many recent works propose more effective and efficient\ngraph attention operators to improve performance. GaAN [566] adds a soft gate at each attention\nhead to adjust its importance. MAGNA [461] proposes a novel graph attention diffusion layer to\nincorporate multi-hop information. One drawback of graph attention is that the time and space\ncomplexities are both \ud835\udc42(\ud835\udc41\n3\n). hGAO [132] performs hard graph attention by limiting node attention\nto its neighborhood. VIB-GSL [433] adopts the information bottleneck principle to guide feature\nmasking in order to drop task-irrelevant information and preserve actionable information for the\ndownstream task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n42 W. Ju, et al.\n9.5 Direct Methods\nDirect methods treat edge weights as free learnable parameters. These methods enjoy more flexibility\nbut are also more difficult to train. The optimization is usually carried out in an alternating way,\ni.e., iteratively updating the adjacency matrix A and the GNN encoder parameters \ud835\udf03.\nGLNN [135]. GLNN uses MAP estimation to learn an optimal adjacency matrix for a joint\nobjective function including sparsity and smoothness. Specifically, it targets at finding the most\nprobable adjacency matrix \ud835\udc34\u02c6 given graph node features \ud835\udc65:\n\ud835\udc34\u02dc\ud835\udc40\ud835\udc34\ud835\udc43 (\ud835\udc65) = argmax\n\ud835\udc34\u02c6\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6)\ud835\udc54(\ud835\udc34\u02c6), (122)\nwhere \ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) measures the likelihood of observing \ud835\udc65 given \ud835\udc34\u02c6, and \ud835\udc54(\ud835\udc34\u02c6) is the prior distribution of\n\ud835\udc34\u02c6. GLNN uses sparsity and property constraint as prior, and defines the likelihood function \ud835\udc53 as:\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) = \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n\ud835\udc3f\ud835\udc65) (123)\n= \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n(\ud835\udc3c \u2212 \ud835\udc34\u02c6)\ud835\udc65), (124)\nwhere \ud835\udf060 is a parameter. This likelihood imposed a smoothness assumption on the learned graph\nstructure. Some other works also model the adjacency matrix in a probabilistic manner. Bayesian\nGCNN [573] adopts a Bayesian framework and treats the observed graph as a realization from a\nfamily of random graphs. Then it estimates the posterior probablity of labels given the observed\ngraph adjacency matrix and features with Monte Carlo approximation. VGCN [104] follows a\nsimilar formulation and estimates the graph posterior through stochastic variational inference.\nPro-GNN [210] learns a clean graph structure from perturbed data and optimizes parameters for a\nrobust GNN, leveraging properties like sparsity, low rank, and feature smoothness.\nGraph Sparsification via Meta-Learning (GSML) [458]. GSML formulates GSL as a meta-learning\nproblem and uses bi-level optimization to find the optimal graph structure. The goal is to find a\nsparse graph structure that leads to high node classification accuracy at the same time given labeled\nand unlabeled nodes. To achieve this, GSML makes the inner optimization as training on the node\nclassification task, and targets the outer optimization at the sparsity of the graph structure, which\nformulates the following bi-level optimization problem:\n\ud835\udc3a\u02c6\n\u2217 = min\n\ud835\udc3a\u02c6 \u2208\u03a6(\ud835\udc3a)\n\ud835\udc3f\ud835\udc60\ud835\udc5d\ud835\udc60 (\ud835\udc53\ud835\udf03\n\u2217 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc48 ), (125)\n\ud835\udc60.\ud835\udc61 . \ud835\udf03 \u2217 = argmin\n\ud835\udf03\n\ud835\udc3f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (\ud835\udc53\ud835\udf03 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc3f). (126)\nIn this bi-level optimization problem, \ud835\udc3a\u02c6 \u2208 \u03a6(\ud835\udc3a) are the meta-parameters and optimized directly\nwithout parameterization. Similarly, LSD-GNN [124] also uses bi-level optimization. It models graph\nstructure with a probability distribution over the graph and reformulates the bi-level program in\nterms of the continuous distribution parameters.\n9.6 Summary\nIn this section, we provide the summary as follows:\n\u2022 Techniques. GSL aims to learn an optimized graph structure for better graph representations.\nIt is also used for more robust graph representation against adversarial attacks. According\nto the way of edge modeling, we categorize GSL into three groups: metric-based methods,\nmodel-based methods, and direct methods. Regularization is also a commonly used principle\nto make the learned graph structure satisfy specific properties including sparsity, low-rank\nand smoothness.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 43\n\u2022 Challenges and Limitations. Since there is no way to access the ground truth or optimal\ngraph structure as training data, the learning objective of GSL is either indirect (e.g., perfor\u0002mance on downstream tasks) or manually designed (e.g., sparsity and smoothness). Therefore,\nthe optimization of GSL is difficult and the performance is not satisfying. In addition, many\nGSL methods are based on homophily assumption, i.e., similar nodes are more likely to\nconnect with each other. However, many other types of connection exist in the real world\nwhich impose great challenges for GSL.\n\u2022 Future Works. In the future we expect more efficient and generalizable GSL methods to\nbe applied to large-scale and heterogeneous graphs. Most existing GSL methods focus on\npair-wise node similarities and thus struggle to scale to large graphs. Besides, they often\nlearn homogeneous graph structure, but in many scenarios graphs are heterogeneous.\n10 Social Analysis\nIn the real world, there usually exist complex relations and interactions between people and multiple\nentities. Taking people, concrete things, and abstract concepts in society as nodes and taking the\ndiverse, changeable, and large-scale connections between data as links, we can form massive and\ncomplex social information as social networks [43, 436]. Compared with traditional data structures\nsuch as texts and forms, modeling social data as graphs has many benefits. Especially with the arrival\nof the \"big data\" era, more and more heterogeneous information is interconnected and integrated,\nand it is difficult and uneconomical to model this information with a traditional data structure. The\ngraph is an effective implementation for information integration, as it can naturally incorporate\ndifferent types of objects and their interactions from heterogeneous data sources [349, 411]. A\nsummarization of social analysis applications is provided in Table 9.\n10.1 Concepts of Social Networks\nA social network is usually composed of multiple types of nodes, link relationships, and node\nattributes, which inherently include rich structural and semantic information. Specifically, a social\nnetwork can be homogeneous or heterogeneous and directed or undirected in different scenarios.\nWithout loss of generality, we define the social network as a directed heterogeneous graph \ud835\udc3a =\n{\ud835\udc49 , \ud835\udc38, T, R}, where \ud835\udc49 = {\ud835\udc5b\ud835\udc56 }\n|\ud835\udc49 |\n\ud835\udc56=1\nis the node set, \ud835\udc38 = {\ud835\udc52\ud835\udc56 }\n|\ud835\udc38|\n\ud835\udc56=1\nis the edge set, T = {\ud835\udc61\ud835\udc56 }\n| T |\n\ud835\udc56=1\nis the node\ntype set, and R = {\ud835\udc5f\ud835\udc56 }\n| R |\n\ud835\udc56=1\nis the edge type set. Each node \ud835\udc5b\ud835\udc56 \u2208 \ud835\udc49 is associated with a node type\nmapping: \ud835\udf19\ud835\udc5b (\ud835\udc5b\ud835\udc56) = \ud835\udc61\ud835\udc57: \ud835\udc49 \u2212\u2192 T and each edge \ud835\udc52\ud835\udc56 \u2208 \ud835\udc38 is associated with a node type mapping:\n\ud835\udf19\ud835\udc52 (\ud835\udc52\ud835\udc56) = \ud835\udc5f\ud835\udc57: \ud835\udc38 \u2212\u2192 R. A node \ud835\udc5b\ud835\udc56 may have a feature set, where the feature space is specific for the\nnode type. An edge \ud835\udc52\ud835\udc56is also represented by node pairs (\ud835\udc5b\ud835\udc57, \ud835\udc5b\ud835\udc58 ) at both ends and can be directed\nor undirected with relation-type-specific attributes. If |T | = 1 and |R| = 1, the social network is a\nhomogeneous graph; otherwise, it is a heterogeneous graph.\nAlmost any data produced by social activities can be modeled as social networks, for example,\nthe academic social network produced by academic activities such as collaboration and citation,\nthe online social network produced by user following and followed on social media, and the\nlocation-based social network produced by human activities on different locations. Based on\nconstructing social networks, researchers have new paths to data mining, knowledge discovery,\nand multiple application tasks on social data. Exploring social networks also brings new challenges.\nOne of the critical challenges is how to succinctly represent the network from the massive and\nheterogeneous raw graph data, that is, how to learn continuous and low-dimensional social network\nrepresentations, so as to researchers can efficiently perform advanced machine learning techniques\non the social network data for multiple application tasks, such as analysis, clustering, prediction,\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n44 W. Ju, et al.\nTable 9. A summarization of social analysis applications\nSocial networks Node type Edge type Applications References\nAcademic\nSocial\nNetwork\nAuthor,\nPublication,\nVenue,\nOrganization,\nKeyword\nAuthorship,\nCo-Author,\nAdvisor\u0002advisee,\nCiting, Cited,\nCo-Citing,\nPublishing\nClassification/\nClustering\nPaper/author classification [92, 370, 471,\n563], name disambiguation [52, 319, 368,\n575]\nRelationship\nprediction\nCo-authorship [69, 72, 610], citation rela\u0002tionship [203, 464, 550], advisor-advisee re\u0002lationship [286, 317, 594]\nRecommen\u0002dation\nCollaborator recommendation [235, 236,\n296], paper recommendation [20, 81, 429],\nvenue recommendation [337, 549]\nSocial\nMedia\nNetwork\nUser, Blog,\nArticle, Image,\nVideo\nFollowing,\nLike, Unlike,\nClicked,\nViewed,\nCommented,\nReposted\nAnomaly\ndetection\nMalicious attacks [294, 395, 434], emer\u0002gency detection [28, 79, 257], and robot dis\u0002covery [117, 304]\nSentiment\nanalysis\nCustomer feedback [389, 449, 572], public\nevents [33, 332, 450]\nInfluence\nanalysis\nImportant node finding [91, 386], informa\u0002tion diffusion modeling [226, 246, 356, 562]\nLocation-based\nSocial\nNetwork\nRestaurant,\nCinema, Mall,\nParking\nFriendship,\nCheck-in\nPOI recom\u0002mendation\nSpatial/temporal influence [416, 484, 589],\nsocial relationship [297, 513], textual infor\u0002mation [469, 483, 515]\nUrban\ncomputing\nTraffic congestion prediction [202, 511], ur\u0002ban mobility analysis [45, 539], event de\u0002tection [420, 548]\nand knowledge discovery. Thus, graph representation learning on the social network becomes the\nfoundational technique for social analysis.\n10.2 Academic Social Network\nAcademic collaboration is a common and important behavior in academic society, and also a major\nway for scientists and researchers to innovate and breakthrough scientific research, which leads to\nsocial relationship between scholars. The academic data generated by academic collaboration usually\ncontains a large number of interconnected entities with complex relationships [237, 602]. Normally,\nin an academic social network, the node type set consists of Author, Publication, Venue, Organization,\nKeyword, etc., and the relation set consists of Authorship, Co-Author, Advisor-advisee, Citing, Cited,\nCo-Citing, Publishing, Co-Word, etc. Note that in most social networks, each relation type always\nconnects two fixed node types with a fixed direction. For example, the relation Authorship points\nfrom the node type Author to Publication, and the Co-Author is an undirected relation between two\nnodes with type Author. Based on the node and relation types in an academic social network, one\ncan divide it into multiple categories. For example, the co-author network with nodes of Author and\nrelations of Co-Author, the citation network with nodes of Publication and relation of Citing, and the\nacademic heterogeneous information graph with multiple academic node and relation types. Many\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 45\nresearch institutes and academic search engines, such as Aminer1, DBLP2, Microsoft Academic\nGraph (MAG)3, have provided open academic social network datasets for research purposes.\nThere are multiple applications of graph representation learning on the academic social net\u0002work. Roughly, they can be divided into three categories\u2013academic entity classification/clustering,\nacademic relationship prediction, and academic resource recommendation.\n\u2022 Academic entities usually belong to different classes of research areas. Research of academic\nentity classification and clustering aims to categorize these entities, such as papers and\nauthors, into different classes [92, 213, 370, 471, 537, 563]. In literature, academic networks\nsuch as Cora, citepSeer, and Pubmed [406] have become the most widely used benchmark\ndatasets for examining the performance of graph representation learning models on paper\nclassification. Also, the author name disambiguation problem [52, 319, 368, 575] is also\nessentially a node clustering task on co-author networks and is usually solved by the graph\nrepresentation learning technique.\n\u2022 Academic relationship prediction represents the link prediction task on various academic\nrelations. Typical applications are co-authorship prediction [69, 72, 610] and citation rela\u0002tionship prediction [203, 464, 550]. Existing methods learn representations of authors and\npapers and use the similarity between two nodes to predict the link probability. Besides, some\nwork [286, 317, 594] studies the problem of advisor-advisee relationship prediction in the\ncollaboration network.\n\u2022 Various academic recommendation systems have been introduced to retrieve academic re\u0002sources for users from large amounts of academic data in recent years. For example, collabo\u0002rator recommendation [235, 236, 296] benefit researchers by finding suitable collaborators\nunder particular topics; paper recommendation [20, 81, 429] help researchers find relevant pa\u0002pers on given topics; venue recommendation [337, 549] help researchers choose appropriate\nvenues when they submit papers.\n10.3 Social Media Network\nWith the development of the Internet in decades, various online social media have emerged in large\nnumbers and greatly changed people\u2019s traditional social models. People can establish friendships\nwith others beyond the distance limit and share interests, hobbies, status, activities, and other\ninformation among friends. These abundant interactions on the Internet form large-scale complex\nsocial media networks, also named online social networks. Usually, in an academic social network,\nthe node type set consists of User, Blog, Article, Image, Video, etc., and the relation type set consists\nof Following, Like, Unlike, Clicked, Viewed, Commented, Reposted, etc. The main property of a social\nmedia network is that it usually contains multi-mode information on the nodes, such as video,\nimage, and text. Also, the relations are more complex and multiplex, including the explicit relations\nsuch as Like and Unlike and the implicit relations such as Clicked. The social media network can\nbe categorized into multiple types based on their media categories. For example, the friendship\nnetwork, the movie review network, and the music interacting network are extracted from different\nsocial media platforms. In a broad sense, the user-item networks in online shopping system can also\nbe viewed as social media networks as they also exist on the Internet and contains rich interactions\nby people. There are many widely used data sources for social media network analysis, such as\nTwitter, Facebook, Weibo, YouTube, and Instagram.\n1https://www.aminer.cn/\n2https://dblp.uni-trier.de/\n3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n46 W. Ju, et al.\nThe mainstream application research on social media networks via graph representation learning\ntechniques mainly includes anomaly detection, sentiment analysis, and influence analysis.\n\u2022 Anomaly detection aims to find strange or unusual patterns in social networks, which has\na wide range of application scenarios, such as malicious attacks [294, 395, 434], emergency\ndetection [28, 79], and robot discovery [117, 304] in social networks. Unsupervised anomaly\ndetection usually learns a reconstructed graph to detect those nodes with higher reconstructed\nerror as the anomaly nodes [5, 588]; Supervised methods model the problem as a binary\nclassification task on the learned graph representations [340, 596].\n\u2022 Sentiment analysis, also named as opinion mining, is to mine the sentiment, opinions, and\nattitudes, which can help enterprises understand customer feedback on products [389, 449,\n572] and help the government analyze the public emotion and make rapid response to public\nevents [33, 332, 450]. The graph representation learning model is usually combined with\nRNN-based [58, 561] or Transformer-based [7, 441] text encoders to incorporate both the\nuser relationship and textual semantic information.\n\u2022 Influence analysis usually aims to find several nodes in a social network to initially spread\ninformation such as advertisements, so as to maximize the final spread of information [91, 386].\nThe core challenge is to model the information diffusion process in the social network. Deep\nlearning methods [226, 246, 356, 562] usually leverage graph neural networks to learn node\nembeddings and diffusion probabilities between nodes.\n10.4 Location-based Social Network\nLocations are the fundamental information of human social activities. With the wide availability of\nmobile Internet and GPS positioning technology, people can easily acquire their precise locations\nand socialize with their friends by sharing their historical check-ins on the Internet. This opens up\na new avenue of research on location-based social network analysis, which gathered significant\nattention from the user, business, and government perspectives. Usually, in a location-based social\nnetwork, the node type set consists of User, and Location, also named Point of Interest(POI) in the\nrecommendation scenario containing multiple categories such as Restaurant, Cinema, Mall, Parking,\netc. The relation type set consists of Friendship, Check-in. Also, those node and relation types that\nexist in traditional social media networks can be included in a location-based social network. The\ndifference with other social networks, the main location-based social networks are spatial and\ntemporal, making the graph representation learning more challenging. For example, in a typical\nsocial network constructed for the POI recommendation, the user nodes are connected with each\nother by their friendship. The location nodes are connected by user nodes with the relations feature\nof timestamps. The location nodes also have a spatial relationship with each other and own have\ncomplex features, including categories, tags, check-in counts, number of users check-in, etc. There\nare many location-based social network datasets, such as Foursquare4, Gowalla5, and Waze6. Also,\nmany social media such as Twitter, Instagram, and Facebook can provide location information.\nThe research of graph representation learning on location-based social networks can be divided\ninto two categories: POI recommendation for business benefits and urban computing for public\nmanagement.\n\u2022 POI recommendation is one of the research hotspots in the field of location-based social\nnetworks and recommendation systems in recent years [195, 219, 489], which aim to uti\u0002lize historical check-ins of users and auxiliary information to recommend potential favor\n4https://foursquare.com/\n5https://www.gowalla.com/\n6https://www.waze.com/live-map/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 47\nplaces for users from a large of location points. Existing researches mainly integrate four\nessential characteristics, including spatial influence, temporal influence [416, 484, 589], social\nrelationship [297, 513], and textual information [469, 483, 515].\n\u2022 Urban computing is defined as a process of analysis of the large-scale connected urban data\ncreated from city activities of vehicles, human beings, and sensors [358, 359, 417]. Besides the\nlocal-based social network, the urban data also includes physical sensors, city infrastructure,\ntraffic roads, and so on. Urban computing aims to improve the quality of public management\nand life quality of people living in city environments. Typical applications including traffic\ncongestion prediction [202, 511], urban mobility analysis [45, 539], event detection [420, 548].\n10.5 Summary\nThis section introduces social analysis by graph representation learning and we provide the\nsummary as follows:\n\u2022 Techniques. Social networks, generated by human social activities, such as communication,\ncollaboration, and social interactions, typically involve massive and heterogeneous data, with\ndifferent types of attributes and properties that can change over time. Thus, social network\nanalysis is a field of study that explores the techniques to understand and analyze the complex\nattributes, heterogeneous structures, and dynamic information of social networks. Social\nnetwork analysis typically learns low-dimensional graph representations that capture the\nessential properties and patterns of the social network data, which can be used for various\ndownstream tasks, such as classification, clustering, link prediction, and recommendation.\n\u2022 Chanllenges and Limitations. Despite the structural heterogeneity in social networks\n(nodes and relations have different types), with the technological advances in social media,\nthe node attributes have become more heterogeneous now, containing text, video, and images.\nAlso, the large-scale problem is a pending issue in social network analysis. The data in\nthe social network has increased exponentially in past decades, containing a high density\nof topological links and a large amount of node attribute information, which brings new\nchallenges to the efficiency and effectiveness of traditional network representation learning\non the social network. Lastly, social networks are often dynamic, which means the network\ninformation usually changes over time, and this temporal information plays a significant\nrole in many downstream tasks, such as recommendations. This brings new challenges to\nrepresentation learning on social networks in incorporating temporal information.\n\u2022 Future Works. Recently, multi-modal big pre-training models that can fuse information\nfrom different modalities have gained increasing attention [369, 379]. These models can\nobtain valuable information from a large amount of unlabeled data and transfer it to various\ndownstream analysis tasks. Moreover, Transformer-based models have demonstrated better\neffectiveness than RNNs in capturing temporal information. In the future, there is potential\nfor introducing multi-modal big pre-training models in social network analysis. Also, it is\nimportant to make the models more efficient for network information extraction and use\nlightweight techniques like knowledge distillation to further enhance the applicability of the\nmodels. These advancements can lead to more effective social network analysis and enable\nthe development of more sophisticated applications in various domains.\n11 Molecular Property Prediction\nMolecular Property Prediction is an essential task in computational drug discovery and cheminfor\u0002matics. Traditional quantitative structure property/activity relationship (QSPR/QSAR) approaches\nare based on either SMILES or fingerprints [344, 522, 570], largely overlooking the topological\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n48 W. Ju, et al.\nfeatures of the molecules. To address this problem, graph representation learning has been widely\napplied to molecular property prediction. A molecule can be represented as a graph where nodes\nstand for atoms and edges stand for atom-bonds (ABs). Graph-level molecular representations\nare learned via the message passing mechanism to incorporate the topological information. The\nrepresentations are then utilized for the molecular property prediction tasks.\nSpecifically, a molecule is denoted as a topological graph G = (V, E), where V = {\ud835\udc63\ud835\udc56|\ud835\udc56 =\n1, . . . , |G|} is the set of nodes representing atoms. A feature vector x\ud835\udc56is associated with each node \ud835\udc63\ud835\udc56\nindicating its type such as Carbon, Nitrogen. E = {\ud835\udc52\ud835\udc56\ud835\udc57 |\ud835\udc56, \ud835\udc57 = 1, . . . , |G|} is the set of edges connecting\ntwo nodes (atoms) \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57 representing atom bonds. Graph representation learning methods\nare used to obtain the molecular representation hG. Then downstream classification or regression\nlayers \ud835\udc53 (\u00b7) are applied to predict the probability of target property of each molecule \ud835\udc66 = \ud835\udc53 (hG).\nIn Section 11.1, we introduce 4 types of molecular properties graph representation learning can\nbe treated and their corresponding datasets. Section 11.2 reviews the graph representation learning\nbackbones applied to molecular property prediction. Strategies for training the molecular property\nprediction methods are listed in Section 11.3.\n11.1 Molecular Property Categorization\nPlenty of molecular properties can be predicted by graph-based methods. We follow Wieder et al.\n[490] to categorize them into 4 types: quantum chemistry, physicochemical properties, biophysics,\nand biological effect.\nQuantum chemistry is a branch of physical chemistry focused on the application of quantum\nmechanics to chemical systems, including conformation, partial charges and energies. QM7, QM8,\nQM9 [501], COD [391] and CSD [154] are datasets for quantum chemistry prediction.\nPhysicochemical properties are the intrinsic physical and chemical characteristics of a substance,\nsuch as bioavailability, octanol solubility, aqueous solubility and hydrophobicity. ESOL, Lipophilicity\nand Freesolv [501] are datasets for physicochemical properties prediction.\nBiophysics properties are about the physical underpinnings of biomolecular phenomena, such\nas affinity, efficacy and activity. PDBbind [466], MUV, and HIV [501] are biophysics property\nprediction datasets.\nBiological effect properties are generally defined as the response of an organism, a population,\nor a community to changes in its environment, such as side effects, toxicity and ADMET. Tox21,\ntoxcast [501] and PTC [448] are biological effect prediction datasets.\nMoleculenet [501] is a widely-used benchmark dataset for molecule property prediction. It\ncontains over 700,000 compounds tested on different properties. For each dataset, they provide\na metric and a splitting pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv,\nLipophilicity and PDBbind are regression tasks, using MAE or RMSE as the evaluation metrics.\nOther tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation metric.\n11.2 Molecular Graph Representation Learning Backbones\nSince node attributes and edge attributes are crucial to molecular representation, most works use\nGNN instead of traditional graph representation learning methods as backbones, since many GNN\nmethods consider edge information. Existing GNNs designed for the general domain can be applied\nto molecular graphs. Table 10 summarizes the GNNs used for molecular property prediction and\nthe types of properties they can be applied to predict.\nFurthermore, many works customize their GNN structure by considering the chemical domain\nknowledge.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 49\nTable 10. Summary of GNNs in molecular property prediction.\nType Spatial/Specrtal Method Application\nReccurent GNN - R-GNN Biological effect [400]\nReccurent GNN - GGNN Quantum chemistry [333],\nBiological effect [9, 115, 492]\nReccurent GNN - IterRefLSTM Biophysics [9], Biological effect [9]\nConvolutional GNN Spatial/Specrtal GCN\nQuantum chemistry [280, 492, 529],\npysicochemical properties [75, 101, 393],\nBiophysics [34, 101, 529]\nBiological effect [261, 501]\nConvolutional GNN Specrtal LanczosNet Quantum chemistry [280]\nConvolutional GNN Specrtal ChebNet Physicochemical properties,\nBiophysics, Biological effect [267]\nConvolutional GNN Spatial GraphSAGE\nPhysicochemical properties [181],\nBiophysics [68, 108, 279],\nBiological effect [181, 328]\nConvolutional GNN Spatial GAT\nPhysicochemical properties [3, 181],\nBiophysics [34, 68],\nBiological effect [181]\nConvolutional GNN Spatial DGCNN Biophysics [63], Biological effect [568]\nConvolutional GNN Spatial GIN\nPhysicochemical properties [34, 181],\nBiophysics [180, 181],\nBiological effect [181]\nConvolutional GNN Spatial MPNN Physicochemical [320]\nTransformer - MAT Physicochemical, Biophysics [616]\n\u2022 First, the chemical bonds and molecule interaction are taken into consideration carefully. For\nexample, Ma et al. [320] use an additional edge GNN to model the chemical bonds separately.\nSpecifically, given an edge (\ud835\udc63,\ud835\udc64), they formulate an Edge-based GNN as:\nm\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = AGGedge ({h\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(\ud835\udc58\u22121)\n\ud835\udc62\ud835\udc63 , x\ud835\udc62 |\ud835\udc62 \u2208 N\ud835\udc63 \\ \ud835\udc64}), h\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = MLPedge ({m\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(0)\n\ud835\udc63\ud835\udc64 }), (127)\nwhere h\n(0)\n\ud835\udc63\ud835\udc64 = \ud835\udf0e(Weine\ud835\udc63\ud835\udc64) is the input state of the Edge-based GNN, Wein \u2208 R\n\ud835\udc51hid\u00d7\ud835\udc51\ud835\udc52\nis the\ninput weight matrix. PotentialNet [115] further uses different message passing operations\nfor different edge types. DGNN-DDI [325] leverage dual graph neural networks to model the\ninteraction between two molecules.\n\u2022 Second, motifs in molecular graphs play an important role in molecular property prediction.\nGSN [34] leverage substructure encoding to construct a topologically-aware message-passing\nmethod. Each node \ud835\udc63 updates its state h\n\ud835\udc61\n\ud835\udc63\nby combining its previous state with the aggregated\nmessages:\nh\n\ud835\udc61+1\n\ud835\udc63 = UP\ud835\udc61+1\nh\n\ud835\udc61\n\ud835\udc63\n, m\n\ud835\udc61+1\n\ud835\udc63\n\u0001\n, (128)\nm\n\ud835\udc61+1\n\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-v)\nor\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-e)\n, (129)\nwhere x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 contains the substructure information associated with nodes and\nedges, [] denotes a multiset. Yu et al. [551] constructs a heterogeneous graph using motifs\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n50 W. Ju, et al.\nand molecules. Motifs and molecules are both treated as nodes and the edges model the\nrelationship between motifs and graphs, for example, if a graph contains a motif, there will\nbe an edge between them. MGSSL [580] leverages a retrosynthesis-based algorithm BRICS\nand additional rules to find the motifs and combines motif layers with atom layers. It is a\nhierarchical framework jointly modeling atom-level information and motif-level information.\nAouichaoui et al. [12] introduce group-contribution-based attention to highlight the most\nsubstructures in molecules.\n\u2022 Third, different feature modalities have been used to improve molecular graph embedding.\nLin et al. [283] combine SMILES modality and graph modality with contrastive learning.\nZhu et al. [608] encode 2D molecular graph and 3D molecular conformation with a unified\nTransformer. It uses a unified model to learn 3D conformation generation given 2D graph\nand 2D graph generation given 3D conformation. Cremer et al.[78] use a Equivariant Graph\nNeural Networks to represent the 3D information of molecules. Liu et al. [293] consider\nmolecular chirality and design a chirality-aware molecular convolution module.\n\u2022 Finally, knowledge graph and literature can provide additional knowledge for molecular\nproperty prediction. Fang et al. [110] introduce a chemical element knowledge graph to\nsummarize microscopic associations between elements and augment the molecular graph\nbased on the knowledge graph, and a knowledge-aware message-passing network is used to\nencode the augmented graph. MuMo [428] introduces biomedical literature to guide molecular\nproperty prediction. It pretrains a GNN and a language model on paired data of molecules\nand literature mentions via contrastive learning:\n\u2113\n(z\n\ud835\udc3a\n\ud835\udc56\n,z\n\ud835\udc47\n\ud835\udc56\n)\n\ud835\udc56\n= \u2212 log\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\n)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc57=1\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc57\n)/\ud835\udf0f)\n, (130)\nwhere z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\nare the representation of molecule and its corresponding literature. Zhao et al.\n[583] propose a unified Transformer architecture to jointly model molecule graph and the\ncorresponding bioassay description.\n11.3 Training strategies\nDespite the encouraging performance achieved by GNNs, the traditional supervised training scheme\nof GNNs faces a severe limitation: The scarcity of available molecules with desired properties.\nAlthough there are a large number of molecular graphs in public databases such as PubChem,\nlabeled molecules are hard to acquire due to the high cost of wet-lab experiments and quantum\nchemistry calculations. Directly training GNNs on such limited molecules in a supervised way\nis prone to over-fitting and lack of generalization. To address this issue, few-shot learning and\nself-supervised learning are widely used in molecular property prediction.\nFew-shot learning. Few-shot learning aims at generalizing to a task with a small labeled data\nset. The prediction of each property is treated as a single task. Metric-based and optimization-based\nfew-shot learning have been adopted for molecular property prediction. Metric-based few-shot\nlearning is similar to nearest neighbors and kernel density estimation, which learns a metric or\ndistance function over objects. IterRefLSTM [9] leverages matching network [455] as the few\u0002shot learning framework, calculating the similarity between support samples and query samples.\nOptimization-based few-shot learning optimizes a meta-learner for parameter initialization which\ncan be fast adapted to new tasks. Meta-MGNN [161] adopts MAML [120] to train a parameter\ninitialization to adapt to different tasks and use self-attentive task weights for each task. PAR [474]\nalso uses MAML framework and learns an adaptive relation graph among molecules for each task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 51\nSelf-supervised learning. Self-supervised learning can pre-train a GNN model with plenty of\nunlabeled molecular graphs and transfer it to specific molecular property prediction tasks. Self\u0002supervised learning contains generative methods and predictive methods. Predictive methods design\nprediction tasks to capture the intrinsic data features. Pre-GNN [181] exploits both node-level and\ngraph-level prediction tasks including context prediction, attribute masking, graph-level property\nprediction and structural similarity prediction. MGSSL [580] provides a motif-based generative\npre-training framework making topology prediction and motif generation iteratively. Contrastive\nmethods learn graph representations by pulling views from the same graph close and pushing\nviews from different graphs apart. Different views of the same graph are constructed by graph\naugmentation or leveraging the 1D SMILES and 3D structure. MolCLR [478] augments molecular\ngraphs by atom masking, bond deletion and subgraph removal and maximizes the agreement\nbetween the original molecular graph and augmented graphs. Fang et al. [110] uses a chemical\nknowledge graph to guide the graph augmentation. SMICLR [366] uses contrastive learning across\nSMILES and 2D molecular graphs. GeomGCL [268] leverages graph contrastive learning to capture\nthe geometry of the molecule across 2D and 3D views. Jiang et al. [201] and Fang et al. [111] integrate\nmolecule graphs with chemical knowledge graph and fuse the two modalities with contrastive\nlearning. Self-supervised learning can also be combined with few-shot learning to fully leverage\nthe hierarchical information in the training set [215].\n11.4 Summary\nThis section introduces graph representation learning in molecular property prediction and we\nprovide the summary as follows:\n\u2022 Techniques. For molecular property prediction, a molecule is represented as a graph whose\nnodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN, GAT, and GraphSAGE\nare adopted to learn the graph-level representation. The representations are then fed into a\nclassification or regression head for the molecular property prediction tasks. Many works\nguide the model structure design with medical domain knowledge including chemical bond\nfeatures, motif features, different modalities of molecular representation, chemical knowledge\ngraph and literature. Due to the scarcity of available molecules with desired properties,\nfew-shot learning and contrastive learning are used to train molecular property prediction\nmodels, so that the model can leverage the information in large unlabeled dataset and can be\nadapted to new tasks with a few examples.\n\u2022 Challenges and Limitations. Despite the great success of graph representation learning\nin molecular property prediction, the methods still have limitations: 1) Few-shot molecular\nproperty prediction are not fully explored. 2) Most methods depend on training with labeled\ndata, but neglect the chemical domain knowledge.\n\u2022 Future Works. In the future, we expect that: 1) More few-shot learning and zero-shot learning\nmethods are studied for molecular property prediction to solve the data scarcity problem. 2)\nHeterogeneous data can be fused for molecular property prediction. There are a large amount\nof heterogeneous data about molecules such as knowledge graphs, molecule descriptions\nand property descriptions. They can be considered to assist molecular property prediction. 3)\nChemical domain knowledge can be leveraged for the prediction model. For example, when\nwe perform affinity prediction, we can consider molecular dynamics knowledge.\n12 Molecular Generation\nMolecular generation is pivotal to drug discovery, where it serves a fundamental role in downstream\ntasks like molecular docking [341] and virtual screening [457]. The goal of molecular generation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n52 W. Ju, et al.\nis to produce chemical structures that satisfy a specific molecular profile, e.g., novelty, binding\naffinity, and SA scores. Traditional methods have relied on 1D string formats like SMILES [148] and\nSELFIES [240]. With the recent advances in graph representation learning, numerous graph-based\nmethods have also emerged, where molecular graph G can naturally embody both 2D topology and\n3D geometry. While recent literature reviews [99, 342] have covered the general topics of molecular\ndesign, this chapter is dedicated to the applications of graph representation learning in the molecular\ngeneration task. Molecular generation is intrinsically a de novo task, where molecular structures\nare generated from scratch to navigate and sample from the vast chemical space. Therefore, this\nchapter does not discuss tasks that restrict chemical structures a priori, such as docking [131, 427]\nand conformation generation [412, 607].\n12.1 Taxonomy for molecular featurization methods\nThis section categorizes the different methods to feature molecules. The taxonomy presented here\nis unique to the task of molecular generation, owing to the various modalities of molecular entities,\ncomplex interactions with other bio-molecular systems and formal knowledge from the laws of\nchemistry and physics.\n2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one thing, a\nmolecule can be unambiguously represented by its 2D topological graph G2D, where atoms are\nnodes and bonds are edges. G2D can be encoded by canonical MPNN models like GCN [230],\nGAT [452], and R-GCN [401], in ways similar to tasks like social networks and knowledge graphs. A\ntypical example of this line of work is GCPN [543], a graph convolutional policy network generating\nmolecules with desired properties such as synthetic accessibility and drug-likeness.\nFor another, the 3D conformation of a molecule can be accurately depicted by its 3D geometric\ngraph G3D, which incorporates 3D atom coordinates. In 3D-GNNs like SchNet [405] and Orb\u0002Net [371], G3D is organized into a \ud835\udc58-NN graph or a radius graph according to the Euclidean distance\nbetween atoms. It is justifiable to approximate G3D as a 3D extension to G2D, since covalent atoms\nare closest to each other in most cases. However, G3D can also find a more long-standing origin\nin the realm of computational chemistry [126], where both covalent and non-covalent atomistic\ninteractions are considered to optimize the potential surface and simulate molecular dynamics.\nTherefore, G3D more realistically represents the molecular geometry, which makes a good fit for\nprotein pocket binding and 3D-QSAR optimization [453].\nMolecules can rotate and translate, affecting their position in the 3D space. Therefore, it is ideal to\nencode these molecules with GNNs equivariant/invariant to roto-translations, which can be \u223c 103\ntimes more efficient than data augmentation [144]. Equivariant GNNs can be based on irreducible\nrepresentation [10, 24, 37, 130, 446], regular representation [121, 192], or scalarization [190, 212,\n232\u2013234, 292, 398, 404, 405, 445], which are explained in more detail in [165]. Recent works like\nGraphVF [430] and MolCode [579] have been incorporating G2D and G3D to accurately capture the\nrelationship between structure and properties in molecular design in a unified way.\nUnbounded vs. binding-based. Earlier works have aimed to generate unbounded molecules in\neither 2D or 3D space, striving to learn good molecular representations through this task. In the 2D\nscenario, GraphNVP [329] first introduces a flow-based model to learn an invertible transformation\nbetween the 2D chemical space and the latent space. GraphAF [413] further adopts an autoregressive\ngeneration scheme to check the valence of the generated atoms and bonds. In the 3D scenario,\nG-SchNet [142] first proposes to utilize G3D (instead of 3D density grids) as the generation backbone.\nIt encodes G3D via SchNet, and uses an auxiliary token to generate atoms on the discretized 3D\nspace autoregressively. G-SphereNet [316] uses symmetry-invariant representations in a spherical\ncoordinate system (SCS) to generate atoms in the continuous 3D space and preserve equivariance.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 53\nUnbounded models adopt certain techniques to optimize specific properties of the generated\nmolecules. GCPN and GraphAF use scores like logP, QED, and chemical validity to tune the model\nvia reinforcement learning. EDM [178] can generate 3D molecules with property \ud835\udc50 by re-training\nthe diffusion model with \ud835\udc50\u2019s feature vector concatenated to the E(n) equivariant dynamics function\n\ud835\udf50\u02c6\ud835\udc61 = \ud835\udf19 (\ud835\udc9b\ud835\udc61, [\ud835\udc61, \ud835\udc50]). cG-SchNet [143] adopts a conditioning network architecture to jointly target\nmultiple electronic properties during conditional generation without the need to re-train the model.\nRetMol [481] uses a retrieval-based model for controllable generation.\nOn the other hand, binding-based methods generate drug-like molecules (aka. ligands) according\nto the binding site (aka. binding pocket) of a protein receptor. Drawing inspirations from the lock\u0002and-key model for enzyme action [122], works like LiGAN [380] and DESERT [302] uses 3D\ndensity grids to fit the density surface between the ligand and the receptor, encoded by 3D-CNNs.\nMeanwhile, a growing amount of literature has adopted G3D for representing ligand and receptor\nmolecules, because G3D more accurately depicts molecular structures and atomistic interactions\nboth within and between the ligand and the receptor. Representative works include 3D-SBDD [306],\nGraphBP [288], Pocket2Mol [361], and DiffSBDD [403]. GraphBP shares a similar workflow with G\u0002SphereNet, except that the receptor atoms are also incorporated into G3D to depict the 3D geometry\nat the binding pocket.\nAtom-based vs.fragment-based. Molecules are inherently hierarchical structures. At the atom\u0002istic level, molecules are represented by encoding atoms and bonds. At a coarser level, molecules\ncan also be represented as molecular fragments like functional groups or chemical sub-structures.\nBoth the composition and the geometry are fixed within a given fragment, e.g., the planar peptide\u0002bond (\u2013CO\u2013NH\u2013) structure. Fragment-based generation effectively reduces the degree of freedom\n(DOF) of chemical structures, and injects well-established knowledge about molecular patterns\nand reactivity. JT-VAE [207] decomposes 2D molecular graph G2D into a junction-tree structure\nT, which is further encoded via tree message-passing. DeepScaffold [270] expands the provided\nmolecular scaffold into 3D molecules. L-Net [272] adopts a graph U-Net architecture and devises\na custom three-level node clustering scheme for pooling and unpooling operations in molecular\ngraphs. A number of works have also emerged lately for fragment-based generation in the binding\u0002based setting, including FLAG [581] and FragDiff [360]. FLAG uses a regression-based approach to\nsequentially decide the type and torsion angle of the next fragment to be placed at the binding site,\nand finally optimizes the molecule conformation via a pseudo-force field. FragDiff also adopts a\nsequential generation process but uses a diffusion model to determine the type and pose of each\nfragment in one go.\n12.2 Generative methods for molecular graphs\nFor a molecular graph generation process, the model first learns a latent distribution \ud835\udc43 (\ud835\udc4d |G)\ncharacterizing the input molecular graphs. A new molecular graph G\u02c6 is then generated by sam\u0002pling and decoding from this learned distribution. Various models have been adopted to generate\nmolecular graphs, including generative adversarial network (GAN), variational auto-encoder (VAE),\nnormalizing flow (NF), diffusion model (DM), and autoregressive model (AR).\nGenerative adversarial network (GAN). GAN [149] is trained to discriminate real data \ud835\udc99\nfrom generated generated data \ud835\udc9b, with the training object formalized as\nmin\n\ud835\udc3a\nmax\n\ud835\udc37\nL (\ud835\udc37,\ud835\udc3a) = E\ud835\udc99\u223c\ud835\udc5ddata [log\ud835\udc37(\ud835\udc99)] + E\ud835\udc9b\u223c\ud835\udc5d (\ud835\udc9b) [log(1 \u2212 \ud835\udc37(\ud835\udc3a(\ud835\udc9b)))], (131)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n54 W. Ju, et al.\nwhere \ud835\udc3a(\u00b7) is the generator function and \ud835\udc37(\u00b7) is the discriminator function. For example, Mol\u0002GAN [82] encodes G2D with R-GCN, trains \ud835\udc37 and \ud835\udc3a with improved W-GAN [13], and uses rein\u0002forcement learning to generate attributed molecules, where the score function is assigned from\nRDKit [249] and chemical validity.\nVaraitional auto-encoder (VAE). In VAE [228], the decoder parameterizes the conditional\nlikelihood distribution \ud835\udc5d\ud835\udf03 (\ud835\udc99|\ud835\udc9b), and the encoder parameterizes an approximate posterior distribution\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99) \u2248 \ud835\udc5d\ud835\udf03 (\ud835\udc9b|\ud835\udc99). The model is optimized by the evidence lower bound (ELBO), consisting of the\nreconstruction loss term and the distance loss term:\nmax\n\ud835\udf03,\ud835\udf19\nL\ud835\udf03,\ud835\udf19 (\ud835\udc99) := E\ud835\udc9b\u223c\ud835\udc5e\ud835\udf19 (\u00b7 |\ud835\udc99)\n\u0014\nln \ud835\udc5d\ud835\udf03 (\ud835\udc99, \ud835\udc9b)\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99)\n\u0015\n= ln \ud835\udc5d\ud835\udf03 (\ud835\udc99) \u2212 \ud835\udc37KL \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc99) \u2225\ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc99)\n\u0001\n. (132)\nMaximizing ELBO is equivalent to simultaneously maximizing the log-likelihood of the observed\ndata, and minimizing the divergence of the approximate posterior \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc65) from the exact poste\u0002rior \ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc65). Representative works along this thread include JT-VAE [207], GraphVAE [419], and\nCGVAE [290] for the 2D generation task, and 3DMolNet [351] for the 3D generation task.\nAutoregressive model (AR). Autoregressive model is an umbrella definition for any model\nthat sequentially generates the components (atoms or fragments) of a molecule. ARs better capture\nthe interdependency within the molecular structure and allows for explicit valency check. For each\nstep in AR, the new component can be produced using different techniques:\n\u2022 Regression/classification, such is the case with 3D-SBDD [306], Pocket2Mol [361], etc.\n\u2022 Reinforcement learning, such is the case with L-Net [272], DeepLigBuilder [273], etc.\n\u2022 Probabilistic models like normalizing flow and diffusion.\nNormalizing flow (NF). Based on the change-of-variable theorem, NF [385] constructs an\ninvertible mapping \ud835\udc53 between a complex data distribution \ud835\udc99 \u223c \ud835\udc4b: and a normalized latent distribu\u0002tion \ud835\udc9b \u223c \ud835\udc4d. Unlike VAE, which has juxtaposed parameters for encoder and decoder, the flow model\nuses the same set of parameter for encoding and encoding: reverse flow \ud835\udc53\n\u22121\nfor generation, and\nforward flow \ud835\udc53 for training:\nmax\n\ud835\udc53\nlog \ud835\udc5d(\ud835\udc99) = log \ud835\udc5d\ud835\udc3e (\ud835\udc9b\ud835\udc3e) (133)\n= log \ud835\udc5d\ud835\udc3e\u22121 (\ud835\udc9b\ud835\udc3e\u22121) \u2212 log\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc3e (\ud835\udc9b\ud835\udc3e\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc3e\u22121\n\u0013\n(134)\n= . . . (135)\n= log \ud835\udc5d0 (\ud835\udc9b0) \u2212\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc56=1\nlog\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc56 (\ud835\udc9b\ud835\udc56\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc56\u22121\n\u0013\n, (136)\nwhere \ud835\udc53 = \ud835\udc53\ud835\udc3e \u25e6 \ud835\udc53\ud835\udc3e\u22121 \u25e6 ... \u25e6 \ud835\udc531 is a composite of \ud835\udc3e blocks of transformation. While GraphNVP [329]\ngenerates the molecular graph with NF in one go, following works tend to use the autoregressive\nflow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439].\nDiffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion\nsteps to slowly add random noise to data \ud835\udc990 \u223c \ud835\udc5e(\ud835\udc99):\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121) = N (\ud835\udc99\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc99\ud835\udc61\u22121, \ud835\udefd\ud835\udc61 \ud835\udc70), (137)\n\ud835\udc5e(\ud835\udc991:\ud835\udc47 |\ud835\udc990) =\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121). (138)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 55\nTable 11. Summary of molecular generation models.\nModel 2D/3D Binding\u0002based\nFragment\u0002basedGNN\nBackbone\nGenerative\nModel\nGCPN [543] 2D GCN [230] GAN\nMolGAN [82] 2D R-GCN [401] GAN\nDEFactor [16] 2D GCN GAN\nGraphVAE [419] 2D ECC [418] VAE\nMDVAE [100] 2D GGNN [274] VAE\nJT-VAE [207] 2D \u2713 MPNN [147] VAE\nCGVAE [290] 2D GGNN VAE\nDeepScaffold [270] 2D \u2713 GCN VAE\nGraphNVP [329] 2D R-GCN NF\nMoFlow [557] 2D R-GCN NF\nGraphAF [413] 2D R-GCN NF + AR\nGraphDF [318] 2D R-GCN NF + AR\nL-Net [272] 3D \u2713 g-U-Net [133] AR\nG-SchNet [142] 3D SchNet [405] AR\nGEN3D [388] 3D EGNN [398] AR\nG-SphereNet [316] 3D SphereNet [292] NF + AR\nEDM [178] 3D EGNN DM\nGCDM [347] 3D GCPNet [346] DM\n3D-SBDD [306] 3D \u2713 SchNet AR\nPocket2Mol [361] 3D \u2713 GVP [211] AR\nFLAG [581] 3D \u2713 \u2713 SchNet AR\nGraphBP [288] 3D \u2713 SchNet NF + AR\nSiamFlow [439] 3D \u2713 R-GCN NF\nDiffBP [282] 3D \u2713 EGNN DM\nDiffSBDD [403] 3D \u2713 EGNN DM\nTargetDiff [156] 3D \u2713 EGNN DM\nFragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR\nGraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR\nMolCode [579] 2D + 3D \u2713 EGNN NF + AR\nThey then learn to reverse the diffusion process to construct desired data samples from the noise:\n\ud835\udc5d\ud835\udf03 (\ud835\udc990:\ud835\udc47 ) = \ud835\udc5d(\ud835\udc99\ud835\udc47 )\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61), (139)\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61) = N (\ud835\udc99\ud835\udc61\u22121; \ud835\udf41\ud835\udf03(\ud835\udc99\ud835\udc61, \ud835\udc61), \ud835\udeba\ud835\udf03 (\ud835\udc99\ud835\udc61, \ud835\udc61)), (140)\nwhile the models are trained using a variational lower bound. Diffusion models have been applied\nto generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands\nin DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate\nmolecular fragments in autoregressive models, as is the case with FragDiff [360].\n12.3 Summary and prospects\nWe wrap up this chapter with Table 11, which profiles existing molecular generation models\naccording to their taxonomy for molecular featurization, the GNN backbone, and the generative\nmethod. This chapter covers the critical topics of molecular generation, which also elicit valuable\ninsights into the promising directions for future research. We summarize these important aspects\nas follows.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n56 W. Ju, et al.\nTechniques. Graph neural networks can be flexibly leveraged to encode molecular features\non different representation levels and across different problem settings. Canonical GNNs like\nGCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs,\nwhile 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular,\nthis 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the\nbinding protein receptor is considered alongside the ligand geometry per se. Fragment-based models\nlike JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure.\nVarious generative methods have also been effectively incorporated into the molecular setting,\nincluding generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive\nmodel (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to\ngenerate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating\nthe search for drug candidates.\nChallenges and Limitations. While there has been an abundant supply of unlabelled molecular\nstructural and geometric data [125, 193, 426], the number of labeled molecular data over certain\ncritical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the\nother hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of\nthe generated molecules, such as QED and Vina [103], rather than actual wet lab experiments.\nFuture Works. Besides the structural and geometric attributes described in this chapter, an\neven more extensive array of data can be applied to aid molecular generation, including chemical\nreactions and medical ontology. These data can be organized into a heterogeneous knowledge\ngraph to aid the extraction of high-quality molecular representations. Furthermore, high through\u0002put experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and\ndruggability of the generated molecules in the wet lab. Concrete case studies, such as the design of\npotential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights\ninto leveraging these molecular generative models to facilitate the design and fabrication of potent\nand applicable drug molecules in the pharmaceutical industry.\nIntegrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations\noffers a promising new direction in molecular generation. Recent studies like those by [196] and\n[160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLM\u0002based approaches in this domain, including those by [338] and [18], predominantly utilize textual\nSMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The\nemerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image,\nand text, which could more comprehensively capture the intricacies of molecular structures. This\napproach marks a significant shift towards utilizing graph-based information alongside traditional\ntext, enhancing the capability of LLMs in molecular generation. Such advances suggest that future\nresearch should focus more on exploiting the synergy between graph-based molecular representa\u0002tions and the evolving landscape of LLMs to address complex challenges in chemistry and material\nsciences.\n13 Recommender Systems\nThe use of graph representation learning in recommender systems has been drawing increasing\nattention as one of the key strategies for addressing the issue of information overload. With their\nstrong ability to capture high-order connectivity between graph nodes, deep graph representation\nlearning has been shown to be beneficial in enhancing recommendation performance across a\nvariety of recommendation scenarios.\nTypical recommender systems take the observed interactions between users and items and\ntheir fixed features as input, and are intended for making proper predictions on which items a\nspecific user is probably interested in. To formulate, given an user set U, an item set I and the\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 57\nTable 12. Summary of graph models for recommender systems.\nModel Recommendation Task Graph Structure Graph Encoder Representation\nGC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer\nNGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation\nMMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer\nLightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling\nDGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling\nCAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling\nSR-GNN [496] Session-based Transition Graph GGNN Soft-Attention\nGC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention\nFGNN [377] Session-based Session Graph GAT Last-Layer\nGAG [378] Session-based Session Graph GCN Self-Attention\nGCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling\nHyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention\nDHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer\nMBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention\nHCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer\nH3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer\nSTHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling\ninteraction matrix between users and items \ud835\udc4b \u2208 {0, 1}\n|U|\u00d7|I|\n, where \ud835\udc4b\ud835\udc62,\ud835\udc63 indicates there is an\nobserved interaction between user \ud835\udc62 and item \ud835\udc56. The target of GNNs on recommender systems is to\nlearn representations \u210e\ud835\udc62, \u210e\ud835\udc56 \u2208 R\n\ud835\udc51\nfor given \ud835\udc62 and \ud835\udc56. The preference score can further be calculated\nby a similarity function:\n\ud835\udc65\u02c6\ud835\udc62,\ud835\udc56 = \ud835\udc53 (\u210e\ud835\udc62, \u210e\ud835\udc56), (141)\nwhere \ud835\udc53 (\u00b7, \u00b7) is the similarity function, e.g. inner product, cosine similarity, multi-layer perceptrons\nthat takes the representation of \ud835\udc62 and \ud835\udc56 and calculate the preference score \ud835\udc65\u02c6\ud835\udc62,\ud835\udc56.\nWhen it comes to adapting graph representation learning in recommender systems, a key step is\nto construct graph-structured data from the interaction set \ud835\udc4b. Generally, a graph is represented\nas G = {V, E} where V, E denotes the set of vertices and edges respectively. According to the\nconstruction of G, we can categorize the existing works as follows into three parts which are\nintroduced in the following subsections. A summary is provided in Table 12.\n13.1 User-Item Bipartite Graph\n13.1.1 Graph Construction A undirected bipartite graph where the vertex set V = U \u222a I and the\nundirected edge set E = {(\ud835\udc62,\ud835\udc56)|\ud835\udc62 \u2208 U \u2227\ud835\udc56 \u2208 I}. Under this case the graph adjacency can be directly\nobtained from the interaction matrix, thus the optimization target on the user-item bipartite graph\nis equivalent to collaborative filtering tasks such as MF [239] and SVD++ [238].\nThere have been plenty of previous works that applied GNNs on the constructed user-item bipar\u0002tite graphs. GC-MC [27] firstly applies graph convolution networks to user-item recommendation\nand optimizes a graph autoencoder (GAE) to reconstruct interactions between users and items.\nNGCF [470] introduces the concept of Collaborative Filtering (CF) into graph-based recommen\u0002dations by modeling the affinity between neighboring nodes on the interaction graph. MMGCN\n[485] extends the graph-based recommendation to multi-modal scenarios by constructing different\nsubgraphs for each modal. LightGCN [169] improves NGCF by removing the non-linear activation\nfunctions and simplifying the message function. With the development of disentangled representa\u0002tion learning, there are works like DGCF [473] that introduce disentangled graph representation\nlearning to represent users and items from multiple disentangled perspectives. Additionally, having\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n58 W. Ju, et al.\nrealized the limitation of the existing message-passing scheme in capturing collaborative signals,\nCAGCN [480] proposes Common Interacted Ratio (CIR) as a recommendation-oriented topological\nmetric for GNN-based recommender models.\n13.1.2 Graph Propagation Scheme A common practice is to follow the traditional message-passing\nnetworks (MPNNs) and design the graph propagation method accordingly. GC-MC adopts vanilla\nGCNs to encode the user-item bipartite graph. NGCF enhances GCNs by considering the affinity\nbetween users and items. The message function of NGCF from node \ud835\udc57 to \ud835\udc56 is formulated as:\n(\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc57 = \u221a 1\n|N\ud835\udc56| |N\ud835\udc57|\n(\ud835\udc4a1\ud835\udc52\ud835\udc57 +\ud835\udc4a2 (\ud835\udc52\ud835\udc56 \u2299 \ud835\udc52\ud835\udc57))\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc56 = \ud835\udc4a1\ud835\udc52\ud835\udc56\n, (142)\nwhere \ud835\udc4a1,\ud835\udc4a2 are trainable parameters, \ud835\udc52\ud835\udc56 represents \ud835\udc56\u2019s representation from previous layer. The\nmatrix form can be further provided by:\n\ud835\udc38\n(\ud835\udc59) = LeakyReLU( (L + \ud835\udc3c)\ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n1\n+ L\ud835\udc38\n(\ud835\udc59\u22121) \u2299 \ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n2\n), (143)\nwhere L represents the Laplacian matrix of the user-item graph. The element-wise product in Eq.\n143 represents the affinity between connected nodes, containing the collaborative signals from\ninteractions.\nHowever, the notable heaviness and burdensome calculation of NGCF\u2019s architecture hinder\nthe model from making faster recommendations on larger graphs. LightGCN solves this issue by\nproposing Light Graph Convolution (LGC), which simplifies the convolution operation with:\n\ud835\udc52\n(\ud835\udc59+1)\n\ud835\udc56\n=\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n1\n\u221a\ufe01\n|N\ud835\udc56||N\ud835\udc57|\n\ud835\udc52\n(\ud835\udc59)\n\ud835\udc57\n. (144)\nWhen an interaction takes place, e.g. a user clicks a particular item, there could be multiple\nintentions behind the observed interaction. Thus it is necessary to consider the various disentangled\nintentions among users and items. DGCF proposes the cross-intent embedding propagation scheme\non the graph, inspired by the dynamic routing algorithm of capsule networks [394]. To formulate,\nthe propagation process maintains a set of routing logits \u02dc\ud835\udc46\ud835\udc58 (\ud835\udc62,\ud835\udc56) for each user \ud835\udc62. The weighted sum\naggregator to get the representation of \ud835\udc62 can be defined as:\n\ud835\udc62\n\ud835\udc61\n\ud835\udc58\n=\n\u2211\ufe01\n\ud835\udc56\u2208N\ud835\udc62\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) \u00b7 \ud835\udc56\n0\n\ud835\udc58\n(145)\nfor \ud835\udc61-th iteration, where L\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) denotes the Laplacian matrix of \ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56), formulated as:\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) =\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n\u221a\ufe03\n[\n\u00cd\n\ud835\udc56\n\u2032\u2208N\ud835\udc62\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56\u2032)] \u00b7 [\u00cd\n\ud835\udc62\n\u2032\u2208N\ud835\udc56\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62\n\u2032\n,\ud835\udc56)]\n. (146)\n13.1.3 Node Representations After the graph propagation module outputs node-level representa\u0002tions, there are multiple methods to leverage node representations for recommendation tasks. A\nplain solution is to apply a readout function on layer outputs like the concatenation operation used\nby NGCF:\n\ud835\udc52\n\u2217 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc52(0)\n, ..., \ud835\udc52 (\ud835\udc3f)) = \ud835\udc52\n(0)\n\u2225...\u2225\ud835\udc52\n(\ud835\udc3f)\n. (147)\nHowever, the readout function among layers would neglect the relationship between the target\nitem and the current user. A general solution is to use the attention mechanism [451] to reweight\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 59\nand aggregate the node representations. SR-GNN adapts soft-attention mechanism to model the\nitem-item relationship:\n\ud835\udefc\ud835\udc56 = q\n\ud835\udc47\n\ud835\udf0e(\ud835\udc4a1\ud835\udc52\ud835\udc61 +\ud835\udc4a2\ud835\udc52\ud835\udc56 + \ud835\udc50),\n\ud835\udc60\ud835\udc54 =\n\ud835\udc5b\u2211\ufe01\u22121\n\ud835\udc56=1\n\ud835\udefc\ud835\udc56\ud835\udc52\ud835\udc56,\n(148)\nwhere q, \ud835\udc4a1, \ud835\udc4a2 are trainable matrices.\nSome methods focus on exploiting information from multiple graph structures. A common\npractice is contrastive learning, which maximizes the mutual information between hidden repre\u0002sentations from several views. HCCF leverage InfoNCE loss as the estimator of mutual information,\ngiven a pair of representation \ud835\udc67\ud835\udc56, \u0393\ud835\udc56 for node \ud835\udc56, controlled by temperature parameter \ud835\udf0f:\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc41\ud835\udc36\ud835\udc38 (\ud835\udc56) = \u2212 log exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56))/\ud835\udf0f\n\u00cd\n\ud835\udc56\n\u2032\u2260\ud835\udc56 exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56\n\u2032 ))/\ud835\udf0f\n. (149)\nBesides InfoNCE, there exist several other ways to combine node representations from different\nviews. For instance, MBHT applies an attention mechanism to fuse multiple semantics, DisenPOI\nadapts bayesian personalized ranking loss (BPR) [384] as a soft estimator for contrastive learning,\nand KBGNN applies pair-wise similarities to ensure the consistency from two views.\n13.2 Transition Graph\n13.2.1 Transition Graph Construction Since sequence-based recommendation (SR) is one of the\nfundamental problems in recommender systems, some researches focus on modeling the sequential\ninformation with GNNs. A commonly applied way is to construct transition graphs based on each\ngiven sequence according to the clicking sequence by a user. To formulate, given a user \ud835\udc62\u2019s clicking\nsequence \ud835\udc60\ud835\udc62 = [\ud835\udc56\ud835\udc62,1,\ud835\udc56\ud835\udc62,2, ...,\ud835\udc56\ud835\udc62,\ud835\udc5b] containing \ud835\udc5b items, noting that there could be duplicated items, the\nsequential graph is constructed via G\ud835\udc60 = {SET(\ud835\udc60\ud835\udc62), E}, where \u2200\n\ud835\udc56\ud835\udc57,\ud835\udc56\ud835\udc58\n\u2208 E indicates there exists\na successive transition from \ud835\udc56\ud835\udc57 to \ud835\udc56\ud835\udc58 . Since G\ud835\udc60 are directed graphs, a widely used way to depict\ngraph connectivity is by building the connection matrix \ud835\udc34\ud835\udc60 \u2208 R\n\ud835\udc5b\u00d72\ud835\udc5b\n. \ud835\udc34\ud835\udc60is the combination of two\nadjacency matrices \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n], which denotes the normalized node degrees of incoming\nand outgoing edges in the session graph respectively.\nThe proposed transition graphs that obtain user behavior patterns have been demonstrated\nimportant to session-based recommendations [263, 291]. SR-GNN and GC-SAN [496, 516] propose\nto leverage transition graphs and apply attention-based GNNs to capture the sequential information\nfor session-based recommendation. FGNN [377] formulates the recommendation within a session\nas a graph classification problem to predict the next item for an anonymous user. GAG [378] and\nGCE-GNN [482] further extend the model to capture global embeddings among multiple session\ngraphs.\n13.2.2 Session Graph Propagation Since the session graphs are directed item graphs, there have\nbeen multiple session graph propagation methods to obtain node representations on session graphs.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n60 W. Ju, et al.\nSR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information\nfrom a given session graph adjacency \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n] and item embedding set {\ud835\udc52\ud835\udc56 }:\n\ud835\udc4e\ud835\udc61 = \ud835\udc34\ud835\udc60 [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121]\n\ud835\udc47\ud835\udc3b + \ud835\udc4f, (150)\n\ud835\udc67\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc67\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc67\ud835\udc52\ud835\udc61\u22121), (151)\n\ud835\udc5f\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc5f\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc61\u22121), (152)\n\ud835\udc52\u02dc\ud835\udc61 = tanh(\ud835\udc4a\ud835\udc5c\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5c (\ud835\udc5f\ud835\udc61 \u2299 \ud835\udc52\ud835\udc61\u22121)), (153)\n\ud835\udc52\ud835\udc61 = (1 \u2212 \ud835\udc67\ud835\udc61) \u2299 \ud835\udc52\ud835\udc61\u22121 + \ud835\udc67\ud835\udc61\ud835\udc52\u02dc\ud835\udc61, (154)\nwhere \ud835\udc4a s and \ud835\udc48 s are trainable parameters. GC-SAN extend GGNN by calculating initial state \ud835\udc4e\ud835\udc61\nseparately to better exploit transition information:\n\ud835\udc4e\ud835\udc61 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc56\ud835\udc5b)\n), \ud835\udc34(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n)). (155)\n13.3 HyperGraph\n13.3.1 Hypergraph Topology Construction Motivated by the idea of modeling hyper-structures\nand high-order correlation among nodes, hypergraphs [119] are proposed as extensions of the\ncommonly used graph structures. For graph-based recommender systems, a common practice is\nto construct hyper structures among the original user-item bipartite graphs. To be specific, an\nincidence matrix of a graph with vertex set V is presented as a binary matrix \ud835\udc3b \u2208 {0, 1}\n|V |\u00d7 | E |\n,\nwhere E represents the set of hyperedges. Each entry \u210e(\ud835\udc63, \ud835\udc52) of \ud835\udc3b depicts the connectivity between\nvertex \ud835\udc63 and hyperedge \ud835\udc52:\n\u210e(\ud835\udc63, \ud835\udc52) =\n(\n1 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2208 \ud835\udc52\n0 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2209 \ud835\udc52\n. (156)\nGiven the formulation of hypergraphs, the degrees of vertices and hyperedges of \ud835\udc3b can then be\ndefined with two diagonal matrices \ud835\udc37\ud835\udc63 \u2208 N\n|V |\u00d7 |V | and \ud835\udc37\ud835\udc52 \u2208 N| E |\u00d7 | E |, where\n\ud835\udc37\ud835\udc63 (\ud835\udc56;\ud835\udc56) =\n\u2211\ufe01\n\ud835\udc52\u2208 E\n\u210e(\ud835\udc63\ud835\udc56, \ud835\udc52), \ud835\udc37\ud835\udc52 (\ud835\udc57; \ud835\udc57) =\n\u2211\ufe01\n\ud835\udc63\u2208V\n\u210e(\ud835\udc63, \ud835\udc52\ud835\udc57). (157)\nThe development of Hypergraph Neural Networks (HGNNs) [119, 188, 598] have shown to\nbe capable of capturing the high-order connectivity between nodes. HyperRec [463] firstly at\u0002tempts to leverage hypergraph structures for sequential recommendation by connecting items\nwith hyperedges according to the interactions with users during different time periods. DHCF\n[198] proposes to construct hypergraphs for users and items respectively based on certain rules, to\nexplicitly capture the collaborative similarities via HGNNs. MBHT [532] combines hypergraphs\nwith a low-rank self-attention mechanism to capture the dynamic heterogeneous relationships\nbetween users and items. HCCF [505] uses the contrastive information between hypergraph and\ninteraction graph to enhance the recommendation performance. To extend the model\u2019s ability to\nmulti-domain categories of items, H3Trans [523] incorporates two hyperedge-based modules and\nleverages hierarchical hypergraph propagation to transfer from domains. STHGCN [524] formulates\na spatio-temporal hypergraph structure for POI recommendation.\n13.3.2 Hyper Graph Message Passing With the development of HGNNs, previous works have\nproposed different variants of HGNN to better exploit hypergraph structures. A classic high-order\nhyper convolution process on a fixed hypergraph G = {V, E} with hyper adjacency \ud835\udc3b is given by:\n\ud835\udc54 \u2605\ud835\udc4b = \ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc3b\ud835\udc37\u22121\n\ud835\udc52 \ud835\udc3b\n\ud835\udc47\ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc4b\u0398, (158)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 61\nwhere \ud835\udc37\ud835\udc63, \ud835\udc37\ud835\udc52 are degree matrices of nodes and hyperedges, \u0398 denotes the convolution kernel. For\nhyper adjacency matrix \ud835\udc3b, DHCF refers to a rule-based hyperstructure via k-order reachable rule,\nwhere nodes in the same hyperedge group are k-order reachable to each other:\n\ud835\udc34\n\ud835\udc58\n\ud835\udc62 = min(1, power(\ud835\udc34 \u00b7 \ud835\udc34\n\ud835\udc47\n, \ud835\udc58)), (159)\nwhere \ud835\udc34 denotes the graph adjacency matrix. By considering the situations where \ud835\udc58 = 1, 2, the\nmatrix formulation of the hyper connectivity of users and items is calculated with:\n(\n\ud835\udc3b\ud835\udc62 = \ud835\udc34\u2225 (\ud835\udc34(\ud835\udc34\n\ud835\udc47\ud835\udc34))\n\ud835\udc3b\ud835\udc56 = \ud835\udc34\n\ud835\udc47\n\u2225 (\ud835\udc34\n\ud835\udc47\n(\ud835\udc34\ud835\udc34\ud835\udc47))\n, (160)\nwhich depicts the dual hypergraphs for users and items.\nHCCF proposes to construct a learnable hypergraph to depict the global dependencies between\nnodes on the interaction graph. To be specific, the hyperstructure is factorized with two low-rank\nembedding matrices to achieve model efficiency:\n\ud835\udc3b\ud835\udc62 = \ud835\udc38\ud835\udc62 \u00b7\ud835\udc4a\ud835\udc62, \ud835\udc3b\ud835\udc63 = \ud835\udc38\ud835\udc63 \u00b7\ud835\udc4a\ud835\udc63 . (161)\n13.4 Other Graphs\nSince there are a variety of recommendation scenarios, several tailored designed graph structures\nhave been proposed accordingly, to better exploit the domain information from different scenarios.\nFor instance, CKE [564] and MKR [462] introduce Knowledge graphs to enhance graph recommen\u0002dation. GSTN [484], KBGNN [219], DisenPOI [373] and Diff-POI [374] propose to build geographical\ngraphs based on the distance between Point-of-Interests (POIs) to better model the locality of users\u2019\nvisiting patterns. TGSRec [109] and DisenCTR [475] empower the user-item interaction graphs with\ntemporal sampling between layers to obtain sequential information from static bipartite graphs.\n13.5 Summary\nThis section introduces the application of different kinds of graph neural networks in recommender\nsystems and can be summarized as follows:\n\u2022 Graph Constructions. There are multiple options for constructing graph-structured data\nfor a variety of recommendation tasks. For instance, the user-item bipartite graphs reveal\nthe high-order collaborative similarity between users and items, and the transition graph\nis suitable for encoding sequential information in clicking history. These diversified graph\nstructures provide different views for node representation learning on users and items, and\ncan be further used for downstream ranking tasks.\n\u2022 Challenges and Limitations. Though the superiority of graph-structured data and GNNs\nagainst traditional methods has been widely illustrated, there are still challenges unsolved.\nFor example, the computational cost of graph methods is normally expensive and thus\nunacceptable in real-world applications. The data sparsity and cold-started issue in graph\nrecommendation remains to be explored as well.\n\u2022 Future Works. In the future, an efficient solution for applying GNNs in recommendation\ntasks is expected. There are also some attempts [109, 372, 475] on incorporating temporal\ninformation in graph representation learning for sequential recommendation tasks.\n14 Traffic Analysis\nIntelligent Transportation Systems (ITS) are essential for safe, reliable, and efficient transportation\nin smart cities, serving the daily commuting and traveling needs of millions of people. To support\nITS, advanced modeling and analysis techniques are necessary, and Graph Neural Networks (GNNs)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n62 W. Ju, et al.\nare a promising tool for traffic analysis. GNNs can effectively model spatial correlations, making\nthem well-suited for analyzing complex transportation networks. As such, GNNs have garnered\nsignificant interest in the traffic domain for their ability to provide insights into traffic patterns and\nbehaviors [260].\nIn this section, we first conclude the main GNN research directions in the traffic domain, and\nthen we summarize the typical graph construction processes in different traffic scenes and datasets.\nFinally, we list the classical GNN workflows for dealing with tasks in traffic networks. A summary\nis provided in Table 13.\n14.1 Research Directions in Traffic Domain\nWe summarize main GNN research directions in the traffic domain as follows,\n\u2022 Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role in ITS [90, 381],\nwhich involves leveraging spatial-temporal data collected by various sensors to gain insights\ninto future traffic patterns and behaviors. Classic methods, like autoregressive integrated\nmoving average (ARIMA) [36], support vector machine (SVM) [171] and recurrent neural\nnetworks (RNN) [76] can only model time series separately without considering their spatial\nconnections. To address this issue, graph neural networks (GNNs) have emerged as a powerful\napproach for traffic forecasting due to their strong ability of modeling complex graph\u0002structured correlations [40, 202, 277, 353, 383, 506, 592].\n\u2022 Trajectory Prediction. Trajectory prediction is a crucial task in various applications, such\nas autonomous driving and traffic surveillance, which aims to forecast future positions of\nagents in the traffic scene. However, accurately predicting trajectories can be challenging, as\nthe behavior of an agent is influenced not only by its own motion but also by interactions\nwith surrounding objects. To address this challenge, Graph Neural Networks (GNNs) have\nemerged as a promising tool for modeling complex interactions in trajectory prediction\n[44, 345, 432, 600]. By representing the scene as a graph, where each node corresponds to an\nagent and the edges capture interactions between them, GNNs can effectively capture spatial\ndependencies and interactions between agents. This makes GNNs well-suited for predicting\ntrajectories that accurately capture the behavior of agents in complex traffic scenes.\n\u2022 Traffic Anomaly Detection. Anomaly detection is an essential support for ITS. There are\nlots of traffic anomalies in daily transportation systems, for example, traffic accidents, extreme\nweather and unexpected situations. Handling these traffic anomalies timely can improve the\nservice quality of public transportation. The main trouble of traffic anomaly detection is the\nhighly twisted spatial-temporal characteristics of traffic data. The criteria and influence of\ntraffic anomaly vary among locations and times. GNNs have been introduced and achieved\nsuccess in this domain [66, 85, 86, 565].\n\u2022 Others. Traffic demand prediction targets at estimating the future number of traveling at\nsome location. It is of vital and practical significance in the resource scheduling for ITS. By\nusing GNNs, the spatial dependencies of demands can be revealed [530, 535]. What is more,\nurban vehicle emission analysis is also considered in recent work, which is closely related to\nenvironmental protection and gains increasing researcher attention [521].\n14.2 Traffic Graph Construction\n14.2.1 Traffic Graph The traffic network is represented as a graph G = (\ud835\udc49 , \ud835\udc38, \ud835\udc34), where \ud835\udc49 is the\nset of \ud835\udc41 traffic nodes, \ud835\udc38 is the set of edges, and \ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is an adjacency matrix representing the\nconnectivity of \ud835\udc41 nodes. In the traffic domain, \ud835\udc49 usually represents a set of physical nodes, like\ntraffic stations or traffic sensors. The features of nodes typically depend on the specific task. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 63\nTable 13. Summary of graph models for traffic analysis.\nModels Tasks Adjcency matrices GNN types Temporal modules\nSTGCN[545] Traffic Flow Forecasting Fixed Matrix GCN TCN\nDCRNN[275] Traffic Flow Forecasting Fixed Matrix ChebNet RNN\nAGCRN [19] Traffic Flow Forecasting Dynamic Matrix GCN GRU\nASTGCN [159] Traffic Flow Forecasting Fixed Matrix GAT Attention&TCN\nGraphWaveNet [500] Traffic Flow Forecasting Dynamic Matrix GCN Gated-TCN\nSTSGCN [422] Traffic Flow Forecasting Dynamic Matrix GCN Cropping\nLSGCN [187] Traffic Flow Forecasting Fixed Matrix GAT GLU\nGAC-Net [424] Traffic Flow Forecasting Fixed Matrix GAT Gated-TCN\nSTGODE [112] Traffic Flow Forecasting Fixed Matrix Graph ODE TCN\nSTG-NCDE [70] Traffic Flow Forecasting Dynamic Matrix GCN NCDE\nDDGCRN [488] Traffic Flow Forecasting Dynamic Matrix GAT RNN\nMS-ASTN [467] OD Flow Forecasting OD Matrix GCN LSTM\nSocial-STGCNN [345] Trajectory Prediction Fixed Matrix GCN TXP-CNN\nRSBG [432] Trajectory Prediction Dynamic Matrix GCN LSTM\nATG [555] Trajectory Prediction Fixed Matrix GODE NODE\nSTGAN [86] Anomaly Detection Fixed Matrix GCN GRU\nDMVST-VGNN [206] Traffic Demand Prediction Fixed Matrix GAT GLU\nDST-GNN [185] Traffic Demand Prediction Dynamic Matrix GCN Transformer\nTC-SGC [355] Traffic Speed Prediction Fixed Matrix GCN GRU\ntraffic flow forecasting as an example. The features are the traffic flows, i.e., the historical time\nseries of nodes. The traffic flow can be represented as a flow matrix \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc47\n, where \ud835\udc41 is the\nnumber of traffic nodes and \ud835\udc47 is the length of historical series, and \ud835\udc4b\ud835\udc5b\ud835\udc61 denotes the traffic flow of\nnode \ud835\udc5b at time \ud835\udc61. The goal of traffic flow forecasting is to learn a mapping function \ud835\udc53 to predict the\ntraffic flow during future \ud835\udc47\n\u2032\nsteps given the historical \ud835\udc47 step information, which can be formulated\nas follows:\n\u0002\n\ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +1, \ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61; G\n\u0003 \ud835\udc53\n\u2212\u2192 \u0002\ud835\udc4b:,\ud835\udc61+1, \ud835\udc4b:,\ud835\udc61+2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61+\ud835\udc47\n\u2032\n\u0003\n. (162)\n14.2.2 Graph Construction Constructing a graph to describe the interactions among traffic nodes,\ni.e., the design of the adjacency matrix \ud835\udc34, is the key part of traffic analysis. The mainstream designs\ncan be divided into two categories, fixed matrix and dynamic matrix.\nFixed matrix. Lots of works assume that the correlations among traffic nodes are fixed and\nconstant over time, and they design a fixed and pre-defined adjacency matrix to capture the spatial\ncorrelation. Here we list several common choices of fixed adjacency matrix.\nThe connectivity matrix is the most natural construction way. It relies on the support of\nroad map data. The element of the connectivity matrix is defined as 1 if two nodes are physically\nconnected and 0 otherwise. This binary format is convenient to construct and easy to interpret.\nThe distance-based matrix is also a common choice, which shows the connection between two\nnodes more precisely. The elements of the matrix are defined as the function of distance between\ntwo nodes (driving distance or geographical distance). A typical way is to use the threshold Gaussian\nfunction as follows,\n\ud835\udc34\ud835\udc56\ud835\udc57 =\n(\nexp(\u2212\ud835\udc51\n2\n\ud835\udc56 \ud835\udc57\n\ud835\udf0e\n2 ), \ud835\udc51\ud835\udc56\ud835\udc57  \ud835\udf16\n, (163)\nwhere \ud835\udc51\ud835\udc56\ud835\udc57 is the distance between node \ud835\udc56 and \ud835\udc57, and \ud835\udf0e and \ud835\udf16 are two hyperparameters to control the\ndistribution and the sparsity of the matrix.\nAnother kind of fixed adjacency matrix is the similarity-based matrix. In fact, a similarity\nmatrix is not an adjacency matrix to some extent. It is constructed according to the similarity of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n64 W. Ju, et al.\ntwo nodes, which means the neighbors in the similarity graph may be far away in the real world.\nThere are various similarity metrics. For example, many works measure the similarity of two nodes\nby their functionality, e.g., the distribution of surrounding points of interest (POIs). The assumption\nbehind this is that nodes that share similar functionality may share similar traffic patterns. We\ncan also define the similarity through the historical flow patterns. To compute the similarity of\ntwo-time series, a common practice is to use Dynamic Time Wrapping (DTW) algorithm [350],\nwhich is superior to other metrics due to its sensitivity to shape similarity rather than point-wise\nsimilarity. Specifically, given two time series \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc5b) and \ud835\udc4c = (\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc5b), DTW is\na dynamic programming algorithm defined as\n\ud835\udc37(\ud835\udc56, \ud835\udc57) = \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) + min (\ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57), \ud835\udc37(\ud835\udc56, \ud835\udc57 \u2212 1), \ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57 \u2212 1)) , (164)\nwhere \ud835\udc37(\ud835\udc56, \ud835\udc57) represents the shortest distance between subseries \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc56) and \ud835\udc4c =\n(\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc57), and\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) is some distance metric like absolute distance. As a result,\ud835\udc37\ud835\udc47\ud835\udc4a (\ud835\udc4b, \ud835\udc4c) =\n\ud835\udc37(\ud835\udc5b, \ud835\udc5b) is set as the final distance between \ud835\udc4b and \ud835\udc4c, which better reflects the similarity of the\ntwo-time series compared to the Euclidean distance.\nDynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect complete\ninformation of spatial correlations. The dynamic adaptive matrix is proposed to solve the issue.\nThe dynamic matrix is learned from input data automatically. To achieve the best prediction\nperformance, the dynamic matrix will manage to infer the hidden correlations among nodes, more\nthan those physical connections.\nA typical practice is learning adjacency matrix from node embeddings [19]. Let \ud835\udc38\ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc51 be a\nlearnable node embedding dictionary, where each row of \ud835\udc38\ud835\udc34 represents the embedding of a node,\n\ud835\udc41 and \ud835\udc51 denote the number of nodes and the dimension of embeddings respectively. The graph\nadjacency matrix is defined as the similarities among node embeddings,\n\ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 = \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \u0010\n\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc38\ud835\udc34 \u00b7 \ud835\udc38\n\ud835\udc47\n\ud835\udc34\n)\n\u0011\n, (165)\nwhere \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 function is to perform row-normalization, and \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 is the Laplacian matrix.\n14.3 Typical GNN Frameworks in Traffic Domain\nSpatial Temporal Graph Convolution Network (STGCN) [545]. STGCN is a pioneering work in the\nspatial-temporal GNN domain. It utilizes graph convolution to capture spatial features, and deploys\na gated causal convolution to extract temporal patterns. Specifically, the graph convolution and\ntemporal convolution are defined as follows,\n\u0398 \u2217G \ud835\udc65 = \ud835\udf03 (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc65 = \ud835\udf03 (\ud835\udc37\u02dc \u2212\n1\n2\ud835\udc34\u02dc\ud835\udc37\u02dc \u2212\n1\n2 )\ud835\udc65, (166)\n\u0393 \u2217T \ud835\udc66 = \ud835\udc43 \u2299 \ud835\udf0e(\ud835\udc44), (167)\nwhere \u0398 is the parameter of graph convolution, \ud835\udc43 and \ud835\udc44 are the outputs of a 1-d convolution\nalong the temporal dimension. The sigmoid gate \ud835\udf0e(\ud835\udc44) controls how the states of \ud835\udc43 are relevant\nfor discovering hidden temporal patterns. In order to fuse features from both spatial and temporal\ndimension, the spatial convolution layer and the temporal convolution layer are combined to\nconstruct a spatial temporal block to jointly deal with graph-structured time series, and more blocks\ncan be stacked to achieve a more scalable and complex model.\nDiffusion Convolutional Recurrent Neural Network (DCRNN) [275]. DCRNN is a representative\nsolution combining graph convolution networks with recurrent neural networks. It captures spatial\ndependencies by bidirectional random walks on the graph. The diffusion convolution operation on\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 65\na graph is defined as:\n\ud835\udc4b \u2217G \ud835\udc53\ud835\udf03 =\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc58=0\n\u0010\n\ud835\udf03\ud835\udc58,1 (\ud835\udc37\n\u22121\n\ud835\udc42 \ud835\udc34)\n\ud835\udc58 + \ud835\udf03\ud835\udc58,2 (\ud835\udc37\u22121\n\ud835\udc3c \ud835\udc34)\n\ud835\udc58\n\u0011\n\ud835\udc4b, (168)\nwhere \ud835\udf03 are parameters for the convolution filter, and \ud835\udc37\n\u22121\n\ud835\udc42\n\ud835\udc34, \ud835\udc37\u22121\n\ud835\udc3c\n\ud835\udc34 represent the bidirectional\ndiffusion processes respectively. In term of temporal dependency, DCRNN utilizes Gated Recurrent\nUnits (GRU), and replace the linear transformation in the GRU with the diffusion convolution as\nfollows,\n\ud835\udc5f\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc5f \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc5f), (169)\n\ud835\udc62\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc62 \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc62), (170)\n\ud835\udc36\n(\ud835\udc61) = tanh(\u0398\ud835\udc36 \u2217G [\ud835\udc4b(\ud835\udc61)\n, (\ud835\udc5f\n(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121)\n] + \ud835\udc4f\ud835\udc50 ), (171)\n\ud835\udc3b\n(\ud835\udc61) = \ud835\udc62(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121) + (1 \u2212 \ud835\udc62(\ud835\udc61)\n) \u2299 \ud835\udc36\n(\ud835\udc61)\n, (172)\nwhere \ud835\udc4b\n(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61) denote the input and output at time \ud835\udc61, \ud835\udc5f\n(\ud835\udc61)\n, \ud835\udc62(\ud835\udc61)are the reset and update gates\nrespectively, and \u0398\ud835\udc5f, \u0398\ud835\udc62, \u0398\ud835\udc36 are parameters of convolution filters. Moreover, DCRNN employs a\nsequence-to-sequence architecture to predict future series. Both the encoder and the decoder are\nconstructed with diffusion convolutional recurrent layers. The historical time series are fed into\nthe encoder and the predictions are generated by the decoder. The scheduled sampling technique is\nutilized to solve the discrepancy problem between training and test distribution.\nAdaptive Graph Convolutional Recurrent Network (AGCRN) [19]. The focuses of AGCRN are\ntwo-fold. On the one hand, it argues that the temporal patterns are diversified and thus parameter\u0002sharing for each node is inferior; on the other hand, it proposes that the pre-defined graph may be\nintuitive and incomplete for the specific prediction task. To mitigate the two issues, it designs a\nNode Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic\nseries, and a Data Adaptive Graph Generation (DAGG) module to infer the hidden correlations\namong nodes from data and to generate the graph during training. Specifically, the NAPL module\nis defined as follows,\n\ud835\udc4d = (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc4b \ud835\udc38G\ud835\udc4aG + \ud835\udc38G\ud835\udc4f G, (173)\nwhere \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc36 is the input feature, \ud835\udc38G \u2208 R\ud835\udc41 \u00d7\ud835\udc51\nis a node embedding dictionary, \ud835\udc51 is the\nembedding dimension (\ud835\udc51",
      "openalex_id": "https://openalex.org/W4392203343",
      "title": "A Comprehensive Survey on Deep Graph Representation Learning",
      "publication_date": "2024-02-27",
      "cited_by_count": 55.0,
      "topics": "Graph Neural Network Models and Applications, Statistical Mechanics of Complex Networks, Recommender System Technologies",
      "keywords": "Feature learning, Representation Learning, Knowledge Graph Embedding, Signal Processing on Graphs, Network Embedding, Deep Learning, Graph embedding",
      "concepts": "Computer science, Deep learning, Feature learning, Artificial intelligence, Graph, Theoretical computer science, Categorization, Embedding, Graph embedding, Machine learning",
      "pdf_urls_by_priority": [
        "https://arxiv.org/pdf/2304.05055"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W103666676",
        "https://openalex.org/W1510073064",
        "https://openalex.org/W1520469672",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W1662382123",
        "https://openalex.org/W1816257748",
        "https://openalex.org/W1888005072",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W1954735160",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W1991252559",
        "https://openalex.org/W1992787746",
        "https://openalex.org/W1993046136",
        "https://openalex.org/W1994389483",
        "https://openalex.org/W1996058270",
        "https://openalex.org/W2006698588",
        "https://openalex.org/W2008056655",
        "https://openalex.org/W2009233867",
        "https://openalex.org/W2022638422",
        "https://openalex.org/W2027482274",
        "https://openalex.org/W2042123098",
        "https://openalex.org/W2053186076",
        "https://openalex.org/W2054141820",
        "https://openalex.org/W2056609785",
        "https://openalex.org/W2062340319",
        "https://openalex.org/W2076498053",
        "https://openalex.org/W2090891622",
        "https://openalex.org/W2095932468",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W2101491865",
        "https://openalex.org/W2110242546",
        "https://openalex.org/W2113052721",
        "https://openalex.org/W2114704115",
        "https://openalex.org/W2115627867",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W2121406124",
        "https://openalex.org/W2128332575",
        "https://openalex.org/W2140310134",
        "https://openalex.org/W2142498761",
        "https://openalex.org/W2142535891",
        "https://openalex.org/W2145658888",
        "https://openalex.org/W2148950790",
        "https://openalex.org/W2152184085",
        "https://openalex.org/W2152630148",
        "https://openalex.org/W2152825437",
        "https://openalex.org/W2153959628",
        "https://openalex.org/W2156718197",
        "https://openalex.org/W2158787690",
        "https://openalex.org/W2170057991",
        "https://openalex.org/W2184148260",
        "https://openalex.org/W2262123273",
        "https://openalex.org/W2319902168",
        "https://openalex.org/W2338678442",
        "https://openalex.org/W2387462954",
        "https://openalex.org/W2393319904",
        "https://openalex.org/W2415243320",
        "https://openalex.org/W2461620095",
        "https://openalex.org/W2481151430",
        "https://openalex.org/W2488133945",
        "https://openalex.org/W2493343568",
        "https://openalex.org/W2509893387",
        "https://openalex.org/W2520633135",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W2550925836",
        "https://openalex.org/W2551706664",
        "https://openalex.org/W2565684601",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2594899909",
        "https://openalex.org/W2602753196",
        "https://openalex.org/W2604738573",
        "https://openalex.org/W2606202972",
        "https://openalex.org/W2612872092",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2624407581",
        "https://openalex.org/W2700550412",
        "https://openalex.org/W2735246657",
        "https://openalex.org/W2743104969",
        "https://openalex.org/W2743930630",
        "https://openalex.org/W2749279690",
        "https://openalex.org/W2754490690",
        "https://openalex.org/W2767094836",
        "https://openalex.org/W2767404761",
        "https://openalex.org/W2772486182",
        "https://openalex.org/W2773515559",
        "https://openalex.org/W2788134583",
        "https://openalex.org/W2788775653",
        "https://openalex.org/W2788919350",
        "https://openalex.org/W2793544332",
        "https://openalex.org/W2798621783",
        "https://openalex.org/W2800415562",
        "https://openalex.org/W2803526748",
        "https://openalex.org/W2809279178",
        "https://openalex.org/W2809307135",
        "https://openalex.org/W2809435178",
        "https://openalex.org/W2884209963",
        "https://openalex.org/W2888164077",
        "https://openalex.org/W2888192920",
        "https://openalex.org/W2889337896",
        "https://openalex.org/W2892880750",
        "https://openalex.org/W2895744665",
        "https://openalex.org/W2895884529",
        "https://openalex.org/W2896202861",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2897862648",
        "https://openalex.org/W2897978524",
        "https://openalex.org/W2901454299",
        "https://openalex.org/W2903871660",
        "https://openalex.org/W2905432015",
        "https://openalex.org/W2907230994",
        "https://openalex.org/W2907492528",
        "https://openalex.org/W2911286998",
        "https://openalex.org/W2912323206",
        "https://openalex.org/W2912351665",
        "https://openalex.org/W2913015533",
        "https://openalex.org/W2913350752",
        "https://openalex.org/W2913825337",
        "https://openalex.org/W2914989158",
        "https://openalex.org/W2916446912",
        "https://openalex.org/W2944538680",
        "https://openalex.org/W2948684689",
        "https://openalex.org/W2948729509",
        "https://openalex.org/W2949103145",
        "https://openalex.org/W2949208225",
        "https://openalex.org/W2949243165",
        "https://openalex.org/W2949865801",
        "https://openalex.org/W2950898568",
        "https://openalex.org/W2951659295",
        "https://openalex.org/W2951970475",
        "https://openalex.org/W2959300817",
        "https://openalex.org/W2962756421",
        "https://openalex.org/W2962810718",
        "https://openalex.org/W2963017945",
        "https://openalex.org/W2963066159",
        "https://openalex.org/W2963084622",
        "https://openalex.org/W2963224980",
        "https://openalex.org/W2963241951",
        "https://openalex.org/W2963341924",
        "https://openalex.org/W2963358464",
        "https://openalex.org/W2963410212",
        "https://openalex.org/W2963456618",
        "https://openalex.org/W2963521729",
        "https://openalex.org/W2963639956",
        "https://openalex.org/W2963664410",
        "https://openalex.org/W2963703618",
        "https://openalex.org/W2963726920",
        "https://openalex.org/W2963919031",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2964044287",
        "https://openalex.org/W2964051675",
        "https://openalex.org/W2964113829",
        "https://openalex.org/W2964321699",
        "https://openalex.org/W2964568038",
        "https://openalex.org/W2964583308",
        "https://openalex.org/W2964926209",
        "https://openalex.org/W2965341826",
        "https://openalex.org/W2965857891",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W2966683369",
        "https://openalex.org/W2966841471",
        "https://openalex.org/W2971126534",
        "https://openalex.org/W2971220558",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2979845147",
        "https://openalex.org/W2981536126",
        "https://openalex.org/W2981790137",
        "https://openalex.org/W2982108874",
        "https://openalex.org/W2982880755",
        "https://openalex.org/W2986423110",
        "https://openalex.org/W2986466936",
        "https://openalex.org/W2986515219",
        "https://openalex.org/W2988115728",
        "https://openalex.org/W2989285747",
        "https://openalex.org/W2992586577",
        "https://openalex.org/W2992613109",
        "https://openalex.org/W2994860160",
        "https://openalex.org/W2996635575",
        "https://openalex.org/W2996847713",
        "https://openalex.org/W2996910665",
        "https://openalex.org/W2997128522",
        "https://openalex.org/W2997574889",
        "https://openalex.org/W2997785591",
        "https://openalex.org/W2997997679",
        "https://openalex.org/W2998004401",
        "https://openalex.org/W2998122931",
        "https://openalex.org/W2998496395",
        "https://openalex.org/W2998604091",
        "https://openalex.org/W2998702685",
        "https://openalex.org/W3000301417",
        "https://openalex.org/W3000478925",
        "https://openalex.org/W3000577518",
        "https://openalex.org/W3000716014",
        "https://openalex.org/W3005552578",
        "https://openalex.org/W3007488165",
        "https://openalex.org/W3008194092",
        "https://openalex.org/W3011358689",
        "https://openalex.org/W3011667710",
        "https://openalex.org/W3012123536",
        "https://openalex.org/W3012871709",
        "https://openalex.org/W3013107657",
        "https://openalex.org/W3013888836",
        "https://openalex.org/W3016124664",
        "https://openalex.org/W3016427665",
        "https://openalex.org/W3025863369",
        "https://openalex.org/W3026887460",
        "https://openalex.org/W3031353169",
        "https://openalex.org/W3033039844",
        "https://openalex.org/W3033706928",
        "https://openalex.org/W3034231628",
        "https://openalex.org/W3034329572",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W3035096461",
        "https://openalex.org/W3035237749",
        "https://openalex.org/W3035285524",
        "https://openalex.org/W3035523484",
        "https://openalex.org/W3035580605",
        "https://openalex.org/W3035649237",
        "https://openalex.org/W3035664258",
        "https://openalex.org/W3035666843",
        "https://openalex.org/W3035702572",
        "https://openalex.org/W3035740499",
        "https://openalex.org/W3036106327",
        "https://openalex.org/W3036167779",
        "https://openalex.org/W3036974265",
        "https://openalex.org/W3038719422",
        "https://openalex.org/W3038981236",
        "https://openalex.org/W3042918615",
        "https://openalex.org/W3044189835",
        "https://openalex.org/W3045200674",
        "https://openalex.org/W3045662942",
        "https://openalex.org/W3045928028",
        "https://openalex.org/W3046470859",
        "https://openalex.org/W3048817558",
        "https://openalex.org/W3080566854",
        "https://openalex.org/W3080997787",
        "https://openalex.org/W3081203761",
        "https://openalex.org/W3081325717",
        "https://openalex.org/W3081836708",
        "https://openalex.org/W3082154031",
        "https://openalex.org/W3082411326",
        "https://openalex.org/W3087318471",
        "https://openalex.org/W3092339997",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3093218977",
        "https://openalex.org/W3093687066",
        "https://openalex.org/W3094231942",
        "https://openalex.org/W3094500523",
        "https://openalex.org/W3095448863",
        "https://openalex.org/W3096831136",
        "https://openalex.org/W3098465726",
        "https://openalex.org/W3098797593",
        "https://openalex.org/W3099152386",
        "https://openalex.org/W3099414221",
        "https://openalex.org/W3100078588",
        "https://openalex.org/W3100278010",
        "https://openalex.org/W3100324210",
        "https://openalex.org/W3101707147",
        "https://openalex.org/W3102554291",
        "https://openalex.org/W3103523530",
        "https://openalex.org/W3103720336",
        "https://openalex.org/W3103736477",
        "https://openalex.org/W3104097132",
        "https://openalex.org/W3104644561",
        "https://openalex.org/W3104667978",
        "https://openalex.org/W3105259638",
        "https://openalex.org/W3105423481",
        "https://openalex.org/W3108202858",
        "https://openalex.org/W3108433857",
        "https://openalex.org/W3110901318",
        "https://openalex.org/W3111430045",
        "https://openalex.org/W3113177135",
        "https://openalex.org/W3114613321",
        "https://openalex.org/W3116239416",
        "https://openalex.org/W3117178429",
        "https://openalex.org/W3120567415",
        "https://openalex.org/W3122934853",
        "https://openalex.org/W3123909522",
        "https://openalex.org/W3124962940",
        "https://openalex.org/W3128443161",
        "https://openalex.org/W3129850062",
        "https://openalex.org/W3133780103",
        "https://openalex.org/W3134509497",
        "https://openalex.org/W3135205495",
        "https://openalex.org/W3135389928",
        "https://openalex.org/W3136999308",
        "https://openalex.org/W3137385578",
        "https://openalex.org/W3137928916",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3148711710",
        "https://openalex.org/W3151900735",
        "https://openalex.org/W3152893301",
        "https://openalex.org/W3154503084",
        "https://openalex.org/W3155056342",
        "https://openalex.org/W3155322940",
        "https://openalex.org/W3155577228",
        "https://openalex.org/W3156642753",
        "https://openalex.org/W3157039246",
        "https://openalex.org/W3157999218",
        "https://openalex.org/W3158827677",
        "https://openalex.org/W3160021293",
        "https://openalex.org/W3163426640",
        "https://openalex.org/W3164446335",
        "https://openalex.org/W3165171933",
        "https://openalex.org/W3165369424",
        "https://openalex.org/W3165924303",
        "https://openalex.org/W3167334189",
        "https://openalex.org/W3168436232",
        "https://openalex.org/W3169168872",
        "https://openalex.org/W3169450514",
        "https://openalex.org/W3169933688",
        "https://openalex.org/W3171581326",
        "https://openalex.org/W3171764584",
        "https://openalex.org/W3174163042",
        "https://openalex.org/W3174174150",
        "https://openalex.org/W3174823757",
        "https://openalex.org/W3175925542",
        "https://openalex.org/W3175971420",
        "https://openalex.org/W3176393519",
        "https://openalex.org/W3176806965",
        "https://openalex.org/W3176890989",
        "https://openalex.org/W3181414820",
        "https://openalex.org/W3184127157",
        "https://openalex.org/W3187985423",
        "https://openalex.org/W3190664711",
        "https://openalex.org/W3191962800",
        "https://openalex.org/W3192448376",
        "https://openalex.org/W3193553875",
        "https://openalex.org/W3194668998",
        "https://openalex.org/W3200806939",
        "https://openalex.org/W3201058350",
        "https://openalex.org/W3201249640",
        "https://openalex.org/W3204651332",
        "https://openalex.org/W3205227354",
        "https://openalex.org/W3206171352",
        "https://openalex.org/W3208638341",
        "https://openalex.org/W3209048663",
        "https://openalex.org/W3209056694",
        "https://openalex.org/W3209451568",
        "https://openalex.org/W3209764902",
        "https://openalex.org/W3210482950",
        "https://openalex.org/W3210611486",
        "https://openalex.org/W3210987203",
        "https://openalex.org/W3211394146",
        "https://openalex.org/W3211477647",
        "https://openalex.org/W3211973371",
        "https://openalex.org/W3213940558",
        "https://openalex.org/W3214642103",
        "https://openalex.org/W3214674106",
        "https://openalex.org/W3214872094",
        "https://openalex.org/W3215452784",
        "https://openalex.org/W4200635484",
        "https://openalex.org/W4205247958",
        "https://openalex.org/W4206174637",
        "https://openalex.org/W4206357214",
        "https://openalex.org/W4206445139",
        "https://openalex.org/W4206776774",
        "https://openalex.org/W4212805305",
        "https://openalex.org/W4213052788",
        "https://openalex.org/W4213147383",
        "https://openalex.org/W4213457653",
        "https://openalex.org/W4214868967",
        "https://openalex.org/W4220742022",
        "https://openalex.org/W4220933119",
        "https://openalex.org/W4221023051",
        "https://openalex.org/W4221138292",
        "https://openalex.org/W4221149947",
        "https://openalex.org/W4221155201",
        "https://openalex.org/W4221157965",
        "https://openalex.org/W4224309748",
        "https://openalex.org/W4224311348",
        "https://openalex.org/W4224311800",
        "https://openalex.org/W4224983022",
        "https://openalex.org/W4225090121",
        "https://openalex.org/W4225338086",
        "https://openalex.org/W4225405705",
        "https://openalex.org/W4225512856",
        "https://openalex.org/W4225596872",
        "https://openalex.org/W4225977739",
        "https://openalex.org/W4226058932",
        "https://openalex.org/W4226060238",
        "https://openalex.org/W4226208698",
        "https://openalex.org/W4229053887",
        "https://openalex.org/W4234842379",
        "https://openalex.org/W4239789016",
        "https://openalex.org/W4240185200",
        "https://openalex.org/W4240592325",
        "https://openalex.org/W4243799827",
        "https://openalex.org/W4246587917",
        "https://openalex.org/W4255866863",
        "https://openalex.org/W4280535976",
        "https://openalex.org/W4281387042",
        "https://openalex.org/W4281563651",
        "https://openalex.org/W4282913028",
        "https://openalex.org/W4282943426",
        "https://openalex.org/W4283121576",
        "https://openalex.org/W4283218438",
        "https://openalex.org/W4283462727",
        "https://openalex.org/W4283798273",
        "https://openalex.org/W4283810298",
        "https://openalex.org/W4283817628",
        "https://openalex.org/W4284666445",
        "https://openalex.org/W4284698122",
        "https://openalex.org/W4285428788",
        "https://openalex.org/W4286588524",
        "https://openalex.org/W4286795917",
        "https://openalex.org/W4286893581",
        "https://openalex.org/W4287123803",
        "https://openalex.org/W4287325738",
        "https://openalex.org/W4287780403",
        "https://openalex.org/W4287863694",
        "https://openalex.org/W4287991183",
        "https://openalex.org/W4287998109",
        "https://openalex.org/W4288052590",
        "https://openalex.org/W4288088467",
        "https://openalex.org/W4288346884",
        "https://openalex.org/W4289389616",
        "https://openalex.org/W4289533979",
        "https://openalex.org/W4289537189",
        "https://openalex.org/W4290648792",
        "https://openalex.org/W4290875097",
        "https://openalex.org/W4290877962",
        "https://openalex.org/W4293112739",
        "https://openalex.org/W4293370878",
        "https://openalex.org/W4293821372",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4294435970",
        "https://openalex.org/W4294558607",
        "https://openalex.org/W4295097398",
        "https://openalex.org/W4295728955",
        "https://openalex.org/W4295846611",
        "https://openalex.org/W4296047560",
        "https://openalex.org/W4296143727",
        "https://openalex.org/W4296185888",
        "https://openalex.org/W4297510052",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W4297791874",
        "https://openalex.org/W4297946153",
        "https://openalex.org/W4297951436",
        "https://openalex.org/W4297999768",
        "https://openalex.org/W4298052734",
        "https://openalex.org/W4298312696",
        "https://openalex.org/W4301329292",
        "https://openalex.org/W4304097971",
        "https://openalex.org/W4306887124",
        "https://openalex.org/W4307416138",
        "https://openalex.org/W4308505492",
        "https://openalex.org/W4309635196",
        "https://openalex.org/W4309801650",
        "https://openalex.org/W4310012576",
        "https://openalex.org/W4311216457",
        "https://openalex.org/W4312126067",
        "https://openalex.org/W4312689497",
        "https://openalex.org/W4313201684",
        "https://openalex.org/W4315708854",
        "https://openalex.org/W4316495377",
        "https://openalex.org/W4317951161",
        "https://openalex.org/W4318150241",
        "https://openalex.org/W4318347779",
        "https://openalex.org/W4318540750",
        "https://openalex.org/W4318812521",
        "https://openalex.org/W4320814985",
        "https://openalex.org/W4321227311",
        "https://openalex.org/W4321367323",
        "https://openalex.org/W4321479940",
        "https://openalex.org/W4321480027",
        "https://openalex.org/W4321480031",
        "https://openalex.org/W4322614756",
        "https://openalex.org/W4322824839",
        "https://openalex.org/W4323650483",
        "https://openalex.org/W4327525152",
        "https://openalex.org/W4361247736",
        "https://openalex.org/W4362682267",
        "https://openalex.org/W4362714312",
        "https://openalex.org/W4366001157",
        "https://openalex.org/W4366198975",
        "https://openalex.org/W4366975604",
        "https://openalex.org/W4366986925",
        "https://openalex.org/W4367047082",
        "https://openalex.org/W4367047244",
        "https://openalex.org/W4367047306",
        "https://openalex.org/W4367060955",
        "https://openalex.org/W4367595602",
        "https://openalex.org/W4376121360",
        "https://openalex.org/W4378696994",
        "https://openalex.org/W4378766933",
        "https://openalex.org/W4378909195",
        "https://openalex.org/W4378976562",
        "https://openalex.org/W4379185762",
        "https://openalex.org/W4379506768",
        "https://openalex.org/W4380091056",
        "https://openalex.org/W4381573056",
        "https://openalex.org/W4381679608",
        "https://openalex.org/W4382202967",
        "https://openalex.org/W4382239955",
        "https://openalex.org/W4382240004",
        "https://openalex.org/W4382317956",
        "https://openalex.org/W4382468395",
        "https://openalex.org/W4383604692",
        "https://openalex.org/W4384895066",
        "https://openalex.org/W4384915287",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385270450",
        "https://openalex.org/W4385568380",
        "https://openalex.org/W4386445678",
        "https://openalex.org/W4386494629",
        "https://openalex.org/W4386587901",
        "https://openalex.org/W4386620158",
        "https://openalex.org/W4386711666",
        "https://openalex.org/W4386746103",
        "https://openalex.org/W4387092606",
        "https://openalex.org/W4387969028",
        "https://openalex.org/W4388185880",
        "https://openalex.org/W4388537645",
        "https://openalex.org/W4388684696",
        "https://openalex.org/W4390534655",
        "https://openalex.org/W4390572965",
        "https://openalex.org/W4391345293",
        "https://openalex.org/W4391561379",
        "https://openalex.org/W637153065"
      ],
      "openalex_rank": 2,
      "num_tokens": 42218,
      "best_oa_location_pdf_url": "https://arxiv.org/pdf/2304.05055",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key advancements in deep graph representation learning, what challenges do researchers currently face in this field, and what future directions could enhance the effectiveness of these techniques in various applications?",
      "pipeline_answer": "### Key Advancements in Deep Graph Representation Learning\n\n- **Graph Neural Networks (GNNs)**: Recent advancements have led to the development of scalable GNN architectures, such as the PaSca framework, which allows for the exploration of a design space with 150,000 different GNN designs, achieving state-of-the-art performance and training speedups of up to 28.3 times compared to traditional GNNs [S8739179638]. The Multi-hop Attention Graph Neural Network (MAGNA) has also improved the ability to capture large-scale structural information, resulting in a 5.7% relative error reduction on benchmark datasets like Cora and Citeseer [S8739179638].\n\n- **Interdisciplinary Approaches**: The integration of GNNs with other machine learning techniques has shown promise. For instance, the Spatial-Temporal Attention Wavenet (STAWnet) model combines convolutional neural networks with attention mechanisms to effectively capture complex spatial-temporal dependencies in traffic prediction tasks, outperforming state-of-the-art methods [S2089320319]. Additionally, the Graph Neural Architecture Search (GNAS) framework enables the automatic learning of optimal GNN architectures, enhancing performance on large-scale datasets [S2089320319].\n\n- **Standardized Evaluation Metrics**: The introduction of platforms like GraphGym provides guidelines for designing and evaluating GNNs, facilitating performance comparisons across various architectures and tasks [S5300631602]. The Multi-Relational Graph Attention Network (MRGAT) emphasizes the importance of self-attention mechanisms in GNNs, improving link prediction tasks [S5300631602].\n\n### Current Challenges in the Field\n\n- **Scalability Issues**: Many GNN models struggle with scalability when applied to large-scale, real-world graphs, affecting their efficiency and accuracy [S5300631602].\n\n- **Overfitting and Data Imbalance**: Overfitting is a persistent issue, particularly with sparse or imbalanced graph data, leading to models that do not generalize well [S5300631602].\n\n- **Interpretability Challenges**: The interpretability of GNNs is limited, making it difficult to understand the decision-making processes within these models, which is crucial for their adoption in sensitive applications [S5300631602].\n\n- **Integration with Other Paradigms**: There is a lack of comprehensive frameworks that effectively integrate GNNs with other machine learning paradigms to address existing challenges [S5300631602].\n\n### Future Directions for Improvement\n\n- **Developing Scalable Architectures**: Future research should focus on creating GNN architectures that can efficiently handle large-scale graphs without sacrificing performance [S5300631602].\n\n- **Mitigating Overfitting**: Innovative techniques for addressing overfitting, especially in sparse and imbalanced datasets, should be explored [S5300631602].\n\n- **Enhancing Interpretability**: Methods to improve the transparency and interpretability of GNN models are needed [S5300631602].\n\n- **Interdisciplinary Integration**: Establishing comprehensive frameworks that integrate GNNs with other AI models can leverage their combined strengths [S5300631602].\n\n- **Standardizing Evaluation Metrics**: Creating standardized evaluation frameworks will help better understand and improve GNN performance across various tasks [S5300631602].\n\n### Conclusion\n\nDeep graph representation learning, particularly through GNNs, has made significant advancements but continues to face challenges in scalability, overfitting, and interpretability. Addressing these issues through interdisciplinary approaches and standardized metrics will be crucial for the future development of effective GNN applications.",
      "pipeline_references": {
        "S2089320319": {
          "id": "S2089320319",
          "text": "The Spatial-Temporal Attention Wavenet (STAWnet) model effectively captures complex spatial-temporal dependencies in traffic prediction tasks by combining convolutional neural networks with attention mechanisms. This model has outperformed state-of-the-art methods on several public datasets, demonstrating the potential of hybrid models in addressing specific challenges in graph representation learning. Additionally, the introduction of the Graph Neural Architecture Search (GNAS) framework allows for the automatic learning of optimal GNN architectures, significantly improving performance on large-scale datasets while reducing the reliance on manual design efforts.",
          "children": [
            {
              "id": "E7863301740",
              "text": "IET Intelligent Transport SystemsVolume 15, Issue 4 p. 549-561 ORIGINAL RESEARCH PAPEROpen Access Spatial-temporal attention wavenet: A deep learning framework for traffic prediction considering spatial-temporal dependencies Chenyu Tian, orcid.org/0000-0002-7748-5873 Tsinghua-Berkeley Shenzhen Institute, Tsinghua Univeristy, Shenzhen, 510006 People's Republic of ChinaSearch for more papers by this authorWai Kin (Victor) Chan, Corresponding Author chanw@sz.tsinghua.edu.cn orcid.org/0000-0002-7202-1922 Tsinghua-Berkeley Shenzhen Institute, Tsinghua Univeristy, Shenzhen, 510006 People's Republic of China Correspondence Wai Kin (Victor) Chan, Tsinghua-Berkeley Shenzhen Institute, Tsinghua Univeristy, Shenzhen 510006, Guangdong, People's Republic of China. Email: chanw@sz.tsinghua.edu.cnSearch for more papers by this author Chenyu Tian, orcid.org/0000-0002-7748-5873 Tsinghua-Berkeley Shenzhen Institute, Tsinghua Univeristy, Shenzhen, 510006 People's Republic of ChinaSearch for more papers by this authorWai Kin (Victor) Chan, Corresponding Author chanw@sz.tsinghua.edu.cn orcid.org/0000-0002-7202-1922 Tsinghua-Berkeley Shenzhen Institute, Tsinghua Univeristy, Shenzhen, 510006 People's Republic of China Correspondence Wai Kin (Victor) Chan, Tsinghua-Berkeley Shenzhen Institute, Tsinghua Univeristy, Shenzhen 510006, Guangdong, People's Republic of China. Email: chanw@sz.tsinghua.edu.cnSearch for more papers by this author First published: 02 March 2021 https://doi.org/10.1049/itr2.12044AboutSectionsPDF ToolsRequest permissionExport citationAdd to favoritesTrack citation ShareShare Give accessShare full text accessShare full-text accessPlease review our Terms and Conditions of Use and check box below to share full-text version of article.I have read and accept the Wiley Online Library Terms and Conditions of UseShareable LinkUse the link below to share a full-text version of this article with your friends and colleagues. Learn more.Copy URL Share a linkShare onEmailFacebookTwitterLinked InRedditWechat Abstract Traffic prediction on road networks is highly challenging due to the complexity of traffic systems and is a crucial task in successful intelligent traffic system applications. Existing approaches mostly capture the static spatial dependency relying on the prior knowledge of the graph structure. However, the spatial dependency can be dynamic, and sometimes the physical structure may not reflect the genuine relationship between roads. To better capture the complex spatial-temporal dependencies and forecast traffic conditions on road networks, a multi-step prediction model named Spatial-Temporal Attention Wavenet (STAWnet) is proposed. Temporal convolution is applied to handle long time sequences, and the dynamic spatial dependencies between different nodes can be captured using the self-attention network. Different from existing models, STAWnet does not need prior knowledge of the graph by developing a self-learned node embedding. These components are integrated into an end-to-end framework. The experimental results on three public traffic prediction datasets (METR-LA, PEMS-BAY, and PEMS07) demonstrate effectiveness. In particular, in the 1 h ahead prediction, STAWnet outperforms state-of-the-art methods with no prior knowledge of the network. 1 INTRODUCTION With the recent development in intelligent traffic system, the scale and dimension of spatial-temporal data from sensors become larger, which serve as critical inputs to a wide range of applications. Traffic prediction that aims to model the dynamic change of the traffic system is a well-studied spatial-temporal prediction problem, and multi-step traffic forecasting on road network is a crucial task in the transportation industry. High-precision traffic prediction has wide applications. It can not only help travelers plan their routes but also provide insightful information for proactive traffic management strategy to improve traffic efficiency and safety. The objective of traffic prediction is to predict the future traffic conditions (e.g. traffic volume or speed) in road networks based on historical observations. Many efforts have been conducted to develop methods for traffic prediction [1]. Different from other prediction tasks, traffic prediction on traffic networks need to model the non-Euclidean topology structure of traffic networks, the stochastic characteristic of the time-varying traffic patterns, and spatial-temporal dependencies. This task has two major features. First, there are non-linear temporal correlations, for example, the traffic conditions can fluctuate periodically (e.g. morning peak and evening peak), affecting the correlations between different time steps. Second, there are dynamic spatial correlations, which mean the dependencies of nodes in a road network can change over time considering different traffic conditions, for example, the propagation of the traffic congestion to the upstream and the dissipation. Recently, deep learning models have been widely applied in traffic prediction and employed in intelligent traffic systems, showing the effectiveness especially when integrating the graph structure into the models [2-8]. Compared with traditional time-series and machine learning methods, deep learning models can flexibly handle relatively long time sequence and large traffic network structure. However, many existing approaches face some major shortcomings. Graph convolution and graph attention based methods highly depend on adjacency matrix whose coefficients are computed by spatial information (e.g. distance between censors), but sometimes the coefficients cannot entail the genuine dependency relationships. To give an example in Figure 1, the close distance between nodes (or detectors) cannot indicate strong spatial dependencies and nodes that have similar distance can have different impacts. There are circumstances when connections can not entail the relationship when the connections are missing, for the reason that the spatial information sometimes can be unavailable due to secrecy or privacy, and the relations are more complicated than connectivity considering various attributes such as the number of lanes, surrounding environment, and infrastructure conditions. The fixed coefficients may fail to model to dynamic spatial dependencies and result in inaccuracy, because different nodes have different impacts and even the same location has varying influence as time goes by in terms of traffic volume, density and relevant emergent events [2]. They fail to simultaneously model the spatial-temporal features and the dynamic correlations of traffic data. FIGURE 1Open in figure viewerPowerPoint A simple example of the distance cannot entail the genuine dependency. Detector A and detector B is close but they are less related because have contrary directions. Detector C and detector D have the same distance to detector A, but they have different effects on it because one is on the upstream and the other is on the downstream To address existing challenges, we propose a novel deep learning framework, named as Spatial-Temporal Attention Wavenet (STAWnet). Specifically, we integrate the convolution neural network [9] and attention mechanism [10] into an end-to-end framework to extract the spatial-temporal dependencies. By developing a self-adaptive node embedding, STAWnet can capture the hidden spatial relationship in the data without knowing the graph structure information. We evaluate STAWnet on three public traffic network datasets, METR-LA, PEMS-BAY and PEMS07. STAWnet can achieve satisfactory performance but without prior knowledge of the network as an input, which means our method can be applied to other tasks flexibly. The main contributions of this work are as follows: Compared to existing model, STAWnet used self-learned node embedding to learn the latent spatial relationship instead of extracting adjacency relationships from prior knowledge of the graph. It brings high flexibility and can be easily extended to other spatial-temporal forecasting tasks. We designed a dynamic attention mechanism that can adjust the coefficients of different nodes based on traffic conditions and spatial information. STAWnet can overcome the difficulty in multi-step prediction considering complex dynamic spatial-temporal dependencies and provide certain explainability. The results on real world datasets indicate that STAWnet yields leading predictions performance in terms of various prediction error measures. The rest of the paper is organized as follows: In Section 2, we give a literature review of related works. In Section 3, we formalize the traffic prediction problem and introduce the overall framework of the STAWnet. In Section 4, experiments are implemented on three datasets to compare with other models. Then we analyze the components of the model in detail in Section 5. Finally, we conclude our work and future directions. 2 LITERATURE REVIEW 2.1 Traffic forecasting Traffic forecasting has been studied for decades, and various emerging methods have been constantly proposed to model traffic characteristics. Lint and Hinsbergen divided these methods into three categories, that is, naive methods, parametric methods and non-parametric methods [11]. Parametric methods often require a wealth of prior knowledge based on queuing theory and traffic flow theory and they cannot handle unpredictability or complex factors. With the rapid development of real-time traffic collection methods, non-parametric (or data-driven) approaches through mass historical data to capture similar traffic patterns prevail in recent years. Further, Zhang et al. divided data-driven methods into three representative sub-categories, that is, statistical models, shallow machine learning models and deep learning models [8]. Given historical observations, many traffic prediction studies only consider temporal dependencies using time-series models. The autoregressive integrated moving average (ARIMA) and Kalman filtering have been widely applied [12, 13]. These methods have difficulty achieving high accuracies because they ignore spatial dependencies and only consider the dynamic change of traffic conditions based on the stationary assumption of time sequences. However, this assumption is usually unsatisfied considering traffic dynamics. Machine learning methods such as KNN [14] and SVM [15] are also applied to model complex traffic data and yield satisfactory results. Guo et al. build feature extraction model, and applied k-means method to divide the stations into different types. Then they proposed a hybrid prediction model based on kernel ridge regression and Gaussian process regression to predict the short-term passenger flow of urban rail transit, and verified it on the Automatic Fare Collection System data [16..",
              "url": "https://openalex.org/W3135694489",
              "openalex_id": "https://openalex.org/W3135694489",
              "title": "Spatial\u2010temporal attention wavenet: A deep learning framework for traffic prediction considering spatial\u2010temporal dependencies",
              "publication_date": "2021-03-02"
            },
            {
              "id": "E5354390581",
              "text": "Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction towards cognition and human-level intelligence. In this survey, we provide a comprehensive review of knowledge graph covering overall research topics about 1) knowledge graph representation learning, 2) knowledge acquisition and completion, 3) temporal knowledge graph, and 4) knowledge-aware applications, and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning, are reviewed. We further explore several emerging topics, including meta relational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of datasets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.",
              "url": "https://openalex.org/W3003265726",
              "openalex_id": "https://openalex.org/W3003265726",
              "title": "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications",
              "publication_date": "2022-02-01"
            },
            {
              "id": "E6562088986",
              "text": "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.",
              "url": "https://openalex.org/W2962756421",
              "openalex_id": "https://openalex.org/W2962756421",
              "title": "node2vec",
              "publication_date": "2016-08-08"
            }
          ]
        },
        "S8739179638": {
          "id": "S8739179638",
          "text": "The PaSca framework introduces a novel Scalable Graph Neural Architecture Paradigm (SGAP) that allows for the systematic exploration of a design space consisting of 150,000 different GNN designs. This framework has demonstrated state-of-the-art performance on benchmark datasets, achieving up to 28.3 times training speedups compared to traditional GNNs. Furthermore, the introduction of the Multi-hop Attention Graph Neural Network (MAGNA) has shown that incorporating multi-hop context information into attention computations can improve the model's ability to capture large-scale structural information, resulting in up to 5.7% relative error reduction on benchmark datasets such as Cora and Citeseer.",
          "children": [
            {
              "id": "E6598505770",
              "text": "Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.",
              "url": "https://openalex.org/W2911286998",
              "openalex_id": "https://openalex.org/W2911286998",
              "title": "Heterogeneous Graph Attention Network",
              "publication_date": "2019-05-13"
            },
            {
              "id": "E7599916961",
              "text": "Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection , has emerged as a critical direction. This article surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in 3 high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages, and disadvantages and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.",
              "url": "https://openalex.org/W3135550350",
              "openalex_id": "https://openalex.org/W3135550350",
              "title": "Deep Learning for Anomaly Detection",
              "publication_date": "2021-03-05"
            },
            {
              "id": "E7082228960",
              "text": "In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.",
              "url": "https://openalex.org/W2914721378",
              "openalex_id": "https://openalex.org/W2914721378",
              "title": "Graph Neural Networks for Social Recommendation",
              "publication_date": "2019-05-13"
            }
          ]
        },
        "S5300631602": {
          "id": "S5300631602",
          "text": "Recent studies emphasize the need for standardized evaluation metrics to assess the performance of GNNs across different tasks and datasets. For instance, the introduction of the GraphGym platform provides a comprehensive set of guidelines for designing and evaluating GNNs, facilitating the comparison of various architectures and their performance on 32 different predictive tasks. Moreover, the development of the Multi-Relational Graph Attention Network (MRGAT) highlights the importance of self-attention mechanisms in GNNs, allowing for the differentiation of neighbor importance in heterogeneous graphs. This approach has shown significant improvements in link prediction tasks, underscoring the necessity of robust benchmarking practices to guide future research and development in GNNs.",
          "children": [
            {
              "id": "E3883105713",
              "text": "..combinations. A major recent advance in machine learning is automating this critical step by learning a suitable representation of the data with deep artificial neural networks (Bengio et al, 2013; LeCun et al, 2015; Schmidhuber, 2015) (Fig 1D). Briefly, a deep neural network takes the raw data at the lowest (input) layer and transforms them into increasingly abstract feature representations by successively combining outputs from the preceding layer in a data-driven manner, encapsulating highly complicated functions in the process (Box 1). Deep learning is now one of the most active fields in machine learning and has been shown to improve performance in image and speech recognition (Hinton et al, 2012; Krizhevsky et al, 2012; Graves et al, 2013; Zeiler & Fergus, 2014; Deng & Togneri, 2015), natural language understanding (Bahdanau et al, 2014; Sutskever et al, 2014; Lipton, 2015; Xiong et al, 2016), and most recently, in computational biology (Eickholt & Cheng, 2013; Dahl et al, 2014; Leung et al, 2014; S\u00f8nderby & Winther, 2014; Alipanahi et al, 2015; Wang et al, 2015; Zhou & Troyanskaya, 2015; Kelley et al, 2016). Box 1: Artificial Neural Network An artificial neural network, initially inspired by neural networks in the brain (McCulloch & Pitts, 1943; Farley & Clark, 1954; Rosenblatt, 1958), consists of layers of interconnected compute units (neurons). The depth of a neural network corresponds to the number of hidden layers, and the width to the maximum number of neurons in one of its layers. As it became possible to train networks with larger numbers of hidden layers, artificial neural networks were rebranded to \u201cdeep networks\u201d. In the canonical configuration, the network receives data in an input layer, which are then transformed in a nonlinear way through multiple hidden layers, before final outputs are computed in the output layer (panel A). Neurons in a hidden or output layer are connected to all neurons of the previous layer. Each neuron computes a weighted sum of its inputs and applies a nonlinear activation function to calculate its output f(x) (panel B). The most popular activation function is the rectified linear unit (ReLU; panel B) that thresholds negative signals to 0 and passes through positive signal. This type of activation function allows faster learning compared to alternatives (e.g. sigmoid or tanh unit) (Glorot et al, 2011). The weights w(i) between neurons are free parameters that capture the model's representation of the data and are learned from input/output samples. Learning minimizes a loss function L(w) that measures the fit of the model output to the true label of a sample (panel A, bottom). This minimization is challenging, since the loss function is high-dimensional and non-convex, similar to a landscape with many hills and valleys (panel C). It took several decades before the backward propagation algorithm was first applied to compute a loss function gradient via chain rule for derivatives (Rumelhart et al, 1988), ultimately enabling efficient training of neural networks using stochastic gradient descent. During learning, the predicted label is compared with the true label to compute a loss for the current set of model weights. The loss is then backward propagated through the network to compute the gradients of the loss function and update (panel A). The loss function L(w) is typically optimized using gradient-based descent. In each step, the current weight vector (red dot) is moved along the direction of steepest descent dw (direction arrow) by learning rate \u03b7 (length of vector). Decaying the learning rate over time allows to explore different domains of the loss function by jumping over valleys at the beginning of the training (left side) and fine-tune parameters with smaller learning rates in later stages of the model training. While learning in deep neural networks remains an active area of research, existing software packages (Table 1) can already be applied without knowledge of the mathematical details involved. Alternative architectures to such fully connected feedforward networks have been developed for specific applications, which differ in the way neurons are arranged. These include convolutional neural networks, which are widely used for modelling images (Box 2), recurrent neural networks for sequential data (Sutskever, 2013; Lipton, 2015), or restricted Boltzmann machines (Salakhutdinov & Larochelle, 2010; Hinton, 2012) and autoencoders (Hinton & Salakhutdinov, 2006; Alain et al, 2012; Kingma & Welling, 2013) for unsupervised learning. The choice of network architecture and other parameters can be made in a data-driven and objective way by assessing the model performance on a validation data set. The potential of deep learning in high-throughput biology is clear: in principle, it allows to better exploit the availability of increasingly large and high-dimensional data sets (e.g. from DNA sequencing, RNA measurements, flow cytometry or automated microscopy) by training complex networks with multiple layers that capture their internal structure (Fig 1C and D). The learned networks discover high-level features, improve performance over traditional models, increase interpretability and provide additional understanding about the structure of the biological data. In this review, we discuss recent and forthcoming applications of deep learning, with a focus on applications in regulatory genomics and biological image analysis. The goal of this review was not to provide comprehensive background on all technical details, which can be found in the more specialized literature (Bengio, 2012; Bengio et al, 2013; Deng, 2014; Schmidhuber, 2015; Goodfellow et al, 2016). Instead, we aimed to provide practical pointers and the necessary background to get started with deep architectures, review current software solutions and give recommendations for applying them to data. The applications we cover are deliberately broad to illustrate differences and commonalities between approaches; reviews focusing on specific domains can be found elsewhere (Park & Kellis, 2015; Gawehn et al, 2016; Leung et al, 2016; Mamoshina et al, 2016). Finally, we discuss both the potential and possible pitfalls of deep learning and contrast these methods to traditional machine learning and classical statistical analysis approaches. Deep learning for regulatory genomics Conventional approaches for regulatory genomics relate sequence variation to changes in molecular traits. One approach is to leverage variation between genetically diverse individuals to map quantitative trait loci (QTL). This principle has been applied to identify regulatory variants that affect gene expression levels (Montgomery et al, 2010; Pickrell et al, 2010), DNA methylation (Gibbs et al, 2010; Bell et al, 2011), histone marks (Grubert et al, 2015; Waszak et al, 2015) and proteome variation (Vincent et al, 2010; Albert et al, 2014; Parts et al, 2014; Battle et al, 2015) (Fig 2A). Better statistical methods have helped to increase the power to detect regulatory QTL (Kang et al, 2008; Stegle et al, 2010; Parts et al, 2011; Rakitsch & Stegle, 2016); however, any mapping approach is intrinsically limited to variation that is present in the training population. Thus, studying the effects of rare mutations in particular requires data sets with very large sample size. Figure 2. Principles of using neural networks for predicting molecular traits from DNA sequence(A) DNA sequence and the molecular response variable along the genome for three individuals. Conventional approaches in regulatory genomics consider variations between individuals, whereas deep learning allows exploiting intra-individual variations by tiling the genome into sequence DNA windows centred on individual traits, resulting in large training data sets from a single sample. (B) One-dimensional convolutional neural network for predicting a molecular trait from the raw DNA sequence in a window. Filters of the first convolutional layer (example shown on the edge) scan for motifs in the input sequence. Subsequent pooling reduces the input dimension, and additional convolutional layers can model interactions between motifs in the previous layer. (C) Response variable predicted by the neural network shown in (B) for a wild-type and mutant sequence is used as input to an additional neural network that predicts a variant score and allows to discriminate normal from deleterious variants. (D) Visualization of a convolutional filter by aligning genetic sequences that maximally activate the filter and creating a sequence motif. (E) Mutation map of a sequence window. Rows correspond to the four possible base pair substitutions, columns to sequence positions. The predicted impact of any sequence change is colour-coded. Letters on top denote the wild-type sequence with the height of each nucleotide denoting the maximum effect across mutations (figure panel adapted from Alipanahi et al, 2015). Download figure Download PowerPoint An alternative is to train models that use variation between regions within a genome (Fig 2A). Splitting the sequence into windows centred on the trait of interest gives rise to tens of thousands of training examples for most molecular traits even when using a single individual. Even with large data sets, predicting molecular traits from DNA sequence is challenging due to multiple layers of abstraction between the effect of individual DNA variants and the trait of interest, as well as the dependence of the molecular traits on a broad sequence context and interactions with distal regulatory elements. The value of deep neural networks in this context is twofold. First, classical machine learning methods cannot operate on the sequence directly, and thus require pre-defining features that can be extracted from the sequence based on prior knowledge (e.g. the presence or absence of single-nucleotide variants (SNVs), k-mer frequencies, motif occurrences, conservation, known regulatory variants or structural elements). Deep neural networks can help circumventing the manual extraction of features by learning them from data. Second, because of their representational richness, they can capture nonlinear dependencies in the sequence and interaction effects and span wider sequence context at multiple genomic scales. Attesting to their utility, deep neural networks have been successfully applied to predict splicing activity (Leung et al, 2014; Xiong et al, 2015),..",
              "url": "https://openalex.org/W2502949459",
              "openalex_id": "https://openalex.org/W2502949459",
              "title": "Deep learning for computational biology",
              "publication_date": "2016-07-01"
            },
            {
              "id": "E9662170716",
              "text": "Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.",
              "url": "https://openalex.org/W2610332124",
              "openalex_id": "https://openalex.org/W2610332124",
              "title": "Deep learning for healthcare: review, opportunities and challenges",
              "publication_date": "2017-04-05"
            },
            {
              "id": "E2082628655",
              "text": "Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction toward cognition and human-level intelligence. In this survey, we provide a comprehensive review of the knowledge graph covering overall research topics about: 1) knowledge graph representation learning; 2) knowledge acquisition and completion; 3) temporal knowledge graph; and 4) knowledge-aware applications and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning are reviewed. We further explore several emerging topics, including metarelational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of data sets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.",
              "url": "https://openalex.org/W3003265726",
              "openalex_id": "https://openalex.org/W3003265726",
              "title": "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications",
              "publication_date": "2021-04-26"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W3200915061",
        "https://openalex.org/W4387343643",
        "https://openalex.org/W4210257598",
        "https://openalex.org/W4361208353",
        "https://openalex.org/W3126928293",
        "https://openalex.org/W3178367256",
        "https://openalex.org/W2994598354",
        "https://openalex.org/W2610332124",
        "https://openalex.org/W4382319519",
        "https://openalex.org/W3027983943",
        "https://openalex.org/W2900974064",
        "https://openalex.org/W4366091323",
        "https://openalex.org/W3199726267",
        "https://openalex.org/W3003265726",
        "https://openalex.org/W2922228302",
        "https://openalex.org/W2612872092",
        "https://openalex.org/W2911286998",
        "https://openalex.org/W3082291914",
        "https://openalex.org/W4390075187",
        "https://openalex.org/W2502949459",
        "https://openalex.org/W3193327410",
        "https://openalex.org/W3108426037",
        "https://openalex.org/W2743104969",
        "https://openalex.org/W3137729291",
        "https://openalex.org/W4322627364",
        "https://openalex.org/W4306317971",
        "https://openalex.org/W4388563693",
        "https://openalex.org/W2962756421",
        "https://openalex.org/W2604319603",
        "https://openalex.org/W3093694263",
        "https://openalex.org/W4286009331",
        "https://openalex.org/W4385483925",
        "https://openalex.org/W3188579603",
        "https://openalex.org/W3171592446",
        "https://openalex.org/W2914721378",
        "https://openalex.org/W3216273815",
        "https://openalex.org/W3172408309",
        "https://openalex.org/W3179205840",
        "https://openalex.org/W4220720005",
        "https://openalex.org/W3175191608",
        "https://openalex.org/W3098230582",
        "https://openalex.org/W3135550350",
        "https://openalex.org/W4315977496",
        "https://openalex.org/W2962936633",
        "https://openalex.org/W4380993339",
        "https://openalex.org/W3197217317",
        "https://openalex.org/W4388036494",
        "https://openalex.org/W3135694489",
        "https://openalex.org/W3208687975"
      ],
      "evaluation": {
        "precision@10": 0.0,
        "recall@10": 0.0,
        "f1@10": 0.0,
        "rouge_1": 0.0296699752750206,
        "rouge_2": 0.012757570327898225,
        "rouge_l": 0.016626652811122657,
        "text_f1": 0.08355321020228672,
        "num_source_papers": 49
      }
    },
    {
      "id": "https://openalex.org/W4388017359",
      "limited_meta": {
        "title": "End-to-End Speech Recognition: A Survey",
        "publication_date": "2024-01-01",
        "cited_by_count": 33,
        "url": ""
      },
      "text": "1\nEnd-to-End Speech Recognition: A Survey\nRohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE,\nRalf Schluter, \u00a8 Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE\nAbstract\u2014In the last decade of automatic speech recognition\n(ASR) research, the introduction of deep learning has brought\nconsiderable reductions in word error rate of more than 50%\nrelative, compared to modeling without deep learning. In the\nwake of this transition, a number of all-neural ASR architectures\nhave been introduced. These so-called end-to-end (E2E) models\nprovide highly integrated, completely neural ASR models, which\nrely strongly on general machine learning knowledge, learn more\nconsistently from data, with lower dependence on ASR domain\u0002specific experience. The success and enthusiastic adoption of deep\nlearning, accompanied by more generic model architectures has\nled to E2E models now becoming the prominent ASR approach.\nThe goal of this survey is to provide a taxonomy of E2E ASR\nmodels and corresponding improvements, and to discuss their\nproperties and their relationship to classical hidden Markov\nmodel (HMM) based ASR architectures. All relevant aspects\nof E2E ASR are covered in this work: modeling, training,\ndecoding, and external language model integration, discussions of\nperformance and deployment opportunities, as well as an outlook\ninto potential future developments.\nIndex Terms\u2014end-to-end, automatic speech recognition.\nI. INTRODUCTION\nThe classical1statistical architecture decomposes an auto\u0002matic speech recognition (ASR) system into four main compo\u0002nents: acoustic feature extraction from speech audio signals,\nacoustic modeling, language modeling and search based on\nBayes\u2019 decision rule [1], [2], [3]. Classical acoustic modeling\nis based on hidden Markov models (HMMs) to account for\nspeaking rate variation. Within the classical approach, deep\nlearning has been introduced into acoustic and language mod\u0002eling. In acoustic modeling, deep learning has replaced Gaus\u0002sian mixture distributions (hybrid HMM [4], [5]) or augmented\nthe acoustic feature set (e.g., non-linear discriminant/tandem\napproach [6], [7]). In language modeling, deep learning has re\u0002placed count-based approaches [8], [9], [10]. However, in these\nearly attempts at introducing deep learning, the classical ASR\narchitecture was unmodified. Classical state-of-the-art ASR\nsystems today are composed of many separate components and\nknowledge sources: especially speech signal preprocessing;\nmethods for robustness with respect to recording conditions;\nphoneme inventories and pronunciation lexica; phonetic clus\u0002tering; handling of out-of-vocabulary words; various methods\nfor adaptation/normalization; elaborate training schedules with\ndifferent objectives including sequence discriminative training,\netc. The potential of deep learning, on the other hand, initiated\nsuccessful approaches to integrate formerly separate modeling\nsteps, e.g., by integrating speech signal pre-processing and\nfeature extraction into acoustic modeling [11], [12].\n1 The term \u201cclassical\u201d here refers to the former, long-term, state-of-the-art\nASR architecture based on the decomposition into acoustic and language\nmodel, and with acoustic modeling based on hidden Markov models.\nMore consequently, the introduction of deep learning to\nASR also initiated research to replace classical ASR archi\u0002tectures based on hidden Markov models (HMM) with more\nintegrated joint neural network model structures [13], [14],\n[15], [16]. These ventures might be seen as trading specific\nspeech processing models for more generic machine learning\napproaches to sequence-to-sequence processing \u2013 akin to how\nstatistical approaches to natural language processing have\ncome to replace more linguistically oriented models. For these\nall-neural approaches recently the term end-to-end (E2E) [14],\n[17], [18], [19] has been established. Therefore, first of all\nan attempt to define the term end-to-end in the context of\nASR is due in this survey. According to the Cambridge\nDictionary, the adjective \u201cend-to-end\u201d is defined as: \u201cinclud\u0002ing all the stages of a process\u201d [20]. We therefore propose\nthe following definition of end-to-end ASR: an integrated\nASR model that enables joint training from scratch; avoids\nseparately obtained knowledge sources; and, provides single\u0002pass recognition consistent with the objective to optimize the\ntask-specific evaluation measure, i.e., usually label (word,\ncharacter, subword, etc.) error rate. While this definition\nsuffices for the present discussion, we note that such an\nidealized definition hides many nuances involved in the term\nE2E and lacks distinctiveness; we elaborate on some of these\nnuances in Sec. II to discuss the various connotations of the\nterm E2E in the context of ASR.\nWhat are potential benefits of E2E approaches to ASR?\nThe primary objective when developing an ASR systems is to\nminimize the expected word error rate; secondary objectives\nare to reduce time and memory complexity of the resulting\ndecoder, and \u2013 assuming a constrained development budget \u2013\ngenericity, and ease of modeling. First of all, an integrated\nASR system, defined in terms of a single neural network\nstructure supports genericity of modeling and may allow for\nfaster development cycles when building ASR systems for\nnew languages or domains. Similarly, ASR models defined\nby a single neural network structure may become more \u2018lean\u2019\ncompared to classical modeling, with a simpler decoding\nprocess, obviating the need to integrate separate models. The\nresulting reduction in memory footprint and power consump\u0002tion supports embedded ASR applications [21], [22]. Further\u0002more, end-to-end joint training may help to avoid spurious\noptima from intermediate training stages. Avoiding secondary\nknowledge sources like pronunciation lexica may be helpful\nfor languages/domains where such resources are not easily\navailable. Also, secondary knowledge sources may themselves\nbe erroneous; avoiding these may improve models trained\ndirectly from data, provided that sufficient amounts of task\u0002specific training data are available.\nWith the current surge of interest in E2E ASR models and an\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nincreasing diversity of corresponding work, the authors of this\nreview think it is time to provide an overview of this rapidly\nevolving domain of research. The goal of this survey is to\nprovide an in-depth overview of the current state of research\non E2E ASR systems, covering all relevant aspects of E2E\nASR, with a contrastive discussion of the different E2E and\nclassical ASR architectures.\nThis survey of E2E speech recognition is structured as fol\u0002lows. Sec. II discusses the nuances in the term E2E as it applies\nto ASR. Sec. III describes the historical evolution of E2E\nspeech recognition, with specific focus on the input-output\nalignment and an overview of prominent E2E ASR models.\nSec. IV discusses improvements of the basic E2E models,\nincluding E2E model combination, training loss functions,\ncontext, encoder/decoder structures and endpointing. Sec. V\nprovides an overview of E2E ASR model training. Decoding\nalgorithms for the different E2E approaches are discussed\nin Sec. VI. Sec. VII discusses the role and integration of\n(separate) language models in E2E ASR. Sec. VIII reviews\nexperimental comparisons of the different E2E as well as\nclassical ASR approaches. Sec. IX provides an overview of\napplications of E2E ASR. Sec. X investigates future directions\nof E2E research in ASR, before concluding in Sec. XI. Finally,\nwe note that this survey paper also includes comparative\ndiscussions between novel E2E models and classical HMM\u0002based ASR approaches in terms of various aspects; most\nsections end with a summarization of the relationship between\nE2E models and HMM-based ASR approaches in relation to\nthe topics covered within the respective sections.\nII. DISTINCTIVENESS OF THE TERM E2E\nAs noted in Sec. I the term E2E provides an idealized\ndefinition of ASR systems, and can benefit from a more\ndetailed discussion based on the following perspectives.\na) Joint Modeling: In terms of ASR, the E2E property\ncan be interpreted as considering all components of an ASR\nsystem jointly as a single computational graph. Even more so,\nthe common understanding of E2E in ASR is that of a single\njoint modeling approach that does not necessarily distinguish\nseparate components, which may also mean dropping the\nclassical separation of ASR into an acoustic model and a\nlanguage model. However, in practice E2E ASR systems are\noften combined with external language models trained on text\u0002only data, which weakens the end-to-end nature of the system\nto some extent.\nb) Joint Training: In terms of model training, E2E can\nbe interpreted as estimating all parameters, of all components\nof a model jointly using a single objective function that is\nconsistent with the task at hand, which in case of ASR means\nminimizing the expected word error rate2. However, the term\nlacks distinctiveness here, as classical and/or modular ASR\nmodel architectures also support joint training with a single\nobjective.\n2 Note that this does not necessarily require Bayes Risk training, as standard\ntraining criteria like cross entropy, maximum mutual information and max\u0002imum likelihood in case of classical ASR models asymptotically guarantee\noptimal performance in the sense of Bayes decision rule, also [23], [24].\nc) Training from Scratch: The E2E property can also be\ninterpreted with respect to the training process itself, by re\u0002quiring training from scratch, avoiding external knowledge like\nprior alignments or initial models pre-trained using different\ncriteria or knowledge sources. However, note that pre-training\nand fine-tuning strategies are also relevant, if the model has\nexplicit modularity, including self-supervised learning [25] or\njoint training of front-end and speech recognition models [26].\nEspecially in case of limited amounts of target task training\ndata, utilizing large pretrained models is important to obtain\nperformant E2E ASR systems.\nd) Avoiding Secondary Knowledge Sources: For ASR,\nstandard secondary knowledge sources are pronunciation lex\u0002ica and phoneme sets, as well as phonetic clustering, which\nin classical state-of-the-art ASR systems usually is based on\nclassification and regression trees (CART) [27]. Secondary\nknowledge sources and separately trained components may\nintroduce errors, might be inconsistent with the overall training\nobjective and/or may generate additional cost. Therefore, in\nan E2E approach, these would be avoided. Standard joint\ntraining of an E2E model requires using a single kind of\ntraining data, which in case of ASR would be transcribed\nspeech audio data. However, in ASR often even larger amounts\nof text-only data, as well as optional untranscribed speech\naudio are available. One of the challenges of E2E modeling\ntherefore is how to take advantage of text-only and audio-only\ndata jointly without introducing secondary (pretrained) models\nand/or training objectives [28], [29].\ne) Direct Vocabulary Modeling: Avoiding pronunciation\nlexica and corresponding subword units leave E2E recognition\nvocabularies to be derived from whole word or character\nrepresentations. Whole word models [30], according to Zipf\u2019s\nlaw [31], would require unrealistically high amounts of tran\u0002scribed training data for large vocabularies, which might not\nbe attainable for many tasks. On the other hand, methods\nto generate subword vocabularies based on characters, like\nthe currently popular byte pair encoding (BPE) approach\n[32], might be seen as secondary approaches outside the E2E\nobjective, even more so if acoustic data is considered for\nsubword derivation [33], [34], [35], [36].\nf) Generic Modeling: Finally, E2E modeling also re\u0002quires genericity of the underlying modeling: task-specific\nconstraints are learned completely from data, in contrast to\ntask-specific knowledge which influences the modeling of\nthe system architecture in the first place. For example, the\nmonotonicity constraint in ASR may be learned completely\nfrom data in an end-to-end fashion (e.g., in attention-based\napproaches [16]), or it may directly be implemented, as in\nclassical HMM structures. However, model constraints may\nbe considered by way of regularization in E2E ASR model\ntraining, and can thus provide an alternative way to introduce\ntask-specific knowledge.\ng) Single-Pass Search: In terms of the recognition/search\nproblem, the E2E property can be interpreted as integrating all\ncomponents (models, knowledge sources) of an ASR system\nbefore coming to a decision. This is in line with Bayes\u2019\ndecision rule, which exactly requires a single global decision\nintegrating all available knowledge sources, which is supported\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nby both classical ASR models as well as E2E models. On\nthe other hand, multipass search is not only exploited by\nclassical ASR models, but also by E2E ASR models, the\nmost prominent case here being (external) language model\nrescoring.\nAll in all, we need to conclude that a) \u201cE2E\u201d does not\nprovide a clear distinction between classical and novel, so\u0002called E2E models, and b) the E2E property often is weakened\nin practice, leaving the term as a more general, idealized\nperspective on ASR modeling.\nIII. A TAXONOMY OF E2E MODELS IN ASR\nBefore we derive a taxonomy of E2E ASR modeling\napproaches, we first introduce our notation. We denote the\ninput speech utterance as X, which we assume has been pa\u0002rameterized into D-dimensional acoustic frames (e.g., log-mel\nfeatures) of length T\n\u2032\n: X = (x1, \u00b7 \u00b7 \u00b7 , xT\u2032 ), where xt \u2208 R\nD.\nWe denote the corresponding word sequences as C, which can\nbe decomposed into a suitable sequence of labels of length L:\nC = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is\nagnostic to the specific representation used for decomposing\nthe word sequence into labels; popular choices include char\u0002acters, words, or sub-word sequences (e.g., BPE [32], word\u0002pieces [37]).\nASR may be viewed as a sequence classification problem\nwhich maps a variable length input, X, into an output,\nC, of unknown length. Following Bayes\u2019 decision rule, any\nstatistical approach to ASR must determine how to model the\nword sequence posterior probability, P(C|X). Thus, a natural\ntaxonomy of E2E ASR modeling can be based on the various\nstrategies for modeling this word sequence posterior: i.e., how\nthe alignment problem between input and output sequence is\nhandled; and, how sequence modeling is decomposed to the\nlevel of individual input vectors xt\n\u2032 and/or output labels cl\n.\nWe find that it is useful to distinguish implicit and explicit\nmodeling approaches, based on the modeling of the sequence\u0002to-sequence alignment:\na) Explicit Alignment Modeling: does not necessarily\nrefer to the determination of a single unique alignment, but\ninstead introduces an explicit alignment modeled as a latent\nvariable, A:\nP(C|X) = X\nA\nP(C, A|X)\nb) Implicit Alignment Modeling: does not introduce a\nlatent alignment variable, but models the label sequence pos\u0002terior P(C|X) directly.\nExplicit alignment modeling approaches can mainly be\ndistinguished by their choice of latent variable; these can be\nencoded in terms of valid emission paths in corresponding\nfinite state automata (FSA) [38] which relate the input and\noutput sequences \u2013 the approach taken in our article. Typically,\nlatent variables in explicit alignment modeling in transducer\nE2E models introduce extensions to the output label set\nwith different forms of continuation labels (including, but not\nlimited to so-called blank labels).3\n3 For example, these extensions may also include explicit duration variables,\nleading to segmental models [39]. Such models can be rewritten into equiv\u0002alent transducer models [40], and vice-versa.\nA. Encoder and Decoder Modules\nIrrespective of the alignment modeling approach, following\nthe notation introduced in [41], it is useful to view all E2E\nASR models as being composed of an encoder module and\na decoder module. The encoder module, denoted H(X),\nmaps an input acoustic frame sequence, X, of length T\n\u2032\ninto a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of\nlength T (typically T \u2264 T\n\u2032\n). Note that the encoder output is\nindependent of the hypothesized label sequence. The decoder\nmodule models the label sequence posterior on top of the\nencoder output:\nP(C|X) = P\nC\nH(X)\n\u0001\nThus, we may distinguish different approaches based upon\nhow the output label sequence distribution (including potential\nlatent variables resulting from the alignment modeling) are de\u0002composed into individual label (and alignment) contributions;\nthese may occur per output label position, per encoder frame\nposition, or combinations thereof:\nP\nC[, A]\nH(X)\n\u0001\n=\nY\nL\ni=1\nP\nci[, ai]\nc\ni\u22121\n1\n[, ai\u22121\n1\n], vi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X))\u0001\nwhere the notation mi\u22121\n1\ncorresponds to the sequence\nof i \u2212 1 previous instances of the variables m; and,\nvi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X)) denotes a context-vector that provides\nthe connection between encoder output, H(X), and the la\u0002bel output position, i. In general the context vector may\ndepend on the label context (and possibly the latent vari\u0002able context, for explicit alignment modeling approaches).\nApart from the underlying alignment model and corresponding\noutput label decomposition, decoder modules differ in terms\nof the assumptions on their label context c\ni\u22121\n1\n(and their\nlatent variable context a\ni\u22121\n1\n), which correspond to different\nconditional independence assumptions, and by their access to\nthe encoder output. For example, the local posterior may only\ndepend on a single encoder frame output (i.e., with the context\nvector being reduced to a single encoder frame\u2019s output):\nvi\nc\ni\u22121\n1\n, H(X)\n\u0001\n= hti(X). As we shall see in detail in the\nfollowing sections, the simplest case of an encoder frame\u0002level decomposition (with L = T, and ti = i) corresponds to\nCTC [13]; AED models [16] and their variants maintain the\nfull dependency of the context vector.\nFinally, different E2E models can also be distinguished by\nthe specific modeling choices that are involved in the design\nof the neural network used to implement the encoder and the\ndecoder. These might involve feed-forward neural networks,\nconvolutional neural networks, recurrent neural networks (ei\u0002ther uni-directional or bi-directional) [42], attention [43],\nand various combinations thereof (e.g., transformers [44] or\nconformers [45]). These modeling choices and corresponding\ntraining methods can be applied across E2E ASR models and\ntherefore do not enter the taxonomy of E2E ASR models\ndiscussed here. However, specific choices will be discussed as\npart of the exemplary E2E ASR models presented in Sec. VIII\nand Sec. IX and.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nB. Explicit Alignment Modeling Approaches\nEarly E2E modeling approaches modeled alignments explic\u0002itly through a latent variable, which is marginalized out (pos\u0002sibly, approximately) during training and inference. Examples\nof this family of approaches include connectionist temporal\nclassification (CTC) [13], the recurrent neural network trans\u0002ducer (RNN-T) [14], the recurrent neural aligner (RNA) [46],\nand the hybrid auto-regressive transducer [47] (HAT). As\nwill be discussed in subsequent sections, the latter modeling\napproaches in this family represent increasingly sophisticated\nmodeling of alignments, with fewer independence assumptions\nand are thus increasingly powerful. A common feature of all\nexplicit alignment models discussed in this section is that\nthey introduce an additional blank symbol, denoted \u27e8b\u27e9, and\ndefine an output probability distribution over symbols in the\nset Cb = C \u222a {\u27e8b\u27e9}. The interpretation of the \u27e8b\u27e9 symbol\nvaries slightly between each of these models, as we discuss in\ngreater details below. For now, it suffices to say that given\na specific training example, (X, C), each of these models\ndefines a set of valid alignments, denoted by A(T ,C), and\ndefine the conditional distribution P(C|X) by marginalizing\nover all valid alignment sequences:\nP(C|X) = X\nA\nP(C|A, H(X))P(A|H(X))\n=\nX\nA\u2208A(T =|H(X)|,C)\nP(A|H(X)) (1)\nwhere, by definition P(C|A, H(X)) = 1 if and only if A \u2208\nA(T ,C) and 0 otherwise.4 We discuss the specific formulations\nof each of these models in the subsequent sections.\n1) Connectionist Temporal Classification (CTC): Connec\u0002tionist Temporal Classification (CTC) was proposed by Graves\net al. [13] as a technique for mapping a sequence of input\ntokens to a corresponding sequence of output tokens. CTC ex\u0002plicitly models alignments between the encoder output, H(X),\nand the label sequence, C, by introducing a special \u201cblank\u201d la\u0002bel, denoted by \u27e8b\u27e9: Cb = C \u222a {\u27e8b\u27e9}. An alignment, A \u2208 C\u2217\nb\n, is\nthus a sequence of labels in C or \u27e8b\u27e9.\n5 Given a specific training\nexample, (X, C), we denote the set of all valid alignments,\nACTC\n(X,C) = {A = (a1, a2, . . . , aT )}, such that each at \u2208 Cb\nwith the additional constraint that A is identical to C after first\ncollapsing consecutive identical labels, and then removing all\nblank symbols. For example, if T = 10, and C = (s, e, e),\nthen A = (s,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ACTC\n(X,C)\n, as\nillustrated in Figure 1. As can be seen in this example, repeated\nlabels in the output can be represented by intervening blanks.\nFollowing Eq. (1), CTC defines the posterior probability\nof the label sequence C conditioned on the input, X, by\n4 This is equivalent to the assumption that the mapping from an alignment\nA to a label sequence C is unique, by definition. 5 S\n\u2217 denotes a Kleene\nclosure: the set of all possible sequences composed of tokens in the set S.\nTime\ns\ne\ne\n \ns\ne\ne\ns\ne\ne\ne\nFig. 1. Example alignment sequence for a CTC model with the target\nsequence C = (s, e, e) (right), alongside a (non-deterministic) finite state\nautomaton (FSA) [38] (left) representing the set of all valid alignment paths.\nEncoder H(X)\nSoftmax\nFig. 2. A representation of the CTC model consisting of an encoder which\nmaps the input speech into a higher-level representation, and a softmax layer\nwhich predicts frame-level probabilities over the set of output labels and blank.\nmarginalizing over all possible CTC alignments as:\nPCTC(C|X) = X\nA\u2208ACTC\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, \u00b7 \u00b7 \u00b7 , a1, H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|ht) (2)\nCritically, as can be seen in Eq. (2), CTC makes a strong\nindependence assumption that the model\u2019s output at time t is\nconditionally independent of the outputs at other timesteps,\ngiven the local encoder output at time t.\nThus, a CTC model consists of a neural network that\nmodels the distribution P(at|X), at each step as shown in\nFigure 2. The encoder is connected to a softmax layer with\n|Cb| targets representing the individual probabilities in Eq. (2):\nP(at = c|X) = P(at = c|H(X)), which comprises the\ndecoder module for CTC. Thus, at each step, t, the model\nconsumes a single encoded frame ht and outputs a distribution\nover the labels; in other words, the model \u201coutputs\u201d a single\nlabel either blank, \u27e8b\u27e9, or one of the targets in C.\n2) Recurrent Neural Network Transducer (RNN-T): The\nRecurrent Neural Network Transducer (RNN-T) [14], [48] was\nproposed by Graves as an improvement over the basic CTC\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 3. An RNN-T Model [14], [48] consists of an encoder which transforms\nthe input speech frames into a high-level representation, and a prediction\u0002network which models the sequence of non-blank labels that have been\noutput previously. The prediction network output, pit\n, represents the output\nafter producing the previous non-blank label sequence c1, . . . , cit\n. The\njoint network produces a probability distribution over the output symbols\n(augmented with blank) given the prediction network state and a specific\nencoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 4. Example alignment sequence (right) for an RNN-T model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs. The FSA (left) represents the set of all valid RNN-T\nalignment paths.\nmodel [13], by removing some of the conditional indepen\u0002dence assumptions that we discussed previously. The RNN\u0002T model, which is depicted in Figure 3, is best understood\nby contrasting it against the CTC model. As with CTC, the\nRNN-T model augments the output symbols with the blank\nsymbol, and thus defines a distribution over label sequences\nin Cb. Similarly, as with CTC, the model consists of an encoder\nwhich processes the input acoustic frames X to generate the\nencoded representation H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ).\nUnlike CTC, however, the blank symbol in RNN-T has a\nslightly different interpretation; for each input encoder frame,\nht, the RNN-T model outputs a sequence of zero or more\nsymbols in C which are terminated by a single blank symbol.\nThus, we may define the set of all valid alignment se\u0002quences in RNN-T as: ARNNT\n(X,C) = {A = (a1, a2, \u00b7 \u00b7 \u00b7 , aT +L)},\nthe set of all sequences of T + L symbols in C\n\u2217\nb\n, which\nare identical to C after removing all blanks. Finally, for a\ngiven output position \u03c4 , let i\u03c4 denote the number of non\u0002blank labels in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121). Thus, the\nnumber of blanks in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121) is\n\u03c4 \u2212 i\u03c4 \u2212 1. For example, if T = 7, and C = (s, e, e),\nthen A = (\u27e8b\u27e9, s,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ARNNT\n(X,C)\n.\nNote that, unlike the CTC model, repeated labels in the output\nrequire no special treatment as illustrated in Figure 4, where,\ni1 = i2 = 0;i3 = i4 = 1;i10 = 3; etc.\nWe may then define the posterior probability P(C|X) as\nbefore:\nPRNNT(C|X) = X\nA\u2208ARNNT\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |a\u03c4\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |ci\u03c4\n, ci\u03c4 \u22121, . . . , c0, h\u03c4\u2212i\u03c4\n)\n(3)\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |pi\u03c4, h\u03c4\u2212i\u03c4)\nwhere, P = (p1, \u00b7 \u00b7 \u00b7 , pL) represents the output of the predic\u0002tion network depicted in Figure 3 which summarizes the se\u0002quence of previously predicted non-blank labels, implemented\nas another neural network: pj = NN(\u00b7|c0, . . . , cj\u22121), where\nc0 is a special start-of-sentence label, \u27e8sos\u27e9. Thus, as can be\nseen in Eq. (2), RNN-T reduces some of the independence\nassumptions in CTC since the output at time t is conditionally\ndependent on the sequence of previous non-blank predictions,\nbut is independent of the specific choice of alignment (i.e.,\nthe choice of the frames at which the non-blank tokens were\nemitted).\nOur presentation of RNN-T alignments considers the\n\u201ccanonical\u201d case. In principle, however, the model can encode\nthe same set of conditional independence assumptions in\nRNN-T (i.e., the model structure), while considering alter\u0002native alignment structures as in the work of [49]. In their\nwork, Moritz et al., represent valid frame-level alignments as\nan arbitrary graph. This formulation, for example, allows for\nthe use of \u201cCTC-like\u201d alignments in the RNN-T model (i.e.,\noutputting a single label \u2013 blank, or non-blank \u2013 at each frame)\nwhile conditioning on the set of previous non-blank symbols\nas in the RNN-T model.\n3) Recurrent Neural Aligner (RNA): The recurrent neural\naligner (RNA) was proposed by Sak et al. [46]. The RNA\nmodel generalizes the RNN-T model by removing one of its\nconditional independence assumptions. The model, depicted\nin Figure 5, is best understood by considering how it differs\nfrom the RNN-T model. As with CTC and RNN-T, the RNA\nmodel defines a probability distribution over blank augmented\nlabels in the set Cb, where \u27e8b\u27e9 has the same semantics\nas in the CTC model: at each frame the model can only\noutput a single label \u2013 either blank, or non-blank \u2013 before\nadvancing to the next frame; unlike CTC (but as in RNN\u0002T) the model only outputs a single instance of each non\u0002blank label. More specifically, the set of valid alignments,\nARNA\n(X,C) = (a1, \u00b7 \u00b7 \u00b7 , aT ), in the RNA model consist of length T\nsequences in C\n\u2217\nb with exactly T \u2212L blank symbols, and which\nare identical to C after removing all blanks. Thus, the blank\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 5. An RNA Model [46] resembles the RNN-T model [14], [48] in\nterms of the model structure. However, this model is only permitted to output\na single label \u2013 either blank, or non-blank \u2013 in a single frame. Unlike\nRNN-T, the prediction network state in the RNA model, qt\u22121, depends on\nthe entire alignment sequence at\u22121, . . . , a1. The joint network produces a\nprobability distribution over the output symbols (augmented with blank) given\nthe prediction network state and a specific encoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 6. Example alignment sequence (right) for an RNA model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs; diagonal transitions correspond to outputting a non-blank\nsymbol. The FSA (left) represents the set of valid alignments for the RNA\nmodel. Although the FSA is identical to the corresponding FSA for RNN-T\nin Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases.\nsymbol has a different interpretation in RNA and the RNN\u0002T models: in RNN-T, outputting a blank symbol advances\nthe model to the next frame; in RNA, however, the model\nadvances to the next frame after outputting a single blank or\nnon-blank label. Restricting the model to output a single non\u0002blank label at each frame improves computational efficiency\nand simplifies the decoding process, by limiting the number\nof model expansions at each frame (in constrast to RNN-T\ndecoding). For example, if T = 8, and C = (s, e, e), then\nA = (\u27e8b\u27e9, s,\u27e8b\u27e9, e,\u27e8b\u27e9,\u27e8b\u27e9, e,\u27e8b\u27e9) \u2208 ARNA\n(X,C)\nas illustrated\nin Figure 6.\nThe RNA posterior probability, P(C|X), is defined as:\nPRNA(C|X) = X\nA\u2208ARNA\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|qt\u22121, ht) (4)\nwhere, as before it denotes the number of non-blank sym\u0002bols in the partial alignment sequence (a1, . . . , at\u22121), and\nqt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu\u0002ral network which summarizes the entire partial alignment\nsequence, where NN(\u00b7) represents a suitable neural network\n(an LSTM in [46]). Thus, RNA removes the one remaining\nconditional independence assumption of the RNN-T model,\nby conditioning on the sequence of previous non-blank labels\nas well as the alignment that generated them. However, this\ncomes at a cost: the exact computation of the log-likelihood in\nEq. (3) (and corresponding gradients) is intractable. Instead,\nRNA makes two simplifying assumption to ensure tractable\ntraining: by assuming that the model can only output a single\nlabel at each frame; and utilizing a straight-through estimator\nfor the alignment [50]. The latter constraint \u2013 allowing only a\nsingle label (blank or non-blank) at each frame \u2013 has also been\nexplored in the context of the monotonic RNN-T model [51].\nFinally, we note that the work in [52] further generalizes\nthe RNA model by employing two RNNs when defining the\nstate: a slow RNN (which corresponds to the sequence of\npreviously predicted non-blank labels), and a fast RNN (which\nalso conditions on the frames at which the non-blank labels\nwere output).\nC. Implicit Alignment Modeling Approaches\nOne of the main benefits of the explicit alignment ap\u0002proaches such as CTC, RNN-T, or RNA is that they result in\nASR models that are easily amenable to frame-synchronous\ndecoding6In this section, we discuss the attention-based\nencoder-decoder (AED) models (also known as, listen-attend\u0002and-spell (LAS)) [15], [16], [53], which employs the attention\nmechanism [43] to implicitly identify and model the portions\nof the input acoustics which are relevant to each output\nunit. These models were first popularized in the context of\nmachine translation [54]. Unlike explicit alignment modeling\napproaches, attention-based encoder-decoder models use an\nattention mechanism [43] to learn a correspondence between\nthe entire acoustic sequence and the individual labels. Such\nmodels support label-synchronous decoding, meaning that\nduring inference, each hypothesis in the beam is expanded\nby 1 label.\nIn the explicit alignment approaches presented in Sec\u0002tion III-B, during inference, the model continues to output\nsymbols until it has processed the final frame at which point\nthe decoding process is complete; similarly, during training,\nthe forward-backward algorithm aligns over all possible align\u0002ment sequences. Since an AED model processes the entire\nacoustic sequence at once, the model needs a mechanism\nby which it can indicate that it is done emitting all output\nsymbols. This is achieved by augmenting the set of outputs\nwith an end-of-sentence symbol, \u27e8eos\u27e9, so that the output\nvocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus,\nthe AED model, depicted in Figure 7, consists of an en\u0002coder network \u2013 which encodes the input acoustic frame\n6 By frame-synchronous decoding, we refer to the ability of the model to\nproduce output label for each input frame of speech. Models such as CTC,\nRNN-T, or RNA, support frame-synchronous decoding.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nSoftmax\nDecoder\nAttention\nEncoder H(X)\nFig. 7. An attention-based encoder decoder (AED) model [15], [16], [53].\nThe output distribution is conditioned on the decoder state, si (which\nsummarizes the previously decoded symbols), and the context vector, vi\n(which summarizes the encoder output based on the decoder state). In the\nseminal work of Chan et al., [16], for example, this is accomplished by\nconcatenating the two vectors, as denoted by the L symbol in the figure.\nsequence, X = (x1, . . . , xT\u2032 ), into a higher-level representa\u0002tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder\nwhich defines the probability distribution over the set of output\nsymbols, Ceos. Thus, given a paired training example, (X, C),\nwe denote by Ce = (c1, . . . , cL,\u27e8eos\u27e9), the ground-truth\nsymbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9\nsymbol. AED models compute the conditional probability of\nthe output sequence augmented with the \u27e8eos\u27e9 symbol as:\nP(Ce|X) = P(Ce|H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, vi)\n=\nL\nY\n+1\ni=1\nP(ci|si, vi) (5)\nwhere, vi corresponds to a context vector, which summarizes\nthe relevant portions of the encoder output, H(X), given\nthe sequence of previous predictions ci\u22121, . . . , c0; and, si\ncorresponds to the corresponding decoder state after outputting\nthe sequence of previous symbols, which is produced by\nupdating the decoder state based on the previous context vector\nand output label:\nsi = Decoder(vi\u22121, si\u22121, ci\u22121)\nThe symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol\nwhich serves as the first input to the attention-based decoder\nbefore it has produced any outputs. As can be seen in Eq. (5),\nan important benefit of AED models over models such as\nCTC or RNN-T is that they do not make any independence\nassumptions between model outputs and the input acoustics,\nTime\ns\ne\ne\nFig. 8. Unlike models such as RNN-T or CTC, AED models do not have\nexplicit alignment. However, it is possible to interpret the attention weights\n\u03b1t,i for a particular output symbol ci as an alignment weight which is\nrepresented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this\nrepresentation, the size of the circle and the darkness level are proportional\nto the corresponding attention weights; thus the total probability mass is the\nsame for each row. As illustrated above, the first few frames correspond to\nthe first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019:\nc3 = e.\nand are thus more general than the implicit alignment models,\nwhile being considerably easier to train and implement since\nwe do not have to explicitly marginalize over all possible\nalignment sequences. However, this comes at a cost: previ\u0002ously generated context vectors (which are analogous to the\ndecoded partial alignment in explicit alignment models) are\nnot revised as the decoding proceeds. Stated another way,\nwhile the encoder processing H(X) might be bi-directional,\nthe decoding process in AED models reveals a left-right\nasymmetry [55].\n1) Computing the Context Vector in AED Models: As\nwe mentioned before, the context vector, vi, is computed\nby employing the attention mechanism [43]. The central\nidea behind these approaches is to define a state vector si\nwhich corresponds to the state of the model after outputting\nc1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then\ndefines a score between the model state after outputting\ni \u2212 1 previous symbols, and each of the encoded frames in\nH(X). These scores can then be normalized using the softmax\nfunction to define a set of weights corresponding to each ht\nas:\n\u03b1t,i =\nexp {atten(ht, si)}\nPT\nt\n\u2032=1 exp {atten(ht\n\u2032 , si)}\nIntuitively, the weight \u03b1t,i represents the relevance of a\nparticular encoded frame ht when outputting the next symbol\nci, after the model has already output the symbols c1, . . . , ci\u22121,\nas illustrated in Figure 8. The context vector summarizes the\nencoder output based on the computed attention weights:\nvi =\nX\nt\n\u03b1t,iht\nA number of possible attention mechanisms have been\nexplored in the literature: the most common forms are called\n\u2018content-based attention\u2019, which include dot-product atten\u0002tion [16] and additive attention [43]. The content-based atten\u0002tion computes the attention score atten(ht, si) based on the\nrelevance between ht and si. However, the score does not\nconsider location information, i.e., it is determined by only the\ncontent, independent of the position. This can lead to incorrect\nattention weights with a large discrepancy against the previous\nsteps. Thus, location-based attention atten(si,fi,j ) has been\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nproposed [15], where fi,j is a convolutional feature vector\nextracted from \u03b1i\u22121, the attention weights in the previous step.\nThe hybrid attention, i.e., a combination of the content- and\nlocation-based attentions, has also been investigated in [15],\nshowing a higher accuracy than the separate ones. Besides,\nother location-based methods use a Gaussian (mixture) model\nestimated with sito obtain attention weights [56], [57].\nTransformer model [44] uses only content-based dot-product\nattention, but also takes location information into account\nthrough positional encoding. Apart from the specific choice\nof the attention mechanism, a common technique to improve\nperformance involves the use of multiple independent attention\nheads \u2013 v\n1\ni\n, . . . , v\nK\ni\n\u2013 which are then concatenated together\nto obtain the final context vector vi =\n\u0002\nv\n1\ni\n; . . . ; v\nK\ni\n\u0003\n, in\nthe so-called multi-head attention approach [44], or indeed\nby stacking together multiple attention-based layers in the\ntransformer decoder presented by Vaswani et al. [44].\nD. From Implicit to Explicit Alignment Modeling\nAED models, which make no conditional independence\nassumptions, are extremely powerful, often outperforming\nexplicit alignment E2E approaches such as CTC, or RNN\u0002T [41]. However, these models also have some significant\ndisadvantages, most notably that the models are typically non\u0002streaming: i.e., the models must process all acoustic frames\nbefore they can generate any output hypotheses. A somewhat\nrelated issue is that the models are extremely sensitive to\nthe length of the acoustic sequences, which requires special\nprocessing to be able to decode long-form audio [58]. There\nis a body of work that lies in between these two extremes:\nmodels such as the neural transducer [59], or those based on\nmonotonic alignments [60] and its variants (e.g., monotonic\nchunkwise alignments (MoChA) [61], monotonic infinite look\u0002back (MILK) [62] etc.) use an explicit alignment model, while\nalso utilizing an attention mechanism that allows the model\nto examine local acoustics in order to refine predictions. In\nother words, this corresponds to a class of streaming AED\nmodels. Generally speaking, these models are motivated by\nthe observation that speech (unlike tasks such as machine\ntranslation) exhibits a \u2018local\u2019 relationship between the encoded\nframes (assuming that the encoder is uni-directional) and\nthe output units; thus, unlike the general AED model which\ncomputes the context vector, vi, as a sum over all input\nframes ht, the various proposed models constrain this sum\nto be computed over a subset of frames to allow for streaming\ndecoding. In the context of our presentation, it is easiest to\nthink of these models as consisting of an underlying alignment\n(whether known or unknown) which can be used to perform\nstreaming inference.\nThe Neural Transducer (NT) [59] explicitly partitions the in\u0002put encoder frames into T W non-overlapping chunks of length\nW: HW\n1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; HW\nT W = [hT W +1, . . . , hT W W ],\nwhere T W =\n\u0006\nT\nW\n\u0007\n, and ht = 0 if t > T. Unlike the AED\nmodel which examines all encoded frames when computing\nthe context vector, the NT model is restricted to process\na single chunk at a time; the model only advances to the\nnext chunk when it outputs a special end-of-chunk symbol\n(analogous to \u27e8eos\u27e9 in the AED model); inference in the model\nterminates when the model has output the end-of-chunk sym\u0002bol in the final chunk HW\nT W . If the alignments of the ground\u0002truth output sequence, C, with respect to the W-length chunks\nare unknown, then it is possible to train the system by using a\nrough initial alignment where symbols are distributed equally\namong the T W chunks, followed by iterative refinement by\ncomputing the most likely output alignments given the current\nmodel parameters [59] similar to forced-alignments in HMM\u0002based systems. An alternate approach [63] consists of using a\nseparate system (e.g., a classical hybrid system) to get initial\nalignments (e.g., word-level alignments), which can be used\nto assign sub-word units to the individual chunks.\nAn alternative approach, proposed by Raffel et al. [60],\nmodifies the vanilla AED model by explicitly introducing an\nalignment module which scans the encoder frames, H(X),\nfrom left-to-right to identify whether the current frame should\nbe used to emit any outputs (modeled as a Bernoulli random\nvariable). If a frame, \u03c4 , is selected, then the model produces\nan output based on the local encoder frame, h\u03c4 . The process\nis then repeated starting from the currently selected frame,\nthus allowing multiple outputs to be generated at the same\nframe. This results in a model with hard monotonic alignments\nbetween the input speech and the output labels since the\nmodels are constrained to generate outputs in a streaming fash\u0002ion. A Monotonic Chunkwise Attention (MoChA) model [61]\nimproves upon the work of Raffel et al., by allowing the model\nto generate the next output using a context vector computed\nusing attention over a local window of frames to the left of the\nselected frame \u03c4 : h\u03c4\u2212W+1, . . . , h\u03c4 . Thus, the MoChA model\nconsists of a two-level process \u2013 identifying frames where\noutput should be produced following [60], followed by an\nAED model over frames to the left of the selected frame. A\nrefinement to the MoChA model, proposed by Arivazhagan et\nal. [62] \u2013 the monotonic infinite lookback (MILK) attention\nmodel \u2013 computes the context vector over all frames to the\nleft of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step.\nAnother two-fold approach to enable streaming operation is\npresented in [64] under the term of triggered attention, where\na CTC-network is used to trigger, i.e. control the activation\nof an AED model with a limited decoder delay. We also\ndirect interested readers to studies of various attention variants:\nMerboldt et al. [65] compare a number of local monotonic\nattention variants; Zeyer et al. [66] discuss segmental attention\nvariants; Zeyer et al. [67] study the related decoding and the\nrelevance of segment length modeling, leading to improved\ngeneralization towards long sequences. Segmental attention\nmodels are related to transducer models [68]. However, seg\u0002mental E2E ASR models are not limited to be realized based\non the attention mechanism and may not only be related to a\ndirect HMM [39], but have also been shown to be equivalent\nto neural transducer modeling [40], thus even providing a clear\nrelation between duration modeling and blank probabilities.\nRelationship to Classical ASR\nIn classical ASR models, these frame-level alignments can\nbe modeled with HMMs while using generative GMMs or\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nneural networks to model the output distribution of acous\u0002tic frames; frame-level alignments to train neural network\nacoustic models may be obtained by force-alignment from a\nbase GMM-HMM systems, but direct sequence training not\nrequiring initial alignments is also possible [69].\nE2E models introduce alternative alignment modeling ap\u0002proaches to ASR. While the attention mechanism provides\na qualitatively novel approach to map acoustic observation\nsequences to label sequences, transducer approaches [13], [14],\n[46], [70] handle the alignment problem in a way that can\nbe interpreted to be similar to HMMs with a specific model\ntopology, including marginalization over alignments [71], [72],\n[73]. CTC models can also be employed in an HMM-like fash\u0002ion during decoding [74]. Moreover, transducer approaches are\nequivalent to segmental models/direct HMM [40].\nAnother prominent feature of E2E systems besides the\nalignment property is their direct character-level modeling\navoiding phoneme-based modeling and pronunciation lexica\n[19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82],\nwith some even heading for whole-word modeling [76], [30].\nHowever, character-level modeling also is viable with classical\nhybrid HMM architectures [83] and has been shown to work\neven with standard HMM models w/o neural networks [84].\nIV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E\nMODELS\nIn this section, we describe various algorithmic changes\nto vanilla E2E models which are critical in order to obtain\nimproved performance over classical ASR systems. First, we\ndescribe various ways of combining different complementary\nE2E models to improve performance. Next, we introduce\nways to incorporate context into these models to improve\nperformance on rare proper noun entities. We then describe\nimproved encoder and decoder architectures that take better\nadvantage of the many cores on specialized architectures such\nas tensor processing units (TPUs) [85]. Finally, we discuss how\nto improve the latency of the model through an integrated E2E\nendpointer.\nA. Combinations of Models\nDifferent end-to-end models are complementary, and there\nhave been numerous attempts at combining these methods.\nFor example, Watanabe et al. [86] find that attention-based\nmodels perform poorly on long or noisy utterances, mainly\nbecause the model has too much flexibility in predicting\nalignments when presented with the entire input utterance.\nIn contrast, models such as CTC \u2013 which have left-to-right\nconstraints during decoding \u2013 perform much better in these\ncases. They propose to employ a multi-task learning strategy\nwith both CTC and attention-based losses, which provides a\n5\u201314% relative improvement in word error rate over attention\u0002based models on Wall Street Journal (WSJ) and Chime tasks.\nPang et al. [87] explore combining the benefits of RNN-T\nand AED. Specifically, RNN-T decodes utterances in a left\u0002to-right fashion, which works well for long utterances. On\nthe other hand, since AED sees the entire utterance, it often\nshows improvements for utterances where surrounding context\nis needed to predict the current word, e.g., \"one dollar\nand fifty cents\" \u2192 $1.50. To combine RNN-T and\nAED, the authors propose to produce a first-pass result with\nRNN-T, that is then rescored with AED in the second-pass.\nTo reduce computation, the authors share the encoder between\nRNN-T and AED. The authors find that RNN-T + AED\nprovides a 17\u201322% relative improvement in word error rate\nover RNN-T alone on a voice search task. Other flavors\nof streaming 1st-pass following by attention-based 2nd-pass\nrescoring, such as deliberation [88], have also been explored.\nOne of the issues with such rescoring approaches is that any\npotential improvements are limited to the lattice produced by\nthe 1st-pass system. To address this, methods which run a\n2nd-pass beam search have also been explored, particularly in\nthe context of streaming ASR \u2013 e.g. cascaded encoder [89],\nY-architecture [90] and Universal ASR [91].\nB. Incorporating Context\nContextual biasing to a specific domain, including a user\u2019s\nsong names, app names and contact names, is an impor\u0002tant component of any production-level automatic speech\nrecognition (ASR) system. Contextual biasing is particularly\nchallenging in E2E models because these models typically\nretain only a small list of candidates during beam search, and\ntend to perform poorly when recognizing words that are seen\ninfrequently during training (typically named entities), which\nis the main source of biasing phrases. There have been a few\napproaches in the literature to incorporate context.\nOne approach, known as shallow-fusion contextual bias\u0002ing [92], constructs a stand-alone weighted finite state trans\u0002ducer (FST) representing the biasing phrases. The scores from\nthe biasing FST are interpolated with the scores of the E2E\nmodel during beam search, with special care taken to ensure\nwe do not over- or under-bias phrases. An alternate approach\nproposes to inject biasing phrases into the model in an all\u0002neural fashion. For example, Pundak et al. [93] represent a\nset of biasing phrases by embedding vectors. These vectors\nare fed as additional input to an attention-based model, which\ncan then choose to attend to the phrases and hence boost the\nchances of predicting the phrases. Kim and Metze [94] propose\nto bias towards dialog context. In addition, Bruguier et al. [95]\nextend [93], by leveraging phonemic pronunciations for the\nbiasing phrases when constructing phrase embeddings. Finally,\nDelcroix et al. [96] use an utterance-wise context vector like an\ni-vector computed by a pooling across frame-by-frame hidden\nstate vectors obtained from a sub network (this sub-network\nis called a sequence-summary network).\nC. Encoder and Decoder Structure\nThere have been improvements to encoder architectures\nof E2E models over time. The first end-to-end models used\nlong short-term memory recurrent neural networks (LSTMs),\nfor both the encoder and decoder. The main drawback of\nthese sequential models is that each frame depends on the\ncomputation from the previous frame, and therefore multiple\nframes cannot be batched in parallel.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nWith the improvement of hardware, specifically on-device\nEdge Tensor Processing Units (TPUs), with thousands of\ncores, architectures that can better take advantage of the\nhardware, have been explored. Such architectures include\nconvolution-based architectures, such as ContextNet [97]. The\nuse of self-attention to replace the sequential recurrence\nin LSTMs was explored in Transformers for ASR [98],\n[99], [100]. Finally, combining self-attention with convolution,\nknown as Conformer [45], or multi-layer perceptron [101],\nwas also explored. Both Transformer and Conformer have\nshown competitive performance to LSTMs on many tasks\n[102], [103].\nOn the decoder side, research for transducer models has\nshown that a large LSTM decoder can be replaced with a sim\u0002ple embedding lookup table, that attends to only a few previous\ntokens from the model [47], [104], [105], [106], [107]. This\ndemonstrates that most of the power of the E2E model is in\nthe encoder, which has been a consistent theme of both E2E\nas well as classical hybrid HMM models. However, improved\ndecoder modeling may also be effective depending on the\nspecific downstream task. Research has shown that extended\ndecoder architectures enable pre-training and adaptation of the\ndecoder using extensive text-only data, leading to accuracy\ngains [108], [109]. For example, one approach separates RNN\u0002T\u2019s prediction network into separate blank and vocabulary\nprediction (LM) components, where the LM component can\nbe trained with text data [108]. In addition, in line with the\ngrowing interest in large language models in recent years,\nresearch has also begun on solving multiple tasks, including\nspeech recognition, using only an auto-regressive, GPT-style\ndecoder [110], [111].\nD. Integrated Endpointing\nAn important characteristic of streaming speech recognition\nsystems is that they must endpoint quickly, so that the ASR\nresult can be finalized and sent to the server for the appro\u0002priate action to be performed. Endpointing is typically done\nwith an external voice-activity detector. Since endpointing is\nboth an acoustic and language model decision, recent works\nin streaming RNN-T models [112], [113] have investigated\npredicting a microphone closing token \u27e8eos\u27e9 at the end of the\nutterance \u2013 e.g., \u201cWhat\u2019s the weather \u27e8eos\u27e9\u201d. Following the\nnotation from Section III, this is done by including an \u27e8eos\u27e9\ntoken as part of the set of class labels C and encouraging\nthe model to predict this token to terminate decoding. These\nmodels have shown improved latency and WER trade-off\nby having the endpointing decision predicted as part of the\nmodel. Furthermore, [114], [115] explored using the CTC\nblank symbol for endpoint detection.\nV. TRAINING E2E MODELS\nIn general, training of E2E models follows deep learn\u0002ing schemes [116], [117], with specific consideration of the\nsequential structure and the latent alignment problem to be\nhandled in ASR. E2E ASR models may be trained end-to\u0002end, notwithstanding potential elaborate training schedules\nand extensive data augmentation. Part of the appeal of end\u0002to-end models is that they do not assume conditional in\u0002dependence between the input frames. Given a training set\nT = {(Xn, Cn)}\nN\nn=1, the training criterion L to be minimized\ncan be written as: L = \u2212\nPN\nn=1 log P(Cn|Xn) (which is\nequivalent to maximizing the total conditional log-likelihood).\nA. Alignment in Training\nE2E models such as RNN-T and CTC introduce an addi\u0002tional blank token \u27e8b\u27e9 for alignment. Therefore optimization\nimplies marginalizing across all alignments, as follows:\nLex = \u2212\nX\nN\nn=1\nX\nAn\nlog P(Cn, An|Xn)\nThis requires the forward-backward algorithm [118], [119] for\nefficient computation of the training criterion and its gradient,\nwith minor modifications for CTC, RNN-T, and RNA models,\nas well as classical (full-sum) hybrid ANN/HMMs correspond\u0002ing to the differences in alignments defined in each of these\nmodels. In comparison, AED models are based on implicit\nalignment modeling approaches, and the training criterion does\nnot have a latent variable A for explicit alignment as:\nLim = \u2212\nX\nN\nn=1\nlog P(Cn|Xn)\nWe refer the interested reader to the individual papers for\nfurther details on the training algorithms [13], [14], [15], [16],\n[46], [48], [53], [71], [120]. As shown in Section III-A, in both\nexplicit and implicit alignment cases, P(C|X) is factorized\nwith respect to input time t and output position i, respectively,\nand the factorized distribution is conditioned on the label\ncontext c\ni\u22121\n1\n, except for CTC. For example, in the AED case:\nlog P(C|X) = PL\ni=1 log P(ci\n|X, ci\u22121\n1\n). During training, we\nuse a teacher-forcing technique where the ground truth history\nis used as a label context.\nAs part of the training procedure, all E2E as well as classical\nhidden Markov models for ASR provide mechanisms to solve\nthe underlying sequence alignment problem - either explicitly\nvia corresponding latent variables, as in CTC, RNN-T or RNA,\nand also hybrid ANN/HMM, or implicitly, as in AED models.\nAlso, the distinction between speech and silence needs to be\nconsidered, which may be handled explicitly by introducing\nsilence as a latent label (hybrid ANN/HMM), or implicitly\nby not labeling silence at all, as currently is the standard in\nvirtually all E2E models.\nE2E models also may take advantage of hierarchical training\nschedules. These schedules may comprise several separate\ntraining passes and explicit, initially generated alignments that\nare kept fixed for some Viterbi-style [121], [122], [123] train\u0002ing epochs before re-enabling E2E-style full-sum training that\nmarginalizes over all possible alignments. Such an alternative\napproach is employed by Zeyer et al. [52], where an initial\nfull-sum RNN-T model is used to generate an alignment and\ncontinue with framewise cross-entropy training. This greatly\nsimplifies the training process by replacing the summation\nover all possible alignments in Eq. (4) by a single term cor\u0002responding to the alignment sequence generated. Recently, a\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nsimilar procedure has been introduced in [124] also employing\nE2E models, only. In this work, CTC is used to initialize\nthe training and to generate an initial alignment, followed by\nintermediate Viterbi-style RNN-T training and final full-sum\nfine tuning, which improved convergence compared to full\u0002sum-only training approaches.\nIt is interesting to note that in contrast to the RNN-T and\nRNA label-topologies, CTC does not require alignments with\nsingle label emissions per label position. However, training\nCTC models eventually does lead to single label emissions\nper hypothesized label. An analysis of this property of CTC\ntraining which is usually called peaky behavior can be found\nin [125] and references therein. Laptev et al. [126] even\nintroduces a CTC variant without non-blank loop transitions.\nB. Training with External Language Models\nE2E ASR models generally are normalized on sequence\nlevel. Therefore, sequence training with the maximum mutual\ninformation criterion [127] is the same as standard cross en\u0002tropy/conditional likelihood training. However, once external\nlanguage models are included in the training phase, sequence\nnormalization needs to be included explicitly, leading to MMI\nsequence discriminative training. This has been exploited as\na further approach to combine E2E models with external\nlanguage models trained on text-only data during the training\nphase itself [128], [129], [130].\nC. Minimum Word Error Rate Training\nSince the objective of speech recognition is to minimize\nword error rate (WER), there has been a growing number of\nresearch studies that incorporate this into the objective function\nby minimizing the model-based expectation of the number of\nword errors, as follows:\nLmwer =\nX\nN\nn=1\nX\nC\u2032\nn\nW(Cn, C\u2032\nn\n)P(C\n\u2032\nn\n|Xn)\nwhere W(Cn, C\u2032\nn\n) is the word error count in a hypothesis\nC\n\u2032\nn given a reference Cn, and n is an index which iterates over\nthe entire training set. These methods, known as sequence\nor discriminative training, have shown great improvements\nfor classical ASR [131], [132], [133], [134], [135], and have\nsince been explored in E2E models. Typically these losses\nare constructed by running in \u2018beam-search\u2019 mode rather than\nteacher-forcing mode, and construct a loss from the errors\nmade from the candidate hypotheses in the beam. Thus, this\ntype of training first requires training the model to optimize\nP(C|X) in order to initialize the model with a good set\nof parameters to run a beam search. However, also direct\napproaches have been introduced that avoid this separation\nto train discriminatively from scratch [69], [136].\nPapers that explore penalizing word errors include, Mini\u0002mum Word Error Rate (MWER) training [137], where the loss\nfunction is constructed such that the expected number of word\nerrors are minimized. Further work includes MWER for RNN\u0002T and self-attention-T [138], as well as MWER using prefix\nsearch instead of n-best [139]. Also, there have been studies\nthat consider MWER in terms of reinforcement learning [140],\n[141]. Optimal Completion Distillation (OCD) [81] proposes\nto minimize the total edit distance using an efficient dynamic\nprogramming algorithm. Finally, another body of research with\nsequence training introduce a separate external language model\nat training time [142], which can also be done efficiently via\napproximate lattice recombination [129] and also lattice-free\napproaches [130].\nD. Pretraining\nAll E2E models as well as classical hidden Markov models\nfor ASR provide holistic models that in principle enable train\u0002ing from scratch. However, many strategies exist to initialize\nand guide the training process to reach optimal performance\nand/or to obtain efficient convergence by applying pretrain\u0002ing and model growing [143], [144]. Supervised layer-wise\npretraining has been successfully applied for classical [5],\n[145], as well as attention-based ASR models [146], which can\nbe combined with intermediate sub-sampling schemes [147],\nand model growing [148]. Pretraining approaches utilizing\nuntranscribed audio, large-scale semi-supervised data and/or\nmultilingual data [149], [150], [151], [152], [153], [154],\n[155], [156], [157], [158], [159], [160] would deserve a\nself-contained survey and they are applicable for hybrid\nDNN/HMM and E2E approaches likewise \u2013 they will not be\nfurther discussed here.\nE. Training Schedules and Curricula\nDedicated training schedules have been developed to guide\nthe optimization process and as part of that reach proper\nalignment behavior explicitly or implicitly [52], [124], [147].\nMany approaches exist for learning rate control [161], [162]:\nNewBob [163], [164] and enhancements [162]; global ver\u0002sus parameter-wise learning rate control (exponential decay,\npower decay, etc.) [165]; learning rate warm-up [44]; warm\nrestarts/cosine annealing [166]; weight decay versus gradually\ndecreasing batch size [167]; fine-tuning [168] or population\u0002based training [169]; etc. For a survey of meta learning\ncf. [170].\nSequence learning approaches also consider curriculum\nlearning [171], [172], e.g., by considering short sequences\nfirst [173], [174]; interim increase of sub-sampling [147]\ninitially more sub-sampling; or, for multi-speaker ASR training\nsort mixed speech by SNR and start with speakers of balanced\nenergy and mixed gender [175].\nF. Optimization and Regularization\nOptimization usually is based on stochastic gradient descent\n[176], with momentum [177], [178], and a number of corre\u0002sponding adaptive approaches, most prominently Adam [179]\nand variants thereof [145], [179], [180].\nInvesting more training epochs seems to provide improve\u0002ments [52, Table 8], and also averaging over epochs has been\nreported to help [102]. For a discussion of the double descent\neffect and its relation to the amount of training data, label\nnoise and early stopping cf. [181].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nRegularization strongly contributes to training performance:\ne.g., L2 and weight decay [182], [166]; weight noise [183];\nadaptive mean L1/L2 [184]; gradient noise [185]; dropout\n[186], [187], [188], layer dropout [189], [190], [191]; dropcon\u0002nect [192]; zoneout [193]; smoothing of attention scores [15];\nlabel smoothing [194]; scheduled sampling [195]; auxiliary\nloss [194], [196]; variable backpropagation through time [197],\n[198]; mixup [199]; increased frame rate [180]; or, batch\nnormalization [200].\nG. Data Augmentation\nTraining of E2E ASR models also benefit from data aug\u0002mentation methods, which might also be viewed as regu\u0002larization methods. However, their diversity and impact on\nperformance justifies a separate overview.\nMost data augmentation methods perform data perturbation\nby exploiting certain dimensions of speech signal variation:\nspeed perturbation [201], [202], vocal tract length perturbation\n[201], [203], frequency axis distortion [201], sequence noise\ninjection [204], SpecAugment [205], or semantic mask [206].\nAlso, text-only data may be used to generate data using text\u0002to-speech (TTS) on feature [207] or signal level [208]. In a\ncomparison of the effect of TTS-based data augmentation on\ndifferent E2E ASR architectures in [208], AED seemed to be\nthe only architecture that appeared to benefit significantly from\nthe TTS data.\nIn a recent study [174] and corresponding follow-up work\n[180], many of the regularization and data augmentation\nmethods listed here have been exploited jointly leading to\nstate-of-the-art performance on the Switchboard task for a\nsingle-headed AED model.\nRelationship to Classical ASR\nE2E systems attempt to define ASR models that integrate\nall knowledge sources into a single global joint model that\ndoes not utilize secondary knowledge sources and avoids the\nclassical separation into acoustic and language models. These\nglobal joint models are completely trained from scratch using\na single global training criterion based on a single kind of\n(transcribed) training data and thus require less ASR domain\u0002specific knowledge provided sufficient amounts of training\ndata are available.\nWhile standard hybrid ANN/HMM training for ASR using\nframe-wise cross entropy already is discriminative, it is not\nyet sequence discriminative, requires prior alignments and\nalso lacks consideration of an (external) language model\nduring training. However, these potential shortcomings may\nbe remedied by using sequence discriminative training criteria\n[127] and lattice-free training approaches [69].\nIn contrast to strict E2E systems, the classical ASR ar\u0002chitecture includes the use of secondary knowledge sources\nbeyond the primary training data, i.e. (transcribed) speech\naudio for acoustic model training, and textual data for language\nmodel training. Most prominently, this includes the use of a\npronunciation lexicon and the definition of a phoneme set.\nSecondary resources like pronunciation lexica may be helpful\nin low-resource scenarios. However, their generation often is\ncostly and may even introduce errors, like pronunciations from\na lexicon not reflecting the actual pronunciations observed.\nTherefore, for large enough training resources, secondary\nknowledge sources might become obsolete [209], or even\nharmful, in case of erroneous information introduced [210],\n[211].\nClassical ASR models usually are trained successively, with\nknowledge derived from models trained earlier injected into\nlater training stages, e.g. in the form of HMM state alignments.\nHowever, such approaches from classical ASR might also\nbe interpreted as specific training schedules. Initializing deep\nlearning models using HMM alignments obtained from acous\u0002tic models based on mixtures of Gaussians may be interpreted\nin this way, with the Gaussian mixtures serving as an initial\nshallow model. In classical ASR, also approaches training\ndeep neural networks from scratch while avoiding interme\u0002diate training of Gaussians has been proposed [212], [213],\n[214], also in combination with character-level modeling [83].\nAnother step towards more integrated training of classical\nsystems has been to apply discriminative training criteria\navoiding intermediate (usually lattice-based) representations of\ncompeting word sequences [215], [69], [216], [217], [136].\nThe training of classical ASR systems usually applies sec\u0002ondary objectives to solve subtasks like phonetic clustering.\nThe classification and regression trees (CART) approach is\nused to cluster triphone HMM states [27], [218]. More re\u0002cent approaches proposed clustering within a neural network\nmodeling framework, while still retaining secondary clustering\nobjectives [219], [213]. However, also in E2E approaches\nsecondary objectives are used, most prominently for subword\ngeneration, e.g. via byte-pair encoding [32]. Also, available\npronunciation lexica can be utilized indirectly for assisting\nsubword generation for E2E systems [35], [36], which are\nshown to outperform byte-pair encoding. Within classical ASR\nsystems, phonetic clustering also can be avoided completely\nby modeling phonemes in context directly [220].\nIt is interesting to observe that specifically attention-based\nencoder-decoder models require larger numbers of training\nepochs to reach high performance, e.g. for a comparison\nof systems trained on Switchboard 300h cf. Table 5 in\n[221]. Also, attention-based encoder-decoder models have\nbeen shown to suffer from low training resources [222], [223],\nwhich can be improved by a number of approaches, including\nregularization techniques [174] as well as data augmentation\nusing SpecAugment [224] and text-to-speech (TTS) [29].\nSpecAugment also is shown to improve classical hybrid HMM\nmodels [225]. TTS on the other hand considerably improved\nattention-based encoder-decoder models trained on limited\nresources, but did not reach the performance of other E2E\napproaches or hybrid HMM models, which in turn were not\nconsiderably improved by TTS [208]. Multilingual approaches\nalso help improve ASR development for low resource tasks,\nagain both for classical [226], as well as for E2E systems\n[227], [228].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nVI. DECODING E2E MODELS\nThis section describes several decoding algorithms for end\u0002to-end speech recognition. The basic decoding algorithm of\nend-to-end ASR tries to estimate the most likely sequence C\u02c6\namong all possible sequences, as follows:\nC\u02c6 = arg max\nC\u2208U\u2217\nP(C|X)\nThe following section describes how to obtain the recognition\nresult C\u02c6.\nA. Greedy Search\nThe Greedy search algorithm is mainly used in CTC, which\nignores the dependency of the output labels as follows:\nA\u02c6 =\nY\nT\nt=1\n\u0012\narg max\nat\nP(at|X)\n\u0013\nwhere at is an alignment token introduced in Section III-B1.\nThe original character sequence is obtained by converting\nalignment token sequence A\u02c6 to the corresponding token se\u0002quence C\u02c6. The argmax operation can be performed in parallel\nover input frame t, yielding fast decoding [13], [229], although\nthe lack of the output dependency causes relatively poor\nperformance than the attention and RNN-T based methods in\ngeneral.\nCTC\u2019s fast decoding is further boosted with transformer\n[44], [98], [102] and its variants [45], [103] since their entire\ncomputation across the frames is parallelized [190], [230].\nFor example, the non-autoregressive models, including Im\u0002puter [231], Mask-CTC [230], Insertion-based modeling [232],\nContinuous integrate-and-fire (CIF) [233] and other variants\n[234], [235] have been actively studied as an alternative non\u0002autoregressive model to CTC. [235] shows that CTC greedy\nsearch and its variants achieve 0.06 real-time factor (RTF)7\nby using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz. The\npaper also shows that the degradation of the non-autoregressive\nmodels from the attention/RNN-T methods with beam search\nis not extremely large (19.7% with self-conditioned CTC [234]\nversus 18.5 and 18.9% with AED and RNN-T, respectively).\nThe greedy search algorithm is also used as approximate\ndecoding for both implicit and explicit alignment modeling\napproaches, including AED, RNA, CTC, and RNN-T, as\nfollows:\nc\u02c6i = arg max\nci\nP(ci|C\u02c6\n1:i\u22121, X) for i = 1, . . . , N\na\u02c6t = arg max\nat\nP(at|A\u02c6\n1:t\u22121, X) for t = 1, . . . , T\nThe greedy search algorithm does not consider alternate\nhypotheses in a sequence compared with the beam search\nalgorithm described below. However, it is known that the\ndegradation of the greedy search algorithm is not very large\n[16], [46], especially when the model is well trained in\nmatched conditions8.\n7 The ratio of the actual decoding time to the duration of the input speech.\n8 On the other hand, in the AED models, increasing the search space does\nnot consistently improve the speech recognition performance [77], [236] \u2013 a\nfact also observed in neural machine translation [237].\nB. Beam Search\nThe beam search algorithm is introduced to approximately\nconsider a subset of possible hypotheses C\u02dc among all possible\nhypotheses U\n\u2217 during decoding, i.e., C \u2282 U \u02dc \u2217\n. A predicted\noutput sequence C\u02c6 is selected among a hypothesis subset C\u02dc\ninstead of all possible hypotheses U\n\u2217\n, i.e.,\nC\u02c6 = arg max\nC\u2208C\u02dc\nP(C|X) (6)\nThe beam search algorithm is to find a set of possible hy\u0002potheses C\u02dc, which can include promising hypotheses efficiently\nby avoiding the combinatorial explosion encountered with all\npossible hypotheses U\n\u2217\n.\nThere are two major beam search categories: 1) frame\nsynchronous beam search and 2) label synchronous beam\nsearch. The major difference between them is whether it\nperforms hypothesis pruning for every input frame t or every\noutput token i. The following sections describe these two\nalgorithms in more detail.\nC. Label Synchronous Beam Search\nSuppose we have a set of partial hypotheses up to (i \u2212 1)th\ntoken C\u02dc\n1:i\u22121. A set of all possible partial hypotheses up to ith\ntoken C1:iis expanded from C\u02dc\n1:i\u22121 as follows:\nC1:i = {(C\u02dc\n1:i\u22121, ci = c)}c\u2208U (7)\nThe number of hypotheses |C1:i| would be |C\u02dc\n1:i\u22121| \u00d7 |U|, at\nmost. The beam search algorithm prunes the low probability\nscore hypotheses from C1:i and only keeps a certain number\n(beam size \u2206) of hypotheses at i among C1:i. This pruning\nstep is represented as follows:\nC\u02dc\n1:i = NBESTC1:i\u2208C1:i P(C1:i\n|X), where |C\u02dc\n1:i\n| = \u2206 (8)\nNote that NBEST(\u00b7) is an operation to extract top \u2206 hypothe\u0002ses in terms of the probability score P(C1:i\n|X) computed from\nan end-to-end neural network, or a fusion of multiple scores\ndescribed in Section VII-B.\nIn the label synchronous beam search, the length of the out\u0002put sequence (N) is unknown. Therefore, during this pruning\nprocess, we also add the hypothesis that reaches the end of an\nutterance (i.e., predict the end of sentence symbol \u27e8eos\u27e9) to a\nset of hypotheses C\u02dc in Eq. (6) as a promising hypothesis.\nThe label synchronous beam search does not explicitly\ndepend on the alignment information; thus, it is often used\nin implicit alignment modeling approaches, including AED.\nDue to this nature, sequence hypotheses of the same length\nmight cover a completely different number of encoder frames,\nunlike the frame synchronous beam search, as pointed out by\n[40]. As a result, we observe that the scores of very short and\nlong segment hypotheses often become the same range, and\nthe beam search wrongly selects such hypotheses. [86] shows\nan example of such extreme cases, resulting in large deletion\nand insertion errors for short and long-segment hypotheses,\nrespectively. Thus, the label synchronous beam search requires\nheuristics to limit the output sequence length to avoid ex\u0002tremely long/short output sequences. Usually, the minimum\nand maximum length thresholds are determined proportionally\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nto the input frame length |X| with tunable parameters \u03c1min and\n\u03c1max as Lmin = \u230a\u03c1min|X|\u230b, Lmax = \u230a\u03c1max|X|\u230b. Although these\nare quite intuitive ways to control the length of a hypothesis,\nthe minimum and maximum output lengths depend on the\ntoken unit or type of script in each language. Another heuristic\nis to provide an additional score related to the output length\nor attention weights \u2013 e.g., a length penalty, and a coverage\nterm [77], [238]. The end-point detection [239] is also used to\nestimate the hypothesis length automatically. [236] redefines\nthe implicit length model of the attention decoder to take into\naccount beam search, resulting in consistent behavior without\ndegradation for increasing beam sizes.\nNote that there are several studies on applying label syn\u0002chronous beam search to explicit alignment modeling ap\u0002proaches. For example, label synchronous beam search al\u0002gorithms for CTC are realized by marginalizing all possible\nalignments for each label hypothesis [13]. [240] extends\nCIF [233] to produce label-level encoder representation and\nrealizes label synchronous beam search in RNN-T.\nD. Frame Synchronous Beam Search\nIn contrast to the label synchronous case in Eq. (8), the\nframe synchronous beam search algorithm performs pruning\nat every input frame t, as follows:\nC\u02dc\n1:i(t) = NBESTC1;i(t) P(C1;i(t)\n|X), where |C\u02dc\n1:i(t)\n| = \u2206\nwhere C1;i(t)is an i(t)-length label sequence obtained from\nthe alignment A1:t, which is introduced in Sec. III-B.\nP(C1;i(t)|X) is obtained by summing up all possible align\u0002ments A1:t \u2208 A(X,C1;i(t))\n. Unlike the label synchronous beam\nsearch, frame synchronous beam search depends on explicit\nalignment A; thus, it is often used for explicit alignment\nmodeling approaches, including CTC, RNN-T, and RNA.\nC1:i(t)is an expanded partial hypotheses up to input frame\nt, similar to Eq. (7).\nCompared with the label synchronous algorithm, the frame\nsynchronous algorithm needs to handle additional output to\u0002ken transitions inside the beam search algorithm. The frame\nsynchronous algorithm can be easily extended in online and/or\nstreaming decoding, thanks to the explicit alignment informa\u0002tion with input frame and output token.\nClassical approaches to beam search for HMM, but also\nCTC and RNN-T variants, are based on weighted finite state\ntransducers (WFST) [38], [74], [241] or lexical prefix trees\n[106], [242], [243]. They are categorized as frame synchronous\nbeam search. These methods are often combined with an N\u0002gram language model or a full-context neural language model\n[244], [245]. RNN-T [14], [246] and CTC prefix search [247]\ncan deal with a neural language model by incorporating the\nlanguage model score in the label transition state. Interestingly,\ntriggered attention approaches [248], [249] allow us to use\nimplicit alignment modeling approaches, including AED, in\nframe-synchronous beam search together with CTC and neural\nLM, which applies on-the-fly rescoring to the hypotheses given\nby CTC prefix search using the AED and LM scores.\nE. Block-wise Decoding\nAnother beam search implementation uses a fixed-length\nblock unit for the input feature. In this block processing, we\ncan use the future context inside the block by using the non\u0002causal encoder network based on the BLSTM, output-delayed\nunidirectional LSTM, or transformer (and its variants). This\nfuture context information avoids the degradation of the fully\ncausal network. In this setup, the chunk size becomes the\ntrade-off of controlling latency and accuracy. This technique is\nused in both RNN-T [100], [250], [251] and AED [61], [252],\n[253], [254]. Block-wise processing is especially important for\nimplicit alignment modeling approaches, including AED, since\nit can provide block-wise monotonic alignment constraint\nbetween the input feature and output label, and realize block\u0002wise streaming decoding.\nF. Model Fusion during Decoding\nSimilar to the classical HMM-based beam search, we com\u0002bine various scores obtained from different modules, including\nthe main end-to-end ASR and LM scores.\n1) Synchronous Score Fusion: The most simple score fu\u0002sion is performed when the scores of multiple modules are\nsynchronized. In this case, we can simply add the multiple\nscores at each frame t or label i. The most well-known score\ncombination is LM shallow fusion.\nLM shallow fusion: As discussed in Sec. VII, various neural\nLMs can be integrated with end-to-end ASR. The most simple\nintegration is based on LM shallow fusion [255][256][257], as\ndiscussed in Sec. VII-B1, which (log-) linearly adds the LM\nscore Plm(C1:i) to E2E ASR scores P(C1:i|X) during beam\nsearch in Eq. (8) as follows:\nlog P(C1:i|X) \u2192 log P(C1:i|X) + \u03b3 log Plm(C1:i)\nwhere \u03b3 is a language model weight. Of course, we can\ncombine other scores, such as the length penalty and coverage\nterms, as discussed in Sec. VI-C.u\n2) Asynchronous Score Fusion: If we combine the frame\u0002dependent score functions, P(at|\u00b7), used in explicit alignment\nmodeling approaches, e.g., CTC, RNN-T, and label-dependent\nscore functions, P(ci|\u00b7), used implicit alignment modeling\napproaches, e.g., AED, language model, we have to deal with\nthe mismatch between the frame and label time indices t and\ni, respectively.\nIn the time-synchronous beam search, this fusion is per\u0002formed by incorporating the language model score in the\nlabel transition state [70], [22], [258]. [247] also combines\na word-based language model and token-based CTC model\nby incorporating the language model score triggered by the\nword delimiter (space) symbol.\nIn the label-synchronous beam search, we first compute\nthe label-dependent scores from the frame-dependent score\nfunction by marginalizing all possible alignments given a\nhypothesis label sequence. CTC/attention joint decoding [86]\nis a typical example, where the CTC score is computed\nby marginalizing all possible alignments based on the CTC\nforward algorithm [229]. This approach eliminates the wrong\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nalignment issues and difficulties of finding the correct end of\nsentences in the label-synchronous beam search [86].\nNote that the model fusion method during beam search can\nrealize simple one-pass decoding, while it limits the time unit\nof the models to be the same or it requires additional dynamic\nprogramming to adjust the different time units, especially for\nthe label-synchronous beam search. This dynamic program\u0002ming computation becomes significantly large when the length\nof the utterance becomes larger and requires some heuristics\nto reduce the computational cost [259].\nG. Lexical Constraint during Score Fusion\nClassically, we use a word-based language model to cap\u0002ture the contextual information with the word unit, and also\nconsider the word-based lexical constraint for ASR. However,\nend-to-end ASR often uses a letter or token unit and it causes\nfurther unit mismatch during beam search. As described in\nprevious sections, the classical approach of incorporating the\nlexical constraint from the token unit to the word unit is\nbased on a WFST. This method first makes a TLG transducer\ncomposed of the token (T), word lexicon (L), and word-based\nlanguage transducers (G) [74]. This TLG transducer has been\nused for both CTC [74] and attention-based [53] models.\nAnother approach used in the time synchronous beam search\nis to insert the word-based language model score triggered by\nthe word delimiter (space) symbol [75]. To synchronize the\nword-based language model with a character-based end-to-end\nASR, [260] combines the word and character-based LMs with\nthe prefix tree representation, while [239], [261] uses look\u0002ahead word probabilities to predict next characters instead of\nusing the character-based LM. The prefix tree representation\nis also used for the sub-word token unit case [262], [263].\nH. Multi-pass Fusion\nThe previous fusion methods are performed during the\nbeam search, which enables a one-pass algorithm. The popular\nalternative methods are based on multi-pass algorithms where\nwe do not care about the synchronization and perform n-best or\nlattice scoring by considering the entire context within an ut\u0002terance. [16] uses the N-best rescoring techniques to integrate\na word-based language model. [55] combines forward and\nbackward searches within a multi-pass decoding framework to\ncombine bidirectional LSTM decoder networks. Recently two\u0002pass algorithms of switching different end-to-end ASR systems\nhave been investigated, including RNN-T \u2192 AED [264]; CTC\n\u2192 AED [265], [266]. This aims to provide streamed output in\nthe first pass and re-scoring with AED in the second pass to\nrefine the previous output, thus satisfying a real-time interface\nrequirement while providing high recognition performance.\nIn addition to the N-best output in the above discussion,\nthere is a strong demand for generating a lattice output\nfor better multi-pass decoding thanks to richer hypothesis\ninformation in a lattice. The lattice output can also be used\nfor spoken term detection, spoken language understanding,\nand word posteriors. However, due to the lack of Markov\nassumptions, RNN-T and AED cannot merge the hypothesis\nand cannot generate a lattice straightforwardly, unlike the\nHMM-based or CTC systems. To tackle this issue, there are\nseveral studies of modifying these models by limiting the\noutput dependencies in the fixed length (i.e., finite-history)\n[47], [267], or keeping the original RNN-T structure but\nmerging the similar hypotheses during beam search [107].\nI. Vectorization across both Hypotheses and Utterances\nWe can accelerate the decoding process by vectorizing\nmultiple hypotheses during the beam search, where we replace\nthe score accumulation steps for each hypothesis with vector\u0002matrix operations for the vectorized hypotheses. This has been\nstudied in RNN-T [22], [258], [268] and attention-based [259]\nmodels. This modification leverages the parallel computing\ncapabilities of multi-core CPUs, GPUs and TPUs, resulting in\nsignificant speedups, while enabling multiple utterances to be\nprocessed simultaneously in a batch. Major deep neural net\u0002work and end-to-end ASR toolkits support this vectorization.\nFor example, Tensorflow9[269], and FAIRESEQ10 [270] pro\u0002vide a vectorized beam search interface for a generic sequence\nto sequence task, and it can be used for attention-based end-to\u0002end ASR. End-to-end ASR toolkits including ESPnet11 [259],\nESPRESSO12[261], LINGVO [271], and, RETURNN13 [272]\nalso support the vectorized beam search algorithm.\nRelationship to Classical ASR\nOne of the most prominent properties shared between E2E\nand classical statistical ASR systems is the use of a single\u0002pass decoding strategy, which integrates all knowledge sources\ninvolved (models, components), before coming to a final\ndecision [123]. This includes the use of full label context\ndependency both for E2E systems [229], [51], [77], [273],\n[174], [262], [274], [275], as well as classical systems via full\u0002context language models [276], [244], [245], [277]. In classical\nASR systems, even HMM alignment path summation may be\nretained in search [278]. Both E2E as well as classical ASR\nsystems employ beam search in decoding. However, compared\nto classical search approaches, E2E beam search usually is\nhighly simplified with very small beam sizes around 1 to\n100 [15], [16], [77], [147]. Very small beam sizes also partly\nmask a length bias exhibited by E2E attention-based encoder\u0002decoder models [279], [280], thus trading model errors against\nsearch errors [281]. An overview of approaches to handle the\nlength bias beyond using small beam sizes in ASR is presented\nin [236].\nMany classical ASR search paradigms are based on mul\u0002tipass approaches that successively generate search space\nrepresentations applying increasingly complex acoustic and/or\nlanguage models [282], [283], [243]. However, multipass\nstrategies also are employed using E2E models, which how\u0002ever softens the E2E concept. Decoder model combination is\npursued in a two-pass approach, while even retaining latency\nconstraints as in [87]. Further multipass approaches include\nE2E adaptation approaches [284], [285], [286], [287].\n9 https://www.tensorflow.org/api docs/python/tf/contrib/seq2seq/BeamSearchDecoder\n10 https://github.com/pytorch/fairseq/blob/master/fairseq/sequence\ngenerator.py 11 https://github.com/espnet/espnet\n12 https://github.com/freewym/espresso\n13 https://github.com/rwth-i6/returnn\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nVII. LM INTEGRATION\nThis section discusses language models (LMs) used for E2E\nASR. Hybrid ASR systems have long been using a pretrained\nLM [2], whereas most end-to-end (E2E) ASR systems employ\na single E2E model that includes a network component acting\nas an LM.14 For example, the prediction network of RNN\u0002T and the decoder network of AED models take on the role\nof a LM covering label back-histories. Therefore, E2E ASR\ndoes not seem to require external LMs. Nevertheless, many\nstudies have demonstrated that external LMs help improve the\nrecognition accuracy in E2E ASR.\nThere are presumably three reasons that E2E ASR still\nrequires an external LM:\na) Compensation for poor generalization: E2E models\nneed to learn a more complicated mapping function than\nclassical modular-based models such as acoustic models. Con\u0002sequently, E2E models tend to face overfitting problems if\nthe amount of training data is not sufficient. Pretrained LMs\npotentially compensate for the less generalized predictions\nmade by E2E models.\nb) Use of external text data: E2E models need to be\ntrained using paired speech and text data, while LMs can\nbe trained with only text data. Generally, text data can be\ncollected more easily than the paired data. The training speed\nof an LM is also faster than that of E2E models for the same\nnumber of sentences. Accordingly, the LM can be improved\nmore effectively with external text data, providing additional\nperformance gain to the ASR system.\nc) Domain adaptation: Domain adaptation helps im\u0002prove recognition accuracy when the E2E model is applied\nto a specific domain. However, domain adaptation of the E2E\nmodel requires a certain amount of paired data in the target\ndomain. Also, when multiple domains are assumed, it may be\ncostly to maintain multiple E2E models for the domains the\nsystem supports. If a pretrained LM for the target domain is\navailable, it may more easily improve recognition accuracy for\ndomain-specific words and speaking styles without updating\nthe E2E model.\nThis section reviews various types of LMs used for E2E\nASR and fusion techniques to integrate LMs into E2E models.\nA. Language Models\nThe LMs provide a prior probability distribution, P(C). If\nthe sentence, C, can be decomposed into a sequence of tokens\nsuch as characters, subwords, and single words, the probability\ndistribution can be computed based on the chain rule as:\nP(C) =\nL\nY\n+1\ni=1\nP(ci|c0:i\u22121)\nwhere ci denotes the i-th token of C, and c0:i\u22121 represents\ntoken sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and\ncL+1 = \u27e8eos\u27e9.\nMost LMs are designed to provide the conditional probabil\u0002ity P(ci\n|c0:i\u22121), i.e., they are modeled to predict the next token\n14 In the simplest case of a CTC model as in Fig. 2, the included LM\ncomponent however is limited to a label prior without label context.\ngiven a sequence of the preceding tokens. We briefly review\nsuch LMs focusing on the different techniques to represent\neach token, ci, and back-history, c0:i\u22121.\n1) N-gram LM: N-gram LMs have long been used for\nASR [2]. Early E2E systems in [53], [74], [77] also employed\nan N-gram LM. The N-gram models rely on the Markov\nassumption that the probability distribution of the next token\ndepends only on the previous N\u22121 tokens, i.e., P(ci|c0:i\u22121) \u2248\nP(ci|ci\u2212N+1:i\u22121), where N is typically 3 to 5 for word-based\nmodels and higher for sub-word and character-based models.\nThe maximum likelihood estimates of N-gram probabilities\nare determined based on the counts of N sequential tokens in\nthe training data set as:\nP(ci|ci\u2212N+1:i\u22121) = K(ci\u2212N+1, . . . , ci)\nP\nci\nK(ci\u2212N+1, . . . , ci)\nwhere, K(\u00b7) denotes the count of each token sequence. Since\nthe data size is finite, it is important to apply a smoothing\ntechnique to avoid estimating the probabilities based on zero or\nvery small counts for rare token sequences. Those techniques\ncompensate the N-gram probabilities with lower order models,\ne.g., (N \u2212 1)-gram models, according to the magnitude of\nthe count [288]. However, since the N-gram probabilities\nstill rely on the discrete representation of each token and the\nhistory, they suffer from data sparsity problems, leading to\npoor generalization.\nThe advantage of the N-gram models is their simplicity,\nalthough they underperform state-of-the-art neural LMs. In the\ntraining, the main step is to just count the N tuples in the data\nset, which is required only once. During decoding, the LM\nprobabilities can be obtained very quickly by table lookup or\ncan be attached to a decoding graph, e.g., WFST, in advance.\n2) FNN-LM: The feed-forward neural network (FNN) LM\nwas proposed in [9], which estimates N-gram probabilities\nusing a neural network. The network accepts N \u2212 1 tokens,\nand predicts the next token as:\nP(ci|ci\u2212N+1:i\u22121) = softmax(Wohi + bo)\nhi = tanh(Whei + bh)\nei = concat(E(ci\u2212N+1), . . . , E(ci\u22121))\nwhere Wo and Wh are weight matrices, and bo and bh are\nbias vectors. E(y) provides an embedding vector of c, and\nconcat(\u00b7) operation concatenates given vectors 15. This model\nfirst maps each input token to an embedding space, and then\nobtains hidden vector, hi, as a context vector representing the\nprevious N \u22121 tokens. Finally, it outputs the probability distri\u0002bution of the next token through the softmax layer. Although\nthis LM still relies on the Markov assumption, it outperforms\nclassical N-gram LMs described in the previous section.\nThe superior performance of FNN-LM is primarily due to\nthe distributed representation of each token and the history.\nThe LM learns to represent token/context vectors such that\nsemantically similar tokens/histories are placed close to each\nother in the embedding space. Since this representation has a\nbetter smoothing effect than the count-based one used for N\u0002gram LMs, FNN-LM can provide a better generalization than\n15 We omit the optional direct connection from the embedding layer to the\nsoftmax layer in [9] for simplicity.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nN-gram LMs for predicting the next token. Neural network\u0002based LMs basically utilize this type of representation.\n3) RNN-LM: A recurrent neural network (RNN) LM was\nintroduced to exploit longer contextual information over N \u2212\n1 previous tokens using recurrent connections [289]. Unlike\nFNN-LM, the hidden vector is computed as:\nhi = recurrence(ei, hi\u22121)\nei = E(ci\u22121)\nwhere, recurrence(ei, hi\u22121) represents a recursive function,\nwhich accepts previous hidden vector hi\u22121 with input ei, and\noutputs next hidden vector hi. In the case of simple (Elman\u0002type) RNN, the function can be computed as\nrecurrence(e, h) = tanh(Whe + Wrh + bh)\nwhere, Wr is a weight matrix for the recurrent connection,\nwhich is applied to the previous hidden vector h. This recurrent\nloop makes it possible to hold the history information in the\nhidden vector without limiting the history to N \u2212 1 tokens.\nHowever, the history information decays exponentially as\ntokens are processed with this recursion. Therefore, currently\nstacked LSTM layers are more widely used for the recurrent\nnetwork, which have separate internal memory cells and gating\nmechanisms to keep long-range history information [290].\nWith this mechanism, RNN-LMs outperform other N-gram\u0002based models in many tasks.\n4) ConvLM: Convolutional neural networks (ConvLM)\nhave also been applied to LMs [291], [292], [293]. ConvLM\n[292] replace the recurrent connections used in RNN-LMs\nwith gated temporal convolutions. The hidden vector is com\u0002puted as\nhi =h\n\u2032\ni \u2297 \u03c3(gi)\nh\n\u2032\ni =ei\u2212k+1:i \u2217 W + b\ngi =ei\u2212k+1:i \u2217 V + c\nwhere \u2297 is element-wise multiplication, \u2217 is a temporal\nconvolution operation, and k is the patch size. \u03c3(gi) represents\na gating function of convoluted activation h\n\u2032\ni\n, and is modeled\nas a sigmoid function. W and V are matrices for convolution\nand b and c are bias vectors. The convolution and gating blocks\nare typically stacked multiple times with residual connections.\nIn [293], a ConvLM with 14 blocks has been applied for E2E\nASR. Similar to FNN-LM, ConvLM allow us to use only a\nfixed history size, but they are more parameter efficient and\neasier to utilize longer histories than the FNN-LM by stacking\nthe layers. Thus, they achieve competitive performance to that\nof RNN-LMs [292], even with the finite history consisting\nof short tokens such as characters [294]. Moreover, they are\nhighly parallelizable and thus suitable for training the model\nwith a large training data set.\n5) Transformer LM: Transformer architecture [44] has been\napplied to LMs [295] and used for ASR [102], [296], where\nthe LMs are designed as a Transformer decoder without any\ninputs from other modules such as encoders. The hidden vector\nis computed as:\nhi = FFN(h\n\u2032\ni\n) + h\n\u2032\ni\nh\n\u2032\ni = MHA(ei\n, e1:i, e1:i) + ei\nwhere FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network\nand a multi-head attention module, respectively. The multi\u0002head attention and feed-forward blocks are typically stacked\nmultiple times, e.g., 6 times [102], to obtain the final hidden\nvector. The advantage of Transformer LMs is that they can\ntake all tokens in the history into account through the self\u0002attention mechanism without summarizing them into a fixed\u0002size memory like RNN-LMs. Thus, the long history can be\nfully considered with attention to predict the next token,\nachieving better performance than RNN-LMs. However, the\ncomputational complexity increases quadratically as the length\nof the sequence. Therefore, the history length is typically\nlimited to a fixed size or within every single sentence. To\novercome this limitation, Transformer-XL [297] reuses already\ncomputed activations, which includes information on farther\nprevious tokens, and the model is trained with a truncated\nback-propagation through time (BPTT) algorithm [298]. Com\u0002pressive Transformer [299] extends this approach to utilize\neven longer contextual information by incorporating a com\u0002pression step to keep older, but important, information in a\nfixed-size memory network.\nB. Fusion Approaches\nThere are several ways to incorporate an external LM into\nE2E ASR, called LM fusion. Their purpose is to improve the\nrecognition accuracy of E2E ASR by leveraging the benefits\nof the external LM described in the first part of this section.\nHowever, there can be a mismatch in the prediction between\nthe E2E model and the LM when trained on different data\nsets, and therefore the LM may not collaborate well with\nthe E2E model. Researchers have investigated various LM\nfusion approaches to reduce the mismatch between models\nin different situations.\n1) Shallow Fusion: Shallow fusion is the most popular\napproach to combine the pretrained E2E model and LM in\nthe inference time. As we described in Sec. VI-F, shallow\nfusion simply combines the E2E and LM scores by a log\u0002linear combination as\nScore(C|X) = log P(C|X) + \u03b3 log P(C) (9)\nwhere \u03b3 is a scaling factor for the LM [255][256][257]. The\nadvantage of this approach is that it is easy and effective when\nthere are no major mismatches between the source and target\ndomains.\n2) Deep Fusion: Deep fusion [300] is an approach to\ncombine an LM with an E2E model using a joint network.\nGiven a pretrained E2E model and an LM, all the network\nparameters are fine-tuned jointly so that the models collaborate\nbetter to improve the recognition accuracy, where the joint\nnetwork is used to combine the E2E and LM states through\na gating mechanism that controls the contribution of the LM\naccording to the current state.\n3) Cold fusion: Cold fusion [301] is another approach to\ncombine a pretrained LM like deep fusion, but the E2E model\nis learned while freezing the LM parameters. Since the E2E\nmodel is aware of the LM throughout training, it learns to use\nthe LM to reduce language specific information and capture\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nonly the relevant information to map the source to the target\nsequence. This mechanism reduces the role of LM in the E2E\nmodel and alleviates the language bias of the training data.\nAccordingly, the E2E model becomes more robust to domain\nmismatches between the training data and the target domain.\nUnlike deep fusion, cold fusion makes it possible to combine\nthe E2E model with a pretrained LM for the target domain,\nimproving the recognition accuracy. Component fusion [302]\nextends cold fusion to use a pretrained LM with transcriptions\nof the training data for the E2E model, more focusing on\nreducing the bias of the training data.\n4) Internal LM Estimation: There is another approach to\nreduce language bias in training data through shallow fusion.\nThe language bias is a problem when a big domain mismatch\nexists between the source domain (training data) and the target\ndomain (test data) because the E2E model scores are strongly\ndependent on the language priors in the source domain. To\nremove such a bias from the score, we can explicitly estimate\nthe LM that represents the language priors, called Internal LM,\nand subtract the LM score from the ASR score of Eq. (9):\nScore(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C)\nwhere subscripts \u03c6 and \u03c4 indicate the source and target\ndomains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub\u0002tracting the internal LM score corresponds to approximating\nacoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d\nP\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR\nscore can be seen as a classical hybrid ASR system. Ac\u0002cordingly, the subtracted E2E model score plays a role of\nacoustic model and makes it more domain independent in\nterms of language, achieving a higher recognition accuracy\nin combination with the external LM P\u03c4 (C).\nThe density ratio method [303] trains an internal LM using\nthe transcript of the training data. Hybrid autoregressive trans\u0002ducer (HAT) [47] extends RNN-T so that the model becomes\nthe internal LM when the encoder output is eliminated, i.e., set\nto zero. This approach simplifies the framework by utilizing\nthe prediction network as the internal LM, which avoids\ntraining an additional LM and using it in the inference time.\nIn the work of [304], an approach similar to HAT has been\nproposed where the internal LM is formulated on top of\nstandard RNN-T and attention-based encoder-decoder models,\nrespectively. In [128], several techniques to estimate internal\nLMs have been proposed for AED models, where an estimated\nbias vector is fed to the LM instead of a zero vector. The bias\nvector can be estimated by averaging encoder states or context\nvectors, or by a small LSTM predicting the context vector\nbased on the decoder label context, only. These techniques to\nestimate the internal LM were also evaluated for RNN-T in\n[305].\nC. Use of Large-scale Pretrained LMs\nIn recent years, LMs trained with large-scale text data are\navailable for different NLP tasks. BERT [306] and GPT-2\n[307] are representative models based on Transformer LMs.\nSuch LMs have also been applied to E2E ASR systems in\ndifferent ways, e.g., N-best rescoring [308] and dialog context\nembedding [309].\nRelationship to Classical ASR\nThe architecture of classical ASR systems provides a sepa\u0002ration between the acoustic model and the language model.\nIn contrast to this, E2E models avoid this separation and\ndefine a joint model. While this allows for training with a\nsingle objective, it limits training of the (implicit) prior to\nthe transcriptions of the audio training data. To exploit further\ntext-only training data, usually a separate LM is combined with\nE2E models, nonetheless. However, due to the implicit prior\nof E2E models, i.e. the internal language model, combination\nwith separate language models is not straightforward and\nrequires corresponding internal language model estimation\nand compensation approaches, e.g. [303], [47], [304], [128],\n[310]. At least from the recognition accuracy perspective, it\nremains unclear, if the clear separation of acoustic modeling\nand language modeling in the classical ASR architecture is a\ndisadvantage because of separate training objectives, or rather\nan advantage, since text-only training data may be used easily.\nAlso, the language model training objective, i.e. language\nmodel perplexity, is observed to correlate well with word error\nrate [311], [312], [313], [314]. Furthermore, discriminative\napproaches to language modeling [315] may be viewed as a\nstep towards joint modeling.\nVIII. OVERALL PERFORMANCE TRENDS OF E2E\nAPPROACHES IN COMMON BENCHMARKS\nThis section summarizes various techniques with the com\u0002mon ASR benchmarks based on switchboard (SWBD) [316]\nin Figure 9 and Librispeech [317] in Figure 10 to see the\ntrajectory of the techniques developed in end-to-end ASR. We\nchoose these two databases because they are widely used in\nspeech and machine learning communities and cover sponta\u0002neous (SWBD) and read speech (Librispeech) speaking styles.\nFigures 9 and 10 show that the performance improvement\nrelative to the initial works [147], [79] based on the E2E\nmodels is significant, and the error rates of all tasks become\nless than half of the original error rates!16\nAlthough the overall trends show that the ASR performance\nhas steadily improved over time, there are several remarkable\ngains. One significant gain observed in both benchmarks in the\nmiddle of 2019 comes from the data augmentation method\nrepresented by SpecAugment [205], [206], as discussed in\nSection V-G. The subsequent gains mostly come from the\nexploration of the new neural network architectures, including\ntransformer [102], [318], conformer [45], [103], and contextnet\n[97] on top of SpecAugment, as discussed in Section IV-C.\nSuch an exploration is also performed in language modeling\nto improve the ASR performance [296], [102]. The final gain\nobserved in the Librispeech benchmark in 2021 is based\non self-supervised learning [25], [319] and semi-supervised\nlearning [320], [321]. These techniques utilize a considerable\n16 For readers who want to know the latest update of these\nbenchmarks can also check https://github.com/syhw/wer are we and\nhttps://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\nAEDAEDAEDAED\nHMM\nAEDAEDAEDAED AEDAED\nWER (%)\n0\n10\n20\n30\n40\n50\n1/1/2017 1/1/2018 1/1/2019 1/1/2020 1/1/2021\nswb chm\nFig. 9. E2E ASR performance improvement in the switchboard task. AED AED\nAEDAEDAED\ntransducer ContextNet Transducer\nCTC\nHMM\nAED transducer\nWER (%)\ntransformer\n0\n5\n10\n15\n1/1/2019 7/1/2019 1/1/2020 7/1/2020 1/1/2021 7/1/2021\ntest_clean test_other\nFig. 10. E2E ASR performance improvement in the Librispeech task.\namount of unlabeled in-domain speech data (e.g., Libri-light\n60K hours [322]).\nRelationship to Classical ASR\nSpeech recognition research has always been pushed by\ninternational evaluation campaigns (e.g. as lead by NIST)\nand corresponding benchmark tasks. The competition between\nclassical and E2E approaches is nicely reflected in the widely\nused Librispeech [317] and Switchboard [316] tasks, showing\nthat E2E models gain momentum. As shown in Figure 10,\non Librispeech, the current best-published classical hybrid\nsystems range around 2.3% (test-clean) and 4.9% (test-other)\nword error rate [323], [222], while there already are a number\nof E2E systems providing similar performance [224], [205],\n[320], [206], with some E2E systems clearly outperforming\nformer state-of-the-art results with word error rates down to\n1.8% (test-clean) and 3.7% (test-other) [324] with similar\nresults reported in [45], [97]. Merging insights from classical\nHMM-based and monotonic RNN-T provided similarly well\nresults with a limited training budget [124]. Finally, when\ntrained on Switchboard 300h, the current best result, obtained\nwith an E2E system [180] is 5.4% compared to 6.6% word\nerror rate for the best hybrid system result [325] on the\nHUB5\u201900 Switchboard test set, in Figure 9\nIX. DEPLOYMENT OF E2E MODELS\nMany of the ideas discussed in this paper have been\nexplored by various industry research labs [326], [327], [328],\n[329], [330], [331], [265], inter alia. In this section, we\nreview the development of on-device production-level systems\nat Google as a typical case study for deployment.\nThe first streaming E2E model, deployed to production,\nwas launched in 2019 for the Pixel 4 smartphone [22], [332].\nThis model used a streaming RNN-T first-pass system, while\nre-scoring first-pass hypotheses with an AED system in the\nsecond pass. In addition, FST-based contextual biasing [92]\nwas employed in the model, which was critical to obtain\naccurate results for diverse queries. This model ran on CPU\nand was much faster than real time.\nIn 2020, for the Pixel 5 smartphone [333], the system was\nimproved further to reduce user-perceived latency (i.e., the\ntime between when the user speaks, and when words appear\non the device). This included advancements such as end-to-end\nendpointing [113] to encourage faster microphone closing; as\nwell as FastEmit [91] to encourage the model to emit tokens\nearlier.\nFinally, in 2021 the model was further improved for the\nPixel 6 smartphone [334], to take advantage of the tensor\nprocessing unit (TPU) [85] on the device. Improvements\ninclude the use of conformer layers for the encoder [45]; a\nsmall embedding prediction network for the decoder [104]; a\n2-pass cascaded encoder to run a 2nd-pass beam search [89];\nand, a neural LM re-scorer to help improve accuracy long-tail\nnamed entities. This model is the best ASR system that Google\nhas released to date, both in terms of quality and latency.\nX. AREAS FOR FUTURE WORK\nCurrently, E2E models dominate the academic debate on\nASR. However, at least partly, this is not (yet?) reflected\nin the corresponding commercial deployment of E2E ASR\narchitectures. E2E models are not yet the perfect match for\nall ASR conditions and further research is needed to take full\nadvantage of the benefits of E2E modeling.\nE2E models seem to perform really well when training data\nis abundant, while not scaling well to low-resource conditions.\nSimilarly, domain change requires a flexible exchange of lan\u0002guage models, which is natural for classical ASR models based\non a separation of acoustic and language models. Ongoing\nresearch on the use of external language models in E2E models\nand internal language model estimation already is promising,\nbut can be expected to see further improvements.\nTop E2E ASR systems usually require orders of magnitude\nmore training epochs than comparable classical ASR systems,\nand further research into efficient and robust optimization and\ntraining schedules is needed.\nThe high level of integration of E2E models also involves a\nloss in modularity, which might support the explainability and\nreusability of models. Also, more efficient training schedules\nmight take advantage of modularity. One assumed advantage\nof E2E models is that everything is trained from data and\nsecondary knowledge sources (e.g. pronunciation lexica and\nphoneme sets) are avoided. However, rare events, like rare\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\nwords in ASR still provide a challenge, which needs further\nresearch.\nWith the missing separation of acoustic and language mod\u0002els, the question arises of how to exploit text-only resources in\nE2E model training - do we foresee solutions beyond training\ndata generation using TTS? We note that a number of recent\nworks have explored approaches to combine speech and text\nmodalities by attempting to implicitly or explicitly map them\ninto a shared space [159], [335], [336], [337], [338], [339],\n[340], [341]. Furthermore, high-performance E2E solutions\nexist for both discriminative problems like ASR, as well as\ngenerative problems like TTS, how can both be exploited\njointly to support semi-supervised training based on text-only\nand/or audio-only data on top of transcribed speech audio [28],\n[342]?\nFor AED architectures, we observe a length bias, which\ncomplicates the decoding process. Although many heuristics\nare known to tackle length bias in AED, we are still missing\na well-founded explanation for it, as well as a corresponding\nremedy of the original model.\nOther open research problems include speaker adaptation\nand robustness to recording conditions, especially in mismatch\nsituations. The E2E principle also provides a promising candi\u0002date to solve multichannel ASR by providing an E2E solution\njointly tackling the source separation, speaker diarization and\nspeech recognition problem [343], [26].\nFinally, we need to investigate, if E2E is a suitable guiding\nprinciple, and how different E2E ASR models relate to each\nother as well as to classical ASR approaches. The most\nimportant guiding principle of ASR research and development\nhas been performance, and ASR has been boosted strongly\nby widely used benchmark tasks and international evaluation\ncampaigns. With the current diversity of classical and E2E\nmodels, we also need to resolve the question of what con\u0002stitutes state-of-the-art in ASR today, and can we expect a\ncommon state-of-the-art ASR architecture in the future?\nXI. CONCLUSIONS\nIn this work, we presented a detailed overview of end-to\u0002end approaches to ASR. Such models, which have grown in\npopularity over the last few years, propose to use highly inte\u0002grated neural network components which allow input speech\nto be converted directly into output text sequences through\ncharacter-based output units. Thus, such models eschew the\nclassical modular ASR architecture consisting of an acoustic\nmodel, a pronunciation model, and a language model, in\nfavor of a single compact structure, and rely on the data to\nlearn effectively. These design choices enable the deployment\nof highly accurate on-device speech recognition models (see\nSection IX), but also come with a number of downsides which\nare still areas of active research (see Section X).\nFinally, we direct interested readers to Li\u2019s excellent\ncontemporaneous overview article on end-to-end ASR [344],\nwhich offers a complementary perspective to our own. In\nparticular, readers of [344] may find a more detailed exposition\non the choice of encoder structure, and the applications of\nE2E approaches to allied ASR areas (e.g., multi-speaker\nrecognition; multilingual ASR; adaptation to new application\ndomains, and speakers; etc.), which we do not cover due to\nspace limitations.\nACKNOWLEDGMENT\nThe authors would like to thank Julian Dierkes, Yifan Peng,\nZoltan T \u00b4 uske, Albert Zeyer, and Wei Zhou for their help on \u00a8\nrefining our manuscript.\nREFERENCES\n[1] T. Bayes, \u201cAn Essay Towards Solving a Problem in the Doctrine of\nChances,\u201d Philosophical Transactions of the Royal Society of London,\nvol. 53, pp. 370\u2013418, 1763.\n[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,\nMA: MIT Press, 1997.\n[3] L. R. Rabiner, \u201cA Tutorial on Hidden Markov Models and Selected\nApplications in Speech Recognition,\u201d Proc. of the IEEE, vol. 77, no. 2,\npp. 257\u2013286, Feb. 1989.\n[4] H. A. Bourlard and N. Morgan, Connectionist Speech Recognition: a\nHybrid Approach. Norwell, MA: Kluwer Academic Publishers, 1993.\n[5] F. Seide, G. Li, and D. Yu, \u201cConversational Speech Transcription Using\nContext-Dependent Deep Neural Networks,\u201d in Proc. Interspeech,\nFlorence, Italy, Aug. 2011, pp. 437\u2013440.\n[6] V. Fontaine, C. Ris, and H. Leich, \u201cNonlinear Discriminant Analysis for\nImproved Speech Recognition,\u201d in Proc. Eurospeech, Rhodes, Greece,\nSep. 1997, pp. 1\u20134.\n[7] H. Hermansky, D. Ellis, and S. Sharma, \u201cTandem connectionist Feature\nExtraction for Conventional HMM Systems,\u201d in Proc. IEEE ICASSP,\nvol. 3, Istanbul, Turkey, Jun. 2000, pp. 1635\u20131638.\n[8] M. Nakamura and K. Shikano, \u201cA Study of English Word Category\nPrediction Based on Neural Networks,\u201d in Proc. IEEE ICASSP, Glas\u0002glow, UK, May 1989, pp. 731\u2013734.\n[9] Y. Bengio, R. Ducharme, and P. Vincent, \u201cA Neural Probabilistic\nLanguage Model,\u201d in Proc. NIPS, vol. 13, Denver, CO, Nov. 2000,\npp. 932\u2013938.\n[10] H. Schwenk and J.-L. Gauvain, \u201cConnectionist Language Modeling\nfor Large Vocabulary Continuous Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 765\u2013768.\n[11] Z. Tuske, P. Golik, R. Schl \u00a8 uter, and H. Ney, \u201cAcoustic Modeling with \u00a8\nDeep Neural Networks Using Raw Time Signal for LVCSR,\u201d in Proc.\nInterspeech, Singapore, Sep. 2014, pp. 890\u2013894.\n[12] T. N. Sainath, R. J. Weiss, K. W. Wilson, A. Narayanan, M. Bacchiani,\nand A. Senior, \u201cSpeaker Location and Microphone Spacing Invariant\nAcoustic Modeling from Raw Multichannel Waveforms,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 30\u201336.\n[13] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \u201cConnection- \u00b4\nist temporal classification: labelling unsegmented sequence data with\nrecurrent neural networks,\u201d in Proc. ICML, Pittsburgh, PA, Jun. 2006,\npp. 369\u2013376.\n[14] A. Graves, \u201cSequence Transduction with Recurrent Neural Networks,\u201d\nin Proc. ICML, Edinburgh, Scotland, Jun. 2012, Workshop on Repre\u0002sentation Learning, arXiv:1211.3711.\n[15] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\n\u201cAttention-Based Models for Speech Recognition,\u201d in Proc. NIPS,\nvol. 28, Laval, Queebec, Canada, Dec. 2015, pp. 577\u2013585. `\n[16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n4960\u20134964.\n[17] P. Liang, A. Bouchard-Cot\u02c6 e, D. Klein, and B. Taskar, \u201cAn End-to- \u00b4\nEnd Discriminative Approach to Machine Translation,\u201d in Proc. ACL,\nSydney, Australia, Jul. 2006, p. 761\u2013768.\n[18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, \u201cNatural Language Processing (Almost) from Scratch,\u201d\nJournal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.\n[19] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recognition\nwith Recurrent Neural Networks,\u201d in Proc. ICML, Beijing, China, Jun.\n2014, pp. 1764\u20131772.\n[20] \u201cCambridge Dictionary,\u201d https://dictionary.cambridge.org/dictionary/\nenglish/end-to-end, accessed: 2020-02-21.\n[21] R. Pang, T. N. Sainath, R. Prabhavalkar, S. Gupta, Y. Wu, S. Zhang, and\nC.-c. Chiu, \u201cCompression of End-to-End Models,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 27\u201331.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[22] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,\nD. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shang\u0002guan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-y. Chang, K. Rao, and\nA. Gruenstein, \u201cStreaming End-to-End Speech Recognition for Mobile\nDevices,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6381\u2013\n6385.\n[23] R. Schluter and H. Ney, \u201cModel-based MCE Bound to the True Bayes\u2019 \u00a8\nError,\u201d IEEE Signal Processing Letters, vol. 8, no. 5, pp. 131\u2013133, May\n2001.\n[24] H. Ney, \u201cOn the Relationship between Classification Error Bounds\nand Training Criteria in Statistical Pattern Recognition,\u201d in Iberian\nConference on Pattern Recognition and Image Analysis (IbPRIA),\nPuerto de Andratx, Spain, Jun. 2003, pp. 636\u2013645.\n[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A\nFramework for Self-Supervised Learning of Speech Representations,\u201d\nin Proc. NeurIPS, Vancouver, BC, Canada, Dec. 2020, pp. 12 449\u2013\n12 460.\n[26] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, \u201cMIMO\u0002Speech: End-to-End Multi-Channel Multi-Speaker Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU. Sentosa, Singapore: IEEE, Dec. 2019,\npp. 237\u2013244.\n[27] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and\nRegression Trees. Belmont, CA: Taylor & Francis, 1984.\n[28] A. Tjandra, S. Sakti, and S. Nakamura, \u201cListening While Speaking:\nSpeech Chain by Deep Learning,\u201d in Proc. IEEE ASRU. Okinawa,\nJapan: IEEE, Dec. 2017, pp. 301\u2013308.\n[29] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and\nJ. Cernock \u02c7 y, \u201cSemi-Supervised Sequence-to-Sequence ASR Using \u00b4\nUnpaired Speech and Text,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3790\u20133794, arXiv:1905.01152.\n[30] H. Soltau, H. Liao, and H. Sak, \u201cNeural Speech Recognizer: Acoustic\u0002to-Word LSTM Model for Large Vocabulary Speech Recognition,\u201d in\nProc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1610.09975.\n[31] G. K. Zipf, Human Behavior and the Principle of Least Effort. Boston,\nMA: Addison-Wesley Press, 1949.\n[32] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural Machine Translation\nof Rare Words with Subword Units,\u201d in Proc. ACL, Berlin, Germany,\nAug. 2015, pp. 1715\u20131725.\n[33] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, \u201cLatent Sequence Decompo\u0002sitions,\u201d in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1610.03035.\n[34] H. Liu, Z. Zhu, X. Li, and S. Satheesh, \u201cGram-CTC: Automatic unit\nselection and target decomposition for sequence labelling,\u201d in Proc.\nICML, ser. Proceedings of Machine Learning Research, D. Precup\nand Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 2188\u20132197,\narXiv:1703.00096.\n[35] H. Xu, S. Ding, and S. Watanabe, \u201cImproving End-to-End Speech\nRecognition with Pronunciation-Assisted Sub-Word Modeling,\u201d in\nProc. IEEE ICASSP, Brighton, UK, Sep. 2019, pp. 7110\u20137114.\n[36] W. Zhou, M. Zeineldeen, Z. Zheng, R. Schluter, and H. Ney, \u201cAcoustic \u00a8\nData-Driven Subword Modeling for End-to-End Speech Recognition,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2886\u20132890.\n[37] M. Schuster and K. Nakajima, \u201cJapanese and Korean Voice Search,\u201d\nin Proc. IEEE ICASSP, Kyoto, Japan, Mar. 2012, pp. 5149\u20135152.\n[38] M. Mohri, F. Pereira, and M. Riley, \u201cWeighted Finite-State Transducers\nin Speech Recognition,\u201d Computer Speech & Language, vol. 16, no. 1,\npp. 69\u201388, 2002.\n[39] E. Beck, M. Hannemann, P. Doetsch, R. Schluter, and H. Ney, \u00a8\n\u201cSegmental Encoder-Decoder Models for Large Vocabulary Automatic\nSpeech Recognition,\u201d in Proc. Interspeech, Hyderabad, India, Sep.\n2018.\n[40] W. Zhou, A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cEquivalence \u00a8\nof Segmental and Neural Transducer Modeling: A Proof of Concept,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2891\u20132895.\n[41] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, \u201cA Comparison of Sequence-to-Sequence Models for Speech\nRecognition,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp.\n939\u2013943.\n[42] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural\nComputation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[43] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by\nJointly Learning to Align and Translate,\u201d in Proc. ICLR, San Diego,\nCA, May 2015, arXiv:1409.0473.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is All You Need,\u201d\nin Proc. NIPS, Los Angeles, CA, Dec. 2017, pp. 5998\u20136008.\n[45] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution\u0002Augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 5036\u20135040.\n[46] H. Sak, M. Shannon, K. Rao, and F. Beaufays, \u201cRecurrent Neural\nAligner: An Encoder-Decoder Neural Network Model for Sequence to\nSequence Mapping,\u201d in Proc. Interspeech, vol. 8, Stockhol, Sweden,\nAug. 2017, pp. 1298\u20131302.\n[47] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid Autore\u0002gressive Transducer (HAT),\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6139\u20136143.\n[48] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech Recognition with\nDeep Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP, Vancouver,\nBC, Canada, May 2013, pp. 6645\u20136649.\n[49] N. Moritz, T. Hori, S. Watanabe, and J. Le Roux, \u201cSequence Trans\u0002duction with Graph-Based Supervision,\u201d in Proc. IEEE ICASSP, Sin\u0002gapore, May 2022, pp. 7212\u20137216.\n[50] Y. Bengio, N. Leonard, and A. Courville, \u201cEstimating or Propagating \u00b4\nGradients through Stochastic Neurons for Conditional Computation,\u201d\nAug. 2013, arXiv:1308.3432.\n[51] A. Tripathi, H. Lu, H. Sak, and H. Soltau, \u201cMonotonic Recurrent\nNeural Network Transducer and Decoding Strategies,\u201d in Proc. IEEE\nASRU, Sentosa, Singapore, Dec. 2019, pp. 944\u2013948.\n[52] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA New Training \u00a8\nPipeline for an Improved Neural Transducer,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 2812\u20132816.\n[53] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-Based Large Vocabulary Speech Recognition,\u201d\nin Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4945\u20134949.\n[54] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, \u0141. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa,\nK. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean,\n\u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap Be\u0002tween Human and Machine Translation,\u201d Oct. 2016, arXiv:1609.08144.\n[55] M. Mimura, S. Sakai, and T. Kawahara, \u201cForward-Backward Attention\nDecoder,\u201d in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2232\u2013\n2236.\n[56] A. Graves, \u201cGenerating Sequences with Recurrent Neural Networks,\u201d\nAug. 2013, arXiv:1308.0850.\n[57] J. Hou, S. Zhang, and L.-R. Dai, \u201cGaussian Prediction Based At\u0002tention for Online End-to-End Speech Recognition,\u201d in Proc. In\u0002terspeech, Stockholm, Sweden, Aug. 2017, pp. 3692\u20133696, DOI:\n10.21437/Interspeech.2017-751.\n[58] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen,\nA. Narayanan, H. Liao, S. Zhang, A. Kannan, R. Prabhavalkar, Z. Chen,\nT. Sainath, and Y. Wu, \u201cA Comparison of End-to-End Models for Long\u0002Form Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 889\u2013896.\n[59] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,\n\u201cAn Online Sequence-to-Sequence Model Using Partial Conditioning,\u201d\nin Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 5067\u20135075.\n[60] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, \u201cOnline and\nLinear-Time Attention by Enforcing Monotonic Alignments,\u201d in Proc.\nICML, Sydney, Australia, Aug. 2017, pp. 2837\u20132846.\n[61] C.-C. Chiu and C. Raffel, \u201cMonotonic Chunkwise Attention,\u201d in Proc.\nICLR, Vancouver, Canada, Apr. 2018, arXiv:1712.05382.\n[62] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz,\nR. Pang, W. Li, and C. Raffel, \u201cMonotonic Infinite Lookback Attention\nfor Simultaneous Machine Translation,\u201d in Proc. ACL, Florence, Italy,\nJun. 2019, pp. 1313\u20131323.\n[63] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu,\nP. Nguyen, and Z. Chen, \u201cImproving the Performance of Online Neural\nTransducer Models,\u201d in Proc. IEEE ICASSP, Calgary, Alberta, Canada,\nApr. 2018, pp. 5864\u20135868.\n[64] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Brighton, England,\nMay 2019, pp. 5666\u20135670.\n[65] A. Merboldt, A. Zeyer, R. Schluter, and H. Ney, \u201cAn Analysis of Local \u00a8\nMonotonic Attention Variants,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 1398\u20131402.\n[66] A. Zeyer, R. Schluter, and H. Ney, \u201cA Study of Latent Monotonic \u00a8\nAttention Variants,\u201d Mar. 2021, arXiv:2103.16710.\n[67] A. Zeyer, R. Schmitt, W. Zhou, R. Schluter, and H. Ney, \u201cMonotonic \u00a8\nSegmental Attention for Automatic Speech Recognition,\u201d in Proc. IEEE\nSLT, Doha, Qatar, Jan. 2023, arXiv:2210.14742.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[68] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, \u201cSynchronous\nTransformers for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, arXiv:1912.02958.\n[69] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely Sequence-Trained Neural\nNetworks for ASR Based on Lattice-Free MMI,\u201d in Proc. Inter\u0002speech. San Francisco, CA: ISCA, Sep. 2016, pp. 2751\u20132755, DOI:\n10.21437/Interspeech.2016-595.\n[70] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: An End\u0002to-End Convnet-Based Speech Recognition System,\u201d Sep. 2016,\narXiv:1609.03193.\n[71] P. Haffner, \u201cConnectionist Speech Recognition with a Global MMI\nAlgorithm,\u201d in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp.\n1929\u20131932.\n[72] A. Zeyer, E. Beck, R. Schluter, and H. Ney, \u201cCTC in the Context of \u00a8\nGeneralized Full-Sum HMM Training,\u201d in Proc. Interspeech, Stock\u0002holm, Sweden, Aug. 2017, pp. 944\u2013948.\n[73] T. Raissi, W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cHMM vs. \u00a8\nCTC for Automatic Speech Recognition: Comparison Based on Full\u0002Sum Training from Scratch,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan.\n2023, arXiv:2210.09951.\n[74] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-End Speech\nRecognition Using Deep RNN Models and WFST-Based Decoding,\u201d\nin Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 167\u2013174.\n[75] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng,\n\u201cDeep Speech: Scaling up End-to-End Speech Recognition,\u201d Dec.\n2014, arXiv:1412.5567.\n[76] L. Lu, X. Zhang, and S. Renals, \u201cOn Training the Recurrent Neural\nNetwork Encoder-Decoder for Large Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n5060\u20135064.\n[77] J. Chorowski and N. Jaitly, \u201cTowards Better Decoding and Language\nModel Integration in Sequence to Sequence Models,\u201d in Proc. Inter\u0002speech, Stockhol, Sweden, Aug. 2017, pp. 523\u2013527.\n[78] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery Deep Convolutional Networks\nfor End-to-End Speech Recognition,\u201d in Proc. IEEE ICASSP, New\nOrleans, LA, Mar. 2017, pp. 4845\u20134849.\n[79] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learning\nwith Low-Level Auxiliary Tasks for Encoder-Decoder based Speech\nRecognition,\u201d in Proc. Interspeech, Stockholm, Sweden, Aug. 2017,\narXiv:1704.01631.\n[80] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti\u0002Modal Data Augmentation for End-to-End ASR,\u201d in Proc. Interspeech,\nHyderabad, India, Mar. 2018, pp. 2394\u20132398.\n[81] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Distillation\nfor Sequence Learning,\u201d in Proc. ICLR, New Orleans, LA, May 2019,\narXiv:1810.01398.\n[82] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu,\n\u201cImproving Attention Based Sequence-to-Sequence Models for End-to\u0002End English Conversational Speech Recognition,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 761\u2013765.\n[83] D. Le, X. Zhang, W. Zheng, C. Fugen, G. Zweig, and M. L. Seltzer, \u00a8\n\u201cFrom Senones to Chenones: Tied Context-Dependent Graphemes for\nHybrid Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 457\u2013464.\n[84] S. Kanthak and H. Ney, \u201cContext-Dependent Acoustic Modeling Using\nGraphemes for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 845\u2013848.\n[85] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., \u201cIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit,\u201d in Proc. of\nthe 44th Annual International Symposium on Computer Architecture,\nToronto, Ontario, Canada, Jun. 2017, pp. 1\u201312.\n[86] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, \u201cHybrid\nCTC Attention Architecture for End-to-End Speech Recognition,\u201d\nIEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8,\npp. 1240\u20131253, 2017.\n[87] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C. Chung\u0002Cheng, \u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech, Brighton, UK, May 2019, pp. 2773\u20132777.\n[88] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, \u201cDeliberation\nModel Based Two-Pass End-to-End Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 7799\u20137803.\n[89] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab\u0002havalkar, E. Variani, and T. Strohman, \u201cCascaded Encoders for Uni\u0002fying Streaming and Non-Streaming ASR,\u201d in Proc. IEEE ICASSP,\nToronto, Ontario, Canada, Jun. 2021, pp. 5629\u20135633.\n[90] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, \u201cTransformer Trans\u0002ducer: One Model Unifying Streaming and Non-Streaming Speech\nRecognition,\u201d Oct. 2020, arXiv:2010.03192.\n[91] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu, and\nR. Pang, \u201cUniversal ASR: Unify and Improve Streaming ASR with\nFull-Context Modeling,\u201d Oct. 2020, arXiv:2010.06030.\n[92] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, and\nR. Pang, \u201cShallow-Fusion End-to-End Contextual Biasing,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 1418\u20131422.\n[93] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao,\n\u201cDeep Context: End-to-end Contextual Speech Recognition,\u201d in Proc.\nIEEE SLT, Athens, Greece, Dec. 2018, pp. 418\u2013425.\n[94] S. Kim and F. Metze, \u201cDialog-Context Aware End-to-End Speech\nRecognition,\u201d in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 434\u2013\n440.\n[95] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, \u201cPhoebe:\nPronunciation-Aware Contextualization for End-to-End Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6171\u2013\n6175.\n[96] M. Delcroix, S. Watanabe, A. Ogawa, S. Karita, and T. Nakatani,\n\u201cAuxiliary Feature Based Adaptation of End-to-End ASR Systems,\u201d\nin Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2444\u20132448.\n[97] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextNet: Improving Convolutional Neural\nNetworks for Automatic Speech Recognition with Global Context,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 3610\u20133614.\n[98] L. Dong, S. Xu, and B. Xu, \u201cSpeech-Transformer: A No-Recurrence\nSequence-to-Sequence Model for Speech Recognition,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5884\u20135888.\n[99] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer Transducer: A Streamable Speech Recognition\nModel with Transformer Encoders and RNN-T Loss,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7829\u20137833.\n[100] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain,\nK. Schubert, C. Fuegen, and M. L. Seltzer, \u201cTransformer-Transducer:\nEnd-to-Snd Speech Recognition with Self-Attention,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 7829\u20137833.\n[101] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, \u201cBranchformer: Parallel\nMLP-Attention Architectures to Capture Local and Global Context for\nSpeech Recognition and Understanding,\u201d in Proc. ICML. Baltimore,\nMD: PMLR, Jul. 2022, pp. 17 627\u201317 643.\n[102] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang, \u201cA Comparative Study on Transformer\nvs RNN in Speech Applications,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 449\u2013456.\n[103] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,\nN. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei,\nW. Zhang, and Y. Zhang, \u201cRecent Developments on ESPNET Toolkit\nBoosted by Conformer,\u201d in Proc. IEEE ICASSP. Toronto, Ontario,\nCanada: IEEE, Jun. 2021, pp. 5874\u20135878.\n[104] R. Botros, T. Sainath, R. David, E. Guzman, W. Li, and Y. He, \u201cTied &\nReduced RNN-T Decoder,\u201d in Proc. Interspeech, Brno, Czechia, Sep.\n2021, pp. 4563\u20134567.\n[105] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, \u201cRNN\u0002Transducer with Stateless Prediction Network,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7049\u20137053.\n[106] W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cPhoneme Based Neural \u00a8\nTransducer for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5644\u20135648.\n[107] R. Prabhavalkar, Y. He, D. Rybach, S. Campbell, A. Narayanan,\nT. Strohman, and T. N. Sainath, \u201cLess is More: Improved RNN-T\nDecoding Using Limited Label Context and Path Merging,\u201d in Proc.\nIEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5659\u20135663.\n[108] X. Chen, Z. Meng, S. Parthasarathy, and J. Li, \u201cFactorized Neural\nTransducer for Efficient Language Model Adaptation,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8132\u20138136, arXiv:2110.01500.\n[109] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi,\nJ. Emond, T. Strohman, B. Ramabhadran, W. R. Huang et al., \u201cModular\nHybrid Autoregressive Transducer,\u201d in Proc. IEEE SLT, Doha, Qatar,\nJan. 2023, pp. 197\u2013204, https://arXiv:2210.17049.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[110] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and\nF. Wei, \u201cVioLA: Unified Codec Language Models for Speech Recog\u0002nition, Synthesis, and Translation,\u201d May 2023, arXiv:2305.16107.\n[111] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor\u0002sos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov\net al., \u201cAudioPaLM: A Large Language Model That Can Speak and\nListen,\u201d Jun. 2023, arXiv:2306.12925.\n[112] S.-Y. Chang, B. Li, and G. Simko, \u201cA Unified Endpointer Using\nMultitask and Multidomain Training,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 100\u2013106.\n[113] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and\nY. Wu, \u201cTowards Fast and Accurate Streaming End-To-End ASR,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6069\u20136073.\n[114] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, \u201cEnd-To\u0002End Automatic Speech Recognition Integrated with CTC-Based Voice\nActivity Detection,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6999\u20137003.\n[115] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, \u201cToward Stream\u0002ing ASR with Non-Autoregressive Insertion-Based Model,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3740\u20133744.\n[116] Y. Bengio, \u201cPractical Recommendations for Gradient-Based Training\nof Deep Architectures,\u201d Jun. 2012, arXiv:1206.5533.\n[117] J. Schmidhuber, \u201cDeep Learning in Neural Networks: An Overview,\u201d\nNeural Networks, vol. 61, pp. 85\u2013117, Jan. 2015, arXiv:1404.7828.\n[118] L. Baum, \u201cAn Inequality and Associated Maximization Technique in\nStatistical Estimation for Probabilistic Functions of Markov Processes,\u201d\nInequalities, vol. 3, pp. 1\u20138, 1972.\n[119] L. Rabiner and B.-H. Juang, \u201cAn Introduction to Hidden Markov Mod\u0002els,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing,\nvol. 3, no. 1, pp. 4\u201316, 1986.\n[120] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, \u201cNeural Network\u0002Gaussian Mixture Hybrid for Speech Recognition or Density Estima\u0002tion,\u201d in Proc. NIPS, vol. 4, Colorado, Dec. 1991, pp. 175\u2013182.\n[121] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton\nUniversity Press, 1957.\n[122] A. Viterbi, \u201cError Bounds for Convolutional Codes and an Asymptoti\u0002cally Optimal Decoding Algorithm,\u201d IEEE Transactions on Information\nTheory, vol. 13, pp. 260\u2013269, 1967.\n[123] H. Ney, \u201cThe Use of a One-Stage Dynamic Programming Algorithm\nfor Connected Word Recognition,\u201d IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, vol. 32, no. 2, pp. 263\u2013271, 1984.\n[124] W. Zhou, W. Michel, R. Schluter, and H. Ney, \u201cEfficient Training \u00a8\nof Neural Transducer for Speech Recognition,\u201d in Proc. Interspeech,\nIncheon, Korea, Sep. 2022, arXiv:2204.10586.\n[125] A. Zeyer, R. Schluter, and H. Ney, \u201cWhy does CTC Result in Peaky \u00a8\nBehavior?\u201d May 2021, arXiv:2105.14849.\n[126] A. Laptev, S. Majumdar, and B. Ginsburg, \u201cCTC Variations Through\nNew WFST Topologies,\u201d in Proc. Interspeech, Incheon, Korea, sep\n2022, DOI: 10.21437/interspeech.2022-10854.\n[127] X. He, L. Deng, and W. Chou, \u201cDiscriminative Learning in Sequential\nPattern Recognition \u2013 A Unifying Review for Optimization-Oriented\nSpeech Recognition,\u201d IEEE Signal Processing Magazine, vol. 25, no. 5,\npp. 14\u201336, 2008.\n[128] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schluter, and \u00a8\nH. Ney, \u201cInvestigating Methods to Improve Language Model Inte\u0002gration for Attention-Based Encoder-Decoder ASR Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Aug. 2021, pp. 2856\u20132860.\n[129] N.-P. Wynands, W. Michel, J. Rosendahl, R. Schluter, and H. Ney, \u00a8\n\u201cEfficient Sequence Training of Attention Models using Approxima\u0002tive Recombination,\u201d in Proc. IEEE ICASSP, Singapore, May 2022,\narXiv:2110.09245.\n[130] Z. Yang, W. Zhou, R. Schluter, and H. Ney, \u201cLattice-Free Sequence \u00a8\nDiscriminative Training for Phoneme-based Neural Transducers,\u201d in\nProc. IEEE ICASSP, Rhodes, Greece, Jun. 2023, arXiv:2212.04325.\n[131] V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young, \u201cMMIE\nTraining of Large Vocabulary Recognition Systems,\u201d Speech Commu\u0002nication, vol. 22, no. 4, pp. 303\u2013314, 1997.\n[132] D. Povey and P. Woodland, \u201cImproved Discriminative Training Tech\u0002niques for Large Vocabulary Continuous Speech Recognition,\u201d in Proc.\nIEEE ICASSP, Salt Lake City, UT, May 2001, pp. 45\u201348.\n[133] R. Schluter, W. Macherey, B. M \u00a8 uller, and H. Ney, \u201cComparison of \u00a8\nDiscriminative Training Criteria and Optimization Methods for Speech\nRecognition,\u201d Speech Communication, vol. 34, no. 3, pp. 287\u2013310, May\n2001, EURASIP Best Paper Award.\n[134] B. Kingsbury, \u201cLattice-Based Optimization of Sequence Classifica\u0002tion Criteria for Neural-Network Acoustic Modeling,\u201d in Proc. IEEE\nICASSP, Taipei, Taiwan, Apr. 2009, pp. 3761\u20133764.\n[135] G. Heigold, R. Schluter, H. Ney, and S. Wiesler, \u201cDiscriminative \u00a8\nTraining for Automatic Speech Recognition: Modeling, Criteria, Opti\u0002mization, Implementation, and Performance,\u201d IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 58\u201369, Nov. 2012.\n[136] W. Michel, R. Schluter, and H. Ney, \u201cComparison of Lattice-Free and \u00a8\nLattice-Based Sequence Discriminative Training Criteria for LVCSR,\u201d\nin Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1601\u20131605,\narXiv:1907.01409.\n[137] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu,\nand A. Kannan, \u201cMinimum Word Error Rate Training for Attention\u0002Based Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4839\u20134843.\n[138] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, \u201cMinimum Bayes Risk\nTraining of RNN-Transducer for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 966\u2013970, DOI:\n10.21437/Interspeech.2020-1221.\n[139] M. K. Baskar, L. Burget, S. Watanabe, M. Karafiat, T. Hori, and \u00b4\nJ. H. Cernock \u02c7 y, \u201cPromising Accurate Prefix Boosting for Sequence- `\nto-Sequence ASR,\u201d in Proc. IEEE ICASSP. Brighton, UK: IEEE,\nMay 2019, pp. 5646\u20135650.\n[140] A. Tjandra, S. Sakti, and S. Nakamura, \u201cSequence-to-Sequence ASR\nOptimization via Reinforcement Learning,\u201d in Proc. IEEE ICASSP.\nCalgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5829\u20135833.\n[141] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, \u201cSequence Training\nof Encoder-Decoder Model Using Policy Gradient for End-to-End\nSpeech Recognition,\u201d in Proc. IEEE ICASSP. Calgary, Alberta,\nCanada: IEEE, Apr. 2018, pp. 5839\u20135843.\n[142] W. Michel, R. Schluter, and H. Ney, \u201cEarly Stage LM Integration Using \u00a8\nLocal and Global Log-Linear Combination,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 3605\u20133609, arXiv:2005.10049.\n[143] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA Fast Learning Algorithm\nfor Deep Belief Nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u2013\n1554, Jul. 2006.\n[144] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy Layer\u0002Wise Training of Deep Networks,\u201d in Proc. NIPS, Barcelona, Spain,\nDec. 2006, pp. 153\u2013160.\n[145] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schluter, and H. Ney, \u00a8\n\u201cA Comprehensive Study of Deep Bidirectional LSTM RNNs for\nAcoustic Modeling in Speech Recognition,\u201d in Proc. IEEE ICASSP,\nNew Orleans, LA, Mar. 2017, pp. 2462\u20132466.\n[146] A. Zeyer, T. Alkhouli, and H. Ney, \u201cRETURNN as a Generic Flexible\nNeural Toolkit with Application to Translation and Speech Recogni\u0002tion,\u201d in Proc. ACL, Melbourne, Australia, Jul. 2018, pp. 128\u2013133.\n[147] A. Zeyer, K. Irie, R. Schluter, and H. Ney, \u201cImproved Training \u00a8\nof End-to-End Attention Models for Speech Recognition,\u201d in Proc.\nInterspeech, Hyderabad, India, Sep. 2018, pp. 7\u201311.\n[148] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA Comprehensive \u00a8\nAnalysis on Attention Models,\u201d in Proc. NIPS, Montreal, Canada, Dec.\n2018.\n[149] Y. Chung, C. Wu, C. Shen, H. Lee, and L. Lee, \u201cAudio Word2Vec:\nUnsupervised Learning of Audio Segment Representations using\nSequence-to-sequence Autoencoder,\u201d in Proc. Interspeech, San Fran\u0002cisco, CA, Sep. 2016, arXiv:1603.00982.\n[150] Y.-C. Chen, S.-F. Huang, H.-y. Lee, Y.-H. Wang, and C.-H. Shen, \u201cAu\u0002dio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised\nLearning of Audio Segmentation and Representation,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 27,\nno. 9, pp. 1481\u20131493, 2019, DOI: 10.1109/TASLP.2019.2922832.\n[151] S. Scanzio, P. Laface, L. Fissore, R. Gemello, and F. Mana, \u201cOn the\nUse of a Multilingual Neural Network Front-End,\u201d in Proc. Interspeech,\nBrisbane, Australia, Sep. 2008, pp. 2711\u20132714.\n[152] Z. Tuske, J. Pinto, D. Willett, and R. Schl \u00a8 uter, \u201cInvestigation on \u00a8\nCross- and Multilingual MLP features under matched and mismatched\nacoustical conditions,\u201d in IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, Vancouver, Canada, May 2013, pp.\n7349\u20137353.\n[153] S. Zhou, S. Xu, and B. Xu, \u201cMultilingual End-to-End Speech Recog\u0002nition with a Single Transformer on Low-Resource Languages,\u201d Jun.\n2018, arXiv:1806.05059.\n[154] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL-HLT,\nMinneapolis, MN, Jun. 2019, arXiv:1904.02210.\n[155] W. Hou, Y. Dong, B. Zhuang, L. Yang, J. Shi, and T. Shinozaki,\n\u201cLarge-scale end-to-end multilingual speech recognition and language\nidentification with multi-task learning,\u201d in Proc. Interspeech, Shanghai,\nChina, Oct. 2020, pp. 1037\u20131041.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\n[156] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Syn\u0002naeve, and R. Collobert, \u201cMassively Multilingual ASR: 50 Languages,\n1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, Shanghai, China,\nOct. 2020, arXiv:2007.03001.\n[157] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani,\nW. R. Huang, M. Ma, and J. Bai, \u201cScaling End-to-End Models for\nLarge-Scale Multilingual ASR,\u201d in Proc. IEEE ASRU, 2021, pp. 1011\u2013\n1018.\n[158] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen,\nY. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li, M. Ma, W. Chan,\nJ. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath,\nF. Beaufays, Z. Chen, Q. V. Le, C.-C. Chiu, R. Pang, and Y. Wu,\n\u201cBigSSL: Exploring the frontier of large-scale semi-supervised learning\nfor automatic speech recognition,\u201d IEEE Journal of Selected Topics\nin Signal Processing, vol. 16, no. 6, pp. 1519\u20131532, oct 2022,\narXiv:2109.13226.\n[159] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Repre\u0002sentations through Modality Matching,\u201d in Proc. Interspeech, Incheon,\nSouth Korea, Sep. 2022, arXiv:2204.03409.\n[160] A. Radford, J. W. Kim, C. McLeavey, P. Mishkin, T. Xu, G. Brockman,\nand I. Sutskever, \u201cIntroducing Whisper - Robust Speech Recognition\nvia Large-Scale Weak Supervision,\u201d Sep. 2022. [Online]. Available:\nhttps://openai.com/blog/whisper/\n[161] T. P. Vogl, J. Mangis, A. Rigler, W. Zink, and D. Alkon, \u201cAcceler\u0002ating the Convergence of the Back-Propagation Method,\u201d Biological\nCybernetics, vol. 59, no. 4, pp. 257\u2013263, 1988.\n[162] N. S. Keskar and G. Saon, \u201cA Nonmonotone Learning Rate Strategy\nfor SGD Training of Deep Neural Networks,\u201d in Proc. IEEE ICASSP.\nQueensland, Australia: IEEE, Apr. 2015, pp. 4974\u20134978.\n[163] S. Renals, N. Morgan, H. Bourlard, C. Wooters, and P. Kohn, \u201cConnec\u0002tionist Speech Recognition: Status and Prospects,\u201d ICSI, 1991, Tech.\nRep. TR-OI-070.\n[164] D. Johnson, D. Ellis, C. Oei, C. Wooters, and P. Faerber, \u201cQuickNet,\u201d\nICSI, Berkeley, 2004. [Online]. Available: http://www.icsi.berkeley.\nedu/Speech/qn.html\n[165] A. Senior, G. Heigold, M. Ranzato, and K. Yang, \u201cAn Empirical Study\nof Learning Rates in Deep Neural Networks for Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Vancouver, BC, Canada: IEEE, May 2013,\npp. 6724\u20136728.\n[166] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d\nin Proc. ICLR, New Orleans, LA, May 2019, arXiv:1711.05101.\n[167] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, \u201cDon\u2019t Decay the\nLearning Rate, Increase the Batch Size,\u201d in Proc. ICLR, New Orleans,\nLA, May 2018, arXiv:1711.00489.\n[168] J. Howard and S. Ruder, \u201cUniversal Language Model Fine-Tuning for\nText Classification,\u201d in Proc. ACL, Melbourne, Australia, Jun. 2018,\npp. 328\u2013339.\n[169] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fer\u0002nando, and K. Kavukcuoglu, \u201cPopulation Based Training of Neural\nNetworks,\u201d Nov. 2017, arXiv:1711.09846.\n[170] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta\u0002Learning in Neural Networks: A Survey,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PP, pp. 1\u201320, 2021.\n[171] J. L. Elman, \u201cLearning and Development in Neural Networks: The\nImportance of Starting Small,\u201d Cognition, vol. 48, no. 1, pp. 71\u201399,\n1993, DOI: 10.1016/0010-0277(93)90058-4.\n[172] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum\nLearning,\u201d in Proc. ICML, Montreal, Quebec, Canada, Jun. 2009, p.\n41\u201348.\n[173] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan\u0002zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen,\nJ. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley,\nL. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh,\nD. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao,\nD. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep Speech 2: End-to-End Speech\nRecognition in English and Mandarin,\u201d in Proc. ICML, New York City,\nNY, Jun. 2016, pp. 173\u2013182.\n[174] Z. Tuske, G. Saon, K. Audhkhasi, and B. Kingsbury, \u201cSingle Headed \u00a8\nAttention Based Sequence-to-Sequence Model for State-of-the-Art\nResults on Switchboard,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 551\u2013555.\n[175] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, \u201cImproving End\u0002to-End Single-Channel Multi-Talker Speech Recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 28, pp. 1385\u2013\n1394, 2020.\n[176] B. Polyak, \u201cSome Methods of Speeding up the Convergence of\nIteration Methods,\u201d USSR Computational Mathematics and Mathe\u0002matical Physics, vol. 4, no. 5, pp. 1\u201317, 1964, DOI: 10.1016/0041-\n5553(64)90137-5.\n[177] Y. Nesterov, \u201cA method of solving a convex programming problem\nwith convergence rate O( 1\nk2\n),\u201d Soviet Mathematics Doklady, vol. 27,\npp. 372\u2013376, 1983.\n[178] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, \u201cOn the Importance\nof Initialization and Momentum in Deep Learning,\u201d in Proc. ICML,\nAtlanta, GA, Jun. 2013, pp. 1139\u20131147.\n[179] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimiza\u0002tion,\u201d in Proc. ICLR, San Diego, CA, May 2015, arXiv:1412.6980.\n[180] Z. Tuske, G. Saon, and B. Kingsbury, \u201cOn the Limit of English Con- \u00a8\nversational Speech Recognition,\u201d in Proc. Interspeech, Brno, Czechia,\nSep. 2021, pp. 2062\u20132066.\n[181] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever,\n\u201cDeep Double Descent: Where Bigger Models and More Data Hurt,\u201d\nin Proc. ICLR, virtual, Apr. 2020, arXiv:1912.02292.\n[182] A. Krogh and J. Hertz, \u201cA Simple Weight Decay Can Improve\nGeneralization,\u201d in Neural Information Processing Systems (NIPS),\nDenver, CO, Dec. 1991, pp. 950\u2013957.\n[183] A. F. Murray and P. J. Edwards, \u201cEnhanced MLP Performance and\nFault Tolerance Resulting from Synaptic Weight Noise during Train\u0002ing,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 5, pp. 792\u2013\n802, Sep. 1994.\n[184] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nAdvances in Neural Information Processing Systems, vol. 24, 2011.\n[185] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach,\nand J. Martens, \u201cAdding Gradient Noise Improves Learning for Very\nDeep Networks,\u201d Nov. 2015, arXiv:1511.06807.\n[186] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, \u201cImproving Neural Networks by Preventing Co\u0002Adaptation of Feature Detectors,\u201d Jul. 2012, arXiv:1207.0580.\n[187] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification\nwith Deep Convolutional Neural Networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), vol. 25, Lake Tahoe, NV, Dec.\n2012.\n[188] Y. Gal and Z. Ghahramani, \u201cDropout as a Bayesian Approximation:\nRepresenting Model Uncertainty in Deep Learning,\u201d in Proc. ICML,\nNew York City, NY, Jun. 2016, pp. 1050\u20131059.\n[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep Net\u0002works with Stochastic Depth,\u201d in European Conference on Computer\nVision, Amsterdam, Netherlands, Oct. 2016, pp. 646\u2013661.\n[190] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Muller, and A. Waibel, \u00a8\n\u201cVery Deep Self-Attention Networks for End-to-End Speech Recogni\u0002tion,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 66\u201370.\n[191] J. Lee and S. Watanabe, \u201cIntermediate Loss Regularization for CTC\u0002Based Speech Recognition,\u201d in Proc. IEEE ICASSP, Toronto, Ontario,\nCanada, Jun. 2021, pp. 6224\u20136228.\n[192] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \u201cRegularization\nof Neural Networks using DropConnect,\u201d in Proc. ICML, 2013, pp.\n1058\u20131066.\n[193] D. Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, \u00b4\nA. Goyal, Y. Bengio, A. Courville, and C. Pal, \u201cZoneout: Regularizing\nRNNs by Randomly Preserving Hidden Activations,\u201d in Proc. ICLR,\nToulon, France, Apr. 2017, arXiv:1606.01305.\n[194] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink\u0002ing the Inception Architecture for Computer Vision,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition, Las Vegas, NV, Jun. 2016,\npp. 2818\u20132826.\n[195] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled Sampling\nfor Sequence Prediction with Recurrent Neural Networks,\u201d Proc. NIPS,\nvol. 28, Dec. 2015.\n[196] T. Trinh, A. Dai, T. Luong, and Q. Le, \u201cLearning Longer-Term Depen\u0002dencies in RNNs with Auxiliary Losses,\u201d in Proc. ICML, Stockholm,\nSweden, Jul. 2018, pp. 4965\u20134974.\n[197] R. J. Williams and J. Peng, \u201cAn Efficient Gradient-Based Algorithm\nfor On-Line Training of Recurrent Network Trajectories,\u201d IEEE Neural\nComputation, vol. 2, no. 4, pp. 490\u2013501, 1990.\n[198] S. Merity, N. S. Keskar, and R. Socher, \u201cAn Analysis of Neural\nLanguage Modeling at Multiple Scales,\u201d Mar. 2018, arXiv:1803.08240.\n[199] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, \u201cMixSpeech:\nData Augmentation for Low-resource Automatic Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021,\npp. 7008\u20137012.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n25\n[200] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift,\u201d in Proc.\nICML, Lille, France, Jul. 2015, pp. 448\u2013456.\n[201] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic Spectral Distortion for\nLow Resource Speech Recognition with Deep Neural Networks,\u201d in\nProc. IEEE ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 309\u2013\n314.\n[202] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmentation\nfor Speech Recognition,\u201d in Proc. Interspeech, Dresden, Germany, Sep.\n2015.\n[203] N. Jaitly and G. E. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nImproves Speech Recognition,\u201d in Proc. ICML, vol. 117, Jun. 2013,\np. 21.\n[204] G. Saon, Z. Tuske, K. Audhkhasi, and B. Kingsbury, \u201cSequence Noise \u00a8\nInjected Training for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Brighton, England, May 2019, pp. 6261\u20136265.\n[205] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le,\nand Y. Wu, \u201cSpecAugment on Large Scale Datasets,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 6879\u20136883.\n[206] C. Wang, Y. Wu, Y. Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao, and\nM. Zhou, \u201cSemantic Mask for Transformer Based End-to-End Speech\nRecognition,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n971\u2013975.\n[207] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo,\nand K. Takeda, \u201cBack-Translation-Style Data Augmentation for End\u0002to-End ASR,\u201d in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018,\npp. 426\u2013433.\n[208] N. Rossenbach, M. Zeineldeen, B. Hilmes, R. Schluter, and H. Ney, \u00a8\n\u201cComparing the Benefit of Synthetic Training Data for Various Au\u0002tomatic Speech Recognition Architectures,\u201d in Proc. IEEE ASRU,\nCartagena, Colombia, Dec. 2021, arXiv:2104.05379.\n[209] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen, and\nC.-C. Chiu, \u201cNo Need for a Lexicon? Evaluating the Value of\nthe Pronunciation Lexica in End-to-End Models,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5859\u20135863, DOI:\n10.1109/ICASSP.2018.8462380.\n[210] C. Wooters and A. Stolcke, \u201cMultiple-Pronunciation Lexical Modeling\nin a Speaker Independent Speech Understanding System,\u201d in Proc.\nICSLP, Yokohama, Japan, Sep. 1994, pp. 1363\u20131366.\n[211] I. McGraw, I. Badr, and J. R. Glass, \u201cLearning Lexicons From Speech\nUsing a Pronunciation Mixture Model,\u201d IEEE/ACM Trans. Audio,\nSpeech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2012.\n[212] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGMM-Free DNN\nAcoustic Model Training,\u201d in Proc. IEEE ICASSP, Florence, Italy, May\n2014, pp. 5602\u20135606, DOI: 10.1109/ICASSP.2014.6854675.\n[213] G. Gosztolya, T. Grosz, and L. T \u00b4 oth, \u201cGMM-Free Flat Start Sequence- \u00b4\nDiscriminative DNN Training,\u201d in Proc. Interspeech, N. Morgan,\nEd. San Francisco, CA: ISCA, Sep. 2016, pp. 3409\u20133413, DOI:\n10.21437/Interspeech.2016-391.\n[214] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cFlat-Start\nSingle-Stage Discriminatively Trained HMM-Based Models for ASR,\u201d\nIEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 26,\nno. 11, pp. 1949\u20131961, 2018.\n[215] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig,\n\u201cThe IBM 2004 Conversational Telephony System for Rich Transcrip\u0002tion,\u201d in Proc. IEEE ICASSP, Philadelphia, PA, Mar. 2005, pp. 205\u2013\n208.\n[216] H. Hadian, D. Povey, H. Sameti, J. Trmal, and S. Khudanpur,\n\u201cImproving LF-MMI Using Unconstrained Supervisions for ASR,\u201d\nin Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 43\u201347, DOI:\n10.1109/SLT.2018.8639684.\n[217] N. Kanda, Y. Fujita, and K. Nagamatsu, \u201cLattice-Free State-Level Min\u0002imum Bayes Risk Training of Acoustic Models,\u201d in Proc. Interspeech,\nB. Yegnanarayana, Ed. Hyderabad, India: ISCA, Sep. 2018, pp. 2923\u2013\n2927, DOI: 10.21437/Interspeech.2018-79.\n[218] S. J. Young and P. C. Woodland, \u201cThe Use of State Tying in Continuous\nSpeech Recognition,\u201d in Proc. Eurospeech, Berlin, Germany, Dec.\n1993, pp. 2203\u20132206.\n[219] S. Wiesler, G. Heigold, M. Nu\u00dfbaum-Thom, R. Schluter, and H. Ney, \u00a8\n\u201cA Discriminative Splitting Criterion for Phonetic Decision Trees,\u201d\nin Proc. Interspeech, Makuhari, Japan, Sep. 2010, pp. 54\u201357, one of\nshortlist for Best Student Paper Award.\n[220] T. Raissi, E. Beck, R. Schluter, and H. Ney, \u201cTowards Consistent \u00a8\nHybrid HMM Acoustic Modeling,\u201d Apr. 2021, arXiv:2104.02387.\n[221] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schluter, and H. Ney, \u00a8\n\u201cA Systematic Comparison of Grapheme-Based vs. Phoneme-Based\nLabel Units for Encoder-Decoder-Attention Models,\u201d Nov. 2020,\narXiv:2005.09336.\n[222] C. Luscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, \u00a8\nR. Schluter, and H. Ney, \u201cRWTH ASR Systems for LibriSpeech: \u00a8\nHybrid vs Attention,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 231\u2013235.\n[223] D. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. Le,\n\u201cImproved Noisy Student Training for Automatic Speech Recognition,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2817\u20132821.\n[224] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and\nQ. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for\nAutomatic Speech Recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 2613\u20132617.\n[225] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schluter, and H. Ney, \u201cThe \u00a8\nRWTH ASR System for TED-LIUM Release 2: Improving Hybrid\nHMM with SpecAugment,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 7839\u20137843.\n[226] J. Cui, B. Kingsbury, B. Ramabhadran, A. Sethy, K. Audhkhasi, X. Cui,\nE. Kislal, L. Mangu, M. Nussbaum-Thom, M. Picheny, Z. Tuske, \u00a8\nP. Golik, R. Schluter, H. Ney, M. J. F. Gales, K. M. Knill, A. Ragni, \u00a8\nH. Wang, and P. Woodland, \u201cMultilingual Representations for Low\nResource Speech Recognition and Keyword Search,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 259\u2013266.\n[227] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL, Min\u0002neapolis, MN, Jun. 2019, pp. 96\u2013108.\n[228] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. Ramabhadran,\nY. Wu, A. Bapna, Z. Chen, and S. Lee, \u201cLarge-Scale Multilingual\nSpeech Recognition with a Streaming End-to-End Model,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2130\u20132134.\n[229] A. Graves, \u201cConnectionist Temporal Classification,\u201d in Supervised\nSequence Labelling with Recurrent Neural Networks. Heidelberg,\nGermany: Springer, 2012, ch. Connectionist Temporal Classification,\npp. 61\u201393.\n[230] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,\n\u201cMask CTC: Non-Autoregressive End-to-End ASR with CTC and\nMask Predict,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n3655\u20133659.\n[231] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, \u201cImputer:\nSequence Modelling via Imputation and Dynamic Programming,\u201d in\nProc. ICML. PMLR, Jul. 2020, pp. 1403\u20131413.\n[232] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, \u201cInsertion-Based\nModeling for End-to-End Automatic Speech Recognition,\u201d in Proc.\nInterspeech, Shanghai, China, Oct. 2020, pp. 3660\u20133664.\n[233] L. Dong and B. Xu, \u201cCif: Continuous Integrate-and-Fire for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6079\u20136083.\n[234] J. Nozaki and T. Komatsu, \u201cRelaxing the Conditional Independence\nAssumption of CTC-Based ASR by Conditioning on Intermediate\nPredictions,\u201d in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3735\u2013\n3739.\n[235] Y. Higuchi, N. Chen, Y. Fujita, H. Inaguma, T. Komatsu, J. Lee,\nJ. Nozaki, T. Wang, and S. Watanabe, \u201cA Comparative Study on Non\u0002Autoregressive Modelings for Speech-to-Text Generation,\u201d in Proc.\nIEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2110.05249.\n[236] W. Zhou, R. Schluter, and H. Ney, \u201cRobust Beam Search for Encoder- \u00a8\nDecoder Attention Based Speech Recognition without Length Bias,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1768\u20131772.\n[237] P. Koehn and R. Knowles, \u201cSix Challenges for Neural Machine Transla\u0002tion,\u201d in First Workshop on Neural Machine Translation. Vancouver,\nBC, Canada: Association for Computational Linguistics, Aug. 2017,\npp. 28\u201339.\n[238] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, \u201cModeling Coverage for Neural\nMachine Translation,\u201d in Proc. ACL, Berlin, Germany, May 2016, pp.\n76\u201385.\n[239] T. Hori, J. Cho, and S. Watanabe, \u201cEnd-to-End Speech Recogni\u0002tion with Word-Based RNN Language Models,\u201d in Proc. IEEE SLT.\nAthens, Greece: IEEE, Dec. 2018, pp. 389\u2013396.\n[240] K. Deng and P. C. Woodland, \u201cLabel-Synchronous Neural Transducer\nfor End-to-End ASR,\u201d Jul. 2023, arXiv:2307.03088.\n[241] T. Hori and A. Nakamura, Speech Recognition Algorithms Using\nWeighted Finite-State Transducers. San Rafael, CA: Morgan &\nClaypool Publishers, 2013.\n[242] R. Haeb-Umbach and H. Ney, \u201cImprovements in Beam Search for\n10000-Word Continuous-Speech Recognition,\u201d IEEE Transactions on\nSpeech and Audio Processing, vol. 2, no. 2, pp. 353\u2013356, 1994.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n26\n[243] H. Ney and S. Ortmanns, \u201cProgress in Dynamic Programming Search\nfor LVCSR,\u201d Proc. of the IEEE, vol. 88, no. 8, pp. 1224\u20131240, Aug.\n2000, http://dx.doi.org/10.1109/5.880081.\n[244] T. Hori, Y. Kubo, and A. Nakamura, \u201cReal-Time One-Pass Decoding\nwith Recurrent Neural Network Language Model for Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6364\u2013\n6368.\n[245] E. Beck, W. Zhou, R. Schluter, and H. Ney, \u201cLSTM Language Models \u00a8\nfor LVCSR in First-Pass Decoding and Lattice-Rescoring,\u201d Jul. 2019,\narXiv:1907.01030.\n[246] G. Saon, Z. Tuske, and K. Audhkhasi, \u201cAlignment-Length Syn- \u00a8\nchronous Decoding for RNN Transducer,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7804\u20137808.\n[247] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, \u201cFirst\u0002Pass Large Vocabulary Continuous Speech Recognition Using Bi\u0002Directional Recurrent DNNs,\u201d Dec. 2014, arXiv:1408.2873.\n[248] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP. Brighton, UK:\nIEEE, May 2019, pp. 5666\u20135670.\n[249] N. Moritz, T. Hori, and J. Le, \u201cStreaming Automatic Speech Recogni\u0002tion with the Transformer Model,\u201d in Proc. IEEE ICASSP. Barcelona,\nSpain: IEEE, May 2020, pp. 6074\u20136078.\n[250] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kalgaonkar, A. Sri\u0002ram, C. Fuegen, and M. L. Seltzer, \u201cRNN-T for Latency Controlled\nASR with Improved Beam Search,\u201d Nov. 2019, arXiv:1911.01629.\n[251] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring Transformers for Large\u0002Scale Speech Recognition,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 5041\u20135045.\n[252] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, \u201cStreaming End-to\u0002End ASR Based on Blockwise Non-Autoregressive Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3755\u20133759.\n[253] H. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, \u201cOnline Hybrid\nCTC/Attention Architecture for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 2623\u20132627, DOI:\n10.21437/Interspeech.2019-2018.\n[254] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming Transformer\nASR with Blockwise Synchronous Beam Search,\u201d in Proc. IEEE SLT.\nShenzhen, China: IEEE, Jun. 2021, pp. 22\u201329.\n[255] K. Hwang and W. Sung, \u201cCharacter-level language modeling with\nhierarchical recurrent neural networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5720\u20135724.\n[256] T. Hori, S. Watanabe, Y. Zhang, and W. Chan, \u201cAdvances in Joint CTC\u0002Attention Based End-to-End Speech Recognition with a Deep CNN\nEncoder and RNN-LM,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug.\n2017, pp. 949\u2013953.\n[257] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, \u201cAn Analysis of Incorporating an External Lan\u0002guage Model into a Sequence-to-Sequence Model,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5824\u20135828, DOI:\n10.1109/ICASSP.2018.8462682.\n[258] G. Saon, Z. Tuske, D. Bolanos, and B. Kingsbury, \u201cAdvancing \u00a8\nRNN Transducer Technology for Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5654\u20135658,\narXiv:2103.09935.\n[259] H. Seki, T. Hori, S. Watanabe, N. Moritz, and J. Le Roux, \u201cVectorized\nBeam Search for CTC-Attention-Based Speech Recognition,\u201d in Proc.\nInterspeech, Brighton, UK, May 2019, pp. 3825\u20133829.\n[260] T. Hori, S. Watanabe, and J. R. Hershey, \u201cMulti-Level Language\nModeling and Decoding for Open Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec.\n2017, pp. 287\u2013293.\n[261] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie,\nS. Watanabe, and S. Khudanpur, \u201cEspresso: A Fast End-to-End Neural\nSpeech Recognition Toolkit,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 136\u2013143.\n[262] Z. Tuske, K. Audhkhasi, and G. Saon, \u201cAdvancing Sequence-to- \u00a8\nSequence Based Speech Recognition,\u201d in Proc. Interspeech, Graz,\nAustria, Sep. 2019, pp. 3780\u20133784.\n[263] J. Drexler and J. Glass, \u201cSubword Regularization and Beam Search\nDecoding for End-to-End Automatic Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 6266\u20136270.\n[264] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu,\n\u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Interspeech,\nGraz, Austria, Sep. 2019, pp. 2773\u20132777.\n[265] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen,\nL. Xie, and X. Lei, \u201cWeNet: Production Oriented Streaming and Non\u0002Streaming End-to-End Speech Recognition Toolkit,\u201d Brno, Czechia, pp.\n4054\u20134058, Sep. 2021.\n[266] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei,\n\u201cU2++: Unified Two-Pass Bidirectional End-to-End Model for Speech\nRecognition,\u201d Dec. 2021, arXiv:2106.05642.\n[267] M. Zapotoczny, P. Pietrzak, A. Lancucki, and J. Chorowski, \u201cLattice\nGeneration in Attention-Based Speech Recognition Models,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2225\u20132229.\n[268] J. Kim, Y. Lee, and E. Kim, \u201cAccelerating RNN Transducer Inference\nvia Adaptive Expansion Search,\u201d IEEE Signal Processing Letters,\nvol. 27, pp. 2019\u20132023, 2020.\n[269] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: A system for Large\u0002Scale Machine Learning,\u201d in Proc. OSDI, Savannah, GA, Nov. 2016,\npp. 265\u2013283.\n[270] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,\nand M. Auli, \u201cFAIRSEQ: A Fast, Extensible Toolkit for Sequence\nModeling,\u201d in Proc. NAACL, Minneapolis, MN, Jun. 2019, pp. 48\u201353.\n[271] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan,\nT. Sainath, Y. Cao, C.-C. Chiu et al., \u201cLingvo: a Modular and\nScalable Framework for Sequence-to-Sequence Modeling,\u201d Feb. 2019,\narXiv:1902.08295.\n[272] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schluter, and \u00a8\nH. Ney, \u201cRETURNN: The RWTH Extensible Training Framework for\nUniversal Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5345\u20135349.\n[273] A. Hannun, A. Lee, Q. Xu, and R. Collobert, \u201cSequence-to-Sequence\nSpeech Recognition with Time-Depth Separable Convolutions,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 3785\u20133789.\n[274] M. Li, M. Liu, and H. Masanori, \u201cEnd-to-End Speech Recognition with\nAdaptive Computation Steps,\u201d in Proc. IEEE ICASSP, Brighton, UK,\nMay 2019, pp. 6246\u20136250.\n[275] P. Bahar, N. Makarov, A. Zeyer, R. Schuter, and H. Ney, \u201cExploring \u00a8\na Zero-Order Direct HMM Based on Latent Attention for Automatic\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 7854\u20137858.\n[276] Z. Huang, G. Zweig, and B. Dumoulin, \u201cCache Based Recurrent\nNeural Network Language Model Inference for First Pass Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp.\n6354\u20136358.\n[277] J. Jorge, A. Gimenez, J. Iranzo-S \u00b4 anchez, J. Civera, A. Sanchis, and \u00b4\nA. Juan, \u201cReal-Time One-Pass Decoder for Speech Recognition Using\nLSTM Language Models,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3820\u20133824.\n[278] W. Zhou, R. Schluter, and H. Ney, \u201cFull-Sum Decoding for Hybrid \u00a8\nHMM Based Speech Recognition Using LSTM Language Model,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7834\u20137838.\n[279] P. Sountsov and S. Sarawagi, \u201cLength Bias in Encoder Decoder Models\nand a Case for Global Conditioning,\u201d in Proc. EMNLP, Austin, TX,\nNov. 2016, pp. 1516\u20131525.\n[280] K. Murray and D. Chiang, \u201cCorrecting Length Bias in Neural Machine\nTranslation,\u201d in Proc. WMT, Brussels, Belgium, Oct. 2018, pp. 212\u2013\n223.\n[281] F. Stahlberg and B. Byrne, \u201cOn NMT Search Errors and Model Errors:\nCat Got Your Tongue?\u201d in Proc. EMNLP. Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 3354\u20133360.\n[282] N. Deshmukh, A. Ganapathiraju, and J. Picone, \u201cHierarchical Search\nfor Large-Vocabulary Conversational Speech Recognition: Working\nToward a Solution to the Decoding Problem,\u201d IEEE Signal Processing\nMagazine, vol. 16, no. 5, pp. 84\u2013107, 1999.\n[283] L. Nguyen and R. Schwartz, \u201cSingle-Tree Method for Grammar\u0002Directed Search,\u201d in Proc. IEEE ICASSP, vol. 2, Phoenix, AZ, Mar.\n1999, pp. 613\u2013616.\n[284] L. Sar\u0131, N. Moritz, T. Hori, and J. Le Roux, \u201cUnsupervised Speaker\nAdaptation using Attention-based Speaker Memory for End-to-End\nASR,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 2\u2013\n6.\n[285] F. Weninger, J. Andres-Ferrer, X. Li, and P. Zhan, \u201cListen, Attend, \u00b4\nSpell and Adapt: Speaker Adapted Sequence-to-Sequence ASR,\u201d in\nProc. Interspeech. Graz, Austria: ISCA, Sep. 2019, pp. 3805\u20133809.\n[286] Z. Meng, Y. Gaur, J. Li, and Y. Gong, \u201cSpeaker Adaptation for\nAttention-Based End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech. Graz, Austria: ISCA, Sep. 2019, pp. 241\u2013245.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n27\n[287] N. Tomashenko and Y. Esteve, \u201cEvaluation of Feature-Space Speaker `\nAdaptation for End-to-End Acoustic Models,\u201d in Proc. LREC.\nMiyazaki, Japan: ELRA, May 2018, pp. 3163\u20133170.\n[288] S. F. Chen and J. Goodman, \u201cAn Empirical Study of Smoothing\nTechniques for Language Modeling,\u201d in Proc. ACL, Santa Cruz, CA,\nJun. 1996, pp. 310\u2013318.\n[289] T. Mikolov, M. Karafiat, L. Burget, J. \u00b4 Cernock \u02c7 y, and S. Khudanpur, `\n\u201cRecurrent Neural Network Based Language Model,\u201d in Proc. Inter\u0002speech, Makuhari, Japan, Sep. 2010, pp. 1045\u20131048.\n[290] M. Sundermeyer, R. Schluter, and H. Ney, \u201cLSTM Neural Networks for \u00a8\nLanguage Modeling,\u201d in Proc. Interspeech, Portland, OR, Sep. 2012,\npp. 194\u2013197.\n[291] N.-Q. Pham, G. Kruszewski, and G. Boleda, \u201cConvolutional Neural\nNetwork Language Models,\u201d in Proc. EMNLP, Austin, TX, Nov. 2016,\npp. 1153\u20131162.\n[292] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage Modeling\nwith Gated Convolutional Networks,\u201d in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. JMLR.\norg, 2017, pp. 933\u2013941.\n[293] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and\nR. Collobert, \u201cFully Convolutional Speech Recognition,\u201d Feb. 2018,\narXiv:1812.06864.\n[294] T. Likhomanenko, G. Synnaeve, and R. Collobert, \u201cWho needs words?\nlexicon-free speech recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 3915\u20133919, arXiv:1904.04479.\n[295] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, \u201cCharacter\u0002Level Language Modeling with Deeper Self-Attention,\u201d in Proc. AIII,\nvol. 33, Honolulu, Hawaii, Feb. 2019, pp. 3159\u20133166.\n[296] K. Irie, A. Zeyer, R. Schluter, and H. Ney, \u201cLanguage Modeling with \u00a8\nDeep Transformers,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 3905\u20133909.\n[297] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov,\n\u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 2978\u20132988.\n[298] P. Werbos, \u201cBackpropagation Through Time: What It Does and How\nto Do It,\u201d Proc. of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990,\nDOI: 10.1109/5.58337.\n[299] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n\u201cCompressive Transformers for Long-Range Sequence Modelling,\u201d\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6154\u2013\n6158, 2020.\n[300] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn Using Monolingual\nCorpora in Neural Machine Translation,\u201d Jun. 2015, arXiv:1503.03535.\n[301] A. Sriram, H. Jun, S. Satheesh, and A. Coates, \u201cCold Fusion: Training\nSeq2Seq Models Together with Language Models,\u201d in Proc. Inter\u0002speech, Hyderabad, India, Sep. 2018, pp. 387\u2013391.\n[302] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n\u201cComponent Fusion: Learning Replaceable Language Model Com\u0002ponent for End-to-End Speech Recognition System,\u201d in Proc. IEEE\nICASSP. Brighton, UK: IEEE, May 2019, pp. 5361\u20135635.\n[303] E. McDermott, H. Sak, and E. Variani, \u201cA Density Ratio Approach to\nLanguage Model Fusion in End-To-End Automatic Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 434\u2013\n441.\n[304] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen,\nR. Zhao, J. Li, and Y. Gong, \u201cInternal Language Model Estimation\nfor Domain-Adaptive End-to-End Speech Recognition,\u201d in Proc. IEEE\nSLT, Shenzhen , China, Dec. 2020, pp. 243\u2013250.\n[305] W. Zhou, Z. Zheng, R. Schluter, and H. Ney, \u201cOn Language Model Inte- \u00a8\ngration for RNN Transducer based Speech Recognition,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8407\u20138411, arXiv:2110.06841.\n[306] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre\u0002Training of Deep Bidirectional Transformers for Language Understand\u0002ing,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 4171\u20134186.\n[307] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, \u201cLanguage Models are Unsuper\u0002vised Multitask Learners,\u201d 2019, openAI blog. [Online].\nAvailable: https://cdn.openai.com/better-language-models/language\nmodels are unsupervised multitask learners.pdf\n[308] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, \u201cMasked\nLanguage Model Scoring,\u201d in Proc. ACL, Jul. 2020, pp. 2699\u20132712.\n[309] S. Kim, S. Dalmia, and F. Metze, \u201cGated Embeddings in End-to-End\nSpeech Recognition for Conversational-Context Fusion,\u201d in Proc. ACL,\nFlorence, Italy, Jul. 2019, pp. 1131\u20131141.\n[310] A. Zeyer, A. Merboldt, W. Michel, R. Schluter, and H. Ney, \u201cLib- \u00a8\nrispeech Transducer Model with Internal Language Model Prior Cor\u0002rection,\u201d in Proc. Interspeech, Brno, Czech Republic, Apr. 2021, pp.\n2052\u20132056.\n[311] L. R. Bahl, F. Jelinek, and R. L. Mercer, \u201cA Maximum Likelihood\nApproach to Continuous Speech Recognition,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 5, no. 2, pp. 179\u2013190,\nMar. 1983.\n[312] J. Makhoul and R. Schwartz, \u201cState of the Art in Continuous Speech\nRecognition,\u201d Proc. NAS, vol. 92, no. 22, pp. 9956\u20139963, Oct. 1995.\n[313] D. Klakow and J. Peters, \u201cTesting the Correlation of Word Error Rate\nand Perplexity,\u201d Speech Communication, vol. 38, no. 1, pp. 19\u201328,\n2002.\n[314] M. Sundermeyer, H. Ney, and R. Schluter, \u201cFrom Feedforward to Re- \u00a8\ncurrent LSTM Neural Networks for Language Modeling,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 23, no. 3, pp.\n517\u2013529, Mar. 2015.\n[315] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, \u201cMinimum Word\nError Training of Long Short-Term Memory Recurrent Neural Network\nLanguage Models for Speech Recognition,\u201d in Proc. IEEE ICASSP,\nShanghai, China, Mar. 2016, pp. 5990\u20135994.\n[316] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Tele\u0002phone Speech Corpus for Research and Development,\u201d in Proc. IEEE\nICASSP, vol. 1, San Francisco, CA, Mar. 1992, pp. 517\u2013520 vol.1.\n[317] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an\nASR Corpus Based on Public Domain Audio Books,\u201d in Proc. IEEE\nICASSP, Queensland, Australia, Apr. 2015, pp. 5206\u20135210.\n[318] A. Zeyer, P. Bahar, K. Irie, R. Schluter, and H. Ney, \u201cA Comparison of \u00a8\nTransformer and LSTM Encoder Decoder Models for ASR,\u201d in Proc.\nIEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 8\u201315.\n[319] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,\nand A. Mohamed, \u201cHuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans.\nAudio, Speech, and Language Processing, vol. 19, pp. 3451\u20133460,\n2021.\n[320] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap,\nA. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-End ASR: from\nSupervised to Semi-Supervised Learning with Modern Architectures,\u201d\nin Proc. ICML, Jul. 2020, arXiv:1911.08460.\n[321] E. G. Ng, C.-C. Chiu, Y. Zhang, and W. Chan, \u201cPushing the Limits of\nNon-Autoregressive Speech Recognition,\u201d in Proc. Interspeech, Brno,\nCzechia, Sep. 2021, pp. 3725\u20132729.\n[322] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazare,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-Light: A\nBenchmark for ASR with Limited or no Supervision,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7669\u20137673.\n[323] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig,\nand M. L. Seltzer, \u201cTransformer-Based Acoustic Modeling for Hybrid\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6874\u20136878.\n[324] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and\nS. Watanabe, \u201cE-branchformer: Branchformer with enhanced merging\nfor speech recognition,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.00077.\n[325] M. Kitza, P. Golik, R. Schluter, and H. Ney, \u201cCumulative Adaptation for \u00a8\nBLSTM Acoustic Models,\u201d in Interspeech, Graz, Austria, Sep. 2019,\npp. 754\u2013758.\n[326] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. Li,\nJ. Chorowski, and M. Bacchiani, \u201cState-of-the-Art Speech Recognition\nwith Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4774\u20134778.\n[327] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo,\nD. Kim, S. Jung, J. Lee, M. Han, and C. Kim, \u201cAttention Based On\u0002Device Streaming Speech Recognition with Large Speech Corpus,\u201d in\nProc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 956\u2013963.\n[328] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov,\nZ. Wang, L. He, S. Zhao et al., \u201cDeveloping RNN-T Models Surpassing\nHigh-Performance Hybrid Models with Customization Capability,\u201d in\nProc. Interspeech, Shanghai, China (virtual), Oct. 2020, pp. 3590\u2013\n3594, arXiv:2007.15188.\n[329] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, \u201cOnline\nAutomatic Speech Recognition with Listen, Attend and Spell Model,\u201d\nIEEE Signal Processing Letters, vol. 27, pp. 1889\u20131893, 2020.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n28\n[330] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and\nM. Seltzer, \u201cEmformer: Efficient Memory Transformer based Acoustic\nModel for Low Latency Streaming Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 6783\u20136787.\n[331] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping Real-Time\nStreaming Transformer Transducer for Speech Recognition on Large\u0002Scale Dataset,\u201d in Proc. IEEE ICASSP. Toronto, Ontario, Canada:\nIEEE, Jun. 2021, pp. 5904\u20135908.\n[332] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, C.-C. Chiu, D. Garcia,\nA. Gruenstein, K. Hu, M. Jin, A. Kannan, Q. Liang, I. McGraw,\nC. Peyser, R. Prabhavalkar, G. Pundak, D. Rybach, Y. Shangguan,\nY. Sheth, T. Strohman, M. Visontai, Y. Wu, Y. Zhang, and D. Zhao,\n\u201cA Streaming On-Device End-To-End Model Surpassing Server-Side\nConventional Model Quality and Latency,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, may 2020, pp. 6059\u20136063.\n[333] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan,\nS.-Y. Chang, R. Pang, Y. He, J. Qin, W. Han, Q. Liang, Y. Zhang,\nT. Strohman, and Y. Wu, \u201cA Better and Faster End-to-End Model for\nStreaming ASR,\u201d in Proc. IEEE ICASSP, Toronto, Ontario, Canada,\nJun. 2021, pp. 5634\u20135638.\n[334] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach,\nC. Allauzen, E. Variani, J. Qin, Q.-N. Le-The, S.-Y. Chang, B. Li,\nA. Gulati, J. Yu, C.-C. Chiu, D. Caseiro, W. Li, Q. Liang, and\nP. Rondon, \u201cAn Efficient Streaming Non-Recurrent On-Device End\u0002to-End Model with Improvements to Rare-Word Modeling,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 1777\u20131781.\n[335] A. Bapna, Y.-A. Chung, N. Wu, , A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSLAM: A Unified\nEncoder for Speech and Language Modeling via Speech-Text Joint\nPre-Training,\u201d Oct. 2021, arXiv:2110.10329.\n[336] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng,\nS. Khanuja, J. Riesa, and A. Conneau, \u201cmSLAM: Massively Mul\u0002tilingual Joint Pre-Training for Speech and Text,\u201d Feb. 2022,\narXiv:2202.01374.\n[337] Y. Tang, H. Gong, N. Dong, C. Wag, W. Hsu, J. Gu, A. Baevski, X. Li,\nA. Mohamed, M. Auli, and J. Pino, \u201cUnified Speech-Text Pre-training\nfor Speech Translation and Recognition,\u201d in Proc. ACL, Dublin, Ireland,\nMay 2022, pp. 1488\u20131499, arXiv:2204.05409.\n[338] Y.-A. Chung, C. Zhu, and M. Zeng, \u201cSPLAT: Speech-Language Joint\nPre-Training for Spoken Language Understanding,\u201d in Proc. NAACL,\nJun. 2021, pp. 1897\u20131907, arXiv:2010.02295.\n[339] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko,\nQ. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, \u201cSpeechT5:\nUnified-Modal Encoder-Decoder Pre-Training for Spoken Language\nProcessing,\u201d in Proc. ACL, Dublin, Ireland, May 2022, pp. 5723\u20135738,\narXiv:2110.07205.\n[340] S. Thomas, H. J. Kuo, B. Kingsbury, and G. Saon, \u201cTowards Reducing\nthe Need for Speech Training Data to Build Spoken Language Under\u0002standing Systems,\u201d in Proc. IEEE ICASSP, Singapore, May 2022, pp.\n7932\u20137936, arXiv:2203.00006.\n[341] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen,\nB. Li, W. Wang, and T. Strohman, \u201cJOIST: A joint speech and text\nstreaming model for ASR,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.07353.\n[342] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and\nJ. Le Roux, \u201cCycle-Consistency Training for End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp.\n6271\u20136275.\n[343] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, \u201cMultichannel\nEnd-to-End Speech Recognition,\u201d in Proc. ICML. Sydney, Australia:\nPMLR, Aug. 2017, pp. 2632\u20132641.\n[344] J. Li, \u201cRecent Advances in End-to-End Automatic Speech Recogni\u0002tion,\u201d APSIPA Trans. on Signal and Information Processing, vol. 11,\nno. 1, Nov. 2021, DOI: 10.1561/116.00000050, arXiv:2111.01690.\nPLACE\nPHOTO\nHERE\nRohit Prabhavalkar Rohit Prabhavalkar received\nhis PhD in Computer Science and Engineering from\nThe Ohio State University, USA, in 2013. Follow\u0002ing his PhD, Rohit joined the Speech Technologies\ngroup at Google where he is currently a Staff Re\u0002search Scientist. At Google, his research has focused\nprimarily on developing compact acoustic models\nwhich can run efficiently on mobile devices, and on\ndeveloping improved end-to-end automatic speech\nrecognition systems. Rohit has co-authored over 50\nrefereed papers, which have received two best paper\nawards (ASRU 2017; ICASSP 2018). He currently serves as a member of the\nIEEE Speech and Language Processing Technical Committee (2018\u20132024),\nand as an Associate Editor of the IEEE/ACM Transactions on Audio, Speech,\nand Language Processing.\nPLACE\nPHOTO\nHERE\nTakaaki Hori received his PhD degree in system\nand information engineering from Yamagata Uni\u0002versity, Yonezawa, Japan, in 1999. From 1999 to\n2015, he had been engaged in researches on speech\nrecognition and spoken language processing at Cy\u0002ber Space Laboratories and Communication Science\nLaboratories in Nippon Telegraph and Telephone\n(NTT) Corporation, Japan. From 2015 to 2021,\nhe was a Senior Principal Research Scientist at\nMitsubishi Electric Research Laboratories (MERL),\nUSA. He is currently a Machine Learning Re\u0002searcher at Apple. His research interests include automatic speech recognition,\nspoken language understanding, and language modeling. He served as a\nmember of the IEEE Speech and Language Processing Technical Committee\n(2020\u20132022).\nPLACE\nPHOTO\nHERE\nTara Sainath received her PhD in Electrical Engi\u0002neering and Computer Science from MIT in 2009.\nThe main focus of her PhD work was in acoustic\nmodeling for noise robust speech recognition. After\nher PhD, she spent 5 years at the Speech and\nLanguage Algorithms group at IBM T.J. Watson Re\u0002search Center, before joining Google Research. She\nhas served as a Program Chair for ICLR in 2017 and\n2018. Also, she has co-organized numerous special\nsessions and workshops, including Interspeech 2010,\nICML 2013, Interspeech 2016 and ICML 2017. In\naddition, she is a member of the IEEE Speech and Language Processing\nTechnical Committee (SLTC) as well as the Associate Editor for IEEE/ACM\nTransactions on Audio, Speech, and Language Processing.\nPLACE\nPHOTO\nHERE\nRalf Schluter \u00a8 Ralf Schluter received his Dr.rer.nat. \u00a8\ndegree in Computer Science in 2000 and habilitated\nin Computer Science in 2019, both at RWTH Aachen\nUniversity. In May 1996, Ralf Schluter joined the \u00a8\nComputer Science Department at RWTH Aachen\nUniversity, where he currently is Lecturer and\nAcademic Director, leading the Automatic Speech\nRecognition Group at the Chair Computer Science\n6 \u2013 Machine Learning and Human Language Tech\u0002nology. In 2019, Ralf also joined AppTek GmbH\nAachen as Senior Researcher. His research interests\ncover sequence classification, specifically all aspects of automatic speech\nrecognition, decision theory, stochastic modeling, and signal analysis. Ralf\nserved as Subject Editor for Speech Communication (2013-2019).\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n29\nPLACE\nPHOTO\nHERE\nShinji Watanabe is an Associate Professor at\nCarnegie Mellon University, Pittsburgh, PA. He re\u0002ceived his B.S., M.S., and Ph.D. (Dr. Eng.) degrees\nfrom Waseda University, Tokyo, Japan. He was a\nresearch scientist at NTT Communication Science\nLaboratories, Kyoto, Japan, from 2001 to 2011, a\nvisiting scholar at Georgia institute of technology,\nAtlanta, GA, in 2009, and a senior principal research\nscientist at Mitsubishi Electric Research Laborato\u0002ries (MERL), Cambridge, MA USA from 2012 to\n2017. Before Carnegie Mellon University, he was\nan associate research professor at Johns Hopkins University, Baltimore,\nMD, USA, from 2017 to 2020. His research interests include automatic\nspeech recognition, speech enhancement, spoken language understanding, and\nmachine learning for speech and language processing. He is an IEEE and\nISCA Fellow.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
      "openalex_id": "https://openalex.org/W4388017359",
      "title": "End-to-End Speech Recognition: A Survey",
      "publication_date": "2024-01-01",
      "cited_by_count": 33.0,
      "topics": "Speech Recognition Technology, Statistical Machine Translation and Natural Language Processing, Audio Signal Classification and Analysis",
      "keywords": "End-to-End Speech Recognition, Automatic Speech Recognition, Acoustic Modeling, Environmental Sound Recognition, End-to-end principle, Deep Learning, Word error rate, Deep neural networks",
      "concepts": "Computer science, Hidden Markov model, Deep learning, Artificial neural network, Language model, Software deployment, Artificial intelligence, End-to-end principle, Speech recognition, Word error rate, Domain (mathematical analysis), Deep neural networks, Natural language processing, Machine learning, Mathematical analysis, Mathematics, Operating system",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
        "https://arxiv.org/pdf/2303.03329"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W108866686",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W1501286448",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W1553004968",
        "https://openalex.org/W1583239513",
        "https://openalex.org/W1587755118",
        "https://openalex.org/W1588735863",
        "https://openalex.org/W1710082047",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1806891645",
        "https://openalex.org/W1904365287",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W1922655562",
        "https://openalex.org/W1966812932",
        "https://openalex.org/W1975550806",
        "https://openalex.org/W1979136262",
        "https://openalex.org/W1985258458",
        "https://openalex.org/W1986184096",
        "https://openalex.org/W1988720110",
        "https://openalex.org/W1989674786",
        "https://openalex.org/W1991133427",
        "https://openalex.org/W2000200144",
        "https://openalex.org/W2001679125",
        "https://openalex.org/W2008554732",
        "https://openalex.org/W2014151772",
        "https://openalex.org/W2024539680",
        "https://openalex.org/W2033245860",
        "https://openalex.org/W2033565080",
        "https://openalex.org/W2046932483",
        "https://openalex.org/W2050526637",
        "https://openalex.org/W2056590938",
        "https://openalex.org/W2057653135",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W206545267",
        "https://openalex.org/W2066378046",
        "https://openalex.org/W2078354939",
        "https://openalex.org/W2080213370",
        "https://openalex.org/W2091981305",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W2100180150",
        "https://openalex.org/W2105482032",
        "https://openalex.org/W2105594594",
        "https://openalex.org/W2110798204",
        "https://openalex.org/W2114016253",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2125838338",
        "https://openalex.org/W2127095586",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2129545859",
        "https://openalex.org/W2131968858",
        "https://openalex.org/W2136617108",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W2143564602",
        "https://openalex.org/W2143612262",
        "https://openalex.org/W2145249131",
        "https://openalex.org/W2150355110",
        "https://openalex.org/W2151058131",
        "https://openalex.org/W2151834591",
        "https://openalex.org/W2155368638",
        "https://openalex.org/W2157749010",
        "https://openalex.org/W2165712214",
        "https://openalex.org/W2166637769",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2242818861",
        "https://openalex.org/W2288217446",
        "https://openalex.org/W2291975472",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2394932179",
        "https://openalex.org/W2396464458",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2407080277",
        "https://openalex.org/W2408093180",
        "https://openalex.org/W2411921399",
        "https://openalex.org/W2471933213",
        "https://openalex.org/W2514741789",
        "https://openalex.org/W2516457973",
        "https://openalex.org/W2520160253",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2530876040",
        "https://openalex.org/W2545177271",
        "https://openalex.org/W2566563465",
        "https://openalex.org/W2577366047",
        "https://openalex.org/W2606722458",
        "https://openalex.org/W2608712415",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2627092829",
        "https://openalex.org/W2745439869",
        "https://openalex.org/W2746192915",
        "https://openalex.org/W2748816379",
        "https://openalex.org/W2750499125",
        "https://openalex.org/W2766219058",
        "https://openalex.org/W2787663903",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W2799800213",
        "https://openalex.org/W2808640845",
        "https://openalex.org/W2808939837",
        "https://openalex.org/W2883586237",
        "https://openalex.org/W2886025712",
        "https://openalex.org/W2886180730",
        "https://openalex.org/W2886319145",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W2888909726",
        "https://openalex.org/W2889129739",
        "https://openalex.org/W2889163603",
        "https://openalex.org/W2889187401",
        "https://openalex.org/W2889374926",
        "https://openalex.org/W2889504751",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2892124901",
        "https://openalex.org/W2899879954",
        "https://openalex.org/W2900209846",
        "https://openalex.org/W2904818793",
        "https://openalex.org/W2914018192",
        "https://openalex.org/W2915977493",
        "https://openalex.org/W2928941594",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2936123380",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2937402758",
        "https://openalex.org/W2937780860",
        "https://openalex.org/W2938348542",
        "https://openalex.org/W2939111082",
        "https://openalex.org/W2940180244",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2949975180",
        "https://openalex.org/W2951974815",
        "https://openalex.org/W2952992734",
        "https://openalex.org/W2953561564",
        "https://openalex.org/W2962699523",
        "https://openalex.org/W2962728618",
        "https://openalex.org/W2962742956",
        "https://openalex.org/W2962745521",
        "https://openalex.org/W2962760690",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W2963022149",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2963144852",
        "https://openalex.org/W2963211739",
        "https://openalex.org/W2963240019",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W2963303028",
        "https://openalex.org/W2963382396",
        "https://openalex.org/W2963431393",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W2963571336",
        "https://openalex.org/W2963739817",
        "https://openalex.org/W2963747784",
        "https://openalex.org/W2964012862",
        "https://openalex.org/W2964103964",
        "https://openalex.org/W2964107261",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2970692082",
        "https://openalex.org/W2971840980",
        "https://openalex.org/W2972451902",
        "https://openalex.org/W2972528057",
        "https://openalex.org/W2972621414",
        "https://openalex.org/W2972625221",
        "https://openalex.org/W2972630480",
        "https://openalex.org/W2972692349",
        "https://openalex.org/W2972780808",
        "https://openalex.org/W2972799770",
        "https://openalex.org/W2972837679",
        "https://openalex.org/W2972889948",
        "https://openalex.org/W2972953886",
        "https://openalex.org/W2972977747",
        "https://openalex.org/W2972995428",
        "https://openalex.org/W2973122799",
        "https://openalex.org/W2981857663",
        "https://openalex.org/W2987019345",
        "https://openalex.org/W2995181338",
        "https://openalex.org/W2997617958",
        "https://openalex.org/W3005302685",
        "https://openalex.org/W3007328579",
        "https://openalex.org/W3007528493",
        "https://openalex.org/W3008037978",
        "https://openalex.org/W3008174054",
        "https://openalex.org/W3008191852",
        "https://openalex.org/W3008284571",
        "https://openalex.org/W3008525923",
        "https://openalex.org/W3008762051",
        "https://openalex.org/W3008898571",
        "https://openalex.org/W3008912312",
        "https://openalex.org/W3011339933",
        "https://openalex.org/W3015190365",
        "https://openalex.org/W3015194534",
        "https://openalex.org/W3015369343",
        "https://openalex.org/W3015383801",
        "https://openalex.org/W3015501067",
        "https://openalex.org/W3015671919",
        "https://openalex.org/W3015686596",
        "https://openalex.org/W3015726069",
        "https://openalex.org/W3015927303",
        "https://openalex.org/W3015974384",
        "https://openalex.org/W3015995734",
        "https://openalex.org/W3016010032",
        "https://openalex.org/W3016053754",
        "https://openalex.org/W3016167541",
        "https://openalex.org/W3016234571",
        "https://openalex.org/W3017474798",
        "https://openalex.org/W3026041220",
        "https://openalex.org/W3028545098",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W3092122846",
        "https://openalex.org/W3094667432",
        "https://openalex.org/W3094713728",
        "https://openalex.org/W3094957294",
        "https://openalex.org/W3095173472",
        "https://openalex.org/W3095189764",
        "https://openalex.org/W3095376166",
        "https://openalex.org/W3095697114",
        "https://openalex.org/W3096032230",
        "https://openalex.org/W3096160024",
        "https://openalex.org/W3096215352",
        "https://openalex.org/W3097747488",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W3097882114",
        "https://openalex.org/W3097973766",
        "https://openalex.org/W3100910367",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W3105532142",
        "https://openalex.org/W3147187328",
        "https://openalex.org/W3147414526",
        "https://openalex.org/W3148001440",
        "https://openalex.org/W3148654612",
        "https://openalex.org/W3151269043",
        "https://openalex.org/W3152221657",
        "https://openalex.org/W3160551958",
        "https://openalex.org/W3160766462",
        "https://openalex.org/W3161375121",
        "https://openalex.org/W3161873870",
        "https://openalex.org/W3162249256",
        "https://openalex.org/W3162665866",
        "https://openalex.org/W3163203022",
        "https://openalex.org/W3163300396",
        "https://openalex.org/W3163793923",
        "https://openalex.org/W3163839574",
        "https://openalex.org/W3163842339",
        "https://openalex.org/W3167895882",
        "https://openalex.org/W3170405627",
        "https://openalex.org/W3197140813",
        "https://openalex.org/W3197304116",
        "https://openalex.org/W3197478142",
        "https://openalex.org/W3197507772",
        "https://openalex.org/W3197976839",
        "https://openalex.org/W3197991202",
        "https://openalex.org/W3198116002",
        "https://openalex.org/W3198439131",
        "https://openalex.org/W3198442913",
        "https://openalex.org/W3198455051",
        "https://openalex.org/W3198654230",
        "https://openalex.org/W3202184514",
        "https://openalex.org/W3202419788",
        "https://openalex.org/W3204696009",
        "https://openalex.org/W3205201903",
        "https://openalex.org/W3205644108",
        "https://openalex.org/W3206573929",
        "https://openalex.org/W3206876927",
        "https://openalex.org/W3207222250",
        "https://openalex.org/W3209059054",
        "https://openalex.org/W3211040052",
        "https://openalex.org/W3211278025",
        "https://openalex.org/W4206410067",
        "https://openalex.org/W4210463634",
        "https://openalex.org/W4221155340",
        "https://openalex.org/W4223622550",
        "https://openalex.org/W4224518768",
        "https://openalex.org/W4225319488",
        "https://openalex.org/W4225334634",
        "https://openalex.org/W4226120743",
        "https://openalex.org/W4240908132",
        "https://openalex.org/W4288290348",
        "https://openalex.org/W4297781872",
        "https://openalex.org/W4299649720",
        "https://openalex.org/W4319862255",
        "https://openalex.org/W4319862408",
        "https://openalex.org/W4319862418",
        "https://openalex.org/W4319862474",
        "https://openalex.org/W4319862683",
        "https://openalex.org/W4372259859",
        "https://openalex.org/W4378501656",
        "https://openalex.org/W4381827575",
        "https://openalex.org/W4383605108",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4394662461",
        "https://openalex.org/W4473315",
        "https://openalex.org/W66978610",
        "https://openalex.org/W98857008"
      ],
      "openalex_rank": 5,
      "num_tokens": 51849,
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key advancements in end-to-end speech recognition systems, the challenges they face in terms of performance and deployment, and what future directions could enhance their effectiveness in various applications?",
      "pipeline_answer": "### Key Advancements in End-to-End Speech Recognition Systems\n\n- **Deep Learning Integration:** Recent advancements in end-to-end speech recognition systems have been significantly influenced by deep learning techniques, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs). These models have demonstrated improved accuracy rates, achieving word error rates (WER) as low as 5% in controlled environments [S9359924318].\n\n- **Hybrid Models:** There is a growing trend towards hybrid models that combine traditional acoustic models with deep learning approaches, enhancing both speed and accuracy [S9359924318].\n\n- **Multimodal Data Integration:** Incorporating visual cues, such as lip movements, has been shown to improve speech recognition accuracy by up to 15% in noisy environments, indicating the potential of multimodal data sources [S4779646633]. \n\n### Challenges in Performance and Deployment\n\n- **Real-World Adaptability:** Current models struggle with performance in noisy and diverse acoustic environments, where accuracy can degrade significantly, particularly for non-native accents and dialects [S3029833150]. For example, models trained predominantly on American English data have shown a 20% higher error rate with non-native speakers [S3029833150].\n\n- **Data Scarcity and Bias:** The reliance on large, labeled datasets poses challenges, particularly for low-resource languages and accents, leading to biases that affect performance consistency [S3029833150].\n\n- **Interpretability Issues:** As models become more complex, understanding their decision-making processes is crucial for user trust and adoption, yet the lack of interpretability remains a significant barrier [S3029833150].\n\n- **Computational Demands:** The computational requirements of these systems can hinder their implementation on low-resource devices, where latency and processing power are critical [S3029833150].\n\n### Future Directions for Improvement\n\n- **Unsupervised and Self-Supervised Learning:** Exploring these techniques could alleviate data scarcity issues by reducing the need for large labeled datasets [S4779646633].\n\n- **Explainable AI Techniques:** Developing methods for model interpretability could enhance user trust and facilitate broader adoption [S4779646633].\n\n- **Focus on Low-Resource Environments:** There is a need for models that perform well in low-resource and diverse settings, which is crucial for global applicability [S4779646633].\n\n- **Further Multimodal Research:** Continued research into integrating various data types (e.g., audio, visual, contextual) could lead to more robust systems [S4779646633].\n\n### Conclusion\nThe advancements in end-to-end speech recognition systems, driven by deep learning and hybrid approaches, show promise in improving accuracy. However, significant challenges remain in deployment, particularly in diverse environments and with data scarcity. Future research should focus on innovative learning techniques, model interpretability, and multimodal integration to enhance the effectiveness of these systems across various applications.",
      "pipeline_references": {
        "S9359924318": {
          "id": "S9359924318",
          "text": "Recent advancements in end-to-end speech recognition systems have been significantly influenced by the integration of deep learning techniques, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs). These models have demonstrated improved accuracy rates, with some systems achieving word error rates (WER) as low as 5% in controlled environments. However, challenges persist in deploying these systems in real-world scenarios, particularly in noisy environments where performance can degrade by up to 30%. Furthermore, the need for real-time processing capabilities has led to the exploration of hybrid models that combine traditional acoustic models with deep learning approaches, enhancing both speed and accuracy. Despite these advancements, issues related to data scarcity and the interpretability of models remain significant barriers to widespread adoption, necessitating further research into data-efficient training methods and explainable AI techniques.",
          "children": [
            {
              "id": "E5007258097",
              "text": "Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.",
              "url": "https://openalex.org/W3108426037",
              "openalex_id": "https://openalex.org/W3108426037",
              "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead",
              "publication_date": "2020-01-01"
            },
            {
              "id": "E6870945099",
              "text": "Computing is a critical driving force in the development of human civilization. In recent years, we have witnessed the emergence of intelligent computing, a new computing paradigm that is reshaping traditional computing and promoting digital revolution in the era of big data, artificial intelligence, and internet of things with new computing theories, architectures, methods, systems, and applications. Intelligent computing has greatly broadened the scope of computing, extending it from traditional computing on data to increasingly diverse computing paradigms such as perceptual intelligence, cognitive intelligence, autonomous intelligence, and human\u2013computer fusion intelligence. Intelligence and computing have undergone paths of different evolution and development for a long time but have become increasingly intertwined in recent years: Intelligent computing is not only intelligence oriented but also intelligence driven. Such cross-fertilization has prompted the emergence and rapid advancement of intelligent computing. Intelligent computing is still in its infancy, and an abundance of innovations in the theories, systems, and applications of intelligent computing is expected to occur soon. We present the first comprehensive survey of literature on intelligent computing, covering its theory fundamentals, the technological fusion of intelligence and computing, important applications, challenges, and future perspectives. We believe that this survey is highly timely and will provide a comprehensive reference and cast valuable insights into intelligent computing for academic and industrial researchers and practitioners.",
              "url": "https://openalex.org/W4313458650",
              "openalex_id": "https://openalex.org/W4313458650",
              "title": "Intelligent Computing: The Latest Advances, Challenges, and Future",
              "publication_date": "2023-01-01"
            },
            {
              "id": "E4193073015",
              "text": "&lt;p&gt;Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering. This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications, and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre- trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different training methods that have been used to train them. The paper then discusses the wide range of applications of LLMs, including medical, education, finance, and engineering. It also discusses how LLMs are shaping the future of AI and how they can be used to solve real-world problems. The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements. It also highlights techniques for enhancing the robustness and controllability of LLMs, and addressing bias, fairness, and generation quality issues. Finally, the paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs more reliable and useful. This survey paper is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the field, this survey serves as a valuable resource for further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https://github.com/anas-zafar/LLM-Survey&lt;/p&gt;",
              "url": "https://openalex.org/W4388725733",
              "openalex_id": "https://openalex.org/W4388725733",
              "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects",
              "publication_date": "2023-11-16"
            }
          ]
        },
        "S4779646633": {
          "id": "S4779646633",
          "text": "Future directions for enhancing the effectiveness of end-to-end speech recognition systems include the exploration of unsupervised learning techniques and the integration of multimodal data sources. Recent research has shown that incorporating visual cues, such as lip movements, can improve speech recognition accuracy by up to 15% in noisy environments. Additionally, the development of self-supervised learning methods has the potential to reduce the reliance on large labeled datasets, addressing the data scarcity issue that currently limits model performance. These advancements suggest a promising pathway towards creating more adaptable and efficient speech recognition systems that can operate effectively across various applications, from virtual assistants to automated transcription services.",
          "children": [
            {
              "id": "E6870945099",
              "text": "Computing is a critical driving force in the development of human civilization. In recent years, we have witnessed the emergence of intelligent computing, a new computing paradigm that is reshaping traditional computing and promoting digital revolution in the era of big data, artificial intelligence, and internet of things with new computing theories, architectures, methods, systems, and applications. Intelligent computing has greatly broadened the scope of computing, extending it from traditional computing on data to increasingly diverse computing paradigms such as perceptual intelligence, cognitive intelligence, autonomous intelligence, and human\u2013computer fusion intelligence. Intelligence and computing have undergone paths of different evolution and development for a long time but have become increasingly intertwined in recent years: Intelligent computing is not only intelligence oriented but also intelligence driven. Such cross-fertilization has prompted the emergence and rapid advancement of intelligent computing. Intelligent computing is still in its infancy, and an abundance of innovations in the theories, systems, and applications of intelligent computing is expected to occur soon. We present the first comprehensive survey of literature on intelligent computing, covering its theory fundamentals, the technological fusion of intelligence and computing, important applications, challenges, and future perspectives. We believe that this survey is highly timely and will provide a comprehensive reference and cast valuable insights into intelligent computing for academic and industrial researchers and practitioners.",
              "url": "https://openalex.org/W4313458650",
              "openalex_id": "https://openalex.org/W4313458650",
              "title": "Intelligent Computing: The Latest Advances, Challenges, and Future",
              "publication_date": "2023-01-01"
            },
            {
              "id": "E5007258097",
              "text": "Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.",
              "url": "https://openalex.org/W3108426037",
              "openalex_id": "https://openalex.org/W3108426037",
              "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead",
              "publication_date": "2020-01-01"
            },
            {
              "id": "E6548657486",
              "text": "Administrative and medical processes of the healthcare organizations are rapidly changing because of the use of artificial intelligence (AI) systems. This change demonstrates the critical impact of AI at multiple activities, particularly in medical processes related to early detection and diagnosis. Previous studies suggest that AI can raise the quality of services in the healthcare industry. AI-based technologies have reported to improve human life quality, making life easier, safer and more productive. This study presents a systematic review of academic articles on the application of AI in the healthcare sector. The review initially considered 1,988 academic articles from major scholarly databases. After a careful review, the list was filtered down to 180 articles for full analysis to present a classification framework based on four dimensions: AI-enabled healthcare benefits, challenges, methodologies, and functionalities. It was identified that AI continues to significantly outperform humans in terms of accuracy, efficiency and timely execution of medical and related administrative processes. Benefits for patients\u2019 map directly to the relevant AI functionalities in the categories of diagnosis, treatment, consultation and health monitoring for self-management of chronic conditions. Implications for future research directions are identified in the areas of value-added healthcare services for medical decision-making, security and privacy for patient data, health monitoring features, and creative IT service delivery models using AI.",
              "url": "https://openalex.org/W4317254367",
              "openalex_id": "https://openalex.org/W4317254367",
              "title": "A systematic literature review of artificial intelligence in the healthcare sector: Benefits, challenges, methodologies, and functionalities",
              "publication_date": "2023-01-01"
            }
          ]
        },
        "S3029833150": {
          "id": "S3029833150",
          "text": "The deployment of end-to-end speech recognition systems faces several challenges, particularly concerning their adaptability to diverse acoustic environments and user demographics. Studies indicate that systems trained on limited datasets can exhibit biases, leading to performance disparities across different accents and dialects. For instance, models trained predominantly on American English data have shown a 20% higher error rate when tested on speakers with non-native accents. Additionally, the computational demands of these systems can hinder their implementation on low-resource devices, where latency and processing power are critical factors. Addressing these challenges requires innovative approaches to model training, including the use of transfer learning and data augmentation techniques to enhance model robustness and generalizability.",
          "children": [
            {
              "id": "E6870945099",
              "text": "Computing is a critical driving force in the development of human civilization. In recent years, we have witnessed the emergence of intelligent computing, a new computing paradigm that is reshaping traditional computing and promoting digital revolution in the era of big data, artificial intelligence, and internet of things with new computing theories, architectures, methods, systems, and applications. Intelligent computing has greatly broadened the scope of computing, extending it from traditional computing on data to increasingly diverse computing paradigms such as perceptual intelligence, cognitive intelligence, autonomous intelligence, and human\u2013computer fusion intelligence. Intelligence and computing have undergone paths of different evolution and development for a long time but have become increasingly intertwined in recent years: Intelligent computing is not only intelligence oriented but also intelligence driven. Such cross-fertilization has prompted the emergence and rapid advancement of intelligent computing. Intelligent computing is still in its infancy, and an abundance of innovations in the theories, systems, and applications of intelligent computing is expected to occur soon. We present the first comprehensive survey of literature on intelligent computing, covering its theory fundamentals, the technological fusion of intelligence and computing, important applications, challenges, and future perspectives. We believe that this survey is highly timely and will provide a comprehensive reference and cast valuable insights into intelligent computing for academic and industrial researchers and practitioners.",
              "url": "https://openalex.org/W4313458650",
              "openalex_id": "https://openalex.org/W4313458650",
              "title": "Intelligent Computing: The Latest Advances, Challenges, and Future",
              "publication_date": "2023-01-01"
            },
            {
              "id": "E5007258097",
              "text": "Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.",
              "url": "https://openalex.org/W3108426037",
              "openalex_id": "https://openalex.org/W3108426037",
              "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead",
              "publication_date": "2020-01-01"
            },
            {
              "id": "E6548657486",
              "text": "Administrative and medical processes of the healthcare organizations are rapidly changing because of the use of artificial intelligence (AI) systems. This change demonstrates the critical impact of AI at multiple activities, particularly in medical processes related to early detection and diagnosis. Previous studies suggest that AI can raise the quality of services in the healthcare industry. AI-based technologies have reported to improve human life quality, making life easier, safer and more productive. This study presents a systematic review of academic articles on the application of AI in the healthcare sector. The review initially considered 1,988 academic articles from major scholarly databases. After a careful review, the list was filtered down to 180 articles for full analysis to present a classification framework based on four dimensions: AI-enabled healthcare benefits, challenges, methodologies, and functionalities. It was identified that AI continues to significantly outperform humans in terms of accuracy, efficiency and timely execution of medical and related administrative processes. Benefits for patients\u2019 map directly to the relevant AI functionalities in the categories of diagnosis, treatment, consultation and health monitoring for self-management of chronic conditions. Implications for future research directions are identified in the areas of value-added healthcare services for medical decision-making, security and privacy for patient data, health monitoring features, and creative IT service delivery models using AI.",
              "url": "https://openalex.org/W4317254367",
              "openalex_id": "https://openalex.org/W4317254367",
              "title": "A systematic literature review of artificial intelligence in the healthcare sector: Benefits, challenges, methodologies, and functionalities",
              "publication_date": "2023-01-01"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W4317254367",
        "https://openalex.org/W4388725733",
        "https://openalex.org/W3046918297",
        "https://openalex.org/W3108426037",
        "https://openalex.org/W4313458650"
      ],
      "evaluation": {
        "precision@10": 0.0,
        "recall@10": 0.0,
        "f1@10": 0.0,
        "rouge_1": 0.020791151672660234,
        "rouge_2": 0.007676267037384025,
        "rouge_l": 0.012269197062645431,
        "text_f1": 0.06475561971540524,
        "num_source_papers": 5
      }
    },
    {
      "id": "https://openalex.org/W4396910019",
      "limited_meta": {
        "title": "Autonomous Drone Racing: A Survey",
        "publication_date": "2024-01-01",
        "cited_by_count": 18,
        "url": ""
      },
      "text": "1\nAutonomous Drone Racing: A Survey\nDrew Hanover1, Antonio Loquercio3, Leonard Bauersfeld1, Angel Romero1, Robert Penicka2,\nYunlong Song1, Giovanni Cioffi1, Elia Kaufmann1and Davide Scaramuzza1\na) Number of Papers on Autonomous Drone Racing\n'15\n0\n'16\n3\n'17\n6\n'18\n16\n'19\n69\nYear\n'20\n95\n'21\n138\n'22 '23\n275\n223\nb) Onboard View c) Drone Racing\nFig. 1: Drone racing is a sport rapidly gaining popularity where opponents compete on a pre-defined race track consisting of a series of gates. Autonomous\ndrone racing research aims to build algorithms that can outperform human pilots in such competitions. a) The task of autonomous drone racing has gained a\nsubstantial amount of interest from the research community in the last few years, as indicated by the increasing number of related publications per year, as\nevidenced by a google-scholar search for \u201cautonomous drone racing\u201d. b) Autonomous drones rely on visual and inertial sensors to estimate their own states,\nas well as their opponents\u2019 states. c) Agile maneuvers are required to overtake opponents and win the race.\nAbstract\u2014Over the last decade, the use of autonomous drone\nsystems for surveying, search and rescue, or last-mile delivery\nhas increased exponentially. With the rise of these applications\ncomes the need for highly robust, safety-critical algorithms that\ncan operate drones in complex and uncertain environments.\nAdditionally, flying fast enables drones to cover more ground,\nincreasing productivity and further strengthening their use case.\nOne proxy for developing algorithms used in high-speed naviga\u0002tion is the task of autonomous drone racing, where researchers\nprogram drones to fly through a sequence of gates and avoid\nobstacles as quickly as possible using onboard sensors and limited\ncomputational power. Speeds and accelerations exceed over\n80 kph and 4 g, respectively, raising significant challenges across\nperception, planning, control, and state estimation. To achieve\nmaximum performance, systems require real-time algorithms\nthat are robust to motion blur, high dynamic range, model\nuncertainties, aerodynamic disturbances, and often unpredictable\nopponents. This survey covers the progression of autonomous\ndrone racing across model-based and learning-based approaches.\nWe provide an overview of the field, its evolution over the years,\nand conclude with the biggest challenges and open questions to\nbe faced in the future.\nI. INTRODUCTION\nThroughout history, humans have been obsessed with racing\ncompetitions, where physical and mental fitness are put to the\ntest. The earliest mention of a formal race dates all the way\nback to 3000 BC in ancient Egypt, where the Pharaoh was\n1D. Hanover, L. Bauersfeld, A. Romero, Y. Song, G. Cioffi, E. Kaufmann\nand D. Scaramuzza are with the Robotics and Perception Group, University\nof Zurich, Switzerland (http://rpg.ifi.uzh.ch). 2R. Penicka is with the Multi\u0002robot Systems Group, Czech Technical University in Prague, Czech Republic.\n3A. Loquercio is with UC Berkeley. This work was supported by the\nSwiss National Science Foundation (SNSF) through the National Centre of\nCompetence in Research (NCCR) Robotics, the Czech Science Foundation\n(GACR) under research projects No. 23-06162M, the European Union\u2019s\nHorizon 2020 Research and Innovation Programme under grant agreement\nNo. 871479 (AERIAL-CORE), and the European Research Council (ERC)\nunder grant agreement No. 864042 (AGILEFLIGHT).\nthought to have run a race at the Sed festival to demonstrate\nhis physical fitness, indicating his ability to rule over the\nkingdom [1], [2]. As time has progressed, humans have\nmoved from racing on-foot to using chariots, cars, planes, and\nmore recently quadcopters [3]. Although the vessel frequently\nchanges, one thing that has remained constant since the early\ndays of racing has been the recurring theme of using the task as\na catalyst for scientific and engineering development. Recently,\nwe have seen a push to remove humans from the loop,\nautomating the highly complex task of racing in order to push\nvehicle performance beyond what humans can achieve [4], [5].\nA. Why Autonomous Drone Racing?\nDrone racing is a popular sport with high-profile interna\u0002tional competitions. In a traditional drone race, each vehicle\nis controlled by a human pilot, who receives a first-person\u0002view (FPV) live stream from an onboard camera and flies\nthe drone via a radio transmitter. An onboard image from\nthe drone can be seen in Fig. 1b. Having access to an FPV\nfeed sets drone racing apart from remote-controlled fixed\u0002wing aircraft racing, where pilots typically control the vehicle\nin a line-of-sight fashion. Human drone pilots need years of\ntraining to master the advanced navigation and control skills\nrequired to succeed in international competitions. Such skills\nwould also be valuable to autonomous systems that must fly\nthrough complex environments in applications such as disaster\nresponse, aerial delivery, and inspection of complex structures.\nFor example, automating inspection tasks can save lives while\nbeing more productive than manual inspection. According to\na recent survey on unmanned aerial vehicle (UAV) use in\nbridge inspection [6], most drones used for inspection tasks\nrely on GPS navigation with the biggest limiting factor on\ninspection efficiency being the drones\u2019 endurance and mobility.\nAdditionally, the most popular drones used for surveying\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nby several US Departments of Transportation are not fully\nautonomous and require expert human pilots [6]. In these\napplications, an increase in autonomy and operational speed\nwill offer gains in utility as faster flight increases the operating\nradius achievable with a given battery [7]. Drone racing\nresearch has made significant progress in bringing the skills\nof autonomous drones closer to those of professional human\npilots [5]. This required advances on all parts of the flight\nstack, i.e., estimation, planning, control, and hardware [8],\nwhich we cover in length in this survey. However, several\nchallenges remain to bridge the gap between drone racing and\nreal-world applications, such as safety [9] and generalization\nover tasks and environments.\nOver the last five years, several projects have been launched\nto encourage rapid progress within the field, such as DARPA\u2019s\nFast Lightweight Autonomy (FLA) [11], European Research\nCouncil\u2019s AgileFlight [12], and the AutoAssess project [13].\nWith million-dollar funding for each of these projects and\nsignificant commercial potential, a large incentive exists for\nresearchers and entrepreneurs alike to achieve autonomous\noperation in GNSS-denied and confined critical infrastructure.\nCompetitions such as the IROS\u201916-19 Autonomous Drone\nRacing series [14], NeurIPS 2019\u2019s Game of Drones [15],\nand the 2019 AlphaPilot Challenge [16], [17] provided further\nopportunity for researchers to compare their methodologies\nagainst one another in a competitive fashion. A depiction of\nthe progress made from these competitions can be seen in\nFig. 2. However, we are far from having solved autonomous\ndrone racing\u2014a notion reflected by the recently announced\ncompetition scheduled for 2025 and to be hosted by the Abu\nDhabi Autonomous Racing League [18].\nDrone racing is a challenging benchmark that can help re\u0002searchers to gauge progress on complex perception, planning,\nand control algorithms. Autonomous drones in a racing setting\nmust be able to perceive, reason, plan, and act on the tens\nof milliseconds scale, all onboard a computationally limited\nplatform. Apart from being very challenging to solve, the\ndrone racing task offers a single measure of the progress of\nthe state-of-the-art in autonomous flying robotics: lap time.\nSolving this problem requires efficient, lightweight algorithms\nto provide optimal decision and control behaviors in real-time.\nJust a few years back, it was unclear whether such a problem\nwas feasible in the first place, even given perfect information.\nDrone racing research has advanced much since its early\nstages [19]. Such advances required radically new algorithmic\nideas, e.g., training learning-based sensorimotor controllers\nonly in simulation, together with engineering advances to the\nplatform and the overall system [8]. Now that superhuman\nperformance has been achieved [5] (despite being in controlled\nconditions), we predict that more work will be done on safety\nand generalization over tasks and environments to bridge\nthe gap between drone racing and real-world applications.\nThis research effort is already evident today, as shown by\nthe increasing number of papers in the field over the years\n(Fig. 1a).\nThis is the first survey on the state of the art in autonomous\ndrone racing. This overview will be useful to researchers who\nwish to make connections between existing works, learn about\nthe strengths and weaknesses of current and past approaches,\nand identify directions moving forward which should progress\nthe field in a meaningful way.\nB. Task Specification\nThe drone racing task is to fly a quadrotor through a\nsequence of gates in a given order in minimum time while\navoiding collisions. Humans are astonishingly good at this\ntask, flying at speeds well over 100kph with only a first-person\nview camera as their sensory input. Beyond this, expert pilots\ncan adapt to new race tracks quickly in a matter of minutes,\nhowever, the sensorimotor skills required by professional\ndrone pilots take years of training to acquire.\nFor an autonomous drone to successfully complete this task,\nit must be able to detect opponents and waypoints along the\ntrack, calculate their location and orientation in 3-dimensional\nspace, and compute an action that enables navigation through\nthe track as quickly as possible while still controlling a highly\nnonlinear system at the limits. This is challenging in three\ndifferent aspects: Perception, Planning, and Control. Poor\ndesign in any of these aspects can make the difference between\nwinning or losing the race, which can be decided by less than\na tenth of a second.\nThe paper is structured as follows: First, the modeling pro\u0002cedure of the drone, including aerodynamics, batteries, motors,\ncameras, and the system nonlinearities, are discussed in detail\nin Sect. II. A classical robotics pipeline is then introduced\nin Sect. III, with a deep dive into literature relevant to agile\nflight split into Perception, Planning, and Control subsections.\nAll of these components are equally important because, at the\nedge of a drone\u2019s performance envelope, all parts\u2014perception,\nstate-estimation, planning, and control\u2014need to meticulously\nwork together. Next, we delve into learning-based methods\nfor Perception, Planning, and Control which rely on recent ad\u0002vancements from the machine learning community in Sect. IV.\nThen, a discussion of the development of simulation tools that\ncan enable rapid development for agile flight applications in\nSect. V. A history of drone racing competitions and the meth\u0002ods used for each are included in Sect. VI. Next, a summary\nof open source code bases, hardware platforms, and datasets\nfor researchers is provided in Sect. VII. Finally, a forward\u0002looking discussion on the Opportunities and Challenges for\nfuture researchers interested in autonomous drone racing in\nSect. VIII.\nII. DRONE MODELING\nTo further advance research on fast and agile flight, it is\nimportant to have accurate models that capture the complex\nnonlinear dynamics of multicopter vehicles at the limit of their\nperformance envelope.\nThis section reviews different dynamics modeling ap\u0002proaches from classic, first-principles modeling to pure data\u0002driven models in the context of drone racing. For the vehicle\ndynamics, the key aspects that need to be modeled are the\nkinematics, aerodynamics, the electric motors, and the battery.\nIn addition to the vehicle dynamics models discussed in this\nsection, many difficulties for autonomous drone racing models\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nThe first autonomous drone racing challenge at\nIROS 2016, Daejong Korea. Slow moving\nquadrotors cautiously navi-gated the course shown\nabove using only onboard sensors. Team KIRD\nfrom KAIST placed 1st, reaching a top speed of\n0.6 m/s.\nIROS ADR I\nIn the third iteration of the ADR challenge held in\nMadrid, teams began implementing learning based\nmethods with optimal control techniques. The\nRobotics and Perception Group from the\nUniversity of Zurich successfully completed the\ncourse the fastest with speeds up to 2.0 m/s.\nIROS ADR III\nThe following year, another autonomous drone\nracing competition took place in Vancouver,\nCanada. Similar to the year prior, teams used\nclassical methods to navigate a challenging course\nwith compute done onboard and the INAOE team\nfrom Mexico won with a speed of 0.7 m/s.\nIROS ADR II\nIn summer of 2022, the Robotics and Perception\nGroup of the University of Zurich hosted a drone\nracing competition to face their autonomous\ndrones off against some of the best human FPV\npilots in the world. Speeds exceeded 20 m/s,\nrelying only on onboard sensing.\nSwiss Drone Days\n2017\n2016 2018 2022\n2019\nIn 2019, Lockheed Martin sponsored a $1M prize\nto teams who could successfully navigate a\nchallenging drone racing course completely\nautonomously. The MAVLAB team from TU Delft\nwon, with top speeds approaching 10 m/s, showing\na significant jump over previous competitions.\nAlphaPilot\nvmax = 0.6 m/s vmax = 0.7 m/s vmax = 2.0 m/s vmax = 10 m/s vmax = 22 m/s\nFig. 2: History of drone racing competitions that use real drones for navigating the race track, IROS ADR II photo credit [10].\nare introduced by the onboard sensors, whose characteristics\nneed to be modeled. For example, IMUs are subject to bias\nand noise, and the intrinsic as well as extrinsic parameters of\nonboard sensors change over time as hard crashes may lead\nto miscalibration.\nA. Kinematics\nTypically, the vehicle is assumed to be a 6 degree-of\u0002freedom rigid body of mass m with a (diagonal) inertia matrix\nJ = diag(Jx, Jy, Jz). Given a dynamic state x \u2208 R\n17\nthe equations describing its evolution in time are commonly\nwritten as:\nx\u02d9 = f(x, u) =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\np\u02d9 W B\nq\u02d9 W B\nv\u02d9 W\n\u03c9\u02d9 B\n\u2126\u02d9\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n=\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\nvW\nqW B \u0014\n0\n\u03c9B/2\n\u0015\n1\nm\nqW B \u2299 f\n\u0001\n+ gW\nJ\n\u22121\n\u03c4 \u2212 \u03c9B \u00d7 J\u03c9B\n\u0001\n1\n\u03c4\u2126\n(\u2126ss \u2212 \u2126)\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n, (1)\nwhere pW B \u2208 R\n3\nis the position of the center of mass given\nin the world frame, qW B \u2208 SO(3) is a quaternion defining\nthe rotation of the body frame relative to the world (vehicle\nattitude), vW \u2208 R\n3\nis the velocity of the vehicle in the world\nframe, \u03c9B \u2208 R\n3\nare the bodyrates of the vehicle, \u2126 \u2208 R\n4\nare\nthe motor speeds, gW = [0, 0, \u22129.81 m/s\n2\n]\n\u22ba denotes earth\u2019s\ngravity, and u \u2208 R\n4\nis the input. Depending on the control\nmodality, the input can be single rotor thrusts or collective\nthrust and body rates. In this setting, the task of the model is\nto calculate the total force f and total torque \u03c4 that acts on\nthe drone as accurately as possible. Note the quaternion-vector\nproduct denoted by \u2299 representing a rotation of the vector by\nthe quaternion as in q \u2299 f = q \u00b7 [0, f\n\u22ba\n]\n\u22ba\n\u00b7 q\u00af, where q\u00af is the\nquaternion\u2019s conjugate. Those forces and torques, collectively\nreferred to as wrench, are determined by the aerodynamics\nof the platform as well as the vehicles\u2019 actuators, e.g. the\npropellers.\nB. Aerodynamics\nThis section discusses the different approaches to modeling\nthe aerodynamics of the drone and its propellers. The most\nwidely used modeling assumption is that the propeller thrust\nand drag torque are proportional to the square of the rotational\nspeed [20]\u2013[24] and that the body drag is negligible. These as\u0002sumptions quickly break down at the high speeds encountered\nin drone racing as this model neglects (a) linear rotor drag [25],\n[26], (b) dynamic lift [25], (c) rotor-to-rotor [27]\u2013[29], (d)\nrotor-to-body [27]\u2013[29] interactions and (e) aerodynamic body\ndrag [26], [28].\nThe accuracy of the propeller model can be improved\nby leveraging blade-element-momentum theory, where the\npropeller is modeled as a rotating wing. Such first-principle ap\u0002proaches [30]\u2013[34] have been shown to provide very accurate\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nmodels of the wrench generated by a single propeller as they\nproperly capture effects (a) and (b). Implemented efficiently, a\nBlade Element Momentum (BEM) model can be run in real\u0002time [35] and has been successfully used to test algorithms in\nsimulation [36], [37].\nAccounting for the remaining open points (c)-(e), the aero\u0002dynamics of the drone body as well as any interaction effects\nneed to be calculated, which requires a full Computational\nFluid Dynamics (CFD) simulation [27]\u2013[29], [38], [39]. Due\nto the extreme computational demands, this is impractical\nin drone racing. To still get close to the accuracy of CFD\nmethods while retaining the computational simplicity of the\npreviously mentioned methods, data-driven approaches are\nemployed [35], [40]\u2013[44]. In the early works [42], [43],\nthe whole vehicle dynamics model was learned from data.\nIn a similar fashion [41] uses a combination of polynomi\u0002als\u2014identified from wind-tunnel flight data\u2014to represent the\nvehicle dynamics. In [35], [40], it has been shown that higher\nmodeling accuracies can be achieved when combining a first\u0002principle model with a data-driven component. Such a com\u0002bination of first-principle and data-driven models also leads\nto improved generalization performance, as shown in [35],\nwhich combines a BEM model with a temporal convolutional\nnetwork [45] to regress the residual wrench. Recently, a similar\nhybrid modeling approach has been applied to moving-horizon\nestimation [46].\nC. Motor and Battery Models\nThe previous section outlines different approaches to how\nthe aerodynamic wrench can be estimated based on the state of\nthe vehicle. However, for all such models, the rotational speed\nof the propeller is assumed to be known. On most multicopters,\nthe motors are not equipped with closed-loop motor speed\ncontrol but are controlled by a \u2018throttle\u2019 command, which\ncontrols the duty cycle of a PWM (pulse-width-modulation)\nsignal applied to the motors. The actual rotational speed that\nthe motor achieves is a function of the throttle command as\nwell as other parameters such as the battery voltage and the\ndrag torque of the rotor [7]. Therefore, in order to have a\ndynamics model for the motors, we need a model of the battery\nto calculate the voltage applied to the motors. Most literature\non battery modeling relies on so-called Peukert models [47],\nbut for lithium-polymer batteries in drone racing, this is hardly\napplicable because the battery discharge current often exceeds\n100 A (e.g. 50-100 C) [48], [49]. Graybox battery models\nfor the voltage that are based on a one-time-constant (OTC)\nequivalent circuit [50], [51] are much more suitable for drone\nracing tasks as shown in [7], because they are applicable to the\nextremely high loads experienced during a racing scenario. In\ncombination with either a polynomial or a constant-efficiency\nmotor model, such OTC models can be used to accurately\nsimulate the battery voltage during agile flight [7]. Given\na simulation of the battery voltage, one can measure the\nperformance characteristics of a given motor-propeller com\u0002bination to determine the mapping of throttle command and\nvoltage to resulting steady-state propeller speed \u2126ss. When the\nhighest model fidelity is desired, a more sophisticated motor\nsimulation [52] can further improve the accuracy, which can be\ndesirable if the controller directly outputs single-rotor thrusts\ninstead of the more commonly used collective-thrust and body\nrates control modality.\nD. Camera and IMU Modeling\nDrone racing pushes not only the mechanical and electrical\ncomponents of drones to their limits, but is also highly\ndemanding in terms of sensor performance. For an in-depth\noverview of the many different sensor options for drone racing\nthe reader is referred to [53]. The most common sensors\naboard autonomous drones are monocular or stereo cameras\ncombined with IMUs (inertial measurement units) thanks to\ntheir low cost, low weight, and mechanical robustness.\nFor vision-based drone racing, having an accurate simula\u0002tion of the perception pipeline is critical for validation and\ncontroller development. In terms of modeling and simulation\nof the camera, it is common to use a pinhole model [54] and\nestimate the focal length, image center, and distortion parame\u0002ters from measurements. Combined with accurate information\non how far the camera is displaced from the center of gravity\nof the vehicle, this allows simulating observations. Either low\u0002level sensory observations (e.g. images) are simulated using a\nrendering engine [22], [55] or more abstract visual features\n(e.g. landmark positions) are simulated using the projection\nequations.\nIn the context of using a simulation to test approaches\nbefore attempting real-world deployment, an accurate model\nof the IMU characteristics is important, as the bias and noise\nstrongly influence the performance of many methods. The\nIMU intrinsic calibration estimates the noise characteristic of\nthe sensor. The camera-IMU extrinsic calibration estimates the\nrelative position and orientation of the two sensors as well as\nthe time offset. Kalibr [56] is a widespread tool to perform\nthese calibrations.\nHowever, the biggest source of measurement error of the\ninertial sensors onboard a drone are not the sensors themselves\nbut the strong high-frequency vibrations introduced by the fast\u0002spinning propellers. The vibrations lead to aliasing effects on\nthe IMU measurements and introduce additional motion blur\non the camera images. The structural vibrations and their effect\non the measurements are extremely difficult to model and\ncorrect for. Therefore, a suitable hardware design is imperative\nwhich dampens the mount of the camera and the IMU with\nrespect to the vehicle frame.\nIII. CLASSICAL PERCEPTION, PLANNING, AND CONTROL\nPIPELINE\nSensors\nHardware\nPerception\nSoftware\nPlanning Control Drone\nFig. 3: Architecture 1: A classic architecture for an autonomous system\nprogrammed using model-based approaches\nSince the inception of mobile robotics, a common architec\u0002ture has been primarily used to achieve autonomous navigation\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\ncapabilities across various systems. In a traditional robotics\nsoftware stack, the navigation task is broken into three main\ncomponents: Perception, Planning, and Control. A diagram of\nthis architecture can be seen in Fig. 3. This section covers\nrecent research in these areas relating specifically to agile\nflight and autonomous drone racing. All approaches detailed in\nthis section rely on first principles modeling and optimization\ntechniques.\nA. Perception\nThe perception block estimates the vehicle state and per\u0002ceives the environment using onboard sensors. The most com\u0002mon solution for state estimation of flying vehicles is visual\u0002inertial odometry (VIO), thanks to its low cost and low weight\nrequirements. VIO uses camera and IMU measurements to\nestimate the state x\u02c6 (position, orientation, and velocity) of the\ndrone platform. The inertial measurements are integrated to\nobtain relative position, orientation, and velocity estimates in\na short time, e.g., between two camera images. However, the\nintegration for a longer time, e.g., a few seconds, accumulates\nlarge drift due to scale factor errors, axis misalignment errors,\nand time-varying biases [57] that commonly affect off-the-self\nIMU measurements. The camera measurements provide rich\ninformation about the environment at a lower rate, usually\naround 30 Hz, than IMU measurements. Unlike the IMU\nmeasurements, the camera measurements are affected by envi\u0002ronmental conditions. The quality of information they provide\nfor state estimation degrades in the case of poor illumination\nconditions, textureless scenes, and motion blur. For this reason,\nthe camera and inertial measurements complement each other\nand are the standard choice for state estimation of flying\nvehicles [58]. In this section, we first give an overview of VIO\nwith a focus on the methods that can be applied for online state\nestimation of a racing drone. Second, we give an overview\nof possible additional sensor modalities that integrated into\nthe classical VIO pipeline have the potential to improve state\nestimation at high speed. Third, we conclude with a discussion\non the application of classical VIO methods to drone racing\ntasks.\n1) VIO: VIO is the most common solution for state esti\u0002mation of aerial vehicles [58] using only onboard sensing and\ncomputing, thanks to its favorable trade-off between accuracy\nand computational requirements. VIO algorithms usually com\u0002prise two main blocks: the frontend and the backend.\nThe frontend uses camera images to estimate the motion\nof the sensor. Two main approaches exist in the literature:\ndirect methods [59], [60] and feature-based methods [61]\u2013\n[63]. Direct methods work directly on the raw pixel intensities.\nThese methods commonly extract image patches and estimate\nthe camera trajectory by tracking the motion of such patches\nthrough consecutive images. The tracking is achieved by\nminimizing a photometric error defined on the raw pixel\nintensities [59]. This tracking method is particularly interesting\nfor drone racing because of its robustness in featureless\nscenarios. In fact, a direct frontend [60] is used to estimate the\nstate of a racing drone in [16]. On the contrary, feature-based\nmethods [61]\u2013[63] extract points of interest, commonly known\nas visual features or keypoints, from the raw image pixels. The\ncamera trajectory is estimated by tracking these points through\nconsecutive images. High-speed motions make it difficult (e.g.,\ndue to motion blur) to track features on many consecutive;\nconsequently, feature-based methods struggle in drone racing\nscenarios. However, feature-based methods exhibit higher ro\u0002bustness than direct methods to brightness changes. The VIO\nmethods used in [64], [65] demonstrate that a hybrid frontend,\ncombining the benefits of direct and feature-based methods, is\nbeneficial for drone racing tasks.\nThe backend fuses the output of the fronted with the inertial\nmeasurements. Two categories exist in the literature according\nto how the sensor fusion problem is solved: filtering meth\u0002ods [61] and fixed-lag smoothing methods [62], [63]. Filtering\nmethods are based on an Extended Kalman Filter (EKF).\nThese methods propagate the system\u2019s state using the inertial\nmeasurements and fuse the camera measurements in the update\nstep. The pioneer filter-based VIO algorithm is the Multi\u0002State Constraint Kalman Filter (MSCKF) originally proposed\nin [61]. Since then, many different versions of MSCKF have\nbeen developed [66]. Fixed-lag smoothing methods, also called\nsliding window estimators, solve a non-linear optimization\nproblem where the variables to be optimized are a window\nof the recent robot states. The cost function to minimize\ncontains visual, inertial, and past states marginalized residuals.\nThanks to their favorable trade-off between accuracy and\ncomputational cost, filter-based methods have been commonly\nused in drone racing [16], [64], [65].\n2) Additional sensor modalities in VIO: Recently, classical\nVIO pipelines have been augmented with event cameras [67]\u2013\n[69] or drone dynamics [70]\u2013[72], to improve state estimation\nat high speed.\nLow latency, high temporal resolution (in the order of \u00b5s),\nand high dynamic range (140 dB compared to 60 dB of stan\u0002dard cameras) are the main properties of event cameras [73],\nwhich make this novel sensor a great complementary sensor\nto standard cameras. Including event data in VIO algorithms\nonboard flying vehicles achieves increased robustness against\nmotion blur as demonstrated in [67]\u2013[69]. Although applica\u0002tions of event cameras in drone racing tasks are yet to be\nexplored, investigating the use of this sensor is a promising\nresearch direction to robustify VIO systems for agile flights.\nThe drone dynamics are used to define additional constraints\nin the estimation process. The work in [70] (VIMO) is the\nfirst to integrate error terms related to the drone transitional\ndynamics in a VIO backend. VIMO inspired a few works [71],\n[74] which propose an improved noise model of the dynam\u0002ics [74] and a learned component to account for unmodeled\naerodynamics [71]. In particular, the results of [71] show that\nthe learned aerodynamics effects help to improve the VIO\nestimates at high speeds.\nThe work in [72] proposes an odometry algorithm that relies\non an IMU as the only sensor modality (no camera is needed),\nand leverages a learned dynamics component to estimate the\nstate of the racing drone. Consequently, this method does not\nuse a visual frontend.\n3) Discussions: The work in [75] presents a benchmark\ncomparison between a number of VIO solutions on the EuRoC\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\ndataset [76]. The EuRoC dataset contains camera and IMU\ndata recorded onboard a drone flying in indoor environments.\nThe drone moves with average linear and angular velocities\nup to 0.9 m/s and 0.75 rad/s, respectively. These values are\nfar below the ones reached in drone racing. The conclusions\nof [75] show that state-of-the-art VIO algorithms provide\nreliable solutions for estimating the state of the drone at\nlimited speeds. However, these classical VIO methods cannot\nprovide accurate state estimates for drone racing tasks. VIO\nmethods accumulate large drift in scenarios characterized by\nmotion blur, low texture, and high dynamic range [77]. These\nscenarios are the norm in drone racing.\nTo help research VIO algorithms for drone racing tasks, the\nwork in [78] proposes the UZH-FPV Drone Racing Dataset.\nThis dataset contains images recorded from standard cameras,\nevent camera data, and IMU data recorded onboard a quadrotor\nflown by a human pilot. All the flights include visual chal\u0002lenges similar to those in drone racing competitions.\nSuccessful state estimation solutions for drone racing [77],\n[79] reduce the drift accumulated in VIO by localizing to a\nprior map of the track. In drone racing competitions, a map of\nthe track in the form of gate positions is known beforehand.\nThe localization process is based on the detection of the gates.\nPerforming gate detection is challenging. Often during the\nrace, none of the gates is visible in the camera\u2019s field of view.\nMoreover, motion blur makes gate corner detection difficult.\nFor this reason, gate detection and VIO are complementary.\nIn [80], a gate detector was proposed that uses an RGB camera\nto identify the gates based on their color. This detector, tailored\nto the IROS drone racing context [19], is aimed at extreme\ncomputational efficiency, which is particularly important for\ntiny drones. The method in [81] relies on detecting gates\nand using a model of the drone dynamics to estimate the\nposition of the racing drone. Differently from [77], [79], this\nmethod does not use a VIO but controls the drone based on a\nvisual-servoing approach. All the other gate detection methods\nin the literature are based on deep learning techniques [82].\nWe review them in Sec. IV. The known gate positions and\nthe detections in the onboard images are used to estimate\nthe relative pose between the camera and the gate using\nthe Perspective-n-Point algorithm (PnP) [83]. This relative\npose is used to constrain the VIO estimates and consequently\nreduce the drift. There is significant room for innovation on\nthis front, as the VIO-PnP paradigm has existed for several\nyears with little innovation. However, one clear benefit of the\nVIO-PnP approach is its ability to use a monocular camera\nsetup with a large FOV. While this comes with a lack of\nscale and higher uncertainty in motion estimation, both can\nbe compensated using inertial sensors and localization with\nrespect to known landmarks (e.g., gates). As evidenced by\nthe rich literature, this makes a monocular setup the preferred\nsolution for autonomous drone racing practitioners. The choice\nof a monocular sensor is very much in agreement with how\nhuman pilots fly: while they have goggles with two monitors,\nthe video stream they receive is from a monocular camera\nsystem on the drone. Other approaches used in early drone\nracing competitions relied on the technique of visual servoing\nvia stereo cameras [10], but relying on a stereo camera pair\ncomes with inherent difficulties. In the presence of motion\u0002blur stereo-matching approaches degrade quickly. Further\u0002more, drones only allow for a very small baseline and require\na wide-angle camera to perceive as much of the surroundings\nas possible. Both lead to very high depth estimation errors\nin the stereo setup. The solution proposed in [10] was found\nto be sensitive to indoor lighting changes and needed to be\nhand-tuned for every flight.\nRecent works [84]\u2013[86] proposed vision-based odometry\nalgorithms that are learned end-to-end. Theoretically, these\nmethods could be specialized to drone racing tasks and poten\u0002tially outperform classical VIO approaches. However, they are\nin the early development phase, and how to customize them\nfor the drone racing task is still an open research question.\nIn addition, they currently have high computational costs that\nmake them impractical for online state estimation onboard\ndrones. We refer the reader to Sec. IV for a detailed review\nof VIO methods based on deep learning.\nB. Planning\nOnce a state estimate x\u02c6 has been obtained from\nthe perception module, the next step in the classical\npipeline is to plan a feasible, time-optimal trajectory\n\u03c4ref = (xref ,uref )k, \u2200k \u2208 0 . . . N, which respects the phys\u0002ical limits of the platform as well as the constraints imposed\nby the environment. This requires predicting the drone\u2019s future\nstates such that minimum lap time is reached without crashing.\nThe planning for drones has matured over the last decade\nfrom works mostly verified in simulation to works shown in\nboth controlled lab environments and unknown unstructured\nenvironments. In the classical pipeline, planning can include\nup to two distinct planning problems, path planning and tra\u0002jectory planning. Path planning tackles the problem of finding\na geometrical path between a given start and goal position\nwhile passing specified waypoints and avoiding obstacles.\nTrajectory planning then uses a found geometric path to either\ncreate a collision-free flight corridor [87], [88], to find new\nwaypoints for the trajectory to avoid collisions [37], [89], to\nconstrain the trajectory to stay close to the found path [90],\n[91] or directly finds time allocation for a given path [92],\n[93]. Therefore, path planning can be seen as a way to select\nthe homotopy class of the collision-free space the drone flies\nthrough, while trajectory planning finds the full (or simplified)\ntime-allocated drone state predictions to be followed by the\ncontrol pipeline (Section III-C). However, many works rely\nsolely on trajectory planning as they assume no collision\nwith the environment when a trajectory is found [94]\u2013[98].\nOther works directly find a collision-free trajectory [99]\u2013[102]\nwithout having a previously found path. On the other hand,\nsome control approaches [103], [104] do not need a specified\ntime-allocated trajectory and rely only on the geometrical path\nfor controlling the drone.\nIn the following text, we first overview the most popular\npath planning approaches for drones that are used for fur\u0002ther trajectory planning. Then, we categorize trajectory plan\u0002ning methods in polynomial and spline trajectory planning,\noptimization-based trajectory planning, search-based trajec\u0002tory planning, and sampling-based trajectory planning.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\n1) Path planning: Path planning approaches can be broadly\ndivided into Sampling-based planning and Combinatorial plan\u0002ning [105]. Sampling-based methods do not construct the\nobstacle space explicitly but rather rely on random sam\u0002pling of the configuration space together with collision de\u0002tection. The most popular variants of the sampling-based\nmethods with numerous modified versions are the Probabilis\u0002tic Roadmaps (PRM) [106] and Rapidly-exploring random\ntrees (RRT) [107]. Important variants of these algorithms\nnamed RRT* and PRM* [108], can find the optimal path given\ninfinite time. The combinatorial planning methods, in contrast\nto the sampling-based methods, directly represent the obstacle\nor free space using e.g. polygonal maps or cell decomposition\nsuch as grid-based maps. With the help of a graph repre\u0002sentation of the decomposed free space, classical path search\nalgorithms such as A* [109] or Dijkstra\u2019s algorithm [110] can\nbe used to find a path.\nVariants of the above path planning approaches are used\nin many of the methods listed in Sections III-B2\u2013III-B5 to\nhelp find trajectories for either fast flight or even drone\nracing. The RRT* algorithm is used to find new waypoints\nfor polynomial trajectory planning in [89], and the PRM* is\nused as a path planning part to guide sampling-based trajectory\nplanning in [37]. The sampling-based planning in [101], [102]\ndirectly performs both path and trajectory planning. While the\ntrajectory planning objective can be to minimize time duration\nof a trajectory, the path planning typically tries to find the\nshortest paths. Therefore, some methods search for multiple\ndistinct paths to enable the trajectory planning to search over\nmultiple options on how to navigate around obstacles [37],\n[90], [91]. Other methods [87] use search-based algorithms to\nfind an initial path and to create a convex flight corridor for\nconstraining the collision-free trajectory planning. Similarly,\nthe search-based methods [99], [100] use a variant of A* to\nperform both path and trajectory planning at the same time.\n2) Polynomial and Spline Trajectory Planning: The Poly\u0002nomial and Spline methods leverage the differential flatness\nproperty [111], [112] of quadrotors and represent a trajectory\nas a continuous-time polynomial or spline. This property\nsimplifies the full-state trajectory planning to a variant where\nonly four flat outputs need to be planned (typically 3D position\nand heading). By taking their high-order derivatives, these flat\noutputs can represent a dynamically feasible trajectory with\ntheir respective control inputs. This property is used by many\npolynomial and spline methods that are nowadays among the\nmost used for general quadrotor flight.\nThe widely used polynomial trajectories [111], [112] min\u0002imize snap (4th order position derivative) of a trajectory.\nDifferent methods opted for minimizing jerk (3rd order po\u0002sition derivative) for planning a trajectory [113]. However,\nthe trajectories that result from having jerk as the primary\nobjective have been shown to minimize the aggressiveness\nof the control inputs [113], which is fundamentally different\nfrom minimizing the lap times, where extremely aggressive\ntrajectories are generally required. Richter et al. [89], there\u0002fore, extended the objective by jointly optimizing both the\nsnap of a trajectory and the total time through a user-specified\npenalty on time. Recently, Han [87] proposed a polynomial\u0002based trajectory planning method for drone racing. It jointly\noptimizes control effort and regularized time and penalizes the\ndynamic feasibility and collisions.\nBecause of their numerical stability, other methods use B\u0002splines to represent trajectories [90], [91] instead of high-order\npolynomial representations that are numerically sensitive.\nThese methods jointly optimize different objectives, simulta\u0002neously smoothness, dynamic feasibility, collision avoidance,\nsafety [91] and vision-based target tracking [94]. Recently,\nthe authors of [114] proposed a polynomial trajectory rep\u0002resentation based on the work of [115] and use it to plan\ntime-optimal trajectories through gates of arbitrary shapes for\ndrone racing, achieving close-to-time-optimal results while\nbeing more computationally efficient than [95].\nAlthough both polynomial and spline trajectories are widely\nused due to their computational efficiency, polynomial-based\ntrajectories (and their derivatives) are smooth by defini\u0002tion. Therefore, only smooth control inputs can be sam\u0002pled from them. For this reason, the traditional polynomial\nplanning [111] with a finite number of coefficients and one\npolynomial segment between every two waypoints (gates)\ncannot represent true time-optimal trajectories [95]. Yet, di\u0002rect collocation methods [116] that rely on polynomials to\napproximate the input and state dynamics can achieve nearly\noptimal performance. This is mainly due to a larger number\nof polynomial segments between the waypoints in collocation\nmethods, joint optimization of both polynomial coefficients\nand collocation points, and due to the approximation of the\nentire dynamics by polynomials. This allows to keep the\nacceleration at the possible maximum at all times similar to\nthe optimization-based shooting method [95]. Therefore, while\nthe classical polynomial and spline methods can be considered\noptimization-based, they only optimize coefficients of a single\npolynomial between every two waypoints to describe quadro\u0002tor position and heading, leveraging the differential flatness\nproperty [111], [112].\n3) Optimization-based Trajectory Planning: Optimization\u0002based trajectory planning enables us to independently select\nthe optimal sequence of states and inputs at every time\nstep, which inherently considers time minimization while\ncomplying with quadrotor dynamics and input constraints.\nOptimization-based approaches have been extensively con\u0002sidered in the literature, ranging from exploiting point-mass\nmodels [96], simplified quadrotor models [97], [117], and full\u0002state quadrotor models [88], [95].\nTime-optimality of a trajectory could also be accomplished\nby using a specific path parameterization that maximizes\nvelocity over a given path [92]. This method was shown for\nquadrotors in [93] for minimizing time of flight considering\nboth translational and rotational quadrotor dynamics. However,\nthe method only creates a velocity profile over a given path\nwhich is not further optimized.\nApart from time optimality, complying with intermediate\nwaypoint constraints is another requirement for path planning\nin autonomous drone racing. A common practice of solving\na trajectory optimization problem with waypoint constraints\nis allocating waypoints to specific time steps and minimizing\nthe spatial distance between these waypoints and the position\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nat the corresponding allocated time steps on the reference\ntrajectory (e.g. [98], [118]). The time allocation of the way\u0002points is, however, non-trivial and difficult to determine. This\nis tackled in [88], but the work uses body rates and collec\u0002tive thrust as control inputs and does not represent realistic\nactuator saturation. Recent work [95] introduces a comple\u0002mentary progress constraints (CPC) approach, which considers\ntrue actuator saturation, uses single rotor thrusts as control\ninputs, and exploits quaternions to create full, singularity\u0002free representations of the orientation space with consistent\nlinearization characteristics. While the above methods create\ntime-optimal trajectories passing through given gates, they are\ncomputationally costly and hence intractable in real-time.\n4) Search-based Trajectory Planning: Search-based plan\u0002ning methods [99], [100] rely on discretized state and time\nspaces. They solve the trajectory planning through graph\nsearch algorithms such as A*. The search graph is built using\nminimum-time motion primitives with discretized velocity,\nacceleration, or jerk input. The algorithms then use trajectories\nof a simpler model, e.g. with velocity input, as heuristics for\nthe search with a more complex model. Search-based planning\nmethods can optimize the flight time up to discretization, but\nthey suffer from the curse of dimensionality which renders\nthem too computationally demanding for a complex quadrotor\nmodel. Furthermore, the employed per-axis dynamic limits\n(velocity, acceleration, jerk) do not represent the true quadrotor\nmodel, further decreasesing the quality of found plans. Finally,\nalthough searching for minimum time trajectories, the methods\nare currently limited to planning between two states which is\nnot suitable for multi-waypoint drone racing.\n5) Sampling-based Trajectory Planning: Sampling-based\nmethods like RRT* [119] can be used for planning trajecto\u0002ries for linearized quadrotor models. Several time-minimizing\napproaches [77], [101] use a point-mass model for high\u0002level time-optimal trajectory planning. In [101], an additional\ntrajectory smoothing step is performed where the gener\u0002ated trajectory is connected with high-order polynomials by\nleveraging the differential flatness property of the quadrotor.\nAuthors of [120] use sampling-based approach with massive\nGPU parallelization and a 6D double integrator system of\nUAV with additional single integrator yaw dynamics. However,\nthese point-mass approaches need to relax the single actuator\nconstraints and instead limit the per-axis acceleration, which\nresults in trajectories that are conservative and sub-optimal\ngiven a minimum time objective. In [102], the authors use\nminimum-jerk motion primitives for connecting randomly\nsampled states inside RRT* to plan a collision-free trajectory.\nSince the authors use polynomials, this approach can only\ngenerate smooth control inputs, meaning that they cannot\nrapidly switch from full thrust to zero thrust if required.\nThe first method for planning minimum-time trajectories\nin a cluttered environment for the full quadrotor model was\nproposed in [37]. It uses a hierarchical sampling-based ap\u0002proach with an incrementally more complex quadrotor model\nto guide the sampling. The authors showed that the method\noutperforms both polynomial and search-based methods in\nminimizing trajectory time. Yet, the method is offline and\nintractable in real-time. Most recently, the authors of [104]\n2016 2017 2018 2019 2020 2021 2022\n0\n10\n20\n30\n[53] [53]\n[16]\n[40]\n[124], [125]\n[4]\nYear\nMaximum Speed [m/s]\nFig. 4: Top speeds demonstrated on autonomous drones over time from both\nliterature and competition data.\nproposed an online replanning approach that plans minimum\u0002time trajectories for a point-mass model. The paths of re\u0002planned trajectories are then consequently used by Model\nPredictive Contouring Control [103] with a full quadrotor\nmodel to maximize the progress along the path. This method\nis capable of outperforming other classical approaches due to\nthe replanning capability and progress maximization with a\nfull quadrotor model.\n6) Discussion: A planned trajectory can be understood as\nan intermediate representation that, given information about\nthe robot\u2019s dynamics and the environment, helps guide the\nplatform through the race track and ultimately perform the task\nat hand. One might argue if this intermediate representation is\nneeded at all, since ultimately, what we are looking for is a\npolicy that maps sensor information and current environment\nknowledge to the actuation space. This is generally achieved\nwith learning-based approaches, discussed in Section IV,\nwhich bypass the planning stage and directly convert sensor\nobservations to actuation commands [121]\u2013[123].\nOne of the biggest benefits of explicit planning is modu\u0002larity. This means that the developed algorithms can be used\noff-the-shelf for different drone tasks outside racing, such as\nsearch and rescue, which is not the case for single-purpose\nlearned approaches. However, explicit planning suffers from\nthe disconnection (or an open loop) between the planning and\nthe deployment stage. Unexpected deviations from the plan,\nbe it in the time domain (like unmodeled system delays) or in\nthe state-space domain (like state estimation drifts or jumps in\nthe VIO pipeline), can lead to compound errors and ultimately,\na complete system failure.\nThis can be tackled with more complex control approaches\nthat do some part of the replanning online [104].\nC. Control\nOver the last decade, significant advancements have been\nmade in agile multicopter control. Every year, increasing top\nspeeds are demonstrated in the literature as shown in Figure 4.\nControllers must be able to make real-time decisions in the\nface of poor sensor information and model mismatch. Control\ninputs, u(t), can come in a variety of modalities for quadrotor\ncontrol, such as velocity and heading, body rates and collective\nthrust, or direct rotor thrust commands [36]. Typically, a high\u0002level controller computes a desired virtual input such as body\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nrates and collective thrust, which is then passed down to a\nlow-level flight controller that directly controls the individual\nrotors on the multicopter.\nCommonly used open source controllers such as PixHawk1\nor BetaFlight2are widely available to the drone racing com\u0002munity. BetaFlight is the most commonly used low-level\ncontroller for agile drone flight and has been widely adopted\nby the First Person View (FPV) racing community.\nIn the following sections, we provide an overview of suc\u0002cessful approaches to achieving high speeds in both simulation\nand real-world applications. We sort the approaches into\nmodel-based control and coupled perception and control.\n1) Model-Based Control: In model-based control, an ex\u0002plicit model of the dynamic system is used to calculate control\ncommands that satisfy a given objective such as minimizing\ntime or tracking error. Models enable the prediction of future\nstates of the drone and provide information about the system\u2019s\nstability properties. In [126], Geometric Tracking control is\nintroduced on the Special Euclidean group SE(3) and com\u0002pletely avoids singularities commonly associated with Euler\nangle formulations on SO(3). This nonlinear controller showed\nthe ability to execute acrobatic maneuvers in simulation and\nwas the first to demonstrate recovery from an inverted initial\nattitude. The dynamic model of a quadrotor is shown to be\ndifferentially flat when choosing its position and heading as\nflat outputs in [112]. In this work, many agile maneuvers are\nperformed onboard real drones with speeds up to 2.6 m/s.\nThe previous work is extended in [26], proving that the\ndynamics model of a quadrotor subject to linear rotor drag\nis also differentially flat. The inclusion of the aerodynamic\nmodel within the nonlinear controller led to demonstrated\nflight speeds up to 4.0 m/s while reducing tracking error by\n50% onboard a real drone.\nThe differential flatness method is further extended in [127]\nby cascading an Incremental Nonlinear Dynamic Inversion\n(INDI) controller with the differential flatness controller de\u0002scribed in [112] but neglects the aerodynamic model addition\nfrom [26]. The INDI controller is designed to track the angular\nacceleration commands \u2126\u02d9from the given reference trajectory.\nTop speeds of nearly 13 m/s and accelerations over 2g are\ndemonstrated onboard a real quadrotor. The controller shows\nrobustness against large aerodynamic disturbances in part due\nto the INDI controller.\nAn investigation of the performance of nonlinear model pre\u0002dictive control (NMPC) against differential flatness methods\nis available in [125]. Cascaded controllers of INDI-NMPC\nand INDI-differential flatness are shown to track aggressive\nracing trajectories which achieve speeds of around 20m/s and\naccelerations of over 4g. While differential flatness methods\nare computationally efficient controllers and relatively easy to\nimplement, they are outperformed on racing tasks by NMPC.\nAn excellent overview of MPC methods applied to micro\naerial vehicles can be found in [128]. Because quadrotors are\nhighly nonlinear systems, nonlinear MPC is often used as\nthe tool of choice for agile maneuvers. The debate of linear\n1https://pixhawk.org/\n2https://github.com/betaflight/betaflight\nversus nonlinear MPC is thoroughly discussed in [129]. Model\nPredictive Path Integral (MPPI) control is a sampling-based\noptimal control method that has found excellent success on\nthe AutoRally project, a 1/5th scale ground vehicle designed to\ndrive as fast as possible on loose dirt surfaces [130], [131]. An\nintroduction to MPPI can be found in https://autorally.github.\nio/. The MPPI approach can be used on agile quadrotors to\nnavigate complex forest environments, however, analysis was\nonly performed in simulation [130]. Most of the successful\ndemonstrations of MPPI come from ground robots [130],\n[131]. Because MPPI is a sampling based algorithm, scaling\nto higher-dimension state spaces of quadrotors can lead to\nperformance issues as shown in [124].\nNonlinear MPC methods are also used in [40] where a\nnominal quadrotor model is augmented with a data-driven\nmodel composed of Gaussian Processes and used directly\nwithin the MPC formulation. The authors found that the\nGaussian-Process model could capture highly nonlinear aero\u0002dynamic behavior which is difficult to model in practice as\ndescribed in Sec. II. The additional terms introduced by the\nGaussian-Process added computational overhead to the MPC\nsolve times, but it was still able to run onboard a Jetson TX2\ncomputer.\nSimilar to [127], authors in [124] question whether or not\nit is necessary to explicitly model the additional aerodynamic\nterms from [40] due to the added computational and modeling\ncomplexity. Instead, they propose to learn residual model\ndynamics online using a cascaded adaptive nonlinear model\npredictive control architecture. Aggressive flight approaching\n20m/s and over 4g acceleration is demonstrated on real rac\u0002ing quadrotors. Additionally, completely unknown payloads\ncan be introduced to the system, with minimal degradation\nin tracking performance. The adaptive inner loop controller\nadded minimal computational overhead and improved tracking\nperformance over the Gaussian Process MPC by 70% on a\nseries of high-speed flights of a racing quadrotor [40], [124].\nContouring control methods can deal with competing op\u0002timization goals such as trajectory tracking accuracy and\nminimum flight times [132]. These methods minimize a cost\nfunction which makes trade-offs between these competing\nobjectives. In [133], Nonlinear Model Predictive Contouring\nControl (MPCC) is applied to control small model racecars.\nMPCC was then extended to agile quadrotor flight in [103].\nAlthough the velocities achieved by the MPCC controller were\nlower than that of [124], [125], the lap times for the same race\ntrack were actually lower due to the ability of the controller to\nfind a new time-allocation that takes into account the current\nstate of the platform at every timestep. The work is further\nextended to solve the time-allocation problem online, and to\nre-plan online [104] while also controlling near the limit of\nthe flight system. Similar work uses tunneling constraints in\nthe MPCC formulation in [134],\n2) Perception Awareness: Other methods that lie in the\nintersection of perception, planning, and control include a\nperception objective in the cost function that helps improve the\nvisibility of an objective or the quality of the state-estimation\npipeline. The methods are called perception aware, and the\nfirst methods were proposed in [135]\u2013[137] This is integral to\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nthe drone-racing problem because, to navigate a challenging\nrace course, the gates that define the course layout must be\nkept in view of the onboard cameras as much as possible.\nAdditionally, coupling the perception with the planning and/or\ncontrol problem can alleviate issues in state estimation because\nthe racing gates are usually feature-rich. Therefore, the use\nof perception-related objectives in both planning and control\npipelines is commonplace [91], [93], [120], [137], [138]. For\nexample, in [93], [137] the authors tackle the problem of\nminimizing the time required by a quadrotor to execute a given\npath, while maintaining a given set of landmarks within the\nfield of view of its on-board camera. Or in [139], where the\nauthors include a perception-aware term in the cost function\nto maximize the visibility of the closest dynamic obstacle,\nin order to readily plan a path that avoids it. These methods\nare called perception-aware [140], and in the following, we\nhighlight their core characteristics.\nThe goal is as follows: navigate a trajectory with low\ntracking error while keeping a point of interest in view while\nminimizing motion blur for maximum feature detection and\ntracking. The first instance applied to agile quadrotors was\nPAMPC introduced in [140]. In this work, a nonlinear pro\u0002gram is optimized using a sequential quadratic programming\napproximation in real time. The cost function contains both\nvehicle dynamic terms as well as perception awareness terms\nsuch as keeping an area of interest in the center of the camera\nframe.\nThis technique is applied to the drone racing problem in\n[141], where an MPPI controller is designed with a Deep\nOptical Flow (DOF) component that predicts the movement\nof relevant pixels (i.e. gates). The perception constraints are\nintroduced into a nonlinear optimization problem and deployed\nin a drone-racing simulator. The approach was not demon\u0002strated onboard real hardware. In [142], a perception-aware\nMPC based on Differential Flatness was used to ensure that\na minimum number of features are tracked between control\nupdates and thus guarantee localization. To achieve this, a\nPerception Chance Constraint within the MPC formulation is\nintroduced to ensure that at least n number of landmarks are\nwithin the field-of-view of the camera at all times with some\nbounded probability.\n3) Discussion: The performance of model-based controllers\ndegrades when the model they operate on is inaccurate [124].\nFor drones, defining a good enough model is an arduous\nprocess due to highly complex aerodynamic forces, which can\nbe difficult to capture accurately within a real-time capable\nmodel. In addition, the tuning process of many model-based\ncontrollers can be arduous, and requires a high level of domain\nexpertise to achieve satisfactory performance.\nIn any optimal control problem, a cost function that the\nuser wants to optimize must be defined. Traditionally, con\u0002venient mathematical functions leveraging convex costs are\nused because these functions are easy to optimize and there\nis a large toolchain available for optimizing such problems\nsuch as Acados [143], CVXGEN [144], HPIPM [145], or\nMosek [146]. In many drone racing papers, the optimal control\nproblem is formulated as follows:\nmin\nu\nx\nT\nN QxN +\nN\nX\u22121\nk=0\nx\nT\nk Qxk + u\nT\nk Ruk , (2)\nsubject to: xk+1 = f RK4(xk,uk, \u03b4t) ,\nx0 = xinit , umin \u2264 uk \u2264 umax ,\nwhere the state is given by xk, the control input is given by\nuk, the state cost matrix is given by Q, and the control cost\nmatrix is given by R. The optimization problem is constrained\nby the dynamics of the system given by f(xk, uk, \u03b4t) where\n\u03b4t is a finite time step. The nonlinear dynamics are typically\npropagated forward using an integrator such as 4th order\nRunge-Kutta, RK4. Additionally, the problem is subject to the\nthrust limits of the platform. umin and umax, and some initial\ncondition of the system x0. In this formulation, a reference\nposition and control are provided by a high-level planner and\nthe goal of the controller is to track the given reference, but\nthis objective is ill-defined for the drone racing problem: in\ndrone racing, we wish to complete the track in as little time\nas possible; therefore, our objective can be better formulated\nas follows:\nmin\nu\nX\nT\nk=0\n\u03b4t , (3)\nsubject to: xk+1 = f RK4(xk,uk, \u03b4t)\nx0 = xinit, x \u2208 X , u \u2208 U\nwhere T is the number of discrete time steps it takes to\ncomplete the race, and the set U contains the input constraints\n(e.g., single-rotor thrust constraints). The set X encodes all\nstate constraints, from possible limits in the state itself (e.g.,\nattitude or velocity constraints), to more complex constraints\nsuch as the fact that the drone has to pass through a set of gates\nin a pre-determined order without colliding. This approach\nrequires a time-horizon that predicts all the way until the end\nof the task which is intractable to optimize online.\nReinforcement learning (RL) methods [36], [122] can opti\u0002mize a proxy of this cost function, however do so in an offline\nfashion, requiring large amounts of training experience to\napproximate the value function. RL methods do not necessarily\ndepend on a high-level planner to provide a reference to track.\nWe will discuss some recent approaches using reinforcement\nlearning methods in the following section.\nIV. LEARNING-BASED APPROACHES\nIn this section, we present various learning-based ap\u0002proaches for drone racing. These approaches replace the plan\u0002ner, controller, and/or perception stack with a neural network.\nLearning-based methods have gained significant traction in the\nlast few years, given their ability to cope with both high\u0002dimensional (e.g. images) or low-dimensional (e.g. states) in\u0002put data, their representation power, and the ease of developing\nand deploying them on hardware.\nThe big advantage of these methods is that they require\nless computational effort than traditional methods, possibly\nenabling low-latency re-planning and control. In addition,\nthey are much more robust to system latencies and sensor\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nnoise, which can be easily accounted for by identifying them\non physical drones and then adding them to the training\nenvironments [36]. However, the major limitation of these\nmethods is their sample complexity. There are currently two\npossibilities for data gathering. The first, mostly popular in\nthe initial stages of learning-based robotics [64], [79], [147]\u2013\n[149] is to collect data in the real world. The data is then\nannotated by a human or an automated process, and used\nfor training. The second, much more popular in recent years\nand currently achieving the best results, consists of using\nsimulation for collecting training data [36], [65], [121], [150],\n[151]. However, significant simulation engineering is required\nto enable generalization if the training data comes from a\nsimulator. Conversely, generalization is easier if data come\nfrom the real world, but the data collection process is very\nslow, tedious, and expensive.\nSurveys covering existing methods for learning-based flight\nalready exist [152], [153]. In contrast to them, we cover the\nmost recent advances and give a broader discussion on the\ncomparison between learning-based and traditional methods\nfor drone racing.\nA. Learned Perception\nSensors\nHardware\nPlanning Control Drone\nSoftware\nFig. 5: Architecture 2: Learned Perception\nFor learned perception modules, the goal of the network\nis to use images from an RGB, depth, or event camera to\ndetect landmarks within the environment and output useful\nrepresentations such as waypoints, or the location of gates\non the track. A depiction of this architecture can be seen in\nFig. 5. An overview of deep learning methods for vision-based\nnavigation specific to drone racing can be found in [153].\nIn [64], a dataset of images is collected from a forward\u0002facing camera mounted on a drone labeled with the relative\nposition to the closest gate. This dataset is used to train a\nnetwork that predicts from an image both the next gate location\nand its uncertainty. Predictions are then fused with a visual\u0002inertial odometry system in an Extended Kalman Filter (EKF)\nto predict the position of the drone on the track. Similarly\nin [16], a Convolutional Neural Network (CNN) is used to\ndetect gate corners in the AlphaPilot challenge. Once the gate\ncorners are detected, classical computer vision algorithms like\nPnP can be used to find the coordinates of the gate in the\ncamera frame. Using an EKF, the gate corner locations can be\nfused with a traditional VIO pipeline to improve the estimates\nof the drone\u2019s location and orientation [16].\nOftentimes, perception networks consume precious re\u0002sources onboard computationally limited drones. To minimize\nthe network processing time, [82], [154] proposed optimized\narchitectures for gate detection on real-world data. A similar\noptimization went into \u201cGateNet\u201d [155] a CNN to detect gate\ncenter locations, distance, and orientation relative to the drone.\nThe same authors developed a follow-up work denoted as\n\u201cPencil-Net\u201d to do gate detection using a lightweight CNN\nin [156]. Most learning-based perception networks can suffer\nfrom poor generalization when deployed in environments that\nwere not included in the training data.To reduce deployment\nsensitivity to lighting conditions or background content, virtual\ngates can be added to real-world backgrounds [157].\nUp until recently, RGB and depth cameras were used\nexclusively in the drone racing task, however, these sensor\nmodalities can be sensitive to changes in the environment such\nas illumination changes. To overcome this, [158] proposed\nusing event cameras coupled with a sparse CNN, recurrent\nmodules, and a You Only Look Once (YOLO) object detector\nto detect gates. The use of event cameras overcomes potential\nissues with motion blur from the rapid movement of the drone\nand is a promising path forward for high-speed navigation.\nOverall, deep learning methods for gate detection are the de\u0002facto standard in all drone racing systems. However, such gate\ndetectors are always coupled with traditional visual-inertial\nodometry systems which explicitly estimate the metric state\nof the drone. These approaches are discussed in Sec. III. It\nis interesting to notice that learning-based odometry systems,\nsuch as [84]\u2013[86] have not yet replaced traditional methods.\nThis is particularly surprising since deep visual odometry\nsystems can specialize to a particular environment, which\ncan be useful for drone racing since the race track is fixed\nand known in advance. A disadvantage of these methods is\nthe high computational cost that makes them impractical for\nonline applications. However, research in end-to-end visual\nodometry is moving forward at a fast pace [86]. Recently,\nworks proposing end-to-end VIO systems for drones have been\npublished [159]\u2013[161]. The work in [159] proposes to learn\nglobal optical flow which is then loosely fused with an IMU\nfor full 6-DoF relative pose estimation. The method in [160]\nand its extension [161] proposes a CNN-based ego-motion\nestimator for fast flights. The performance of this method in\nthe UZH-FPV dataset shows that although end-to-end VIO\nmethods are a promising solution for agile flights, they are\nnot yet mature for drone racing. We foresee that in the near\nfuture, researchers will be able to apply these methods to the\ndrone racing task.\nB. Learned Planning & Perception\nSensors\nHardware\nControl Drone\nSoftware\nFig. 6: Architecture 3: Learned Planning and Perception\nA tightly-coupled planning and perception stack (Figure 6)\nis a very attractive algorithmic perspective. First, it greatly\nsimplifies the perception task: an explicit notion of a map\nor globally-consistent metric state is not required. Second, it\nlargely reduces computational costs, both in the pre-training\nand evaluation stages. Finally, it can leverage large amounts\nof data, collected either in simulation or the real world, to\nbecome robust against noise in perception or dynamics. Yet,\nan interesting observation is that these methods still work\nbest when coupled with an explicit estimator of the metric\nstate [5]. In contrast to traditional methods, a locally consistent\nodometry system is sufficient [65], [79], [150], waving away\nthe complexities of full-slam methods (e.g. loop-closure).\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nIn [79], a coupled perception and planning stack for drone\nracing is trained using real-world flight demonstrations. While\ngood performance is indicated on the racing task as well as\nrobustness against drift in state estimation, the method requires\nre-training for each new environment. Therefore, in the follow\u0002up work [65], data generated entirely in simulation is used\nto train the perception-planning stack, waiving the labor and\ntime-consuming requirement of data collection in the real\nworld. A similar pipeline was used for high-speed autonomous\nflight through complex environments in [150], which pro\u0002poses to train a neural network in simulation to map noisy\nsensory observations to collision-free trajectories directly.\nThis approach was later extended to nano-quadcopters [162],\nwhich won the authors the first position in the IMAV 2022\nNanocopter AI Challenge. Recent work [163], [164] has shown\nthe possibility of training sensorimotor controllers for obstacle\navoidance end-to-end using reinforcement learning, paving the\nway towards a system that could solve drone racing completely\nend-to-end. However, these works still rely on explicit state\nestimation and a controller to execute velocity commands.\nSeveral other works apply a similar stacked perception\nand planning pipeline for other autonomous drone racing\ntasks [147]\u2013[149], [165]. We point the interested reader to ex\u0002isting surveys on the role of learning in drone navigation [152].\nA few works also studied the planning problem using data\u0002driven methods, decoupling it from the perception problem.\nAn interesting approach demonstrated in the NeurIPS Game of\nDrones competition [166] used an off-the-shelf reinforcement\nlearning algorithm in place of a classic model-based planner\nfor drone racing [167].\nC. Learned Control\nSensors\nHardware\nPerception\nSoftware\nPlanning Drone\nFig. 7: Architecture 4: Learned Control\nData-driven control, like reinforcement learning, allows for\novercoming many limitations of prior model-based controller\ndesigns by learning effective controllers directly from expe\u0002rience. For example, model-free RL was applied to low-level\nattitude control [168], in which a learned low-level controller\ntrained with PPO outperformed a fully tuned PID controller\non almost every metric. Similarly, [169] used model-based RL\nfor low-level control of an a priori unknown dynamic system.\nMore related to drone racing, recent works showcased the\npotential of learning-based controllers for high-speed trajec\u0002tory tracking and drone racing [36]. Imitation learning is more\ndata efficient compared to model-free RL. In [170], aggressive\nonline control of a quadrotor has been achieved via training\na network policy offline to imitate the control command\nproduced by a model-based controller. Similarly, [171] studied\nreal-time optimal control via deep neural networks in an\nautonomous landing problem. Other work in this category\nhas shown that reinforcement learning can find optimal [122],\n[172] or highly adaptive controllers [173].\nWith a learning-based controller, it can be difficult to\nprovide robustness guarantees as with traditional methods such\nas the Linear Quadratic Regulator (LQR). While a learning\u0002based controller may provide superior performance to classical\nmethods in simulation, it may be that they cannot be used\nin the real world due to the inability to provide an analysis\nof the controller\u2019s stability properties. This is particularly\nproblematic for tracking the time-optimal trajectories required\nby drone racing. Recent works have attempted to address this\nusing Lyapunov-stable neural network design for the control\nof quadrotors [174]. This work shows that it is possible to have\na learning-based controller with guarantees that can also out\u0002perform classical LQR methods. Building upon this concept,\nreachability analysis, and safety checks can be embedded in a\nlearned Safety Layer [175].\nD. Learned Planning & Control\nSensors\nHardware\nPerception\nSoftware\nDrone\nFig. 8: Architecture 5: Learned Planning & Control\nThe second paradigm of learned control is to produce the\ncontrol command directly from state inputs without requiring\na high-level trajectory planner, as shown in the architecture\ndiagram of Figure 8. This approach enabled an autonomous\ndrone with only onboard perception, for the first time, to\noutperform a professional human, and is state-of-the-art at\nthe time of writing [5]. In autonomous drone racing, this\nwas proposed by [4], [122], where a neural network policy\nis trained with reinforcement learning to fly through a race\ntrack in simulation in near-minimum time. Major advantages\nof the reinforcement-learning-based method are its capability\nto handle large track changes and the scalability to tackle\nlarge-scale random track layouts while retaining computational\nefficiency. In [123], deep reinforcement learning is combined\nwith classical topological path planning to train robust neural\nnetwork controllers for minimum-time quadrotor flight in\ncluttered environments. The learned policy solves the planning\nand control problem simultaneously, forgoing the need for\nexplicit trajectory planning and control.\nIn this same category, another class of algorithms try\nto exploit the benefits of model-based and learning-based\napproaches using differentiable optimizers approaches [176]\u2013\n[178], which leverage differentiability through controllers. For\nexample, for tuning linear controllers by getting the analytic\ngradients [179], or for creating a differentiable prediction,\nplanning and controller pipeline for autonomous vehicles\n[180]. On this same direction, [181] equips the RL agent with\na differentiable MPC [176], located at the last layer of the\nactor network that provides the system with online replanning\ncapabilities and allows the policy to predict and optimize\nthe short-term consequences of its actions while retaining the\nbenefits of RL training.\nAll these methods inherit the classic advantage of policy\nlearning. In addition, they do not require an external controller\nto track the plan. This eliminates the discrepancy between the\nplanning and deployment stages, which is one of the main\nlimitations of traditional planning methods (Sec. III-B). Some\nof the limitations of traditional planning remain, such as the\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nrequirement of a globally-consistent state estimation and a map\nof the environment. Also, they have not yet been demonstrated\nin sparse long-horizon planning problems, e.g. flying through\na maze at high speeds, where their performance would likely\ndrop due to sample complexity.\nE. End-to-End Flight\nSensors\nHardware\nDrone\nSoftware\nFig. 9: Architecture 7: End to End Learning\nExpert pilots take raw sensory images from a first-person\u0002view camera stream and map directly to control commands.\nIn this section, we explore approaches emulating this holistic\nnavigation paradigm in autonomous drones.\nTwo families of approaches can be used to pursue an end-to\u0002end navigation paradigm. The first is substituting each of the\nperception, planning, and control blocks with a neural network.\nThis structure is followed by [182], [183], where the authors\ntrain a perception-planning network and a control network\nusing imitation learning. The perception network takes raw\nimages as input and predicts waypoints to the next gate.\nThe control network uses such predictions with ground-truth\nvelocity and attitude information to predict control commands\nfor tracking the waypoints. They showed improvements over\npure end-to-end approaches, which directly map pixels to con\u0002trol commands and were able to show competitive lap times\non par with intermediate human pilots within the Sim4CV\nsimulator [184]. Yet, the division into independent blocks leads\nto compounding errors and latencies, which negatively affect\nperformance when flying at high speeds [150].\nThe second family of approaches directly maps sensor\nobservation to commands without any modularity. This design\nis used by [185], which to date remains the only example of\nthe completely end-to-end racing system. Indeed, other end\u0002to-end systems generally require an inner-loop controller and\ninertial information to be executed. For instance, [186] trains\nan end-to-end CNN to directly predict roll, pitch, yaw, and\naltitude from camera images. Similarly, [187], [188] use a\nneural network to predict commands directly from vision. To\nimprove sample complexity, they use contrastive learning to\nextract robust feature representations from images and leverage\na two-stage learning-by-cheating framework.\nIndependently of the design paradigm they follow, end\u0002to-end navigation algorithms are currently bound to simula\u0002tion. The reasons why no method was successfully deployed\nin the real world include weak generalization to unseen\nenvironments, large computational complexity, and inferior\nperformance to other modular methods. Another interesting\nobservation is that humans can pilot a drone exclusively\nfrom visual observations. Conversely, except for [185], end\u0002to-end systems still rely on the state extracted from other\nmeasurement modalities, e.g. an IMU. The question of whether\nautonomous drones can race in the real world at high-speed\nwithout any inertial information remains open. We provide\nmore details on this question in Section VIII.\nF. Discussion\nData-driven approaches are revolutionizing the research in\nautonomous drone racing, ranging from improving the system\nmodel to end-to-end control. Currently, the best-performing\nalgorithms for drone racing include a learning-based com\u0002ponent [16], [17], and this trend is unlikely to change in\nthe coming years. Indeed, compared to classical model-driven\ndesign, they can process high-dimensional sensory inputs\ndirectly, can be made robust to any modeling uncertainty (e.g.\nlatency) by simply incorporating it in the training pipeline, and\nrequire far less engineering effort for tuning and deploying\nthem [36].\nOur analysis shows that the majority of learning-based\napproaches heavily rely on simulators. While simulators may\nget better and faster in the near future, recent advances in real\u0002world training [189], [190] and fine-tuning [191], [192] offer a\npotential alternative for zero-shot simulation to reality transfer\nfor sensorimotor policies. However, so far, these works have\nbeen limited to legged locomotion. Extension to agile drones\ncould lead to the successful deployment of end-to-end policies,\npossibly improving the state of the art in agile flight.\nAnother limitation of the approaches discussed in this\nsection is their inability to adapt to new and uncertain en\u0002vironments quickly. The field of adaptive control has studied\nthis problem extensively [193]\u2013[195]. Inspired by these works,\nthere has been a recent push to use advancements in machine\nlearning within the adaptive control framework. A method to\nlearn parametric uncertainty functions is introduced in [196].\nThese uncertainty functions could be learned offline using data\ncaptured from agile flight experiments, and then embedded\nwithin an adaptive controller to adjust controller parameters\nonline during flight. Results indicate that highly accurate\ntrajectory tracking can be achieved with this approach, even\nin the face of strong wing gusts exceeding 6.5 m/s. More\nrecently, learning-based controllers have shown the ability to\nadapt zero-shot to large variations in hardware and external\ndisturbances [197]. We see this as a promising area of research\nand one that is integral for reliable performance in changing\nenvironmental conditions.\nV. DRONE RACING SIMULATORS\nOne tool that has drastically accelerated the progress of\nresearch in autonomous drone flight is the use of simulation\nenvironments that attempt to recreate the conditions that\nreal drones experience when flying. Over the years, several\nsimulation environments have been developed for the use of\ngeneral research.\nIn 2016, the widely used RotorS simulation environment\nwas published, which extends the capabilities of the popular\nGazebo simulation engine to multi-rotors [23]. Gazebo uses\nthe Bullet physics engine for basic dynamic simulation and\ncontact forces. Linear drag on the body of the multicopter is\nsimulated based on the cross-sectional area and linear velocity\nof the simulated object. The RotorS extension features many\neasy-to-use plugins for developing multi-rotors, however, it\ndistinctly lacks the photorealistic details needed to simulate\naccurate behavior of estimation and perception pipelines.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nAirSim was introduced by Microsoft in 2018 as a photo\u0002realistic simulator for the control of drones [21]. It is built on\nthe Unreal graphics engine and features easy-to-use plugins\nfor popular flight controllers such as PX43, ArduPilot4, and\nothers. It was used in the 2019 NeurIPS Game of Drones\nchallenge [166]. Because of the photorealism of AirSim, it\nis possible to simulate the entire perception and estimation\npipeline with a good possibility of transfer to real-world drone\nsystems. Additionally, AirSim comes pre-packaged with an\nOpenAI-Gym environment for training Reinforcement Learn\u0002ing algorithms. Organizations such as Bell, Airtonomy, and\nNASA are using AirSim to generate training data for learning\u0002based perception models.\nFlightGoggles [55] was developed as another photorealistic\nsimulator and was used as the primary simulation environment\nfor the Lockheed Martin AlphaPilot challenge. FlightGoggles\ncontains two separate components: a photorealistic render\u0002ing engine built with Unity3D and a dynamic simulation\nimplemented in C++. FlightGoggles provides an interface\nwith real-world vehicles using a motion capture system; such\nan interface allows the rendering of simulated images that\ncorrespond to the position of physical vehicles in the real\nworld.\nA recent simulator focused on Safe RL was proposed\nin [198]. It uses Gazebo and the Pybullet physics engine as\nthe backend. Leaderboards for several safety-focused training\nenvironments exist, encouraging researchers to submit their\napproaches and compete with other researchers around the\nworld.\nFlightmare [22] is a simulation environment featuring pho\u0002torealistic graphics provided by the Unity engine. The physics\nengine is decoupled and can be swapped out with various\nengines for user-defined levels of simulation fidelity. Similar to\nFlightGoggles, Flightmare can also provide hardware-in-the\u0002loop simulation functions where a virtual, synthetic camera\nimage can be provided to the drone for use in control and\nestimation [8].\nFinally, Aerial Gym [199] is a GPU-accelerated simulator\nthat allows simulating millions of multirotor vehicles in paral\u0002lel with nonlinear geometric controllers for attitude, velocity\nand position tracking. Additionally, the simulator offers a\nflexible interface for modeling a large number of obstacles\nand generating data such as RGB, depth, segmentation, and\noptical flow.\nVI. COMPETITIONS\nTo gauge the progress of the field as a whole, several\ndrone racing competitions have taken place since 2016. We\ninclude a graphical overview of these events in Figure 2. The\nAutonomous Drone Racing (ADR) competition was an annual\ncompetition which took place during the IROS conference\nbetween 2016 and 2019. In 2016, 11 teams competed in\nautonomous drone racing and were tasked to navigate a series\nof gates in sequence. The positions of the gates were not\nknown to the participating teams ahead of time, therefore\n3https://px4.io/\n4https://ardupilot.org/\nteams flew very cautiously identifying the next waypoints\nonline. Each team was given 30 minutes prior to the official\ncompetition to fly the course as many times as they wished.\nThe winning team, from KAIST, made it through 10 of the 26\ngates in 1 minute and 26 seconds. For comparison, a human\nwas able to complete the entire 26-gate course in 1 minute 31\nseconds. A survey summarizing the approaches used for these\nearly competitions can be found in [19]. The following year,\na similar competition took place during IROS in Vancouver,\nCanada, with better results. This time, 14 teams participated\nand were given a CAD drawing of the course prior to the\nevent with locations and dimensions of all gates. Only 5 teams\nparticipated in the final in-person event, with the winning team\nmaking it through 9 out of 13 gates in over 3 minutes. A\nsummary of the winning approaches can be found in [14].\nTwo more ADR competitions took place at IROS 2018 and\n2019, with drones navigating courses faster and more reliably.\nIn 2019, Lockheed Martin sponsored the AlphaPilot AI\nDrone Racing Innovation Challenge where a 1 million dollar\ngrand prize was awarded to the winning team [200]. The\ncompetition took place first in a virtual qualifying round which\nused the FlightGoggles simulation environment [55]. Nine\nteams out of more than 400 worldwide qualified for the final\nchallenge which included navigating a new track in a time-trial\nsetting against an expert human pilot. Such competition took\nthe form of a tournament, with three seasonal races and a final\nchampionship race. This made it very different from previous\nsingle-day competitions. Ultimately, professional pilot Gabriel\nKocher, from the Drone Racing League, manually piloted his\ndrone through the course in only 6 seconds. It took 11 seconds\nto the winner, MAVLab from TU Delft, and 15 seconds\nto the second-place winner, UZH-RPG from the University\nof Zurich, to complete the course autonomously. The two\ndifferent approaches are documented in [16], [17]. Further\ncomments are provided by the winner in [201]. Perez et al.\nprovide an overview of the types of hardware used for some\nof the drone racing competitions mentioned so far [53].\nIn 2019, the Game of Drones competition took place at the\nNeurIPS conference. This competition was purely simulation\u0002based and used the AirSim simulation environment built by\nMicrosoft [15], [21], [166]. Participants in the Game of Drones\ncompetition raced against simulated opponents in a head-to\u0002head fashion, similar to how humans compete in FPV drone\nracing. Teams raced against a single simulated opponent,\nnavigating through a complex series of gates in three different\ntiers: Planning Only, Perception Only, and Perception with\nPlanning.\nIn 2022, at the Swiss Drone Days event in Zurich, Switzer\u0002land, three of the world\u2019s best human pilots competed against\nresearchers from the Robotics and Perception Group of the\nUniversity of Zurich. Flight speeds exceeding 100 kph were\ndemonstrated by the autonomous drones. When relying on\nmotion capture, the autonomous drones were able to achieve\nsignificantly faster laptimes than the expert human pilots.\nThey additionally demonstrated it was possible to win races\nwithout motion capture, using only onboard computing and\nsensors to navigate the race track. IEEE Spectrum author Evan\nAckermann discusses the multi-day event in [202].\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nLooking into the future, the Abu Dhabi Autonomous Racing\nLeague recently announced plans for an autonomous drone\nracing competition in 2025.\nVII. DATASETS, HARDWARE, AND OPEN SOURCE CODE\nIn this section, we provide an overview of the existing open\nsource code bases, useful datasets for autonomous drone racing\nas well as hardware considerations. We first discuss datasets,\nand then group the existing open source code bases by their\nuse-cases in table I and conclude with a brief overview over\ndrone racing hardware.\nA. Datasets\nIn 2018, researchers from MIT released a large scale dataset\nfor perception during aggressive UAV flight [205]. This dataset\ncontains over 10 hours of flight data which includes simulated\nstereo and downward-facing camera images at 120 Hz, real\u0002world IMU data at 100 Hz, motor speed data at 190 Hz, and\nmotion capture data at 360 Hz. The sensor suite was chosen\nsuch that algorithms like Visual-Inertial Odometry (VIO) or\nSimultaneous Localization and Mapping (SLAM) could be\nevaluated on the dataset.\nIn 2019, the UZH-FPV Drone Racing Dataset was released,\nwhich contains many agile maneuvers flown by a professional\nracing pilot [78]. The dataset includes indoors and outdoors\nreal-world camera images, inertial measurements, event cam\u0002era data, and ground truth poses provided by an advanced\nmotion capture system (a total station) providing millimeter\u0002level accuracy. In 2024, the dataset was extended with new\ndata recorded onboard an autonomous racing drone flying\nin a racing track with peak speed exceeding 20 m/s. This\nnew data includes large field-of-view camera images, inertial\nmeasurements, and ground truth from a motion capture system.\nSimilar to the authors in [205], the authors of this dataset hope\nto push the state of the art in state estimation during aggressive\nmotion and have created competitions to allow researchers to\ncompete against one another on this agile flight benchmark.5.\nA recent effort reported in [207] open-sourced high-quality\ndata from both autonomous and human-piloted flights. This\neffort enables the study of both the perception and control\nproblem without actual hardware, lowering the barrier of entry\nfor studying drone racing.\nResearch on how expert human pilots focus on their targets\nduring flying and provide a dataset that contains flight trajec\u0002tories, videos, and data from the pilots is examined in [206]\nNeuroBEM [35] is a hybrid aerodynamic quadrotor model\nwhich combines blade-element-momentum-theory models\nwith learned aerodynamic representations from highly ag\u0002gressive maneuvers. While the model is fit to the specific\nquadrotor platform defined in [8], the approach can be used\nfor any quadrotor platform and provides over 50% reduction in\nmodel prediction errors compared to traditional, exclusively\u0002first-principles approaches.\n5https://fpv.ifi.uzh.ch/uzh/uzh-fpv-leader-board/\nB. Open-Source Code\nA significant amount of autonomous drone racing research\nhas been open sourced to the community, making implemen\u0002tation less daunting for newcomers to the field. A collection of\nall known drone racing repositories has been provided to the\nreader in Table I. These code bases range across controllers,\nplanners, sensor calibration, and even entire software stacks\ndedicated to drone racing. We encourage both newcomers and\nexperienced researchers to check out the extensive amount of\nopen source code bases available and contribute back to the\ncommunity.\nC. Hardware\nThis survey does not intend to cover the hardware design\nof racing drones rigorously. For an in-depth overview, see [8],\nwhere the hardware and software design for developing a very\ncapable research platform are discussed. To make this survey\nself-contained, this section presents a brief overview of the\nhardware design of a racing drone nevertheless.\n1) Racing Drone Design: A suitable hardware design\nshould maximize the agility and acceleration of the drone,\nand hence, it needs to be as lightweight as possible [210].\nFor drones featuring onboard compute, the drone size is thus\nlower-bounded by the size of the computer. Currently, the\nNVIDIA Jetson family is the smallest off-the-shelf hardware\nwith sufficient compute to run complex neural networks, and\nit leads to drones built on 6 inch frames. Carbon fiber offers\nan excellent compromise between the weight and durability of\nthe frame, while other parts (such as holders for the computer)\ncan be designed using a 3D printer.\nFor actuation, fast-spinning brushless DC motors are ideal\nbecause of their high specific power output, often exceeding\n500 W for a 50 g motor. In general, larger propellers will\nimprove the energy efficiency of the drone [7] while smaller\npropellers lead to a faster motor response. On a 6 inch frame,\nthree-bladed 5 inch propellers present a good compromise. To\nsustain the power demand of brushless drone-racing motors\n(often exceeding 2 kW at full throttle [7]) a lithium-polymer\nbattery with a sufficiently high discharge current rating (e.g.,\n120 C) is required.\nThe Pixhawk PX4 flightstack, despite being commonly used\nfor quadrotors [11], [211], fixed-wings [212], and hybrid\nVTOL platforms [213], is not optimized for agile flight.\nConversely, agile autonomous research platforms [5], [8] use\nBetaflight as a low-level controller, similar to professional\nhuman racing pilots.\nThe design of a capable racing drone is important for re\u0002searchers developing new technology. However, in many drone\nracing competitions, the hardware design is not left to the\nparticipants but is standardized. This approach is common in\nhuman drone racing, where thousands of identical drones are\nbuilt before each competition. This concept was also adopted\nby the AlphaPilot [200] competition, where all participants\nused a given platform. Overall, this approach ensures fair\ncompetition.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nTABLE I: Open Source Software and Datasets\nName and Reference Category Year Link\nPAMPC [140] Controller 2018 https://github.com/uzh-rpg/rpg mpc\nDeep Drone Acrobatics [121] Controller 2019 https://github.com/uzh-rpg/deep drone acrobatics\nData Driven MPC [40] Controller 2020 https://github.com/uzh-rpg/data driven mpc\nHigh MPC [203] Controller 2022 https://github.com/uzh-rpg/high mpc\nAutoTune [204] Controller Tuner 2022 https://github.com/uzh-rpg/mh autotune\nBlackbird [205] Dataset 2018 https://github.com/mit-aera/Blackbird-Dataset\nUZH-FPV [78] Dataset 2019 https://fpv.ifi.uzh.ch/\nNeuroBEM [35] Dataset 2020 https://rpg.ifi.uzh.ch/NeuroBEM.html\nEye Gaze Drone Racing [206] Dataset 2021 https://osf.io/gvdse/\nTII Drone Racing Dataset [207] Dataset 2024 https://github.com/tii-racing/drone-racing-dataset\nTime-optimal Planning for Quadrotor Waypoint Flight [95] Planner 2021 https://github.com/uzh-rpg/rpg time optimal\nMinimum-Time Quadrotor Waypoint Flight in Cluttered\nEnvironments [37]\nPlanner 2022 https://github.com/uzh-rpg/sb min time quadrotor planning\nRotorS [23] Simulator 2016 https://github.com/ethz-asl/rotors simulator\nAirSim [166] Simulator 2018 https://microsoft.github.io/AirSim/\nFlightGoggles [55] Simulator 2019 https://github.com/mit-aera/FlightGoggles\nFlightmare [22] Simulator 2020 https://uzh-rpg.github.io/flightmare/\nLearning to fly\u2014a gym environment with pybullet physics for\nreinforcement learning of multi-agent quadcopter control [198]\nSimulator 2021 https://github.com/utiasDSL/gym-pybullet-drones\nAerial Gym [199] Simulator 2023 https://github.com/ntnu-arl/aerial gym simulator\nSim 2 Real Domain Randomization [65] Sim2Real Transfer 2019 https://github.com/uzh-rpg/sim2real drone racing\nRPG Quadrotor Control [26] Software Stack 2017 https://github.com/uzh-rpg/rpg quadrotor control\nAgilicious [8] Software Stack 2022 https://github.com/uzh-rpg/agilicious\nKalibr [56] Camera Calibration 2022 https://github.com/ethz-asl/kalibr\nVID-Fusion [74] Estimation 2021 https://github.com/ZJU-FAST-Lab/VID-Fusion\nFast-Racing [87] Planner 2021 https://github.com/ZJU-FAST-Lab/Fast-Racing\nEgo-planner [208] Planner 2021 https://github.com/ZJU-FAST-Lab/ego-planner\nGCOPTER [115] Planner 2022 https://github.com/ZJU-FAST-Lab/GCOPTER\nFASTER [209] Planner 2021 https://github.com/mit-acl/faster\nPanther [139] Planner 2022 https://github.com/mit-acl/panther\nDeep Panther [138] Planner 2023 https://github.com/mit-acl/deep panther\nRaptor [91] Planner 2021 https://github.com/HKUST-Aerial-Robotics/Fast-Planner\n2) Beyond Quadcopters: While this survey focuses on\nmulti-copter drones, future drone racing competitions will\ngo beyond this platform. Indeed, FPV Fixed-Wing Racing is\nalready a popular sport among human pilots [214]. For exam\u0002ple, vertical takeoff and landing (VTOL) drones might offer\na great alternative to quadcopters. VTOL aircraft combine\nthe high speeds achieved by fixed-wing drones with some of\nthe maneuverability of multicopters. Pioneering works on this\nplatform have already shown agile control [215] and trajectory\ngeneration for aerobatic VTOL flight [216]. Perhaps, once\nsuch research platforms are available off the shelf, VTOL\naircraft racing will become a popular platform for autonomous\ndrone racing research.\nVIII. OPEN RESEARCH QUESTIONS AND CHALLENGES\nWhile a lot of progress has been made, there are still many\nchallenges to be overcome in drone racing research. In the\nfollowing, we discuss the most interesting challenges in detail.\nA. Challenge 1: Reliable State Estimation at High-Speeds\nIn its current form, online, robust, and accurate state\nestimation is highly beneficial when pushing autonomous\ndrones to their limits. Currently, classical state estimation\napproaches based on visual-inertial odometry cannot cope\nwith the perceptual challenges present in drone racing tasks.\nMotion blur, low texture, and high dynamic range are some\nreasons why classical VIO algorithms accumulate large errors\nin localization. The miscalibration of intrinsic and extrinsic\ncamera parameters can lead to improper estimates of the\ncamera pose on a drone. This is due to local movements of the\ncamera frame relative to the drone body, as well as changes\nin temperature and pressure. VIO drift can render the state\nestimates unusable unless corrected through localizations to\na prior map. New sensor modalities, such as event cameras,\ncould potentially alleviate this issue. Although event-aided\nVIO algorithms for drones have been proposed to improve\nrobustness to motion blur, they have not been demonstrated at\nhigh speeds as seen in drone racing. Future research in agile\nflight may focus on finding new event representations that are\ncomputationally efficient and compatible with classical VIO\nformulations. One example is to exploit direct methods [217].\nOther promising sensor modalities are motor speed controllers\nand force sensors. These sensor measurements could be used\nto include more advanced drone models in VIO, e.g. modeling\naerodynamics effects, in order to limit the drift that accu\u0002mulates where camera measurements are degraded. One of\nthe main consequences of motion blur, low texture, and high\ndynamic range is unreliable feature extraction and matching.\nThis consequently degrades the performance of the visual\nfrontend. Deep learning methods have the potential to solve\nthis problem. What hinders the application of these methods to\ndrone racing at the moment is their computational cost. Future\nresearch should work on lightweight neural networks that can\nprovide inference at a high rate. Neural networks could also\nbe used to remove non-zero mean noise and constant errors\nfrom the inertial measurements. A potentially fruitful area of\nresearch is in combining neural networks for input processing\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nwith a geometry-based VIO backend. This could lead to the\nnext step in the research on VIO for drone racing. Current\nworks [86], [218] have shown that this direction outperforms\nend-to-end visual-based odometry methods.\nB. Challenge 2: Flying from Purely Vision\nState-of-the-art autonomous navigation methods rely on\nvisual and inertial information, usually combined with classic\nperception algorithms. Conversely, expert human pilots rely\non nothing more than a first-person-view video stream, which\nthey use to identify goals and estimate the ego-motion of the\ndrone. Building systems that, similarly to human pilots, only\nrely on visual information is very interesting from a scientific\nperspective. Indeed, since simulating RGB is very challenging,\nsolving this question might require lifelong learning algo\u0002rithms operating in the real world. In addition, eliminating\ninertial information might have some engineering advantages\ntoo, e.g., data throughput, power consumption, and lower cost.\nSeminal works in this direction try to understand how humans\nsolve this task [206], [219]. They found that expert pilots can\ncontrol drones despite a 200ms latency, which is compensated\nby the human brain. Taking inspiration from biology, a recent\nwork [220] shows that it is possible to fly with camera images\nand an onboard gyroscope (e.g., removing the accelerometer),\nas long as the system never hovers. However, the above\nquestions still remain mostly open and a good avenue for\nresearch at the intersection of computer vision, neuroscience,\nand biology.\nC. Challenge 3: Multiplayer Racing\nMuch of the work done up until this point on autonomous\ndrone racing has focused on time-optimal flight without con\u0002sidering how a capable opponent might impact the compe\u0002tition dynamics. In FPV races, pilots can compete against\nup to 5 opponents simultaneously, bringing about the need\nto anticipate how their opponents might behave. Humans\nare astonishingly capable of recognizing opportunities for\novertaking and executing complex maneuvers in the face of\nlarge aerodynamic disturbances caused by flying close to\nanother drone. Achieving such capabilities requires an agent\nto estimate their opponent\u2019s state using only onboard visual\nsensors. However, these observations in drone racing are\nsparse because the camera faces forward along the heading\naxis, meaning that the only time an opponent is observable\nis when the ego-agent is behind them. Sophisticated motion\nand planning models which can propagate predictions of the\nopponents\u2019 states and racing lines through time are necessary\nto anticipate collisions or overtaking opportunities. One way\nto simplify the problem is combining classical vision with\nlearning-based control, which has shown promising results in\nmulti-agent zero-sum games for locomotion [221]. An initial\nstudy [222] examined how game-theoretic planners can lead\nto highly competitive behavior in two-player drone racing,\nhowever, this work was confined to racing on a 2D plane.\nThe work was further extended to 3D spaces in [223], but\nthere is a significant opportunity for researchers to explore\nthe competitive nature of drone racing and develop interesting\nracing strategies that lead to time-optimal agents that are able\nto deal with complex opponent behavior.\nD. Challenge 4: Safety\nAutonomous drone racing research has so far focused\non demonstrating that superhuman performance in racing is\npossible in controlled conditions [5] but has put less em\u0002phasis on risk and safety. We predict that this trend will\nsoon change. Adding safety to agile flight has gained much\nattention recently [9], [198]. Initial works focused on gen\u0002erating a collision-free trajectory [224]\u2013[226] with less em\u0002phasis on performance. More geared towards agile flight, the\nworks [209], [227]\u2013[229] have studied the problem of trading\noff safety and performance. All the aforementioned works\nrely on solving constrained optimization problems. Outside\nof drone racing, similar paradigms have been developed and\nhave the potential to inspire future algorithms. Such methods\nare, for example, conformal analysis [230], chance-constrained\ndynamic programming [231], control barrier functions [232],\nor reachability analysis [233]. The latter has been successfully\napplied in the context of autonomous driving with collision\navoidance [234], [235].\nMore modern, learning-based methods have been explored\nfor risk-aware autonomous driving in the context a of map\u0002prediction approach [236] and in combination with Tube\nMPC [237], a form of MPC that takes stochasticity into\naccount. However, such approaches generally do not scale\nto high-dimensional perception but rely on robust state\u0002estimation for all involved agents. Combining such algorithms\nwith the methods for vision-based, high-speed drone racing\npresented in this survey could solve both of these prob\u0002lems simultaneously. As a first step in this direction, recent\nwork [238] has shown that a learned control policy can be\nconditioned on an auxiliary input signal from a user. The\nsignal regulates the maximally available thrust, leading to a\nsingle learned policy that can race at various speeds and risk\nlevels.\nE. Challenge 5: Transfer to Real-World Applications\nDrone racing, while an extraordinarily challenging research\nenvironment, is ultimately not the end goal. Opportunities\nexist for technology transfer between the drone racing research\ncommunity to real-world applications such as search and res\u0002cue, inspection, agriculture, videography, delivery, passenger\nair vehicles, law enforcement, and defense. However, applica\u0002tions that leverage the full agility of the platform have much\nto gain. Drones that fly fast, fly farther, therefore increasing\nthe productivity of drones in every commercial sector [7].\nOne of the major challenges to real-world application is gen\u0002eralization to conditions where the environmental knowledge\nbefore deployment is limited. For example, we often do not\nhave a known map ahead of time for real-world applications,\nwhich requires simultaneous estimation of the state of the\ndrone while mapping the environment. However, a central\ntheme of drone racing research has been the development of\nadaptive control strategies and decision-making algorithms to\nenable drones to react rapidly to changes in the race track or\nthe robot condition(Sec. III-C and Sec. III-A). These strategies\ncan be used to handle real-world applications where envi\u0002ronmental knowledge is imperfect and to enable adaptation\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nto unforeseen obstacles and challenges. In addition, learning\u0002based sensorimotor controllers for drones, increasingly more\npopular due to research on racing, have been designed with\nthe ability to generalize from limited data, adapt, and improve\ntheir performance over time (Sec. IV). Such generalization and\nadaptation abilities have already been applied to cases where\nthere is no previous knowledge of the environment [150].\nBuilding algorithms that can continually improve from their\nexperience is another alternative to favor this transfer. While\nrecent advances in reinforcement learning research point to the\nfeasibility of this path [191], [192], [239], it is unclear when\nand how such recent approaches would be applicable to drones\nor similarly agile platforms in the real world. Collecting data\nfor continual RL onboard a drone is notoriously difficult. This\nis because the drone does not have the luxury of remaining in\ncontact with the ground like legged robots and cars, and thus\nhas to immediately know how to hover otherwise a crash will\noccur. One interesting area that may be useful for continual RL\nin drones is the notion of \u201csafe-RL\u201d. The goal of safe RL is to\nenable exploration without ever incurring catastrophic failure\nof the system. Initial work on this topic can be found in [240].\nA survey paper covering safe RL methods can be found in [9].\nFurthermore, a thorough review paper on continual, or life\u0002long RL can be found in [241].\nIX. CONCLUSIONS AND SUMMARY\nFrom racing at a pace comparable to walking speed [19],\nautonomous drones have advanced to surpassing world cham\u0002pions [5]. Such an exponential advance has been driven by\nboth algorithmic innovations, e.g., learning sensorimotor con\u0002trollers in simulation, and system engineering improvements.\nSuch advances span the entire navigation pipeline: perception,\nplanning, and control. Our paper comprehensively covers each\nof these topics. Methodologically, the dominant trend has been\na shift from conventional methods to data-driven solutions.\nHowever, in contrast to fields like computer vision and natural\nlanguage processing, neural networks did not replace but\ncoexist with traditional methods: no method with competitive\nperformance in the real world is fully data-driven. The most\nresilient part of the pipeline is state estimation, where strong\nprior knowledge about the dynamics and environment are still\nneeded to cope with the lack of sensorimotor data. In the short\nterm, we predict that such a hybrid approach could be applied\nto other physical systems, e.g., autonomous ground vehicles\nand personal robots. However, in the long term, we predict\nthat, similarly to research in computer vision and natural\nlanguage processing, neural networks will replace each part of\nthe pipeline. This will require many innovations, e.g., compu\u0002tationally efficient architectures, offline pre-training strategies,\nand fast adaptation schemes to previously unseen conditions.\nWhile autonomous drones are already superhuman in con\u0002trolled scenarios, many challenges are yet to be solved to\noutperform human champions in official drone racing leagues\nand transfer the findings to real-world applications.\nX. ACKNOWLEDGEMENTS\nThe authors thank Manasi Muglikar for her valuable inputs\non event-camera methods for state estimation and perception.\nXI. CONTRIBUTIONS\nDrew Hanover initiated the idea of this paper, created the\npaper structure, and contributed to all sections of this paper\nwhile coordinating efforts amongst the co-authors. Antonio\nLoquercio contributed to the paper structure and the learning\u0002based sections. Leonard Bauersfeld authored the Drone Mod\u0002eling section and created the graphics seen throughout. Angel\nRomero contributed to the Classical Planning and Control\nsections. Giovanni Cioffi contributed to the Classical Percep\u0002tion and Challenges sections. Yunlong Song contributed to\nthe Simulators and Learning-Based Planning/Control sections.\nRobert Penicka contributed to both Classical and Learning\u0002Based Planning sections. Elia Kaufmann contributed to the\npaper structure and throughout the Learning-Based sections.\nDavide Scaramuzza contributed to the general paper structure\nand revised the paper thoroughly and critically.\nREFERENCES\n[1] T. A. Wilkinson, Early Dynastic Egypt. Routledge, 2002.\n[2] S. M. Arab, \u201cThe sed-festival (heb sed) renewal of the kings\u2019 reign,\u201d\nArab World Books, Nov 2017.\n[3] J. Betz, H. Zheng, A. Liniger, U. Rosolia, P. Karle, M. Behl, V. Krovi,\nand R. Mangharam, \u201cAutonomous vehicles on the edge: A survey on\nautonomous vehicle racing,\u201d IEEE Open J. Intell. Transp. Syst., vol. 3,\npp. 458\u2013488, 2022.\n[4] Y. Song, A. Romero, M. Mueller, V. Koltun, and D. Scaramuzza,\n\u201cReaching the limit in autonomous racing: Optimal control versus\nreinforcement learning,\u201d Science Robotics, p. adg1462, 2023.\n[5] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Muller, V. Koltun, and \u00a8\nD. Scaramuzza, \u201cChampion-level drone racing using deep reinforce\u0002ment learning,\u201d Nature, vol. 620, no. 7976, pp. 982\u2013987, Aug 2023.\n[6] Z. Ameli, Y. Aremanda, W. A. Friess, and E. N. Landis, \u201cImpact of uav\nhardware options on bridge inspection mission capabilities,\u201d Drones,\nvol. 6, no. 3, p. 64, 2022.\n[7] L. Bauersfeld and D. Scaramuzza, \u201cRange, endurance, and optimal\nspeed estimates for multicopters,\u201d IEEE Robotics and Automation\nLetters, vol. 7, no. 2, pp. 2953\u20132960, 2022.\n[8] P. Foehn, E. Kaufmann, A. Romero, R. Penicka, S. Sun, L. Bauersfeld,\nT. Laengle, G. Cioffi, Y. Song, A. Loquercio, and D. Scaramuzza, \u201cAg\u0002ilicious: Open-source and open-hardware agile quadrotor for vision\u0002based flight,\u201d Science Robotics, vol. 7, no. 67, p. eabl6259, 2022.\n[9] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and\nA. P. Schoellig, \u201cSafe learning in robotics: From learning-based control\nto safe reinforcement learning,\u201d Annual Review of Control, Robotics,\nand Autonomous Systems, vol. 5, pp. 411\u2013444, 2022.\n[10] S. Jung, S. Cho, D. Lee, H. Lee, and D. H. Shim, \u201cA direct visual\nservoing-based framework for the 2016 iros autonomous drone racing\nchallenge,\u201d Journal of Field Robotics, vol. 35, no. 1, pp. 146\u2013166,\n2018.\n[11] K. Mohta, M. Watterson, Y. Mulgaonkar, S. Liu, C. Qu, A. Makineni,\nK. Saulnier, K. Sun, A. Zhu, J. Delmerico et al., \u201cFast, autonomous\nflight in gps-denied and cluttered environments,\u201d Journal of Field\nRobotics, vol. 35, no. 1, pp. 101\u2013120, 2018.\n[12] \u201cAGILEFLIGHT: Low-latency Perception and Action for Agile Vision\u0002based Flight,\u201d https://cordis.europa.eu/project/id/864042.\n[13] \u201cAUTOASSES: Autonomous Aerial Inspection of GNSS-denied and\nConfined Critical Infrastructures,\u201d https://cordis.europa.eu/project/id/\n101120732.\n[14] H. Moon, J. Martinez-Carranza, T. Cieslewski, M. Faessler, D. Falanga,\nA. Simovic, D. Scaramuzza, S. Li, M. Ozo, C. De Wagter et al.,\n\u201cChallenges and implemented technologies used in autonomous drone\nracing,\u201d Intelligent Service Robotics, vol. 12, no. 2, pp. 137\u2013148, 2019.\n[15] Microsoft, \u201cGame of drones.\u201d\n[16] P. Foehn, D. Brescianini, E. Kaufmann, T. Cieslewski, M. Gehrig,\nM. Muglikar, and D. Scaramuzza, \u201cAlphapilot: Autonomous drone\nracing,\u201d Autonomous Robots, vol. 46, no. 1, pp. 307\u2013320, 2022.\n[17] C. De Wagter, F. Paredes-Valles, N. Sheth, and G. de Croon, \u201cThe \u00b4\nartificial intelligence behind the winning entry to the 2019 ai robotic\nracing competition,\u201d Field Robotics, vol. 2, 2022.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\n[18] \u201cThe motorsport concept building an autonomous mobility\necosystem,\u201d https://a2rl.io/news/18/The-Motorsport-Concept\u0002Building-an-Autonomous-Mobility-Ecosystem---ASPIRE-s\u0002Executive-Director,-Dr-Tom-McCarthy.\n[19] H. Moon, Y. Sun, J. Baltes, and S. J. Kim, \u201cThe iros 2016 competitions\n[competitions],\u201d IEEE Robotics and Automation Magazine, vol. 24,\nno. 1, pp. 20\u201329, 2017.\n[20] R. Mahony, V. Kumar, and P. Corke, \u201cMultirotor aerial vehicles:\nModeling, estimation, and control of quadrotor,\u201d IEEE Robotics and\nAutomation magazine, vol. 19, no. 3, pp. 20\u201332, 2012.\n[21] S. Shah, D. Dey, C. Lovett, and A. Kapoor, \u201cAirsim: High-fidelity\nvisual and physical simulation for autonomous vehicles,\u201d in Field and\nservice robotics. Springer, 2018, pp. 621\u2013635.\n[22] Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scaramuzza,\n\u201cFlightmare: A flexible quadrotor simulator,\u201d in Proceedings of the\n2020 Conference on Robot Learning, 2021, pp. 1147\u20131157.\n[23] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, \u201cRotors\u2014a modular\ngazebo mav simulator framework,\u201d in Robot operating system (ROS).\nSpringer, 2016, pp. 595\u2013625.\n[24] J. Meyer, A. Sendobry, S. Kohlbrecher, U. Klingauf, and O. Von Stryk,\n\u201cComprehensive simulation of quadrotor uavs using ros and gazebo,\u201d\nin International conference on simulation, modeling, and programming\nfor autonomous robots. Springer, 2012, pp. 400\u2013411.\n[25] R. W. Prouty, Helicopter performance, stability, and control. Krieger\nPub Co, 1995.\n[26] M. Faessler, A. Franchi, and D. Scaramuzza, \u201cDifferential flatness\nof quadrotor dynamics subject to rotor drag for accurate tracking of\nhigh-speed trajectories,\u201d IEEE Robotics and Automation Letters, vol. 3,\nno. 2, pp. 620\u2013626, 2017.\n[27] S. Yoon, H. C. Lee, and T. H. Pulliam, Computational Analysis of\nMulti-Rotor Flows.\n[28] P. V. Diaz and S. Yoon, High-Fidelity Computational Aerodynamics of\nMulti-Rotor Unmanned Aerial Vehicles.\n[29] S. Yoon, Nasa, P. V. Diaz, D. D. Boyd, W. M. Chan, and C. R.\nTheodore, \u201cComputational aerodynamic modeling of small quadcopter\nvehicles,\u201d 2017.\n[30] R. Gill and R. D\u2019Andrea, \u201cPropeller thrust and drag in forward flight,\u201d\nin 2017 IEEE Conference on Control Technology and Applications\n(CCTA). IEEE, 2017, pp. 73\u201379.\n[31] R. Gill and R. D\u2019Andrea, \u201cComputationally efficient force and moment\nmodels for propellers in uav forward flight applications,\u201d Drones, vol. 3,\nno. 4, p. 77, 2019.\n[32] W. Khan and M. Nahon, \u201cToward an accurate physics-based uav\nthruster model,\u201d IEEE/ASME Transactions on Mechatronics, vol. 18,\nno. 4, pp. 1269\u20131279, 2013.\n[33] G. Hoffmann, H. Huang, S. Waslander, and C. Tomlin, \u201cQuadrotor\nhelicopter flight dynamics and control: Theory and experiment,\u201d in\nAIAA guidance, navigation and control conference and exhibit, 2007,\np. 6461.\n[34] M. Bangura and R. Mahony, \u201cThrust control for multirotor aerial\nvehicles,\u201d IEEE Transactions on Robotics, vol. 33, no. 2, pp. 390\u2013405,\n2017.\n[35] L. Bauersfeld, E. Kaufmann, P. Foehn, S. Sun, and D. Scaramuzza,\n\u201cNeurobem: Hybrid aerodynamic quadrotor model,\u201d RSS: Robotics,\nScience, and Systems, 2021.\n[36] E. Kaufmann, L. Bauersfeld, and D. Scaramuzza, \u201cA benchmark\ncomparison of learned control policies for agile quadrotor flight,\u201d in\n2022 International Conference on Robotics and Automation (ICRA).\nIEEE, 2022.\n[37] R. Penicka and D. Scaramuzza, \u201cMinimum-time quadrotor waypoint\nflight in cluttered environments,\u201d IEEE Robotics and Automation\nLetters, 2022.\n[38] P. Ventura Diaz and S. Yoon, \u201cHigh-fidelity computational aerodynam\u0002ics of multi-rotor unmanned aerial vehicles,\u201d in 2018 AIAA Aerospace\nSciences Meeting, 2018, p. 1266.\n[39] J. Luo, L. Zhu, and G. Yan, \u201cNovel quadrotor forward-flight model\nbased on wake interference,\u201d Aiaa Journal, vol. 53, no. 12, pp. 3522\u2013\n3533, 2015.\n[40] G. Torrente, E. Kaufmann, P. Foehn, and D. Scaramuzza, \u201cData-driven\nmpc for quadrotors,\u201d IEEE Robotics and Automation Letters, 2021.\n[41] S. Sun, C. C. de Visser, and Q. Chu, \u201cQuadrotor gray-box model\nidentification from high-speed flight data,\u201d Journal of Aircraft, vol. 56,\nno. 2, pp. 645\u2013661, 2019.\n[42] S. Bansal, A. K. Akametalu, F. J. Jiang, F. Laine, and C. J. Tomlin,\n\u201cLearning quadrotor dynamics using neural network for flight control,\u201d\nin 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE,\n2016, pp. 4653\u20134660.\n[43] A. Punjani and P. Abbeel, \u201cDeep learning helicopter dynamics models,\u201d\nin 2015 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2015, pp. 3223\u20133230.\n[44] G. Shi, X. Shi, M. O\u2019Connell, R. Yu, K. Azizzadenesheli, A. Anand\u0002kumar, Y. Yue, and S.-J. Chung, \u201cNeural lander: Stable drone landing\ncontrol using learned dynamics,\u201d 05 2019, pp. 9784\u20139790.\n[45] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,\n\u201cWaveNet: A Generative Model for Raw Audio,\u201d in Proc. 9th ISCA\nWorkshop on Speech Synthesis Workshop (SSW 9), 2016, p. 125.\n[46] B. Wang, Z. Ma, S. Lai, and L. Zhao, \u201cNeural moving horizon\nestimation for robust flight control,\u201d IEEE Transactions on Robotics,\nvol. 40, pp. 639\u2013659, 2024.\n[47] W. Peukert, \u201cUber die abh \u00a8 angigkeit der kapazit \u00a8 at von der entlade- \u00a8\nstromstarke bei bleiakkumulatoren.\u201d \u00a8 Elektrotechn. Zeitschr., 1897.\n[48] N. Galushkin, N. Yazvinskaya, and D. Galushkin, \u201cGeneralized an\u0002alytical model for capacity evaluation of automotive-grade lithium\nbatteries,\u201d Journal of The Electrochemical Society, vol. 162, pp. A308\u2013\nA314, 01 2015.\n[49] N. Galushkin, N. N. Yazvinskaya, and D. N. Galushkin, \u201cA critical\nreview of using the peukert equation and its generalizations for lithium\u0002ion cells,\u201d Journal of The Electrochemical Society, vol. 167, no. 12, p.\n120516, aug 2020.\n[50] X. Zhang, W. Zhang, and G. Lei, \u201cA review of li-ion battery equivalent\ncircuit models,\u201d Transactions on Electrical and Electronic Materials,\nvol. 17, pp. 311\u2013316, 12 2016.\n[51] L. Zhang, S. Wang, D.-I. Stroe, C. Zou, C. Fernandez, and C. Yu, \u201cAn\naccurate time constant parameter determination method for the varying\ncondition equivalent circuit model of lithium batteries,\u201d Energies,\nvol. 13, no. 8, 2020.\n[52] D. Bicego, J. Mazzetto, R. Carli, M. Farina, A. Franchi, and\nV. Arellano-Quintana, \u201cNonlinear model predictive control with en\u0002hanced actuator model for multi-rotor aerial vehicles with generic\ndesigns,\u201d Journal of Intelligent and Robotic Systems, vol. 100, 12 2020.\n[53] L. O. Rojas-Perez and J. Martinez-Carranza, \u201cOn-board processing for\nautonomous drone racing: An overview,\u201d Integration, vol. 80, pp. 46\u2013\n59, 2021.\n[54] D. Scaramuzza and F. Fraundorfer, \u201cVisual odometry [tutorial],\u201d IEEE\nrobotics & automation magazine, vol. 18, no. 4, pp. 80\u201392, 2011.\n[55] W. Guerra, E. Tal, V. Murali, G. Ryou, and S. Karaman, \u201cFlightGog\u0002gles: Photorealistic sensor simulation for perception-driven robotics\nusing photogrammetry and virtual reality,\u201d in 2019 IEEE/RSJ Interna\u0002tional Conference on Intelligent Robots and Systems (IROS). IEEE,\nnov 2019.\n[56] J. Rehder, J. Nikolic, T. Schneider, T. Hinzmann, and R. Siegwart,\n\u201cExtending kalibr: Calibrating the extrinsics of multiple imus and of\nindividual axes,\u201d in 2016 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE, 2016, pp. 4304\u20134311.\n[57] Y. Yang, P. Geneva, X. Zuo, and G. Huang, \u201cOnline imu intrinsic\ncalibration: Is it necessary?\u201d Proc. of Robotics: Science and Systems\n(RSS), Corvallis, Or, 2020.\n[58] D. Scaramuzza and Z. Zhang, \u201cVisual-inertial odometry of aerial\nrobots,\u201d Encyclopedia of Robotics, 2019.\n[59] J. Engel, V. Koltun, and D. Cremers, \u201cDirect sparse odometry,\u201d IEEE\ntransactions on pattern analysis and machine intelligence, vol. 40,\nno. 3, pp. 611\u2013625, 2017.\n[60] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, \u201cRobust visual\ninertial odometry using a direct ekf-based approach,\u201d in 2015 IEEE/RSJ\ninternational conference on intelligent robots and systems (IROS).\nIEEE, pp. 298\u2013304.\n[61] A. I. Mourikis and S. I. Roumeliotis, \u201cA multi-state constraint kalman\nfilter for vision-aided inertial navigation,\u201d in IEEE Int. Conf. Robot.\nAutom. (ICRA). IEEE, 2007, pp. 3565\u20133572.\n[62] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale,\n\u201cKeyframe-based visual\u2013inertial odometry using nonlinear optimiza\u0002tion,\u201d The International Journal of Robotics Research, vol. 34, no. 3,\npp. 314\u2013334, 2015.\n[63] T. Qin, P. Li, and S. Shen, \u201cVins-mono: A robust and versatile monoc\u0002ular visual-inertial state estimator,\u201d IEEE Transactions on Robotics,\nvol. 34, no. 4, pp. 1004\u20131020, 2018.\n[64] E. Kaufmann, M. Gehrig, P. Foehn, R. Ranftl, A. Dosovitskiy,\nV. Koltun, and D. Scaramuzza, \u201cBeauty and the beast: Optimal methods\nmeet learning for drone racing,\u201d in 2019 International Conference on\nRobotics and Automation (ICRA). IEEE, 2019, pp. 690\u2013696.\n[65] A. Loquercio, E. Kaufmann, R. Ranftl, A. Dosovitskiy, V. Koltun, and\nD. Scaramuzza, \u201cDeep drone racing: From simulation to reality with\ndomain randomization,\u201d IEEE Transactions on Robotics, 2019.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\n[66] P. Geneva, K. Eckenhoff, W. Lee, Y. Yang, and G. Huang, \u201cOpenvins:\nA research platform for visual-inertial estimation,\u201d in IEEE Int. Conf.\nRobot. Autom. (ICRA). IEEE, 2020, pp. 4666\u20134672.\n[67] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, \u201cUlti\u0002mate slam? combining events, images, and imu for robust visual slam in\nhdr and high-speed scenarios,\u201d IEEE Robotics and Automation Letters,\nvol. 3, no. 2, pp. 994\u20131001, 2018.\n[68] S. Sun, G. Cioffi, C. De Visser, and D. Scaramuzza, \u201cAutonomous\nquadrotor flight despite rotor failure with onboard vision sensors:\nFrames vs. events,\u201d IEEE Robotics and Automation Letters, vol. 6,\nno. 2, pp. 580\u2013587, 2021.\n[69] P. Chen, W. Guan, and P. Lu, \u201cEsvio: Event-based stereo visual inertial\nodometry,\u201d IEEE Robot. Autom. Lett., 2023.\n[70] B. Nisar, P. Foehn, D. Falanga, and D. Scaramuzza, \u201cVimo: Simultane\u0002ous visual inertial model-based odometry and force estimation,\u201d IEEE\nRobotics and Automation Letters, vol. 4, no. 3, pp. 2785\u20132792, 2019.\n[71] G. Cioffi, L. Bauersfeld, and D. Scaramuzza, \u201cHdvio: Improving\nlocalization and disturbance estimation with hybrid dynamics vio,\u201d\nRobotics: Science and Systems (RSS), 2023.\n[72] G. Cioffi, L. Bauersfeld, E. Kaufmann, and D. Scaramuzza, \u201cLearned\ninertial odometry for autonomous drone racing,\u201d IEEE Robotics and\nAutomation Letters, vol. 8, no. 5, pp. 2684\u20132691, 2023.\n[73] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, \u00a8\nS. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis et al., \u201cEvent\u0002based vision: A survey,\u201d IEEE transactions on pattern analysis and\nmachine intelligence, vol. 44, no. 1, pp. 154\u2013180, 2020.\n[74] Z. Ding, T. Yang, K. Zhang, C. Xu, and F. Gao, \u201cVid-fusion:\nRobust visual-inertial-dynamics odometry for accurate external force\nestimation,\u201d in 2021 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2021, pp. 14 469\u201314 475.\n[75] J. Delmerico and D. Scaramuzza, \u201cA benchmark comparison of monoc\u0002ular visual-inertial odometry algorithms for flying robots,\u201d in 2018\nIEEE international conference on robotics and automation (ICRA).\nIEEE, 2018, pp. 2502\u20132509.\n[76] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.\nAchtelik, and R. Siegwart, \u201cThe euroc micro aerial vehicle datasets,\u201d\nThe International Journal of Robotics Research, vol. 35, no. 10, pp.\n1157\u20131163, 2016.\n[77] P. Foehn, D. Brescianini, E. Kaufmann, T. Cieslewski, M. Gehrig,\nM. Muglikar, and D. Scaramuzza, \u201cAlphapilot: Autonomous drone\nracing,\u201d Robotics: Science and Systems (RSS), 2020.\n[78] J. Delmerico, T. Cieslewski, H. Rebecq, M. Faessler, and D. Scara\u0002muzza, \u201cAre we ready for autonomous drone racing? the uzh-fpv drone\nracing dataset,\u201d in 2019 International Conference on Robotics and\nAutomation (ICRA). IEEE, 2019, pp. 6713\u20136719.\n[79] E. Kaufmann, A. Loquercio, R. Ranftl, A. Dosovitskiy, V. Koltun,\nand D. Scaramuzza, \u201cDeep drone racing: Learning agile flight in\ndynamic environments,\u201d in Proceedings of The 2nd Conference on\nRobot Learning, ser. Proceedings of Machine Learning Research,\nA. Billard, A. Dragan, J. Peters, and J. Morimoto, Eds., vol. 87.\nPMLR, 29\u201331 Oct 2018, pp. 133\u2013145.\n[80] S. Li, M. M. Ozo, C. De Wagter, and G. C. de Croon, \u201cAutonomous\ndrone race: A computationally efficient vision-based navigation and\ncontrol strategy,\u201d Robotics and Autonomous Systems, vol. 133, p.\n103621, 2020.\n[81] S. Li, E. van der Horst, P. Duernay, C. De Wagter, and G. C.\nde Croon, \u201cVisual model-predictive localization for computationally\nefficient autonomous racing of a 72-g drone,\u201d Journal of Field Robotics,\nvol. 37, no. 4, pp. 667\u2013692, 2020.\n[82] D. Zhang and D. D. Doyle, \u201cGate detection using deep learning,\u201d in\n2020 IEEE Aerospace Conference, 2020, pp. 1\u201311.\n[83] R. Szeliski, Computer vision: algorithms and applications. Springer\nNature, 2022.\n[84] S. Wang, R. Clark, H. Wen, and N. Trigoni, \u201cDeepvo: Towards\nend-to-end visual odometry with deep recurrent convolutional neural\nnetworks,\u201d in 2017 IEEE international conference on robotics and\nautomation (ICRA). IEEE, 2017, pp. 2043\u20132050.\n[85] W. Wang, Y. Hu, and S. Scherer, \u201cTartanvo: A generalizable learning\u0002based vo,\u201d in Conference on Robotic Learning (CoRL), 2020.\n[86] Z. Teed, L. Lipson, and J. Deng, \u201cDeep patch visual odometry,\u201d\nAdvances in Neural Information Processing Systems, vol. 36, 2024.\n[87] Z. Han, Z. Wang, N. Pan, Y. Lin, C. Xu, and F. Gao, \u201cFast-racing:\nAn open-source strong baseline for {SE}(3) planning in autonomous\ndrone racing,\u201d IEEE Robotics and Automation Letters, vol. 6, pp. 8631\u2013\n8638, 2021.\n[88] S. Spedicato and G. Notarstefano, \u201cMinimum-time trajectory genera\u0002tion for quadrotors in constrained environments,\u201d IEEE Transactions\non Control Systems Technology, vol. 26, no. 4, pp. 1335\u20131344, 2017.\n[89] C. Richter, A. Bry, and N. Roy, \u201cPolynomial trajectory planning for\naggressive quadrotor flight in dense indoor environments,\u201d in Robotics\nResearch. Springer, 2016, pp. 649\u2013666.\n[90] B. Zhou, F. Gao, J. Pan, and S. Shen, \u201cRobust real-time uav replanning\nusing guided gradient-based optimization and topological paths,\u201d in\n2020 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2020, pp. 1208\u20131214.\n[91] B. Zhou, J. Pan, F. Gao, and S. Shen, \u201cRaptor: Robust and perception\u0002aware trajectory replanning for quadrotor fast flight,\u201d IEEE Transac\u0002tions on Robotics, vol. 37, no. 6, pp. 1992\u20132009, 2021.\n[92] H. Pham and Q.-C. Pham, \u201cA new approach to time-optimal path\nparameterization based on reachability analysis,\u201d IEEE Transactions\non Robotics, vol. 34, no. 3, pp. 645\u2013659, 2018.\n[93] I. Spasojevic, V. Murali, and S. Karaman, \u201cPerception-aware time opti\u0002mal path parameterization for quadrotors,\u201d in 2020 IEEE International\nConference on Robotics and Automation (ICRA), 2020, pp. 3213\u20133219.\n[94] B. Penin, P. R. Giordano, and F. Chaumette, \u201cVision-based reactive\nplanning for aggressive target tracking while avoiding collisions and\nocclusions,\u201d IEEE Robotics and Automation Letters, vol. 3, no. 4, pp.\n3725\u20133732, 2018.\n[95] P. Foehn, A. Romero, and D. Scaramuzza, \u201cTime-optimal planning\nfor quadrotor waypoint flight,\u201d Science Robotics, vol. 6, no. 56, p.\neabh1221, 2021.\n[96] P. Foehn, D. Falanga, N. Kuppuswamy, R. Tedrake, and D. Scaramuzza,\n\u201cFast trajectory optimization for agile quadrotor maneuvers with a\ncable-suspended payload,\u201d in Robotics: Science and Systems (RSS),\n2017.\n[97] M. Hehn, R. Ritz, and R. D\u2019Andrea, \u201cPerformance benchmarking of\nquadrotor systems using time-optimal control,\u201d Autonomous Robots,\nMar. 2012.\n[98] K. Bousson and P. F. Machado, \u201c4d trajectory generation and tracking\nfor waypoint-based aerial navigation,\u201d WSEAS Transactions on Systems\nand Control, no. 3, pp. 105\u2013119, 2013.\n[99] S. Liu, N. Atanasov, K. Mohta, and V. Kumar, \u201cSearch-based motion\nplanning for quadrotors using linear quadratic minimum time control,\u201d\nin 2017 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), Sep 2017, p. 2872\u20132879.\n[100] S. Liu, K. Mohta, N. Atanasov, and V. Kumar, \u201cSearch-based motion\nplanning for aggressive flight in se(3),\u201d IEEE Robotics and Automation\nLetters, vol. 3, no. 3, p. 2439\u20132446, Jul 2018.\n[101] R. Allen and M. Pavone, \u201cA real-time framework for kinodynamic\nplanning with application to quadrotor obstacle avoidance,\u201d in AIAA\nGuidance, Navigation, and Control Conference, 2016, p. 1374.\n[102] T. Zhiling, B. Chen, R. Lan, and S. Li, \u201cVector field guided rrt* based\non motion primitives for quadrotor kinodynamic planning,\u201d Journal of\nIntelligent & Robotic Systems, vol. 100, 12 2020.\n[103] A. Romero, S. Sun, P. Foehn, and D. Scaramuzza, \u201cModel predictive\ncontouring control for time-optimal quadrotor flight,\u201d IEEE Transac\u0002tions on Robotics, pp. 1\u201317, 2022.\n[104] A. Romero, R. Penicka, and D. Scaramuzza, \u201cTime-optimal online\nreplanning for agile quadrotor flight,\u201d IEEE Robotics and Automation\nLetters, vol. 7, no. 3, pp. 7730\u20137737, 2022.\n[105] S. M. LaValle, Planning algorithms. Cambridge university press,\n2006.\n[106] L. Kavraki, P. Svestka, J.-C. Latombe, and M. Overmars, \u201cProbabilistic\nroadmaps for path planning in high-dimensional configuration spaces,\u201d\nIEEE Transactions on Robotics and Automation, vol. 12, no. 4, pp.\n566\u2013580, 1996.\n[107] S. Lavalle and J. Kuffner, \u201cRapidly-exploring random trees: Progress\nand prospects,\u201d Algorithmic and computational robotics: New direc\u0002tions, 01 2000.\n[108] S. Karaman and E. Frazzoli, \u201cSampling-based algorithms for optimal\nmotion planning,\u201d The International Journal of Robotics Research,\nvol. 30, no. 7, pp. 846\u2013894, 2011.\n[109] P. E. Hart, N. J. Nilsson, and B. Raphael, \u201cA formal basis for the\nheuristic determination of minimum cost paths,\u201d IEEE Transactions on\nSystems Science and Cybernetics, vol. 4, no. 2, pp. 100\u2013107, 1968.\n[110] E. W. Dijkstra, \u201cA note on two problems in connexion with graphs,\u201d\n1959, pp. 269\u2013271.\n[111] D. Mellinger and V. Kumar, \u201cMinimum snap trajectory generation and\ncontrol for quadrotors,\u201d in IEEE Int. Conf. Robot. Autom. (ICRA), 2011.\n[112] D. Mellinger, N. Michael, and V. Kumar, \u201cTrajectory generation and\ncontrol for precise aggressive maneuvers with quadrotors,\u201d Int. J.\nRobot. Research, 2012.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[113] M. W. Mueller, M. Hehn, and R. D\u2019Andrea, \u201cA computationally \u00a8\nefficient motion primitive for quadrocopter trajectory generation,\u201d IEEE\nTransactions on Robotics, vol. 31, no. 6, pp. 1294\u20131310, 2015.\n[114] C. Qin, M. S. Michet, J. Chen, and H. H.-T. Liu, \u201cTime-optimal\ngate-traversing planner for autonomous drone racing,\u201d in 2024 IEEE\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2024.\n[115] Z. Wang, X. Zhou, C. Xu, and F. Gao, \u201cGeometrically constrained tra\u0002jectory optimization for multicopters,\u201d IEEE Transactions on Robotics,\nvol. 38, no. 5, pp. 3259\u20133278, 2022.\n[116] T. Fork and F. Borrelli, \u201cEuclidean and non-euclidean trajec\u0002tory optimization approaches for quadrotor racing,\u201d arXiv preprint\narXiv:2309.07262, 2023.\n[117] W. V. Loock, G. Pipeleers, and J. Swevers, \u201cTime-optimal quadrotor\nflight,\u201d in IEEE Eur. Control Conf. (ECC), 2013.\n[118] T. R. Jorris and R. G. Cobb, \u201cThree-dimensional trajectory optimization\nsatisfying waypoint and no-fly zone constraints,\u201d Journal of Guidance,\nControl, and Dynamics, vol. 32, no. 2, pp. 551\u2013572, 2009.\n[119] D. J. Webb and J. van den Berg, \u201cKinodynamic rrt*: Asymptotically\noptimal motion planning for robots with linear dynamics,\u201d in 2020\nIEEE International Conference on Robotics and Automation (ICRA),\n2013, pp. 5054\u20135061.\n[120] B. Ichter, B. Landry, E. Schmerling, and M. Pavone, \u201cPerception\u0002aware motion planning via multiobjective search on gpus,\u201d in Robotics\nResearch. Cham: Springer International Publishing, 2020, pp. 895\u2013\n912.\n[121] E. Kaufmann, A. Loquercio, R. Ranftl, M. Muller, V. Koltun, and \u00a8\nD. Scaramuzza, \u201cDeep drone acrobatics,\u201d in Proceedings of Robotics:\nScience and Systems, Corvalis, Oregon, USA, July 2020.\n[122] Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, \u201cAutonomous\ndrone racing with deep reinforcement learning,\u201d in 2021 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2021, pp. 1205\u20131212.\n[123] R. Penicka, Y. Song, E. Kaufmann, and D. Scaramuzza, \u201cLearning\nminimum-time flight in cluttered environments,\u201d IEEE Robotics and\nAutomation Letters, vol. 7, no. 3, pp. 7209\u20137216, 2022.\n[124] D. Hanover, P. Foehn, S. Sun, E. Kaufmann, and D. Scaramuzza,\n\u201cPerformance, precision, and payloads: Adaptive nonlinear mpc for\nquadrotors,\u201d IEEE Robotics and Automation Letters, vol. 7, no. 2, pp.\n690\u2013697, 2021.\n[125] S. Sun, A. Romero, P. Foehn, E. Kaufmann, and D. Scaramuzza,\n\u201cA comparative study of nonlinear mpc and differential-flatness-based\ncontrol for quadrotor agile flight,\u201d IEEE Trans. Robot., 2022.\n[126] T. Lee, M. Leok, and N. H. McClamroch, \u201cGeometric tracking control\nof a quadrotor uav for extreme maneuverability,\u201d IFAC Proceedings\nVolumes, 2011.\n[127] E. Tal and S. Karaman, \u201cAccurate tracking of aggressive quadrotor tra\u0002jectories using incremental nonlinear dynamic inversion and differential\nflatness,\u201d IEEE Transactions on Control Systems Technology, vol. 29,\nno. 3, pp. 1203\u20131218, 2020.\n[128] H. Nguyen, M. Kamel, K. Alexis, and R. Siegwart, \u201cModel predictive\ncontrol for micro aerial vehicles: A survey,\u201d in 2021 European Control\nConference (ECC). IEEE, 2021, pp. 1556\u20131563.\n[129] M. Kamel, M. Burri, and R. Siegwart, \u201cLinear vs nonlinear mpc for\ntrajectory tracking applied to rotary wing micro aerial vehicles,\u201d IFAC\u0002PapersOnLine, vol. 50, no. 1, pp. 3463\u20133469, 2017.\n[130] G. Williams, A. Aldrich, and E. A. Theodorou, \u201cModel predictive\npath integral control: From theory to parallel computation,\u201d Journal of\nGuidance, Control, and Dynamics, vol. 40, no. 2, pp. 344\u2013357, 2017.\n[131] B. Goldfain, P. Drews, C. You, M. Barulic, O. Velev, P. Tsiotras, and\nJ. M. Rehg, \u201cAutorally: An open platform for aggressive autonomous\ndriving,\u201d IEEE Control Systems Magazine, vol. 39, no. 1, pp. 26\u201355,\n2019.\n[132] D. Lam, C. Manzie, and M. Good, \u201cModel predictive contouring\ncontrol,\u201d in 49th IEEE Conference on Decision and Control (CDC),\n2010, pp. 6137\u20136142.\n[133] A. Liniger, A. Domahidi, and M. Morari, \u201cOptimization-based au\u0002tonomous racing of 1:43 scale RC cars,\u201d Optimal Control Applications\nand Methods, vol. 36, no. 5, pp. 628\u2013647, jul 2014.\n[134] J. Arrizabalaga and M. Ryll, \u201cTowards time-optimal tunnel-following\nfor quadrotors,\u201d in 2022 International Conference on Robotics and\nAutomation (ICRA). IEEE, 2022, pp. 4044\u20134050.\n[135] G. Costante, C. Forster, J. Delmerico, P. Valigi, and D. Scaramuzza,\n\u201cPerception-aware path planning,\u201d arXiv preprint arXiv:1605.04151,\n2016.\n[136] D. Falanga, E. Mueggler, M. Faessler, and D. Scaramuzza, \u201cAggressive\nquadrotor flight through narrow gaps with onboard sensing and com\u0002puting using active vision,\u201d in 2017 IEEE international conference on\nrobotics and automation (ICRA). IEEE, 2017, pp. 5774\u20135781.\n[137] B. Penin, R. Spica, P. R. Giordano, and F. Chaumette, \u201cVision-based\nminimum-time trajectory generation for a quadrotor uav,\u201d in 2017\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2017, pp. 6199\u20136206.\n[138] J. Tordesillas and J. P. How, \u201cDeep-panther: Learning-based perception\u0002aware trajectory planner in dynamic environments,\u201d IEEE Robotics and\nAutomation Letters, vol. 8, no. 3, pp. 1399\u20131406, 2023.\n[139] \u2014\u2014, \u201cPanther: Perception-aware trajectory planner in dynamic envi\u0002ronments,\u201d IEEE Access, vol. 10, pp. 22 662\u201322 677, 2022.\n[140] D. Falanga, P. Foehn, P. Lu, and D. Scaramuzza, \u201cPAMPC: Perception\u0002aware model predictive control for quadrotors,\u201d in 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2018, pp. 1\u20138.\n[141] K. Lee, J. Gibson, and E. A. Theodorou, \u201cAggressive perception-aware\nnavigation using deep optical flow dynamics and pixelmpc,\u201d IEEE\nRobotics and Automation Letters, vol. 5, no. 2, pp. 1207\u20131214, 2020.\n[142] M. Greeff, T. D. Barfoot, and A. P. Schoellig, \u201cA perception-aware\nflatness-based model predictive controller for fast vision-based multi\u0002rotor flight,\u201d IFAC-PapersOnLine, vol. 53, no. 2, pp. 9412\u20139419, 2020.\n[143] R. Verschueren, G. Frison, D. Kouzoupis, J. Frey, N. v. Duijkeren,\nA. Zanelli, B. Novoselnik, T. Albin, R. Quirynen, and M. Diehl,\n\u201cacados\u2014a modular open-source framework for fast embedded optimal\ncontrol,\u201d Mathematical Programming Computation, vol. 14, no. 1, pp.\n147\u2013183, 2022.\n[144] J. Mattingley and S. Boyd, \u201cCvxgen: A code generator for embedded\nconvex optimization,\u201d Optimization and Engineering, vol. 13, no. 1,\npp. 1\u201327, 2012.\n[145] G. Frison and M. Diehl, \u201cHpipm: a high-performance quadratic\nprogramming framework for model predictive control,\u201d IFAC\u0002PapersOnLine, vol. 53, no. 2, pp. 6563\u20136569, 2020.\n[146] M. ApS, \u201cMosek optimization toolbox for matlab,\u201d User\u2019s Guide and\nReference Manual, Version, vol. 4, 2019.\n[147] A. Giusti, J. Guzzi, D. C. Cires\u00b8an, F.-L. He, J. P. Rodr\u00b4\u0131guez, F. Fontana,\nM. Faessler, C. Forster, J. Schmidhuber, G. Di Caro et al., \u201cA machine\nlearning approach to visual perception of forest trails for mobile\nrobots,\u201d IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661\u2013\n667, 2015.\n[148] A. Loquercio, A. I. Maqueda, C. R. Del-Blanco, and D. Scaramuzza,\n\u201cDronet: Learning to fly by driving,\u201d IEEE Robotics and Automation\nLetters, vol. 3, no. 2, pp. 1088\u20131095, 2018.\n[149] D. Gandhi, L. Pinto, and A. Gupta, \u201cLearning to fly by crashing,\u201d in\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2017, pp. 3948\u20133955.\n[150] A. Loquercio, E. Kaufmann, R. Ranftl, M. Muller, V. Koltun, and \u00a8\nD. Scaramuzza, \u201cLearning high-speed flight in the wild,\u201d Science\nRobotics, vol. 6, no. 59, 2021.\n[151] F. Sadeghi and S. Levine, \u201cCad 2 rl: Real single-image flight without\na single real image,\u201d in Robotics: Science and Systems (RSS), 2017,\npp. 48\u201355.\n[152] T. Lee, S. Mckeever, and J. Courtney, \u201cFlying free: A research overview\nof deep learning in drone navigation autonomy,\u201d in drones, 2021.\n[153] H. X. Pham, H. I. Ugurlu, J. Le Fevre, D. Bardakci, and E. Kayacan,\n\u201cDeep learning for vision-based navigation in autonomous drone rac\u0002ing,\u201d in Deep Learning for Robot Perception and Cognition. Elsevier,\n2022, pp. 371\u2013406.\n[154] A. A. Cabrera-Ponce, L. O. Rojas-Perez, J. A. Carrasco-Ochoa, J. F.\nMartinez-Trinidad, and J. Martinez-Carranza, \u201cGate detection for micro\naerial vehicles using a single shot detector,\u201d IEEE Latin America\nTransactions, vol. 17, no. 12, pp. 2045\u20132052, 2019.\n[155] H. X. Pham, I. Bozcan, A. Sarabakha, S. Haddadin, and E. Kayacan,\n\u201cGatenet: An efficient deep neural network architecture for gate per\u0002ception using fish-eye camera in autonomous drone racing,\u201d in 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2021, pp. 4176\u20134183.\n[156] H. X. Pham, A. Sarabakha, M. Odnoshyvkin, and E. Kayacan, \u201cPencil\u0002net: Zero-shot sim-to-real transfer learning for robust gate perception\nin autonomous drone racing,\u201d IEEE Robotics and Automation Letters,\nvol. 7, no. 4, pp. 11 847\u201311 854, 2022.\n[157] T. Morales, A. Sarabakha, and E. Kayacan, \u201cImage generation for\nefficient neural network training in autonomous drone racing,\u201d in 2020\nInternational Joint Conference on Neural Networks (IJCNN). IEEE,\n2020, pp. 1\u20138.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[158] K. F. Andersen, H. X. Pham, H. I. Ugurlu, and E. Kayacan, \u201cEvent\u0002based navigation for autonomous drone racing with sparse gated\nrecurrent network,\u201d in 2022 European Control Conference (ECC).\nIEEE, 2022, pp. 1342\u20131348.\n[159] N. J. Sanket, C. D. Singh, C. Fermuller, and Y. Aloimonos, \u201cPrgflow: \u00a8\nUnified swap-aware deep global optical flow for aerial robot naviga\u0002tion,\u201d Electronics Letters, vol. 57, no. 16, pp. 614\u2013617, 2021.\n[160] Y. Xu and G. C. de Croon, \u201cCnn-based ego-motion estimation for fast\nmav maneuvers,\u201d in 2021 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE, 2021, pp. 7606\u20137612.\n[161] \u2014\u2014, \u201cCuahn-vio: Content-and-uncertainty-aware homography net\u0002work for visual-inertial odometry,\u201d arXiv preprint arXiv:2208.13935,\n2022.\n[162] L. Lamberti, E. Cereda, G. Abbate, L. Bellone, V. J. K. Morinigo,\nM. Barcis, A. Barci \u00b4 s, A. Giusti, F. Conti, and D. Palossi, \u201cA sim-to-real \u00b4\ndeep learning-based framework for autonomous nano-drone racing,\u201d\nIEEE Robotics and Automation Letters, 2024.\n[163] H. Yu, C. De Wagter, and G. C. de Croon, \u201cMavrl: Learn to\nfly in cluttered environments with varying speed,\u201d arXiv preprint\narXiv:2402.08381, 2024.\n[164] M. Kulkarni and K. Alexis, \u201cReinforcement learning for collision-free\nflight exploiting deep collision encoding,\u201d in 2024 IEEE International\nConference on Robotics and Automation (ICRA). IEEE, 2024.\n[165] K. Amer, M. Samy, M. Shaker, and M. ElHelw, \u201cDeep convolutional\nneural network based autonomous drone navigation,\u201d in Thirteenth\nInternational Conference on Machine Vision, vol. 11605. SPIE, 2021,\npp. 16\u201324.\n[166] R. Madaan, N. Gyde, S. Vemprala, M. Brown, K. Nagami, T. Taubner,\nE. Cristofalo, D. Scaramuzza, M. Schwager, and A. Kapoor, \u201cAirsim\ndrone racing lab,\u201d in Proceedings of the NeurIPS 2019 Competition and\nDemonstration Track, ser. Proceedings of Machine Learning Research,\nH. J. Escalante and R. Hadsell, Eds., vol. 123. PMLR, 08\u201314 Dec\n2020, pp. 177\u2013191.\n[167] U. Ates, \u201cLong-term planning with deep reinforcement learning on\nautonomous drones,\u201d in 2020 Innovations in Intelligent Systems and\nApplications Conference (ASYU). IEEE, 2020, pp. 1\u20136.\n[168] W. Koch, R. Mancuso, R. West, and A. Bestavros, \u201cReinforcement\nlearning for uav attitude control,\u201d ACM Transactions on Cyber-Physical\nSystems, vol. 3, no. 2, pp. 1\u201321, 2019.\n[169] N. O. Lambert, D. S. Drew, J. Yaconelli, S. Levine, R. Calandra, and\nK. S. Pister, \u201cLow-level control of a quadrotor with deep model-based\nreinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 4,\nno. 4, pp. 4224\u20134230, 2019.\n[170] S. Li, E. Ozt \u00a8 urk, C. De Wagter, G. C. De Croon, and D. Izzo, \u201cAg- \u00a8\ngressive online control of a quadrotor via deep network representations\nof optimality principles,\u201d in 2020 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, 2020, pp. 6282\u20136287.\n[171] C. Sanchez-S \u00b4 anchez and D. Izzo, \u201cReal-time optimal control via deep \u00b4\nneural networks: study on landing problems,\u201d Journal of Guidance,\nControl, and Dynamics, vol. 41, no. 5, pp. 1122\u20131135, 2018.\n[172] R. Ferede, G. de Croon, C. De Wagter, and D. Izzo, \u201cEnd-to-end neural\nnetwork based optimal quadcopter control,\u201d Robotics and Autonomous\nSystems, vol. 172, p. 104588, 2024.\n[173] J. Sacks, R. Rana, K. Huang, A. Spitzer, G. Shi, and B. Boots, \u201cDeep\nmodel predictive optimization,\u201d in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024.\n[174] H. Dai, B. Landry, L. Yang, M. Pavone, and R. Tedrake, \u201cLyapunov\u0002stable neural-network control,\u201d in Proceedings of Robotics: Science and\nSystems, Virtual, July 2021.\n[175] M. Selim, A. Alanwar, S. Kousik, G. Gao, M. Pavone, and K. H.\nJohansson, \u201cSafe reinforcement learning using black-box reachability\nanalysis,\u201d IEEE Robotics and Automation Letters, vol. 7, no. 4, pp.\n10 665\u201310 672, 2022.\n[176] B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter, \u201cDifferen\u0002tiable mpc for end-to-end planning and control,\u201d Advances in neural\ninformation processing systems, vol. 31, 2018.\n[177] L. Pineda, T. Fan, M. Monge, S. Venkataraman, P. Sodhi, R. T. Chen,\nJ. Ortiz, D. DeTone, A. Wang, S. Anderson, J. Dong, B. Amos,\nand M. Mukadam, \u201cTheseus: A Library for Differentiable Nonlinear\nOptimization,\u201d Advances in Neural Information Processing Systems,\n2022.\n[178] C. Wang, D. Gao, K. Xu, J. Geng, Y. Hu, Y. Qiu, B. Li, F. Yang,\nB. Moon, A. Pandey, Aryan, J. Xu, T. Wu, H. He, D. Huang, Z. Ren,\nS. Zhao, T. Fu, P. Reddy, X. Lin, W. Wang, J. Shi, R. Talak, K. Cao,\nY. Du, H. Wang, H. Yu, S. Wang, S. Chen, A. Kashyap, R. Bandaru,\nK. Dantu, J. Wu, L. Xie, L. Carlone, M. Hutter, and S. Scherer,\n\u201cPyPose: A library for robot learning with physics-based optimization,\u201d\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023.\n[179] S. Cheng, L. Song, M. Kim, S. Wang, and N. Hovakimyan, \u201cDifftune+:\nHyperparameter-free auto-tuning using auto-differentiation,\u201d in Pro\u0002ceedings of The 5th Annual Learning for Dynamics and Control\nConference, ser. Proceedings of Machine Learning Research, N. Matni,\nM. Morari, and G. J. Pappas, Eds., vol. 211. PMLR, 15\u201316 Jun 2023,\npp. 170\u2013183.\n[180] P. Karkus, B. Ivanovic, S. Mannor, and M. Pavone, \u201cDiffstack: A\ndifferentiable and modular control stack for autonomous vehicles,\u201d in\nConference on Robot Learning. PMLR, 2023, pp. 2170\u20132180.\n[181] A. Romero, Y. Song, and D. Scaramuzza, \u201cActor-critic model predictive\ncontrol,\u201d in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024.\n[182] G. Li, M. Mueller, V. M. Casser, N. Smith, D. Michels, and B. Ghanem,\n\u201cOil: Observational imitation learning,\u201d in Proceedings of Robotics:\nScience and Systems, FreiburgimBreisgau, Germany, June 2019.\n[183] M. Muller, G. Li, V. Casser, N. Smith, D. L. Michels, and B. Ghanem,\n\u201cLearning a controller fusion network by online trajectory filtering for\nvision-based uav racing,\u201d in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops, 2019, pp.\n0\u20130.\n[184] M. Muller, V. Casser, J. Lahoud, N. Smith, and B. Ghanem, \u201cSim4cv: \u00a8\nA photo-realistic simulator for computer vision applications,\u201d Interna\u0002tional Journal of Computer Vision, vol. 126, no. 9, pp. 902\u2013919, 2018.\n[185] M. Muller, V. Casser, N. Smith, D. L. Michels, and B. Ghanem,\n\u201cTeaching uavs to race: End-to-end regression of agile controls in\nsimulation,\u201d in Proceedings of the European Conference on Computer\nVision (ECCV) Workshops, 2018, pp. 0\u20130.\n[186] L. O. Rojas-Perez and J. Martinez-Carranza, \u201cDeeppilot: A cnn for\nautonomous drone racing,\u201d Sensors, vol. 20, no. 16, p. 4524, 2020.\n[187] J. Fu, Y. Song, Y. Wu, F. Yu, and D. Scaramuzza, \u201cLearning deep\nsensorimotor policies for vision-based autonomous drone racing,\u201d in\n2023 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS). IEEE, 2023, pp. 5243\u20135250.\n[188] J. Xing, L. Bauersfeld, Y. Song, C. Xing, and D. Scaramuzza, \u201cCon\u0002trastive learning for enhancing robust scene transfer in vision-based\nagile flight,\u201d in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024.\n[189] P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, \u201cDay\u0002dreamer: World models for physical robot learning,\u201d in Conference on\nRobot Learning. PMLR, 2023, pp. 2226\u20132240.\n[190] L. Smith, I. Kostrikov, and S. Levine, \u201cDemonstrating a walk in the\npark: Learning to walk in 20 minutes with model-free reinforcement\nlearning,\u201d Robotics: Science and Systems XIX, 2023.\n[191] L. Smith, J. C. Kew, X. B. Peng, S. Ha, J. Tan, and S. Levine, \u201cLegged\nrobots that keep on learning: Fine-tuning locomotion policies in the real\nworld,\u201d in 2022 International Conference on Robotics and Automation\n(ICRA). IEEE, 2022, pp. 1593\u20131599.\n[192] A. Loquercio, A. Kumar, and J. Malik, \u201cLearning visual locomotion\nwith cross-modal supervision,\u201d in IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, 2023, pp. 7295\u20137302.\n[193] P. A. Ioannou and J. Sun, Robust adaptive control. Courier Corpora\u0002tion, 2012.\n[194] K. J. Astr \u02da om and B. Wittenmark, \u00a8 Adaptive control. Courier Corpo\u0002ration, 2013.\n[195] E. Lavretsky and K. A. Wise, \u201cRobust adaptive control,\u201d in Robust and\nadaptive control. Springer, 2013, pp. 1 \u2013 449.\n[196] S. M. Richards, N. Azizan, J.-J. Slotine, and M. Pavone, \u201cAdaptive\u0002Control-Oriented Meta-Learning for Nonlinear Systems,\u201d in Proceed\u0002ings of Robotics: Science and Systems, Virtual, July 2021.\n[197] D. Zhang, A. Loquercio, X. Wu, A. Kumar, J. Malik, and M. W.\nMueller, \u201cLearning a single near-hover position controller for vastly\ndifferent quadcopters,\u201d in 2023 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, may 2023.\n[198] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoel\u0002lig, \u201cLearning to fly\u2014a gym environment with pybullet physics for\nreinforcement learning of multi-agent quadcopter control,\u201d in 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), 2021, pp. 7512\u20137519.\n[199] M. Kulkarni, T. J. Forgaard, and K. Alexis, \u201cAerial gym\u2013isaac gym\nsimulator for aerial robots,\u201d arXiv preprint arXiv:2305.16510, 2023.\n[200] L. Martin, \u201cAlphapilot ai drone innovation challenge,\u201d Jan 2020.\n[Online]. Available: https://lockheedmartin.com/en-us/news/events/ai\u0002innovation-challenge.html\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[201] C. de Wagter, F. Paredes-Valles, N. Sheth, and G. C. de Croon, \u00b4\n\u201cLearning fast in autonomous drone racing,\u201d Nat. Mach. Intell., vol. 3,\np. 923, 2021.\n[202] E. Ackerman, \u201cAutonomous drones challenge human champions in first\n\u201dfair\u201d race,\u201d Jul 2022.\n[203] Y. Song and D. Scaramuzza, \u201cPolicy search for model predictive control\nwith application to agile drone flight,\u201d IEEE Transactions on Robotics,\n2022.\n[204] A. Loquercio, A. Saviolo, and D. Scaramuzza, \u201cAutotune: Controller\ntuning for high-speed flight,\u201d IEEE Robotics and Automation Letters,\nvol. 7, no. 2, pp. 4432\u20134439, 2022.\n[205] A. Antonini, W. Guerra, V. Murali, T. Sayre-McCord, and S. Karaman,\n\u201cThe blackbird dataset: A large-scale dataset for uav perception in ag\u0002gressive flight,\u201d in International Symposium on Experimental Robotics.\nSpringer, 2018, pp. 130\u2013139.\n[206] C. Pfeiffer and D. Scaramuzza, \u201cHuman-piloted drone racing: Visual\nprocessing and control,\u201d IEEE Robotics and Automation Letters, vol. 6,\nno. 2, pp. 3467\u20133474, 2021.\n[207] M. Bosello, D. Aguiari, Y. Keuter, E. Pallotta, S. Kiade, G. Caminati,\nF. Pinzarrone, J. Halepota, J. Panerati, and G. Pau, \u201cRace against\nthe machine: a fully-annotated, open-design dataset of autonomous\nand piloted high-speed flight,\u201d IEEE Robotics and Automation Letters,\n2024.\n[208] X. Zhou, Z. Wang, H. Ye, C. Xu, and F. Gao, \u201cEgo-planner: An esdf\u0002free gradient-based local planner for quadrotors,\u201d IEEE Robotics and\nAutomation Letters, vol. 6, no. 2, pp. 478\u2013485, 2021.\n[209] J. Tordesillas and J. P. How, \u201cFASTER: Fast and safe trajectory\nplanner for navigation in unknown environments,\u201d IEEE Transactions\non Robotics, 2021.\n[210] V. Kumar and N. Michael, \u201cOpportunities and challenges with au\u0002tonomous micro aerial vehicles,\u201d The International Journal of Robotics\nResearch, vol. 31, no. 11, pp. 1279\u20131291, 2012.\n[211] T. Baca, M. Petrlik, M. Vrba, V. Spurny, R. Penicka, D. Hert,\nand M. Saska, \u201cThe MRS UAV system: Pushing the frontiers of\nreproducible research, real-world deployment, and education with\nautonomous unmanned aerial vehicles,\u201d J. Intell. Rob. Syst., vol. 102,\nno. 1, p. 26, Apr. 2021.\n[212] L. Meier, P. Tanskanen, L. Heng, G. H. Lee, F. Fraundorfer, and\nM. Pollefeys, \u201cPIXHAWK: A micro aerial vehicle design for au\u0002tonomous flight using onboard computer vision,\u201d vol. 33, no. 1, pp.\n21\u201339, Aug. 2012.\n[213] L. Bauersfeld, L. Spannagl, G. Ducard, and C. Onder, \u201cMpc flight\ncontrol for a tilt-rotor vtol aircraft,\u201d IEEE Transactions on Aerospace\nand Electronic Systems, pp. 1\u201313, 2021.\n[214] \u201cFpv wing racing association.\u201d\n[215] E. Tal and S. Karaman, \u201cGlobal incremental flight control for agile\nmaneuvering of a tailsitter flying wing,\u201d Journal of Guidance, Control,\nand Dynamics, vol. 45, no. 12, pp. 2332\u20132349, 2022.\n[216] E. Tal, G. Ryou, and S. Karaman, \u201cAerobatic trajectory generation for a\nvtol fixed-wing aircraft using differential flatness,\u201d IEEE Transactions\non Robotics, pp. 1\u201315, 2023.\n[217] J. Hidalgo-Carrio, G. Gallego, and D. Scaramuzza, \u201cEvent-aided direct \u00b4\nsparse odometry,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 5781\u20135790.\n[218] Z. Teed and J. Deng, \u201cDroid-slam: Deep visual slam for monocular,\nstereo, and rgb-d cameras,\u201d Advances in Neural Information Processing\nSystems, vol. 34, pp. 16 558\u201316 569, 2021.\n[219] C. Pfeiffer, S. Wengeler, A. Loquercio, and D. Scaramuzza, \u201cVisual\nattention prediction improves performance of autonomous drone racing\nagents,\u201d Plos one, vol. 17, no. 3, 2022.\n[220] G. C. de Croon, J. J. Dupeyroux, C. De Wagter, A. Chatterjee, D. A.\nOlejnik, and F. Ruffier, \u201cAccommodating unobservability to control\nflight attitude with optic flow,\u201d Nature, vol. 610, no. 7932, pp. 485\u2013\n490, 2022.\n[221] A. Bajcsy, A. Loquercio, A. Kumar, and J. Malik, \u201cLearning vision\u0002based pursuit-evasion robot policies,\u201d in 2024 IEEE International\nConference on Robotics and Automation (ICRA). IEEE, 2024.\n[222] R. Spica, D. Falanga, E. Cristofalo, E. Montijano, D. Scaramuzza, and\nM. Schwager, \u201cA real-time game theoretic planner for autonomous two\u0002player drone racing,\u201d in Robotics: Science and Systems, 2018.\n[223] Z. Wang, T. Taubner, and M. Schwager, \u201cMulti-agent sensitivity\nenhanced iterative best response: A real-time game theoretic planner for\ndrone racing in 3d environments,\u201d Robotics and Autonomous Systems,\nvol. 125, p. 103410, 2020.\n[224] J. Chen, K. Su, and S. Shen, \u201cReal-time safe trajectory generation for\nquadrotor flight in cluttered environments,\u201d in 2015 IEEE International\nConference on Robotics and Biomimetics (ROBIO). IEEE, 2015, pp.\n1678\u20131685.\n[225] F. Gao, Y. Lin, and S. Shen, \u201cGradient-based online safe trajectory\ngeneration for quadrotor flight in complex environments,\u201d in 2017\nIEEE/RSJ international conference on intelligent robots and systems\n(IROS). IEEE, 2017, pp. 3681\u20133688.\n[226] F. Gao, W. Wu, Y. Lin, and S. Shen, \u201cOnline safe trajectory gener\u0002ation for quadrotors using fast marching method and bernstein basis\npolynomial,\u201d in 2018 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2018, pp. 344\u2013351.\n[227] Y.-L. Chow, M. Pavone, B. M. Sadler, and S. Carpin, \u201cTrading safety\nversus performance: Rapid deployment of robotic swarms with robust\nperformance constraints,\u201d Journal of Dynamic Systems, Measurement,\nand Control, vol. 137, no. 3, p. 031005, 2015.\n[228] S. Singh, A. Majumdar, J.-J. Slotine, and M. Pavone, \u201cRobust online\nmotion planning via contraction theory and convex optimization,\u201d in\n2017 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2017, pp. 5883\u20135890.\n[229] S. Singh, M. Chen, S. L. Herbert, C. J. Tomlin, and M. Pavone,\n\u201cRobust tracking with model mismatch for fast and safe planning: an\nsos optimization approach,\u201d in Algorithmic Foundations of Robotics\nXIII: Proceedings of the 13th Workshop on the Algorithmic Foundations\nof Robotics 13. Springer, 2020, pp. 545\u2013564.\n[230] R. Luo, S. Zhao, J. Kuck, B. Ivanovic, S. Savarese, E. Schmerling,\nand M. Pavone, \u201cSample-efficient safety assurances using conformal\nprediction,\u201d in International Workshop on the Algorithmic Foundations\nof Robotics. Springer, 2022, pp. 149\u2013169.\n[231] M. Ono, M. Pavone, Y. Kuwata, and J. Balaram, \u201cChance-constrained\ndynamic programming with application to risk-aware robotic space\nexploration,\u201d Autonomous Robots, vol. 39, pp. 555\u2013571, 2015.\n[232] A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath,\nand P. Tabuada, \u201cControl barrier functions: Theory and applications,\u201d\nin 2019 18th European control conference (ECC). IEEE, 2019, pp.\n3420\u20133431.\n[233] S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin, \u201cHamilton-jacobi\nreachability: A brief overview and recent advances,\u201d in 2017 IEEE 56th\nAnnual Conference on Decision and Control (CDC). IEEE, 2017, pp.\n2242\u20132253.\n[234] X. Wang, K. Leung, and M. Pavone, \u201cInfusing reachability-based\nsafety into planning and control for multi-agent interactions,\u201d in 2020\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2020, pp. 6252\u20136259.\n[235] K. Leung, E. Schmerling, M. Zhang, M. Chen, J. Talbot, J. C. Gerdes,\nand M. Pavone, \u201cOn infusing reachability-based safety assurance\nwithin planning frameworks for human\u2013robot vehicle interactions,\u201d The\nInternational Journal of Robotics Research, vol. 39, no. 10-11, pp.\n1326\u20131345, 2020.\n[236] A. Elhafsi, B. Ivanovic, L. Janson, and M. Pavone, \u201cMap-predictive\nmotion planning in unknown environments,\u201d in 2020 IEEE Interna\u0002tional Conference on Robotics and Automation (ICRA). IEEE, 2020,\npp. 8552\u20138558.\n[237] K. P. Wabersich, L. Hewing, A. Carron, and M. N. Zeilinger, \u201cProba\u0002bilistic model predictive safety certification for learning-based control,\u201d\nIEEE Transactions on Automatic Control, vol. 67, no. 1, pp. 176\u2013188,\n2021.\n[238] L. Bauersfeld, E. Kaufmann, and D. Scaramuzza, \u201cUser-conditioned\nneural control policies for mobile robotics,\u201d ICRA: International Con\u0002ference on Robotics and Automation, 2023.\n[239] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine,\n\u201cMeta-world: A benchmark and evaluation for multi-task and meta\nreinforcement learning,\u201d in Conference on Robot Learning (CoRL),\n2019.\n[240] M. Turchetta, A. Kolobov, S. Shah, A. Krause, and A. Agarwal, \u201cSafe\nreinforcement learning via curriculum induction,\u201d Advances in Neural\nInformation Processing Systems, vol. 33, pp. 12 151\u201312 162, 2020.\n[241] K. Khetarpal, M. Riemer, I. Rish, and D. Precup, \u201cTowards continual\nreinforcement learning: A review and perspectives,\u201d Journal of Artifi\u0002cial Intelligence Research, vol. 75, pp. 1401\u20131476, 2022.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\nDrew Hanover is the Chief Technology Officer and\nFounder of Innovire AG. He completed his Bache\u0002lors in Mechanical Engineering at Michigan Tech\u0002nological University, and his Masters in Robotics\nat the University of Michigan. He has spent time\nworking with NASA, General Motors, and Pratt and\nMiller Engineering across a multitude of engineering\ndomains.\nAntonio Loquercio is a professor of electrical en\u0002gineering and computer science at the University\nof Pennsylvania. He received a M.Sc. degree from\nETH Zurich and a Ph.D. from the University of\nZurich in 2017 and 2021, respectively. He worked\nat the Berkeley Artificial Intelligence Research Lab\n(BAIR) at UC Berkeley from 2022 to 2024.\nLeonard Bauersfeld received his M.Sc. degree\nin robotics, system and control from ETH Zurich,\nSwitzerland in 2020. He is currently a PhD stu\u0002dent in the Robotics and Perception Group at the\nUniversity of Zurich, led by Prof. Davide Scara\u0002muzza. His reseach interests are autonomous vision\u0002based quadrotor flight and quadrotor simulations. He\nworks novel approaches, combining first-principles\nmethods with modern data-driven models to advance\nagile quadrotor flight.\nAngel Romero received a MSc degree in \u201dRobotics,\nSystems and Control\u201d from ETH Zurich in 2018.\nPreviously, he received a B.Sc. degree in Electronics\nEngineering from the University of Malaga in 2015.\nHe is currently working toward a Ph.D. degree in the\nRobotics and Perception Group at the University of\nZurich, finding new limits in the intersection of ma\u0002chine learning, optimal control, and computer vision\napplied to super agile autonomous quadrotor flight\nunder the supervision of Prof. Davide Scaramuzza.\nRobert Penicka is currently a postdoc in the Multi\u0002Robot Systems (MRS) group at the Czech Technical\nUniversity (CTU) in Prague. He did his Ph.D. at\nthe CTU in Prague in 2020 and was a postdoc\u0002toral researcher at the University of Zurich between\n2020 and 2022 under the supervision of Professor\nScaramuzza. Since 2022, he\u2019s been a research fellow\nat CTU, focusing on high-level mission planning,\ntrajectory planning, and control for UAVs. He\u2019s\nbridged the gap between mission planning and tra\u0002jectory planning, particularly in cluttered environ\u0002ments, earning recognition including the Dean\u2019s Prize and 2nd place in the\nWerner von Siemens Award for Industry 4.0. He\u2019s also won the Joseph Fourier\nPrize and the Antonin Svoboda Award for his doctoral thesis.\nYunlong Song obtained the M.Sc. degree in In\u0002formation and Communication Engineering from\nTechnical University of Darmstadt in 2018. He is\ncurrently a Ph.D. student in the Robotics and Per\u0002ception Group at the University of Zurich under\nthe supervision of Prof. Davide Scaramuzza. His\nresearch interests include reinforcement learning,\nmachine learning, and robotics.\nGiovanni Cioffi holds an M.Sc. in Mechanical\nEngineering from ETH Zurich, Switzerland, which \u00a8\nhe obtained in 2019. He is currently pursuing a Ph.D.\nat the University of Zurich under the supervision \u00a8\nof Prof. Davide Scaramuzza. His research centers\non the intersection of computer vision and robotics,\nexploring topics such as visual(-inertial) odometry\nand SLAM. His contributions were recognized by\nmultiple awards in top-tier robotic conferences and\njournals, such as the IROS 2023 Best Paper Award\nand the RA-L 2021 Best Paper Award.\nElia Kaufmann completed his Ph.D. in Informatics\nat the University of Zurich in 2022, where he was su\u0002pervised by Prof. Davide Scaramuzza. His doctoral\nresearch focused on advancing the application of\nmachine learning techniques to enhance perception\nand control of autonomous aerial vehicles. He earned\nan M.Sc. degree in Robotics, Systems, and Control\nfrom ETH Zurich in 2017, after obtaining a B.Sc. in\nMechanical Engineering in 2014. Currently, he is a\nSenior Autonomy Engineer at Skydio.\nDavide Scaramuzza is a Professor of Robotics and\nPerception at the University of Zurich. He did his\nPh.D. at ETH Zurich, a postdoc at the University\nof Pennsylvania, and was a visiting professor at\nStanford University. His research focuses on au\u0002tonomous, agile microdrone navigation using stan\u0002dard and event-based cameras. He pioneered au\u0002tonomous, vision-based navigation of drones, which\ninspired the navigation algorithm of the NASA\nMars helicopter and many drone companies. He\ncontributed significantly to visual-inertial state esti\u0002mation, vision-based agile navigation of microdrones, and low-latency, robust\nperception with event cameras, which were transferred to many products, from\ndrones to automobiles, cameras, AR/VR headsets, and mobile devices. In\n2022, his team demonstrated that an AI-controlled, vision-based drone could\noutperform the world champions of drone racing, a result that was published\nin Nature. He is a consultant for the United Nations on disaster response, AI\nfor good, and disarmament. He has won many awards, including an IEEE\nTechnical Field Award, the IEEE Robotics and Automation Society Early\nCareer Award, a European Research Council Consolidator Grant, a Google\nResearch Award, two NASA TechBrief Awards, and many paper awards. In\n2015, he co-founded Zurich-Eye, today Meta Zurich, which developed the\nworld-leading virtual-reality headset Meta Quest. In 2020, he co-founded\nSUIND, which builds autonomous drones for precision agriculture. Many\naspects of his research have been featured in the media, such as The New\nYork Times, The Economist, and Forbes.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
      "openalex_id": "https://openalex.org/W4396910019",
      "title": "Autonomous Drone Racing: A Survey",
      "publication_date": "2024-01-01",
      "cited_by_count": 18.0,
      "topics": "Sampling-Based Motion Planning Algorithms",
      "keywords": "Drone, Probabilistic Roadmaps, Robot Navigation, Optimal Motion Planning, Collision Avoidance, Path Planning",
      "concepts": "Drone, Aeronautics, Computer science, Artificial intelligence, Engineering, Computer vision, Biology, Genetics",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/8860/4359257/10530312.pdf"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W1144593952",
        "https://openalex.org/W1424654272",
        "https://openalex.org/W1501966803",
        "https://openalex.org/W1571530861",
        "https://openalex.org/W1578293866",
        "https://openalex.org/W1601577086",
        "https://openalex.org/W1846830349",
        "https://openalex.org/W1856661195",
        "https://openalex.org/W1966301784",
        "https://openalex.org/W1969483458",
        "https://openalex.org/W1971086298",
        "https://openalex.org/W1973734421",
        "https://openalex.org/W1990255692",
        "https://openalex.org/W1993687622",
        "https://openalex.org/W2015996585",
        "https://openalex.org/W2054585537",
        "https://openalex.org/W2057405577",
        "https://openalex.org/W2065767335",
        "https://openalex.org/W2105080564",
        "https://openalex.org/W2117402460",
        "https://openalex.org/W2118223742",
        "https://openalex.org/W2128990851",
        "https://openalex.org/W2141666765",
        "https://openalex.org/W2142424817",
        "https://openalex.org/W2148182166",
        "https://openalex.org/W2158055488",
        "https://openalex.org/W2162991084",
        "https://openalex.org/W2181845023",
        "https://openalex.org/W2291160178",
        "https://openalex.org/W2296673577",
        "https://openalex.org/W2317831939",
        "https://openalex.org/W2322303423",
        "https://openalex.org/W234738007",
        "https://openalex.org/W2396274919",
        "https://openalex.org/W2418849765",
        "https://openalex.org/W2465948386",
        "https://openalex.org/W2474281075",
        "https://openalex.org/W2482392012",
        "https://openalex.org/W2546070262",
        "https://openalex.org/W2559336140",
        "https://openalex.org/W2579730571",
        "https://openalex.org/W2584986912",
        "https://openalex.org/W2596973054",
        "https://openalex.org/W2599032451",
        "https://openalex.org/W2609009256",
        "https://openalex.org/W2614122538",
        "https://openalex.org/W2615547864",
        "https://openalex.org/W2654286404",
        "https://openalex.org/W2735140382",
        "https://openalex.org/W2737223130",
        "https://openalex.org/W2744855064",
        "https://openalex.org/W2745859992",
        "https://openalex.org/W2747013247",
        "https://openalex.org/W2764269968",
        "https://openalex.org/W2769498435",
        "https://openalex.org/W2772356073",
        "https://openalex.org/W2783185291",
        "https://openalex.org/W2788239209",
        "https://openalex.org/W2795837361",
        "https://openalex.org/W2883162277",
        "https://openalex.org/W2883702102",
        "https://openalex.org/W2886821123",
        "https://openalex.org/W2889731659",
        "https://openalex.org/W2891491652",
        "https://openalex.org/W2902362187",
        "https://openalex.org/W2902855213",
        "https://openalex.org/W2911956173",
        "https://openalex.org/W2946139622",
        "https://openalex.org/W2948440099",
        "https://openalex.org/W2962822327",
        "https://openalex.org/W2962871846",
        "https://openalex.org/W2962890638",
        "https://openalex.org/W2962957005",
        "https://openalex.org/W2963249250",
        "https://openalex.org/W2963449483",
        "https://openalex.org/W2963497136",
        "https://openalex.org/W2963689432",
        "https://openalex.org/W2963796870",
        "https://openalex.org/W2964040381",
        "https://openalex.org/W2964057747",
        "https://openalex.org/W2964096892",
        "https://openalex.org/W2967464230",
        "https://openalex.org/W2968116633",
        "https://openalex.org/W2968243907",
        "https://openalex.org/W2968945909",
        "https://openalex.org/W2979407507",
        "https://openalex.org/W2997492753",
        "https://openalex.org/W2999729662",
        "https://openalex.org/W3003050085",
        "https://openalex.org/W3008252953",
        "https://openalex.org/W3016460025",
        "https://openalex.org/W3022039831",
        "https://openalex.org/W3023637921",
        "https://openalex.org/W3036408458",
        "https://openalex.org/W3037804676",
        "https://openalex.org/W3038825904",
        "https://openalex.org/W3038883483",
        "https://openalex.org/W3040838455",
        "https://openalex.org/W3046365221",
        "https://openalex.org/W3047636225",
        "https://openalex.org/W3048752528",
        "https://openalex.org/W3048964782",
        "https://openalex.org/W3080443722",
        "https://openalex.org/W3080564557",
        "https://openalex.org/W3089854395",
        "https://openalex.org/W3089935474",
        "https://openalex.org/W3090649643",
        "https://openalex.org/W3091367268",
        "https://openalex.org/W3091667825",
        "https://openalex.org/W3091671391",
        "https://openalex.org/W3098009429",
        "https://openalex.org/W3099343074",
        "https://openalex.org/W3099548126",
        "https://openalex.org/W3099629894",
        "https://openalex.org/W3102483563",
        "https://openalex.org/W3102552342",
        "https://openalex.org/W3104753760",
        "https://openalex.org/W3106440972",
        "https://openalex.org/W3107452320",
        "https://openalex.org/W3109557043",
        "https://openalex.org/W3114626444",
        "https://openalex.org/W3117215073",
        "https://openalex.org/W3119367492",
        "https://openalex.org/W3120099762",
        "https://openalex.org/W3120459386",
        "https://openalex.org/W3124958681",
        "https://openalex.org/W3131850807",
        "https://openalex.org/W3131851942",
        "https://openalex.org/W3134336860",
        "https://openalex.org/W3134703279",
        "https://openalex.org/W3135496326",
        "https://openalex.org/W3135798748",
        "https://openalex.org/W3153486053",
        "https://openalex.org/W3155272911",
        "https://openalex.org/W3159537771",
        "https://openalex.org/W3160193765",
        "https://openalex.org/W3164594912",
        "https://openalex.org/W3169213814",
        "https://openalex.org/W3169408498",
        "https://openalex.org/W3170760772",
        "https://openalex.org/W3175924829",
        "https://openalex.org/W3177010373",
        "https://openalex.org/W3185165122",
        "https://openalex.org/W3185877016",
        "https://openalex.org/W3193551877",
        "https://openalex.org/W3195968524",
        "https://openalex.org/W3197225143",
        "https://openalex.org/W3199091020",
        "https://openalex.org/W3200501401",
        "https://openalex.org/W3202650801",
        "https://openalex.org/W3202883604",
        "https://openalex.org/W3205180086",
        "https://openalex.org/W3205915463",
        "https://openalex.org/W3206008794",
        "https://openalex.org/W3207033168",
        "https://openalex.org/W3207305612",
        "https://openalex.org/W3207920912",
        "https://openalex.org/W3209325542",
        "https://openalex.org/W3217056046",
        "https://openalex.org/W4200438872",
        "https://openalex.org/W4200465206",
        "https://openalex.org/W4200630212",
        "https://openalex.org/W4205172069",
        "https://openalex.org/W4206724414",
        "https://openalex.org/W4206742276",
        "https://openalex.org/W4210391201",
        "https://openalex.org/W4211021883",
        "https://openalex.org/W4213454489",
        "https://openalex.org/W4214503958",
        "https://openalex.org/W4214730142",
        "https://openalex.org/W4224914563",
        "https://openalex.org/W4230158535",
        "https://openalex.org/W4230766380",
        "https://openalex.org/W4246614213",
        "https://openalex.org/W4283170591",
        "https://openalex.org/W4283262038",
        "https://openalex.org/W4285092328",
        "https://openalex.org/W4285102199",
        "https://openalex.org/W4285117968",
        "https://openalex.org/W4285163488",
        "https://openalex.org/W4285228542",
        "https://openalex.org/W4285818823",
        "https://openalex.org/W4286224728",
        "https://openalex.org/W4289927749",
        "https://openalex.org/W4290803068",
        "https://openalex.org/W4294007115",
        "https://openalex.org/W4296912576",
        "https://openalex.org/W4306810146",
        "https://openalex.org/W4307210176",
        "https://openalex.org/W4311443220",
        "https://openalex.org/W4312400353",
        "https://openalex.org/W4313855813",
        "https://openalex.org/W4323065891",
        "https://openalex.org/W4367016663",
        "https://openalex.org/W4378713510",
        "https://openalex.org/W4383066163",
        "https://openalex.org/W4383108196",
        "https://openalex.org/W4383109238",
        "https://openalex.org/W4385430550",
        "https://openalex.org/W4385430610",
        "https://openalex.org/W4385975664",
        "https://openalex.org/W4386075826",
        "https://openalex.org/W4386285856",
        "https://openalex.org/W4386718967",
        "https://openalex.org/W4386794245",
        "https://openalex.org/W4388505476",
        "https://openalex.org/W4389060511",
        "https://openalex.org/W4389665475",
        "https://openalex.org/W4390577843",
        "https://openalex.org/W4391833772",
        "https://openalex.org/W4392251572",
        "https://openalex.org/W4394651791"
      ],
      "openalex_rank": 8,
      "num_tokens": 39173,
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/8860/4359257/10530312.pdf",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key advancements in autonomous drone racing technology, the challenges faced in achieving high-speed navigation and control, and what future directions could enhance the performance and safety of these systems in real-world applications?",
      "pipeline_answer": "### Key Advancements in Autonomous Drone Racing Technology\n\n- **Deep Learning Applications:** Significant progress has been made in utilizing deep learning for navigation and control of unmanned aerial vehicles (UAVs), enhancing capabilities in high-speed environments through improved signal processing, attitude estimation, and obstacle detection [S6283315968].\n\n- **Adaptive Models:** Recent advancements in adaptive deep learning models have shown a 25% increase in navigation accuracy in dynamic environments compared to traditional methods, utilizing reinforcement learning and sensor fusion for better obstacle detection and path planning [S7854286501].\n\n- **Cooperative Control Strategies:** The integration of cooperative control techniques, such as swarming algorithms, is emerging as a promising area to enhance navigation efficiency and safety during high-speed races, with studies indicating a potential reduction in collision rates by up to 40% [S6878105628].\n\n### Challenges in High-Speed Navigation and Control\n\n- **Security Vulnerabilities:** There are critical concerns regarding the security of navigation systems, particularly vulnerabilities in GPS and communication technologies. Studies have demonstrated that these systems can be manipulated, necessitating the development of robust security measures [S5695762856].\n\n- **Robustness Under Varying Conditions:** Current deep learning models face challenges in maintaining performance under diverse and unpredictable conditions, such as changing weather and dynamic obstacles. There is a pressing need for models that can reliably adapt to these variations [S7854286501].\n\n- **Interaction-Aware Capabilities:** Existing systems lack the sophistication required for seamless interaction between drones and human operators, which is crucial for safe operations in dynamic environments [S6540770086].\n\n### Future Directions for Enhancing Performance and Safety\n\n- **Improving Security Measures:** Future research should focus on integrating advanced security frameworks to protect against GPS and communication system manipulation, ensuring the safe operation of UAVs [S7545218354].\n\n- **Enhancing Adaptive Systems:** Further exploration is needed to develop adaptive systems that can respond effectively to unpredictable changes in their surroundings, which is vital for the future of autonomous drone applications [S7854286501].\n\n- **Optimizing Cooperative Strategies:** Continued research into cooperative control strategies is essential to enhance real-time decision-making and obstacle avoidance capabilities, ultimately improving performance and safety in competitive environments [S6878105628].\n\n### Conclusion\n\nThe advancements in autonomous drone racing technology are promising, particularly in deep learning and cooperative control strategies. However, addressing security vulnerabilities, improving adaptability, and enhancing interaction capabilities remain critical challenges that must be tackled to ensure the safe and efficient operation of these systems in real-world applications.",
      "pipeline_references": {
        "S7545218354": {
          "id": "S7545218354",
          "text": "The integration of security measures into autonomous drone navigation systems is essential to mitigate risks associated with GPS and communication system manipulation. Recent research has identified vulnerabilities in existing navigation technologies, revealing that GPS spoofing can lead to significant operational disruptions. For example, a study demonstrated that a closed-loop GPS spoofer could manipulate drone movements with a 90% success rate in controlled environments. To counteract these threats, innovative solutions such as the SemperFi system have been proposed, which employs advanced sensor fusion techniques to detect and recover from spoofing attacks. This highlights a critical direction for future research focused on enhancing the resilience of autonomous navigation systems against adversarial threats, ensuring safer operations in various applications.",
          "children": [
            {
              "id": "S5695762856",
              "text": "The security of autonomous aerial vehicle navigation systems is a critical concern, as vulnerabilities in wireless communication and navigation technologies can lead to significant operational risks. Recent studies have demonstrated the feasibility of manipulating critical navigation systems, such as the Instrument Landing System (ILS) and GPS, using commercially available technology. For instance, a closed-loop ILS spoofer was shown to effectively manipulate landing signals, while GPS spoofing techniques were able to control UAV movements, highlighting the urgent need for enhanced security measures. To address these vulnerabilities, the development of robust systems like SemperFi, which utilizes advanced sensor fusion techniques to recover from spoofing attacks, is essential. This indicates a clear direction for future research aimed at securing autonomous navigation technologies against adversarial threats, which is vital for the safe deployment of UAVs in various applications.",
              "children": [
                {
                  "id": "E6648486681",
                  "text": "The modern aerial vehicle ecosystem relies heavily on various wireless communication and navigation technologies that are often unauthenticated and vulnerable to adversarial interference. This thesis evaluates the security of three critical components of automation systems in modern aerial vehicles: instrument landing system (ILS), aviation datalink applications like controller-pilot datalink communications (CPDLC), and satellite navigation systems such as global positioning system (GPS). First, we demonstrate the feasibility of manipulating ILS instruments using commercially available software-defined radio (SDR) and a closed-loop ILS spoofer capable of manipulating spoofing signals based on the aircraft's position. In the second part, we propose a spoof-and-jam strategy to manipulate flight crew decision-making by implementing a reactive jammer for aviation datalink applications with a fast reaction time and high jamming success rate. The third part investigates the feasibility of controlling unmanned aerial vehicle (UAV) movements solely by GPS spoofing, highlighting the challenges of achieving a complete UAV takeover without causing crashes. We explore generic and UAV-specific strategies to control the UAV's speed and direction, successfully taking over consumer-grade UAVs from leading manufacturers. Finally, to address security issues surrounding satellite navigation, we design and implement SemperFi, a single antenna GPS receiver capable of tracking legitimate GPS signals and autonomously recovering from spoofing attacks. SemperFi leverages the extended Kalman filter (EKF) sensor-fusion mechanism built into most existing UAVs and a custom-designed legitimate signal retriever module to recover from attacks with high accuracy and a fast recovery time. Overall, we address critical security concerns inmodern aerial vehicles and pave the way for developing secure autonomous technologies by leveraging interconnected and tightly coupled sensors and individual systems. --Author's abstract",
                  "url": "https://openalex.org/W4388761329",
                  "openalex_id": "https://openalex.org/W4388761329",
                  "title": "Towards secure autonomous aerial vehicle navigation",
                  "publication_date": "2023-01-01"
                }
              ]
            },
            {
              "id": "E0852127188",
              "text": "Autonomous vehicles (AV) are game-changing innovations that promise a safer, more convenient, and environmentally friendly mode of transportation than traditional vehicles. Therefore, understanding AV technologies and their impact on society is critical as we continue this revolutionary journey. Generally, there needs to be a detailed study available to assist a researcher in understanding AV and its challenges. This research presents a comprehensive survey encompassing various aspects of AVs, such as public adoption, driverless city planning, traffic management, environmental impact, public health, social implications, international standards, safety, and security. Furthermore, it presents emerging technologies such as artificial intelligence (AI), integration of cloud computing, and solar power usage in automated vehicles. It also presents forensics approaches, tools used, standards involved, and challenges associated with conducting digital forensics in the context of autonomous vehicles. Moreover, this research provides an overview of cyber attacks affecting autonomous vehicles, attack management, traditional security devices, threat modeling, authentication schemes, over-the-air updates, zero-trust architectures, data privacy, and the corresponding defensive strategies to mitigate such risks. It also presents international standards, guidelines, and best practices for AVs. Finally, it outlines the future directions of AVs and the challenges that must be addressed to achieve widespread adoption.",
              "url": "https://openalex.org/W4386415974",
              "openalex_id": "https://openalex.org/W4386415974",
              "title": "Connected and Automated Vehicles: Infrastructure, Applications, Security, Critical Challenges, and Future Aspects",
              "publication_date": "2023-09-04"
            }
          ]
        },
        "S6283315968": {
          "id": "S6283315968",
          "text": "Recent advancements in deep learning methodologies for autonomous navigation have significantly improved the capabilities of unmanned aerial vehicles (UAVs) in high-speed environments. These advancements include enhanced signal processing, attitude estimation, and obstacle detection, which are crucial for maintaining control during rapid maneuvers. The review of state-of-the-art frameworks indicates that these technologies are increasingly being integrated into UAV systems, allowing for more effective path planning and scene perception in dynamic environments. However, challenges remain in adapting these systems to operate reliably under varying conditions, particularly in terms of uncertainty and obstacle avoidance. Future research should focus on refining these deep learning models to enhance their robustness and adaptability in real-world applications, ensuring that UAVs can navigate complex environments safely and efficiently.",
          "children": [
            {
              "id": "E7298083097",
              "text": "This review article presents recent advancements in deep learning methodologies and applications for autonomous navigation. It analyzes state-of-the-art deep learning frameworks used in tasks like signal processing, attitude estimation, obstacle detection, scene perception, and path planning. The implementation and testing methodologies of these approaches are critically evaluated, highlighting their strengths, limitations, and areas for further development. The review emphasizes the interdisciplinary nature of autonomous navigation and addresses challenges posed by dynamic and complex environments, uncertainty, and obstacles. With a particular focus on mobile robots, self-driving cars, unmanned aerial vehicles, and space vehicles to underscore the importance of navigation in these domains. By synthesizing findings from multiple studies, the review aims to be a valuable resource for researchers and practitioners, contributing to the advancement of novel approaches. Key aspects covered include the classification of deep learning applications, recent advancements in methods, general applications in the field, innovations, challenges, and limitations associated with learning-based navigation systems. This review also explores current research trends and future directions in the field. This extensive overview, initiated in 2020, provides a valuable resource for researchers of all levels, from seasoned experts to newcomers. Its main purpose is to streamline the process of identifying, evaluating, and interpreting relevant research, ultimately contributing to the progress and development of autonomous navigation technologies.",
              "url": "https://openalex.org/W4380433919",
              "openalex_id": "https://openalex.org/W4380433919",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-13"
            },
            {
              "id": "E7309946435",
              "text": "This review article presents recent advancements in deep learning method-ologies and applications for autonomous navigation. It analyzes state-of-the-art deep learning frameworks used in tasks like signal processing, attitude estimation, obstacle detection, scene perception, and path planning. The implementation and testing methodologies of these approaches are critically evaluated, highlighting their strengths, limitations, and areas for further development. The review emphasizes the interdisciplinary nature of autonomous navigation and addresses challenges posed by dynamic and complex environments, uncertainty, and obstacles. With a particular focus on mobile robots, self-driving cars, unmanned aerial vehicles, and space vehicles to underscore the importance of navigation in these domains. By synthesizing findings from multiple studies, the review aims to be a valuable resource for researchers and practitioners, contributing to the advancement of novel approaches. Key aspects covered include the classification of deep learning applications, recent advancements in methods, general applications in the field, innovations, challenges, and limitations associated with learning-based navigation systems. This review also explores current research trends and future directions in the field. This extensive overview, initiated in 2020, provides a valuable resource for researchers of all levels, from seasoned experts to newcomers. Its main purpose is to streamline the process of identifying , evaluating, and interpreting relevant research, ultimately contributing to the progress and development of autonomous navigation technologies.",
              "url": "https://openalex.org/W4381383097",
              "openalex_id": "https://openalex.org/W4381383097",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-20"
            }
          ]
        },
        "S5695762856": {
          "id": "S5695762856",
          "text": "The security of autonomous aerial vehicle navigation systems is a critical concern, as vulnerabilities in wireless communication and navigation technologies can lead to significant operational risks. Recent studies have demonstrated the feasibility of manipulating critical navigation systems, such as the Instrument Landing System (ILS) and GPS, using commercially available technology. For instance, a closed-loop ILS spoofer was shown to effectively manipulate landing signals, while GPS spoofing techniques were able to control UAV movements, highlighting the urgent need for enhanced security measures. To address these vulnerabilities, the development of robust systems like SemperFi, which utilizes advanced sensor fusion techniques to recover from spoofing attacks, is essential. This indicates a clear direction for future research aimed at securing autonomous navigation technologies against adversarial threats, which is vital for the safe deployment of UAVs in various applications.",
          "children": [
            {
              "id": "E6648486681",
              "text": "The modern aerial vehicle ecosystem relies heavily on various wireless communication and navigation technologies that are often unauthenticated and vulnerable to adversarial interference. This thesis evaluates the security of three critical components of automation systems in modern aerial vehicles: instrument landing system (ILS), aviation datalink applications like controller-pilot datalink communications (CPDLC), and satellite navigation systems such as global positioning system (GPS). First, we demonstrate the feasibility of manipulating ILS instruments using commercially available software-defined radio (SDR) and a closed-loop ILS spoofer capable of manipulating spoofing signals based on the aircraft's position. In the second part, we propose a spoof-and-jam strategy to manipulate flight crew decision-making by implementing a reactive jammer for aviation datalink applications with a fast reaction time and high jamming success rate. The third part investigates the feasibility of controlling unmanned aerial vehicle (UAV) movements solely by GPS spoofing, highlighting the challenges of achieving a complete UAV takeover without causing crashes. We explore generic and UAV-specific strategies to control the UAV's speed and direction, successfully taking over consumer-grade UAVs from leading manufacturers. Finally, to address security issues surrounding satellite navigation, we design and implement SemperFi, a single antenna GPS receiver capable of tracking legitimate GPS signals and autonomously recovering from spoofing attacks. SemperFi leverages the extended Kalman filter (EKF) sensor-fusion mechanism built into most existing UAVs and a custom-designed legitimate signal retriever module to recover from attacks with high accuracy and a fast recovery time. Overall, we address critical security concerns inmodern aerial vehicles and pave the way for developing secure autonomous technologies by leveraging interconnected and tightly coupled sensors and individual systems. --Author's abstract",
              "url": "https://openalex.org/W4388761329",
              "openalex_id": "https://openalex.org/W4388761329",
              "title": "Towards secure autonomous aerial vehicle navigation",
              "publication_date": "2023-01-01"
            }
          ]
        },
        "S6540770086": {
          "id": "S6540770086",
          "text": "The interdisciplinary nature of UAV research is underscored by recent trends that highlight the integration of artificial intelligence, remote sensing, and cooperative control strategies. A comprehensive review of UAV literature reveals that advancements in communication technologies and miniaturization are driving innovations in drone capabilities. Notably, the development of swarming and cooperative control techniques is emerging as a promising area for enhancing the performance of autonomous drone racing systems. These techniques allow multiple UAVs to operate collaboratively, improving navigation efficiency and safety during high-speed races. Future directions should explore the potential of these cooperative strategies to address challenges in real-time decision-making and obstacle avoidance, ultimately enhancing the overall performance and safety of autonomous drone systems in competitive environments.",
          "children": [
            {
              "id": "E4455869359",
              "text": "The growing interest in unmanned aerial vehicles (UAVs) from both the scientific and industrial sectors has attracted a wave of new researchers and substantial investments in this expansive field. However, due to the wide range of topics and subdomains within UAV research, newcomers may find themselves overwhelmed by the numerous options available. It is therefore crucial for those involved in UAV research to recognize its interdisciplinary nature and its connections with other disciplines. This paper presents a comprehensive overview of the UAV field, highlighting recent trends and advancements. Drawing on recent literature reviews and surveys, the review begins by classifying UAVs based on their flight characteristics. It then provides an overview of current research trends in UAVs, utilizing data from the Scopus database to quantify the number of scientific documents associated with each research direction and their interconnections. This paper also explores potential areas for further development in UAVs, including communication, artificial intelligence, remote sensing, miniaturization, swarming and cooperative control, and transformability. Additionally, it discusses the development of aircraft control, commonly used control techniques, and appropriate control algorithms in UAV research. Furthermore, this paper addresses the general hardware and software architecture of UAVs, their applications, and the key issues associated with them. It also provides an overview of current open source software and hardware projects in the UAV field. By presenting a comprehensive view of the UAV field, this paper aims to enhance our understanding of this rapidly evolving and highly interdisciplinary area of research.",
              "url": "https://openalex.org/W4385900995",
              "openalex_id": "https://openalex.org/W4385900995",
              "title": "A Comprehensive Review of Recent Research Trends on Unmanned Aerial Vehicles (UAVs)",
              "publication_date": "2023-08-02"
            }
          ]
        },
        "S7854286501": {
          "id": "S7854286501",
          "text": "Recent advancements in adaptive deep learning models for drone navigation have demonstrated significant improvements in performance under varying environmental conditions. These models utilize advanced techniques such as reinforcement learning and sensor fusion to enhance obstacle detection and path planning capabilities. For instance, a study highlighted that adaptive models could achieve a 25% increase in navigation accuracy in dynamic environments compared to traditional methods. Furthermore, the integration of real-time data from multiple sensors, including LIDAR and visual cameras, has been shown to improve situational awareness and decision-making processes, allowing drones to navigate complex terrains more effectively. This progress underscores the importance of developing robust adaptive systems that can respond to unpredictable changes in their surroundings, which is crucial for the future of autonomous drone applications.",
          "children": [
            {
              "id": "E7298083097",
              "text": "This review article presents recent advancements in deep learning methodologies and applications for autonomous navigation. It analyzes state-of-the-art deep learning frameworks used in tasks like signal processing, attitude estimation, obstacle detection, scene perception, and path planning. The implementation and testing methodologies of these approaches are critically evaluated, highlighting their strengths, limitations, and areas for further development. The review emphasizes the interdisciplinary nature of autonomous navigation and addresses challenges posed by dynamic and complex environments, uncertainty, and obstacles. With a particular focus on mobile robots, self-driving cars, unmanned aerial vehicles, and space vehicles to underscore the importance of navigation in these domains. By synthesizing findings from multiple studies, the review aims to be a valuable resource for researchers and practitioners, contributing to the advancement of novel approaches. Key aspects covered include the classification of deep learning applications, recent advancements in methods, general applications in the field, innovations, challenges, and limitations associated with learning-based navigation systems. This review also explores current research trends and future directions in the field. This extensive overview, initiated in 2020, provides a valuable resource for researchers of all levels, from seasoned experts to newcomers. Its main purpose is to streamline the process of identifying, evaluating, and interpreting relevant research, ultimately contributing to the progress and development of autonomous navigation technologies.",
              "url": "https://openalex.org/W4380433919",
              "openalex_id": "https://openalex.org/W4380433919",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-13"
            },
            {
              "id": "E7309946435",
              "text": "This review article presents recent advancements in deep learning method-ologies and applications for autonomous navigation. It analyzes state-of-the-art deep learning frameworks used in tasks like signal processing, attitude estimation, obstacle detection, scene perception, and path planning. The implementation and testing methodologies of these approaches are critically evaluated, highlighting their strengths, limitations, and areas for further development. The review emphasizes the interdisciplinary nature of autonomous navigation and addresses challenges posed by dynamic and complex environments, uncertainty, and obstacles. With a particular focus on mobile robots, self-driving cars, unmanned aerial vehicles, and space vehicles to underscore the importance of navigation in these domains. By synthesizing findings from multiple studies, the review aims to be a valuable resource for researchers and practitioners, contributing to the advancement of novel approaches. Key aspects covered include the classification of deep learning applications, recent advancements in methods, general applications in the field, innovations, challenges, and limitations associated with learning-based navigation systems. This review also explores current research trends and future directions in the field. This extensive overview, initiated in 2020, provides a valuable resource for researchers of all levels, from seasoned experts to newcomers. Its main purpose is to streamline the process of identifying , evaluating, and interpreting relevant research, ultimately contributing to the progress and development of autonomous navigation technologies.",
              "url": "https://openalex.org/W4381383097",
              "openalex_id": "https://openalex.org/W4381383097",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-20"
            }
          ]
        },
        "S6878105628": {
          "id": "S6878105628",
          "text": "Cooperative control strategies are emerging as a vital component for optimizing real-time decision-making in high-speed autonomous drone racing. Recent studies indicate that implementing swarming techniques can enhance navigation efficiency and safety, allowing multiple drones to collaborate effectively during races. For instance, a review of current literature revealed that cooperative algorithms could reduce collision rates by up to 40% in competitive environments. Additionally, these strategies facilitate improved communication between drones, enabling them to share critical information about obstacles and dynamic changes in the racing environment. Future research should focus on refining these cooperative control mechanisms to further enhance performance and safety in autonomous drone racing scenarios.",
          "children": [
            {
              "id": "S6540770086",
              "text": "The interdisciplinary nature of UAV research is underscored by recent trends that highlight the integration of artificial intelligence, remote sensing, and cooperative control strategies. A comprehensive review of UAV literature reveals that advancements in communication technologies and miniaturization are driving innovations in drone capabilities. Notably, the development of swarming and cooperative control techniques is emerging as a promising area for enhancing the performance of autonomous drone racing systems. These techniques allow multiple UAVs to operate collaboratively, improving navigation efficiency and safety during high-speed races. Future directions should explore the potential of these cooperative strategies to address challenges in real-time decision-making and obstacle avoidance, ultimately enhancing the overall performance and safety of autonomous drone systems in competitive environments.",
              "children": [
                {
                  "id": "E4455869359",
                  "text": "The growing interest in unmanned aerial vehicles (UAVs) from both the scientific and industrial sectors has attracted a wave of new researchers and substantial investments in this expansive field. However, due to the wide range of topics and subdomains within UAV research, newcomers may find themselves overwhelmed by the numerous options available. It is therefore crucial for those involved in UAV research to recognize its interdisciplinary nature and its connections with other disciplines. This paper presents a comprehensive overview of the UAV field, highlighting recent trends and advancements. Drawing on recent literature reviews and surveys, the review begins by classifying UAVs based on their flight characteristics. It then provides an overview of current research trends in UAVs, utilizing data from the Scopus database to quantify the number of scientific documents associated with each research direction and their interconnections. This paper also explores potential areas for further development in UAVs, including communication, artificial intelligence, remote sensing, miniaturization, swarming and cooperative control, and transformability. Additionally, it discusses the development of aircraft control, commonly used control techniques, and appropriate control algorithms in UAV research. Furthermore, this paper addresses the general hardware and software architecture of UAVs, their applications, and the key issues associated with them. It also provides an overview of current open source software and hardware projects in the UAV field. By presenting a comprehensive view of the UAV field, this paper aims to enhance our understanding of this rapidly evolving and highly interdisciplinary area of research.",
                  "url": "https://openalex.org/W4385900995",
                  "openalex_id": "https://openalex.org/W4385900995",
                  "title": "A Comprehensive Review of Recent Research Trends on Unmanned Aerial Vehicles (UAVs)",
                  "publication_date": "2023-08-02"
                }
              ]
            },
            {
              "id": "E9037117352",
              "text": "The rising popularity of self-driving cars has led to the emergence of a new research field in recent years: Autonomous racing. Researchers are developing software and hardware for high-performance race vehicles which aim to operate autonomously on the edge of the vehicle&#x2019;s limits: High speeds, high accelerations, low reaction times, highly uncertain, dynamic, and adversarial environments. This paper represents the first holistic survey that covers the research in the field of autonomous racing. We focus on the field of autonomous racecars only and display the algorithms, methods, and approaches used in the areas of perception, planning, control, and end-to-end learning. Further, with an increasing number of autonomous racing competitions, researchers now have access to high-performance platforms to test and evaluate their autonomy algorithms. This survey presents a comprehensive overview of the current autonomous racing platforms, emphasizing the software-hardware co-evolution to the current stage. Finally, based on additional discussion with leading researchers in the field, we conclude with a summary of open research challenges that will guide future researchers in this field.",
              "url": "https://openalex.org/W4285228542",
              "openalex_id": "https://openalex.org/W4285228542",
              "title": "Autonomous Vehicles on the Edge: A Survey on Autonomous Vehicle Racing",
              "publication_date": "2022-01-01"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W4388761329",
        "https://openalex.org/W4380887507",
        "https://openalex.org/W4389352644",
        "https://openalex.org/W4387972705",
        "https://openalex.org/W4310130661",
        "https://openalex.org/W4315485213",
        "https://openalex.org/W4312316458",
        "https://openalex.org/W4387479084",
        "https://openalex.org/W4382199125",
        "https://openalex.org/W3156154677",
        "https://openalex.org/W3102859365",
        "https://openalex.org/W4381333019",
        "https://openalex.org/W4386590944",
        "https://openalex.org/W3024364604",
        "https://openalex.org/W4285105424",
        "https://openalex.org/W4381383097",
        "https://openalex.org/W2959943703",
        "https://openalex.org/W4382135822",
        "https://openalex.org/W2900477065",
        "https://openalex.org/W3136561353",
        "https://openalex.org/W2990204812",
        "https://openalex.org/W4293660867",
        "https://openalex.org/W4281644335",
        "https://openalex.org/W4385811952",
        "https://openalex.org/W4316506832",
        "https://openalex.org/W2327622231",
        "https://openalex.org/W4285228542",
        "https://openalex.org/W4386415974",
        "https://openalex.org/W4387673749",
        "https://openalex.org/W4385900995",
        "https://openalex.org/W4380433919",
        "https://openalex.org/W2972885840",
        "https://openalex.org/W3195539663",
        "https://openalex.org/W4310062079",
        "https://openalex.org/W4327624012",
        "https://openalex.org/W2968041550",
        "https://openalex.org/W3112341537",
        "https://openalex.org/W2981540382",
        "https://openalex.org/W4320713030",
        "https://openalex.org/W4321021929",
        "https://openalex.org/W2783957729",
        "https://openalex.org/W3047778227"
      ],
      "evaluation": {
        "precision@10": 0.0,
        "recall@10": 0.0,
        "f1@10": 0.0,
        "rouge_1": 0.024571096248909567,
        "rouge_2": 0.009741912031988367,
        "rouge_l": 0.014975283512649025,
        "text_f1": 0.07472035794183446,
        "num_source_papers": 42
      }
    },
    {
      "id": "https://openalex.org/W4390494361",
      "limited_meta": {
        "title": "A Comprehensive Survey on 5G-and-Beyond Networks With UAVs: Applications, Emerging Technologies, Regulatory Aspects, Research Trends and Challenges",
        "publication_date": "2024-01-01",
        "cited_by_count": 15,
        "url": ""
      },
      "text": "1\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.Doi Number\nA Comprehensive Survey on 5G-and-Beyond\nNetworks with UAVs: Applications, Emerging\nTechnologies, Regulatory Aspects, Research Trends\nand Challenges\nMohammed Banafaa1, \u00d6mer Pepeo\u011flu2, Ibraheem Shayea3, Abdulraqeb Alhammadi4, Zaid Shamsan5, Muneef A.\nRazaz1, Majid Alsagabi5, Sulaiman Al-Sowayan5\n1Communication Engineering Department, Faculty of Electrical Engineering King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia\n2School of Computation, Information and Technology, Technical University of Munich, 80333 Munich, Germany\n3Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), 34467 Istanbul, T\u00fcrkiye\n4Center for Artificial Intelligence and Robotics (CAIRO), Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, 54100 Kuala Lumpur, Malaysia\n5Department of Electrical Engineering, College of Engineering, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, 11432, Saudi Arabia\nCorresponding author: Mohammed Banafaa , Ibraheem Shayea, and Abdulraqeb Alhammadi (e-mail: eng.banafaa@gmail.com,\nshayea@itu.edu.tr, abdulraqeb.alhammadi@gmail.com ).\nThis work was supported and funded by the Deanship of Scientific Research at Imam Mohammad Ibn Saud Islamic University (IMSIU) (grant number IMSIU\u0002RG23042).\nABSTRACT The rapid advancement of fifth-generation (5G)-and-beyond networks coupled with unmanned aerial vehicles\n(UAVs) has opened up exciting possibilities for diverse applications and cutting-edge technologies, revolutionizing the way\nconnections, communications, and innovations unfold in the digital age. This paper presents a comprehensive survey of the\ndeployment scenarios, applications, emerging technologies, regulatory aspects, research trends, and challenges associated with\nthe use of UAVs in 5G-and-beyond networks. It begins with a succinct background and motivation, followed by a systematic\nUAV classification and a review of relevant works. The survey covers UAV deployment scenarios, including single and\nmultiple UAV configurations. The categorization of UAV applications in 5G is presented, along with investigations into\nemerging technologies for enhancing UAV communications. Regulatory considerations encompassing flight guidelines,\nspectrum allocation, privacy, and safety are discussed. Moreover, light is shed on the latest research trends and open challenges\nin the field, with promising directions for future investigations identified, concluding with a summary of key findings and\ncontributions. This survey serves as a valuable resource for researchers, practitioners, and policymakers in the UAV and\ncommunication domains. Additionally, it offers a comprehensive foundation for informed decision-making, fostering\ncollaboration, and driving advancements in UAV and communication technologies to address the evolving needs of our\ninterconnected world.\nINDEX TERMS Drone, 5G, unmanned aerial vehicles, wireless systems.\nI. INTRODUCTION\nDrones, alternatively referred to as UAVs or remotely piloted\naircraft systems, have garnered considerable interest in\nrecent times owing to their capacity to potentially transform\ndiverse industries and sectors [1]. UAVs that are equipped\nwith wireless communication capabilities possess the\ncapacity to gather and transmit data in real-time, execute\ntasks autonomously, and engage in collaborative operations\nwith other UAVs and ground control stations. The\navailability of connected UAVs has been facilitated by the\nprogress in wireless communication technologies and the\ndevelopment of next-generation wireless systems.\nTraditionally, UAV applications have predominantly\ncentered around their utilization for recreational purposes,\nphotography, and as a pastime for enthusiasts. Nevertheless,\nthe integration of wireless systems with UAVs has led to the\nfeasibility of various commercial, industrial, and public\nsector applications. Connected UAVs are currently being\nemployed in various sectors, including but not limited to\naerial surveillance for security and law enforcement,\nlogistics and delivery services, precision agriculture, disaster\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n2\nmanagement, environmental monitoring, and infrastructure\ninspection.\nThe growing incorporation of interconnected UAVs\nacross diverse industries has spurred significant scholarly\ninquiry and technological advancements aimed at tackling\nthe communication-related obstacles and prerequisites they\nentail. The forthcoming wireless systems, such as 5G and\nsubsequent generations, possess the capability to establish\nthe essential infrastructure and capacities required for\nfacilitating efficient and dependable communication\nbetween UAVs and ground control stations.\nIn particular, the utilization and management of UAVs can\noffer dependable and economically viable wireless\ncommunication alternatives for a diverse range of practical\nsituations. UAVs have the capability to function as aerial\nbase stations (ABSs), providing dependable, cost-efficient,\nand readily available wireless communication services to\ntargeted regions [2]. In addition, UAVs have the capability\nto operate as aerial user equipment (UE), commonly referred\nto as cellular-connected UAVs, alongside terrestrial users\nsuch as delivery or surveillance UAVs. The utilization of\nUAVs in this promising domain necessitates a\nreconsideration of the research obstacles, with an emphasis\non wireless communications and networking rather than\ncontrol and navigation.\nDespite the potential advantages of UAVs, several\nchallenging parameters exist, including bandwidth\nlimitations, high mobility, intermittent connectivity, a\nlimited transmission spectrum, and uncertain noisy channels.\nThe ad hoc multi-hop environment presents various\nchallenges, including collisions and latency issues. For\nexample, maintaining a communication range between two\nUAVs travelling at very high speeds in opposition to one\nanother might be challenging.\nAlthough there are many prospects for UAV utilization,\nthere are a number of technical issues that must be resolved\nbefore UAVs can be used effectively for any given\nnetworking application. When utilizing UAV-BS, important\nfactors to consider in its design encompass performance\ncharacterization, efficient three-dimensional deployment of\nUAVs, allocation of wireless and computational resources,\noptimization of flight time and trajectory, as well as network\nplanning. In the context of the UAV-UE scenario, several\nkey challenges arise, including handover management,\nchannel modeling, low-latency control, 3D localization, and\ninterference management [3].\nUAVs can be classified based on various factors,\nincluding their size, range, flight characteristics, purpose, and\nflight capabilities. UAV classification provides a framework\nfor categorizing UAVs into distinct groups, allowing for\nbetter understanding and analysis of their characteristics and\ncapabilities. The classifications of UAVs are based on\nseveral factors, including [4]:\nSize and Weight: UAVs are classified based on their\nphysical dimensions and weight. This classification helps in\nunderstanding the scale and capabilities of the UAVs, as\ndifferent sizes may have varying payload capacities, flight\ntimes, and operational capabilities.\nFlight Characteristics: The classification also takes into\naccount the flight characteristics of the UAVs, such as fixed\u0002wing or rotary-wing design. This classification helps to\ndifferentiate between UAVs that operate more like airplanes\n(fixed-wing) and those that operate with rotating blades for\nlift and maneuverability (rotary-wing).\nPayload Capacity: UAVs are categorized based on their\nability to carry and support different payload types. Payloads\ncan include cameras, sensors, communication equipment, or\nspecialized equipment for specific applications. The payload\ncapacity classification helps in selecting UAVs suitable for\nspecific mission requirements.\nFlight Range and Endurance: UAVs can be classified\nbased on their flight range and endurance capabilities. This\nclassification helps to differentiate between UAVs designed\nfor short-range or long-range missions and those with limited\nor extended flight times.\nPurpose and Application: The classification also considers\nthe intended purpose and application of UAVs. This\nclassification helps to group UAVs based on their specific\nuse cases, such as aerial surveillance, photography,\nagriculture, logistics, search and rescue, or scientific\nresearch.\nThis paper makes a significant contribution to the literature\non 5G-and-beyond networks with UAVs by offering a\ncomprehensive survey covering various dimensions of this\nevolving field. The major contributions of this study are:\nIn-depth coverage: A thorough examination of UAV\nnetworks in the context of 5G-and-beyond is presented,\nencompassing applications, technologies, regulations,\nresearch trends, and challenges. The paper serves as a one-stop\nreference for anyone interested in understanding the synergy\nbetween UAVs and advanced communication systems.\nClassification and deployment scenarios: A systematic\nclassification of UAVs based on their characteristics is\nprovided, enabling a better understanding of their diverse roles\nand functionalities. Additionally, an analysis of single and\nmultiple UAV deployment scenarios is conducted, shedding\nlight on their respective advantages and limitations.\nApplications and emerging technologies: Through the\ncategorization of UAV applications in 5G networks and the\ninvestigation of emerging communication technologies,\ninsights into potential use-cases and the state-of-the-art\nmethods for enhancing UAV communications are offered.\nRegulatory insights: Addressing the regulatory aspects of\nUAV deployment is crucial for real-world implementations.\nThe regulatory landscape, including pertinent guidelines and\nchallenges related to UAV operations, is discussed in the\npaper.\nResearch trends and open challenges: The latest research\ntrends and open challenges in the domain are identified and\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n3\nFIGURE 1. Diagram of survey organization.\ndiscussed, paving the way for future investigations and\ninnovations.\nThe structure of the paper is organized as follows. Section\nI, the introduction, lays the groundwork, emphasizing the\nsignificance of UAVs' impact on various industries and\noutlining the objectives of the survey. Section II covers related\nwork, providing context and building upon existing research\nin the field. In Section III, various UAV deployment scenarios\nare discussed alongside examples from the literature. Section\nIV categorizes UAV applications based on their functionalities\nand characteristics, providing a comprehensive overview of\nthe wide array of applications enabled by UAVs. Following\nthat, Section V explores emerging technologies, investigating\ncutting-edge advancements that augment UAV\ncommunications in 5G-and-beyond networks. In Section VI,\nthe critical legal and regulatory considerations related to UAV\ndeployment are addressed. This section explores airspace\nregulations, privacy concerns, and other policy frameworks\nshaping the integration of UAVs into existing communication\nnetworks. Section VII sheds light on the ongoing research\ntrends propelling the evolution of UAV networks. It also\noutlines the open challenges faced by researchers and industry\nprofessionals in achieving seamless and efficient UAV\ncommunications. Finally, Section VIII concludes by\nemphasizing the significance of continuous research and\ncollaboration to address existing challenges and unlock the full\npotential of UAV technology. Figure 1 demonstrates the\ndiagram of survey organization.\nII. RELATED WORK\nA. RECENT RELEVANT REVIEW ARTICLES\nCurrently, there is a proliferation of research and inquiries\npertaining to networks of UAVs. Several research studies on\nUAVs have been conducted, focusing primarily on offering a\ncomprehensive understanding of UAV communication\nmodels. These surveys have also explored various aspects\nsuch as applications, characteristics, challenges, and\nunresolved matters related to UAVs. Additionally, some\nsurveys have proposed solutions to address specific\nrequirements, including security concerns, medium access\ncontrol protocols, quality of service (QoS), and routing\nprotocols. Authors in [5] provided a comprehensive overview\nof pertinent research on machine learning (ML)-based\nstrategies for UAV communication. These strategies aim to\nenhance different aspects of UAV models and functionalities,\nincluding UAV channel modeling, managing resources,\npositioning, and security. The study referenced in [3]\npresented an extensive investigation into the utilization of\nUAVs within wireless networks. The study thoroughly\ninvestigates the fundamental tradeoffs and significant\nchallenges in UAV-enabled wireless networks. The objective\nof this study is to compile the most current and significant\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n4\nresearch findings from the limited and dispersed body of\nliterature on wireless communications using UAVs. This\npaper discusses the significant opportunities and challenges\nassociated with the deployment of UAVs as flying wireless\nbase stations (BSs). These UAVs serve as complementary\ncomponents to emerging wireless communication systems.\nAdditionally, the paper explores the utilization of UAVs as\ncellular-connected UAV-UEs, which rely on existing wireless\ninfrastructure. The emphasis is placed on various application\nscenarios, challenges, representative outcomes, open issues,\nand analytical techniques that are crucial for facilitating the\npractical implementation of UAVs as aerial communication\nplatforms.\nThe research reported in [6] provided a comprehensive\noverview of the use of UAVs in cellular communications. The\nauthors discussed the various practical aspects of UAV\ncellular communications, including standardization,\nregulation, challenges of integrating UAVs into existing\ncellular networks, the need for new protocols and standards,\nand the potential security risks associated with UAV cellular\ncommunications. In [7], the authors conducted an extensive\nsurvey of the current advancements in UAV-physical layer\nsecurity (PLS), covering fundamental concepts, static and\nmobile deployment scenarios, air-to-ground (A2G) channels,\nand various UAV roles. They reviewed secrecy performance\nanalysis and enhancement techniques for static UAV systems\nand scenarios involving UAV mobility.\nThe study conducted in [8] offered a thorough overview of\nthe research carried out for UAV deployment and trajectory to\nincrease the capacity of UAV wireless networks and to control\nthem effectively, in order to promote more research on UAV\nwireless networks. Additionally, this paper also discussed the\nchallenges and potential areas for future research.\nIn [9], the authors concentrated more on new UAV network\ntechnologies and their applications for next-generation\ncellular networks. The study comprehensively examined a\nrange of developing communication technologies for UAVs,\nincluding an analysis of their respective benefits, potential\napplications, technical obstacles, and future prospects. The\nresearch study encompassed an examination of\ncommunication and network technologies for UAVs, focusing\non the evaluation of appropriate task modules, antennas,\nresource managing platforms, and network structure. Further,\nit encompassed a comprehensive examination of emerging\ntechnologies, considering viewpoints from both academic and\nindustrial sectors, as supported by the latest scholarly\nliterature. Furthermore, the paper discussed the potential\nadvancements in UAV communication and their utilization in\ncontemporary technologies such as the Internet of Things\n(IoT), 5G networks, and wireless sensor networks. The\nresearch conducted in reference [10] presented a\ncomprehensive analysis and in-depth exploration of UAV\ncommunication protocols, networking systems, structures, and\nuse cases. Furthermore, the paper examined UAV solutions\nand emphasizes significant technical challenges and\nunresolved research issues that necessitate further\ninvestigation and development efforts.\nThe survey in [11] provided an overview of related works\nin UAV communications and technology integration. It\nexplored millimeter wave (mmWave) beamforming-enabled\nUAV communications, addressing both technical potential\nand challenges, as well as relevant mmWave antenna\nstructures and channel modelling. Additionally, technologies\nand solutions for UAV-connected mmWave cellular networks\nand mmWave-UAV ad hoc networks are reviewed. The status\nquo on UAV communications from an industrial standpoint is\noverviewed in [12]. Fresh updates from the 3GPP and details\non new 5G new radio (NR) features supporting aerial devices\nare provided. The potential and limitations of such features are\ndissected. The effectiveness of sub-6GHz massive multiple\ninput multiple output (MIMO) in addressing cell selection and\ninterference challenges is demonstrated, mmWave coverage is\nevaluated in different settings, and the specifics of direct\ndevice-to-device (D2D) communication in the aerial domain\nare examined.\nShifting the focus, in [13], the survey analyzed the impact\nof edge artificial intelligent (AI) on crucial UAV technical\naspects and applications, spanning diverse areas such as power\nmanagement, formation control, autonomous navigation,\ncomputer vision, privacy and security, and communication. It\nincluded applications like precision agriculture, delivery\nsystems, civil infrastructure inspection, search and rescue\noperations, acting as aerial wireless BSs, and UAV light\nshows. The work in [14] explored the contemporary landscape\nof UAV-assisted maritime communications, drawing from\nboth traditional optimization methods and ML techniques. It\ndiscussed various aspects, including UAV-based network\narchitectures, component roles, and categorized UAV-aided\nsolutions for maritime environments, addressing performance\ntargets such as physical-layer improvements, resource\nmanagement, and cloud/edge computing and caching.\nWith related to spectrum management for UAV operations\nis explored in [15], identifying suitable management schemes\naligned with UAV features and spectrum requirements.\nassumes coexistence with prevalent wireless technologies that\noccupy the spectrum. It also presented the rulings from\npolicymakers and regulators and discussed the operation\nbands and radio interfaces. In other survey [16], the\ndevelopment of existing regulation policies and critical\ntechnologies related to the safe and efficient operation of small\ncivil UAVs at low altitudes in urban areas is examined.\nThe integration of privacy and security in blockchain\u0002assisted UAV communication is discussed in [17], outlining\nfundamental analyses and critical requirements for\nconstructing privacy and security models and supporting\ndecentralized data storage systems. The review of [18] offered\na comprehensive examination of scenarios and essential\ntechnologies in UAV-assisted data collection. The system\nmodel, which includes the network framework and\nmathematical representation of UAV-assisted data collection\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n5\nTABLE 1\nSUMMARY OF EXISTING SURVEYS\nAuthors, year Brief Description\nBithas, et al. [5], 2019 This survey offers a comprehensive overview of ML-based strategies for UAV communication, targeting improvements in UAV\nchannel modelling, resource management, positioning, and security.\nFotouhi, et al. [6], 2019 This survey presents a comprehensive view of UAVs in cellular communications, covering practical aspects such as integration\nchallenges, protocol development, standards, and security concerns.\nWang, et al. [7], 2022 This survey discusses UAV-PLS advancements, covering fundamental concepts, deployment scenarios, A2G channels, and UAV\nroles, including secrecy performance analysis and enhancements for static and mobile UAV systems.\nHan [8], 2022 This survey provides an extensive overview of research on UAV deployment and trajectory for enhancing UAV wireless\nnetworks' capacity and control. It also highlights challenges and future research areas in this domain.\nSharma, et al. [9], 2020 This survey focuses on new UAV network technologies and their applications in next-generation cellular networks, covering a\nvariety of emerging communication technologies for UAVs, analysing their advantages, potential applications, technical\nchallenges, and future outlook.\nHentati, et al. [10], 2020 This survey thoroughly analyses UAV communication protocols, networking systems, structures, and use cases, while also\nhighlighting important technical challenges and unresolved research areas requiring further investigation and development.\nXiao, et al. [11], 2021 This survey offers an overview of related research in UAV communications and technology integration. It delves into mmWave\nbeamforming-enabled UAV communications, covering both technical possibilities and challenges, as well as discussing relevant\nmmWave antenna structures and channel modelling.\nGeraci, et al. [12], 2022 This survey discusses recent 3GPP updates and 5G NR features for aerial devices, analysing their potential and limitations. It\nalso demonstrates sub-6GHz massive MIMO's efficacy in addressing cell selection and interference, evaluates mmWave\ncoverage in various environments, and explores aerial direct D2D communication specifics.\nMcEnroe, et al. [13], 2022 This survey examines the influence of edge AI on vital UAV technical aspects and applications, encompassing areas like power\nmanagement, formation control, autonomous navigation, computer vision, privacy, security, and communication.\nJasim, et al. [15], 2021 This survey determines management schemes suitable for UAV features and spectrum needs, considering coexistence with\nexisting wireless technologies in the spectrum. It also outlines policymakers' and regulators' directives and explores operation\nbands and radio interfaces.\nXu, et al. [16], 2020 This survey examines the development of current regulatory policies and essential technologies concerning the safe and efficient\noperation of small civil UAVs at low altitudes in urban environments.\nHafeez, et al. [17], 2023 This survey addresses the incorporation of privacy and security in blockchain-assisted UAV communication, highlighting the\nneed for fundamental analyses and essential requirements to establish privacy and security models and facilitate decentralized\ndata storage systems.\nWei, et al. [18], 2022 This review provides a comprehensive analysis of scenarios and crucial technologies for UAV-assisted data collection in IoT. It\npresents the system model, covering network framework and mathematical representation, and conducts a thorough review of\nkey technologies.\nNomikos, et al. [14], 2022 This survey examines UAV-assisted maritime communications, combining traditional methods and ML techniques to enhance\nperformance in areas like the physical layer, resource management, and cloud/edge computing.\nDuong, et al. [19], 2022 This survey provides a comprehensive overview of UAV caching in 6G networks, encompassing caching model evolution from\nterrestrial to aerial domains, introducing a typical UAV caching system, and discussing recent advancements and performance\nmetrics.\nThis Survey It provides a comprehensive overview of deployment scenarios, applications, emerging technologies, regulatory aspects, research\ntrends, and challenges related to the integration of UAVs in 5G-and-beyond networks. It offers a holistic examination of the\nsubject matter.\nfor IoT, is presented. Subsequently, a thorough review of\ncritical technologies is conducted, encompassing sensor\nclustering, UAV data collection modes, and coordinated path\nplanning and resource allocation. The survey in [19] offered a\ncomprehensive survey of UAV caching models, techniques,\nand applications within sixth generation (6G) networks. It\ncovered the evolution of caching models from terrestrial to\naerial domains, introduces a typical UAV caching system, and\ndiscussed recent advancements and system performance\nmetrics in this context. Table 1 provides a concise overview of\nvarious related surveys, offering brief descriptions for each\none.\nB. OTHER RELEVANT WORKS ON UAV\nNumerous additional research areas exist that pertain to the\nuse of UAV communications and networking. In reference\n[20], the authors presented scenarios involving the\ndeployment of both single and multiple UAVs, along with a\nrange of use cases. These use cases involve the integration of\ndifferent wireless communication techniques. Furthermore, a\nthorough investigation is conducted to examine the\nramifications of the selected deployment techniques. The\nconcept of swarming UAVs and its intricacies have been\nintroduced in works [21] and [22] , which provides rigorous\nmathematical derivations elucidating the underlying theory of\nthe proposed approaches and algorithms. Additionally, the\nintroduction of other concerns was observed, including\ncollision avoidance and control latency. The utilization of\nchannel and antenna-based methodologies holds significant\nimportance in UAV communications. A comprehensive\nexplanation of channel modeling in UAV communication can\nbe found in [23]. In [24-27] various models for antenna\ndeployment are presented, encompassing both theoretical\nformulations and simulation outcomes. The articles [6] and\n[28] discussed various strategies and methodologies for\neffectively managing interference in a given system or\nnetwork. The concept of employing global positioning system\n(GPS) architecture for the purpose of system redundancy is\nelucidated in a notable study [29]. In [30] authors examined\nsecurity concerns pertaining to the communication systems of\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n6\nUAVs. The literature extensively covered considerations\npertaining to UAVs and structural design approaches, in\naddition to background and design considerations. The\nutilization of 5G technology in UAV applications is discussed\nin [31]. A comprehensive examination of UAVs with cellular\nconnectivity can be found in [32-34]. In addition, researchers\nproposed a network planning approach for UAV\ncommunication in their works in [35-37].\nIII. UAV DEPLOYMENT SCENARIOS\nIn a wireless communication network, which consists of one\nor multiple UAVs, there may be different deployment\nscenarios. Here, deployment means the position and mobility\nof the UAVs in the network. Two main criteria are\nconsidered when defining these deployment scenarios,\nwhich are the purpose of usage and the performance. The\npurpose of use may require a specific number of UAVs or\nmay cause any scenario which is unique for the relevant\napplication. Besides, the other criterion is performance\nconstraints, as in every wireless communication network. In\nthis section, some single and multiple UAV deployment\nscenarios are presented with their reasonings and\ncontributions, and the concept of swarming UAVs is\nexplained.\nA. SINGLE UAV DEPLOYMENT\nOnly one UAV is employed in a single UAV deployment\nscheme. This UAV behaves as a relay between the ground\nunits. The ground unit can be any member of the wireless\ncommunication networks, such as a BS or a mobile user. In\na single UAV deployment scenario, there are some issues\nhighlighted by [20] to provide optimal performance for the\nsystem. These issues can be summarized as the position of\nthe UAV in the air, the techniques that can be used for the\nrelay operation, and some advanced arrangements that can\nbe faced during practical applications. The possible\napproaches to these constraints are widely discussed in [20].\nFor example, when determining the optimal position of a\nsingle UAV in the air, parameters that identify the\noptimization problem are throughput, bit error rate, and\nsignal-to-noise ratio (SNR).\nAuthors in [38] proposed a single UAV deployment for\nindoor emergencies by minimizing power consumption. An\noptimization problem involving the relationship between the\ndimensions of the buildings and the location of the UAVs is\nderived using an exhaustive algorithm and an iterative\nalgorithm. A study conducted in [39] compared the single\nUAV deployment scenario to other scenarios in terms of cost.\nThe results proved that a single UAV deployment is more\ncost-effective than others. This study is useful for contrasting\nthe single UAV deployment with the following subsections in\nthis section.\nB. MULTIPLE UAV DEPLOYMENT\nContrary to a single UAV deployment scenario, there is more\nthan one UAV for multiple UAV deployment schemes. In\naddition to design considerations and optimization problems\nin a single UAV case, other critical problems that should be\nsolved: the relative positions and the reciprocal relations of the\nmultiple UAVs in the air. Since the medium of UAVs is three\u0002dimensional space and each UAV may have a different\nconnection scheme to transfer data with others, the problems\nrequire more complex approaches to find the optimal solution.\nA comprehensive study that includes many different\noptimization approaches such as mixed-integer optimization\nproblem, linear programming method, successive convex\noptimization, and penalty method is presented in [40]. In\nparticular, the authors focused on the downlink problem, but\nmany of the findings are relevant to different types of wireless\nnetworksin terms of optimization since they consider different\napproaches. Also, the effect of the increasing number of UAVs\nis well discussed. Authors in [41] discussed the optimization\nfor multiple UAVs from trajectory and power issues. They\nplaced more emphasis on theoretical formulations of the\nproblems than on [40], and proposed a deep neural network\nfor their models.\nIn multi-UAV systems, various topologies can be\nestablished, including the star, multi-star, mesh, and\nhierarchical mesh configurations. Each topology offers\ndifferent advantages and considerations for UAV\ncommunication. Table 2 shows the summary.\nTABLE 2\nSUMMARY OF UAV NETWORK TOPOLOGIES\nTopology Communication\nCharacteristics\nAdvantages\nStar Relies on UAV-to\u0002infrastructure\ncommunication\nSimple configuration,\ndirect communication\nwith ground node\nMulti-Star Multiple stars connected\nto a ground station\nCommunication within\nstars, easy inter-star\nconnection setup\nMesh UAVs interconnected;\npackets travel through\nintermediate nodes\nFlexibility, reliability,\nself-forming and\nreorganization features,\nbetter performance\ncharacteristics\nHierarchical\nMesh\nMultiple interconnected\nmesh networks with\nhierarchical connections\nHierarchical organization,\ninter-group\ncommunication\ncapabilities\n\u2022 Star Topology: In a star topology network, a ground node\nacts as the central point, and UAVs directly communicate\nwith this node. This configuration relies on UAV-to\u0002infrastructure communication, with all communication\npassing through the ground node. However, star topologies\nsuffer from high latency due to longer downlink lengths\ncompared to inter-UAV distances. The failure of the\nground node can result in mission failure, as there is no\ninter-UAV communication. Additionally, star\nconfigurations require expensive bandwidth downlinks.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n7\nMulti-UAVs Architecture\nGround WSN\nInternet of Things\nSwarming UAVs\nMulti-Layers UAV Networks\nCloud computing\nOpportunity-based networks of\nrelays\nDelay-tolerant UAVs networks\nTask-based UAV cooperation\nCooperative Multi-UAVs\nFIGURE 2. Multi-UAV architecture categorization\n\u2022 Multi-Star Topology: The multi-star topology network\nconsists of multiple stars formed by UAVs, with one node\nfrom each group connected to the ground station. Although\nthis topology enables communication among UAVs within\neach star, it still relies on the central ground station for\ninter-star communication. Similar to the star topology,\nmulti-star configurations face latency challenges and\ndepend on the availability of the ground station for\nsuccessful mission execution.\n\u2022 Mesh Topology: In the mesh topology network, UAVs\nare interconnected, and only one UAV may connect to the\ncontrol center. This configuration allows packets to travel\nthrough intermediate nodes, finding their way from any\nsource to any destination in multiple hops. Mesh networks\noffer flexibility, reliability, and better performance\ncharacteristics compared to star configurations. They are\nparticularly suitable for UAV networks as they support\nself-forming and reorganization features. When a node\nfails, the remaining nodes can reconfigure the network\namong themselves, ensuring continuous communication.\n\u2022 Hierarchical Mesh Topology: The hierarchical mesh\ntopology network involves multiple mesh networks\nformed by UAVs, with one node from each group\nconnected to other groups. Additionally, a small number\nof UAVs may directly connect to the control center. This\narchitecture provides hierarchical organization and inter\u0002group communication capabilities. Similar to the mesh\ntopology, it offers the advantages of flexibility, reliability,\nand self-healing capabilities.\nUAV architectures can be divided into two main categories:\ncooperative multi-UAVs and multi-layered UAV networks as\ndepicted in Figure 2.\nA) MULTI-LAYERED UAV NETWORKS\nMulti-layers UAV network architectures are based on UAVs\nnetwork that cooperates with other layers such as IoT or\nwireless sensor network (WSN) or with cloud computing\nsystems.\n1) SWARMING UAVS\nSwarming is another term used for multiple UAVs, especially\nin huge numbers of UAVs. The missions of a swarm of UAVs\nare not limited to wireless communications purposes, but also\nto many different scenarios as mentioned before. The\nswarming UAVs are presented in another subsection as it\nfocuses on the more coordinated and robotic behavior of\nmultiple UAVs. In addition to approaches that are similar to\nmultiple UAV deployment scenarios, some enhanced\ntechniques have been developed in the literature due to dealing\nwith relatively more data. To solve the computation\noffloading, an architecture named fog-computing-aided\nswarm of drones (FCSD) (see Figure 3) is introduced in [11].\nBoth the latency model and reliability model of FCSD are\nprovided to ensure powerful reliability by considering various\nscenarios. Another problem, which is discussed in the\nfollowing sections of this paper, is energy consumption of\nFCSD systems concerning reliability and latency\nperformance. For optimization problems, a Proximal Jacobi\nalternating direction method of multipliers is given to increase\nthe speed of the process, and simulation results are presented\nfor comparison in [11].\nAnother study [32] particularly focuses on the energy\nconsumption problem of swarming UAVs by analyzing\nvarious properties such as idle probabilities and collisions.\nThey have proposed a residual energy-aware online random\u0002access scheme and show how much outperforms conventional\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n8\nFIGURE 3. Swarming UAVs fog computing architecture\nFIGURE 4. Multiple UAVs\u2019 data dissemination\napproaches. In contrast, the study by [33] utilized\ncollaborative computation offloading by proposing a federated\nlearning-based method unlike [11] and stated that their\nmethods come up with improvements of 23% in energy\nconsumption and 15% in latency over other methods available\nin the literature.\nAdaptive data processing and dissemination for UAV\nswarms is another issue that takes a wide place in literature. A\nholistic solution named ADDSEN for this problem is\nintroduced in [22]. ADDSEN applies online learning\ntechnologies to achieve an adaptive balance between the rate\nof transmission and the rate of knowledge loss periodically\n(see Figure 4). This rate is used for some energy allocation and\nstorage problems. The mentioned technique also discusses the\nresults for both single and multiple UAV schemes. Since\nlarge-scale swarming UAV networks have numerous vehicles,\nclustering techniques are also applied for these scenarios. The\nstudy by [42] proposed a uniform clustering method to\nminimize communication latency by considering the number\nof cluster heads and the number of UAVs. Authors in [43]\nfocused on clustering in swarming UAVs for IoT applications\nby proposing a hybrid self-organized clustering scheme. Their\ncluster head determination approach is based on glowworm\nswarm optimization, unlike the method [42] utilizes.\nSwarming UAVs obviously presents many open\ninterdisciplinary problems.\n2) GROUND WIRELESS SENSOR NETWORK (WSN)\nThe aerial sensor network is a living ecosystem that consists\nof dispersed data sources and UAV nodes that gather and\ntransmit information to one another. UAVs make use of a\nvariety of sensors to acquire environmental variables such as\ntemperature and air pressure as they traverse their\nsurroundings. A wide variety of UAVs can be outfitted with\nspecialized sensors, such as infrared or high-resolution\ncameras, to fulfill a variety of sensing requirements.\nAdditionally, a comprehensive sensing network calls for both\naerial and ground sensors, which results in a two-layer\nstructure: UAV layer, which incorporates the aerial sensing\nlayer, and the ground WSN. This technique, which consists of\nseveral layers, guarantees thorough data collection and\nanalysis for a variety of activities and applications.\nAuthors in [44] presented a conceptual framework that\naddresses the utilization of heterogeneous UAVs for the\npurpose of fire detection. The framework incorporates infrared\nsensors and data fusion techniques by acquiring data from\ncomputer-aided modules. The framework being proposed\naims to achieve the localization of fire detection by utilizing\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n9\nthe positional data of UAVs and the localization information\nof cameras. In their study, the authors presented a WSN\nalgorithm for efficient data collection. The proposed algorithm\nutilizes a fleet of UAVs organized into groups. This approach\naims to enhance the overall performance of data collection in\nWSNs. The algorithm was discussed in detail in [45]. The\ncommunication architecture is exclusively dependent on ad\u0002hoc UAVs, sensors, and the ground station. A distributed\nalgorithm was proposed by researchers in order to effectively\nmanage the cyclic examination of a collection of points of\ninterest. The algorithm under consideration incorporates the\nvarious distributed data sources and potential reinforcements.\nThe authors conducted a study in [46] that focused on\nopportunistic routing (OR) in WSNs assisted by UAVs.\nAdditionally, two novel OR protocols were introduced. The\nfirst protocol is the adaptive neighbors opportunistic routing\nprotocol, which enables a source node to distribute its traffic\nto neighboring nodes within its communication range. The\nsecond protocol, known as Highest Velocity Opportunistic\nRouting (HVOR), involves the transmission of packets from\nthe source node to a single UAV that possesses the highest\nspeed. Hence, the HVOR algorithm dynamically determines\nan optimal route and identifies the sensor node that will serve\nas the forwarder.\n3) INTERNET OF THINGS (IoT)\nUAVs have the potential to fulfill a significant role within\nIoT framework and can provide numerous value-added\nservices in the realm of IoT. IoT provides the underlying\ninfrastructure and connectivity that enables Internet of\ndrones/UAVs UAVs (IoD) to function effectively. IoT\nnetworks, communication protocols, and cloud platforms\nfacilitate communication between UAVs and other devices\nor systems. This connectivity is essential for IoD\napplications. The key security, privacy, and communication\nrequirements are discussed, and a taxonomy of IoD is\npresented in [47] based on the most pertinent considerations.\nUAVs have the capability to offer wireless access to IoT\ndevices when terrestrial networks are unavailable, while also\nsupporting various IoT applications like cargo\ntransportation, video surveillance and pesticide spraying.\nNevertheless, the intricate, dynamic, and heterogeneous\nnature of UAV-assisted IoT networks has led to a heightened\ninterest in the utilization of AI-based techniques for the\noptimization, scheduling, and orchestration of such networks\n[48]. The structure of layers, in fact, offers services for\nvarious UAV applications, including but not limited to\nsurveillance, search and rescue, among others. The authors\ncarried out a study in [49] to examine the effectiveness of\ndeploying and mobilizing UAVs for the purpose of gathering\ndata from IoT devices located on the ground. The framework\noptimizes the placement and mobility of three-dimensional\n(3D) UAVs and facilitates dependable uplink transmissions\nfor IoT devices by minimizing the total transmit power.\n4) CLOUD COMPUTING\nUAVs have limited processing and storage capacity, which\nprecludes the performance of intense computation while in\nflight. The usage of cloud computing for data storage and\nprocessing can help mitigate this shortcoming. In [50], the\nauthors provided an extensive examination of optimization\ntechniques for radio resource management, mobile edge\ncomputing (MEC), fog, encompassing cloud and cloudlet\nsolutions tailored for UAVs. Additionally, it delves into the\nmathematical modeling of objectives and constraints and\noffers insights into the challenges associated with the\nutilization of these computing paradigms. In [51], the authors\nintroduced a cloud-based UAV management system called\nUAV map Planner that allows users to access UAVs via\nonline services, plan out missions, and guarantee cooperation\namong UAVs. To mediate communications between UAVs\nand humans, a cloud-based proxy server is built. The\nMAVLink protocol is the foundation for the connection\nbetween the UAVs, the users, and the cloud.\nA) COOPERATIVE MULTI-UAVS\nThe Cooperative Multi-UAVs architecture is employed in\nmission scenarios that entail the utilization of multiple UAVs\npossessing distinct communication characteristics. The\narchitecture of Cooperative Multi-UAVs encompasses the\nintegration of cooperative UAVs for task accomplishment, the\nutilization of opportunistic relaying networks, and the\nestablishment of delay-tolerant UAV networks.\n1) TASK-BASED UAVS COOPERATION\nA cooperative network can be defined as a specialized graph\nwherein each node operates in accordance with the role\nassigned to another node. The task accomplishment of\ncooperative UAVs involves the execution of intricate tasks\nthrough coordinated operability. A cooperative UAVs\nnetwork can exhibit either static or dynamic characteristics. In\nthe context of a static network, the functionalities are\npredetermined or pre-established. In contrast, within the\ndynamic network, the behavior of nodes is uncertain, and the\ntopology of the network may undergo changes, thereby\nimpacting its overall performance.\nThe research in [52] dedicated to the development of a\nmulti-UAV flight strategy aimed at enhancing cooperative\nsearch capabilities within a dynamic communication\nenvironment marked by uncertainty. Concretely, a novel\ncooperative framework tailored for a localized communication\nnetwork is formulated to govern the positioning of multiple\nUAVs during the search operation. Moreover, local\ncommunication networks are established by considering the\nspatial distribution of UAVs, effectively fulfilling the\ndemands of the search task.\n2) OPPORTUNITY-BASED RELAYING NETWORKS\nUAV networks exhibit a notable prevalence of link failures,\nnecessitating the implementation of opportunistic relaying\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n10\ntechniques. This enables an improved utilization of network\nparameters and resources. The impact of employing UAVs for\npurely opportunistic relay purposes in cooperative awareness\napplications within vehicular networks is examined in [53]. It\nis not required that UAVs alter their trajectory or speed for\nopportunistic relaying, ensuring minimal interference with the\nexecution of their primary missions. In [54], the authors\nintroduced a novel algorithm based on multi-agent DRL. It\nincludes the design of UAV trajectories to serve mobile\nground users while maintaining network connectivity, the\nproper allocation of frequency resources among UAVs to\nmitigate interference, and the selection of suitable next-hop\nUAVs for each data packet to minimize transmission time and\nreduce the probability of network congestion.\n3) DELAY-TOLERANT UAVS NETWORKS\nDelay-tolerant UAV networks exhibit a notable feature of\nlimited connectivity, necessitating the adoption of a store\u0002and-forward mechanism for the routing protocol due to the\nintermittent availability of links. These networks encounter\ncomparable challenges to conventional networks.\nThe authors in [55] employed UAVs as delay-tolerant\nnetwork relays in order to facilitate communication among the\nground nodes. Every UAV remains stationary above its\ndesignated home-ground node until it receives a signal\nprompting it to initiate the transmission of messages to other\nground nodes. The system employs a genetic algorithm to\nascertain the optimal route for maximizing the efficiency of\ndeliveries. The authors in [56] examined a network of UAVs\nthat are capable of tolerating delays. The researchers\nemployed a spray and wait methodology for the purpose of\npath selection and management. The presence of end-to-end\nconnectivity issues, continuous link fractures, and high latency\nresults in an increase in overheads during path selection. The\nsuggested methodology exhibits suboptimal performance.\nIV. UAV APPLICATIONS AND CATEGORIZATIONS\nUAVs and terrestrial BSs (TBSs) each have unique roles in\nwireless communication networks. UAVs are mobile and\nrapidly deployable, ideal for immediate coverage or areas\nlacking terrestrial infrastructure. However, they face\nlimitations like flight time and payload. In contrast, TBSs\noffer reliable, cost-effective, and long-term network support,\ncovering larger areas and handling higher capacity. The\nchoice depends on specific use cases and deployment needs.\nTable 3 highlights the key differences between UAV BSs\nand TBSs. UAVs have emerged as a groundbreaking\ntechnology with diverse applications across various\ndomains. This section delves into the extensive range of\napplications and categorizations of UAV networks. By\nexploring the main functionalities of UAVs and their specific\nuse cases, an in-depth understanding of the potential impact\nand opportunities they offer in different sectors is provided.\nThe section categorizes UAV networks based on their\napplications, including surveillance and monitoring,\ncommunication relay, IoT support, and swarm intelligence.\nTABLE 3\nUAVS VS. TBSS\nAspect UAV BS TBS\nDeployment\nFlexibility\nRapid and flexible\ndeployment\nFixed and time\u0002consuming\nFlight Time Limited by battery/fuel\ncapacity\nContinuous\noperation\nLine-of-Sight\nRequirements\nRequire clear line of\nsight\nLess affected by\nobstacles\nMobility High mobility and\ndynamic positioning\nStationary and fixed\nlocation\nPayload Capacity Limited due to weight\nconstraints\nLarger capacity for\nequipment\nReliability and\nStability\nProne to disruptions and\ninterference\nMore stable and\nreliable\nSafety Concerns Safety and airspace\nconflicts\nFewer safety\nconcerns\nCoverage Range Adaptable coverage\nrange\nFixed coverage area\nConnectivity in\nEmergencies\nVital for emergency\ncommunications\nSusceptible to\ninfrastructure\ndamage\nSignal\nInterference\nPotentially lower\ninterference Higher interference\nCost Potentially lower\ndeployment costs\nHigher initial\ninvestment and\nmaintenance\nCapacity and\nThroughput\nScalable capacity and\nthroughput\nLimited scalability\nand throughput\nRegulatory\nConsiderations\nAirspace regulations and\nsafety compliance\nLocal regulations\nand zoning permits\nEnergy\nEfficiency\nVariable energy\nefficiency\nStable energy\nconsumption\nA. UAV APPLICATIONS USING 5G\nA) 5G-AND-BEYOND UAV BS:\nThe incorporation of UAVs as flying BSs represents a notable\nprogression in the domain of 5G and subsequent generations\nof communication networks. The utilization of ABSs presents\ndistinct benefits, leading to transformative advancements in\ndiverse applications and augmenting connectivity in\ndemanding settings. This section examines the significant\nsignificance of UAVs as ABSs and investigates their various\napplications, highlighting their potential in influencing the\nfuture of communication.\n1) ENHANCING BEYOND 5G'S COVERAGE AND CAPACITY\nIn the realm of 5G-and-beyond wireless communication\nnetworks, the escalating demand for enhanced capacity and\nubiquitous coverage has propelled UAVs into the spotlight.\nThis heightened attention stems from their remarkable\nqualities, which surpass the capabilities of traditional BSs and\nrelays [57]. Consequently, the current cellular wireless\nnetworks have experienced significant strain in terms of their\ncapacity and coverage, resulting in the birth of numerous\nwireless technologies aimed at addressing this issue. The\nmentioned technologies, including D2D communications,\nultra dense tiny cell networks, and mmWave communications,\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n11\nhave been widely recognized as the focal point of future\nbeyond 5G wireless networks [58]. Nevertheless, although\ntheir immense advantages, these solutions possess inherent\nrestrictions. One example of a requirement for D2D\ncommunication in cellular networks is the need for improved\nfrequency planning and resource utilization. Figure 5\nillustrates UAVs assistance that utilizes for 5G cellular\nnetwork and beyond. In the realm of extremely dense small\ncell networks, numerous issues arise pertaining to backhaul,\ninterference, and the comprehensive modeling of the network\nas a whole. In a similar vein, mmWave communication\nencounters limitations due to blockage and a heavy need of\nline-of-sight (LoS) connection in order to successfully fulfill\nthe potential of providing high-speed, low latency\ncommunications. The aforementioned issues will be amplified\nin scenarios involving UAV and unmanned aerial systems\n(UASs). Also, UAVs equipped with high-definition cameras\nwere being used for surveillance and monitoring purposes in\nvarious industries, including public safety, agriculture, and\ninfrastructure inspection. These UAVs could capture and\ntransmit high-quality video feeds in real-time, providing\nvaluable data for decision-making. In [59], the authors\nconstructed a simulation-level model of a 6G system, and\nconducted a case study to examine the application of high\u0002definition video monitoring, utilizing the principles of UAV\u0002swarm-based surveillance.\nThe integration of UAVs as flying BSs is anticipated to be\nan essential addition to the diverse 5G landscape. This\nintegration holds the potential to address certain obstacles\nassociated with current technology. The utilization of low\naltitude platform UAVs presents a viable and economically\nefficient strategy for delivering wireless connection to regions\nthat possess inadequate cellular infrastructure.\nFIGURE 5. Cellular network for 5G-and-beyond\nFurthermore, the utilization of UAV BSs shows potential in\nsituations, where deploying tiny cells only to cater to brief\nevents, such as sports events and festivals, is not economically\nfeasible due to the limited duration of wireless connection\nrequired for these events [60]. In contrast, high altitude\nplatform UAVs have the potential to offer a viable and\nenduring resolution for addressing coverage concerns in rural\nsettings.\nFurthermore, the utilization of UAVs for mmWave\ncommunications is a promising application wherein UAVs\ncan establish LoS communication linkages with users.\nConsequently, this can serve as an appealing approach to offer\nwireless transmission with a large capacity, capitalizing on the\nbenefits of both UAVs and mmWave communications.\nAdditionally, the integration of UAVs with mmWave\ntechnology, together with the potential utilization of large\nMIMO techniques, has the potential to establish a novel and\ndynamic airborne cellular network. This network can\neffectively deliver high-capacity wireless services, provided\nthat it is meticulously planned and controlled.\nUAVs have the capability to provide support to several\nterrestrial networks, including D2D and vehicle networks. For\nexample, because of their ability to move easily and utilize\nLoS communication, UAVs have the capacity to expedite the\ndistribution of information among equipment on the ground.\nIn addition, the utilization of UAVs has the potential to\nenhance the dependability of wireless connections in D2D and\nvehicle-to-vehicle (V2V) communications by leveraging\ntransmit diversity. Flying UAVs have the potential to assist in\nthe dissemination of general information to ground devices,\nhence mitigating interference in ground networks through a\nreduction in the frequency of communications between\nequipment.\nIn addition, UAV BSs have the capability to employ air-to\u0002air (A2A) connections in order to provide service to other\ncellular-connected UAV- UEs, thereby reducing the burden on\nthe terrestrial network. Numerous investigations have been\nconducted pertaining to the utilization of UAVs in the context\nof 5G wireless communication technology. For instance,\nauthors in [61] addressed radio resource slicing in 5G uplink\nradio access networks. The radio resource slicing problem has\nbeen investigated by using UAV cells to minimize the\nconsumption of uplink resources. They claimed that their\nmethod improves performance in terms of network coverage,\ncosts, and resource utilization. The study undertaken by [62]\ndeals with the uplink issue in 5G networks from the\nperspective of mitigating UAVs\u2019 interference and inter-cell\ninterference. The resource allocation scheme, so-called\nreverse frequency allocation, has been utilized to mitigate the\naforementioned interferences. A study focusing on the\nplacement of UAVs in 5G networks by considering factors\nsuch as latency, throughput, and data rate is introduced by\n[63]. Several approaches have been investigated and\ncompared using methods such as simulated annealing and\ngenetic algorithms and have been aimed to provide efficient\ndata for service providers. Another study focusing on UAV\nplacements is presented by [64]. While [63] focuses more on\napplications during an emergency, this study focuses on\nlogistics applications. They propose a new algorithm that\nmaximizes network capacity by optimizing the placement of\nUAVs.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n12\n2) UAVS AS PLATFORMS FOR PUBLIC SAFETY\nDisasters caused by nature, including floods, hurricanes,\ntornadoes, and heavy snowstorms, frequently result in\ncatastrophic outcomes. During natural catastrophes, it is\ncommon for cellular BSs and ground communications\ninfrastructure to be susceptible to compromise. In situations of\nthis nature, there exists a crucial requirement for the\nestablishment of public safety communication channels\nbetween first responders and individuals in distress, with the\nprimary objective of facilitating search and rescue endeavors.\nTherefore, it is imperative to implement a resilient,\nexpeditious, and proficient emergency communication system\nin order to facilitate efficient communication amid public\nsafety endeavors. In the context of public safety scenarios, the\nimplementation of a dependable communication system is not\nonly vital in enhancing connection, but also in potentially\nmitigating loss of life. Efficiently assessing disaster situations\nposes a formidable challenge for public safety organizations.\nIn such scenarios, where extensive radio coverage of the\naffected area is paramount, UAVs emerge as the most fitting\nsolution [65].\nThe utilization of UAV-based aerial networks as depicted\nin Figure 6 holds significant potential in facilitating rapid,\nadaptable, and dependable wireless communication in public\nsafety contexts. UAVs, due to their lack of reliance on costly\nand restrictive infrastructure such as cables, possess the ability\nto effortlessly navigate and adapt their positions. This attribute\nenables them to promptly deliver communication services to\nindividuals on the ground during emergency scenarios. In fact,\nthe distinctive attributes of UAVs, including their mobility,\nflexible installation, and instant reconfiguration enable them\nto efficiently build on-demand public safety communication\nnetworks.\nFIGURE 6. UAV application in emergency situations\nFor example, UAVs can be utilized as mobile ABSs to\nprovide broadband connection to regions that have\nexperienced disruptions in their terrestrial wireless\ninfrastructure. In addition, UAVs have the capability to\nmaintain a continuous state of motion, enabling them to\neffectively traverse an entire designated region in the shortest\nfeasible amount of time. Hence, the utilization of UAV\u0002mounted BSs presents a viable approach to ensure rapid and\npervasive connection in public safety situations.\n3) MMWAVE COMMUNICATIONS WITH 3D MIMO\nUAV operating in the mmWave spectrum represent an\nexciting frontier in wireless communication technology.\nHowever, UAV mmWave communication presents several\nsignificant challenges that need to be addressed for successful\nimplementation and reliable operation [66, 67]. On the other\nhand, UAVs equipped with mmWave communication systems\noffer impressive data rates and low latency, making them well\u0002suited for various high-bandwidth applications. The mmWave\nsignals are highly directional and require a clear LoS path\nbetween the UAV and the ground station or another UAV for\neffective communication. Obstructions like buildings, trees,\nand even minor terrain variations can disrupt LoS connections,\nlimiting the coverage area and reliability. It is challenging to\nobtain LoS with UAVs is accurate and can be attributed to\nseveral factors: propagation characteristics, propagation loss,\nmobility challenges, weather, and environmental factors.\nDespite these challenges, mmWave frequencies are still\nattractive for UAV communication in certain scenarios. They\noffer high bandwidths and data rates, which are essential for\napplications like high-definition video streaming, real-time\nsurveillance, and UAV swarming.\nIn [68], the authors focused on optimizing the 3D placement\nand orientation of UAVs to ensure LoS coverage for users\nwhile maximizing the SNR between UAV-user pairs. In\nanother work presented in [69], the researchers studied how\nLoS blockage probability affects the connectivity of UAVs in\nterrestrial urban deployments of mmWave NR systems and its\nimplications for communication reliability and performance.\nUAVs also facilitate the utilization of mmWave\ncommunications, capitalizing on LoS links with terrestrial\nusers and mitigating signal attenuation at elevated frequencies.\nThe utilization of diminutive antennas on UAVs enables the\nimplementation of sophisticated MIMO techniques, including\nmassive MIMO, for mmWave communications. Moreover,\nthe utilization of several UAVs might facilitate the\nestablishment of adaptable antenna arrays in the atmosphere,\nhence augmenting the overall efficacy of communication\nsystems [70]. The densely congested sub-6 GHz frequency\nrange falls short of fulfilling the demands for ultra-high data\ntraffic. Exploring the mmWave frequency bands emerges as a\nprospective avenue for UAV communications, as they allow\nfor the deployment of sizable antenna arrays in a compact\nspace on the UAV, enabling three-dimensional (3D)\nbeamforming capabilities [11].\nIn recent times, there has been an increasing level of\nattention towards the concept of 3D MIMO technology, which\ninvolves the utilization of both vertical and horizontal\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n13\ndimensions inside terrestrial cellular networks [71]. The\nutilization of 3D beamforming facilitates the concurrent\ngeneration of distinct beams within 3D spatial domain, thereby\nmitigating inter-cell interference and facilitating the\naccommodation of a larger user population. UAV-based flying\nBSs possess the advantageous ability to effectively discern\nground users situated at varying heights due to their elevated\naltitude. Consequently, they are highly suitable for\naccommodating 3D MIMO scenarios characterized by a\nsubstantial density of users.\nIn addition, the utilization of UAV-based wireless antenna\narrays presents distinct possibilities for airborne beamforming.\nThe array comprises individual units, each representing a\nsingle-antenna UAV. This design enables the array to have the\nflexibility to modify the spacing between its elements and\nprovide effective mechanical beam-steering in any 3D\ndirection. The flexibility and mobility exhibited by UAVs\nfacilitate the provision of effective services to terrestrial\ncustomers in both downlink and uplink situations.\n4) ENHANCING IOT CONNECTIVITY WITH UAVS\nThe rapid progress of wireless networking technologies has\nled to the emergence of a significant IoT ecosystem,\nencompassing a diverse range of devices. To unlock the full\npotential of IoT applications, including the management of\nsmart city infrastructure, healthcare systems, transportation\nnetworks, and energy management, it becomes crucial to\nestablish efficient wireless communication for a vast multitude\nof IoT devices [72]. The immense scale of IoT presents\ndistinctive obstacles that necessitate a reconsideration of\ntraditional wireless networks, such as cellular systems. In the\ncontext of IoT environments, certain key criteria assume vital\nimportance, namely energy efficiency, ultra-low latency,\ndependability, and high-speed uplink communications.\nFurthermore, the deployment of IoT devices in regions with\ninsufficient terrestrial wireless infrastructure presents\nsignificant connectivity obstacles. In [73], the authors delve\ninto the concept of smart cities that harness innovative\ntechnologies such as the IoT and UAVs to improve the\nresidents' quality of life. Mobile UAVs offer a potential option\nto effectively tackle the issues faced by IoT networks. UAVs\nhave the capability to function as airborne BSs, effectively\ncatering to IoT focused situations by delivering dependable\nand energy-efficient uplink IoT communications. Through the\nutilization of their aerial characteristics and elevated\npositioning, UAVs possess the capability to alleviate the\nnegative impacts of shadowing and obstruction. This, in turn,\nenhances the communication channel between IoT devices\nand UAVs. Consequently, IoT devices that are constrained by\nbattery capacity necessitate reduced transmit power in order to\nestablish connections with UAVs, thereby prolonging their\nbattery longevity.\nFurthermore, UAVs have the capability to adaptively adjust\ntheir positions in response to the activation patterns of IoT\ndevices. This feature enables UAVs to effectively support\nlarge-scale IoT systems, eliminating the necessity for\nsignificant development of terrestrial small cell BSs. The\nutilization of UAVs can greatly boost the connection and\nenergy efficiency of IoT networks by leveraging their distinct\nattributes, including mobility, agility, and aerial deployment.\nUAVs present a viable solution for addressing the\ncommunication obstacles encountered in the IoT domain,\nhence enabling the successful implementation of a wide range\nof IoT applications.\nThe implementation of caching mechanisms at small base\nstations (SBSs) holds significant potential in improving user\nthroughput and minimizing transmission delay. Nevertheless,\nstatic ground BSs may have difficulties in catering to mobile\ncustomers who frequently undergo handovers, as the desired\ncontent may not be accessible at the next BS. To tackle this\nissue, the utilization of UAVs as airborne BSs equipped with\ndynamic caching functionalities is suggested. This approach\naims to effectively monitor user mobility patterns and\nfacilitate the delivery of necessary content.\n5) ENHANCING WIRELESS NETWORKS WITH CACHE\u0002ENABLED UAVS\nIntegrating caching mechanisms within SBSs represents a\npromising avenue for enhancing user throughput and reducing\ntransmission latency. Numerous caching models, initially\ndeployed at different types of BSs and mobile devices, have\nbeen thoroughly investigated and are now being extended to\naerial deployment through UAVs. This adaptation addresses\nthe unique challenges posed by 6G networks [19].\nStatic ground BSs may have difficulties in catering to\nmobile customers who frequently undergo handovers, as the\ndesired content may not be accessible at the next BS. In order\nto tackle this issue, the utilization of UAVs as airborne BSs\nequipped with dynamic caching functionalities is suggested.\nThis approach aims to effectively monitor user mobility\npatterns and facilitate the delivery of necessary content. In\n[74], the authors examined the coverage performance analysis\nof a cellular network assisted by cache-enabled UAV-BS by\ndeveloping analytical models to explore both the network's\noverall coverage probability and the average achievable rate\nof cellular users.\nCache-enabled UAVs present a viable approach to alleviate\ntraffic congestion in wireless networks. By utilizing user\u0002centric data such as the distribution of content requests and\npatterns of movement, UAVs can be strategically deployed in\norder to efficiently cater to the needs of users [75]. In contrast\nto static BSs, the utilization of UAVs in caching operations\nresults in a reduction in complexity. This is due to the UAVs'\nability to monitor and analyze user mobility patterns, hence\nobviating the necessity for supplementary caching at ground\u0002based stations. The utilization of a central cloud processor\ninvolves the integration of user-centric information, which is\nderived from past user data, in order to effectively oversee the\ndeployment of UAVs and ascertain the most advantageous\nplaces and mobility patterns. This minimizes the additional\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n14\ncomputational burden associated with updating the content\nstored in the cache. Cache-enabled UAVs employ predictive\nalgorithms to anticipate user mobility patterns and content\nrequest information, thereby decreasing the need for regular\nupdates of content requests across various locations. By using\nthe caching capabilities of mobile UAVs, efficient service\ndelivery to consumers on the ground is facilitated.\nB. RADIO-BASED SENSING\nTo fully integrate UAVs into 5G-and-beyond networks, it is\ncrucial to ensure high-performance wireless communications\nand effective sensing capabilities. Currently, commercial\nUAVs are fitted with a range of embedded sensors, including\nthe inertial measurement unit, accelerometers, tilt sensors,\nand current sensors. The sensors offer instantaneous data for\nensuring the secure operation of UAVs. This includes\nproviding estimations of the UAV's location and orientation,\nmaintaining the desired flight route, and managing power\nconsumption. Nevertheless, as UAVs are increasingly\nincluded into terrestrial communication networks in\nextensive implementations, depending simply on the sensors\nimplanted inside the UAVs will become insufficient. In order\nto get the highest level of sensing performance, it is\nimperative to employ a synergistic approach that combines\nboth sensing capabilities inherent inside UAVs and sensing\ninfrastructure. This integrated strategy offers better reaction\ntime, sensing range, coverage, dependability, precision, and\nefficiency.\nAs depicted in Figure 7, the use of radio-based sensing in\nUAVs may be categorized into two primary frameworks,\nspecifically sensing conducted by UAVs and UAVs employed\nfor sensing purposes. In the aforementioned scenario, sensing\ntechnologies are employed to facilitate the secure operation of\nUAVs and to monitor and manage air traffic in low-altitude\nairspace. In contrast, the paradigm of utilizing UAVs for\nsensing involves the deployment of specialized UAVs as\nairborne platforms to offer sensing assistance from an aerial\nperspective.\n1) SENSING FOR UAV\nSensing for UAVs involves two typical use case scenarios:\nsense-and-avoid (SAA) and UAV detection, tracking, and\nclassification. SAA is essential for safe UAV flying, especially\nfor autonomous or semi-autonomous UAVs [76]. UAVs use\nsensor data to prevent collisions and obstacles without pilots.\nDue to radio propagation delays and ground pilot response\ntimes, SAA is necessary for real-time remote-controlled\nUAVs. Ground-based SAA is an alternative to on-board\nsensors for fast reactions in dynamic settings. UAV-enabled\nspraying requires sensing for constant-altitude maintenance to\nensure consistent spraying even in difficult terrain [77].\nVision- or light-based sensing makes many commercial UAVs\nwith SAA capabilities vulnerable in adverse conditions.\nAnother notable application for UAV sensing involves the\nidentification, monitoring, and categorization of potentially\nunlawful and perilous UAVs. UAVs have the potential to be\nutilized in a manner that compromises public safety and\ninfringes upon individuals' privacy. The mitigation of non\u0002cooperative or deceptive UAVs necessitates the utilization of\npassive radar sensing techniques that rely on the analysis of\nechoed or dispersed signals [78, 79]. The domain under\nconsideration presents several challenges, one of which\npertains to the limited radar cross-section exhibited by UAVs.\nThis characteristic poses difficulties in their identification, as\nit becomes arduous to differentiate them from stationary\nclutter or other airborne entities like avian species.\nCurrent research endeavors have been primarily directed\ntowards the exploration of radar sensing techniques in the\ncontext of UAV networks. The exploration of methods for the\ndetection, tracking, and interception of non-professional\nUAVs has been conducted in [80]. An exhaustive survey on\nsecurity and privacy issues of UAV was presented in [81],\nwith a comprehensive examination of UAV security issues\nconducted at four distinct levels: the software-level, the\nhardware-level, the sensor, and the communication-level.\n2) UAV FOR SENSING\nOne potential and interesting approach in the field of UAV\nsensing is the utilization of UAVs as aerial nodes to offer\nwireless sensing capabilities from an aerial perspective. This\napproach, commonly referred to as UAV for sensing, holds\nsignificant potential. When comparing UAV-based sensing to\nconventional ground sensing, it becomes evident that the\nformer offers numerous advantages. Initially, it is important to\nacknowledge that UAV-based sensing exhibits some\nadvantages due to its increased height and less signal\nblockage. Consequently, UAV-based sensing generally offers\na broader field of vision in comparison to ground sensors.\nMoreover, the exceptional level of control over the three\u0002dimensional movement of UAVs enables the flexible\ndeployment of UAV sensors in challenging and inaccessible\nenvironments, including regions that are toxic or pose\nsignificant risks. In addition, the increased maneuverability of\nUAVs presents a novel opportunity for enhancing sensing\nperformance through the optimization of 3D sensor\ntrajectories. The aforementioned feature holds significant\nappeal in the context of target tracking, as it allows for the\ndynamic adjustment of UAV placements in order to optimize\ntarget tracking capabilities. Hence, the utilization of UAV for\nsensing purposes exhibits a diverse array of prospective\napplications, including but not limited to law enforcement,\nprecision agriculture, 3D environmental mapping, search and\nrescue operations, and military endeavors.\nUAV-based sensing has gained increasing attention due to\nits numerous advantages. Notable applications of UAV-based\nsensor platforms have been outlined in [77], including\nexperiments with an imaging radar. The use of UAVs for\nremote sensing in field-based crop phenotyping was surveyed\nin [82].\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n15\n(a) Sensing for UAV (b) UAV for Sensing\nAdversary UAV\nMonitoring UAV\nAdversary UAV\nSensing and Avoid\nFIGURE 7. Models of sensing in UAV networks: (a) sensing for UAV and (b) UAV for sensing\nIn [83], the authors explored UAV-aided air quality\nsensing. Furthermore,[84] introduced a dynamic and\nreconfigurable aerial radar network composed of UAVs to\ndetect and track unauthorized/malicious UAVs. The study\ndemonstrated that optimizing UAV trajectories offers\nadditional degrees of freedom, leading to improved tracking\nperformance compared to conventional terrestrial radar\nnetworks with fixed deployment.\nC. CELLULAR-CONNECTED UAVS\nThe utilization of cellular-connected UAVs has emerged as\na possible avenue for expanding cellular networks, as UAVs\ncan function as both aerial UE and ABSs. The utilization of\nUAVs in these specific functions offers distinct possibilities\nfor augmenting network coverage, capacity, and the\navailability of services. Nevertheless, additional research is\nnecessary to address the technological obstacles associated\nwith handover management, channel characterization, and\ninterference reduction in order to achieve seamless\nintegration of UAVs into the current cellular infrastructure.\nOngoing research and development efforts in this particular\ndomain have the potential to bring about a significant\ntransformation in communication networks and effectively\nhandle connectivity requirements across diverse situations\nthrough the utilization of cellular-connected UAVs.\nStudies on the use of UAVs in cellular networks have\nrecently gained momentum. Integration into current\napplications brings the problems related to the feasibility of\nUAVs to discussion and paves the way for new research areas.\nAuthors in [85] pointed out flying at different altitudes and\nhaving different operating bands as one of these new\nfeasibility problems. For examining the effects of these\nfactors, they used experimental measurements made in the\nfield and discussed the positive and negative aspects of UAV\nuse in 4G and future 5G applications. They emphasized that\ncarried out measurements show the need for further\nimprovements in cellular-connected UAVs. The same\nproblem was discussed for a carrier frequency of 1800 MHz\nand 2100 MHz, and various altitudes from 15 m to 100 m by\nthe same authors in [86]. As a result of these trials, they also\naddressed the importance of other factors such as antenna\ntilting and 3D coverage models. A study focusses on the\nperformance of different antenna deployments for cellular\u0002connected UAV scenarios can be found in [87]. Another study\nthat considers three-dimensional mobility and LoS channel\nproperties for cellular-connected UAVs is presented by [88].\nThey defined the essential challenges for them and introduced\nthe key performance indicators with analytical models by\nconsidering several performance outputs. Finally, ML solution\nwas proposed for the optimization problem that they\nconstituted. A network planning approach that comprises both\nUAV users and UAV BSs in a three-dimensional medium was\nintroduced by [36]. They claimed that their approach reduces\nthe average latency by 46% compared with the conventional\nsignal-to-interference-plus-noise ratio-based cell association.\nThe proposed approach also provides an improvement in\nspectral efficiency.\nThe present study investigates the potential utilization of\nUAVs as aerial UE and ABSs inside cellular networks.\nSpecifically, the focus is on exploring the integration of\ncellular connectivity in UAVs, thereby enabling them to\nfunction as mobile devices and serve as communication hubs\nin the sky.\n1) UAVS AS AERIAL USER EQUIPMENT\nUAVs have recently been developed as a distinct type of\naerial UE within the context of cellular networks. UAVs\nserve as airborne UE, functioning as communication devices\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n16\nwith the ability to establish cellular connectivity. The\nintroduction of this technology presents numerous benefits,\nincluding improved mobility and the capacity to utilize\ncellular services in geographically distant or temporarily\ndisconnected regions. UAVs serve as airborne UE,\nfacilitating real-time communication for diverse purposes\nsuch as aerial surveillance, disaster response, and\nenvironmental monitoring. Nonetheless, the incorporation of\nUAVs into cellular networks gives rise to various technical\nobstacles, including the need for effective management of\nhandovers, accurate characterization of aerial channels,\ncyber-physical threats, energy consumption, and\nauthentication purposes, and the potential influence of UAV\nmobility on the overall performance of the network. In order\nto tackle these complex issues, the academic community is\nactively engaged in the development of computational and\nalgorithmic solutions to these issues.\nThe influence of the implementation of aerial UEs on the\noverall network efficiency was analyzed by the authors in [89].\nThe simulation findings indicate that the substitution of a\nground user with UAV operating at an altitude of 100 meters\nresulted in a tenfold reduction in throughput. Additionally, the\ncoverage was reduced from 76% to 30%.\nHence, in order to facilitate a satisfactory integration of\nUAVs into cellular networks, the authors in [89] developed a\nmethod that aims to achieve optimal tilting of the directional\nantennas in UAV UEs, thereby enabling their operation at\nelevated altitudes. The simulation findings indicate a\nsignificant improvement in coverage, with an increase from\n23% to 89%. Additionally, the throughput has also shown a\nnotable enhancement, rising from 3.5 b/s/Hz to 5.8 b/s/Hz. The\nsuggested technique exhibits advantages in sparse and\nmoderately dense cellular networks, while presenting\ndisadvantages in highly dense cellular networks.\nThe ideal inter-cell interference coordination technique and\nair-ground performance trade-off were examined by the\nauthors in [90]. The researchers achieved the highest possible\nweighted sum-rate for both the ground users and the UAV by\ncollectively optimizing the uplink cell connections and power\nallocations across various resource blocks. The researchers put\nup a centralized strategy for coordinating inter-cell\ninterference in order to achieve an optimal solution at the local\nlevel. Additionally, they offered a decentralized scheme\nwherein the cellular BSs are divided into clusters, allowing for\ndata exchange solely between the cluster-head and the UAV.\n2) UAVS AS ABS\nThere has been an increasing interest in the utilization of\nUAVs as ABSs within cellular networks in recent times.\nUAVs function as ABSs, operating as mobile\ncommunication nodes that can be strategically positioned to\noptimize network coverage and capacity. ABS technology\npresents a novel approach to enhancing cellular connectivity\nin regions with inadequate infrastructure or on occasions\nnecessitating temporary network reinforcement. The\nutilization of UAVs in aerial deployment facilitates\nenhanced LoS circumstances, resulting in a reduction of\nsignal blockage and shadowing effects. This, in turn, can\ncontribute to an enhancement in communication\nperformance. Nevertheless, it is imperative to tackle certain\nobstacles in order to fully capitalize on the capabilities of\nUAVs as ABSs. These problems encompass the efficient\nplacement of UAVs, seamless coordination with pre-existing\nTBSs, power usage, and effective management of potential\ninterference.\nThe authors in [91] introduced a PLS technique designed to\nenhance the security of wireless access provided by numerous\nUAV-BSs to ground UEs. The researchers examined a cohort\nof individuals who engage in eavesdropping activities with the\nintention of disrupting the transmission of data from UAV\u0002BSs to the ground UEs. The developed technique aims to\nachieve maximum secrecy rate by simultaneously optimizing\nthe transmit beamforming and the power consumption of the\nUAV. The difficulty of enhancing network performance and\ncoverage in the deployment of UAV-BSs was addressed by\nthe authors in [92]. In order to address these concerns, the\nresearchers have developed a proposal for the deployment of\nUAV-BSs using a UAV-artificial bee colony method. The\nalgorithm that has been proposed has the capability to\nascertain the most suitable flying location for each UAV-BS\nin order to achieve the highest possible network throughput.\nThe simulation findings demonstrate that the suggested\nmechanism exhibits superior performance compared to some\nbio-inspired algorithms in terms of enhancing network\nthroughput and improving the coverage rate of UE.\nD. UAV-BASED FLYING AD-HOC NETWORKS\nThe idea of flying ad-hoc networks has become increasingly\nsignificant in the context of 5G and subsequent generations\nof wireless communication technology. Flying ad-hoc\nnetworks (FANETs) are a notable application of UAVs,\nwherein numerous UAVs establish communication linkages\nin an ad-hoc fashion [93, 94]. FANETs play vital roles in a\nwide range of applications, such as traffic monitoring,\nremote sensing, border surveillance, disaster management,\nagricultural management, wildfire management, and relay\nnetworks [95]. FANETs play a significant role in\nestablishing dependable communication connections\nbetween far transmitters and receivers that encounter barriers\nor considerable spatial separation, hence enabling\nuninterrupted connectivity in demanding settings.\nThe utilization of many small UAVs in FANETs offers\nseveral advantages. In contrast to single UAV operations, the\nutilization of FANETs comprising several tiny UAVs has\nsome noteworthy advantages [96].\n\u2022 Scalability: FANETs demonstrate notable scalability,\nfacilitating the seamless enlargement of their operating\ncoverage through the integration of additional UAVs and\nthe implementation of efficient dynamic routing strategies.\nThe scalability of the network enables it to effectively\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n17\nserve a wider geographical area and accommodate\nincreased communication requirements.\n\u2022 Cost Efficiency: The cost associated with deploying and\nmaintaining small UAVs is relatively lower as compared\nto larger UAVs that are equipped with intricate gear and\nsubstantial payloads. The cost-effectiveness of small\nUAVs enables the efficient extension and operation of\nnetworks at a reduced cost.\n\u2022 Enhanced Survivability: One of the advantages of\nFANETs is the improved ability to sustain operations in\nchallenging conditions or in the event of a UAV\nmalfunction. In such situations, the missions can go\nseamlessly by employing the remaining operating UAVs.\nThe improved ability to survive in challenging conditions\nguarantees the uninterrupted execution of missions and the\nresilience of the network, a quality that is lacking in\nindividual UAV systems.\nE. SATELLITE NETWORKS WITH UAVS\nSatellite networks have been utilized for several decades to\noffer a range of services, encompassing Earth observation,\nremote sensing, and satellite communication. The\nconventional architecture of satellite systems, though widely\nused, has certain limitations in terms of cost, reconfigurability,\nand real-time data provision. The incorporation of UAVs into\nsatellite networks offers a potential resolution to these\nobstacles. UAVs have the capability to serve as aerial\nplatforms for the deployment and maintenance of satellite\npayloads. This utilization of UAVs results in reduced costs\nassociated with satellite launches and improved\nreconfigurability of satellite constellations. UAVs have the\npotential to function as relays or data collection nodes for\nsatellites, thereby enhancing the data gathering and\ndistribution capabilities of satellite systems [97].\nIn this particular context, the authors in [98] developed an\nidea regarding the utilization of UAVs and satellites as a\nmeans to facilitate the integration of a substantial quantity of\nIoT solutions within the 5G framework. The proposed\nmethodology utilizes satellites as intermediary nodes, UAVs\nas 5G-UE, and the 5G-gNB is placed on the Earth's surface.\nThe suggested methodology enables a solution of challenges\nassociated with terrestrial infrastructure, such as the\nincreased concentration of IoT devices and limited coverage\nareas.\nThe study conducted by researchers [99] examined a\ncooperative network consisting of many antennas, UAVs,\nand satellites, with the aim of offering uninterrupted access\nto consumers. UAVs are employed as aerial relays to\nfacilitate the transmission of signals between satellites and\nmany ground users. This communication process is\nfacilitated through the implementation of an amplify-and\u0002forward protocol. The researchers derived the expression for\nthe maximum SNR output of the suggested architecture\nusing an opportunistic user scheduling methodology. The\nexpression for the outage probability of the proposed design\nwas derived based on the SNR expression. The equation for\noutage probability is utilized in the context of high SNR to\nascertain the coding gain and diversity order of the examined\ndesign.\nThe authors in reference [100] have presented a safe\ntechnique for relaying UAVs in a hybrid network consisting\nof both terrestrial and satellite components, while\nconsidering the potential threat of an eavesdropper. Three\nUAV Relay Selection (URS) approaches were investigated,\nwhich were based on the Closest (CURS), Maximum\n(MURS), or Uniform (UURS) SNR. Subsequently, an\nassessment was conducted to determine the effects of\ndifferent tactics on the hybrid network, specifically in\nrelation to the risk of secrecy outage. The simulation findings\nindicate that the secrecy performance of MURS is superior,\nwhereas the secrecy performance of UURS is comparatively\ninferior. Additionally, the CURS and UURS techniques do\nnot improve the secrecy diversity order.\nF. OTHER POTENTIAL UAV APPLICATIONS\n1) SURVEILLANCE AND MONITORING\nUAVs assume a paramount role in various surveillance and\nmonitoring applications, rendering valuable contributions\nacross diverse domains [101]. In border and coastal\nsurveillance, UAVs provide an indispensable tool for real\u0002time monitoring of vast and challenging terrains, bolstering\nborder security and maritime operations. Equipped with\nadvanced imaging and sensing technologies, UAVs offer an\nunmatched perspective, enabling researchers and\nconservationists to conduct wildlife studies, assess\nenvironmental changes, and monitor protected areas with\nheightened accuracy. This capability proves invaluable in\npreserving biodiversity and facilitating effective\nenvironmental conservation efforts.\nAdditionally, UAVs herald a transformative shift in\ninfrastructure inspection and maintenance practices. By\nreducing the need for labor-intensive manual inspections,\nUAVs optimize resource utilization and operational\nefficiency. With their ability to capture high-resolution\nimagery and conduct aerial surveys, UAVs empower\nengineers and infrastructure managers to perform detailed\nassessments of structures, utilities, and assets. This enhanced\ndata collection allows for proactive identification of potential\nissues, leading to timely maintenance and cost-effective asset\nmanagement. The integration of UAVs in surveillance and\nmonitoring operations epitomizes their potential to\nrevolutionize critical sectors, bolstering safety, efficiency, and\nprecision in safeguarding environments, structures, and\nresources [71].\n2) COMMUNICATION RELAY\nUAVs play an indispensable role as communication relays,\nparticularly in the face of natural disasters and emergencies.\nWhen terrestrial communication infrastructure is\ncompromised or rendered inaccessible, UAVs swiftly step in\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n18\nto restore critical connectivity [102]. In disaster-stricken areas,\nwhere communication lines may be disrupted, UAVs serve as\nlifelines, relaying essential information and facilitating timely\nrescue and relief efforts. These aerial relays bridge the\ncommunication gap, enabling affected communities to\nconnect with emergency responders and access vital\nresources.\nBeyond disaster scenarios, UAVs extend communication\nservices to remote and underserved regions, overcoming\ngeographical barriers that hinder conventional connectivity. In\nremote and challenging terrains, where establishing and\nmaintaining terrestrial communication networks is\nimpractical, UAVs serve as the much-needed link,\nempowering communities with access to information,\neducation, and essential services. This role is particularly\nsignificant in remote rural areas and isolated communities,\nwhere connectivity fosters socio-economic development and\nempowers individuals with knowledge and resources [103].\nMoreover, UAVs prove instrumental in industrial settings\nwhere communication infrastructure may be limited or non\u0002existent. In remote industrial operations such as mining, oil\nand gas exploration, or infrastructure development, UAVs act\nas communication relays, enabling real-time data transfer and\ncoordination among workers and management. This enhances\noperational efficiency, safety, and remote monitoring\ncapabilities, optimizing industrial processes and minimizing\ndowntime. In emergency response scenarios, such as search\nand rescue missions or medical aid delivery, UAVs serve as\nvital communication relays, facilitating seamless coordination\nbetween on-ground teams and command centers.\n3) INTERNET OF THINGS (IOT) SUPPORT\nUAVs play a crucial function in facilitating the IoT by offering\na wide array of applications. UAVs serve as data collection\nnodes, functioning as aerial sensors to acquire significant data\nfrom distant and demanding locations, catering to a range of\nIoT applications. In the field of precision agriculture, UAVs\nthat are equipped with specialized sensors play a crucial role\nin assisting farmers with the monitoring of crops, analysis of\nsoil conditions, and optimization of agricultural techniques\n[104]. These technological advancements aim to improve\nproductivity and optimize resource management in the\nagricultural sector. Moreover, UAVs that are equipped with\nenvironmental sensors play a significant role in the\nimplementation of smart city applications. These UAVs are\ncapable of monitoring various environmental parameters such\nas air quality and noise levels [105]. The data collected by\nthese sensors is crucial for urban planning and the efficient\nmanagement of cities. The capabilities of UAVs that support\nthe IoT demonstrate their considerable capacity to transform\ndata collection and improve decision-making in several fields.\n4) SWARM INTELLIGENCE\nThe concept of swarm intelligence has revolutionized the\ncapabilities of UAV networks, propelling them towards\nunprecedented levels of efficiency, adaptability, and\ncollaborative potential. Within the realm of UAV swarms,\nseveral key aspects stand out, showcasing their transformative\nimpact on various applications, thus including [106]:\n\u2022 Collaborative Sensing and Mapping: UAV swarms\ndemonstrate exceptional prowess in data collection and\nmapping tasks. By working together cohesively, these\nswarms enable collaborative sensing, efficiently covering\nvast areas and generating accurate 3D maps. This\ncapability finds valuable application in diverse fields,\nincluding land surveying, environmental monitoring, and\ndisaster assessment. Case studies exemplify the swarms'\nefficiency in mapping and monitoring large-scale\nenvironments, revealing their potential in expediting data\nacquisition and enhancing situational awareness in critical\nscenarios as in [107].\n\u2022 Distributed Task Execution: A defining strength of UAV\nswarms lies in their ability to efficiently distribute tasks\namong individual UAVs. In search and rescue operations,\ndisaster response, and emergency medical support, the\ncoordination and resource allocation of UAV swarms lead\nto heightened efficiency and rapid response times. By\ndividing complex tasks into smaller sub-tasks, UAV\nswarms leverage their collective intelligence,\naccomplishing missions that would be arduous or\nimpractical for a single UAV [108].\n\u2022 Adaptive and Resilient Networks: Swarm intelligence\nempowers UAV networks with a remarkable degree of\nadaptability and resilience. In dynamic and unpredictable\nenvironments, such as disaster-stricken areas or regions\nwith limited communication infrastructure, UAV swarms\ndemonstrate their capacity to swiftly reroute\ncommunication paths, ensuring the establishment of robust\nand reliable communication links. This adaptability\nenables UAV swarms to maintain seamless connectivity in\nthe face of challenges, further enhancing their suitability\nfor critical operations. Case studies exemplify the\neffectiveness of UAV swarm networks in maintaining\ncommunication under challenging conditions, solidifying\ntheir position as a reliable communication backbone in\nadverse scenarios as in [109].\nThe integration of swarm intelligence in UAV networks has\nunlocked new frontiers of efficiency and cooperation. UAV\nswarms exemplify a synergy of collective intelligence,\nenabling collaborative data sensing, agile task execution, and\nadaptive networking. Their capacity to tackle complex\nchallenges, provide comprehensive coverage, and respond\nrapidly to dynamic situations positions UAV swarms as a\nformidable force across a wide spectrum of applications. As\nresearch and development in swarm intelligence advance,\nUAV networks are poised to redefine the boundaries of aerial\ncapabilities, opening up new possibilities in fields ranging\nfrom disaster response and environmental monitoring to\ninfrastructure maintenance and beyond.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n19\nV. EXPLORING EMERGING TECHNOLOGIES\nUAVs have emerged as versatile platforms with vast\npotential in the field of wireless communication. Over the\nyears, significant advancements in communication\ntechnologies have been leveraged to enhance UAV\ncapabilities, enabling them to play critical roles in various\napplications. In this section, the recent technological\nbreakthroughs that have revolutionized UAV\ncommunications are explored. These advancements focus on\nsupporting enhanced mobile broadband (eMBB) services\nand enabling seamless communication for massive machine\u0002type communication (mMTC). Specific topics such as\nmassive MIMO (M-MIMO) and mmWave technologies,\nintelligent reflecting surfaces (IRS), non-orthogonal multiple\naccess (NOMA), energy harvesting, energy-efficient\ndesigns, and the integration of artificial intelligence (AI) will\nbe delved into. Through an in-depth analysis of these cutting\u0002edge technologies, the aim is to understand their\nimplications, benefits, and challenges for UAV\ncommunications. This knowledge will equip with the tools\nneeded to develop efficient and robust communication\nsystems for UAVs, leading toward a future where UAVs are\nseamlessly integrated into daily life, industries are\ntransformed, and wireless communication is revolutionized.\nA. M-MIMO AND mmWave\nM-MIMO is a crucial technology in the current 5G\nstandard, showing promise for supporting cellular-connected\nUAV communications [110, 111]. Large arrays at ground\u0002based BSs enable fine-grained 3D beamforming, reducing\ninterference between high-altitude UAVs and low-altitude\nterrestrial users, resulting in higher network throughput.\nHowever, accurate channel state information at ground BSs\nis essential for effective M-MIMO beamforming, presenting\nchallenges with cellular-connected UAVs. UAVs introduce\ncomplexities to the M-MIMO system. Their strong LoS\nchannels cause severe pilot contamination across ground\nBSs, not resolved by conventional decontamination\ntechniques for terrestrial users. Efficient beam tracking is\nchallenging due to UAVs' high mobility in 3D space, leading\nto excessive pilot overhead [112]. The application of hybrid\nbeamforming-based M-MIMO in practical scenarios can\nfacilitate the coordination of UAV groups or swarms.\nHowever, this approach introduces additional challenges\nrelated to pilot contamination and beam tracking [113].\nLeveraging LoS-dominant air-ground channels, it offers\nmore degrees of freedom for macro diversity. However,\nchallenges remain, including efficient power control, low\u0002complexity fronthaul/backhaul provisioning, and network\nscalability for UAV swarms [114].\nFurther, an alternative approach to support rate\u0002demanding eMBB services in 3D space involves utilizing the\nabundant spectrum available in the mmWave bands [92].\nAlthough mmWave communications have inherent\nlimitations like signal attenuation and vulnerability to\nblockage, UAV mobility can mitigate these challenges.\nUAVs, whether acting as aerial platforms or users, can adjust\ntheir trajectories intelligently to reduce propagation loss by\nmoving towards ground nodes and bypass obstacles to\nincrease LoS paths. However, like M-MIMO, mmWave\ncommunication requires a large number of antennas, and the\nhigh UAV mobility and shorter wavelength signals lead to\nfast channel variations. As a result, effective dynamic beam\ntraining and tracking techniques become crucial. Existing\nworks propose the use of movement prediction filters, like\nKalman filters, to track time-varying UAV-ground channels\n[115]. Future research should also address low-complexity\nspectrum management [116] and high-speed, reliable\nbackhaul design as important topics in this domain.\nB. INTELLIGENT REFLECTING SURFACE (IRS)\nAlthough M-MIMO and mmWave communications provide\npotential benefits, their actual application is hindered by\ndifficulties such as high complexity, hardware expense, and\nhigher energy consumption [117]. IRS have recently\nemerged as a viable and economically efficient approach to\nimprove received power and mitigate A2G interference in\nthree-dimensional space [118]. IRS is composed of passive\nreflecting elements, each possessing a configurable\nreflection coefficient. This characteristic allows for\nintelligent coordination of reflections, facilitating the\nreconfiguration of the wireless channel. The integration of\ndesired signals and interference cancellation in a coherent\nmanner leads to a substantial increase in communication\nthroughput, without the need for additional active BSs or.\nrelays. In addition, it should be noted that IRSs possess\npractical advantages such as their lightweight nature, which\nenables convenient deployment on walls or high-speed\nmoving vehicles, hence facilitating a wide range of\napplications [119]. IRS serves as a revolutionary technology\nthat effectively converts the radio environment into an\nintelligent one, hence yielding significant advantages for\nvarious industries such as transportation, manufacturing, and\nsmart cities. The use of IRS has garnered attention as a\nprospective technology for the development of 6G networks.\nExtensive research has been conducted on IRS in several\nsystem configurations, as seen by studies referenced in\nsources [120, 121]. The device has the capability to be\ninstalled on land in order to facilitate communications for\nUAVs (see Figure 8), or it can be affixed to UAVs for\ncommunications on the ground, as depicted in Figure 9\n[122]. Table 4 presents a comprehensive overview and\ncomparative analysis of prior research efforts pertaining to\nthe domains of IRS and UAV.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n20\nTABLE 4\nCOMPARISON OF CURRENT STUDIES ON IRS AND UAV\nFIGURE 8. IRS optimized UAV communications\nFIGURE 9. Improving terrestrial communications using UAV-IRS\nC. NON-ORTHOGONAL MULTIPLE ACCESS (NOMA)\nNOMA has garnered significant attention for its application\nin supporting machine-type devices (MTD). NOMA allows\nmultiple devices to share the same time-frequency resources\nsimultaneously using power domain multiplexing. This\ntechnology is particularly beneficial for the massive\nconnectivity requirements of the IoT and machine-type\ncommunications (mMTC), as it efficiently manages\nresources, increases capacity, and lowers latency. UAVs can\nleverage NOMA to connect and manage a large number of\nmachine-type devices efficiently, making it ideal for\napplications such as remote sensing, environmental\nmonitoring, and automated surveillance [131]. NOMA\nemploys the utilization of superposition coding at the\ntransmitters and successive interference cancellation at the\nreceivers in order to attain effective access and partially\nalleviate co-channel interference. Research has\ndemonstrated that NOMA has notable efficacy in scenarios\nwhen consumers encounter significantly disparate channel\ncircumstances [132].\n1) ADVANCING CELLULAR-CONNECTED UAVS WITH\nNOMA\nNOMA confers pragmatic merits within cellular networks\nhosting both coexisting UAV and ground users. It facilitates\nthe reutilization of resource blocks by UAVs, concurrently\naccommodating a greater number of aerial users, particularly\nin densely populated scenarios, in contrast to the non\u0002scalable orthogonal multiple access (OMA). The inclusion of\nLoS links engenders A2G communications of heightened\nreliability, surpassing that of non-LoS terrestrial channels.\nThe robust LoS air-ground pathways also empower UAVs to\nbe concurrently visible to multiple ground BSs, thereby\nattaining an elevated macro-diversity advantage for user\naffiliation. Nevertheless, the deployment of NOMA within\nsuch air-ground contexts presents its own set of challenges.\nUplink transmissions from UAVs have the potential to\nnotably impair signals from ground users across several\nground BSs, thereby constraining the performance gain of\nNOMA over OMA, particularly in scenarios marked by\nunfavorable channel conditions emanating from individual\nBSs. A resolution to this conundrum materialized in the form\nof a decode-and-forward (DF)-oriented collaborative\nNOMA strategy, which harnessed interference cancellation\nmechanisms among neighboring BSs interconnected by\nbackhaul links [133]. This particular strategy attains\namplified data rates compared to OMA and non\u0002collaborative methodologies, a distinction that becomes\nIRS Use Case No. of IRS Design Objective Approach Ref.\nTerrestrial IRS\nSingle Maximizing the rate successive convex approximation (SCA) [123]\nSingle Power reduction successive convex approximation, and Lagrange duality [124]\nMultiple Achieve maximum weighted rate Reinforcement Learning [125]\nMultiple Enhance received power successive convex approximation [126]\nMultiple Minimizing BER Penalty based algorithm [127]\nUAV-IRS\nSingle Maximum SNR with the lowest\npossible Two-step method [122]\nSingle Maximizing the rate Reinforcement learning [128]\nSingle Optimizing energy efficiency\nsecurity SCA [129]\nSingle Maximizing energy efficiency Fractional programming [130]\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n21\nmore pronounced during congested ground traffic\ncircumstances. Further refinements were introduced through\na quantize-and-forward-oriented cooperative interference\ncancellation schema [134], wherein proximate BSs quantize\nreceived UAV signals sans decoding them.\nIn the downlink phase, UAV receivers confront robust co\u0002channel interference stemming from multiple ground BSs.\nThe conventional methods of interference alleviation utilized\nin the uplink are inapplicable due to the transformation of\nUAVs' roles from sources of interference to recipients. An\napplicable strategy is cooperative beamforming, where non\u0002serving BSs collaborate in transmission to enhance the\nreceived power at UAVs and surmount co-channel\ninterference. Nevertheless, its efficacy wanes with the\nescalation of ground user density, thereby circumscribing the\npool of available BSs for cooperative beamforming\nendeavors. To surmount this predicament, a novel\ncooperative beamforming schema integrating interference\ntransmission and cancellation (ITC) was introduced in [135].\nThis schema involves the forwarding of signals from\nterrestrial users who share the same resource block as the\nUAV to the BSs catering to the UAV. Subsequently, these\nsignals, alongside the UAV's signals, are dispatched through\ncooperative beamforming. This results in the augmentation\nof desired signal potency at the UAV's receiver, concurrently\nquelling terrestrial interference sans exerting any influence\non existing transmissions. While the centralized\nimplementation of this approach involves extensive\nbackhaul transmissions among different BSs, a distributed\nalgorithm based on the concept of divide-and-conquer was\nconducted in [135]. The distributed design requires only\nlocal information exchange among BSs, reducing\nimplementation complexity and signaling overhead.\nResearch results show that the distributed design\nsignificantly improves UAV performance compared to\nconventional schemes without ITC, particularly in high\u0002density terrestrial user scenarios [135]. However, supporting\nmassive UAVs or UAV swarms remains a challenging\nproblem that requires further exploration [136].\n2) NOMA WITH UAV\nUAVs, operating as BSs, can efficiently exploit the varying\nchannel conditions of different ground devices in order to\nmaximize the potential performance advantages offered by\nNOMA. In a previous study, researchers in [137] examined\nthe utilization of UAVs for NOMA transmissions involving\ntwo stationary ground users. The capacity region was\ndetermined by simultaneously optimizing the trajectory of\nthe UAV and the allocation of transmit power/rate over time,\ntaking into account practical limitations such as the\nmaximum speed and transmit power of UAVs. The findings\nindicate that NOMA exhibits superior performance\ncompared to OMA, which includes time division multiple\naccess (TDMA) and frequency division multiple access\n(FDMA). Additionally, the capacity advantage of NOMA\nover OMA diminishes as the maximum speed and/or flight\nduration of UAVs increase. The comparison between two\u0002user NOMA and OMA for UAV-assisted communication\nwas expanded to include other design objectives, such as\nsum-rate [138] and outage probability. The study examined\na Rician air-ground channel conducted in [139] , where a\nUAV followed a circular trajectory at a constant speed. The\ngoal was to reduce the probability of outages, and criteria for\nthe superiority of NOMA over TDMA were established\nusing channel and UAV trajectory parameters.\nIn the context of a multi-user environment, it is imperative\nto establish appropriate user pairing and allocate bandwidth\neffectively in order to fully exploit the capabilities of\nNOMA. Ref. [140], the UAV paired with one user in close\nproximity (cell-centered) and one user at a greater distance\n(cell-edge). Subsequently, the problem of multi-user rate\nmax-min optimization was formulated by simultaneously\noptimizing the allocation of bandwidth, power, UAV height,\nand antenna beamwidth. In subsequent literature, the\napplication of NOMA was further expanded to encompass\nnetworks with multiple antennas and numerous UAVs. For\ninstance, the study conducted in [141] focused on the\ndownlink transmission from a multi-antenna UAV to many\nclusters of ground users. The researchers generated\nanalytical formulas for both the outage probability and the\nergodic rate. The exploration of several UAVs in a large\u0002scale cellular network was further expanded upon in [142].\nIn the study [143], the utilization of the user angle was\nemployed as a means of feedback information for mmWave\nNOMA communications. In circumstances involving multi\u0002antenna transmission, angle information has been\ndemonstrated to possess great potential in enhancing the\nseparation NOMA users in the power domain, as compared\nto the typical limited feedback system that relies solely on\ndistances of users. The study conducted in [144] focused on\nthe resource allocation problem inside a multi-UAV aided\nIoT NOMA uplink transmission system. The objective was\nto optimize the channel assignment, uplink transmit power,\nand flying heights of UAVs in order to maximize the system\ncapacity.\nD. ENERGY EFFICIENCY WITH INNOVATIVE DESIGNS\nEnergy harvesting and energy-efficient designs play a vital\nrole in supporting machine-type devices onboard UAVs.\nMachine-type devices, which are often resource-constrained\nand battery-powered, require efficient energy management\nfor extended operation. UAVs can integrate energy\nharvesting techniques, such as solar cells or kinetic energy\nharvesting, to recharge or supplement the power of\nconnected devices. Additionally, employing energy-efficient\ncommunication protocols and hardware design minimizes\npower consumption, prolonging the battery life of machine\u0002type devices. These advancements enable UAVs to\neffectively manage energy resources and ensure the\nsustained operation of connected devices for prolonged\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n22\nmissions.\nUAVs offer a promising solution to overcome these\nconstraints. In the initial study [145],\nfocused on examining a novel UAV-enabled wireless power\ntransfer system, in which a UAV-mounted energy\ntransmitter is utilized to wirelessly charge distributed energy\nreceivers situated on the ground. Specifically, we delve into\na fundamental scenario involving two users and explore how\nthe UAV can best utilize its mobility through trajectory\nplanning to maximize the energy transferred to both energy\nreceivers within a defined charging timeframe. In [146], the\nauthors introduced a novel wireless power transfer system,\nfacilitated by an UAV equipped with a mobile energy\ntransmitter. The system involves dispatching the UAV to\nprovide wireless energy to a group of energy receivers\nsituated at predetermined ground locations. The research\nexamines the optimal utilization of UAV mobility through\ntrajectory planning to maximize the energy delivered to all\nenergy receivers (ERs) within a finite charging duration.\nAn UAV-enabled wireless power transfer network is\ninvestigated in [147], in which a UAV operates at a\nconsistent altitude in the sky to deliver wireless energy to a\nlinearly arranged set of ground nodes. The primary aim is to\nenhance the minimum received energy for all ground nodes\nby optimizing the UAV's one-dimensional path while\nadhering to the maximum UAV flying speed limitation.\nTo illustrate, the pursuit of system energy amplification\nwas explored in [148], while [149] delved into the\nmaximization of max-min throughput. Broadening the\nscope, [150] extended the optimization endeavors to\nencompass a two-user interference channel housing two\nUAVs. Venturing into advanced methodologies, [151]\nintroduced multi-agent deep reinforcement learning as a\nmeans to grapple with the overarching max-min optimization\nquandary inherent in multi-UAV-enabled WPCNs. More\nrecently, the concept of UAV-empowered simultaneous\nwireless information and power transfer (SWIPT) has been\nintroduced, wherein the UAV undertakes the role of an aerial\nBS for the dual transmission of information and energy to\nterrestrial recipients [152, 153]. The sphere of optimization\ninquiries has embraced factors such as UAV trajectory,\ntransmission power, and the power splitting ratios of users.\nThese considerations are geared towards the maximization\nof achievable rates within the confines of energy harvesting\nconstraints. Furthermore, within the purview of IoT\napplications pertinent to emergency communications, [153]\nfurnishes an encompassing overview of UAV-facilitated\nSWIPT.\nE. ARTIFICIAL INTELLIGENCE INTEGRATION IN UAV\nThe integration of AI in UAV communication systems has\nshown remarkable potential for enhancing efficiency and\nintelligence. AI techniques, such as ML and deep\nreinforcement learning, can be leveraged to optimize UAV\ntrajectory planning, resource allocation, and signal\nprocessing. These approaches enable UAVs to dynamically\nadjust communication parameters for optimal network\nperformance and sensing services. Enormous recent surveys\npapers have been published that mainly focused on AI/ML.\nSeveral surveys discussed UAV communications with ML\ne.g., [154-156]. The survey in [157] explored the training of\nML models across a collection of geographically dispersed\nclusters of resource-limited devices using swarms of UAV.\nIn [158],the survey offered a current and extensive\noverview of ML techniques applied in UAV operations and\ncommunications while identifying areas of potential\nexpansion and research voids. A few general surveys\ndiscussed ML techniques for UAV trajectory in terms of\noptimization [159], planning [160].\nIn UAV-aided communication scenarios, AI-driven\nbeamforming and power control can optimize flight paths in\nreal-time based on wind patterns, detect and avoid obstacles,\nmanage battery usage, while intelligent spectrum\nmanagement can mitigate interference and improve overall\nsystem performance. Additionally, AI-based algorithms can\nenhance UAV sensing capabilities for applications such as\ntarget tracking, environmental monitoring, and disaster\nresponse. The synergy of AI and UAV technologies opens\nup new horizons for advanced communication and sensing\nservices.\nThe integration of AI is a pivotal element in the\ndevelopment of forthcoming cellular networks, playing a\nsignificant role in the achievement of network intelligence\n[161]. The integration of AI technology has given rise to two\nsignificant paradigms in the field of wireless\ncommunications: AI-empowered wireless communications\n[162] and edge intelligence [163]. AI and ML techniques are\nutilized in the former to optimize wireless systems and\nimprove communication performance. This departure from\nconventional model-driven approaches emphasized the use\nof data-driven mathematical techniques. In the\naforementioned framework, the integration of AI and MEC\ncapabilities occurs within BSs and access points located at\nthe network edge [164]. This integration facilitates the\ndeployment of intelligent applications that demand\nsignificant communication and computation resources,\nincluding autonomous driving and industrial automation\n[163]. Edge intelligence provides a reduction in end-to-end\nlatency and a decrease in traffic loads to the core network\nwhen compared to traditional cloud and on-device\nintelligence methods.\nIn addition to the aforementioned paradigms, AI is poised to\nplay a crucial role in the realm of 5G and future UAV\ncommunication networks, offering two pivotal features. AI\nand ML emerge as viable computational tools to address\nintricate challenges in highly dynamic UAV-enabled 3D\nnetworks. These methodologies present a promising\nalternative to conventional model-driven optimization\napproaches, particularly in scenarios where obtaining precise\nnetwork state information proves challenging. A notable\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n23\nproblem that benefits from these techniques is the joint\noptimization of UAV trajectory and communication design.\nThe integration of UAVs into edge intelligence not only\nenables the development of innovative applications, such as\nUAV virtual reality and UAV swarms, but also introduces\nnovel complexities in efficiently managing computationally\ndemanding and time-sensitive AI tasks from an aerial\nperspective. This situation necessitates the collaborative\ndevelopment of mobility and trajectory control for UAVs,\nalongside the allocation of communication and compute\nresources. However, this task presents significant challenges\ndue to the dual roles that UAVs can assume, acting as either\naerial users or aerial edge servers, or even adopting both\nroles simultaneously. The concurrent management of these\nfunctionalities requires careful consideration and effective\nresource allocation strategies to ensure seamless integration\nand optimal performance within UAV communication\nnetworks. In this subsection, the examination begins with\nML techniques utilized for the design of UAV trajectories\nand communications. Subsequently, the design of\ncomputation offloading for UAVs integrated with MEC is\ndelved into. Lastly, the concept of distributed edge ML\ninvolving UAVs is presented.\n1) OPTIMIZING UAV TRAJECTORY USING ML\nThe optimization of both UAV movements and\ncommunication utilization of resources is of utmost\nimportance in order to achieve optimal performance in 5G\nand future UAV communication networks. Conventional\noffline methodologies that rely on model-driven\noptimization presuppose complete or partial knowledge of\nnetwork state information. However, these approaches may\nnot be well-suited for dynamic environments characterized\nby fluctuating traffic demands, user mobility, and\ncomplicated channel propagation caused by UAVs. In order\nto solve this, academics are now adopting ML techniques and\ndata-driven methodologies.\nML can be categorized into three main types: supervised\nlearning, unsupervised learning, and reinforcement learning\n(RL). RL holds considerable promise in the realm of\ndesigning coordinated movement and communication\nstrategies for UAVs. RL facilitates the swift adaptation of\nUAV communication networks to dynamic environments by\nimproving various aspects such as UAV actions (e.g.,\ndeployment, trajectory, and resource allocations) and reward\nfunctions (e.g., communication rate). Two study areas have\nused RL to optimize UAV operations in the literature:\ncellular-connected UAVs [165] and UAV-assisted\ncommunication networks [166], where UAVs act as BSs and\nusers, respectively.\nIn [167], the authors proposed a deep reinforcement\nlearning (DRL) approach for optimizing the trajectories of\nUAVs in the context of ultra-dense small cell networks. The\npremise involves UAVs equipped with sensing radios to\nacquire distance data pertaining to the UE and other UAVs\nwithin the network, which is subsequently employed to\nadjust the trajectories of the UAVs. However, the complex\npath planning challenge for each UAV remains a persistent\nissue. An optimal operational strategy is introduced in [168]\nby utilizing multi-agent RL to address these challenges.\nMultiple parameters, including the quantity of deployed\nUAVs, initial charging capacity, and charging completion\ncapacity, define a multi-UAV system. In [169], the authors\npresented a multi-agent deep Q-network scheme, where the\nUAVs operate as agents, independently performing actions\nbased on their observations while sharing a common reward.\nAdditionally, a multi-agent meta-RL algorithm is introduced\nfor rapid adaptation to new tasks to address tasks with limited\nprior experience.\nThe authors in [170] introduced utilization of an UAV\u0002enabled relaying system in emergency communications\nbased on RL methods. The primary objective is to maximize\nthe aggregate data transmission from the users to the BS,\nachieved through the optimization of user communication\nscheduling, user association, power allocation, and UAV\ntrajectory.\nThe localization of ground users through the utilization of\nUAVs as aerial anchors is investigated in [171]. It introduced\nan innovative localization framework that incorporates FL\nand RL. This framework involves multiple UAVs learning\ntrajectories within diverse environmental settings, resulting\nin faster convergence of the RL model and reduced\nlocalization errors.\nThe researcher in [172] presented beamforming control\nand trajectory design algorithm based on as multi-pass deep\nQ-network. In this algorithm, the UAV serves as an agent\nresponsible for periodically observing the state of the UAV\nmulticast network and taking actions to adapt to the dynamic\nenvironment.\nA novel DRL technique named pointer network-A*\ndesigned in [173] to efficiently learn a UAV trajectory policy\nthat minimizes energy consumption. The parameters of\npointer network-A* are trained using small-scale cluster\nproblem instances, aiming for quicker training through an\nunsupervised approach with the actor-critic algorithm. In\n[174], the authors presented a new scheme for joint trajectory\nand communication scheduling in wireless caching\nnetworks, involving multiple UAVs. To simplify the solution\non each UAV, a model-specific deep neural network (DNN)\nis introduced to learn the optimal control solution in real\u0002time. The DNN is designed in accordance with the structural\nproperties of the value function and stationary distribution,\nbased on the analysis of the homotopy perturbation method.\n2) COMPUTATION OFFLOADING FOR UAV-ENABLED\nEDGE INTELLIGENCE\nEdge intelligence applications that involve the integration of\nUAVs and AI rely on the generation and processing of\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n24\nFIGURE 10. Computation Offloading Utilizing UAVs: (a) UAV Connected to Cellular Network; (b) UAV-Enhanced MEC System\nvast volumes of data at distributed edge devices. These\ndevices encompass not only UAVs but also conventional\nsmart sensors and smartphones. The seamless execution of\nsophisticated AI training and inference algorithms is\nessential to derive meaningful insights and enable intelligent\ndecision-making processes. However, the implementation of\nthese AI tasks is often computationally demanding and data\u0002intensive, surpassing the computational capacities of the\nlocal wireless devices themselves. To address this challenge,\ncomputation task offloading emerges as an appealing\nsolution. It allows UAV edge servers to offload their\nresource-intensive AI tasks to MEC servers equipped with\nhigh computation capabilities. Once offloaded, the MEC\nservers process the tasks and transmit the computation\nresults back to the UAV edge servers for further analysis or\naction. This approach optimizes the utilization of\ncomputational resources and ensures efficient AI execution\nwhile minimizing the computational burden on the UAVs\n[175].\nFigure 10(a) illustrates a typical scenario where a UAV\nserves as either an aerial user or an edge device within\ncellular networks. In such cases, the UAV may possess\ncomputation tasks that can be efficiently executed through\noffloading to ground-based BSs. This strategy enables\neffective collaboration between UAVs and BSs to handle\nsophisticated AI tasks, enhancing the overall network\nperformance. Moreover, UAVs have the potential to carry\nMEC servers themselves, extending their support to on\u0002ground devices' AI implementations. Figure 10(b)\ndemonstrates how UAVs can aid widely distributed devices\non the ground during computationally intensive AI tasks,\nparticularly in urgent situations, such as emergency response\nscenarios. The UAV's mobility and proximity to the devices\nenable rapid deployment of computational resources and\nseamless offloading of AI tasks, contributing to timely and\nintelligent decision-making processes. Under both scenarios,\nthe joint design of UAV trajectory, communication, and\ncomputation becomes paramount. The proper coordination\nof these aspects ensures efficient and effective task\noffloading, enabling edge intelligence applications to\nfunction seamlessly in real-world environments. In this line\nof research, AI tasks are typically modeled as general\ncomputation tasks with specific data and computation\nrequirements, facilitating the integration of AI capabilities\nwithin UAV-enabled communication networks and MEC\nenvironments. This integration promises to unlock novel\nopportunities for improving network intelligence, enhancing\ncommunication performance, and enabling intelligent\napplications with extensive communication and computation\nrequirements.\n3) UAV-DRIVEN FEDERATED EDGE LEARNING\nAside from the process of diverting computations to a central\nMEC server, edge devices such as UAVs have the potential\nto participate in cooperative AI tasks, capitalizing on their\nlocally dispersed data and computational proficiencies. This\nmethodology, recognized as distributed edge learning,\nencompasses the concept of federated edge learning, which\nconfers benefits in terms of data security and privacy [163].\nWithin the realm of federated edge learning, an assemblage\nof edge devices, encompassing UAVs, collaboratively\nemploy their dispersed data to facilitate the training of shared\nAI models without necessitating data exchange. The edge\nserver in this context can take the form of a terrestrial-based\nBS in instances involving cellular-connected UAVs, or\nanother UAV within UAV swarms, as portrayed in Figure\n11(a) and Figure 11(b). The iterative execution of federated\nedge learning encompasses UAVs revising their local AI\nmodels during each iteration and subsequently consolidating\nthese updates at the edge server to refine global AI models.\nThis undertaking demands recurrent interchange of AI model\nparameters between UAVs and the edge server, thereby\nnecessitating meticulous optimization of multiple UAV\ntrajectories, communication strategies, and computation\nscheduling over temporal intervals, attributing to the 3D\nmobility of the UAVs. Despite the escalating interest in\nfederated edge learning within the realms of wireless\ncommunications and ML communities, research pertaining\nto the integration of UAVs in edge learning is still in its\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n25\nFIGURE 11. Federated Edge Learning Utilizing UAVs: (a) Coordination with Ground BSs; (b) UAV Peers\nnascent stages. Several studies have investigated the\npotential of federated learning applications within UAV\nwireless communication networks [176]. The allocation of\nwireless resources to ensure the efficiency of UAV-enabled\nfederated edge learning was the subject of examination in\n[177]. Moreover, inquiries have delved into the realm of\nfederated learning for UAV swarms as documented in [178].\nFurthermore, proposals have been put forth wherein UAVs\nundertake the role of edge servers in the context of federated\nlearning, as presented in [179], including considerations for\nsupport within the Internet of vehicles framework as\nexplored in [180]. Nonetheless, a comprehensive exploration\ninto the fundamental performance thresholds of federated\nlearning when integrated with mobile UAV nodes remains\nlargely uncharted.\nIn [181], the authors introduced a federated learning\nframework for UAV swarms, incorporating MEC where\nmodel aggregation is shifted to edge servers. In this\nframework, the overall federated learning cost is\ncharacterized as a weighted combination of the total delay\nincurred by UAV swarms to complete the federated learning\ntask and the energy consumption of the system. To facilitate\ndynamic and intelligent UAV services, a centralized\ndynamic service algorithm called deep deterministic policy\ngradient based centralized has been introduced in [182],\nrelying on deep reinforcement learning. Nonetheless,\nconsidering the training complexities associated with the\ncentralized approach, a more favourable distributed learning\nalgorithm, federated learning-based federated, has been\nproposed, which integrates federated learning methods.\nA cognitive network of UAVs has been introduced in\n[183], with the primary objective of providing dependable\nedge computing services to IoT devices within a specified\narea. To minimize latency for IoT devices, a partial federated\nlearning model has been devised and implemented on UAVs.\nA critical challenge arises from handling the non\u0002independent and non-identically distributed nature of\nheterogeneous data while ensuring learning convergence. To\neffectively tackle this issue, a novel and high-performance\nfederated learning scheme, referred to as the hierarchical\nfederated learning algorithm, is introduced in [184] for the\nedge-assisted UAV network. This approach leverages edge\nservers positioned in BSs as intermediate aggregators,\nincorporating commonly shared data to address the challenge\neffectively.\nThe effectiveness of global federated learning models may\nbe hampered by the significant heterogeneity of local data,\npotentially hindering the training process, and undermining\nthe performance of local agents. To overcome these\nchallenges, a novel approach is introduced in [185], referred\nto as personalized federated DRL (PF-DRL), designed for\nmulti-UAV trajectory optimization. PF-DRL seeks to create\npersonalized models for each agent, effectively addressing\ndata scarcity concerns and alleviating the adverse effects of\ndata heterogeneity.\nF. SYMBIOTIC RADIO COMMUNICATION AND SENSING\nActive radio technology, depending on the design of\ntransmitters that consist of highly power-consuming\nelements such as oscillators, up-converters, and other\ncomponents, has a negative impact on the newly developed\nservices in the next wireless networks by shortening battery\nlife, especially for those devices that have a limited battery\nor are difficult to recharge and replace, such as IoT devices\nand UAVs that need to utilize the available battery efficiently\n[186].\nBackscattering radio technology is a recent approach to\ndesigning transmitters with zero active components, leading\nto the elimination of the consumed power by means of active\ntransmission. One more efficient technology is called\nambient backscatter communication (AmBC), which uses\nambient radio frequency (RF) signals such as cellular\nsignals, Wi-Fi signals, or any form of RF transmission that\ncoexists to serve a primary user. The transmitter in AmBC\nembeds its message in the ambient RF signal by varying the\nreflection coefficients through varying the load impedance at\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n26\nthe backscatter transmitter. Then an intelligent receiver can\ndetect changes in the reflection coefficients to decode the\nmessage transmitted by the AmBC transmitter.\nRecently, a new technology known as symbiotic radio\n(SR), which benefits from both CR and AmBC to overcome\ntheir drawbacks effectively. SR overcomes CR by having\ntwo spectrum sharing systems, primary and secondary,\nwhich helps in providing mutually beneficial spectrum\nsharing. On the other hand, SR overcomes AmBC by\nenhancing more reliable backscattering through joint\ndecoding [187]. Therefore, the utilization of SR technology\nhas considerable potential as a viable approach to facilitate\ncollaborative resource allocation across radio systems via\nsymbiotic associations [186].\nKnowing that, UAVs will play a major role in the next\ngeneration of wireless networks with a wide range of\napplications, as discussed in Section IV [188]. With limited\nbattery life, UAVs need efficient energy management to\nensure sustainability and long battery life [90]. To address\nthis challenge, UAVs can leverage SR to reduce the need for\nactive transmitters. Utilizing SR in 5G-and-beyond will\nsignificantly reduce the power consumption of UAVs,\nenabling long-term and sustainable applications in the future\n[187].\nFigure 12 shows that not only SR can be used for\ncommunication but also for sensing UAVs. In this figure, a\nscenario includes air-to-air and air-to-ground symbiotic\ncommunication and sensing (SCAS), in which a\ncommunication signal is transmitted from a satellite to an\nUAV and a ground station. The UAV utilizes backscatter\ntechnology to transmit its data via backscattering\ncommunication. Upon receiving the backscattered signal, the\nbase station performs SCAS to determine the location of the\nUAV as well as retrieve the accompanying data [186].\nFIGURE 12. Air-to-Air and Air-to-Ground SCAS\nVI. UAV REGULATIONS\nThe widespread deployment and operation of UAV-based\ncommunication systems face substantial challenges and\nlimitations due to regulatory issues. Governments and\nregulatory entities across the globe are currently engaged in\nthe formulation of regulations aimed at guaranteeing the\nsafe, secure, and responsible utilization of UAVs [189, 190].\nThe regulatory factors and considerations that influence\nUAV operations encompass the following:\n1) LICENSING AND REGISTRATION\nIn many regulatory frameworks, operators of UAVs are\nmandated to acquire licenses or certifications for both\ncommercial and recreational purposes. However, some\ncountries like Vietnam does not mandate the possession of a\npilot license for UAV operations, distinguishing its\nregulations from those of notable jurisdictions like the\nUnited Kingdom, the United States, Singapore, and Australia\n[191]. The acquisition of these licenses may entail the\ncompletion of knowledge assessments, the attainment of\npilot certifications, or the acquisition of specific\nauthorizations, which are contingent upon the weight and\nfunctionalities of the UAV. Furthermore, it may be necessary\nto register UAVs with the relevant governing bodies in order\nto establish a system of responsibility and the ability to track\ntheir movements.\n2) FLIGHT RESTRICTIONS AND AIRSPACE REGULATIONS\nUAVs are required to adhere to airspace regulations in order\nto uphold safety standards and mitigate the risk of potential\ndisruptions to manned aircraft operations. Restricted\nairspace zones, no-fly zones, and altitude limitations are\nestablished by authorities in order to mitigate the potential\nfor collisions and uphold the integrity of manned aviation\noperations [16, 190]. Adherence to these regulations is of\nutmost importance, in order to mitigate the risk of\nunauthorized access to restricted zones and ensure the\npreservation of a secure distance between aircraft.\n3) REMOTE IDENTIFICATION AND TRACKING\nRemote identification technology for UAVs is an emerging\ncapability enabling ground-based observers to identify UAVs\nin airspace while gathering relevant information about the\nUAV and its operator [192]. The overarching concept and\nstandardized framework for remote identification of UAS\nprimarily pertain to the electronic identification of airborne\nunmanned aircraft. These standards establish the baseline\nperformance criteria for direct remote identification [193].\nAdditionally, 3GPP has initiated various endeavors to cater to\nthe connectivity requirements of UAS using mobile networks,\nincluding the 5G system. The comprehensive requisites for\nremote identification of UAS are outlined in [194]. In order to\naugment accountability and streamline identification\nprocesses, it may be necessary for regulations to mandate the\ninclusion of remote identification and tracking capabilities in\nUAVs. This capability allows authorities to ascertain the\nidentity of the operator and monitor the UAV throughout its\nduration of flight. Remote identification systems commonly\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n27\nemploy the transmission of distinct identifiers, such as serial\nnumbers or digital signatures, to ground control stations or\nauthorized receivers.\n4) OPERATIONAL LIMITATIONS AND RESTRICTIONS\nRegulatory measures have the potential to impose operational\nconstraints on UAVs, encompassing limitations on factors\nsuch as flight altitude, proximity to airports, and traversing\nspecific regions such as densely populated areas, vulnerable\nfacilities, or essential infrastructure. The aforementioned\nlimitations are implemented with the objective of safeguarding\nthe well-being of the general public, ensuring the preservation\nof personal privacy, and maintaining the overall security of the\nsystem. Airspace restriction represents a prevalent regulatory\napproach applied across numerous countries and regions\nconcerning UAV operations [16]. Particularly in nations with\nstringent UAV regulations, such as the United States and\nChina, UAV operators must seek airspace authorization before\nconducting flights. In this process, assuming the UAV is\nregistered under the operator's real name and the pilot holds\nthe necessary certification, the pilot submits both the flight\nplan and airspace authorization request to the relevant airspace\nmanagement authority.\n5) PRIVACY AND DATA PROTECTION\nThe emergence of privacy issues pertaining to UAV\noperations has prompted the establishment of regulatory\nmeasures aimed at preserving individual privacy and ensuring\nthe security of personal data [195]. Recent developments in\nsecurity and privacy concerns impact the IoD network, along\nwith contemporary techniques for mitigating IoD attacks\n[196].\nRegulations have the potential to impose restrictions on the\ncapturing and processing of personal data without obtaining\nconsent, as well as to impose limitations on the utilization of\nsurveillance equipment in specific situations. Ensuring\nadherence to privacy regulations frequently entails the\nacquisition of explicit consent, the anonymization of collected\ndata, and the implementation of secure data handling\nprotocols.\n6) SPECTRUM ALLOCATION\nUAV communication systems heavily rely on the allocation of\nwireless spectrum to enable the seamless transmission of data\nand control signals. Regulatory authorities hold a pivotal role\nin this spectrum allocation process, ensuring the provision of\nsuitable frequency bands for UAV communication. Their\nprimary objective is to safeguard uninterrupted UAV\noperations by effectively managing potential interference\nissues. The overarching goal of these spectrum allocation\nregulations is to strike a delicate balance between the\nescalating demand for spectrum resources and the specific\nrequirements of UAV communication systems.\nThe operation of UAVs introduces numerous complexities\nin the realm of radio spectrum management, with a focus on\nensuring operational safety, efficient spectrum utilization, and\nharmonious coexistence with pre-existing wireless networks.\nConventional spectrum allocation methodologies prove\ninadequate when applied to UAV networks, primarily due to\nthe dynamic nature of UAV operations. This dynamicity\nnecessitates adaptive spectrum strategies and robust\nmechanisms to ensure the continuous and reliable delivery of\nservices [15]. For UAS operation, radio spectrum usage\nencompasses a range of critical functions, including\ncommunication, navigation and surveillance electronic\nconspicuity, command and control, detect and avoid, as well\nas the relay of payload data [197].\n7) OPERATIONAL PROCEDURES AND SAFETY\nSTANDARDS:\nRegulations have the potential to delineate operational\nprocedures, safety standards, and maintenance requirements\npertaining to UAVs. The primary objective of these\nregulations is to establish measures that guarantee the secure\nfunctioning of UAVs and minimize the potential hazards\narising from malfunctions, collisions, or incidents. The\nstandardization in [198] outlines specific prerequisites for\nensuring the quality and safety of UAS design and production,\nencompassing unmanned aircraft, remote pilot stations,\ndatalinks, payloads, and associated support equipment. The\nwork in [199] examined New Zealand's regulatory framework\nfor aviation safety concerning unmanned aircraft, considering\nthe viewpoint of unmanned aircraft operators. Generally, any\ndocuments provided by an organization may encompass a\nrange of essential components, such as pre-flight check\nprotocols, records of maintenance activities, emergency\nresponse protocols, and the necessary training prerequisites for\noperators.\n8) BEYOND VISUAL LINE OF SIGHT (BVLOS) OPERATIONS\nRegulations frequently impose limitations on BVLOS\noperations, referring to the operation of UAVs beyond the\noperator's direct visual range. BVLOS operations necessitate\nthe utilization of sophisticated technologies, seamless\nintegration into existing airspace systems, and the\nimplementation of comprehensive safety protocols. In\nBVLOS operations, the UAV is permitted to function beyond\nthe LoS, adhering to a predetermined flight path and relying\non instrumentation-based flight data, including onboard\ncameras and detect-and-avoid technologies [200].\nRegulatory bodies are currently engaged in active\nexploration and development of regulations aimed at\nfacilitating and overseeing BVLOS operations. These\nregulations are being formulated with the intention of\naddressing various concerns pertaining to safety, collision\navoidance, and the establishment of effective command and\ncontrol mechanisms.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n28\n9) INTEGRATION WITH AIR TRAFFIC MANAGEMENT\nSYSTEMS\nThe integration of UAVs into established air traffic\nmanagement systems assumes paramount importance as the\nfrequency of UAV operations continues to rise. A shared\ncomprehension of the essential functions and structural\naspects of UAS traffic management is presented in [201],\noffering a comprehensive explanation of the system layer\nwithin the UAS traffic management framework. Additionally,\nin [202], a framework for ensuring adherence to regulatory\nguidelines, encompassing safety, security, privacy, and\nvarious organizational prerequisites, is presented for entities\noffering UAS traffic management services. In order to ensure\nthe safe integration of UAVs and to prevent conflicts with\nmanned aircraft in controlled airspace, regulations may\nnecessitate the adherence of UAVs to designated protocols,\ncommunication standards, and coordination procedures. The\nfuture perspective of 6G-enabled UAV traffic management\necosystems in highly congested urban airspace, emphasizing\nnon-terrestrial aspects, encompassing aerial and satellite\ncommunication [203].\n10) SECURITY AND COUNTER-UAV MEASURES\nRegulations have the capacity to effectively mitigate security\nconcerns that arise from the operations of UAVs,\nencompassing issues such as unauthorized access, potential\nthreats, and the implementation of counter-UAV measures.\nThese regulations encompass a wide range of operational\nguidelines, safety measures, and legal frameworks aimed at\npromoting responsible UAV use while addressing potential\nsecurity threats posed by UAVs [204]. Government entities\nhave the ability to establish regulations regarding the\nutilization of specific UAV technologies, deploy counter\u0002UAV systems in areas of high sensitivity, and develop\nprocedures for reporting and addressing security incidents. A\nmethodology for assessing attack and countermeasure\ntechniques in of commercial UAVs is outlined in [205]. In\n[206] delineates prerequisites for the operational procedures\nof unmanned aircraft, which, when combined with existing\nand forthcoming standards on UAS, constitute a\ncomprehensive safety and quality standard for unmanned\naircrafts. Additionally, it applies universally to all commercial\nUAS, regardless of their size, classification, purpose, or\nlocation, and represents the global benchmark for the secure\noperation of commercial UAS.\nRegulations pertaining to UAVs exhibit variability across\ndifferent countries and geographical regions, with distinctions\nmade based on factors such as urban or rural settings.\nRegulations governing UAVs operations in the United States\nare promulgated by the federal aviation authority (FAA) and\nthe national aeronautics and space administration (NASA).\nNASA is currently engaged in a collaborative effort with the\nfederal communications commission (FCC) and the FAA to\nundertake the development of UAV control frameworks. FCC\nis presently engaged in an investigation to determine the\nnecessity of establishing a new spectrum policy specifically\ntailored to regulate UAV operations.\nVII. RESEARCH TRENDS AND OPEN CHALLENGES\nA. RESEARCH TRENDS\nThe integration of UAVs into 5G-and-beyond networks has\nemerged as a promising paradigm to revolutionize wireless\ncommunications, enabling a wide array of innovative\napplications and services. As the deployment of UAVs\nbecomes more prevalent, researchers and industry\nstakeholders are actively exploring novel approaches to\nharness the full potential of these aerial platforms in synergy\nwith advanced communication technologies. This section\ndelves into the latest trends shaping UAV-enabled\ncommunication systems, encompassing heterogeneous\nnetwork integration, UAV swarm communication, security\nand privacy, ML and AI for UAVs, green UAV\ncommunication, and spectrum management.\n1) HETEROGENEOUS NETWORK INTEGRATION\nThe integration of UAVs into terrestrial heterogeneous\nnetworks (HetNets) is currently a topic that has been\ncapturing significant attention in the research community.\nThis integration aims to enable UAVs to interact seamlessly\nwith various types of networks, such as 5G, 6G, Wi-Fi, and\nsatellite networks. By integrating UAVs into HetNets, the\nscope of applications for UAV communication and\nnetworking expands significantly.\nOne of the key benefits of heterogeneous network\nintegration is the enhanced coverage and connectivity that\nUAVs can provide. As UAVs can operate at varying altitudes\nand positions, they can extend the reach of terrestrial\nnetworks to remote or difficult-to-access areas. Additionally,\nUAVs can act as aerial relays to facilitate communication in\nareas with limited ground infrastructure. The flexibility of\nUAVs to switch between different networks based on factors\nlike network availability, demand, and application\nrequirements offers numerous advantages. For instance,\nduring emergencies or events with high data traffic, UAVs\ncan be deployed as temporary network boosters to offload\ntraffic from congested TBSs. Moreover, UAVs can be\nutilized to bridge communication gaps in disaster-stricken\nregions, enabling critical communication and coordination.\nResearch in this area is focused on addressing various\nchallenges, such as efficient handover mechanisms,\ndeveloping intelligent network selection algorithms [207],\ncross-layer design approaches, and seamless integration of\nUAVs into existing HetNets. By effectively addressing these\nchallenges, the integration of UAVs into HetNets promises\nto revolutionize the way wireless communication is\napproached in 5G-and-beyond networks [208].\n2) UAV SWARM COMMUNICATION\nThe concept of UAV swarms, where multiple UAVs operate\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n29\nin coordinated groups, is an exciting area of research with\npromising applications. UAV swarms have garnered\nconsiderable interest in fields like surveillance, delivery\nservices, and emergency response scenarios. These swarms\noffer significant advantages over individual UAVs, such as\nincreased reliability, enhanced coverage, and improved\nscalability.\nIn UAV swarm communication, research is focused on\ndeveloping efficient communication protocols and\nalgorithms to facilitate seamless and reliable inter-UAV\ncommunication. This entails addressing challenges related to\nswarm coordination, dynamic formation and dissolution of\nswarm members, and information exchange among UAVs.\nTo achieve effective swarm communication, researchers are\nexploring communication strategies that ensure\nsynchronized actions and cooperation among UAVs in\nswarm scenarios.\nOne of the key areas of interest is establishing robust\ncommunication links between UAVs within the swarm while\nminimizing interference and latency. By optimizing\ncommunication paths and dynamically adapting\ncommunication strategies based on swarm dynamics,\nresearchers aim to enhance the performance and resilience of\nUAV swarm communication [209].\n3) SECURITY AND PRIVACY\nAs UAVs become increasingly integrated into daily life,\nensuring secure and private communication is of paramount\nimportance. UAV communication networks handle sensitive\ndata and perform critical tasks, making them potential targets\nfor cyberattacks and privacy breaches.\nTo address these concerns, ongoing research focuses on\ncryptographic techniques, trust-based protocols, and\nprivacy-preserving mechanisms designed specifically for\nUAV communication networks. Encryption and secure key\nexchange protocols ensure that transmitted data remains\nconfidential and protected from unauthorized access. Trust\u0002based protocols establish a system of trust between UAVs\nand ground stations, enabling secure communication and\npreventing unauthorized access to the network. Additionally,\nprivacy-preserving mechanisms aim to protect user identities\nand sensitive information, ensuring that UAV operations\nremain anonymous and secure.\nMoreover, secure and robust communication protocols are\nessential to safeguard against potential jamming, spoofing,\nand eavesdropping attacks. By incorporating advanced\nsecurity measures into UAV communication networks,\nresearchers aim to establish reliable and resilient systems\nthat can withstand potential security threats. The challenge\nlies in striking a balance between security measures and the\nperformance and efficiency of UAV communication.\nResearchers are actively working to develop lightweight and\nscalable security solutions that do not compromise the real\u0002time communication demands of UAV applications.\nMeanwhile, they also focused on addressing the unique\nsecurity challenges posed by UAVs, such as aerial attacks\nand privacy invasion [6].\n4) AI TECHNIQUES FOR UAVS\nIn the upcoming decade, the utilization of AI methods in\nUAV communication systems is set to expand significantly.\nResearchers will exploit artificial neural networks, deep\nlearning (DL), and ML algorithms to enhance the\noptimization of UAV communication networks, as these\nmethodologies have demonstrated notable advantages across\nvarious applications. The research community still needs to\nfurther explore the development of an acceptable AI\ntechnique for the UAV communication system, which is a\nmulti-dimensional network that is more sophisticated than\ncurrent terrestrial communication networks. Meanwhile, AI\nhas shown great potential in addressing complex challenges\nfaced by UAVs, such as path planning, resource allocation,\nand interference management.\nOne of the primary applications of ML in UAV\ncommunication is predictive modeling for optimal path\nplanning [210]. By analyzing historical data, environmental\nconditions, and user requirements, ML algorithms can\npredict the most efficient and safe routes for UAVs. These\npredictive models enable UAVs to autonomously plan their\ntrajectories to optimize communication coverage and reduce\nflight time. In addition to path planning, ML techniques are\nalso utilized for dynamic resource allocation. By\ncontinuously monitoring network conditions and demand\npatterns, ML algorithms can optimize resource allocation,\nensuring efficient utilization of available bandwidth and\npower resources. Furthermore, AI-driven interference\nmanagement is a key focus area to enhance the reliability and\nperformance of UAV communication networks. AI\nalgorithms can intelligently analyze interference patterns and\nadapt communication parameters in real-time to mitigate\ninterference and improve overall network efficiency.\nThe successful implementation of AI in UAV\ncommunication and networking requires the availability of\nhigh-quality training data and the development of robust\nlearning models. Moreover, the integration of AI techniques\ninto UAV communication systems must be complemented\nby stringent security and privacy measures to safeguard\nsensitive data and prevent potential malicious attacks.\n5) BLOCKCHAIN IN UAV COMMUNICATIONS\nBlockchain is an important ongoing research trend driving\nthe evolution of 5G-and-beyond networks with UAVs. As a\ndistributed ledger technology, Blockchain offers secure and\ntransparent transactions, enabling trust and accountability in\nUAV communication and data management. It provides a\ndecentralized and tamper-resistant platform for sharing\nUAV-generated data among multiple stakeholders, ensuring\ndata integrity and authenticity in collaborative UAV\nmissions. Moreover, Blockchain enables the creation and\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n30\nmanagement of decentralized digital identities for UAVs,\nenhancing security through decentralized access\nmanagement. Smart contracts, programmable self-executing\ncontracts on the Blockchain, automate various aspects of\nUAV missions and enable autonomous decision-making\namong UAVs. Blockchain's support for cryptocurrency and\nmicrotransactions facilitates secure and real-time\ntransactions among UAVs, promoting resource exchange\nand service collaboration without traditional intermediaries\n[211]. Additionally, Blockchain-based consensus\nmechanisms can be applied to UAV traffic management,\nenabling decentralized coordination and conflict resolution\namong multiple UAVs in congested airspace, thereby\nimproving overall traffic efficiency and safety. Furthermore,\nBlockchain ensures data integrity and traceability, making\nrecorded data tamper-proof and auditable, vital for\napplications like UAV-based critical infrastructure\ninspections.\nWhile the integration of Blockchain in UAV\ncommunications offers numerous benefits, there are\nchallenges to address, including scalability, energy\nefficiency, and regulatory considerations. Researchers are\nactively exploring innovative approaches to optimize\nBlockchain solutions for UAV communications, unlocking\ntheir full potential in 5G-and-beyond UAV networks.\n6) SPECTRUM MANAGEMENT\nEfficient spectrum management stands as a paramount\nresearch trend in the context of 5G-and-beyond networks\nwith UAVs. With the increasing proliferation of UAVs in\ndiverse applications, ranging from aerial surveillance and\nmonitoring to disaster response and communications,\neffective spectrum allocation and utilization become\nimperative for ensuring seamless and reliable UAV\ncommunication [212]. Spectrum sharing techniques lie at the\ncore of spectrum management research. These techniques\nare designed to enable UAVs to efficiently share spectrum\nresources with existing communication systems, fostering\ncoexistence without causing harmful interference. The\ndevelopment of intelligent spectrum sharing mechanisms\nensures that UAVs can dynamically access available\nspectrum bands based on real-time requirements, optimizing\ncommunication performance while efficiently utilizing the\navailable resources. Dynamic spectrum access is a key focus\nwithin the spectrum management domain. By implementing\ndynamic access mechanisms, UAVs can opportunistically\nutilize underutilized spectrum bands, leveraging temporal\nand spatial variations in spectrum availability. Dynamic\nspectrum access empowers UAVs to adapt swiftly to\nchanging environmental conditions and communication\ndemands, resulting in enhanced communication reliability\nand throughput [213]. For instance, cognitive radio\ntechniques are being explored to enable UAVs to\ndynamically access and utilize underutilized spectrum bands.\nCognitive UAV communication enhances spectral efficiency\nby opportunistically accessing available frequency bands\nwhile avoiding interference with primary users [214].\nInterference management represents a critical aspect of\nspectrum management for UAVs. As UAVs often operate in\nhighly congested and dynamic environments, interference\nfrom neighboring communication systems can significantly\nimpact communication performance. Research efforts\nconcentrate on devising innovative interference mitigation\ntechniques that allow UAVs to coexist harmoniously with\nother wireless networks. Advanced interference avoidance\nalgorithms, intelligent beamforming strategies, and\ncooperative spectrum sensing mechanisms are among the\nkey solutions being explored to mitigate unwanted\ninterference. By effectively managing spectrum resources\nand addressing interference challenges, the spectrum\nmanagement research trend seeks to optimize spectral\nefficiency for UAV communication. This optimization not\nonly ensures the reliability and quality of UAV\ncommunication links but also enhances the overall\nperformance and capacity of UAV networks.\nB. OPEN CHALLENGES\nEvery system containing complex elements brings different\nchallenges to be solved. Since connected UAV schemes form\na complex system with elements of the transmission medium,\nthe complexity is greater than conventional networks. Also,\nthere are many different approaches to creating a wireless\nnetwork with UAVs as presented up to this point. In order to\nanalyze these approaches correctly, a correct relationship\nmust be established between UAVs and conventional\nwireless communication networks. In this way, difficulties\ncan be identified more easily. These difficulties will also\ndetermine future research trends. This section discusses the\nmain challenges in wireless communication with connected\nUAVs and research on these issues. These challenges are\npresented under the following subsections: collision\navoidance, control latency, limited energy, privacy problems,\nmobility management, channel modeling, UAV antenna\nconfigurations, interference management, GPS architecture\nfor system redundancy, and cyber-physical security.\n1) COLLISION AVOIDANCE\nIn multiple UAV scenarios, collision avoidance is also\nanother consideration for the system designs. This issue does\nnot cover only collision between different UAVs but also\ncollision between UAVs and other obstacles in the\nenvironment. A comprehensive review paper that focused on\ncomparison and analysis of different collision avoidance\nalgorithms available in the literature is provided by authors in\n[215]. Several research papers representing different\napproaches to this issue are described in this subsection. One\nof the interesting scenarios for collision avoidance algorithms\nis studied by [216]. They adapted an algorithm that considers\nsome channel restrictions, position data, and video\nparameters for first person view UAV applications. They\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n31\nclaimed that their algorithms made the probability of collision\n58.63% better than previous studies in this matter. A recent\nstudy that solves the avoidance problem by reinforcement\nlearning is presented in [217]. The study provided collision\navoidance without any previous data about other UAVs\u2019\ntrajectories in the network. They solved the optimization\nproblems for their given approach and proposed that the\nresults are promising compared to other studies. Authors in\n[218] provided a collision avoidance approach that considers\nthe quality of coverage for the UAVs in surveillance\nmissions. They developed a new coverage model by proving\nits convergence and providing its simulation results.\n2) CONTROL LATENCY\nControl latency is one of the crucial topics in connected\nUAVs. Control latency which is provided by varying cellular\nsystems using UAVs will be better thanks to technologies\nsuch as 5G. Advantages of 5G for improving control latency\nand additional benefits over 4G are discussed in [20]. Results\nare presented according to the 20 MHz carrier bandwidth and\n2.6 GHz carrier frequency using LTE-Advance Network.\nBesides; 50 m, 100 m and 300 m are selected as different\naltitudes for UAVs. Results related to these altitudes are\nillustrated for varying latency data samples. Details about\ncontrol latency and a more comprehensive literature review\ncan be found in [20].\n3) LIMITED ENERGY\nIn a network which consists of UAVs, energy is one of the\nmost significant challenges which limits performance. Since\nUAVs have limited energy, all scenarios related to UAVs\nmust be designed by considering energy issues. It requires\nmuch different research that must be done, such as battery\ndesign and charging optimization. In [219], a cloud-based\nUAV navigation system is explained to obtain a more\nefficient battery charging scenario for networks which use\nUAVs as relays. Since lack of coordination between UAVs\nduring battery charging may cause a blockage on the network,\nthey propose a solution by using globally coordinated routes\n[219]. In another research, an auction-based multiple UAV\ncharging plan is proposed to increase the performance of\nprocurement of energy on time [220]. Also, problem of how\nto place the charging station for UAVs is discussed in [221].\nThey propose a deployment solution for placement of UAVs\nand this system recursively changes the configuration of\ncharging stations with respect to usage data. All the provided\narticles show that charging optimization is quite popular in\nthe literature. It is not mentioned in this paper, but there are\nmany articles related to battery design, and it is another\ncrucial branch of this issue that requires much further\nresearch.\n4) PRIVACY PROBLEMS\nAs with every wireless communication system, privacy is one\nof the key aspects. UAVs were not designed to consider\nsecurity problems originally. Thus, when they are used in\ncommunication networks, privacy issues should be taken into\nconsideration. Security challenges in connected UAVs are\nsorted in detail in [222]. It highlights that UAV networks are\nquite different than classical wireless networks. Less power\nrequirement and carrying of less information are sorted as\nexamples for differences between classical networks than\nUAV networks [222]. Requirements of privacy and security\nfor wireless UAV networks and their architecture are studied\nin [223]. They also propose prospective approaches for\nproblems such as flexibility, protection, and leakage. Their\nsolutions cover location protection and privacy protection.\nMore detail related to these solutions can be found in [223].\n5) MOBILITY MANAGEMENT\nTo fully take advantage of UAV installation, further visual\nLoS processes is of critical significance as UAVs act as air\nusers, which maintain connection with the TBSs for direction\nand control objectives in the downlink [224]. Typically, BS\nantennas sidelobes may serve UAVs while flying in the sky\nwhich deliver lower antenna gains [26, 224]. This will create\nsignificant challenges to the mobility management of\ncellular-connected UAVs in terms of reference signal\nreceived power (RSRP). The highest RSRP will be\nmaintained when the distance is far between TBSs and UAV.\nFigure 13 illustrates the cellular-connected UAV when it is\nflying over a rural area. This scenario of irregular signal\ncoverage of TBSs will lead to reduced mobility performance\nsuch as radio link failure, handover failure, in addition to\nuseless handovers, named ping-pong events [225].\nRegardless of that, due to the loss of direction and control of\nsignal, the UAV may hit a civil aircraft or even collide into a\npopulated zone which might cause dangerous events.\nTherefore, functional mobility management to deliver\nauthoritative connections between UAVs and TBSs is of\ncritical significance. In [225, 226] provided more details on\nthe mobility management, particularly handover procedures,\nfor connected UAVs within forthcoming mobile networks,\nencompassing technologies such as 5G, 6G networks.\n6) CHANNEL MODELING\nOne of the main issues in UAV communication is channel\nmodelling. Similar to other wireless communication\napplications, determining aspects of wireless communication\nchannels in connected UAVs is critical to be able to design an\noptimal system in order to provide reliability. Unlike other\ncommonly known fading channels, there are more parameters\nthat characterize UAV communication channels. For example,\nsince a UAV may act as a component during connection, it\nshows some Rician fading channel characteristics. However,\nLoS components can disappear in certain time slots. Then, it\ncan be said that channel turns into a Rayleigh fading channel.\nThese examples can be increased. To model UAV\ncommunication, different approaches and formulations are\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n32\nFIGURE 13. HO scenario of a cellular-connected UAV moving towards horizontal direction. (a) UAV is associated with TBS2 due to its higher side-lobe\ngain than TBS1. (b) After moving forward, the UAV is now associated with TBS1 due to its higher sidelobe gain.\npresented in the literature. These approaches and formulations\ninclude parameters such as path loss exponents, shadowing\neffects, etc. In addition to these parameters, there are some\ndifferent channel definitions for A2G and A2A. A detailed\nreview of these differences is presented in [3]. There are other\npossible methods for characterizing UAV channels. One of\nthem is obtaining empirical results. Due to the complexity of\ntheoretical derivations in UAV communications, empirical\nresults are more supportive than other classical wireless\ncommunication studies for determining the future directions\nof channel modelling. In practical channel modelling studies,\nthere are many different setups and considerations. As an\nexample, a very effective method is proposed in [23].\nIn [227], the authors investigated continuous phase\nmodulated signal transmission in UAV communication\nnetworks with doubly-selective channels. It proposed a two\u0002stage receiver design that includes a linear time-varying\nequalizer and a recursive symbol recovery process from\npseudo-symbols in the Laurent representation.\nA2A channel modeling describes the wireless\ncommunication channels between UAVs when they are in\nflight. Several works have been done in A2A channel\nmodeling such as [228-230]. The researchers in [228] employed\nthe ray tracing method to conduct path gain analyses based on\nthe distance between two UAVs in distinct scenarios. The\nstudy explores various antenna types employed in A2A\ncommunication channels when one UAV functions as a\ntransmitter and the other as a receiver, assuming a LoS\nconnection between them. In [229], a 3D geometry-based\nstochastic channel model is introduced to account for unique\nA2A channel characteristics, including arbitrary 3D mobility\nand time-domain non-stationarity. They focused on deriving\nand examining critical channel attributes such as the root mean\nsquare delay spread, space-time correlation function,\nstationary interval and Doppler power spectrum density, as per\nthe theoretical framework. The authors in [230] presented the\nmeasurement of an A2A channel at a frequency of 1420 MHz\nwithin an urban environment. This study analysed the delay\nand power characteristics of multipath components using the\ngathered measurement data. Additionally, the study calibrated\na ray-tracing simulator to extend its applicability to various\nUAV scenarios.\nA2G channel modeling in UAV communication describes\nthe wireless communication channel characteristics between a\nUAV and a ground station or terrestrial device. In the\nresearchers in [231] measured A2G signals from UAVs and\ndeveloped a channel model incorporating a two-ray ground\u0002reflection effect for the UAV communication system. This\nchannel modelling approach offers a suitable path loss model\nfor assessing the quality of service in A2G wireless\ncommunication. Several channel measurement campaigns\nwere conducted within the airport vicinity, covering the UHF\nand L-band frequencies (approximately 433 MHz and 1.518\nGHz) [232]. In cases where there was a clear LoS, distance\u0002dependent path loss models were developed for both\nfrequency bands. In [233], the authors investigated A2G\nchannel modeling and transmission performance in a cellular\u0002connected M-MIMO UAV swarm system. They introduced a\ncorrelated A2G channel model that considers factors like non\u0002isotropic scattering, LoS propagation, and mobile scatterers.\nAdditionally, a novel analytical expression for uplink signal\u0002to-interference-and-noise ratio is derived, accounting for\nchannel aging and strong LoS effects. A 3-D elliptic-cylinder\nMIMO channel model for UAV communication in A2G\nscenarios is proposed in [234]. This study focused on the\nmobility and altitude of UAV transmitters in the elevation\nplane, utilizing the newly proposed UAV-MIMO channel\nmodel as a basis. The authors in [235] performed cluster-based\ncharacterization and modeling of A2G channels. This study\nrepresented the first of its kind to focus on the clustering and\ntracking of multipath components within dynamic A2G\nchannels\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n33\n7) UAV ANTENNA CONFIGURATIONS\nLike in all wireless communication applications, there are\nmany antennas configuration approaches for UAV\ncommunications. In addition to previous antenna knowledge\nfor wireless communications, some issues have to be taken\ninto consideration for UAV communication. For example,\nnoise which is caused by the UAV itself changes previous\napproaches. Moreover, the specific characteristics of aerial\ncommunications can be counted as another example.\nAccording to [24], optimal tilting of the UAV antenna\nenhanced the throughput from 3.5 to 5.8 b/s/Hz and the\ncoverage from 23% to 89%. And they also declare that their\nconfiguration increases performance of microcells limits and\nnetwork density performance. In [25], a new reconfigurable\nmicrostrip antenna including adjustable conical beams is\nproposed for UAV communications. That antenna covers a\nbandwidth from 2.39 to 2.49 GHz which is also so suitable for\n2.4 GHz applications. Beamforming is another hot topic in\nantenna designs and performance of beamforming with down\u0002tilted antennas for UAV communications applications is\npresented in [26].\n8) INTERFERENCE MANAGEMENT\nInterference is one of the inevitable problems with wireless\ncommunications systems, and it requires specific solutions to\nhandle it. Some possible solutions for interference problems\nmentioned by standardization institutions for UAV\ncommunications users are surveyed in [6]. As a novel\napproach, [28] it proposes an interference-aware path planning\nstructure for a UAV communication network. Their main\ncontribution is for solving the tradeoff problem between\nminimizing both the interference caused on the ground\nnetwork along its route and maximizing the energy efficiency.\nThey propose a deep reinforcement method based on the echo\nstate network (ESN). In their system, each UAV utilizes the\nESN to identify the optimal transmission power, path, and cell\nassociation vector at varying positions along its route. Also,\nthe proposed method is supported by simulation results. The\ninterference management problem in UAV communications\ncontains many possible approaches in itself thanks to ML\ntechniques for further studies.\n9) GPS ARCHITECTURE FOR SYSTEM REDUNDANCY\nThere are many applications for determining the position of\nUAVs exactly about Real Time Kinematic GPS (RTK-GPS).\nNevertheless, these applications are found to be instable for\nnetworks and cause some errors in services. Thus, GPS\narchitecture is another problem that has to be solved for better\nsystems. An effective architecture for improving system\nperformance is proposed in [29] by parallelizing or switching\nthe GPS data resources. They also discuss how to allocate\nmessages through the UAV network. Evaluation on empirical\ntestbeds and validation results are also available. Figure 14\nshows an example propagation system model.\n10) CYBER-PHYSICAL SECURITY\nIt is unarguable that security is so critical to any\ncommunication system. It can be considered as a more crucial\nproblem for UAV communication applications as UAVs\nrequire remote control by nature. When UAVs become an\ninevitable part of cellular systems, security becomes more\nimportant for providing coherence in networks. There is a\ncomprehensive background about security in UAV\ncommunication are introduced in [6]. Cyber-attacks on UAV\nsystems are increasing day by day. Thus, techniques should be\nFIGURE 14. Radio technical commission maritime services propagation system\ndesign.\ndeveloped continuously. Examples of these threats can be\ncounted as jamming, hijacking, eavesdropping, denial of\nservice, and spoofing. In addition to cyber-attacks, there may\nalso be physical attacks. These attacks are detailed in [6]. As a\nspecific example, a homomorphic cryptography system is\nproposed in [30] to provide the security of controllers. They\nalso proposed a linearly homomorphic authenticated\nencryption (LinHAE) architecture to make real-time\noperations safer for autonomous flight.\nA comprehensive overview of PLS within the context of\nUAV systems is provided in [228], with an examination of\nvarious communication channels, including ground-to\u0002ground, ground-to-air, A2G, and A2A [236]. PLS metrics,\nsuch as secrecy outage probability, average secrecy capacity,\nand the probability of strictly positive secrecy capacity, are\nstudied in UAV-to-ground communications, considering the\npresence of shadowing, as discussed in [237]. Also, an\noverview of the improvement of the PLS of UAV networks by\nthe IRS is presented in [238]. Various use cases of PLS for\nenhancing UAV communications by the IRS are examined,\nand recent advancements in this field are briefly summarized.\nNonetheless, a significant limitation of IRS lies in its\nconfinement of communication to the reflective dimension,\nthereby rendering it inaccessible to users positioned behind the\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n34\nIRS surface. In [239], the authors proposed the utilization of\nintelligent omni-surfaces as an alternative to IRS, alongside\nthe deployment of UAVs, to attain secure communication\nwithin an IoT communication system.\nAn emergent self-Awareness module is suggested to be\nincorporated into the physical layer of cognitive UAV radios\nfor the enhancement of PLS, particularly in the context of\ncountering jamming attacks. Self-Awareness is founded on the\nacquisition of a hierarchical representation of the radio\nenvironment through the utilization of a proposed hierarchical\ndynamic Bayesian network. DL-empowered punctured low\ndensity parity check codes are introduced in [240] for the\npurpose of ensuring secure and dependable data transmission\nfor UAVs over the additive white gaussian noise channel,\nindependent of the computational capabilities and channel\nstate information (CSI) of the Eavesdropper. Similarly, the\nauthors in [241] conducted the analysis of PLS for a dual-hop\nwireless network based on UAV, taking into account\nimperfect CSI and the influence of mobility effects.\nThe PLS of UAV-based communication is enhanced\nthrough the implementation of NOMA techniques. In [242],\nthe authors examined the secrecy performance of a full-duplex\nrelay NOMA system utilizing UAVs over the Nakagami-m\nfading channel. The insertion of an artificial noise component\ninto the transmit signal of the full-duplex aerial relay station,\nwith power allocation based on the NOMA protocol, is\nsuggested for the purpose of ensuring secure communication\nfor users. Besides, the secrecy outage probability in the\ncontext of in terrestrial networks aided by UAVs is\ninvestigated in [243], utilizing cooperative user selection. In\nthis system, the user is chosen from those directly connected\nto the UAV. With a particular emphasis on achieving optimal\nresource allocation, the research in [244] delved into\noptimizing PLS for UAV communication by employing\nNOMA. It studied the feasibility of pairing users with\ntrustworthiness disparities and the influence of optimal power\nallocation coefficients.\nVIII. CONCLUSION\nIn this paper, a comprehensive survey of the deployment\nscenarios, applications, emerging technologies, regulatory\naspects, research trends, and challenges associated with the\nuse of UAVs in 5G-and-beyond networks has been presented.\nThe paper begins with a brief background on UAVs and 5G\nnetworks, followed by a systematic classification of UAVs\nand a review of relevant works. Various UAV deployment\nscenarios, including single and multiple UAV configurations,\nare then discussed. UAV applications in 5G are categorized,\nand emerging technologies for enhancing UAV\ncommunications are investigated. Additionally, regulatory\nconsiderations, such as flight guidelines, spectrum allocation,\nprivacy, and safety, are addressed in the context of deploying\nUAVs in 5G networks. The latest research trends and open\nchallenges in the field are highlighted, and promising\ndirections for future investigations are identified. This survey\nis intended to serve as a valuable resource for researchers,\npractitioners, and policymakers in the UAV and\ncommunication domains, providing a comprehensive\noverview of the state-of-the-art in UAV-enabled 5G networks\nand outlining the key challenges and research directions\nessential for realizing the full potential of this technology. The\nhope is that this survey will inspire further research in this\nexciting area and accelerate the development of UAV-enabled\n5G networks.\nABBREVIATIONS LIST\nTerm Description\n3GPP 3rd generation partnership project\n5G fifth generation\n6G sixth generation\nA2A air-to-air\nA2G air-to-ground\nABS aerial base stations\nAI artificial intelligence\nAmBC ambient backscattering communications\nBS base stations\nBVLOS beyond visual line of sight\nCSI channel state information\nCURS closest UAV Relay Selection\nD2D device-to-device\nDL deep learning\nDNN deep neural network\nDRL deep reinforcement learning\neMBB enhanced mobile broadband\nESN echo state network\nFAA federal aviation authority\nFANETs flying ad-hoc networks\nFCC federal communications commission\nFCSD Fog computing-aided swarm of drones\nFDMA frequency division multiple access\nHetNets heterogeneous networks\nHVOR highest velocity opportunistic routing\nIoD Internet of drones\nIoT Internet of things\nIRS intelligent reflecting surfaces\nITC interference transmission and cancellation\nLoS line-of-sight\nLTE long term evolution\nMEC mobile edge computing\nMIMO multiple input multiple output\nML machine learning\nM-MIMO massive multiple input multiple output\nmMTC massive machine-type communication\nmmWave millimeter wave\nMURS maximum UAV relay selection\nNASA national aeronautics and space administration\nNOMA non-orthogonal multiple access\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n35\nNR new radio\nOMA orthogonal multiple access\nOR opportunistic routing\nPLS physical layer security\nQoS quality of service\nRL reinforcement learning\nRSRP reference signal received power\nRF radio frequency\nSAA sense-and-avoid\nSBSs small base stations\nSNR signal-to-noise ratio\nSWIPT simultaneous wireless information and power transfer\nSR symbiotic radio\nSCAS symbiotic communication and sensing\nTBSs terrestrial base stations\nTDMA time division multiple access\nUASs unmanned aerial systems\nUAV unmanned aerial vehicles\nUE user equipment\nURS UAV relay selection\nV2V vehicle-to-vehicle\nWPCNs wireless powered communication networks\nREFERENCES\n[1] M. Banafaa, M. \u00d6zg\u00fcm\u00fc\u015f, R. Ekin, I. Shayea, and A.\nAlhammadi, \"Connected Drone in Future Mobile Networks,\" in\n2022 Workshop on Microwave Theory and Techniques in\nWireless Communications (MTTW), 2022, pp. 183-188: IEEE.\n[2] I. Shayea et al., \"Handover Management for Drones in Future\nMobile Networks\u2014A Survey,\" vol. 22, no. 17, p. 6424, 2022.\n[3] M. Mozaffari, W. Saad, M. Bennis, Y. Nam, and M. Debbah, \"A\nTutorial on UAVs for Wireless Networks: Applications,\nChallenges, and Open Problems,\" IEEE Communications\nSurveys & Tutorials, vol. 21, no. 3, pp. 2334-2360, 2019.\n[4] S. A. H. Mohsan, M. A. Khan, F. Noor, I. Ullah, and M. H.\nAlsharif, \"Towards the unmanned aerial vehicles (UAVs): A\ncomprehensive review,\" Drones, vol. 6, no. 6, p. 147, 2022.\n[5] P. S. Bithas, E. T. Michailidis, N. Nomikos, D. Vouyioukas, and\nA. G. J. S. Kanatas, \"A survey on machine-learning techniques\nfor UAV-based communications,\" vol. 19, no. 23, p. 5170, 2019.\n[6] A. Fotouhi et al., \"Survey on UAV Cellular Communications:\nPractical Aspects, Standardization Advancements, Regulation,\nand Security Challenges,\" IEEE Communications Surveys &\nTutorials, vol. 21, no. 4, pp. 3417-3442, 2019.\n[7] J. Wang et al., \"Physical layer security for UAV\ncommunications: A comprehensive survey,\" China\nCommunications, vol. 19, no. 9, pp. 77-115, 2022.\n[8] S. I. J. I. Han, \"Survey on UAV deployment and trajectory in\nwireless communication networks: applications and challenges,\"\nvol. 13, no. 8, p. 389, 2022.\n[9] A. Sharma et al., \"Communication and networking technologies\nfor UAVs: A survey,\" vol. 168, p. 102739, 2020.\n[10] A. I. Hentati, L. C. J. C. S. Fourati, and Interfaces,\n\"Comprehensive survey of UAVs communication networks,\"\nvol. 72, p. 103451, 2020.\n[11] Z. Xiao et al., \"A survey on millimeter-wave beamforming\nenabled UAV communications and networking,\" IEEE\nCommunications Surveys & Tutorials, vol. 24, no. 1, pp. 557-\n610, 2021.\n[12] G. Geraci et al., \"What will the future of UAV cellular\ncommunications be? A flight from 5G to 6G,\" IEEE\ncommunications surveys & tutorials, vol. 24, no. 3, pp. 1304-\n1335, 2022.\n[13] P. McEnroe, S. Wang, and M. Liyanage, \"A survey on the\nconvergence of edge computing and AI for UAVs: Opportunities\nand challenges,\" IEEE Internet of Things Journal, vol. 9, no. 17,\npp. 15435-15459, 2022.\n[14] N. Nomikos, P. K. Gkonis, P. S. Bithas, and P. Trakadas, \"A\nsurvey on UAV-aided maritime communications: Deployment\nconsiderations, applications, and future challenges,\" IEEE Open\nJournal of the Communications Society, vol. 4, pp. 56-78, 2022.\n[15] M. A. Jasim, H. Shakhatreh, N. Siasi, A. H. Sawalmeh, A.\nAldalbahi, and A. Al-Fuqaha, \"A survey on spectrum\nmanagement for unmanned aerial vehicles (uavs),\" IEEE Access,\nvol. 10, pp. 11443-11499, 2021.\n[16] C. Xu, X. Liao, J. Tan, H. Ye, and H. Lu, \"Recent research\nprogress of unmanned aerial vehicle regulation policies and\ntechnologies in urban low altitude,\" IEEE Access, vol. 8, pp.\n74175-74194, 2020.\n[17] S. Hafeez et al., \"Blockchain-Assisted UAV Communication\nSystems: A Comprehensive Survey,\" IEEE Open Journal of\nVehicular Technology, 2023.\n[18] Z. Wei et al., \"UAV-assisted data collection for internet of\nthings: A survey,\" IEEE Internet of Things Journal, vol. 9, no.\n17, pp. 15460-15483, 2022.\n[19] T. Q. Duong, K. J. Kim, Z. Kaleem, M.-P. Bui, and N.-S. Vo,\n\"UAV caching in 6G networks: A Survey on models, techniques,\nand applications,\" Physical Communication, vol. 51, p. 101532,\n2022.\n[20] H. Ullah, N. G. Nair, A. Moore, C. Nugent, P. Muschamp, and\nM. Cuevas, \"5G Communication: An Overview of Vehicle-to\u0002Everything, Drones, and Healthcare Use-Cases,\" IEEE Access,\nvol. 7, pp. 37251-37268, 2019.\n[21] X. Hou, Z. Ren, J. Wang, S. Zheng, W. Cheng, and H. Zhang,\n\"Distributed Fog Computing for Latency and Reliability\nGuaranteed Swarm of Drones,\" IEEE Access, vol. 8, pp. 7117-\n7130, 2020.\n[22] D. Wu et al., \"ADDSEN: Adaptive Data Processing and\nDissemination for Drone Swarms in Urban Sensing,\" IEEE\nTransactions on Computers, vol. 66, no. 2, pp. 183-198, 2017.\n[23] P. A. Catherwood, B. Black, E. B. Mohamed, A. A. Cheema, J.\nRafferty, and J. A. D. Mclaughlin, \"Radio Channel\nCharacterization of Mid-Band 5G Service Delivery for Ultra\u0002Low Altitude Aerial Base Stations,\" IEEE Access, vol. 7, pp.\n8283-8299, 2019.\n[24] M. M. Azari, F. Rosas, and S. Pollin, \"Cellular Connectivity for\nUAVs: Network Modeling, Performance Analysis, and Design\nGuidelines,\" IEEE Transactions on Wireless Communications,\nvol. 18, no. 7, pp. 3366-3381, 2019.\n[25] Z. Liang, Y. Li, J. Liu, J. Qin, and Y. Long, \"Reconfigurable\nMicrostrip Magnetic Dipole Antenna With Switchable Conical\nBeams for Aerial Drone Applications,\" IEEE Access, vol. 7, pp.\n31043-31054, 2019.\n[26] R. Amer, W. Saad, and N. Marchetti, \"Toward a Connected Sky:\nPerformance of Beamforming With Down-Tilted Antennas for\nGround and UAV User Co-Existence,\" IEEE Communications\nLetters, vol. 23, no. 10, pp. 1840-1844, 2019.\n[27] M. K. Banafaa, M. H. Jamaluddin, S. H. Dahlan, and A. A.\nAlthuwayb, \"Miniature Dual Band Button Antenna Using\nCylindrical Dielectric Resonator Antenna for On/Off Body\nCommunication Devices,\" The Applied Computational\nElectromagnetics Society Journal (ACES), pp. 479-485, 2021.\n[28] U. Challita, W. Saad, and C. Bettstetter, \"Interference\nManagement for Cellular-Connected UAVs: A Deep\nReinforcement Learning Approach,\" IEEE Transactions on\nWireless Communications, vol. 18, no. 4, pp. 2125-2140, 2019.\n[29] I. Um, S. Park, H. T. Kim, and H. Kim, \"Configuring RTK-GPS\nArchitecture for System Redundancy in Multi-Drone\nOperations,\" IEEE Access, vol. 8, pp. 76228-76242, 2020.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n36\n[30] J. H. Cheon et al., \"Toward a Secure Drone System: Flying With\nReal-Time Homomorphic Authenticated Encryption,\" IEEE\nAccess, vol. 6, pp. 24325-24339, 2018.\n[31] A. Garcia-Rodriguez, G. Geraci, D. Lopez-Perez, L. G.\nGiordano, M. Ding, and E. Bjornson, \"The Essential Guide to\nRealizing 5G-Connected UAVs with Massive MIMO,\" IEEE\nCommunications Magazine, vol. 57, no. 12, pp. 84-90, 2019.\n[32] Y. Zeng, J. Lyu, and R. Zhang, \"Cellular-Connected UAV:\nPotential, Challenges, and Promising Technologies,\" IEEE\nWireless Communications, vol. 26, no. 1, pp. 120-127, 2019.\n[33] X. Lin et al., \"Mobile Network-Connected Drones: Field Trials,\nSimulations, and Design Insights,\" IEEE Vehicular Technology\nMagazine, vol. 14, no. 3, pp. 115-125, 2019.\n[34] R. Amer, W. Saad, and N. Marchetti, \"Mobility in the Sky:\nPerformance and Mobility Analysis for Cellular-Connected\nUAVs,\" IEEE Transactions on Communications, vol. 68, no. 5,\npp. 3229-3246, 2020.\n[35] W. Shi, H. Zhou, J. Li, W. Xu, N. Zhang, and X. Shen, \"Drone\nAssisted Vehicular Networks: Architecture, Challenges and\nOpportunities,\" IEEE Network, vol. 32, no. 3, pp. 130-137, 2018.\n[36] M. Mozaffari, A. T. Z. Kasgari, W. Saad, M. Bennis, and M.\nDebbah, \"Beyond 5G With UAVs: Foundations of a 3D Wireless\nCellular Network,\" IEEE Transactions on Wireless\nCommunications, vol. 18, no. 1, pp. 357-372, 2019.\n[37] Y. Zeng, S. Jin, Q. Wu, and F. Gao, \"Network-connected UAV\ncommunications,\" China Communications, vol. 15, no. 5, pp. iii\u0002v, 2018.\n[38] J. Cui, H. Shakhatreh, B. Hu, S. Chen, and C. Wang, \"Power\u0002Efficient Deployment of a UAV for Emergency Indoor Wireless\nCoverage,\" IEEE Access, vol. 6, pp. 73200-73209, 2018-01-01\n2018.\n[39] X. Wang and L. Duan, \"Economic Analysis of Unmanned Aerial\nVehicle (UAV) Provided Mobile Services,\" IEEE Transactions\non Mobile Computing, vol. 20, no. 5, pp. 1804-1816, 2021-05-\n01 2021.\n[40] S. Yin, L. Li, and F. R. Yu, \"Resource Allocation and\nBasestation Placement in Downlink Cellular Networks Assisted\nby Multiple Wireless Powered UAVs,\" IEEE Transactions on\nVehicular Technology, vol. 69, no. 2, pp. 2171-2184, 2020-02-\n01 2020.\n[41] S. Chai and V. K. N. Lau, \"Multi-UAV Trajectory and Power\nOptimization for Cached UAV Wireless Networks With Energy\nand Content Recharging-Demand Driven Deep Learning\nApproach,\" IEEE Journal on Selected Areas in\nCommunications, vol. 39, no. 10, pp. 3208-3224, 2021-10-01\n2021.\n[42] X. Zhu, C. Bian, Y. Chen, and S. Chen, \"A Low Latency\nClustering Method for Large-Scale Drone Swarms,\" IEEE\nAccess, vol. 7, pp. 186260-186267, 2019-01-01 2019.\n[43] F. Aftab, A. Khan, and Z. Zhang, \"Hybrid Self-Organized\nClustering Scheme for Drone Based Cognitive Internet of\nThings,\" IEEE Access, vol. 7, pp. 56217-56227, 2019-01-01\n2019.\n[44] L. Merino, F. Caballero, J. Martinez-de Dios, and A. Ollero,\n\"Cooperative fire detection using unmanned aerial vehicles,\" in\nProceedings of the 2005 IEEE international conference on\nrobotics and automation, 2005, pp. 1884-1889: IEEE.\n[45] B. Olivieri and M. Endler, \"An algorithm for aerial data\ncollection from wireless sensors networks by groups of UAVs,\"\nin 2017 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), 2017, pp. 967-972: IEEE.\n[46] X. Ma, S. Chisiu, R. Kacimi, and R. Dhaou, \"Opportunistic\ncommunications in WSN using UAV,\" in 2017 14th IEEE\nAnnual Consumer Communications & Networking Conference\n(CCNC), 2017, pp. 510-515: IEEE.\n[47] A. Abdelmaboud, \"The internet of drones: Requirements,\ntaxonomy, recent advances, and challenges of research trends,\"\nSensors, vol. 21, no. 17, p. 5718, 2021.\n[48] N. Cheng et al., \"AI for UAV-Assisted IoT Applications: A\nComprehensive Review,\" IEEE Internet of Things Journal,\n2023.\n[49] M. Mozaffari, W. Saad, M. Bennis, and M. J. I. T. o. W. C.\nDebbah, \"Mobile unmanned aerial vehicles (UAVs) for energy\u0002efficient Internet of Things communications,\" vol. 16, no. 11, pp.\n7574-7589, 2017.\n[50] Z. Shah, M. Naeem, U. Javed, W. Ejaz, and M. Altaf, \"A\ncompendium of radio resource management in UAV-assisted\nnext generation computing paradigms,\" Ad Hoc Networks, vol.\n131, p. 102844, 2022.\n[51] A. Koub\u00e2a, B. Qureshi, M.-F. Sriti, Y. Javed, and E. Tovar, \"A\nservice-oriented Cloud-based management system for the\nInternet-of-Drones,\" in 2017 IEEE International Conference on\nAutonomous Robot Systems and Competitions (ICARSC), 2017,\npp. 329-335: IEEE.\n[52] B. Fei, W. Bao, X. Zhu, D. Liu, T. Men, and Z. Xiao,\n\"Autonomous cooperative search model for multi-UAV with\nlimited communication network,\" IEEE Internet of Things\nJournal, vol. 9, no. 19, pp. 19346-19361, 2022.\n[53] T. Hardes and C. Sommer, \"Opportunistic UAV Relaying for\nUrban Vehicular Networks,\" in 2022 17th Wireless On-Demand\nNetwork Systems and Services Conference (WONS), 2022, pp. 1-\n8: IEEE.\n[54] R. Ding, J. Chen, W. Wu, J. Liu, F. Gao, and X. Shen, \"Packet\nrouting in dynamic multi-hop UAV relay network: A multi-agent\nlearning approach,\" IEEE Transactions on Vehicular\nTechnology, vol. 71, no. 9, pp. 10059-10072, 2022.\n[55] C. Barroca, A. Grilo, and P. R. Pereira, \"Improving message\ndelivery in UAV-based delay tolerant networks,\" in 2018 16th\nInternational Conference on Intelligent Transportation Systems\nTelecommunications (ITST), 2018, pp. 1-7: IEEE.\n[56] T. Spyropoulos, K. Psounis, and C. S. Raghavendra, \"Spray and\nwait: an efficient routing scheme for intermittently connected\nmobile networks,\" in Proceedings of the 2005 ACM SIGCOMM\nworkshop on Delay-tolerant networking, 2005, pp. 252-259.\n[57] I. A. Elnabty, Y. Fahmy, and M. Kafafy, \"A survey on UAV\nplacement optimization for UAV-assisted communication in 5G\nand beyond networks,\" Physical Communication, vol. 51, p.\n101564, 2022.\n[58] O. Semiari, W. Saad, and M. J. I. T. o. W. C. Bennis, \"Joint\nmillimeter wave and microwave resources allocation in cellular\nnetworks with dual-mode base stations,\" vol. 16, no. 7, pp. 4802-\n4816, 2017.\n[59] R. Kunst, E. Pignaton, T. Zhou, and H. Hu, \"Application of\nfuture 6G technology to support heavy data traffic in highly\nmobile networks,\" in 2020 First International Conference of\nSmart Systems and Emerging Technologies (SMARTTECH),\n2020, pp. 144-148: IEEE.\n[60] G. Castellanos, G. Vallero, M. Deruyck, L. Martens, M. Meo,\nand W. Joseph, \"Evaluation of flying caching servers in UAV\u0002BS based realistic environment,\" Vehicular Communications,\nvol. 32, p. 100390, 2021.\n[61] H. Shen, Q. Ye, W. Zhuang, W. Shi, G. Bai, and G. Yang,\n\"Drone-Small-Cell-Assisted Resource Slicing for 5G Uplink\nRadio Access Networks,\" IEEE Transactions on Vehicular\nTechnology, vol. 70, no. 7, pp. 7071-7086, 2021-07-01 2021.\n[62] M. S. Haroon et al., \"Interference Management in Ultra-Dense\n5G Networks With Excessive Drone Usage,\" IEEE Access, vol.\n8, pp. 102155-102164, 2020-01-01 2020.\n[63] F. Al-Turjman, J. P. Lemayian, S. Alturjman, and L. Mostarda,\n\"Enhanced Deployment Strategy for the 5G Drone-BS Using\nArtificial Intelligence,\" IEEE Access, vol. 7, pp. 75999-76008,\n2019-01-01 2019.\n[64] S. Iranmanesh, F. S. Abkenar, R. Raad, and A. Jamalipour,\n\"Improving Throughput of 5G Cellular Networks via 3D\nPlacement Optimization of Logistics Drones,\" IEEE\nTransactions on Vehicular Technology, vol. 70, no. 2, pp. 1448-\n1460, 2021-02-01 2021.\n[65] N. Mehallegue, M. Djellab, and K. Loukhaoukha, \"Efficient use\nof UAVs for public safety in disaster and crisis management,\"\nWireless Personal Communications, vol. 116, no. 1, pp. 369-\n380, 2021.\n[66] S. K. Khan, U. Naseem, H. Siraj, I. Razzak, and M. Imran, \"The\nrole of unmanned aerial vehicles and mmWave in 5G: Recent\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n37\nadvances and challenges,\" Transactions on Emerging\nTelecommunications Technologies, vol. 32, no. 7, p. e4241,\n2021.\n[67] C. Zhang, W. Zhang, W. Wang, L. Yang, and W. Zhang,\n\"Research challenges and opportunities of UAV millimeter\u0002wave communications,\" IEEE Wireless Communications, vol.\n26, no. 1, pp. 58-62, 2019.\n[68] J. Sabzehali, V. K. Shah, H. S. Dhillon, and J. H. Reed, \"3D\nplacement and orientation of mmWave-based UAVs for\nguaranteed LoS coverage,\" IEEE Wireless Communications\nLetters, vol. 10, no. 8, pp. 1662-1666, 2021.\n[69] V. Begishev, D. Moltchanov, A. Gaidamaka, and K. Samouylov,\n\"Closed-Form UAV LoS Blockage Probability in Mixed\nGround-and Rooftop-Mounted Urban mmWave NR\nDeployments,\" Sensors, vol. 22, no. 3, p. 977, 2022.\n[70] M. Mozaffari, W. Saad, M. Bennis, and M. J. I. T. o. C. Debbah,\n\"Communications and control for wireless drone-based antenna\narray,\" vol. 67, no. 1, pp. 820-834, 2018.\n[71] S. D. Muruganathan et al., \"An overview of 3GPP release-15\nstudy on enhanced LTE support for connected drones,\" vol. 5,\nno. 4, pp. 140-146, 2021.\n[72] M. Whaiduzzaman et al., \"A review of emerging technologies\nfor IoT-based smart cities,\" Sensors, vol. 22, no. 23, p. 9271,\n2022.\n[73] N. Abbas, Z. Abbas, X. Liu, S. S. Khan, E. D. Foster, and S.\nLarkin, \"A Survey: Future Smart Cities Based on Advance\nControl of Unmanned Aerial Vehicles (UAVs),\" Applied\nSciences, vol. 13, no. 17, p. 9881, 2023.\n[74] Q. Zhu, J. Zheng, and A. Jamalipour, \"Coverage Performance\nAnalysis of a Cache-Enabled UAV Base Station Assisted\nCellular Network,\" IEEE Transactions on Wireless\nCommunications, 2023.\n[75] R. Amer, W. Saad, H. ElSawy, M. M. Butt, and N. Marchetti,\n\"Caching to the sky: Performance analysis of cache-assisted\nCoMP for cellular-connected UAVs,\" in 2019 IEEE Wireless\nCommunications and Networking Conference (WCNC), 2019,\npp. 1-6: IEEE.\n[76] E. Balestrieri, P. Daponte, L. De Vito, F. Picariello, and I.\nTudosa, \"Sensors and measurements for UAV safety: An\noverview,\" Sensors, vol. 21, no. 24, p. 8253, 2021.\n[77] P. H\u00fcgler, F. Roos, M. Schartel, M. Geiger, and C. J. I. M. M.\nWaldschmidt, \"Radar taking off: New capabilities for UAVs,\"\nvol. 19, no. 7, pp. 43-53, 2018.\n[78] Z. Xiao and Y. J. S. C. I. S. Zeng, \"An overview on integrated\nlocalization and communication towards 6G,\" vol. 65, pp. 1-46,\n2022.\n[79] M. Banafaa et al., \"6G mobile communication technology:\nRequirements, targets, applications, challenges, advantages, and\nopportunities,\" 2022.\n[80] I. Guvenc, F. Koohifar, S. Singh, M. L. Sichitiu, and D. J. I. C.\nM. Matolak, \"Detection, tracking, and interdiction for amateur\ndrones,\" vol. 56, no. 4, pp. 75-81, 2018.\n[81] Y. Mekdad et al., \"A survey on security and privacy issues of\nUAVs,\" Computer Networks, vol. 224, p. 109626, 2023.\n[82] G. Yang et al., \"Unmanned aerial vehicle remote sensing for\nfield-based crop phenotyping: current status and perspectives,\"\nvol. 8, p. 1111, 2017.\n[83] Z. Hu, Z. Bai, Y. Yang, Z. Zheng, K. Bian, and L. J. I. N. Song,\n\"UAV aided aerial-ground IoT for air quality sensing in smart\ncity: Architecture, technologies, and implementation,\" vol. 33,\nno. 2, pp. 14-22, 2019.\n[84] A. Guerra, D. Dardari, and P. M. J. I. V. T. M. Djuric, \"Dynamic\nradar networks of UAVs: A tutorial overview and tracking\nperformance comparison with terrestrial radar networks,\" vol.\n15, no. 2, pp. 113-120, 2020.\n[85] S. Homayouni et al., \"On the Feasibility of Cellular-Connected\nDrones in Existing 4G/5G Networks: Field Trials,\" in 2021 IEEE\n4th 5G World Forum (5GWF), 2021: IEEE.\n[86] S. Homayouni, M. Paier, C. Benischek, G. Pernjak, M. Reichelt,\nand C. Fuchsjager, \"Field Trials and Design Insights of Cellular\u0002Connected Drones,\" in 2021 IEEE 94th Vehicular Technology\nConference (VTC2021-Fall), 2021: IEEE.\n[87] R. Amer, W. Saad, B. Galkin, and N. Marchetti, \"Performance\nAnalysis of Mobile Cellular-Connected Drones under Practical\nAntenna Configurations,\" in ICC 2020 - 2020 IEEE\nInternational Conference on Communications (ICC), 2020:\nIEEE.\n[88] A. Azari, F. Ghavimi, M. Ozger, R. Jantti, and C. Cavdar,\n\"Machine Learning assisted Handover and Resource\nManagement for Cellular Connected Drones,\" in 2020 IEEE 91st\nVehicular Technology Conference (VTC2020-Spring), 2020:\nIEEE.\n[89] M. M. Azari, F. Rosas, and S. J. I. T. o. W. C. Pollin, \"Cellular\nconnectivity for UAVs: Network modeling, performance\nanalysis, and design guidelines,\" vol. 18, no. 7, pp. 3366-3381,\n2019.\n[90] W. Mei, Q. Wu, and R. J. I. T. o. w. c. Zhang, \"Cellular\u0002connected UAV: Uplink association, power control and\ninterference coordination,\" vol. 18, no. 11, pp. 5380-5393, 2019.\n[91] B. Li, Z. Fei, Y. Zhang, and M. J. I. W. C. Guizani, \"Secure UAV\ncommunication networks over 5G,\" vol. 26, no. 5, pp. 114-120,\n2019.\n[92] J. Li, D. Lu, G. Zhang, J. Tian, and Y. J. I. A. Pang, \"Post-disaster\nunmanned aerial vehicle base station deployment method based\non artificial bee colony algorithm,\" vol. 7, pp. 168327-168336,\n2019.\n[93] A. S. Parihar and S. K. Chakraborty, \"Flying Ad Hoc Network\n(FANET): Opportunities, Trending Applications and\nSimulators,\" in 2022 IEEE Pune Section International\nConference (PuneCon), 2022, pp. 1-5: IEEE.\n[94] F. Noor, M. A. Khan, A. Al-Zahrani, I. Ullah, and K. A. Al\u0002Dhlan, \"A review on communications perspective of flying ad\u0002hoc networks: key enabling wireless technologies, applications,\nchallenges and open research topics,\" Drones, vol. 4, no. 4, p.\n65, 2020.\n[95] F. Pasandideh, J. P. J. da Costa, R. Kunst, N. Islam, W.\nHardjawana, and E. Pignaton de Freitas, \"A review of flying ad\nhoc networks: Key characteristics, applications, and wireless\ntechnologies,\" Remote Sensing, vol. 14, no. 18, p. 4459, 2022.\n[96] A. Srivastava and J. Prakash, \"Future FANET with application\nand enabling techniques: Anatomization and sustainability\nissues,\" Computer science review, vol. 39, p. 100359, 2021.\n[97] M. Giordani and M. J. I. N. Zorzi, \"Non-terrestrial networks in\nthe 6G era: Challenges and opportunities,\" vol. 35, no. 2, pp.\n244-251, 2020.\n[98] M. Marchese, A. Moheddine, and F. J. S. Patrone, \"IoT and UAV\nintegration in 5G hybrid terrestrial-satellite networks,\" vol. 19,\nno. 17, p. 3704, 2019.\n[99] X. Liu, M. Lin, Q. Huang, J. Wang, and J. J. P. C. Ouyang,\n\"Performance analysis for multi-user integrated satellite and\nUAV cooperative networks,\" vol. 36, p. 100762, 2019.\n[100] P. K. Sharma and D. I. J. I. T. o. W. C. Kim, \"Secure 3D mobile\nUAV relaying for hybrid satellite-terrestrial networks,\" vol. 19,\nno. 4, pp. 2770-2784, 2020.\n[101] D. Hein, T. Kraft, J. Brauchle, and R. J. I. I. J. o. G.-I. Berger,\n\"Integrated UAV-based real-time mapping for security\napplications,\" vol. 8, no. 5, p. 219, 2019.\n[102] B. Li, S. Zhao, R. Miao, and R. Zhang, \"A survey on unmanned\naerial vehicle relaying networks,\" IET Communications, vol. 15,\nno. 10, pp. 1262-1272, 2021.\n[103] D. Liu et al., \"Task-driven relay assignment in distributed UAV\ncommunication networks,\" vol. 68, no. 11, pp. 11003-11017,\n2019.\n[104] N. Cheng et al., \"AI for UAV-Assisted IoT Applications: A\nComprehensive Review,\" 2023.\n[105] H. Dai, H. Zhang, C. Li, and B. J. C. C. Wang, \"Efficient\ndeployment of multiple UAVs for IoT communication in\ndynamic environment,\" vol. 17, no. 1, pp. 89-103, 2020.\n[106] Y. Zhou, B. Rao, and W. J. I. A. Wang, \"UAV swarm\nintelligence: Recent advances and future trends,\" vol. 8, pp.\n183856-183878, 2020.\n[107] C. Qin, F. Candan, L. S. Mihaylova, and E. J. a. p. a. Pournaras,\n\"3, 2, 1, Drones Go! a testbed to take off UAV swarm\nintelligence for distributed sensing,\" 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n38\n[108] J. Schwarzrock, I. Zacarias, A. L. Bazzan, R. Q. de Araujo\nFernandes, L. H. Moreira, and E. P. J. E. A. o. A. I. de Freitas,\n\"Solving task allocation problem in multi unmanned aerial\nvehicles systems using swarm intelligence,\" vol. 72, pp. 10-20,\n2018.\n[109] M. Chen, H. Wang, C.-Y. Chang, and X. J. I. A. Wei, \"SIDR: A\nswarm intelligence-based damage-resilient mechanism for UAV\nswarm networks,\" vol. 8, pp. 77089-77105, 2020.\n[110] Y. Zeng, Q. Wu, and R. J. P. o. t. I. Zhang, \"Accessing from the\nsky: A tutorial on UAV communications for 5G and beyond,\"\nvol. 107, no. 12, pp. 2327-2375, 2019.\n[111] Y. Huang, Q. Wu, R. Lu, X. Peng, and R. J. I. C. M. Zhang,\n\"Massive MIMO for cellular-connected UAV: Challenges and\npromising solutions,\" vol. 59, no. 2, pp. 84-90, 2021.\n[112] Y. Huang, Q. Wu, T. Wang, G. Zhou, and R. J. I. W. C. L. Zhang,\n\"3D beam tracking for cellular-connected UAV,\" vol. 9, no. 5,\npp. 736-740, 2020.\n[113] H. Q. Ngo, A. Ashikhmin, H. Yang, E. G. Larsson, and T. L. J.\nI. T. o. W. C. Marzetta, \"Cell-free massive MIMO versus small\ncells,\" vol. 16, no. 3, pp. 1834-1850, 2017.\n[114] C. D\u2019Andrea, A. Garcia-Rodriguez, G. Geraci, L. G. Giordano,\nand S. J. I. O. J. o. t. C. S. Buzzi, \"Analysis of UAV\ncommunications in cell-free massive MIMO systems,\" vol. 1,\npp. 133-147, 2020.\n[115] W. Yuan, C. Liu, F. Liu, S. Li, and D. W. K. J. I. W. C. L. Ng,\n\"Learning-based predictive beamforming for UAV\ncommunications with jittering,\" vol. 9, no. 11, pp. 1970-1974,\n2020.\n[116] Z. Feng, L. Ji, Q. Zhang, and W. J. I. C. M. Li, \"Spectrum\nmanagement for mmWave enabled UAV swarm networks:\nChallenges and opportunities,\" vol. 57, no. 1, pp. 146-153, 2018.\n[117] R. Hussain and M. S. Sharawi, \"5G MIMO antenna designs for\nbase station and user equipment: Some recent developments and\ntrends,\" IEEE Antennas and Propagation Magazine, vol. 64, no.\n3, pp. 95-107, 2021.\n[118] Q. Wu, S. Zhang, B. Zheng, C. You, and R. J. I. T. o. C. Zhang,\n\"Intelligent reflecting surface-aided wireless communications: A\ntutorial,\" vol. 69, no. 5, pp. 3313-3351, 2021.\n[119] Y. Liu, J. Zhao, M. Li, and Q. J. I. T. o. C. Wu, \"Intelligent\nreflecting surface aided MISO uplink communication network:\nFeasibility and power minimization for perfect and imperfect\nCSI,\" vol. 69, no. 3, pp. 1975-1989, 2020.\n[120] Q. Wu, X. Zhou, and R. J. I. W. C. L. Schober, \"IRS-assisted\nwireless powered NOMA: Do we really need different phase\nshifts in DL and UL?,\" vol. 10, no. 7, pp. 1493-1497, 2021.\n[121] C. Chaccour, M. N. Soorki, W. Saad, M. Bennis, and P.\nPopovski, \"Risk-based optimization of virtual reality over\nterahertz reconfigurable intelligent surfaces,\" in ICC 2020-2020\nIEEE International Conference on Communications (ICC),\n2020, pp. 1-6: IEEE.\n[122] H. Lu, Y. Zeng, S. Jin, and R. J. I. T. o. W. C. Zhang, \"Aerial\nintelligent reflecting surface: Joint placement and passive\nbeamforming design with 3D beam flattening,\" vol. 20, no. 7,\npp. 4128-4143, 2021.\n[123] S. Li, B. Duo, X. Yuan, Y.-C. Liang, and M. J. I. W. C. L. Di\nRenzo, \"Reconfigurable intelligent surface assisted UAV\ncommunication: Joint trajectory design and passive\nbeamforming,\" vol. 9, no. 5, pp. 716-720, 2020.\n[124] Y. Cai, Z. Wei, S. Hu, D. W. K. Ng, and J. Yuan, \"Resource\nallocation for power-efficient IRS-assisted UAV\ncommunications,\" in 2020 IEEE International Conference on\nCommunications Workshops (ICC Workshops), 2020, pp. 1-7:\nIEEE.\n[125] L. Wang, K. Wang, C. Pan, and N. J. I. T. o. M. C. Aslam, \"Joint\ntrajectory and passive beamforming design for intelligent\nreflecting surface-aided UAV communications: A deep\nreinforcement learning approach,\" 2022.\n[126] L. Ge, P. Dong, H. Zhang, J.-B. Wang, and X. J. I. A. You, \"Joint\nbeamforming and trajectory optimization for intelligent\nreflecting surfaces-assisted UAV communications,\" vol. 8, pp.\n78702-78712, 2020.\n[127] M. Hua, L. Yang, Q. Wu, C. Pan, C. Li, and A. L. J. I. T. o. W.\nC. Swindlehurst, \"UAV-assisted intelligent reflecting surface\nsymbiotic radio system,\" vol. 20, no. 9, pp. 5769-5785, 2021.\n[128] Q. Zhang, W. Saad, and M. Bennis, \"Reflections in the sky:\nMillimeter wave communication with UAV-carried intelligent\nreflectors,\" in 2019 IEEE Global Communications Conference\n(GLOBECOM), 2019, pp. 1-6: IEEE.\n[129] H. Long et al., \"Reflections in the sky: Joint trajectory and\npassive beamforming design for secure UAV networks with\nreconfigurable intelligent surface,\" 2020.\n[130] T. Shafique, H. Tabassum, and E. J. I. T. o. C. Hossain,\n\"Optimization of wireless relaying with flexible UAV-borne\nreflecting surfaces,\" vol. 69, no. 1, pp. 309-325, 2020.\n[131] V. W. Wong, R. Schober, D. W. K. Ng, and L.-C. Wang, Key\ntechnologies for 5G wireless systems. Cambridge university\npress, 2017.\n[132] Z. Ding, X. Lei, G. K. Karagiannidis, R. Schober, J. Yuan, and\nV. K. J. I. J. o. S. A. i. C. Bhargava, \"A survey on non-orthogonal\nmultiple access for 5G networks: Research challenges and future\ntrends,\" vol. 35, no. 10, pp. 2181-2195, 2017.\n[133] W. Mei and R. J. I. J. o. S. T. i. S. P. Zhang, \"Uplink cooperative\nNOMA for cellular-connected UAV,\" vol. 13, no. 3, pp. 644-\n656, 2019.\n[134] W. Mei and R. J. I. W. C. L. Zhang, \"Uplink cooperative\ninterference cancellation for cellular-connected UAV: A\nquantize-and-forward approach,\" vol. 9, no. 9, pp. 1567-1571,\n2020.\n[135] W. Mei and R. J. I. T. o. C. Zhang, \"Cooperative downlink\ninterference transmission and cancellation for cellular-connected\nUAV: A divide-and-conquer approach,\" vol. 68, no. 2, pp. 1297-\n1311, 2019.\n[136] W. K. New, C. Y. Leow, K. Navaie, and Z. J. I. T. o. W. C. Ding,\n\"Robust non-orthogonal multiple access for aerial and ground\nusers,\" vol. 19, no. 7, pp. 4793-4805, 2020.\n[137] Q. Wu, J. Xu, and R. J. I. J. o. S. A. i. C. Zhang, \"Capacity\ncharacterization of UAV-enabled two-user broadcast channel,\"\nvol. 36, no. 9, pp. 1955-1971, 2018.\n[138] M. F. Sohail, C. Y. Leow, and S. J. I. A. Won, \"Non-orthogonal\nmultiple access for unmanned aerial vehicle assisted\ncommunication,\" vol. 6, pp. 22716-22727, 2018.\n[139] P. K. Sharma and D. I. Kim, \"UAV-enabled downlink wireless\nsystem with non-orthogonal multiple access,\" in 2017 IEEE\nGlobecom Workshops (GC Wkshps), 2017, pp. 1-6: IEEE.\n[140] A. A. Nasir, H. D. Tuan, T. Q. Duong, and H. V. J. I. T. o. C.\nPoor, \"UAV-enabled communication using NOMA,\" vol. 67, no.\n7, pp. 5126-5138, 2019.\n[141] T. Hou, Y. Liu, Z. Song, X. Sun, and Y. J. I. T. o. C. Chen,\n\"Multiple antenna aided NOMA in UAV networks: A stochastic\ngeometry approach,\" vol. 67, no. 2, pp. 1031-1044, 2018.\n[142] T. Hou, Y. Liu, Z. Song, X. Sun, and Y. J. I. T. o. C. Chen,\n\"Exploiting NOMA for UAV communications in large-scale\ncellular networks,\" vol. 67, no. 10, pp. 6897-6911, 2019.\n[143] R. Duan, J. Wang, C. Jiang, H. Yao, Y. Ren, and Y. J. I. I. o. T.\nJ. Qian, \"Resource allocation for multi-UAV aided IoT NOMA\nuplink transmission systems,\" vol. 6, no. 4, pp. 7025-7037, 2019.\n[144] N. Rupasinghe, Y. Yap\u0131c\u0131, I. G\u00fcven\u00e7, M. Ghosh, and Y. J. I. J.\no. S. T. i. S. P. Kakishima, \"Angle feedback for NOMA\ntransmission in mmWave drone networks,\" vol. 13, no. 3, pp.\n628-643, 2019.\n[145] J. Xu, Y. Zeng, and R. Zhang, \"UAV-enabled wireless power\ntransfer: Trajectory design and energy region characterization,\"\nin 2017 IEEE Globecom Workshops (GC Wkshps), 2017, pp. 1-\n7: IEEE.\n[146] J. Xu, Y. Zeng, and R. J. I. t. o. w. c. Zhang, \"UAV-enabled\nwireless power transfer: Trajectory design and energy\noptimization,\" vol. 17, no. 8, pp. 5092-5106, 2018.\n[147] Y. Hu, X. Yuan, J. Xu, and A. J. I. T. o. C. Schmeink, \"Optimal\n1D trajectory design for UAV-enabled multiuser wireless power\ntransfer,\" vol. 67, no. 8, pp. 5674-5688, 2019.\n[148] Z. Yang, W. Xu, and M. J. I. T. o. V. T. Shikh-Bahaei, \"Energy\nefficient UAV communication with energy harvesting,\" vol. 69,\nno. 2, pp. 1913-1927, 2019.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n39\n[149] L. Xie, J. Xu, and R. J. I. I. o. T. J. Zhang, \"Throughput\nmaximization for UAV-enabled wireless powered\ncommunication networks,\" vol. 6, no. 2, pp. 1690-1703, 2018.\n[150] L. Xie, J. Xu, and Y. J. I. T. o. C. Zeng, \"Common throughput\nmaximization for UAV-enabled interference channel with\nwireless powered communications,\" vol. 68, no. 5, pp. 3197-\n3212, 2020.\n[151] J. Tang, J. Song, J. Ou, J. Luo, X. Zhang, and K.-K. J. I. A.\nWong, \"Minimum throughput maximization for multi-UAV\nenabled WPCN: A deep reinforcement learning method,\" vol. 8,\npp. 9124-9132, 2020.\n[152] J.-M. Kang and C.-J. J. I. S. J. Chun, \"Joint trajectory design, Tx\npower allocation, and Rx power splitting for UAV-enabled\nmulticasting SWIPT systems,\" vol. 14, no. 3, pp. 3740-3743,\n2020.\n[153] W. Feng et al., \"UAV-enabled SWIPT in IoT networks for\nemergency communications,\" vol. 27, no. 5, pp. 140-147, 2020.\n[154] A. O. Hashesh, S. Hashima, R. M. Zaki, M. M. Fouda, K.\nHatano, and A. S. T. Eldien, \"AI-enabled UAV communications:\nChallenges and future directions,\" IEEE Access, vol. 10, pp.\n92048-92066, 2022.\n[155] S. Ben Aissa and A. Ben Letaifa, \"UAV communications with\nmachine learning: challenges, applications and open issues,\"\nArabian Journal for Science and Engineering, vol. 47, no. 2, pp.\n1559-1579, 2022.\n[156] B. Baig and A. Q. Shahzad, \"Machine learning and AI approach\nto improve UAV communication and networking,\" in\nComputational intelligence for unmanned aerial vehicles\ncommunication networks: Springer, 2022, pp. 1-15.\n[157] S. Wang, S. Hosseinalipour, M. Gorlatova, C. G. Brinton, and\nM. Chiang, \"UAV-assisted online machine learning over multi\u0002tiered networks: A hierarchical nested personalized federated\nlearning approach,\" IEEE Transactions on Network and Service\nManagement, 2022.\n[158] H. Kurunathan, H. Huang, K. Li, W. Ni, and E. Hossain,\n\"Machine learning-aided operations and communications of\nunmanned aerial vehicles: A contemporary survey,\" IEEE\nCommunications Surveys & Tutorials, 2023.\n[159] S. Rajendran, K. K. Samy, J. Chinnathevar, and D. P. Sethuraj,\n\"Machine Learning Techniques for UAV Trajectory\nOptimization\u2014A Survey,\" in Computational Intelligence for\nUnmanned Aerial Vehicles Communication Networks: Springer,\n2022, pp. 35-44.\n[160] G. Afifi and Y. Gadallah, \"Cellular Network-Supported Machine\nLearning Techniques for Autonomous UAV Trajectory\nPlanning,\" IEEE Access, vol. 10, pp. 131996-132011, 2022.\n[161] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. J. I. c. m.\nZhang, \"The roadmap to 6G: AI empowered wireless networks,\"\nvol. 57, no. 8, pp. 84-90, 2019.\n[162] D. G\u00fcnd\u00fcz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R.\nMurthy, and M. J. I. J. o. S. A. i. C. van der Schaar, \"Machine\nlearning in the air,\" vol. 37, no. 10, pp. 2184-2199, 2019.\n[163] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. J. I. c. m. Huang,\n\"Toward an intelligent edge: Wireless communication meets\nmachine learning,\" vol. 58, no. 1, pp. 19-25, 2020.\n[164] F. Wang, J. Xu, X. Wang, and S. J. I. T. o. W. C. Cui, \"Joint\noffloading and computing optimization in wireless powered\nmobile-edge computing systems,\" vol. 17, no. 3, pp. 1784-1797,\n2017.\n[165] Y. Zeng, X. Xu, S. Jin, and R. J. I. T. o. W. C. Zhang,\n\"Simultaneous navigation and radio mapping for cellular\u0002connected UAV with deep reinforcement learning,\" vol. 20, no.\n7, pp. 4205-4220, 2021.\n[166] X. Liu, M. Chen, Y. Liu, Y. Chen, S. Cui, and L. J. I. W. C.\nHanzo, \"Artificial intelligence aided next-generation networks\nrelying on UAVs,\" vol. 28, no. 1, pp. 120-127, 2020.\n[167] I. Orikumhi, J. Bae, H. Park, and S. Kim, \"DRL-based Multi\u0002UAV trajectory optimization for ultra-dense small cells,\" ICT\nExpress, 2023.\n[168] M. Seong, O. Jo, and K. Shin, \"Multi-UAV trajectory optimizer:\nA sustainable system for wireless data harvesting with deep\nreinforcement learning,\" Engineering Applications of Artificial\nIntelligence, vol. 120, p. 105891, 2023.\n[169] S. Zhou, Y. Cheng, X. Lei, and H. Duan, \"Multi-agent few-shot\nmeta reinforcement learning for trajectory design and channel\nselection in UAV-assisted networks,\" China Communications,\nvol. 19, no. 4, pp. 166-176, 2022.\n[170] C. Zhang, X. Li, C. He, X. Li, and D. Lin, \"Trajectory\noptimization for UAV-enabled relaying with reinforcement\nlearning,\" Digital Communications and Networks, 2023.\n[171] A. Shahbazi, I. Donevski, J. J. Nielsen, and M. Di Renzo,\n\"Federated reinforcement learning UAV trajectory design for\nfast localization of ground users,\" in 2022 30th European Signal\nProcessing Conference (EUSIPCO), 2022, pp. 663-666: IEEE.\n[172] P. Ji, J. Jia, J. Chen, L. Guo, A. Du, and X. Wang,\n\"Reinforcement learning based joint trajectory design and\nresource allocation for RIS-aided UAV multicast networks,\"\nComputer Networks, vol. 227, p. 109697, 2023.\n[173] B. Zhu, E. Bedeer, H. H. Nguyen, R. Barton, and J. Henry,\n\"UAV trajectory planning in wireless sensor networks for energy\nconsumption minimization by deep reinforcement learning,\"\nIEEE Transactions on Vehicular Technology, vol. 70, no. 9, pp.\n9540-9554, 2021.\n[174] S. Chai and V. K. Lau, \"Multi-UAV trajectory and power\noptimization for cached UAV wireless networks with energy and\ncontent recharging-demand driven deep learning approach,\"\nIEEE Journal on Selected Areas in Communications, vol. 39, no.\n10, pp. 3208-3224, 2021.\n[175] X. Cao, J. Xu, and R. Zhang, \"Mobile edge computing for\ncellular-connected UAV: Computation offloading and trajectory\noptimization,\" in 2018 IEEE 19th International Workshop on\nSignal Processing Advances in Wireless Communications\n(SPAWC), 2018, pp. 1-5: IEEE.\n[176] B. Brik, A. Ksentini, and M. J. I. A. Bouaziz, \"Federated learning\nfor UAVs-enabled wireless networks: Use cases, challenges, and\nopen problems,\" vol. 8, pp. 53841-53849, 2020.\n[177] T. Zeng, O. Semiari, M. Mozaffari, M. Chen, W. Saad, and M.\nBennis, \"Federated learning in the sky: Joint power allocation\nand scheduling with UAV swarms,\" in ICC 2020-2020 IEEE\nInternational Conference on Communications (ICC), 2020, pp.\n1-6: IEEE.\n[178] Y. Liu, J. Nie, X. Li, S. H. Ahmed, W. Y. B. Lim, and C. J. I. I.\no. T. J. Miao, \"Federated learning in the sky: Aerial-ground air\nquality sensing framework with UAV swarms,\" vol. 8, no. 12,\npp. 9827-9837, 2020.\n[179] C. Dong et al., \"UAVs as an intelligent service: Boosting edge\nintelligence for air-ground integrated networks,\" vol. 35, no. 4,\npp. 167-175, 2021.\n[180] J. S. Ng et al., \"Joint auction-coalition formation framework for\ncommunication-efficient federated learning in UAV-enabled\ninternet of vehicles,\" vol. 22, no. 4, pp. 2326-2344, 2020.\n[181] T. Liu, T. Zhang, J. Loo, and Y. Wang, \"Deep reinforcement\nlearning-based resource allocation for UAV-enabled federated\nedge learning,\" Journal of Communications and Information\nNetworks, vol. 8, no. 1, pp. 1-12, 2023.\n[182] P. Hou, X. Jiang, Z. Wang, S. Liu, and Z. Lu, \"Federated Deep\nReinforcement Learning-Based Intelligent Dynamic Services in\nUAV-Assisted MEC,\" IEEE Internet of Things Journal, 2023.\n[183] S. R. Sabuj, M. Elsharief, and H.-S. Jo, \"A Partial Federated\nLearning Model in Cognitive UAV-enabled Edge Computing\nNetworks,\" in 2022 13th International Conference on\nInformation and Communication Technology Convergence\n(ICTC), 2022, pp. 1437-1440: IEEE.\n[184] J. Tursunboev, Y.-S. Kang, S.-B. Huh, D.-W. Lim, J.-M. Kang,\nand H. Jung, \"Hierarchical federated learning for edge-aided\nunmanned aerial vehicle networks,\" Applied Sciences, vol. 12,\nno. 2, p. 670, 2022.\n[185] Z. Song, C. Ma, M. Ding, H. H. Yang, Y. Qian, and X. Zhou,\n\"Personalized Federated Deep Reinforcement Learning-based\nTrajectory Optimization for Multi-UAV Assisted Edge\nComputing,\" in 2023 IEEE/CIC International Conference on\nCommunications in China (ICCC), 2023, pp. 1-6: IEEE.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n40\n[186] M. B. Janjua and H. J. S. Arslan, \"A Survey of Symbiotic Radio:\nMethodologies, Applications, and Future Directions,\" vol. 23,\nno. 5, p. 2511, 2023.\n[187] Y.-C. Liang, Q. Zhang, E. G. Larsson, G. Y. J. I. T. o. C. C. Li,\nand Networking, \"Symbiotic radio: Cognitive backscattering\ncommunications for future wireless networks,\" vol. 6, no. 4, pp.\n1242-1255, 2020.\n[188] K. K. Vaigandla, S. Thatipamula, and R. K. J. I. J. o. S. W. S.\nKarne, \"Investigation on unmanned aerial vehicle (uav): An\noverview,\" vol. 4, no. 3, pp. 130-148, 2022.\n[189] H. Nakamura and Y. Kajikawa, \"Regulation and innovation:\nHow should small unmanned aerial vehicles be regulated?,\"\nTechnological forecasting and social change, vol. 128, pp. 262-\n274, 2018.\n[190] D. Lee, D. J. Hess, and M. A. Heldeweg, \"Safety and privacy\nregulations for unmanned aerial vehicles: A multiple\ncomparative analysis,\" Technology in Society, vol. 71, p.\n102079, 2022.\n[191] T.-H. Tran and D.-D. Nguyen, \"Management and Regulation of\nDrone Operation in Urban Environment: A Case Study,\" Social\nSciences, vol. 11, no. 10, p. 474, 2022.\n[192] K. Belwafi, R. Alkadi, S. A. Alameri, H. Al Hamadi, and A.\nShoufan, \"Unmanned aerial vehicles\u2019 remote identification: A\ntutorial and survey,\" IEEE Access, vol. 10, pp. 87577-87601,\n2022.\n[193] ISO, \"UAS traffic management (UTM) \u2014 Part 8: Remote\nidentification, ISO 23629-8,\" 2023.\n[194] 3GPP, \"Unmanned Aerial System (UAS) support in 3GPP, TS\n22.125,\" 2023.\n[195] Y. Zhi, Z. Fu, X. Sun, and J. Yu, \"Security and privacy issues of\nUAV: a survey,\" Mobile Networks and Applications, vol. 25, pp.\n95-101, 2020.\n[196] M. Yahuza et al., \"Internet of drones security and privacy issues:\nTaxonomy and open challenges,\" IEEE Access, vol. 9, pp.\n57243-57270, 2021.\n[197] OFCOM, \"Spectrum for Unmanned Aircraft Systems (UAS),\"\n2022.\n[198] ISO, \"Unmanned aircraft systems \u2014 Part 2: UAS components.\nISO 21384-2:2021,\" 2021.\n[199] I. L. Henderson, \"Aviation safety regulations for unmanned\naircraft operations: Perspectives from users,\" Transport Policy,\nvol. 125, pp. 192-206, 2022.\n[200] E. Politi, I. Varlamis, K. Tserpes, M. Larsen, and G.\nDimitrakopoulos, \"The future of safe BVLOS drone operations\nwith respect to system and service engineering,\" in 2022 IEEE\nInternational Conference on Service-Oriented System\nEngineering (SOSE), 2022, pp. 133-140: IEEE.\n[201] ISO, \"UAS traffic management (UTM) \u2014 Part 5: UTM\nfunctional structure, ISO 23629-5:2023,\" 2023.\n[202] ISO, \"UAS traffic management (UTM) \u2014 Part 12:\nRequirements for UTM service providers, ISO 23629-12:2022,\"\n2022.\n[203] R. Shrestha, R. Bajracharya, and S. Kim, \"6G enabled unmanned\naerial vehicle traffic management: A perspective,\" IEEE Access,\nvol. 9, pp. 91119-91136, 2021.\n[204] A. Shelley, \"Essays in the Regulation of Drones and Counter\u0002Drone Systems,\" 2020.\n[205] B. Nassi, R. Bitton, R. Masuoka, A. Shabtai, and Y. Elovici,\n\"SoK: Security and privacy in the age of commercial drones,\" in\n2021 IEEE Symposium on Security and Privacy (SP), 2021, pp.\n1434-1451: IEEE.\n[206] ISO, \"Unmanned aircraft systems \u2014 Part 3: Operational\nprocedures, ISO 21384-3:2019(en),\" 2019.\n[207] A. H. Arani, P. Hu, and Y. J. I. O. J. o. t. C. S. Zhu, \"HAPS\u0002UAV-Enabled Heterogeneous Networks: A Deep\nReinforcement Learning Approach,\" 2023.\n[208] D. Mishra, A. M. Vegni, V. Loscr\u00ed, and E. J. I. C. S. M. Natalizio,\n\"Drone networking in the 6G era: A technology overview,\" vol.\n5, no. 4, pp. 88-95, 2021.\n[209] M. A. Khan et al., \"Swarm of UAVs for network management in\n6G: A technical review,\" 2022.\n[210] J. Luo, Z. Wang, M. Xia, L. Wu, Y. Tian, and Y. J. A. C. S.\nChen, \"Path Planning for UAV Communication Networks:\nRelated Technologies, Solutions, and Opportunities,\" vol. 55,\nno. 9, pp. 1-37, 2023.\n[211] S. Hafeez et al., \"Blockchain-Assisted UAV Communication\nSystems: A Comprehensive Survey,\" 2023.\n[212] M. Basharat, M. Naeem, Z. Qadir, and A. J. T. o. E. T. T.\nAnpalagan, \"Resource optimization in UAV\u2010assisted wireless\nnetworks\u2014A comprehensive survey,\" vol. 33, no. 7, p. e4464,\n2022.\n[213] Z. Xu, I. Petrunin, A. J. J. o. I. Tsourdos, and R. Systems,\n\"Dynamic spectrum management with network function\nvirtualization for uav communication,\" vol. 101, pp. 1-18, 2021.\n[214] S. K. Nobar, M. H. Ahmed, Y. Morgan, S. A. J. I. T. o. C. C.\nMahmoud, and Networking, \"Resource allocation in cognitive\nradio-enabled UAV communication,\" vol. 8, no. 1, pp. 296-310,\n2021.\n[215] J. N. Yasin, S. A. S. Mohamed, M.-H. Haghbayan, J. Heikkonen,\nH. Tenhunen, and J. Plosila, \"Unmanned Aerial Vehicles\n(UAVs): Collision Avoidance Systems and Approaches,\" IEEE\nAccess, vol. 8, pp. 105139-105155, 2020-01-01 2020.\n[216] S. Singh et al., \"FPV Video Adaptation for UAV Collision\nAvoidance,\" IEEE Open Journal of the Communications\nSociety, vol. 2, pp. 2095-2110, 2021-01-01 2021.\n[217] Y.-H. Hsu and R.-H. Gau, \"Reinforcement Learning-Based\nCollision Avoidance and Optimal Trajectory Planning in UAV\nCommunication Networks,\" IEEE Transactions on Mobile\nComputing, vol. 21, no. 1, pp. 306-320, 2022-01-01 2022.\n[218] H. Huang and A. V. Savkin, \"An Algorithm of Reactive\nCollision Free 3-D Deployment of Networked Unmanned Aerial\nVehicles for Surveillance and Monitoring,\" IEEE Transactions\non Industrial Informatics, vol. 16, no. 1, pp. 132-140, 2020-01-\n01 2020.\n[219] J. Kim, S. Kim, J. Jeong, H. Kim, J. Park, and T. Kim, \"CBDN:\nCloud-Based Drone Navigation for Efficient Battery Charging\nin Drone Networks,\" IEEE Transactions on Intelligent\nTransportation Systems, vol. 20, no. 11, pp. 4174-4191, 2019.\n[220] M. Shin, J. Kim, and M. Levorato, \"Auction-Based Charging\nScheduling With Deep Learning Framework for Multi-Drone\nNetworks,\" IEEE Transactions on Vehicular Technology, vol.\n68, no. 5, pp. 4235-4248, 2019.\n[221] H. Huang and A. V. Savkin, \"A Method of Optimized\nDeployment of Charging Stations for Drone Delivery,\" IEEE\nTransactions on Transportation Electrification, vol. 6, no. 2, pp.\n510-518, 2020.\n[222] D. He, S. Chan, and M. Guizani, \"Drone-Assisted Public Safety\nNetworks: The Security Aspect,\" IEEE Communications\nMagazine, vol. 55, no. 8, pp. 218-223, 2017.\n[223] C. Lin, D. He, N. Kumar, K. R. Choo, A. Vinel, and X. Huang,\n\"Security and Privacy for the Internet of Drones: Challenges and\nSolutions,\" IEEE Communications Magazine, vol. 56, no. 1, pp.\n64-69, 2018.\n[224] G. Geraci, A. Garcia-Rodriguez, L. G. Giordano, D. L\u00f3pez\u0002P\u00e9rez, and E. Bj\u00f6rnson, \"Understanding UAV cellular\ncommunications: From existing networks to massive MIMO,\"\nIEEE Access, vol. 6, pp. 67853-67865, 2018.\n[225] J. Angjo, I. Shayea, M. Ergen, H. Mohamad, A. Alhammadi, and\nY. I. Daradkeh, \"Handover management of drones in future\nmobile networks: 6G technologies,\" IEEE access, vol. 9, pp.\n12803-12823, 2021.\n[226] W. Alshaibani, I. Shayea, R. Caglar, J. Din, and Y. I. Daradkeh,\n\"Mobility Management of Unmanned Aerial Vehicles in Ultra\u2013\nDense Heterogeneous Networks,\" Sensors, vol. 22, no. 16, p.\n6013, 2022.\n[227] D. Darsena, G. Gelli, I. Iudice, and F. Verde, \"Equalization\ntechniques of control and non-payload communication links for\nunmanned aerial vehicles,\" IEEE Access, vol. 6, pp. 4485-4496,\n2018.\n[228] A. Y\u0131lmaz and C. Toker, \"Air-to-Air Channel Model for UAV\nCommunications,\" in 2022 30th Signal Processing and\nCommunications Applications Conference (SIU), 2022, pp. 1-4:\nIEEE.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n41\n[229] X. Mao, C.-X. Wang, and H. Chang, \"A 3D non-stationary\ngeometry-based stochastic model for 6G UAV air-to-air\nchannels,\" in 2021 13th International Conference on Wireless\nCommunications and Signal Processing (WCSP), 2021, pp. 1-5:\nIEEE.\n[230] H. An et al., \"Measurement and Ray-tracing for UAV Air-to-air\nChannel Modeling,\" in 2022 IEEE 5th International Conference\non Electronic Information and Communication Technology\n(ICEICT), 2022, pp. 415-420: IEEE.\n[231] C.-C. Chiu, A.-H. Tsai, H.-P. Lin, C.-Y. Lee, and L.-C. Wang,\n\"Channel modeling of air-to-ground signal measurement with\ntwo-ray ground-reflection model for UAV communication\nsystems,\" in 2021 30th Wireless and Optical Communications\nConference (WOCC), 2021, pp. 251-256: IEEE.\n[232] C. Ge et al., \"Pathloss and Airframe Shadowing Loss of Air-to\u0002Ground UAV Channel in the Airport Area at UHF-and L-Band,\"\nIEEE Transactions on Vehicular Technology, 2023.\n[233] H. Li, L. Ding, Y. Wang, and Z. Wang, \"Air-to-Ground Channel\nModeling and Performance Analysis for Cellular-Connected\nUAV Swarm,\" IEEE Communications Letters, 2023.\n[234] H. Jiang, Z. Zhang, L. Wu, and J. Dang, \"Three-dimensional\ngeometry-based UAV-MIMO channel modeling for A2G\ncommunication environments,\" IEEE communications letters,\nvol. 22, no. 7, pp. 1438-1441, 2018.\n[235] Z. Cui, K. Guan, C. Oestges, C. Briso-Rodr\u00edguez, B. Ai, and Z.\nZhong, \"Cluster-based characterization and modeling for uav\nair-to-ground time-varying channels,\" IEEE Transactions on\nVehicular Technology, vol. 71, no. 7, pp. 6872-6883, 2022.\n[236] F. Xu et al., \"Beyond Encryption: Exploring the Potential of\nPhysical Layer Security in UAV Networks,\" Journal of King\nSaud University-Computer and Information Sciences, p. 101717,\n2023.\n[237] R. Polus, C. D\u2019Amours, and B. Kantarci, \"Physical Layer\nSecurity Over UAV-to-Ground Channels with Shadowing,\" in\n2023 IEEE 97th Vehicular Technology Conference (VTC2023-\nSpring), 2023, pp. 1-5: IEEE.\n[238] W. U. Khan et al., \"Opportunities for physical layer security in\nUAV communication enhanced with intelligent reflective\nsurfaces,\" IEEE Wireless Communications, vol. 29, no. 6, pp. 22-\n28, 2022.\n[239] A. Benaya, M. H. Ismail, A. S. Ibrahim, and A. A. Salem,\n\"Physical Layer Security Enhancement via Intelligent Omni\u0002Surfaces and UAV-Friendly Jamming,\" IEEE Access, vol. 11,\npp. 2531-2544, 2023.\n[240] H. Sharma, N. Kumar, R. K. Tekchandani, and N. Mohammad,\n\"Deep learning enabled channel secrecy codes for physical layer\nsecurity of UAVs in 5G and beyond networks,\" in ICC 2022-\nIEEE International Conference on Communications, 2022, pp.\n1-6: IEEE.\n[241] E. Illi, M. Qaraqe, F. El Bouanani, and S. Al-Kuwari, \"On the\nPhysical Layer Security of a Dual-Hop UAV-based Network in\nthe Presence of per-hop Eavesdropping and Imperfect CSI,\"\nIEEE Internet of Things Journal, 2022.\n[242] T. T. Nguyen, T. T. H. Le, and X. N. Tran, \"Physical Layer\nSecurity for UAV-Based Full-Duplex Relay NOMA System,\" in\n2022 International Conference on Advanced Technologies for\nCommunications (ATC), 2022, pp. 395-400: IEEE.\n[243] A. B. B\u00fcy\u00fck\u015far, M. Can, and \u0130. Altunba\u015f, \"Physical Layer\nSecurity Improvement of NOMA-Based UAV-Aided Terrestrial\nNetworks via Cooperative User Selection,\" in 2022 30th Signal\nProcessing and Communications Applications Conference\n(SIU), 2022, pp. 1-4: IEEE.\n[244] M. Prajapati and P. Tripathi, \"Physical Layer Security\nOptimisation for NOMA Based UAV Communication for\nOptimal Resource Allocation,\" in International Conference on\nComputing Science, Communication and Security, 2023, pp.\n133-147: Springer.\nBIOGRAPHY\nMOHAMMED BANAFAA hailing from\nYemen, holds a B.Eng. (First Class Hons.) from\nUniversity of Aden (2017) and an M.Eng.\n(Hons.) from Universiti Teknologi Malaysia\n(2019). He's pursuing a Ph.D. in Electronic and\nCommunication at King Fahd University of\nPetroleum and Minerals (KFUPM). An IEEE\nmember, his research interests span wireless\ncommunications, 5G/6G technologies, UAV\nmobility management, and Mobile Edge\nComputing. .[E-mail: eng.banafaa@gmail.com]\n\u00d6MER PEPEO\u011eLU received the B.S. degree in\nelectronics and communication engineering from\nIstanbul Technical University, T\u00fcrkiye, in 2020.\nHe is currently pursuing the M.S. degree in\ncommunications engineering at the Technical\nUniversity of Munich, Germany. His research\ninterests are mainly based on wireless\ncommunications, signal processing, and their\napplications.[E-mail: omerpepeoglu@gmail.com]\nIBRAHEEM SHAYEA is an Associate\nResearcher of Electrical Engineering in Istanbul\nTechnical University. Previously, he worked as a\npostdoctoral research fellow at Istanbul\nTechnical University (Turkey) and at Wireless\nCommunication Centre (WCC), Universiti\nTeknologi Malaysia (UTM) (Kuala Lumpur,\nMalaysia). He obtained his BSc. in Electronic\nEngineering from University of Diyala Iraq in\n2004, MSc in Computer and Communication\nEngineering from Universiti Kebangsaan\nMalaysia (UKM) Malaysia in 2010, and PhD in Mobile Communication\nEngineering from UKM, Malaysia in 2015. His research interests in\nMobility Management, Handover, LTE/LTE-A, Carrier Aggregation, radio\npropagation, indoors and outdoors wireless communication, IoT, 5G, and\n6G. He has published several papers related to wireless communication in\nnational/international journal and conference. [E-mail:\nibr.shayea@gmail.com].\nABDULRAQEB ALHAMMADI received\nhis B. Eng. in Electronic majoring in\ntelecommunications, M.S degree and PhD in\nwireless communication from Multimedia\nUniversity, Malaysia, in 2011, 2015 and\n2020, respectively. He served as a research\nassistant/research scholar at Multimedia\nUniversity from 2012 till 2019. He is\ncurrently a postdoctoral fellow at the\nMalaysia-Japan International Institute of\nTechnology, Universiti Teknologi Malaysia. He received several awards,\nincluding the Excellent Researcher Award 2019, Multimedia University and\nthe Outstanding Doctoral Dissertation Award 2020, IEEE Malaysia\n(Communication Society). He is also the author of more than 50 articles in\ninternational journals and conferences. His main research interests include\nheterogeneous networks, indoor localization, artificial intelligence, data\nanalytics, cognitive radio networks and IoT. He is a member of professional\ninstitutes and societies, such as IEEE, IEICE, IACSIT, and IAENG. He is\nalso a member of program committees at international conferences and\nworkshops.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n42\nZAID AHMED SHAMSAN is a Senior Member\nof IEEE with a B.Sc. (Hons.) degree in\nelectronics and communication engineering from\nSudan University of Sciences and Technology\n(SUST). He completed his master's and Ph.D.\ndegrees in telecommunication and electrical\nengineering from Universiti Teknologi Malaysia\n(UTM). Dr. Shamsan has worked as a\nPostdoctoral Research Fellow at UTM's Wireless\nCommunication Center (WCC) and has been\ninvolved in research projects on frequency spectrum management funded by\nthe Malaysian Communications and Multimedia Commission (MCMC). He\nreceived a Bronze Medal in Geneva Innovation 2021 for his RFID Lock\nSecurity Mechanism project. Currently a Professor at Imam Mohammad Ibn\nSaud Islamic University, he specializes in teaching various electrical\nengineering courses and has published papers on topics including wireless\nnetworks, wave propagation, and optical communications.\nMUNEEF A. RAZAZ, received his B.s. degree\nin Communications and Computer Engineering\nwith (First Class Hons.) from Taiz University in\n2020. Currently He's pursuing a M.S. in\nElectrical Engineering at King Fahd University\nof Petroleum and Minerals (KFUPM). His\nresearch focuses on cutting-edge areas such as\nHybrid RF/VLC, Symbiotic Radio, AI/DL for\nwireless communications, 5G/6G Network\nSlicing, and IoT. [E-mail:\nmneefbasha@gmail.com]\nMAJID ALSAGABI is an Assistant Professor of\nSignal processing and communication in\nElectrical Engineering Department, College of\nEngineering, Imam Mohammad Ibn Saud Islamic\nUniversity (IMSIU). Before joining the EE Dept\nat IMSIU he received his B.Sc. (Hons) in\nElectrical Engineering from king Saud university\nin Riyadh, Saudi Arabia in 2000, then he received\nhis MSc. (in the field of digital signal processing)\nand PhD (in the field of Bioinformatics) from\nUniversity of Minnesota, USA in 2008 and 2012, respectively. He was a\nlecturer at Almajmah University. From 2000 to 2006, Dr. Majid worked as\nan energy engineer at the Saudi Electricity Company.\nSULAIMAN AL-SOWAYAN is an Associate\nProfessor of Electronics and Communications\nin Electrical Engineering Department, College\nof Engineering, Imam Mohammad Ibn Saud\nIslamic University (IMSIU). Before joining the\nEE Dept at IMSIU he received his B.Sc. (Hons)\nfrom king Saud university in Riyadh, Saudi\nArabia in 1994, then he received his MSc. and\nPhD from Colorado State University, USA in\n2001 and 2005 respectively. He works on\nOptoelectronics, and Optical Communications. His focus is on optical fiber\ncommunications, optical sensors, Optical Coherence Tomography, and\nelectromagnetic applications. He is an author and coauthor of several\npublications in high impact journals and conferences. He worked as a part\ntime and full time consultant in the area of telecommunications for several\ninstitutions in the government and the private sector.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4",
      "openalex_id": "https://openalex.org/W4390494361",
      "title": "A Comprehensive Survey on 5G-and-Beyond Networks With UAVs: Applications, Emerging Technologies, Regulatory Aspects, Research Trends and Challenges",
      "publication_date": "2024-01-01",
      "cited_by_count": 15.0,
      "topics": "Unmanned Aerial Vehicle Communications, Intelligent Reflecting Surfaces in Wireless Communications, Visual Object Tracking and Person Re-identification",
      "keywords": "5G and Beyond, Open research, UAV Networks, Drone Applications, Emerging technologies",
      "concepts": "Software deployment, Computer science, Open research, Emerging technologies, Resource (disambiguation), Field (mathematics), Data science, Key (lock), Telecommunications, Systems engineering, Computer security, Engineering, World Wide Web, Artificial intelligence, Computer network, Mathematics, Pure mathematics, Operating system",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10379625.pdf"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W2118548017",
        "https://openalex.org/W2125957038",
        "https://openalex.org/W2286275639",
        "https://openalex.org/W2461414758",
        "https://openalex.org/W2604830243",
        "https://openalex.org/W2604974201",
        "https://openalex.org/W2608727628",
        "https://openalex.org/W2613890548",
        "https://openalex.org/W2627863163",
        "https://openalex.org/W2728224506",
        "https://openalex.org/W2736565351",
        "https://openalex.org/W2772600596",
        "https://openalex.org/W2777180355",
        "https://openalex.org/W2782556342",
        "https://openalex.org/W2783189583",
        "https://openalex.org/W2783406823",
        "https://openalex.org/W2783764093",
        "https://openalex.org/W2785633818",
        "https://openalex.org/W2792798301",
        "https://openalex.org/W2795242006",
        "https://openalex.org/W2797821788",
        "https://openalex.org/W2800354041",
        "https://openalex.org/W2800793214",
        "https://openalex.org/W2809397343",
        "https://openalex.org/W2889655092",
        "https://openalex.org/W2898004290",
        "https://openalex.org/W2901594284",
        "https://openalex.org/W2903528516",
        "https://openalex.org/W2905025677",
        "https://openalex.org/W2905200871",
        "https://openalex.org/W2910734690",
        "https://openalex.org/W2910780793",
        "https://openalex.org/W2912159494",
        "https://openalex.org/W2919407654",
        "https://openalex.org/W2919779463",
        "https://openalex.org/W2921698889",
        "https://openalex.org/W2940785943",
        "https://openalex.org/W2942635578",
        "https://openalex.org/W2944533689",
        "https://openalex.org/W2945675984",
        "https://openalex.org/W2948019083",
        "https://openalex.org/W2952060029",
        "https://openalex.org/W2953091967",
        "https://openalex.org/W2957157287",
        "https://openalex.org/W2957382329",
        "https://openalex.org/W2962684895",
        "https://openalex.org/W2962691117",
        "https://openalex.org/W2962733038",
        "https://openalex.org/W2962748258",
        "https://openalex.org/W2963009535",
        "https://openalex.org/W2963035841",
        "https://openalex.org/W2963138728",
        "https://openalex.org/W2963288612",
        "https://openalex.org/W2963336322",
        "https://openalex.org/W2963427127",
        "https://openalex.org/W2963533436",
        "https://openalex.org/W2963539580",
        "https://openalex.org/W2963576178",
        "https://openalex.org/W2963896360",
        "https://openalex.org/W2963899602",
        "https://openalex.org/W2963988154",
        "https://openalex.org/W2964010262",
        "https://openalex.org/W2964135632",
        "https://openalex.org/W2969519626",
        "https://openalex.org/W2969731533",
        "https://openalex.org/W2969823908",
        "https://openalex.org/W2973289967",
        "https://openalex.org/W2974170798",
        "https://openalex.org/W2981125566",
        "https://openalex.org/W2982361136",
        "https://openalex.org/W2990088730",
        "https://openalex.org/W2990204812",
        "https://openalex.org/W2994008524",
        "https://openalex.org/W2994963916",
        "https://openalex.org/W2995268338",
        "https://openalex.org/W2998280308",
        "https://openalex.org/W2998842060",
        "https://openalex.org/W2999389395",
        "https://openalex.org/W3000577820",
        "https://openalex.org/W3002104503",
        "https://openalex.org/W3004018467",
        "https://openalex.org/W3004236608",
        "https://openalex.org/W3004277316",
        "https://openalex.org/W3004720322",
        "https://openalex.org/W3004946548",
        "https://openalex.org/W3006052720",
        "https://openalex.org/W3006653661",
        "https://openalex.org/W3010601625",
        "https://openalex.org/W3011766871",
        "https://openalex.org/W3016373553",
        "https://openalex.org/W3016657056",
        "https://openalex.org/W3016688072",
        "https://openalex.org/W3019497660",
        "https://openalex.org/W3019532323",
        "https://openalex.org/W3019585464",
        "https://openalex.org/W3022095849",
        "https://openalex.org/W3026439725",
        "https://openalex.org/W3030787901",
        "https://openalex.org/W3032973218",
        "https://openalex.org/W3033237219",
        "https://openalex.org/W3033271745",
        "https://openalex.org/W3033338032",
        "https://openalex.org/W3036252686",
        "https://openalex.org/W3037170119",
        "https://openalex.org/W3040137968",
        "https://openalex.org/W3040741665",
        "https://openalex.org/W3042391361",
        "https://openalex.org/W3042876727",
        "https://openalex.org/W3043932162",
        "https://openalex.org/W3045197138",
        "https://openalex.org/W3045898871",
        "https://openalex.org/W3046105742",
        "https://openalex.org/W3046150221",
        "https://openalex.org/W3048944357",
        "https://openalex.org/W3086892208",
        "https://openalex.org/W3090147969",
        "https://openalex.org/W3092163236",
        "https://openalex.org/W3093728622",
        "https://openalex.org/W3102483098",
        "https://openalex.org/W3102696584",
        "https://openalex.org/W3104458414",
        "https://openalex.org/W3106203633",
        "https://openalex.org/W3106530718",
        "https://openalex.org/W3106758637",
        "https://openalex.org/W3109400146",
        "https://openalex.org/W3112984084",
        "https://openalex.org/W3113018973",
        "https://openalex.org/W3118203807",
        "https://openalex.org/W3121070929",
        "https://openalex.org/W3125516798",
        "https://openalex.org/W3127504873",
        "https://openalex.org/W3127940641",
        "https://openalex.org/W3128146114",
        "https://openalex.org/W3129486236",
        "https://openalex.org/W3132105662",
        "https://openalex.org/W3134033224",
        "https://openalex.org/W3136760479",
        "https://openalex.org/W3152632745",
        "https://openalex.org/W3154816389",
        "https://openalex.org/W3155402977",
        "https://openalex.org/W3156154677",
        "https://openalex.org/W3159886835",
        "https://openalex.org/W3160082076",
        "https://openalex.org/W3162720533",
        "https://openalex.org/W3165716753",
        "https://openalex.org/W3169147363",
        "https://openalex.org/W3176508160",
        "https://openalex.org/W3182369559",
        "https://openalex.org/W3189230021",
        "https://openalex.org/W3191505721",
        "https://openalex.org/W3193427565",
        "https://openalex.org/W3194030134",
        "https://openalex.org/W3195306699",
        "https://openalex.org/W3195746043",
        "https://openalex.org/W3195948154",
        "https://openalex.org/W3198132193",
        "https://openalex.org/W3202150210",
        "https://openalex.org/W3211189031",
        "https://openalex.org/W3213912259",
        "https://openalex.org/W3215773138",
        "https://openalex.org/W4200037328",
        "https://openalex.org/W4200078133",
        "https://openalex.org/W4200490383",
        "https://openalex.org/W4200498111",
        "https://openalex.org/W4200618407",
        "https://openalex.org/W4206462583",
        "https://openalex.org/W4206770380",
        "https://openalex.org/W4210552697",
        "https://openalex.org/W4210555601",
        "https://openalex.org/W4210932344",
        "https://openalex.org/W4224290235",
        "https://openalex.org/W4224885329",
        "https://openalex.org/W4225097017",
        "https://openalex.org/W4225856095",
        "https://openalex.org/W4225925743",
        "https://openalex.org/W4226094091",
        "https://openalex.org/W4226199332",
        "https://openalex.org/W4226379933",
        "https://openalex.org/W4282970004",
        "https://openalex.org/W4283323538",
        "https://openalex.org/W4285105424",
        "https://openalex.org/W4285209996",
        "https://openalex.org/W4285211583",
        "https://openalex.org/W4289520778",
        "https://openalex.org/W4290996181",
        "https://openalex.org/W4291109519",
        "https://openalex.org/W4291238460",
        "https://openalex.org/W4292164052",
        "https://openalex.org/W4292186360",
        "https://openalex.org/W4292387473",
        "https://openalex.org/W4293499257",
        "https://openalex.org/W4293660867",
        "https://openalex.org/W4293863293",
        "https://openalex.org/W4293863302",
        "https://openalex.org/W4294599003",
        "https://openalex.org/W4294741352",
        "https://openalex.org/W4296105750",
        "https://openalex.org/W4296924472",
        "https://openalex.org/W4306160390",
        "https://openalex.org/W4309107718",
        "https://openalex.org/W4309532095",
        "https://openalex.org/W4309995310",
        "https://openalex.org/W4310364078",
        "https://openalex.org/W4312425871",
        "https://openalex.org/W4312460693",
        "https://openalex.org/W4312576847",
        "https://openalex.org/W4312663258",
        "https://openalex.org/W4312691328",
        "https://openalex.org/W4312932456",
        "https://openalex.org/W4313043980",
        "https://openalex.org/W4313123734",
        "https://openalex.org/W4313288712",
        "https://openalex.org/W4313591201",
        "https://openalex.org/W4317438544",
        "https://openalex.org/W4318217967",
        "https://openalex.org/W4319338698",
        "https://openalex.org/W4321789362",
        "https://openalex.org/W4360604201",
        "https://openalex.org/W4361282702",
        "https://openalex.org/W4362653871",
        "https://openalex.org/W4367665843",
        "https://openalex.org/W4378575495",
        "https://openalex.org/W4379984580",
        "https://openalex.org/W4381747944",
        "https://openalex.org/W4384161756",
        "https://openalex.org/W4384787787",
        "https://openalex.org/W4385623557",
        "https://openalex.org/W4385640809",
        "https://openalex.org/W4385801429",
        "https://openalex.org/W4386170213",
        "https://openalex.org/W4386380402",
        "https://openalex.org/W4386464315",
        "https://openalex.org/W4386590944"
      ],
      "openalex_rank": 10,
      "num_tokens": 49714,
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10379625.pdf",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key applications, emerging technologies, regulatory challenges, and future research directions related to the integration of unmanned aerial vehicles in 5G-and-beyond networks?",
      "pipeline_answer": "### Key Applications of UAVs in 5G-and-Beyond Networks\n- **Healthcare Logistics:** UAVs can significantly enhance operational efficiency by reducing delivery times for medical supplies by up to 75%, especially in remote areas. Successful integration requires collaboration among healthcare providers, technology developers, and regulatory bodies to address logistical challenges and ensure compliance with safety standards [S0645706102] [S0425088416].\n- **Cross-Industry Applications:** UAVs are essential in various sectors, including logistics and urban planning, where they can improve service delivery and operational capabilities through partnerships between technology firms and regulatory agencies [S2234189078] [S9413457971].\n\n### Emerging Technologies\n- **Machine Learning:** Recent studies highlight the role of machine learning in enhancing UAV operations, indicating a trend towards more intelligent and autonomous systems within the Internet of Drones (IoD) framework [S0425088416].\n\n### Regulatory Challenges\n- **Need for Adaptive Frameworks:** Current research emphasizes the necessity for adaptive regulatory frameworks that can evolve alongside rapid technological advancements in UAV and 5G technologies. Understanding the mechanisms for regulatory adaptation is crucial for seamless integration [S1035856420] [S9413457971].\n- **Privacy and Safety Concerns:** Ethical considerations include significant privacy issues and potential misuse in surveillance applications, necessitating clear regulatory frameworks that balance innovation with public safety [S1079775045].\n\n### Future Research Directions\n- **Socio-Economic Impacts:** There is a need for interdisciplinary studies to explore the socio-economic impacts of UAV integration across various sectors, particularly in agriculture and emergency services. Understanding public acceptance and trust is vital for successful deployment [S0645706102] [S2234189078].\n- **Security Measures:** Research should focus on developing robust security measures to protect UAV operations within the IoD framework, ensuring safe and secure integration [S1079775045].\n- **Cross-Industry Collaboration Models:** Investigating effective models and mechanisms for facilitating cross-industry collaborations is essential for enhancing UAV integration in 5G-and-beyond networks [S2234189078] [S9413457971].\n\n### Conclusion\nThe integration of UAVs into 5G-and-beyond networks presents significant opportunities for enhancing operational efficiency across various sectors, particularly healthcare. However, addressing regulatory challenges, ethical concerns, and the need for interdisciplinary research on socio-economic impacts is crucial for successful implementation and public acceptance.",
      "pipeline_references": {
        "S1035856420": {
          "id": "S1035856420",
          "text": "Ethical considerations surrounding the deployment of unmanned aerial vehicles (UAVs) in advanced communication networks include significant privacy concerns, safety issues, and the potential for misuse in surveillance applications. A technoethical review emphasizes that while UAVs can enhance operational efficiency, their use raises ethical dilemmas related to data privacy and intrusive surveillance. The study advocates for establishing clear regulatory frameworks that balance innovation with ethical governance to mitigate risks associated with UAV operations in populated areas.",
          "children": [
            {
              "id": "E2430491832",
              "text": "With the rapid expansion in the number of Unmanned Aircraft Vehicles (UAVs) available and the development of modern technologies, the commercial applications of UAVs in urban areas, such as urban remote sensing (RS), express services, urban road traffic monitoring, urban police security, urban air shows and so on, have increased greatly. However, most UAVs, especially light and small civil UAVs, have been operating in low-altitude airspace, and a conflict may exist between increasing the number of UAVs and the limited low airspace. To promote low-altitude airspace resource development and to standardize the operation and management of UAVs in urban regions, some global laws and regulations and key technologies for urban low-altitude applications of UAVs have been implemented. This paper reviews the development of current policies and key technologies concerning safe and efficient operations of the light-and-small civil UAVs in low altitude in urban areas. Discussions are made progressively on measures and methods of airspace restriction, airspace structuring and air route planning in China primarily and the rest of world. After surveying the practical industry tests and the initial studies of air routes, the survey results indicate that the construction of air route networks is a scientific and effective measure to standardize and improve the efficiency of low-altitude UAV operations. From the view point of safety and efficiency, the most valuable direction for UAV regulation in urban regions involves deepening the research which largely relies on urban RS and Geographic Information System (GIS) technology, and application demonstrations of low-altitude public air route networks.",
              "url": "https://openalex.org/W3022095849",
              "openalex_id": "https://openalex.org/W3022095849",
              "title": "Recent Research Progress of Unmanned Aerial Vehicle Regulation Policies and Technologies in Urban Low Altitude",
              "publication_date": "2020-01-01"
            },
            {
              "id": "E4503399428",
              "text": "Until now, every evolution of communication standard was driven by the need for providing high-speed connectivity to the end-user. However, 5G marks a radical shift from this focus as 5G and beyond networks are being designed to be future-proof by catering to diverse requirements of several use cases. These requirements include Ultra-Reliable Low Latency Communications, Massive Machine-Type Communications and Enhanced Mobile Broadband. To realize such features in 5G and beyond, there is a need to rethink how current cellular networks are deployed because designing new radio access technologies and utilizing the new spectrum are not enough. Several technologies, such as software-defined networking, network function virtualization, machine learning and cloud computing, are being integrated into the 5G networks to fulfil the need for diverse requirements. These technologies, however, give rise to several challenges associated with decentralization, transparency, interoperability, privacy and security. To address these issues, Blockchain has emerged as a potential solution due to its capabilities such as transparency, data encryption, auditability, immutability and distributed architecture. In this paper, we review the state-of-art application of Blockchain in 5G network and explore how it can facilitate enabling technologies of 5G and beyond to enable various services at the front-haul, edge and the core. Based on the review, we present a taxonomy of Blockchain application in 5G networks and discuss several issues that can be solved using Blockchain integration. We then present various field-trials and Proof of concept that are using Blockchain to address the challenges faced in the current 5G deployment. Finally, we discuss various challenges that need to be addressed to realize the full potential of Blockchain in beyond 5G networks. The survey presents a broad range of ideas related to Blockchain integration in 5G and beyond networks that address issues such as interoperability, security, mobility, resource allocation, resource sharing and management, energy efficiency and other desirable features.",
              "url": "https://openalex.org/W3035900025",
              "openalex_id": "https://openalex.org/W3035900025",
              "title": "A Review on Application of Blockchain in 5G and Beyond Networks: Taxonomy, Field-Trials, Challenges and Opportunities",
              "publication_date": "2020-01-01"
            },
            {
              "id": "S1079775045",
              "text": "Ethical considerations surrounding the deployment of unmanned aerial vehicles (UAVs) in advanced communication networks include privacy concerns, safety issues, and the potential for misuse in surveillance applications. A technoethical review highlights that while UAVs can enhance operational efficiency, their use raises significant ethical dilemmas related to data privacy and the potential for intrusive surveillance. The study emphasizes the importance of establishing clear regulatory frameworks that balance innovation with ethical governance to mitigate risks associated with UAV operations in populated areas.",
              "children": [
                {
                  "id": "E0419770571",
                  "text": "Surveillance unmanned aerial vehicles (UAVs), commonly known as drones, were traditionally used by militaries in countries like the United States, the United Kingdom, and Australia in their surveillance operations in Iraq, Pakistan, and Afghanistan. In the last five years, the extension of drone use into commercial applications has grown exponentially apart from military and recreational domains. The growth in domestic use has generated public concern and debate mainly surrounding safety and privacy issues. Guided by a technoethical lens (the study of technology\u2019s impact on ethics), a systematic review of commercial drone literature from 2010 to 2015 was conducted to explore social, governance, privacy, and ethical aspects of commercial drone use. Drawing on research, magazine, and newspaper articles, the study identifies the following key areas of social and ethical concern connected with commercial drones: safety, ethics and morals, legality, privacy, air space, informational integrity, humans versus machines, and commercial concerns. The study contributes to technology studies and media ethics research by providing insights into the state of public knowledge concerning commercial drones from an ethical perspective. The outcome of the technoethical review suggests that, although commercial drone use can improve lifestyle and increase efficiency, there is a need to invest more attention to possible negative and unknown consequences to facilitate the ethical use of commercial drones.",
                  "url": "https://openalex.org/W2415159963",
                  "openalex_id": "https://openalex.org/W2415159963",
                  "title": "A technoethical review of commercial drone use in the context of governance, ethics, and privacy",
                  "publication_date": "2016-06-06"
                },
                {
                  "id": "E8683462869",
                  "text": "This article argues that the use of just war theory as the principal framework for ethical assessment of the use of drones for targeted killing is hampered by the absence of a spatial dimension. Drawing on critical political geography, the article develops a concept of \u201cdistant intimacy\u201d that explores the spatial characteristics of the relationship between drone deployers and their targets, revealing that the asymmetry of this relationship extends beyond conventional analysis to establish \u201cdronespace\u201d as a place where the autonomy of the target and the possibility of reciprocity are structurally precluded. This extends ethical critique of drone use beyond established concerns and establishes the importance of space and spatiality to the possibility of ethics in a way that just war theory has, to date, been unable to fully appreciate.",
                  "url": "https://openalex.org/W2086945948",
                  "openalex_id": "https://openalex.org/W2086945948",
                  "title": "Distant Intimacy: Space, Drones, and Just War",
                  "publication_date": "2015-01-01"
                }
              ]
            }
          ]
        },
        "S1079775045": {
          "id": "S1079775045",
          "text": "Ethical considerations surrounding the deployment of unmanned aerial vehicles (UAVs) in advanced communication networks include privacy concerns, safety issues, and the potential for misuse in surveillance applications. A technoethical review highlights that while UAVs can enhance operational efficiency, their use raises significant ethical dilemmas related to data privacy and the potential for intrusive surveillance. The study emphasizes the importance of establishing clear regulatory frameworks that balance innovation with ethical governance to mitigate risks associated with UAV operations in populated areas.",
          "children": [
            {
              "id": "E0419770571",
              "text": "Surveillance unmanned aerial vehicles (UAVs), commonly known as drones, were traditionally used by militaries in countries like the United States, the United Kingdom, and Australia in their surveillance operations in Iraq, Pakistan, and Afghanistan. In the last five years, the extension of drone use into commercial applications has grown exponentially apart from military and recreational domains. The growth in domestic use has generated public concern and debate mainly surrounding safety and privacy issues. Guided by a technoethical lens (the study of technology\u2019s impact on ethics), a systematic review of commercial drone literature from 2010 to 2015 was conducted to explore social, governance, privacy, and ethical aspects of commercial drone use. Drawing on research, magazine, and newspaper articles, the study identifies the following key areas of social and ethical concern connected with commercial drones: safety, ethics and morals, legality, privacy, air space, informational integrity, humans versus machines, and commercial concerns. The study contributes to technology studies and media ethics research by providing insights into the state of public knowledge concerning commercial drones from an ethical perspective. The outcome of the technoethical review suggests that, although commercial drone use can improve lifestyle and increase efficiency, there is a need to invest more attention to possible negative and unknown consequences to facilitate the ethical use of commercial drones.",
              "url": "https://openalex.org/W2415159963",
              "openalex_id": "https://openalex.org/W2415159963",
              "title": "A technoethical review of commercial drone use in the context of governance, ethics, and privacy",
              "publication_date": "2016-06-06"
            },
            {
              "id": "E8683462869",
              "text": "This article argues that the use of just war theory as the principal framework for ethical assessment of the use of drones for targeted killing is hampered by the absence of a spatial dimension. Drawing on critical political geography, the article develops a concept of \u201cdistant intimacy\u201d that explores the spatial characteristics of the relationship between drone deployers and their targets, revealing that the asymmetry of this relationship extends beyond conventional analysis to establish \u201cdronespace\u201d as a place where the autonomy of the target and the possibility of reciprocity are structurally precluded. This extends ethical critique of drone use beyond established concerns and establishes the importance of space and spatiality to the possibility of ethics in a way that just war theory has, to date, been unable to fully appreciate.",
              "url": "https://openalex.org/W2086945948",
              "openalex_id": "https://openalex.org/W2086945948",
              "title": "Distant Intimacy: Space, Drones, and Just War",
              "publication_date": "2015-01-01"
            }
          ]
        },
        "S0645706102": {
          "id": "S0645706102",
          "text": "The integration of unmanned aerial vehicles (UAVs) into healthcare logistics can significantly enhance operational efficiency and patient outcomes, particularly in remote areas. A study indicates that UAVs can reduce delivery times for medical supplies by up to 75%, thereby improving access to critical healthcare services. This integration necessitates collaboration across various sectors, including healthcare providers, technology developers, and regulatory bodies, to address logistical challenges and ensure compliance with safety standards. Furthermore, the socio-economic implications of UAV deployment in healthcare highlight the need for public acceptance and trust, which are crucial for successful implementation.",
          "children": [
            {
              "id": "E5956443688",
              "text": "The integration of drones into health care as a supplement to existing logistics methods may generate a need for cooperation and involvement across multiple resource areas. It is currently not well understood whether such integrations would merely represent a technical implementation or if they would cause more significant changes to laboratory services. By choosing socio-technical theory as the theoretical lens, this paper intends to harvest knowledge from the literature on various organizational concepts and examine possible synergies between such theories to determine optimal strategies for introducing the use of drones in a health care context. Our particular interest is to examine whether the insights generated from the multi-level perspective (MLP) may have the potential to create dynamic spin-offs related to the organizational transitions associated with the implementation of drones in health services. We built our study on a scoping literature review of topics associated with the MLP and socio-technical studies from differing arenas, supplemented with studies harvested on a broader basis. The scoping review is based on 25 articles that were selected for analysis. As a way of organizing the literature, the niche, regime, and landscape levels of the MLP are translated to the corresponding health care-related terms, i.e., clinic, institution, and health care system. Furthermore, subcategories emerged inductively during the process of analysis. The MLP provides essential knowledge regarding the context for innovation and how the interaction between the different levels can accelerate the diffusion of innovations. Several authors have put both ethical topics and public acceptance into a socio-technological perspective. Although a socio-technical approach is not needed to operate drones, it may help in the long run to invest in a culture that is open to innovation and change.",
              "url": "https://openalex.org/W4210726806",
              "openalex_id": "https://openalex.org/W4210726806",
              "title": "A Socio-Analytical Approach to the Integration of Drones into Health Care Systems",
              "publication_date": "2022-01-26"
            },
            {
              "id": "E4204669225",
              "text": "As an emerging field of aerial robotics, Unmanned Aerial Vehicles (UAVs) have gained significant research interest within the wireless networking research community. As soon as national legislations allow UAVs to fly autonomously, we will see swarms of UAV populating the sky of our smart cities to accomplish different missions: parcel delivery, infrastructure monitoring, event filming, surveillance, tracking, etc. The UAV ecosystem can benefit from 5G/B5G cellular networks, which can be exploited in different ways to enhance UAV communications. Because of the inherent characteristics of UAV pertaining to flexible mobility in 3D space, autonomous operation and intelligent placement, these smart devices cater to wide range of wireless applications and use cases. This work aims at presenting an in-depth exploration of integration synergies between 5G/B5G cellular systems and UAV technology, where the UAV is integrated as a new aerial User Equipment (UE) to already deployed cellular networks. In this integration, the UAVs perform the role of flying users within cellular coverage, thus they are termed as cellular-connected UAVs. The main focus of this work is to present an extensive study of integration challenges along with key 5G/B5G technological innovations and ongoing efforts in design prototyping and field trials corroborating cellular-connected UAVs. This study highlights recent progress updates with respect to 3GPP standardization and emphasizes socio-economic concerns that must be accounted before successful adoption of this promising technology. Various open problems paving the path to future research opportunities are also discussed.",
              "url": "https://openalex.org/W3047778227",
              "openalex_id": "https://openalex.org/W3047778227",
              "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
              "publication_date": "2020-08-07"
            }
          ]
        },
        "S9413457971": {
          "id": "S9413457971",
          "text": "Cross-industry collaborations are essential for the successful integration of unmanned aerial vehicles (UAVs) in 5G-and-beyond networks, particularly in sectors such as logistics, healthcare, and urban planning. Research indicates that partnerships between technology firms, regulatory agencies, and industry stakeholders can facilitate the development of standardized protocols and best practices, thereby enhancing the operational capabilities of UAVs. For instance, collaborative efforts in the healthcare sector have shown that UAVs can effectively deliver medical supplies, but require coordinated logistics and regulatory compliance to ensure safety and efficiency.",
          "children": [
            {
              "id": "E6253876701",
              "text": "What will the future of UAV cellular communications be? In this tutorial article, we address such a compelling yet difficult question by embarking on a journey from 5G to 6G and expounding a large number of case studies supported by original results. We start by overviewing the status quo on UAV communications from an industrial standpoint, providing fresh updates from the 3GPP and detailing new 5G NR features in support of aerial devices. We then dissect the potential and the limitations of such features. In particular, we demonstrate how sub-6 GHz massive MIMO can successfully tackle cell selection and interference challenges, we showcase encouraging mmWave coverage evaluations in both urban and suburban/rural settings, and we examine the peculiarities of direct device-todevice communications in the sky. Moving on, we sneak a peek at next-generation UAV communications, listing some of the use cases envisioned for the 2030s. We identify the most promising 6G enablers for UAV communication, those expected to take the performance and reliability to the next level. For each of these disruptive new paradigms (non-terrestrial networks, cellfree architectures, artificial intelligence, reconfigurable intelligent surfaces, and THz communications), we gauge the prospective benefits for UAVs and discuss the main technological hurdles that stand in the way. All along, we distil our numerous findings into essential takeaways, and we identify key open problems worthy of further study.",
              "url": "https://openalex.org/W3162720533",
              "openalex_id": "https://openalex.org/W3162720533",
              "title": "What Will the Future of UAV Cellular Communications Be? A Flight From 5G to 6G",
              "publication_date": "2022-01-01"
            },
            {
              "id": "E4503399428",
              "text": "Until now, every evolution of communication standard was driven by the need for providing high-speed connectivity to the end-user. However, 5G marks a radical shift from this focus as 5G and beyond networks are being designed to be future-proof by catering to diverse requirements of several use cases. These requirements include Ultra-Reliable Low Latency Communications, Massive Machine-Type Communications and Enhanced Mobile Broadband. To realize such features in 5G and beyond, there is a need to rethink how current cellular networks are deployed because designing new radio access technologies and utilizing the new spectrum are not enough. Several technologies, such as software-defined networking, network function virtualization, machine learning and cloud computing, are being integrated into the 5G networks to fulfil the need for diverse requirements. These technologies, however, give rise to several challenges associated with decentralization, transparency, interoperability, privacy and security. To address these issues, Blockchain has emerged as a potential solution due to its capabilities such as transparency, data encryption, auditability, immutability and distributed architecture. In this paper, we review the state-of-art application of Blockchain in 5G network and explore how it can facilitate enabling technologies of 5G and beyond to enable various services at the front-haul, edge and the core. Based on the review, we present a taxonomy of Blockchain application in 5G networks and discuss several issues that can be solved using Blockchain integration. We then present various field-trials and Proof of concept that are using Blockchain to address the challenges faced in the current 5G deployment. Finally, we discuss various challenges that need to be addressed to realize the full potential of Blockchain in beyond 5G networks. The survey presents a broad range of ideas related to Blockchain integration in 5G and beyond networks that address issues such as interoperability, security, mobility, resource allocation, resource sharing and management, energy efficiency and other desirable features.",
              "url": "https://openalex.org/W3035900025",
              "openalex_id": "https://openalex.org/W3035900025",
              "title": "A Review on Application of Blockchain in 5G and Beyond Networks: Taxonomy, Field-Trials, Challenges and Opportunities",
              "publication_date": "2020-01-01"
            },
            {
              "id": "S2234189078",
              "text": "Cross-industry collaborations are essential for the successful integration of unmanned aerial vehicles (UAVs) in 5G-and-beyond networks, particularly in sectors such as logistics, healthcare, and urban planning. Research indicates that partnerships between technology firms, regulatory agencies, and industry stakeholders can facilitate the development of standardized protocols and best practices, thereby enhancing the operational capabilities of UAVs. For instance, collaborative efforts in the healthcare sector have demonstrated that UAVs can effectively deliver medical supplies, but require coordinated logistics and regulatory compliance to ensure safety and efficiency.",
              "children": [
                {
                  "id": "E4204669225",
                  "text": "As an emerging field of aerial robotics, Unmanned Aerial Vehicles (UAVs) have gained significant research interest within the wireless networking research community. As soon as national legislations allow UAVs to fly autonomously, we will see swarms of UAV populating the sky of our smart cities to accomplish different missions: parcel delivery, infrastructure monitoring, event filming, surveillance, tracking, etc. The UAV ecosystem can benefit from 5G/B5G cellular networks, which can be exploited in different ways to enhance UAV communications. Because of the inherent characteristics of UAV pertaining to flexible mobility in 3D space, autonomous operation and intelligent placement, these smart devices cater to wide range of wireless applications and use cases. This work aims at presenting an in-depth exploration of integration synergies between 5G/B5G cellular systems and UAV technology, where the UAV is integrated as a new aerial User Equipment (UE) to already deployed cellular networks. In this integration, the UAVs perform the role of flying users within cellular coverage, thus they are termed as cellular-connected UAVs. The main focus of this work is to present an extensive study of integration challenges along with key 5G/B5G technological innovations and ongoing efforts in design prototyping and field trials corroborating cellular-connected UAVs. This study highlights recent progress updates with respect to 3GPP standardization and emphasizes socio-economic concerns that must be accounted before successful adoption of this promising technology. Various open problems paving the path to future research opportunities are also discussed.",
                  "url": "https://openalex.org/W3047778227",
                  "openalex_id": "https://openalex.org/W3047778227",
                  "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
                  "publication_date": "2020-08-07"
                },
                {
                  "id": "E5956443688",
                  "text": "The integration of drones into health care as a supplement to existing logistics methods may generate a need for cooperation and involvement across multiple resource areas. It is currently not well understood whether such integrations would merely represent a technical implementation or if they would cause more significant changes to laboratory services. By choosing socio-technical theory as the theoretical lens, this paper intends to harvest knowledge from the literature on various organizational concepts and examine possible synergies between such theories to determine optimal strategies for introducing the use of drones in a health care context. Our particular interest is to examine whether the insights generated from the multi-level perspective (MLP) may have the potential to create dynamic spin-offs related to the organizational transitions associated with the implementation of drones in health services. We built our study on a scoping literature review of topics associated with the MLP and socio-technical studies from differing arenas, supplemented with studies harvested on a broader basis. The scoping review is based on 25 articles that were selected for analysis. As a way of organizing the literature, the niche, regime, and landscape levels of the MLP are translated to the corresponding health care-related terms, i.e., clinic, institution, and health care system. Furthermore, subcategories emerged inductively during the process of analysis. The MLP provides essential knowledge regarding the context for innovation and how the interaction between the different levels can accelerate the diffusion of innovations. Several authors have put both ethical topics and public acceptance into a socio-technological perspective. Although a socio-technical approach is not needed to operate drones, it may help in the long run to invest in a culture that is open to innovation and change.",
                  "url": "https://openalex.org/W4210726806",
                  "openalex_id": "https://openalex.org/W4210726806",
                  "title": "A Socio-Analytical Approach to the Integration of Drones into Health Care Systems",
                  "publication_date": "2022-01-26"
                }
              ]
            }
          ]
        },
        "S0425088416": {
          "id": "S0425088416",
          "text": "The deployment of unmanned aerial vehicles (UAVs) in healthcare logistics has demonstrated significant socio-economic benefits, particularly in remote areas. A recent study indicates that UAVs can reduce delivery times for medical supplies by up to 75%, enhancing access to critical healthcare services. This integration requires collaboration among healthcare providers, technology developers, and regulatory bodies to address logistical challenges and ensure compliance with safety standards. Furthermore, public acceptance and trust are crucial for successful implementation, highlighting the socio-economic implications of UAV deployment in healthcare.",
          "children": [
            {
              "id": "E4204669225",
              "text": "As an emerging field of aerial robotics, Unmanned Aerial Vehicles (UAVs) have gained significant research interest within the wireless networking research community. As soon as national legislations allow UAVs to fly autonomously, we will see swarms of UAV populating the sky of our smart cities to accomplish different missions: parcel delivery, infrastructure monitoring, event filming, surveillance, tracking, etc. The UAV ecosystem can benefit from 5G/B5G cellular networks, which can be exploited in different ways to enhance UAV communications. Because of the inherent characteristics of UAV pertaining to flexible mobility in 3D space, autonomous operation and intelligent placement, these smart devices cater to wide range of wireless applications and use cases. This work aims at presenting an in-depth exploration of integration synergies between 5G/B5G cellular systems and UAV technology, where the UAV is integrated as a new aerial User Equipment (UE) to already deployed cellular networks. In this integration, the UAVs perform the role of flying users within cellular coverage, thus they are termed as cellular-connected UAVs. The main focus of this work is to present an extensive study of integration challenges along with key 5G/B5G technological innovations and ongoing efforts in design prototyping and field trials corroborating cellular-connected UAVs. This study highlights recent progress updates with respect to 3GPP standardization and emphasizes socio-economic concerns that must be accounted before successful adoption of this promising technology. Various open problems paving the path to future research opportunities are also discussed.",
              "url": "https://openalex.org/W3047778227",
              "openalex_id": "https://openalex.org/W3047778227",
              "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
              "publication_date": "2020-08-07"
            },
            {
              "id": "E9546465219",
              "text": "This report investigates the role of drones as part of the future transport mix. It specifically addresses the issues policy makers face in engaging with the emerging private drone sector. Drones have the potential to improve existing practices, for instance in the surveying of infrastructure. They also have innovative uses in areas such as freight delivery, passenger transport in both urban and rural areas, or in disaster relief. With the sector developing at a rapid pace, regulators will want to create frameworks for drone use that allow innovation while ensuring positive overall outcomes.",
              "url": "https://openalex.org/W4232466264",
              "openalex_id": "https://openalex.org/W4232466264",
              "title": "(Un)certain Skies?",
              "publication_date": "2018-05-21"
            },
            {
              "id": "S0645706102",
              "text": "The integration of unmanned aerial vehicles (UAVs) into healthcare logistics can significantly enhance operational efficiency and patient outcomes, particularly in remote areas. A study indicates that UAVs can reduce delivery times for medical supplies by up to 75%, thereby improving access to critical healthcare services. This integration necessitates collaboration across various sectors, including healthcare providers, technology developers, and regulatory bodies, to address logistical challenges and ensure compliance with safety standards. Furthermore, the socio-economic implications of UAV deployment in healthcare highlight the need for public acceptance and trust, which are crucial for successful implementation.",
              "children": [
                {
                  "id": "E5956443688",
                  "text": "The integration of drones into health care as a supplement to existing logistics methods may generate a need for cooperation and involvement across multiple resource areas. It is currently not well understood whether such integrations would merely represent a technical implementation or if they would cause more significant changes to laboratory services. By choosing socio-technical theory as the theoretical lens, this paper intends to harvest knowledge from the literature on various organizational concepts and examine possible synergies between such theories to determine optimal strategies for introducing the use of drones in a health care context. Our particular interest is to examine whether the insights generated from the multi-level perspective (MLP) may have the potential to create dynamic spin-offs related to the organizational transitions associated with the implementation of drones in health services. We built our study on a scoping literature review of topics associated with the MLP and socio-technical studies from differing arenas, supplemented with studies harvested on a broader basis. The scoping review is based on 25 articles that were selected for analysis. As a way of organizing the literature, the niche, regime, and landscape levels of the MLP are translated to the corresponding health care-related terms, i.e., clinic, institution, and health care system. Furthermore, subcategories emerged inductively during the process of analysis. The MLP provides essential knowledge regarding the context for innovation and how the interaction between the different levels can accelerate the diffusion of innovations. Several authors have put both ethical topics and public acceptance into a socio-technological perspective. Although a socio-technical approach is not needed to operate drones, it may help in the long run to invest in a culture that is open to innovation and change.",
                  "url": "https://openalex.org/W4210726806",
                  "openalex_id": "https://openalex.org/W4210726806",
                  "title": "A Socio-Analytical Approach to the Integration of Drones into Health Care Systems",
                  "publication_date": "2022-01-26"
                },
                {
                  "id": "E4204669225",
                  "text": "As an emerging field of aerial robotics, Unmanned Aerial Vehicles (UAVs) have gained significant research interest within the wireless networking research community. As soon as national legislations allow UAVs to fly autonomously, we will see swarms of UAV populating the sky of our smart cities to accomplish different missions: parcel delivery, infrastructure monitoring, event filming, surveillance, tracking, etc. The UAV ecosystem can benefit from 5G/B5G cellular networks, which can be exploited in different ways to enhance UAV communications. Because of the inherent characteristics of UAV pertaining to flexible mobility in 3D space, autonomous operation and intelligent placement, these smart devices cater to wide range of wireless applications and use cases. This work aims at presenting an in-depth exploration of integration synergies between 5G/B5G cellular systems and UAV technology, where the UAV is integrated as a new aerial User Equipment (UE) to already deployed cellular networks. In this integration, the UAVs perform the role of flying users within cellular coverage, thus they are termed as cellular-connected UAVs. The main focus of this work is to present an extensive study of integration challenges along with key 5G/B5G technological innovations and ongoing efforts in design prototyping and field trials corroborating cellular-connected UAVs. This study highlights recent progress updates with respect to 3GPP standardization and emphasizes socio-economic concerns that must be accounted before successful adoption of this promising technology. Various open problems paving the path to future research opportunities are also discussed.",
                  "url": "https://openalex.org/W3047778227",
                  "openalex_id": "https://openalex.org/W3047778227",
                  "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
                  "publication_date": "2020-08-07"
                }
              ]
            }
          ]
        },
        "S2234189078": {
          "id": "S2234189078",
          "text": "Cross-industry collaborations are essential for the successful integration of unmanned aerial vehicles (UAVs) in 5G-and-beyond networks, particularly in sectors such as logistics, healthcare, and urban planning. Research indicates that partnerships between technology firms, regulatory agencies, and industry stakeholders can facilitate the development of standardized protocols and best practices, thereby enhancing the operational capabilities of UAVs. For instance, collaborative efforts in the healthcare sector have demonstrated that UAVs can effectively deliver medical supplies, but require coordinated logistics and regulatory compliance to ensure safety and efficiency.",
          "children": [
            {
              "id": "E4204669225",
              "text": "As an emerging field of aerial robotics, Unmanned Aerial Vehicles (UAVs) have gained significant research interest within the wireless networking research community. As soon as national legislations allow UAVs to fly autonomously, we will see swarms of UAV populating the sky of our smart cities to accomplish different missions: parcel delivery, infrastructure monitoring, event filming, surveillance, tracking, etc. The UAV ecosystem can benefit from 5G/B5G cellular networks, which can be exploited in different ways to enhance UAV communications. Because of the inherent characteristics of UAV pertaining to flexible mobility in 3D space, autonomous operation and intelligent placement, these smart devices cater to wide range of wireless applications and use cases. This work aims at presenting an in-depth exploration of integration synergies between 5G/B5G cellular systems and UAV technology, where the UAV is integrated as a new aerial User Equipment (UE) to already deployed cellular networks. In this integration, the UAVs perform the role of flying users within cellular coverage, thus they are termed as cellular-connected UAVs. The main focus of this work is to present an extensive study of integration challenges along with key 5G/B5G technological innovations and ongoing efforts in design prototyping and field trials corroborating cellular-connected UAVs. This study highlights recent progress updates with respect to 3GPP standardization and emphasizes socio-economic concerns that must be accounted before successful adoption of this promising technology. Various open problems paving the path to future research opportunities are also discussed.",
              "url": "https://openalex.org/W3047778227",
              "openalex_id": "https://openalex.org/W3047778227",
              "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
              "publication_date": "2020-08-07"
            },
            {
              "id": "E5956443688",
              "text": "The integration of drones into health care as a supplement to existing logistics methods may generate a need for cooperation and involvement across multiple resource areas. It is currently not well understood whether such integrations would merely represent a technical implementation or if they would cause more significant changes to laboratory services. By choosing socio-technical theory as the theoretical lens, this paper intends to harvest knowledge from the literature on various organizational concepts and examine possible synergies between such theories to determine optimal strategies for introducing the use of drones in a health care context. Our particular interest is to examine whether the insights generated from the multi-level perspective (MLP) may have the potential to create dynamic spin-offs related to the organizational transitions associated with the implementation of drones in health services. We built our study on a scoping literature review of topics associated with the MLP and socio-technical studies from differing arenas, supplemented with studies harvested on a broader basis. The scoping review is based on 25 articles that were selected for analysis. As a way of organizing the literature, the niche, regime, and landscape levels of the MLP are translated to the corresponding health care-related terms, i.e., clinic, institution, and health care system. Furthermore, subcategories emerged inductively during the process of analysis. The MLP provides essential knowledge regarding the context for innovation and how the interaction between the different levels can accelerate the diffusion of innovations. Several authors have put both ethical topics and public acceptance into a socio-technological perspective. Although a socio-technical approach is not needed to operate drones, it may help in the long run to invest in a culture that is open to innovation and change.",
              "url": "https://openalex.org/W4210726806",
              "openalex_id": "https://openalex.org/W4210726806",
              "title": "A Socio-Analytical Approach to the Integration of Drones into Health Care Systems",
              "publication_date": "2022-01-26"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W4386223811",
        "https://openalex.org/W4306393483",
        "https://openalex.org/W4380887507",
        "https://openalex.org/W4387533110",
        "https://openalex.org/W4389352644",
        "https://openalex.org/W4319988682",
        "https://openalex.org/W4293736052",
        "https://openalex.org/W2415159963",
        "https://openalex.org/W4310130661",
        "https://openalex.org/W4207001164",
        "https://openalex.org/W3035900025",
        "https://openalex.org/W4285298660",
        "https://openalex.org/W3202150210",
        "https://openalex.org/W3156154677",
        "https://openalex.org/W4200323219",
        "https://openalex.org/W3162720533",
        "https://openalex.org/W4225727203",
        "https://openalex.org/W3050374582",
        "https://openalex.org/W3186285124",
        "https://openalex.org/W3024364604",
        "https://openalex.org/W4387327827",
        "https://openalex.org/W4210726806",
        "https://openalex.org/W2966996929",
        "https://openalex.org/W4385067438",
        "https://openalex.org/W4363672396",
        "https://openalex.org/W2026072405",
        "https://openalex.org/W3193596395",
        "https://openalex.org/W4388295271",
        "https://openalex.org/W4386415974",
        "https://openalex.org/W4385900995",
        "https://openalex.org/W4386380402",
        "https://openalex.org/W4291109519",
        "https://openalex.org/W4386824964",
        "https://openalex.org/W3022095849",
        "https://openalex.org/W2968746919",
        "https://openalex.org/W4310062079",
        "https://openalex.org/W4293317302",
        "https://openalex.org/W4382790294",
        "https://openalex.org/W4206462583",
        "https://openalex.org/W3112341537",
        "https://openalex.org/W3023617420",
        "https://openalex.org/W2921698889",
        "https://openalex.org/W4232466264",
        "https://openalex.org/W4292186360",
        "https://openalex.org/W2086945948",
        "https://openalex.org/W3087948960",
        "https://openalex.org/W3047778227"
      ],
      "evaluation": {
        "precision@10": 0.0,
        "recall@10": 0.0,
        "f1@10": 0.0,
        "rouge_1": 0.016613557752539695,
        "rouge_2": 0.008225520904262564,
        "rouge_l": 0.01040390010077076,
        "text_f1": 0.07032776210115042,
        "num_source_papers": 47
      }
    },
    {
      "id": "https://openalex.org/W4391065733",
      "limited_meta": {
        "title": "Computational Pathology: A Survey Review and The Way Forward",
        "publication_date": "2024-01-21",
        "cited_by_count": 13,
        "url": ""
      },
      "text": "Computational Pathology: A Survey Review and The Way Forward\nMahdi S. Hosseinia,\u2217, Babak Ehteshami Bejnordib,1, Vincent Quoc-Huy Trinhc, Lyndon Chand, Danial Hasand, Xingwen Lid,\nStephen Yangd, Taehyo Kimd, Haochen Zhangd, Theodore Wud, Kajanan Chinniahd, Sina Maghsoudloua, Ryan Zhangd,\nJiadai Zhud, Samir Khakid, Andrei Buine, Fatemeh Chajia, Ala Salehif, Bich Ngoc Nguyeng,\nDimitris Samarash, Konstantinos N. Plataniotisd\naDepartment of Computer Science and Software Engineering (CSSE), Concordia Univeristy, Montreal, QC H3H 2R9, Canada\nbQualcomm AI Research, Qualcomm Technologies Netherlands B.V., Amsterdam, The Netherlands\nc\nInstitute for Research in Immunology and Cancer of the University of Montreal, Montreal, QC H3T 1J4, Canada\ndThe Edward S. Rogers Sr. Department of Electrical & Computer Engineering (ECE), University of Toronto, Toronto, ON M5S 3G4, Canada\neHuron Digitial Pathology, St. Jacobs, ON N0B 2N0, Canada\nfDepartment of Electrical and Computer Engineering, University of New Brunswick, Fredericton, NB E3B 5A3, Canada\ngUniversity of Montreal Hospital Center, Montreal, QC H2X 0C2, Canada\nhDepartment of Computer Science, Stony Brook University, Stony Brook, NY 11794, United States\nThis work is dedicated to the beloved memories of Kuanhou Fang, Shahnaz Habibpanah, Zakiyeh Khaliji-Oskoui, James Liang,\nMahsa MohammadiMoghaddam, Vily Panoutsakopoulou, Athanasia Samara, Huoyuan Yu, Dexi Zhang\nand all people around the world who have lost their lives because of cancer\nAbstract. Computational Pathology (CPath) is an interdisciplinary science that augments developments of computational\napproaches to analyze and model medical histopathology images. The main objective for CPath is to develop infrastructure and\nworkflows of digital diagnostics as an assistive CAD system for clinical pathology, facilitating transformational changes in the\ndiagnosis and treatment of cancer that are mainly address by CPath tools. With evergrowing developments in deep learning\nand computer vision algorithms, and the ease of the data flow from digital pathology, currently CPath is witnessing a paradigm\nshift. Despite the sheer volume of engineering and scientific works being introduced for cancer image analysis, there is still a\nconsiderable gap of adopting and integrating these algorithms in clinical practice. This raises a significant question regarding the\ndirection and trends that are undertaken in CPath. In this article we provide a comprehensive review of more than 700 papers to\naddress the challenges faced in problem design all-the-way to the application and implementation viewpoints. We have catalogued\neach paper into a model-card by examining the key works and challenges faced to layout the current landscape in CPath. We hope\nthis helps the community to locate relevant works and facilitate understanding of the field\u2019s future directions. In a nutshell, we\noversee the CPath developments in cycle of stages which are required to be cohesively linked together to address the challenges\nassociated with such multidisciplinary science. We overview this cycle from different perspectives of data-centric, model-centric,\nand application-centric problems. We finally sketch remaining challenges and provide directions for future technical developments\nand clinical integration of CPath. For updated information on this survey review paper and accessing to the original model cards\nrepository, please refer to GitHub.\nKeywords: digital pathology, whole slide image (WSI), deep learning, computer aided diagnosis (CAD), clinical pathology, survey\nContents\n1 Introduction 2\n2 Clinical Applications for CPath 4\n2.1 Clinical Pathology Workflow . . . . . . . . . . . . . . 4\n2.2 Diagnostic Tasks . . . . . . . . . . . . . . . . . . . . 5\n2.3 Prognosis . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.4 Prediction of treatment response . . . . . . . . . . . . 6\n2.5 Organs and Diseases . . . . . . . . . . . . . . . . . . 6\n3 Data Collection for CPath 9\n3.1 Tissue Slide Preparation . . . . . . . . . . . . . . . . 10\n\u2217Corresponding author.\nE-mail address: mahdi.hosseini@concordia.ca\n1Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n3.2 Whole Slide Imaging (WSI) . . . . . . . . . . . . . . 10\n3.3 Cohort Selection, Scale, and Challenges . . . . . . . . 11\n4 Domain Expert Knowledge Annotation 13\n4.1 Supervised Annotation . . . . . . . . . . . . . . . . . 13\n4.2 Optimum Labeling Workflow Design . . . . . . . . . . 15\n5 Model Learning for CPath 16\n5.1 Classification Architectures . . . . . . . . . . . . . . . 17\n5.2 Segmentation Architectures . . . . . . . . . . . . . . . 17\n5.3 Object Detection Architectures . . . . . . . . . . . . . 17\n5.4 Multi-Task Learning . . . . . . . . . . . . . . . . . . 18\n5.5 Multi-Modal Learning . . . . . . . . . . . . . . . . . 18\n5.6 Vision-Language Models . . . . . . . . . . . . . . . . 18\n5.7 Sequential Models . . . . . . . . . . . . . . . . . . . 19\n5.8 Synthetic Data and Generative Models . . . . . . . . . 19\n5.9 Multi-Instance Learning (MIL) Models . . . . . . . . 19\narXiv:2304.05482v3 [eess.IV] 27 Jan 2024\n2\n5.10 Contrastive Self-Supervised Learning for Few-Shot\nGeneralization . . . . . . . . . . . . . . . . . . . . . . 20\n5.11 Novel CPath Architectures . . . . . . . . . . . . . . . 20\n5.12 Model Comparison . . . . . . . . . . . . . . . . . . . 20\n6 Evaluation and Regulations 21\n6.1 Clinical Validation . . . . . . . . . . . . . . . . . . . 21\n6.2 FDA Regulations . . . . . . . . . . . . . . . . . . . . 22\n7 Emerging Trends in CPath Research 22\n7.1 Contrastive Self-Supervised Learning becomes Main\u0002stream . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n7.2 Prediction becoming increasingly High-Level . . . . . 22\n7.3 Spatial and Hierarchical Relationships receiving At\u0002tention . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n7.4 Vision-Language Models for Explainable Predictions . 22\n7.5 Synthetic Data now Realistic Enough . . . . . . . . . 23\n8 Existing Challenges and Future Opportunities 23\n8.1 CPath as Anomaly Detection . . . . . . . . . . . . . . 23\n8.2 Leveraging Existing Datasets . . . . . . . . . . . . . . 23\n8.3 Creating New Datasets . . . . . . . . . . . . . . . . . 24\n8.4 Pre- and Post-Analytical CAD Tools . . . . . . . . . . 24\n8.5 Multi Domain Learning . . . . . . . . . . . . . . . . . 24\n8.6 Federated Learning for Multi-Central CPath . . . . . . 24\n8.7 CPath-specific Architecture Designs . . . . . . . . . . 25\n8.8 Digital and Computational Pathology Adoption . . . . 25\n8.9 Institutional Challenges . . . . . . . . . . . . . . . . . 26\n8.10 Clinical Alignment of CPath Tasks . . . . . . . . . . . 26\n8.11 Concluding Remarks . . . . . . . . . . . . . . . . . . 26\n9 Appendix 51\n9.1 Clinical Pathology Workflow . . . . . . . . . . . . . . 51\n9.2 Diagnostic Tasks . . . . . . . . . . . . . . . . . . . . 52\n9.3 Prognosis . . . . . . . . . . . . . . . . . . . . . . . . 53\n9.4 Prediction of Treatment Response . . . . . . . . . . . 54\n9.5 Cancer Statistics . . . . . . . . . . . . . . . . . . . . . 54\n9.6 Whole Slide Imaging . . . . . . . . . . . . . . . . . . 55\n9.7 Organs and Diseases . . . . . . . . . . . . . . . . . . 56\n9.8 Ground Truth Labelling and Annotation . . . . . . . . 58\n9.9 Surveyed Datasets . . . . . . . . . . . . . . . . . . . . 59\n9.9.1 Table Creation Details . . . . . . . . . . . . . 59\n9.10 Organ Overview . . . . . . . . . . . . . . . . . . . . . 59\n9.11 Technicalities by Task . . . . . . . . . . . . . . . . . . 59\n9.12 Model Card Categorization . . . . . . . . . . . . . . . 81\n9.12.1 Template . . . . . . . . . . . . . . . . . . . . 81\n9.12.2 Samples . . . . . . . . . . . . . . . . . . . . . 82\n1. Introduction\nApril 2017 marked a turning point for digital pathology when\nthe Philips IntelliSite digital scanner received the US Food &\nDrugs Administration (FDA) approval (with limited use case)\nfor diagnostic applications in clinical pathology [1, 2]. A sub\u0002sequent validation guideline was created to help ensure the pro\u0002duced Whole Slide Image (WSI) scans could be used in clin\u0002ical settings without compromising patient care, while main\u0002taining similar results to the current gold standard of optical\nmicroscopy [3, 4, 5, 6]. The use of WSIs offers significant ad\u0002vantages to the pathologist\u2019s workflow: digitally captured im\u0002ages, compared to tissue slides, are immune from accidental\nphysical damage and maintain their quality over time [7, 8].\nClinics and practices can share and store these high-resolution\nimages digitally enabling asynchronous viewing/collaboration\nworldwide [9, 10]. The development of digital pathology shows\ngreat promise as a framework to improve work efficiency in the\npractice of pathology [11, 10]. Adopting a digital workflow also\nopens immense opportunities for using computational methods\nto augment and expedite their workflow\u2013the field of Computa\u0002tional Pathology (CPath) is dedicated to researching and devel\u0002oping these methods [12, 13, 14, 15, 16, 17].\nHowever, despite the aforementioned advantages, the adop\u0002tion of digital pathology, and hence computational pathology,\nhas been slow. Some pathologists consider the analysis of WSIs\nas opposed to glass slides as an unnecessary change in their\nworkflow [18, 19, 9, 20] and recent surveys indicate that the\nswitch to digital pathology does not provide enough financial\nincentive [21, 22, 23, 24, 25, 8]. This is where advances from\nCPath can address or overpower many of the concerns in adopt\u0002ing a digital workflow. For example, CPath models to identify\nmorphological features that correlate with breast cancer [26]\nprovide substantial benefits to clinical accuracy. Further, CPath\nmodels that identify lymph node metastases with better sensi\u0002tivity while reducing diagnostic time [27] can streamline work\u0002flows to increase pathologist throughput and generate more rev\u0002enue [28, 29].\nSimilar to digital pathology, the adoption of CPath methods\nhas also lagged despite the many benefits it offers to improve\nefficiency and accuracy in pathology [30, 31, 2, 32]. This lack\nof adoption and integration into clinical practice raises a sig\u0002nificant question regarding the direction and trends of current\nwork in CPath. This survey looks to review the field of CPath\nin a systematic fashion by breaking down the various steps in\u0002volved in a CPath workflow and categorizing CPath works to\nboth determine trends in the field and provide a resource for the\ncommunity to reference when creating new works.\nExisting survey papers in the field of CPath can be clus\u0002tered into a few groups. The first focuses on the design and\napplications of smart diagnosis tools [33, 34, 35, 36, 37, 38,\n17, 16, 15, 39, 40, 41, 42, 43]. These works focus on de\u0002signing novel architectures for artificial intelligence (AI) mod\u0002els with regards to specific clinical tasks, although they may\nbriefly discuss clinical challenges and limitations. A sec\u0002ond group of works focus on clinical barriers for AI integra\u0002tion discussing specific certifications and regulations required\nfor the development of medical devices under clinical settings\n[44, 45, 46, 47, 48, 49]. Lastly, the final group focuses on both\nthe design and the integration of AI tools with clinical applica\u0002tions [50, 51, 12, 13, 52, 29, 53, 54, 14, 55, 56]. These works\nspeak to both the computer vision and pathology communities\nin developing machine learning (ML) models that can satisfy\nclinical use cases.\nOur work is situated in this final group as we breakdown the\nend-to-end CPath workflow into stages and systematically re\u0002view works related to and addressing those stages. We oversee\nthis as a workflow for CPath research that breaks down the pro\u0002cess of problem definition, data collection, model creation, and\nclinical validation into a cycle of stages. A visual representation\nof this cycle is provided in Figure 1. We review over 700 papers\n3\nFig. 1: We divide the data science workflow for pathology into multiple stages,\nwherein each brings a different level of experience. For example, the anno\u0002tation/ground truth labelling stage (c) is where domain expert knowledge is\nconsulted as to augment images with associated metadata. Meanwhile, in the\nevaluation phase (e), we have computer vision scientists, software developers,\nand pathologists working in concert to extract meaningful results and implica\u0002tions from the representation learning.\nfrom all areas of the CPath field to examine key works and chal\u0002lenges faced. By reviewing the field so comprehensively, our\ngoal is to layout the current landscape of key developments to\nallow computer scientists and pathologists alike to situate their\nwork in the overall CPath workflow, locate relevant works, and\nfacilitate an understanding of the field\u2019s future directions. We\nalso adopt the idea of generating model cards from [57] and de\u0002signed a card format specifically tailored for CPath. Each paper\nwe reviewed was catalogued as a model card that concisely de\u0002scribes (1) the organ of application, (2) the compiled dataset, (3)\nthe machine learning model, and (4) the target task. The com\u0002plete model card categorization of the reviewed publications is\nprovided in Appendix 9.12 for the reader\u2019s use.\nIn our review of the CPath field, we find that two main ap\u0002proaches emerge in works: 1) a data-centric approach and 2) a\nmodel-centric approach. Considering a given application area,\nsuch as specific cancers, e.g. breast ductal carcinoma in-situ\n(DCIS), or a specific task, e.g. segmentation of benign and\nmalignant regions of tissue, researchers in the CPath field fo\u0002cus generally on either improving the data or innovating on the\nmodel used.\nWorks with data-centric approaches focus on collecting\npathology data and compiling datasets to train models on cer\u0002tain tasks based on the premise that the transfer of domain ex\u0002pert knowledge to models is captured by the process of collect\u0002ing and labeling high-quality data [58, 51, 59]. The motivation\nbehind this approach in CPath is driven by the need to 1) ad\u0002dress the lack of labeled WSI data representing both histology\nand histopathology cases due to the laborious annotation pro\u0002cess [24] and 2) capture a predefined pathology ontology pro\u0002vided by domain expert pathologists for the class definitions\nand relations in tissue samples. Regarding the lack of labeled\nWSI data our analysis reveals that there are a larger number of\ndatasets with granular labels, but there is a larger total amount\nof data available for a given organ and disease application that\nhave weakly supervised labels at the Slide or Patient-level. Al\u0002though some tasks, such as segmentation and detection, require\nWSI data to have more granular labels at the region-of-interest\n(ROI) or image mosaic/tiles (known as patch) levels, to capture\nmore precise information for training models, there is a poten\u0002tial gap to leverage the large amount of weakly-supervised data\nto train models that can be later used downstream on smaller\nstrongly-supervised datasets for those tasks. When considering\nthe ontology of pathology as compared to the field of computer\nvision, we note that pathology datasets have far fewer classes\nthan computer vision (e.g. ImageNet-20K contains 20,000 class\ncategories for natural images [60] whereas CAMELYON17 has\nfour annotated classes for breast cancer metastases [61]), but\nhas much more variation within each of these classes in terms\nof representations and fuzzy boundaries around the grade of\ncancers which subdivides each class into many more in reality.\nThere are also very rare classes in the form of rare diseases and\ncancers, as presented in Figure 12 and discussed in Section 2,\nwhich present a class imbalance challenge when compiling data\nor training models. If one considers the complexities involved\nin representational learning of related tissues and diseases, it\nraises the question of whether there is a clear understanding\nand consensus in the field of how an efficient dataset should\nbe compiled for model development. Our survey analyzes the\navailability of CPath datasets along with what area of applica\u0002tion they address and their annotation level in detail in Section\n3.3, and the complete table of datasets we have covered is avail\u0002able in Appendix 9.9. Section 4 goes into more depth about the\nvarious levels of annotation, the annotation process, and select\u0002ing the appropriate annotation level for a task.\nThe model-centric approach, by contrast, is favoured by\ncomputer scientists and engineers, who design algorithmic ap\u0002proaches based on the available pathology data. Selection\nof a modelling approach, such as self-supervised, weakly\u0002supervised, or strongly-supervised learning, is dictated directly\nby the amount of data available for a given annotation level and\ntask. Currently, many models are developed on datasets with\nstrongly-supervised labels at the ROI, Patch, or Pixel-levels to\naddress tasks such as tissue type classification or disease de\u0002tection. However, a recent trend is developing to apply self\u0002supervised and weakly-supervised learning methods to leverage\nthe large amount of data with Slide and Patient-level annota\u0002tions [62]. Models are trained in a self or weakly supervised\nmanner to learn representations on a wider range of pathol\u0002ogy data across organs and diseases, which can be leveraged\nfor other tasks requiring more supervision but without the need\nfor massive labeled datasets [63, 64, 65]. This trend points to\nthe future direction of CPath models following a similar trend\nto that in computer vision, where large-scale models are being\n4\npre-trained using self-supervised techniques to achieve state-of\u0002the-art performance in downstream tasks [66, 67].\nAlthough data and model centric approaches are both im\u0002portant in advancing the performance of models and tools in\nCPath, we note a need for much more application centric work\nin CPath. We define a study to be application centric if the\nprimary focus is on addressing a particularly impactful task or\nneed in the clinical workflow, ideally including clinical valida\u0002tion of the method or tool. To this end, Section 2 details the\nclinical pathology workflow from specimen collection to report\ngeneration, major task categories in CPath, and specific applica\u0002tions per organ. Particularly, we find that very few works focus\non the pre or post-analytical phases of the pathology workflow\nwhere many errors can occur, instead focusing on the analyt\u0002ical phase where interpretation tasks take place. Additionally,\ncertain types of cancer with deadly survival rates are underrep\u0002resented in CPath datasets and works. Very few CPath models\nand tools have been validated in a clinical setting by pathol\u0002ogists, suggesting that there may still be massive barriers to\nactually using CPath tools in practice. All of this points to a\nsevere oversight by the CPath community towards considering\nthe actual application and implementation of tools in a clinical\nsetting. We suspect this to be a major reason as to why there is\na slow uptake in adopting CPath tools by pathology labs.\nThe contributions of this survey include the provision of an\nend-to-end workflow for developing CPath work which outlines\nthe various stages involved and is reflected within the survey\nsections. Further, we propose and provide a comprehensive\nconceptual model card framework for CPath that clearly cat\u0002egorizes works by their application of interest, dataset usage,\nand model, enabling consistent and easy comparison and re\u0002trieval of papers in relevant areas. Based on our analysis of\nthe field, we highlight several challenges and trends, including\nthe availability of datasets, focus on models leveraging exist\u0002ing data, disregard of impactful application areas, and lack of\nclinical validation. Finally, we give suggestions for address\u0002ing these aforementioned challenges and provide directions for\nfuture work in the hopes of aiding the adoption and implemen\u0002tation of CPath tools in clinical settings.\nThe structure of this survey closely follows the CPath data\nworkflow illustrated in Figure 1. Section 2 begins by outlining\nthe clinical pathology workflow and covers the various task do\u0002mains in CPath, along with organ specific tasks and diseases.\nThe next step of the workflow involves the processes and meth\u0002ods of histopathology data collection, which is outlined in Sec\u0002tion 3. Following data collection, Section 4 details the corre\u0002sponding annotation and labeling methodology and considera\u0002tions. Section 5 covers deep learning designs and methodolo\u0002gies for CPath applications. Section 6 focuses on regulatory\nmeasures and clinical validation of CPath tools. Section 7 ex\u0002plores emerging trends in recent CPath research. Finally, we\nprovide our perceived challenges and future outlook of CPath\nin Section 8.\n2. Clinical Applications for CPath\nThe field of CPath is dedicated to the creation of tools\nthat address and aid steps in the clinical pathology workflow.\nThus, a grounded understanding of the clinical workflow is\nof paramount importance before development of any CPath\ntool. The outcomes of clinical pathology are diagnostics, prog\u0002nostics, and predictions of therapy response. Computational\npathology systems that focus on diagnostic tasks aim to as\u0002sist the pathologists in tasks such as tumour detection, tumour\ngrading, quantification of cell numbers, etc. Prognostic systems\naim to predict survival for individual patients while therapy re\u0002sponse predictive models aid personalized treatment decisions\nbased on histopathology images. Figure 3 visualizes the goals\npertaining to these tasks. In this section, we provide detail on\nthe clinical pathology workflow, the major application areas in\ndiagnostics, prognostics, and therapy response, and finally de\u0002tail the cancers and CPath applications in specific organs. The\ngoal is to outline the tasks and areas of application in pathology\nwhere CPath tools and systems can be developed and imple\u0002mented.\n2.1. Clinical Pathology Workflow\nThis subsection provides a general overview of the clini\u0002cal workflow in pathology covering the collection of a tissue\nsample, its subsequent processing into a slide, inspection by\na pathologist, and compilation of the analysis and diagnosis\ninto a pathology. Figure 2 summarizes these steps at a high\nlevel and provides suggestions for corresponding CPath appli\u0002cations. The steps are organized under the conventional pathol\u0002ogy phases for samples: pre-analytical, analytical, and post\u0002analytical. These phases were developed to categorize qual\u0002ity control measures, as each phase has its own set of potential\nsources of errors [68], and thus potential sources of corrections\nduring which CPath and healthcare artificial intelligence tools\ncould prove useful. For details about each step of the workflow,\nplease refer to the Appendix 9.1.\nPre Analytical Phase The first step of the pre-analytical\nphase is a biopsy performed to collect a tissue sample, where\nthe biopsy method is dependent on the type of sample required\nand the tissue characteristics. Sample collection is followed by\naccessioning of the sample which involves entering of the pa\u0002tient and specimen information into a Laboratory Information\nSystem (LIS) and linking to the Electronic Medical Records\n(EMR) and potentially a Slide Tracking System (STS). Af\u0002ter accessioning, smaller specimens that have not already been\npreserved by fixation in formalin are fixated. Once the basic\nspecimen preparation has occurred, the tissue is analyzed by\nthe pathology team without the use of a microscope; a step\ncalled grossing. Grossing involves cross-referencing the clin\u0002ical findings and the EMR reports, with the operator localiz\u0002ing the disease, locating the pathological landmarks, describing\nthese landmarks, and measuring disease extent. Specific sam\u0002pling of these landmarks is performed, and these samples are\nthen put into cassettes for the final fixation. Subsequently, the\nsamples are then sliced using a microtome, stained using the\nrelevant stains for diagnosis, and covered with a glass slide.\nAnalytical Phase After a slide is processed and prepared, a\npathologist views the slide to analyze and interpret the sample.\nThe approach to interpretation varies depending on the spec\u0002imen type. Interpretation of smaller specimens is focused on\n5\nFig. 2: Quality assurance and control phases developed by pathologists to over\u0002see the clinical pathology workflow into three main phases of pre-analytical,\nanalytical, and post-analytica phases. We further show how each of these pro\u0002cesses can be augmented under the potential CPath applications in an end-to\u0002end pipeline.\ndiagnosis of any disease. Analysis is performed in a decision\u0002tree style approach to add diagnosis-specific parameters, e.g.\nesophagus biopsy \u2192 type of sampled mucosa \u2192 presence of\nfolveolar-type mucosa \u2192 identify Barrett\u2019s metaplasia \u2192 iden\u0002tify degree of dysplasia. Once the main diagnosis has been\nidentified and characterized, the pathologist sweeps the remain\u0002ing tissue for secondary diagnoses which can also be charac\u0002terized depending on their nature. Larger specimens are more\ncomplex and usually focus on characterizing the tissue and\nidentifying unexpected diagnoses beyond the prior diagnosis\nfrom a small specimen biopsy. Microscopic interpretation of\nlarge specimens is highly dependent on the quality of the gross\u0002ing and the appropriate detection and sampling of landmarks.\nEach landmark (e.g., tumor surface, tumor at deepest point,\nsurgical margins, lymph node in mesenteric fat) is character\u0002ized either according to guidelines, if available, or according\nto the pathologist\u2019s judgment. After the initial microscopic\ninterpretation additional deeper cuts (\u201clevels\u201d), special stains,\nimmunohistochemistry (IHC), and/or molecular testing may be\nperformed to hone the diagnosis by generating new material or\nslides from the original tissue block.\nPost-Analytical Phase The pathologist synthesizes a diagno\u0002sis by aggregating their findings from grossing and microscopic\nexamination in combination with the patient\u2019s clinical informa\u0002tion, all of which are included in a final pathology report. The\nclassic sections of a pathology report are patient information,\na list of specimens included, clinical findings, grossing report,\nmicroscopic description, final diagnosis, and comment. The\nlength and degree of complexity of the report again depends\non the specimen type. Small specimen reports are often suc\u0002cinct, clearly and unambiguously listing relevant findings which\nguide treatment and follow-up. Large specimen reports depend\non the disease, for example, in cancer resection specimens the\ngrossing landmarks are specifically targeted at elements that\nwill guide subsequent treatment.\nIn the past, pathology reports had no standardized format,\nusually taking a narrative-free text form. Free text reports can\nomit necessary data, include irrelevant information, and con\u0002tain inconsistent descriptions [69]. To combat this, synoptic re\u0002porting was introduced to provide a structured and standardized\nreporting format specific to each organ and cancer of interest\n[69, 70]. Over the last 15 years, synoptic reporting has enabled\npathologists to communicate information to surgeons, oncolo\u0002gists, patients, and researchers in a consistent manner across in\u0002stitutions and even countries. The College of American Pathol\u0002ogists (CAP) and the International Collaboration on Cancer Re\u0002porting (ICCR) are the two major institutions publishing synop\u0002tic reporting protocols. The parameters included in these pro\u0002tocols are determined and updated by CAP and ICCR respec\u0002tively to remain up-to-date and relevant for diagnosis of each\ncancer type. For the field of computational pathology, synoptic\nreporting provides a significant advantage in dataset and model\ncreation, as a pre-normalized set of labels exist across a vari\u0002ety of cases and slides in the form of the synoptic parameters\nfilled out in each report. Additionally, suggestion or prediction\nof synoptic report values are a possible CPath application area.\n2.2. Diagnostic Tasks\nComputational pathology systems that focus on diagnostic\ntasks can broadly be categorized as: (1) disease detection, (2)\ntissue subtype classification, (3) disease diagnosis, and (4) seg\u0002mentation. These tasks are visually depicted in Figure 3. Note\nhow the detection tasks all involve visual analysis of the tissue\nin WSI format. Thus computer vision approach is primarily\nadopted towards tackling diagnostic tasks in computer aided di\u0002agnosis (CAD). For additional detail on some previous works\non these diagnostic tasks, we refer the reader to Appendix 9.2\nDetection We define the detection task as a binary classifica\u0002tion problem where inputs are labeled as positive or negative,\nindicating the presence or absence of a certain feature. There\nmay be variations in the level of annotation required, e.g. slide\u0002level, patch-level, pixel-level detection depending on the fea\u0002ture in question. Although detection tasks may not provide\nan immediate disease diagnosis, it is a highly relevant task in\nmany pathology workflows as pathologists incorporate the pres\u0002ence or absence of various histological features into synoptic\nreports that lead to diagnosis. Broadly, detection tasks fall into\ntwo main categories: (1) screening the presence of cancers and\n(2) detecting histopathological features specific to certain diag\u0002noses.\nCancer detection algorithms can assist the pathologists by fil\u0002tering obviously normal WSIs and directing pathologist\u2019s focus\n6\nFig. 3: The categorization of diagnostic tasks in computational pathology along\nwith examples A) Detection: common detection task such as differentiating\npositive from negative classes like malignant from benign, B) Tissue Subtype\nClassification: classification task for tumorous tissue, Stroma, and adipose tis\u0002sue, C) Disease Diagnosis: common disease diagnosis task like cancer stag\u0002ing, D) Segmentation: tumor segmentation in WSIs, and E) Prognosis tasks:\nshows a graph comparing survival rate and months after surgery.\nto metastatic regions [71]. Although pathologists have to re\u0002view all the slides to check for multiple conditions regardless of\nthe clinical diagnosis, an accurate cancer detection CAD would\nexpedite the workflow by pinpointing the ROIs and summariz\u0002ing results into synoptic reports, ultimately leading to a reduces\ntime per slide. Due to this potential impact, cancer detection\ntasks have been explored in a broad set of organs. Additionally,\nthe simple labeling in binary detection tasks allows for deep\nlearning methods to generalize across different organs where\nsimilar cancers form [72, 73, 74].\nTissue Subtype Classification Treatment and patient progno\u0002sis can vary widely depending on the stage of cancer, and finely\nclassifying specific tissue structures associated with a specific\ndisease type provides essential diagnostic and prognostic in\u0002formation [75]. Accordingly, accurately classifying tissue sub\u0002types is a crucial component of the disease diagnosis process.\nAs an example, discriminating between two forms of glioma (a\ntype of brain cancer), glioblastoma multiforme and lower grade\nglioma, is critical as they differ by over 45% in patient survival\nrates [76]. Additionally, accurate classification is key in col\u0002orectal cancer (CRC) diagnosis, as high morphological varia\u0002tion in tumor cells [77] makes certain forms of CRC difficult to\ndiagnose by pathologists [78]. We define this classification of\nhistological features as tissue subtype classification.\nDisease Diagnosis The most frequently explored design of\ndeep learning in digital pathology involves emulating pathol\u0002ogist diagnosis. We define this multi-class diagnosis problem\nas a disease diagnosis task. Note the similarity with detection\u2013\ndisease diagnosis can be considered a fine-grained classification\nproblem which subdivides the general positive disease class\ninto finer disease-specific labels based on the organ and patient\ncontext.\nSegmentation The segmentation task moves one step beyond\nclassification by adding an element of spatial localization to the\npredicted label(s). In semantic segmentation, objects of inter\u0002est are delineated in an image by assigning class labels to every\npixel. These class labels can be discrete or non-discrete, the lat\u0002ter being a more difficult task [79]. Another variant of the seg\u0002mentation task is instance segmentation, which aims to achieve\nboth pixel-level segmentation accuracy as well as clearly de\u0002fined object (instance) boundaries. Segmentation approaches\ncan accurately capture many morphological statistics [80] and\ntextural features [81], both of which are relevant for cancer di\u0002agnosis and prognosis. Most frequently, segmentation is used\nto capture characteristics of individual glands, nuclei, and tu\u0002mor regions in WSIs. For instance, glandular structure is a\ncritical indicator of the severity of colorectal carcinoma [82],\nthus accurate segmentation could highlight particularly abnor\u0002mal glands to the pathologist as demonstrated in [82, 83, 84].\nOverall, segmentation provides localization and classification\nof cancer-specific tumors and of specific histological features\nthat can be meaningful for the pathologist\u2019s clinical interpreta\u0002tion.\n2.3. Prognosis\nPrognosis involves predicting the likely development of a\ndisease based on given patient features. For accurate survival\nprediction, models must learn to both identify and infer the\neffects of histological features on patient risk. Prognosis rep\u0002resents a merging of the diagnosis classification task and the\ndisease-survivability regression task.\nTraining a model for prognosis requires a comprehensive set\nof both histopathology slides and patient survival data (i.e. a\nvariant of multi-modal representation learning). Despite the\ncomplexity of the input data, ML models are still capable of\nextracting novel histological patterns for disease-specific sur\u0002vivability [85, 86, 87]. Furthermore, strong models can dis\u0002cover novel prognostically-relevant histological features from\nWSI analysis [88, 89]. As the quality and comprehensiveness\nof data improves, additional clinical factors could be incorpo\u0002rated into deep learning analysis to improve prognosis.\n2.4. Prediction of treatment response\nWith the recent advances in targeted therapy for cancer treat\u0002ment, clinicians are able to use treatment options that precisely\nidentify and attack certain types of cancer cells. While the num\u0002ber of options for targeted therapy are constantly increasing,\nit becomes increasingly important to identify patients who are\npotential therapy responders to a specific therapy option and\navoid treating non-responding patients who may experience se\u0002vere side effects. Deep learning can be used to detect structures\nand transformations in tumour tissue that could be used as pre\u0002dictive markers of a positive treatment response. Training such\ndeep learning models usually requires large cohorts of patient\ndata for whom the specific type of treatment option and the cor\u0002responding response is known.\n2.5. Organs and Diseases\nThis section presents an overview of the various anatomical\napplication areas for computational pathology grouped by the\ntargeted organ. Each organ section gives a brief overview of the\ntypes of cancers typically found and the content of the pathol\u0002ogy report as noted from the corresponding CAP synoptic re\u0002porting outline (discussed at 2.1). Figure 4 highlights the inter\u0002section between the major diagnostic tasks and the anatomical\n7\nFig. 4: Distribution of diagnostic tasks in CPath for different organs from Table\n9.11. This distribution includes more than 400 cited works from 2018 to 2022\ninclusive. The x-axis covers different organs, the y-axis displays different di\u0002agnostic tasks, and the height of the bars along the vertical axis measures the\nnumber of works that have examined the specific task and organ. Please refer\nto Table 9.11 in the supplementary section for more information.\nfocuses in state-of-the-art research. The majority of papers are\ndedicated to the four most common cancer sites: breast, colon,\nprostate, and lung [90]. Additionally, a significant amount of re\u0002search is also done on cancer types with highest mortality, brain\nand liver. [90]. Note that details of some additional works that\nmay be of interest for each organ type can be found in Appendix\n9.7\nBreast Breast cancers can start from different parts of the\nbreast and majorly consist of 1) Lobular cancers that start from\nlobular glands, 2) Ductal cancers, 3) Paget cancer which in\u0002volves the nipple, 4) Phyllodes tumor that stems from fat and\nconnective tissue surrounding the ducts and lobules, and 5) An\u0002giosarcoma which starts in the lining of the blood and lymph\nvessels. In addition, based on whether the cancer has spread\nor not, breast cancers can be categorized into in situ or inva\u0002sive/infiltrating forms. DCIS is a precancerous state and is still\nconfined to the ducts. Once the cancerous cells grow out of the\nducts, the carcinoma is now considered invasive or infiltrative\nand can metastasize [91].\nSynoptic reports for breast cancer diagnosis are divided\nbased on the type of cancers mentioned above. For DCIS and\ninvasive breast cancers, synoptic reports focus on the histo\u0002logic type and grade, along with the nuclear grade, evidence\nof necrosis, margin, involvement of regional lymph nodes, and\nbiomarker status. Notably, architectural patterns are no longer\na valuable predictive tool compared to nuclear grade and necro\u0002sis to determine a relative ordering of diagnostic importance for\nDCIS [92]. In contrast to DCIS and invasive cancers, Phyl\u0002lodes tumours vary due to their differing origin in the fat and\nconnective tissue, focusing on analyzing the stroma character\u0002istics, existence of heterologous elements, mitotic rate, along\nwith the involvement of lymph nodes. Finally, to determine\ntherapy response and treatments, biomarker tests for Estrogen,\nProgesterone [93] and HER-2 [94] receptors are recommended,\nalong with occasional tests for Ki67 antigens [95, 96].\nMost breast cancer-focused works in CPath propose vari\u0002ous solutions for carcinoma detection and metastasis detection,\nan important step for assessing cancer stage and morbidity.\nMetastasis detection using deep learning methods was shown\nto outperform pathologists\u2019 exhaustive diagnosis by 9% free\u0002response receiver operating characteristic (FROC) in [97].\nProstate Prostate cancer is the second most prevalent cancer\namong the total population and the most common cancer among\nmen (both excluding non-melanoma skin cancers). However,\nmost prostate cancers are not lethal. Prostate cancer can oc\u0002cur in any of the three prostate zones: Central (CZ), Periph\u0002eral (PZ), and Transition (TZ), in increasing order of aggres\u0002siveness. Prostate cancers are almost always adenocarcino\u0002mas, which develop from the gland cells that make prostate\nfluid. The other types of prostate cancers are small cell car\u0002cinomas, neuroendocrine tumors, transitional cell carcinomas,\nisolated intraductal carcinoma, and sarcomas (which are very\nrare). Other than cancers, there are multiple conditions that\nare important to identify or diagnose as precursors to cancer\nor not. Prostatic intraepithelial neoplasia (PIN) is diagnosed as\neither low-grade PIN or high-grade PIN. Men with high-grade\nPIN need closely monitored follow-up sessions to screen for\nprostate cancer. Similarly, atypical small acinar proliferation\n(ASAP) is another precancerous condition requiring follow-up\nbiopsies. [98]\nTo grade and score tumours, pathologists use a Tumor,\nNodes, Metastasis (TNM) framework. In the synoptic report,\npathologists identify and report the histologic type and grades,\nand involvement of regional lymph nodes to help grade and\nprovide prognosis for any tumours. Specifically for prostate\nanalysis, tumour size and volume are both important factors\nin prognosis according to multiple studies [99, 100, 101, 102].\nSimilarly, location is important to note for both prognosis and\ntherapy response [103]. Invasion to nearby (except perineural\ninvasion) tissues is noted and can correlate to TMN classifi\u0002cation [104]. Additionally, margin analysis is especially im\u0002portant in prostate cancers as the presence of a positive margin\nincreases the risk of cancer recurrence and metastasis [105]. Fi\u0002nally, intraductal carcinoma (IDC) must be identified and dis\u0002tinguished from PIN and PIA; as it is strongly associated with\na high Gleason score, a high-volume tumor, and metastatic dis\u0002ease [106, 107, 108, 109, 110].\nAfter a prostate cancer diagnosis is established, pathologists\nassign a Gleason Score to determine the cancer\u2019s grade: a grade\nfrom 1 to 5 is assigned to the two most common areas and\nthose two grades are summed to make a final Gleason Score\n[111]. For Gleason scores of 7, where survival and clinical out\u0002comes demonstrate large variance, the identification of Crib\u0002riform glands is key in helping to narrow possible outcomes\n[112, 113].\nOvary Ovarian cancer is the deadliest gynecologic malig\u0002nancy and accounts for more than 14, 000 deaths each year\n[114]. Ovarian cancer manifests in three types: 1) epithelial cell\ntumors that start from the epithelial cells covering the outer sur\u0002face of the ovary, 2) germ cell tumors which start from the cells\nthat produce eggs, and 3) stromal tumors which start from cells\nthat hold the ovary together and produce the hormones estrogen\nand progesterone. Each of these cancer types can be classified\ninto benign, intermediate and malignant categories. Overall,\nepithelial cell tumors are the most common ovarian cancer and\nhave the worst prognosis [115].\nWhen compiling a synoptic report for ovarian cancer diag-\n8\nnosis, pathologists focus on histologic type and grade, extra\u0002organ involvement, regional lymph nodes, T53 gene mutations,\nand serous tubal intraeptithelial carconma (STIC). Varying his\u0002tologic tissue types are vital to determine the pathology char\u0002acteristics and determining eventual prognosis. For example,\ngenerally endometrioid, mucinous, and clear cell carcinomas\nhave better outcomes than serous carcinomas [116]. Addition\u0002ally, lymph node involvement and metastasis in both regional\nand distant nodes has a direct correlation to patient survival,\ngrading, and treatment. Determining the presence of STICs\ncorrelates directly to the presence of ovarian cancer, as 60% of\novarian cancer patients will also have an associated STIC [114].\nFinally, T53 gene mutations are the most common in epithelial\novarian cancer; which has the worst prognosis among ovarian\ncancers, so determining their presence is critical to patient can\u0002cer risk and therapy response [117, 118]. There are not a large\nnumber of works dedicated to the ovary specifically, but most\nworks on ovary focus on classification of its five most common\ncancer subtypes: high-grade serous (HGSC), low-grade serous\n(LGSC), endometriod (ENC), clear cell (CCC), and mucinous\n(MUC) [119, 120].\nLung Lung cancer is the third most common cancer type,\nnext to breast and prostate cancer [121]. Lung cancers mostly\nstart in the bronchi, bronchioles, or alveoli and are divided\ninto two major types, non-small cell lung carcinomas (NSCLC)\n(80\u221285%) and small cell lung carcinomas (SCLC) (10\u221215%).\nAlthough NSCLS cancers are different in terms of origin, they\nare grouped because they have similar outcomes and treatment\nplans. Common NSCLS cancers are 1) adenocarcinoma, 2)\nsquamous cell carcinoma 3) large cell carcinoma, and some\nother uncommon subtypes [122].\nFor reporting, histologic type helps determine NSCLC vs\nSCLC and the subtype of NSCLC. Although NSCLC gener\u0002ally has favourable survival rates and prognosis as compared\nto SCLC, certain subtypes of NSCLC can have lower survival\nrates due to co-factors [123]. Histologic patterns are applicable\nin adenocarcinomas, consisting of favourable types: lepidic, in\u0002termediate: acinar and papillary, and unfavourable: micropap\u0002illary and solid [124]. Grading each histologic type aids in cat\u0002egorization but is differentiated based on each type, and thus is\nout of scope for this paper. Importantly for lung cancers, tu\u0002mour size is an independent prognostic factor for early cancer\nstages, lymph node positivity, and locally invasive disease. Ad\u0002ditionally, the size of the invasive portion is an important fac\u0002tor for prognosis of nonmucinous adenocarcinoma with lepidic\npattern [125, 126, 127, 128, 129, 123]. Other important lung\nspecific features are visceral pleural invasion, which is associ\u0002ated with worse prognosis in early-stage lung cancer even with\ntumors",
      "openalex_id": "https://openalex.org/W4391065733",
      "title": "Computational Pathology: A Survey Review and The Way Forward",
      "publication_date": "2024-01-21",
      "cited_by_count": 13.0,
      "topics": "Deep Learning in Medical Image Analysis, Advanced Techniques in Bioimage Analysis and Microscopy, Automated Analysis of Blood Cell Images",
      "keywords": "Viewpoints, Digital Pathology, Cyberinfrastructure, Sketch",
      "concepts": "Computer science, Digital pathology, Viewpoints, Data science, Workflow, Multidisciplinary approach, Field (mathematics), Cyberinfrastructure, Sketch, Interoperability, Management science, Artificial intelligence, World Wide Web, Art, Social science, Mathematics, Algorithm, Database, Sociology, Pure mathematics, Economics, Visual arts",
      "pdf_urls_by_priority": [
        "https://arxiv.org/pdf/2304.05482"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W1147193425",
        "https://openalex.org/W1173721425",
        "https://openalex.org/W1213336605",
        "https://openalex.org/W1536377526",
        "https://openalex.org/W1548827537",
        "https://openalex.org/W1576445103",
        "https://openalex.org/W1603593764",
        "https://openalex.org/W1812458631",
        "https://openalex.org/W1847493486",
        "https://openalex.org/W1888786797",
        "https://openalex.org/W1901164038",
        "https://openalex.org/W1932469787",
        "https://openalex.org/W1932924519",
        "https://openalex.org/W1950315773",
        "https://openalex.org/W1968629884",
        "https://openalex.org/W1970962264",
        "https://openalex.org/W1972719937",
        "https://openalex.org/W1974467617",
        "https://openalex.org/W1974728334",
        "https://openalex.org/W1978874224",
        "https://openalex.org/W1983346554",
        "https://openalex.org/W1985098533",
        "https://openalex.org/W1987944428",
        "https://openalex.org/W1989514509",
        "https://openalex.org/W1990362208",
        "https://openalex.org/W1994052710",
        "https://openalex.org/W1994829455",
        "https://openalex.org/W1995531130",
        "https://openalex.org/W1997864537",
        "https://openalex.org/W1997882893",
        "https://openalex.org/W2003103742",
        "https://openalex.org/W2004558777",
        "https://openalex.org/W2005393216",
        "https://openalex.org/W2011831342",
        "https://openalex.org/W2013413079",
        "https://openalex.org/W2021481801",
        "https://openalex.org/W2025464786",
        "https://openalex.org/W2026526178",
        "https://openalex.org/W2027925344",
        "https://openalex.org/W2038264173",
        "https://openalex.org/W2039544142",
        "https://openalex.org/W2047176521",
        "https://openalex.org/W2055145522",
        "https://openalex.org/W2055541283",
        "https://openalex.org/W2055965251",
        "https://openalex.org/W2057114171",
        "https://openalex.org/W2061872473",
        "https://openalex.org/W2065360903",
        "https://openalex.org/W2073501891",
        "https://openalex.org/W2075923670",
        "https://openalex.org/W2081139503",
        "https://openalex.org/W2083542943",
        "https://openalex.org/W2083927153",
        "https://openalex.org/W2085678918",
        "https://openalex.org/W2092123708",
        "https://openalex.org/W2093795918",
        "https://openalex.org/W2095114940",
        "https://openalex.org/W2103061399",
        "https://openalex.org/W2110243528",
        "https://openalex.org/W2110972788",
        "https://openalex.org/W2111574404",
        "https://openalex.org/W2115521901",
        "https://openalex.org/W2119460059",
        "https://openalex.org/W2122394460",
        "https://openalex.org/W2126179037",
        "https://openalex.org/W2128252595",
        "https://openalex.org/W2129439022",
        "https://openalex.org/W2130805253",
        "https://openalex.org/W2131673797",
        "https://openalex.org/W2132031490",
        "https://openalex.org/W2141292206",
        "https://openalex.org/W2141486320",
        "https://openalex.org/W2141967861",
        "https://openalex.org/W2150134401",
        "https://openalex.org/W2151554678",
        "https://openalex.org/W2152397906",
        "https://openalex.org/W2153702861",
        "https://openalex.org/W2158826452",
        "https://openalex.org/W2162548780",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W2164582889",
        "https://openalex.org/W2164589152",
        "https://openalex.org/W2166581609",
        "https://openalex.org/W2170566684",
        "https://openalex.org/W2185094885",
        "https://openalex.org/W2200290088",
        "https://openalex.org/W2221930818",
        "https://openalex.org/W2253429366",
        "https://openalex.org/W2264887978",
        "https://openalex.org/W2267490210",
        "https://openalex.org/W2279098554",
        "https://openalex.org/W2281825886",
        "https://openalex.org/W2282915343",
        "https://openalex.org/W2288892845",
        "https://openalex.org/W2302302587",
        "https://openalex.org/W2312404985",
        "https://openalex.org/W2314178310",
        "https://openalex.org/W2335197832",
        "https://openalex.org/W2338965302",
        "https://openalex.org/W2342014610",
        "https://openalex.org/W2342985385",
        "https://openalex.org/W2344480160",
        "https://openalex.org/W2401520370",
        "https://openalex.org/W2419518659",
        "https://openalex.org/W2435090885",
        "https://openalex.org/W2470130773",
        "https://openalex.org/W2470317160",
        "https://openalex.org/W2482581235",
        "https://openalex.org/W2491123226",
        "https://openalex.org/W2494544274",
        "https://openalex.org/W2497979631",
        "https://openalex.org/W2502134830",
        "https://openalex.org/W2504150216",
        "https://openalex.org/W2514628397",
        "https://openalex.org/W2521492299",
        "https://openalex.org/W2526468814",
        "https://openalex.org/W2529923829",
        "https://openalex.org/W2531813393",
        "https://openalex.org/W2564463480",
        "https://openalex.org/W2573152477",
        "https://openalex.org/W2581082771",
        "https://openalex.org/W2581851997",
        "https://openalex.org/W2592905743",
        "https://openalex.org/W2593345132",
        "https://openalex.org/W2594760301",
        "https://openalex.org/W2597507805",
        "https://openalex.org/W2601530120",
        "https://openalex.org/W2604440528",
        "https://openalex.org/W2605850958",
        "https://openalex.org/W2606429533",
        "https://openalex.org/W2607075141",
        "https://openalex.org/W2611254939",
        "https://openalex.org/W2613181504",
        "https://openalex.org/W2614808277",
        "https://openalex.org/W2616747498",
        "https://openalex.org/W2618999197",
        "https://openalex.org/W2620578070",
        "https://openalex.org/W2622416784",
        "https://openalex.org/W2623902889",
        "https://openalex.org/W2693096534",
        "https://openalex.org/W2716665989",
        "https://openalex.org/W2730845691",
        "https://openalex.org/W2740784636",
        "https://openalex.org/W2741128077",
        "https://openalex.org/W2741544066",
        "https://openalex.org/W2743136221",
        "https://openalex.org/W2749613778",
        "https://openalex.org/W2751723768",
        "https://openalex.org/W2752060020",
        "https://openalex.org/W2755347608",
        "https://openalex.org/W2756270667",
        "https://openalex.org/W2759577099",
        "https://openalex.org/W2759912964",
        "https://openalex.org/W2760354783",
        "https://openalex.org/W2760946358",
        "https://openalex.org/W2761290139",
        "https://openalex.org/W2761668583",
        "https://openalex.org/W2762672048",
        "https://openalex.org/W2764072425",
        "https://openalex.org/W2765375430",
        "https://openalex.org/W2769999077",
        "https://openalex.org/W2772723798",
        "https://openalex.org/W2774064151",
        "https://openalex.org/W2786974903",
        "https://openalex.org/W2788072220",
        "https://openalex.org/W2789755511",
        "https://openalex.org/W2791081619",
        "https://openalex.org/W2791915981",
        "https://openalex.org/W2793249006",
        "https://openalex.org/W2794803511",
        "https://openalex.org/W2795587190",
        "https://openalex.org/W2796409016",
        "https://openalex.org/W2801370692",
        "https://openalex.org/W2801540580",
        "https://openalex.org/W2801876365",
        "https://openalex.org/W2803416021",
        "https://openalex.org/W2803432927",
        "https://openalex.org/W2803469628",
        "https://openalex.org/W2805735218",
        "https://openalex.org/W2805886241",
        "https://openalex.org/W2806242585",
        "https://openalex.org/W2806562834",
        "https://openalex.org/W2808210572",
        "https://openalex.org/W2808324059",
        "https://openalex.org/W2809209815",
        "https://openalex.org/W2809275911",
        "https://openalex.org/W2809293192",
        "https://openalex.org/W2810818658",
        "https://openalex.org/W2811017491",
        "https://openalex.org/W2811123232",
        "https://openalex.org/W2883198641",
        "https://openalex.org/W2884083106",
        "https://openalex.org/W2884988214",
        "https://openalex.org/W2885343725",
        "https://openalex.org/W2885628138",
        "https://openalex.org/W2885650974",
        "https://openalex.org/W2885824038",
        "https://openalex.org/W2886093703",
        "https://openalex.org/W2886175403",
        "https://openalex.org/W2886631828",
        "https://openalex.org/W2889232360",
        "https://openalex.org/W2894917609",
        "https://openalex.org/W2896494478",
        "https://openalex.org/W2897068067",
        "https://openalex.org/W2897434820",
        "https://openalex.org/W2898020899",
        "https://openalex.org/W2899537006",
        "https://openalex.org/W2900118953",
        "https://openalex.org/W2900257566",
        "https://openalex.org/W2900654702",
        "https://openalex.org/W2901612843",
        "https://openalex.org/W2903119292",
        "https://openalex.org/W2903759724",
        "https://openalex.org/W2903829201",
        "https://openalex.org/W2905969903",
        "https://openalex.org/W2906233029",
        "https://openalex.org/W2906774465",
        "https://openalex.org/W2909488080",
        "https://openalex.org/W2910592851",
        "https://openalex.org/W2912194425",
        "https://openalex.org/W2912814480",
        "https://openalex.org/W2913326971",
        "https://openalex.org/W2913510405",
        "https://openalex.org/W2914038321",
        "https://openalex.org/W2914568698",
        "https://openalex.org/W2915853139",
        "https://openalex.org/W2915860004",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2922239620",
        "https://openalex.org/W2922268597",
        "https://openalex.org/W2924158535",
        "https://openalex.org/W2928842276",
        "https://openalex.org/W2929968583",
        "https://openalex.org/W2933939325",
        "https://openalex.org/W2938004941",
        "https://openalex.org/W2938704257",
        "https://openalex.org/W2941548848",
        "https://openalex.org/W2943370629",
        "https://openalex.org/W2943857514",
        "https://openalex.org/W2945123041",
        "https://openalex.org/W2945334889",
        "https://openalex.org/W2945500496",
        "https://openalex.org/W2945807221",
        "https://openalex.org/W2946027615",
        "https://openalex.org/W2946099713",
        "https://openalex.org/W2947078801",
        "https://openalex.org/W2947825023",
        "https://openalex.org/W2948141910",
        "https://openalex.org/W2948930564",
        "https://openalex.org/W2949226441",
        "https://openalex.org/W2949306187",
        "https://openalex.org/W2952003460",
        "https://openalex.org/W2952367870",
        "https://openalex.org/W2952481429",
        "https://openalex.org/W2952527443",
        "https://openalex.org/W2952800276",
        "https://openalex.org/W2952833648",
        "https://openalex.org/W2952846726",
        "https://openalex.org/W2955375681",
        "https://openalex.org/W2956019261",
        "https://openalex.org/W2956228567",
        "https://openalex.org/W2956362951",
        "https://openalex.org/W2956998909",
        "https://openalex.org/W2958836289",
        "https://openalex.org/W2960960151",
        "https://openalex.org/W2962804068",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W2963129226",
        "https://openalex.org/W2963258365",
        "https://openalex.org/W2963432486",
        "https://openalex.org/W2963519862",
        "https://openalex.org/W2964282006",
        "https://openalex.org/W2964345665",
        "https://openalex.org/W2964358045",
        "https://openalex.org/W2964756323",
        "https://openalex.org/W2965481926",
        "https://openalex.org/W2967444033",
        "https://openalex.org/W2968027356",
        "https://openalex.org/W2969278648",
        "https://openalex.org/W2969657290",
        "https://openalex.org/W2970152602",
        "https://openalex.org/W2971045153",
        "https://openalex.org/W2972214324",
        "https://openalex.org/W2972535786",
        "https://openalex.org/W2973237362",
        "https://openalex.org/W2973431617",
        "https://openalex.org/W2974825848",
        "https://openalex.org/W2975107127",
        "https://openalex.org/W2978575375",
        "https://openalex.org/W2978639826",
        "https://openalex.org/W2978882452",
        "https://openalex.org/W2979417040",
        "https://openalex.org/W2980998394",
        "https://openalex.org/W2981358604",
        "https://openalex.org/W2981466131",
        "https://openalex.org/W2982406227",
        "https://openalex.org/W2985301765",
        "https://openalex.org/W2986235590",
        "https://openalex.org/W2986668407",
        "https://openalex.org/W2988037626",
        "https://openalex.org/W2989695963",
        "https://openalex.org/W2989743390",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W2991151034",
        "https://openalex.org/W2994908874",
        "https://openalex.org/W2994910508",
        "https://openalex.org/W2995325438",
        "https://openalex.org/W2995682783",
        "https://openalex.org/W2996092187",
        "https://openalex.org/W2996780833",
        "https://openalex.org/W2998010746",
        "https://openalex.org/W2998141119",
        "https://openalex.org/W2998472909",
        "https://openalex.org/W2998794254",
        "https://openalex.org/W2999400187",
        "https://openalex.org/W2999417355",
        "https://openalex.org/W3003199121",
        "https://openalex.org/W3004016611",
        "https://openalex.org/W3004713990",
        "https://openalex.org/W3005130116",
        "https://openalex.org/W3005226777",
        "https://openalex.org/W3005999761",
        "https://openalex.org/W3006954975",
        "https://openalex.org/W3007110023",
        "https://openalex.org/W3007464329",
        "https://openalex.org/W3007935259",
        "https://openalex.org/W3008355217",
        "https://openalex.org/W3008693315",
        "https://openalex.org/W3008724309",
        "https://openalex.org/W3008782862",
        "https://openalex.org/W3009210879",
        "https://openalex.org/W3009332900",
        "https://openalex.org/W3009334705",
        "https://openalex.org/W3009926465",
        "https://openalex.org/W3009928129",
        "https://openalex.org/W3010168554",
        "https://openalex.org/W3010834500",
        "https://openalex.org/W3011037969",
        "https://openalex.org/W3011592483",
        "https://openalex.org/W3011599283",
        "https://openalex.org/W3011941780",
        "https://openalex.org/W3013627541",
        "https://openalex.org/W3014402422",
        "https://openalex.org/W3015101370",
        "https://openalex.org/W3015357052",
        "https://openalex.org/W3015450039",
        "https://openalex.org/W3016265934",
        "https://openalex.org/W3016293126",
        "https://openalex.org/W3016315468",
        "https://openalex.org/W3016553397",
        "https://openalex.org/W3017354830",
        "https://openalex.org/W3018429932",
        "https://openalex.org/W3018647685",
        "https://openalex.org/W3020560751",
        "https://openalex.org/W3020916919",
        "https://openalex.org/W3021413349",
        "https://openalex.org/W3021500318",
        "https://openalex.org/W3022952790",
        "https://openalex.org/W3024962477",
        "https://openalex.org/W3027616379",
        "https://openalex.org/W3027869849",
        "https://openalex.org/W3030555654",
        "https://openalex.org/W3030889987",
        "https://openalex.org/W3032131871",
        "https://openalex.org/W3035834839",
        "https://openalex.org/W3036686253",
        "https://openalex.org/W3037083567",
        "https://openalex.org/W3037683614",
        "https://openalex.org/W3037771725",
        "https://openalex.org/W3038704997",
        "https://openalex.org/W3040472729",
        "https://openalex.org/W3040734937",
        "https://openalex.org/W3040784645",
        "https://openalex.org/W3043535018",
        "https://openalex.org/W3044364303",
        "https://openalex.org/W3044753181",
        "https://openalex.org/W3045295565",
        "https://openalex.org/W3046305306",
        "https://openalex.org/W3046400384",
        "https://openalex.org/W3046446892",
        "https://openalex.org/W3049608122",
        "https://openalex.org/W3081006013",
        "https://openalex.org/W3081081614",
        "https://openalex.org/W3083699157",
        "https://openalex.org/W3086458417",
        "https://openalex.org/W3086667591",
        "https://openalex.org/W3089090082",
        "https://openalex.org/W3090112028",
        "https://openalex.org/W3092571470",
        "https://openalex.org/W3092698347",
        "https://openalex.org/W3093002967",
        "https://openalex.org/W3093046161",
        "https://openalex.org/W3094111018",
        "https://openalex.org/W3096540902",
        "https://openalex.org/W3098027819",
        "https://openalex.org/W3098525219",
        "https://openalex.org/W3099287508",
        "https://openalex.org/W3099661382",
        "https://openalex.org/W3100003598",
        "https://openalex.org/W3100084586",
        "https://openalex.org/W3100279624",
        "https://openalex.org/W3100887207",
        "https://openalex.org/W3101216323",
        "https://openalex.org/W3104041126",
        "https://openalex.org/W3104135675",
        "https://openalex.org/W3104211245",
        "https://openalex.org/W3104370314",
        "https://openalex.org/W3105141008",
        "https://openalex.org/W3105980361",
        "https://openalex.org/W3106231035",
        "https://openalex.org/W3107410755",
        "https://openalex.org/W3108329040",
        "https://openalex.org/W3109000640",
        "https://openalex.org/W3109246949",
        "https://openalex.org/W3112301260",
        "https://openalex.org/W3113328935",
        "https://openalex.org/W3113891464",
        "https://openalex.org/W3114754248",
        "https://openalex.org/W3115970425",
        "https://openalex.org/W3119036516",
        "https://openalex.org/W3119330513",
        "https://openalex.org/W3119441468",
        "https://openalex.org/W3121859091",
        "https://openalex.org/W3121943121",
        "https://openalex.org/W3126783352",
        "https://openalex.org/W3127935213",
        "https://openalex.org/W3128158496",
        "https://openalex.org/W3128202743",
        "https://openalex.org/W3128210037",
        "https://openalex.org/W3129033913",
        "https://openalex.org/W3130735597",
        "https://openalex.org/W3131525335",
        "https://openalex.org/W3132095500",
        "https://openalex.org/W3132964805",
        "https://openalex.org/W3133782060",
        "https://openalex.org/W3135068374",
        "https://openalex.org/W3135547872",
        "https://openalex.org/W3135550350",
        "https://openalex.org/W3137675974",
        "https://openalex.org/W3138548427",
        "https://openalex.org/W3138848412",
        "https://openalex.org/W3138973186",
        "https://openalex.org/W3141797743",
        "https://openalex.org/W3144167624",
        "https://openalex.org/W3148150040",
        "https://openalex.org/W3152579480",
        "https://openalex.org/W3154566661",
        "https://openalex.org/W3159302505",
        "https://openalex.org/W3160261825",
        "https://openalex.org/W3160817203",
        "https://openalex.org/W3161570989",
        "https://openalex.org/W3163363865",
        "https://openalex.org/W3164754318",
        "https://openalex.org/W3165730810",
        "https://openalex.org/W3166254754",
        "https://openalex.org/W3170284304",
        "https://openalex.org/W3172675935",
        "https://openalex.org/W3174246647",
        "https://openalex.org/W3174477388",
        "https://openalex.org/W3176446229",
        "https://openalex.org/W3176719058",
        "https://openalex.org/W3178519255",
        "https://openalex.org/W3180779372",
        "https://openalex.org/W3181044391",
        "https://openalex.org/W3181082054",
        "https://openalex.org/W3181414820",
        "https://openalex.org/W3181481201",
        "https://openalex.org/W3181599570",
        "https://openalex.org/W3183075879",
        "https://openalex.org/W3183916696",
        "https://openalex.org/W3185094524",
        "https://openalex.org/W3185935705",
        "https://openalex.org/W3186317940",
        "https://openalex.org/W3186679119",
        "https://openalex.org/W3186966887",
        "https://openalex.org/W3187624058",
        "https://openalex.org/W3189996993",
        "https://openalex.org/W3190070581",
        "https://openalex.org/W3191409400",
        "https://openalex.org/W3191831606",
        "https://openalex.org/W3192682950",
        "https://openalex.org/W3195127593",
        "https://openalex.org/W3196270674",
        "https://openalex.org/W3196396697",
        "https://openalex.org/W3197713479",
        "https://openalex.org/W3199764674",
        "https://openalex.org/W3201077454",
        "https://openalex.org/W3201366746",
        "https://openalex.org/W3202910984",
        "https://openalex.org/W3203957008",
        "https://openalex.org/W3204013916",
        "https://openalex.org/W3205594709",
        "https://openalex.org/W3205605830",
        "https://openalex.org/W3206263253",
        "https://openalex.org/W3206795303",
        "https://openalex.org/W3207188103",
        "https://openalex.org/W3207878045",
        "https://openalex.org/W3208672800",
        "https://openalex.org/W3208934141",
        "https://openalex.org/W3209331124",
        "https://openalex.org/W3210191307",
        "https://openalex.org/W3210579795",
        "https://openalex.org/W3211109300",
        "https://openalex.org/W3211190672",
        "https://openalex.org/W3211324396",
        "https://openalex.org/W3211647829",
        "https://openalex.org/W3212108649",
        "https://openalex.org/W3212889265",
        "https://openalex.org/W3213974477",
        "https://openalex.org/W3214733388",
        "https://openalex.org/W3214930412",
        "https://openalex.org/W3215159356",
        "https://openalex.org/W3215375644",
        "https://openalex.org/W3215409948",
        "https://openalex.org/W3216717225",
        "https://openalex.org/W3217644914",
        "https://openalex.org/W4200008359",
        "https://openalex.org/W4200040427",
        "https://openalex.org/W4200124884",
        "https://openalex.org/W4200219316",
        "https://openalex.org/W4200268060",
        "https://openalex.org/W4205382356",
        "https://openalex.org/W4205876996",
        "https://openalex.org/W4205900565",
        "https://openalex.org/W4206174637",
        "https://openalex.org/W4206759694",
        "https://openalex.org/W4207066074",
        "https://openalex.org/W4210426457",
        "https://openalex.org/W4210957076",
        "https://openalex.org/W4211119275",
        "https://openalex.org/W4214547251",
        "https://openalex.org/W4220675364",
        "https://openalex.org/W4220677231",
        "https://openalex.org/W4220712379",
        "https://openalex.org/W4220795582",
        "https://openalex.org/W4220896502",
        "https://openalex.org/W4220911728",
        "https://openalex.org/W4221022458",
        "https://openalex.org/W4221030665",
        "https://openalex.org/W4221165931",
        "https://openalex.org/W4224294628",
        "https://openalex.org/W4225142522",
        "https://openalex.org/W4225480823",
        "https://openalex.org/W4225746601",
        "https://openalex.org/W4225919190",
        "https://openalex.org/W4225981821",
        "https://openalex.org/W4226089600",
        "https://openalex.org/W4226190792",
        "https://openalex.org/W4226236462",
        "https://openalex.org/W4226369938",
        "https://openalex.org/W4226409235",
        "https://openalex.org/W4226426271",
        "https://openalex.org/W4226512818",
        "https://openalex.org/W4229049871",
        "https://openalex.org/W4229080888",
        "https://openalex.org/W4229958410",
        "https://openalex.org/W4229980196",
        "https://openalex.org/W4232611972",
        "https://openalex.org/W4234173207",
        "https://openalex.org/W4235809187",
        "https://openalex.org/W4236965008",
        "https://openalex.org/W4238455477",
        "https://openalex.org/W4239252183",
        "https://openalex.org/W4241929126",
        "https://openalex.org/W4242628528",
        "https://openalex.org/W4242705165",
        "https://openalex.org/W4243985867",
        "https://openalex.org/W4246673929",
        "https://openalex.org/W4247943011",
        "https://openalex.org/W4247960443",
        "https://openalex.org/W4248810809",
        "https://openalex.org/W4249502209",
        "https://openalex.org/W4249751009",
        "https://openalex.org/W4249876349",
        "https://openalex.org/W4249914127",
        "https://openalex.org/W4251573514",
        "https://openalex.org/W4253944268",
        "https://openalex.org/W4255542875",
        "https://openalex.org/W4255556797",
        "https://openalex.org/W4256012820",
        "https://openalex.org/W4280531683",
        "https://openalex.org/W4280555205",
        "https://openalex.org/W4280595708",
        "https://openalex.org/W4281386768",
        "https://openalex.org/W4283266161",
        "https://openalex.org/W4283520680",
        "https://openalex.org/W4283806266",
        "https://openalex.org/W4283813454",
        "https://openalex.org/W4285105280",
        "https://openalex.org/W4285236225",
        "https://openalex.org/W4285651992",
        "https://openalex.org/W4285721697",
        "https://openalex.org/W4287774581",
        "https://openalex.org/W4288257105",
        "https://openalex.org/W4288860568",
        "https://openalex.org/W4289286782",
        "https://openalex.org/W4289454179",
        "https://openalex.org/W4289704839",
        "https://openalex.org/W4290973455",
        "https://openalex.org/W4291019968",
        "https://openalex.org/W4292318690",
        "https://openalex.org/W4292369513",
        "https://openalex.org/W4292907187",
        "https://openalex.org/W4293163051",
        "https://openalex.org/W4293213992",
        "https://openalex.org/W4293583832",
        "https://openalex.org/W4294612686",
        "https://openalex.org/W4295093036",
        "https://openalex.org/W4296083357",
        "https://openalex.org/W4296693015",
        "https://openalex.org/W4297249536",
        "https://openalex.org/W4297371220",
        "https://openalex.org/W4298204340",
        "https://openalex.org/W4302192999",
        "https://openalex.org/W4309214336",
        "https://openalex.org/W4310051789",
        "https://openalex.org/W431215260",
        "https://openalex.org/W4312747027",
        "https://openalex.org/W4312906155",
        "https://openalex.org/W4313028954",
        "https://openalex.org/W4313397115",
        "https://openalex.org/W4313830579",
        "https://openalex.org/W4318220659",
        "https://openalex.org/W4320350422",
        "https://openalex.org/W4322721845",
        "https://openalex.org/W4323926462",
        "https://openalex.org/W4377010966",
        "https://openalex.org/W4381573401",
        "https://openalex.org/W4382142072",
        "https://openalex.org/W4383101683",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385948838",
        "https://openalex.org/W4386076316",
        "https://openalex.org/W4386171305",
        "https://openalex.org/W4387211376",
        "https://openalex.org/W4387211411",
        "https://openalex.org/W4393053949",
        "https://openalex.org/W611401390",
        "https://openalex.org/W92498844"
      ],
      "openalex_rank": 15,
      "num_tokens": 8965,
      "best_oa_location_pdf_url": NaN,
      "successful_pdf_url": NaN,
      "question_generated": "What are the current methodologies, challenges, and future directions in the field of computational pathology, particularly regarding the integration of digital diagnostics into clinical practice?",
      "pipeline_answer": "### Current Methodologies in Computational Pathology\n- **AI and Machine Learning Integration:** Significant advancements have been made in integrating artificial intelligence (AI) and machine learning (ML) techniques into digital diagnostics, particularly through methodologies like graph-based deep learning and explainable AI (XAI) [S0347138424] [S8843848626]. These approaches enhance diagnostic accuracy and trust in automated systems by addressing the limitations of traditional methods [S0347138424].\n\n### Challenges in Integration\n- **Model Interpretability:** The 'black box' nature of many AI models raises concerns about accountability and trust among healthcare professionals, which is crucial for their acceptance in clinical settings [S4279500671] [S8843848626]. There is a pressing need for more transparent and interpretable models that can explain their decision-making processes [S4279500671].\n- **Standardized Protocols:** A lack of standardized protocols for integrating AI systems into existing clinical workflows complicates the adoption process and hinders the seamless integration of digital diagnostics into routine practice [S4279500671] [S1725467410].\n- **Regulatory Frameworks:** The regulatory landscape for AI in healthcare is still evolving, necessitating clear guidelines to ensure that AI systems meet safety and efficacy standards [S4279500671] [S8843848626]. \n- **Validation Frameworks:** Robust validation frameworks are essential to ensure that AI models perform reliably across diverse datasets and clinical settings [S4279500671] [S1725467410]. \n\n### Future Directions\n- **Developing Explainable AI Techniques:** Future research should prioritize the development of explainable AI techniques to enhance model transparency and trustworthiness [S8843848626].\n- **Establishing Standardized Integration Protocols:** There is a need to establish standardized protocols for AI integration into clinical workflows to facilitate smoother adoption [S4279500671] [S1725467410].\n- **Fostering Collaboration:** Collaborative efforts among AI developers, clinicians, and regulatory bodies are crucial to effectively tackle the challenges of integrating digital diagnostics into clinical practice [S4279500671] [S1725467410].\n\n### Conclusion\nThe integration of digital diagnostics into clinical practice through computational pathology is promising but faces significant challenges, particularly regarding model interpretability, standardization, and regulatory frameworks. Addressing these issues through future research and collaboration will be essential for improving patient outcomes and ensuring the successful adoption of AI technologies in healthcare.",
      "pipeline_references": {
        "S4279500671": {
          "id": "S4279500671",
          "text": "Despite the advancements in AI applications for digital pathology, significant challenges remain in the clinical translation of these technologies. A systematic review highlights that while AI models have shown promise in improving diagnostic accuracy, issues such as data privacy, model interpretability, and the need for robust validation against independent datasets persist. These challenges hinder the widespread adoption of AI in clinical workflows, necessitating a collaborative approach among AI developers, healthcare providers, and regulatory bodies to address these barriers effectively.",
          "children": [
            {
              "id": "E5961640803",
              "text": "This paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.",
              "url": "https://openalex.org/W3030790048",
              "openalex_id": "https://openalex.org/W3030790048",
              "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
              "publication_date": "2020-05-29"
            },
            {
              "id": "E5966078882",
              "text": "Artificial Intelligence (AI) techniques of deep learning have revolutionized the disease diagnosis with their outstanding image classification performance. In spite of the outstanding results, the widespread adoption of these techniques in clinical practice is still taking place at a moderate pace. One of the major hindrance is that a trained Deep Neural Networks (DNN) model provides a prediction, but questions about why and how that prediction was made remain unanswered. This linkage is of utmost importance for the regulated healthcare domain to increase the trust in the automated diagnosis system by the practitioners, patients and other stakeholders. The application of deep learning for medical imaging has to be interpreted with caution due to the health and safety concerns similar to blame attribution in the case of an accident involving autonomous cars. The consequences of both a false positive and false negative cases are far reaching for patients' welfare and cannot be ignored. This is exacerbated by the fact that the state-of-the-art deep learning algorithms comprise of complex interconnected structures, millions of parameters, and a \u2018black box\u2019 nature, offering little understanding of their inner working unlike the traditional machine learning algorithms. Explainable AI (XAI) techniques help to understand model predictions which help develop trust in the system, accelerate the disease diagnosis, and meet adherence to regulatory requirements. This survey provides a comprehensive review of the promising field of XAI for biomedical imaging diagnostics. We also provide a categorization of the XAI techniques, discuss the open challenges, and provide future directions for XAI which would be of interest to clinicians, regulators and model developers.",
              "url": "https://openalex.org/W4321350922",
              "openalex_id": "https://openalex.org/W4321350922",
              "title": "Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks",
              "publication_date": "2023-02-20"
            }
          ]
        },
        "S0347138424": {
          "id": "S0347138424",
          "text": "Graph-based deep learning techniques have emerged as a promising methodology in computational pathology, addressing the limitations of traditional convolutional neural networks (CNNs) that often fail to capture global contextual information in histopathological images. These techniques utilize entity-graph constructions to model tissue representations and interactions at both intra- and inter-entity levels, significantly enhancing tumor localization, classification, and survival prediction tasks. A systematic review of current applications indicates that graph-based methods outperform traditional approaches in various diagnostic tasks, suggesting a paradigm shift in how digital pathology is approached. Future research directions include refining graph architectures and exploring their applications across different organ systems and scales.",
          "children": [
            {
              "id": "E5826217745",
              "text": "With the remarkable success of representation learning for prediction problems, we have witnessed a rapid expansion of the use of machine learning and deep learning for the analysis of digital pathology and biopsy image patches. However, learning over patch-wise features using convolutional neural networks limits the ability of the model to capture global contextual information and comprehensively model tissue composition. The phenotypical and topological distribution of constituent histological entities play a critical role in tissue diagnosis. As such, graph data representations and deep learning have attracted significant attention for encoding tissue representations, and capturing intra- and inter- entity level interactions. In this review, we provide a conceptual grounding for graph analytics in digital pathology, including entity-graph construction and graph architectures, and present their current success for tumor localization and classification, tumor invasion and staging, image retrieval, and survival prediction. We provide an overview of these methods in a systematic manner organized by the graph representation of the input image, scale, and organ on which they operate. We also outline the limitations of existing techniques, and suggest potential future research directions in this domain.",
              "url": "https://openalex.org/W3175191608",
              "openalex_id": "https://openalex.org/W3175191608",
              "title": "A survey on graph-based deep learning for computational histopathology",
              "publication_date": "2021-12-21"
            }
          ]
        },
        "S8843848626": {
          "id": "S8843848626",
          "text": "The integration of explainable artificial intelligence (XAI) techniques in biomedical imaging is crucial for enhancing the trustworthiness of AI-driven diagnostic systems. Current studies indicate that while deep learning models demonstrate high accuracy in image classification tasks, their 'black box' nature raises concerns regarding interpretability and accountability in clinical settings. Implementing XAI can help elucidate the decision-making processes of these models, thereby fostering greater acceptance among healthcare professionals and patients. The ongoing development of XAI frameworks is essential to meet regulatory requirements and ensure safe deployment in clinical practice.",
          "children": [
            {
              "id": "E5966078882",
              "text": "Artificial Intelligence (AI) techniques of deep learning have revolutionized the disease diagnosis with their outstanding image classification performance. In spite of the outstanding results, the widespread adoption of these techniques in clinical practice is still taking place at a moderate pace. One of the major hindrance is that a trained Deep Neural Networks (DNN) model provides a prediction, but questions about why and how that prediction was made remain unanswered. This linkage is of utmost importance for the regulated healthcare domain to increase the trust in the automated diagnosis system by the practitioners, patients and other stakeholders. The application of deep learning for medical imaging has to be interpreted with caution due to the health and safety concerns similar to blame attribution in the case of an accident involving autonomous cars. The consequences of both a false positive and false negative cases are far reaching for patients' welfare and cannot be ignored. This is exacerbated by the fact that the state-of-the-art deep learning algorithms comprise of complex interconnected structures, millions of parameters, and a \u2018black box\u2019 nature, offering little understanding of their inner working unlike the traditional machine learning algorithms. Explainable AI (XAI) techniques help to understand model predictions which help develop trust in the system, accelerate the disease diagnosis, and meet adherence to regulatory requirements. This survey provides a comprehensive review of the promising field of XAI for biomedical imaging diagnostics. We also provide a categorization of the XAI techniques, discuss the open challenges, and provide future directions for XAI which would be of interest to clinicians, regulators and model developers.",
              "url": "https://openalex.org/W4321350922",
              "openalex_id": "https://openalex.org/W4321350922",
              "title": "Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks",
              "publication_date": "2023-02-20"
            }
          ]
        },
        "S1725467410": {
          "id": "S1725467410",
          "text": "The application of machine learning and artificial intelligence in the analysis of histopathological images has led to the development of computer-aided diagnosis (CAD) systems that assist pathologists in early cancer detection. Recent studies indicate that CAD systems utilizing deep learning techniques have achieved accuracy rates exceeding 90% in identifying malignant tissues. However, the integration of these systems into routine clinical practice is still limited due to concerns regarding their reliability and the need for comprehensive training datasets. Future research should focus on enhancing the robustness of these models and establishing standardized protocols for their implementation in clinical settings.",
          "children": [
            {
              "id": "E2261642689",
              "text": "Histopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to disease progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field\u2019s progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.",
              "url": "https://openalex.org/W3089090082",
              "openalex_id": "https://openalex.org/W3089090082",
              "title": "Deep neural network models for computational histopathology: A survey",
              "publication_date": "2021-01-01"
            },
            {
              "id": "E9731184825",
              "text": "Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe.",
              "url": "https://openalex.org/W2103243046",
              "openalex_id": "https://openalex.org/W2103243046",
              "title": "Histopathological Image Analysis: A Review",
              "publication_date": "2009-01-01"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W3089090082",
        "https://openalex.org/W4321350922",
        "https://openalex.org/W2103243046",
        "https://openalex.org/W3175191608",
        "https://openalex.org/W3030790048"
      ],
      "evaluation": {
        "precision@10": 0.2,
        "recall@10": 0.0015625,
        "f1@10": 0.0031007751937984496,
        "rouge_1": 0.06417569880205362,
        "rouge_2": 0.016262482168330955,
        "rouge_l": 0.032515687393040504,
        "text_f1": 0.10285714285714286,
        "num_source_papers": 5
      }
    }
  ]
}