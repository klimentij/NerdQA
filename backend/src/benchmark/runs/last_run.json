{
  "average_metrics": {
    "precision@10": 0.16111111111111112,
    "recall@10": 0.12,
    "f1@10": 0.13383458646616542,
    "rouge_1": 0.026513055786352036,
    "rouge_2": 0.01023074743548183,
    "rouge_l": 0.015218534698916892,
    "text_f1": 0.06953166304192218
  },
  "papers": [
    {
      "id": "https://openalex.org/W4392203343",
      "limited_meta": {
        "title": "A Comprehensive Survey on Deep Graph Representation Learning",
        "publication_date": "2024-02-27",
        "cited_by_count": 55,
        "url": ""
      },
      "text": "A Comprehensive Survey on Deep Graph Representation\nLearning\nWEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University,\nChina\nZIYUE QIAO, The Hong Kong University of Science and Technology, China\nYIFANG QIN and JIANHAO SHEN, Peking University, China\nFANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA\nJUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China\nYIFAN WANG, University of International Business and Economics, China\nXIAO LUO\u2217, University of California, Los Angeles, USA\nMING ZHANG\u2217, Peking University, China\nGraph representation learning aims to effectively encode high-dimensional sparse graph-structured data into\nlow-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields,\nincluding machine learning and data mining. Classic graph embedding methods follow the basic idea that the\nembedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby\npreserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i)\ntraditional methods have limited model capacity which limits the learning performance; (ii) existing techniques\ntypically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii)\nrepresentation learning and downstream tasks are dependent on each other which should be jointly enhanced.\nWith the remarkable success of deep learning, deep graph representation learning has shown great potential\nand advantages over shallow (traditional) methods, there exist a large number of deep graph representation\nlearning techniques have been proposed in the past decade, especially graph neural networks. In this survey,\nwe conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a\nnew taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential\ncomponents of graph representation learning and categorize existing approaches by the ways of graph neural\nnetwork architectures and the most recent advanced learning paradigms. Moreover, this survey also provides\nthe practical and promising applications of deep graph representation learning. Last but not least, we state\nnew perspectives and suggest challenging directions which deserve further investigations in the future.\nCCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations.\n\u2217Corresponding authors.\nAuthors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn;\nZequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871;\nZiyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453;\nYifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang\nSun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei\nYang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking\nUniversity, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics,\nBeijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang,\nmzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0004-5411/2024/2-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\narXiv:2304.05055v3 [cs.LG] 28 Feb 2024\n2 W. Ju, et al.\nAdditional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural\nNetwork, Survey\nACM Reference Format:\nWei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun,\nZhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024.\nA Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nGraphs have recently emerged as a powerful tool for representing a variety of structured and\ncomplex data, including social networks, traffic networks, information systems, knowledge graphs,\nprotein-protein interaction networks, and physical interaction networks. As a kind of general form\nof data organization, graph structures are capable of naturally expressing the intrinsic relationship\nof these data, and thus can characterize plenty of non-Euclidean structures that are crucial in\na variety of disciplines and domains due to their flexible adaptability. For example, to encode a\nsocial network as a graph, nodes on the graph are used to represent individual users, and edges are\nused to represent the relationship between two individuals, such as friends. In the field of biology,\nnodes can be used to represent proteins, and edges can be used to represent biological interactions\nbetween various proteins, such as the dynamic interactions between proteins. Thus, by analyzing\nand mining the graph-structured data, we can understand the deep meaning hidden behind the\ndata, and further discover valuable knowledge, so as to benefit society and human beings.\nIn the last decade years, a wide range of machine learning algorithms have been developed for\ngraph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410]\nusually break down graphs into different atomic substructures and then use kernel functions\nto measure the similarity between all pairs of them. Although graph kernels could provide a\nperspective on modeling graph topology, these approaches often generate substructures or feature\nrepresentations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer\nfrom high computational complexity, and therefore have weak scalability and subpar performance.\nIn the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have ever\u0002increasing emerged, which attempt to encode the structural information of the graph (usually a\nhigh-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to\npreserve the topology information and attribute information in the embedding space as much\nas possible, so that the learned graph embeddings can be naturally integrated into traditional\nmachine learning algorithms. Compared to previous works which use feature engineering in the\npre-processing phase to extract graph structural features, current graph embedding algorithms are\nconducted in a data-driven way leveraging machine learning algorithms (such as neural networks)\nto encode the structural information of the graph. Specifically, existing graph embedding methods\ncan be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354]\nthat factorize the matrix to learn node embedding which preserves the graph property; (ii) deep\nlearning based methods [155, 362, 443, 460] that apply deep learning techniques specifically de\u0002signed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either\nmaximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these\nmethods typically depend on shallow architectures, and fail to exploit the potential and capacity of\ndeep neural networks, resulting in sub-optimal representation quality and learning performance.\nInspired by the recent remarkable success of deep neural networks, a range of deep learning\nalgorithms has been developed for graph-structured data learning. The core of these methods is to\ngenerate effective node and graph representations using graph neural networks (GNNs), followed\nby a goal-oriented learning paradigm. In this way, the derived representations can be adaptively\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 3\ncoupled with a variety of downstream tasks and applications. Following this line of thought, in\nthis paper, we propose a new taxonomy to classify the existing graph representation learning\nalgorithms, i.e., graph neural network architectures, learning paradigms, and various promising\napplications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the\nstudies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer.\nFor the learning paradigms, we explore three advanced types namely supervised/semi-supervised\nlearning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate\nthe effectiveness of the learned graph representations, we provide several promising applications\nto build tight connections between representation learning and downstream tasks, such as social\nanalysis, molecular property prediction and generation, recommender systems, and traffic analysis.\nLast but not least, we present some perspectives for thought and suggest challenging directions\nthat deserve further study in the future.\nDifferences between this survey and existing ones. Up to now, there exist some other overview\npapers focusing on different perspectives of graph representation learning[17, 50, 53, 57, 227, 499,\n502, 577, 601, 603] that are closely related to ours. However, there are very few comprehensive\nreviews have summarized deep graph representation learning simultaneously from the perspective\nof diverse GNN architectures and corresponding up-to-date learning paradigms. Therefore, we\nhere clearly state their distinctions from our survey as follows. There have been several surveys\non classic graph embedding[42, 151], these works categorize graph embedding methods based on\ndifferent training objectives. Wang et al. [468] goes further and provides a comprehensive review of\nexisting heterogeneous graph embedding approaches. With the rapid development of deep learning,\nthere are a handful of surveys along this line. For example, Wu et al. [499] and Zhang et al. [577]\nmainly focus on several classical and representative GNN architectures without exploring deep\ngraph representation learning from a view of the most recent advanced learning paradigms such as\ngraph self-supervised learning and graph structure learning. Xia et al. [502] and Chami et al. [50]\njointly summarize the studies of graph embeddings and GNNs. Zhou et al. [601] explores different\ntypes of computational modules for GNNs. One recent survey under review [227] categorizes the\nexisting works in graph representation learning from both static and dynamic graphs. However,\nthese taxonomies emphasize the basic GNN methods but pay insufficient attention to the learning\nparadigms, and provide few discussions of the most promising applications, such as recommender\nsystems as well as molecular property prediction and generation. To the best of our knowledge, the\nmost relevant survey published formally is [603], which presents a review of GNN architectures\nand roughly discusses the corresponding applications. Nevertheless, this survey merely covers\nmethods up to the year of 2020, missing the latest developments in the past three years.\nTherefore, it is highly desired to summarize the representative GNN methods, the most recent\nadvanced learning paradigms, and promising applications into one unified and comprehensive\nframework. Moreover, we strongly believe this survey with a new taxonomy of literature and more\nthan 600 studies will strengthen future research on deep graph representation learning.\nContribution of this survey. The goal of this survey is to systematically review the literature\non the advances of deep graph representation learning and discuss further directions. It aims\nto help the researchers and practitioners who are interested in this area, and support them in\nunderstanding the panorama and the latest developments of deep graph representation learning.\nThe key contributions of this survey are summarized as follows:\n\u2022 Systematic Taxonomy. We propose a systematic taxonomy to organize the existing deep\ngraph representation learning approaches based on the ways of GNN architectures and the\nmost recent advanced learning paradigms via providing some representative branches of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n4 W. Ju, et al.\nGraph Self-Supervised Learning\nSemi-Supervised Learning on Graphs\nGraph Structure Learning\nLearning Paradigms\nGraph-Related Applications\nMolecular Generation\nMolecular Property Prediction\nSocial Analysis\nRecommender Systems\nTra c Analysis\nFuture Directions\nGraph Neural Network Architectures\nGraph Kernel Neural Networks\nGraph Pooling\nGraph Convolutions\nGraph Transformer\nGraph Representations\nGraph Data\nOptimized Graph Representations\nFig. 1. The architecture of this paper.\nmethods. Moreover, several promising applications are presented to illustrate the superiority\nand potential of graph representation learning.\n\u2022 Comprehensive Review. For each branch of this survey, we review the essential components\nand provide detailed descriptions of representative algorithms, and systematically summarize\nthe characteristics to make the overview comparison.\n\u2022 Future Directions. Based on the properties of existing deep graph representation learning\nalgorithms, we discuss the limitations and challenges of current methods and propose the\npotential as well as promising research directions deserving of future investigations.\n2 Background\nIn this section, we first briefly introduce some definitions in deep graph representation learning that\nneed to be clarified, and then we explain the reasons why we need graph representation learning.\n2.1 Problem Definition\nDefinition: Graph. Given a graph \ud835\udc3a = (\ud835\udc49 , \ud835\udc38, X), where \ud835\udc49 = {\ud835\udc631, \u00b7 \u00b7 \u00b7 , \ud835\udc63|\ud835\udc49 | } is the set of nodes,\n\ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52|\ud835\udc49 | } is the set of edges, and the edge \ud835\udc52 = (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38 represent the connection\nrelationship between nodes \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57in the graph. X \u2208 R\n|\ud835\udc49 |\u00d7\ud835\udc40 is the node feature matrix with\n\ud835\udc40 being the dimension of each node feature. The adjacency matrix of a graph can be defined as\nA \u2208 R\n|\ud835\udc49 |\u00d7 |\ud835\udc49 |\n, where A\ud835\udc56\ud835\udc57 = 1 if (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38, otherwise A\ud835\udc56\ud835\udc57 = 0.\nThe adjacency matrix can be regarded as the structural representation of the graph-structured\ndata, in which each row of the adjacency matrix A represents the connection relationship between\nthe corresponding node of the row and all other nodes, which can be regarded as a discrete repre\u0002sentation of the node. However, in real-life circumstances, the adjacency matrix A corresponding\nto \ud835\udc3a is a highly sparse matrix, and if A is used directly as node representations, it will be seriously\naffected by impractical storage demands and computational overhead. The storage space of the\nadjacency matrix A is |\ud835\udc49 |\u00d7 |\ud835\udc49 |, which is usually unacceptable when the total number of nodes grows\nto the order of millions. At the same time, the value of most dimensions in the node representation\nis 0. The sparsity will make subsequent machine learning tasks very difficult.\nGraph representation learning is a bridge between the original input data and the task objectives\nin the graph. The fundamental idea of the graph representation learning algorithm is first to learn\nthe embedded representations of nodes or the entire graph from the input graph structure data and\nthen apply these embedded representations to downstream related tasks, such as node classification,\ngraph classification, link prediction, community detection, and visualization, etc. Specifically, it\naims to learn low-dimensional, dense distributed embedding representations for nodes in the graph.\nFormally, the goal of graph representation learning is to learn its embedding vector representation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 5\nTable 1. Summary of traditional graph embedding methods.\nType Method Similarity measure Loss function (\ud835\udc3f)\nMatrix Factorization\nLLE [390] general |\ud835\udc67\ud835\udc56 \u2212\n\u00cd\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56 \ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57\n|\n2\nLE [11] general \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d,s.t.\ud835\udc4d\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c\nGF [4] \ud835\udc34\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nGraRep [46] \ud835\udc34\ud835\udc56,\ud835\udc57, \ud835\udc342\n\ud835\udc56,\ud835\udc57, ..., \ud835\udc34\ud835\udc58\n\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56\n, \ud835\udc67\ud835\udc57\u27e9|2\nHOPE [354] general |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nRandom Walk\nDeepWalk [362] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nNode2vec [155] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nHARP [59] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nLINE [443] Two-order Similarities Corresponding Loss\nNon-GNN Deep SDNE [460] Two-order Proximities Corresponding Loss\nDNGR [47] Two-order Proximities Corresponding Loss\n\ud835\udc45\ud835\udc63 \u2208 R\n\ud835\udc51\nfor each node \ud835\udc63 \u2208 \ud835\udc49 , where the dimension \ud835\udc51 of the vector is much smaller than the total\nnumber of nodes |\ud835\udc49 | in the graph.\n2.2 Traditional Graph Embedding\nTraditional graph embedding learning methods, as part of dimensionality reduction techniques,\naimed to embed graph data into a lower-dimensional vector space with the idea that connected nodes\nin the graph should still be closer to each other in this lower-dimensional space, thereby preserving\nthe structural information between nodes in the graph. Influenced by classical dimensionality\nreduction techniques, early graph embedding methods are primarily inspired by classic matrix\nfactorization techniques [25] and multi-dimensional scaling [245]. The following three sections\ndescribe these methods in more detail, distinguishing among matrix factorization-based methods,\nrandom walks-based methods and other non-GNN deep methods. In Table 1, we summarize different\ncategories of traditional graph embedding methods.\n2.2.1 Matrix factorization-based methods Matrix factorization-based methods are the early en\u0002deavors in graph embedding learning. These approaches can be outlined in a two-step process.\nIn the initial step, a proximity-based matrix is constructed for the graph, where each element of\nthe matrix represents the proximity measure between two nodes in the graph. Subsequently, a\ndimensionality reduction technique is employed on this matrix in the second step to generate the\nnode embeddings.\nLocally Linear Embedding (LLE) [390]. LLE assumes that node representations are sampled from\nthe same manifold space, and any node in the graph and its neighboring nodes are located in a\nlocal region of that manifold space. Therefore, node representations can be obtained by linearly\ncombining them with their neighboring nodes. LLE first constructs a local reconstruction weight\nmatrix, \ud835\udc4a\ud835\udc56\ud835\udc57 , for nodes in the graph to linearly combine neighboring nodes. By computing the\ndistance between the linear combination and the central node, the problem is reduced to solving\nfor matrix eigenvalues to learn low-dimensional vector representations for nodes. The objective\nfunction is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56 \u2212\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56\n\ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57|\n2\n, (1)\nwhere \ud835\udc67\ud835\udc56 represents the low-dimensional representation of the \ud835\udc56-th node, and \ud835\udc41\ud835\udc56is the set of\nneighboring nodes for the central node \ud835\udc56.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n6 W. Ju, et al.\nLaplacian Eigenmaps (LE) [11]. LE believes that nodes directly connected in graph data should\nbe kept as close as possible in the embedding space. Specifically, it achieves this by defining the\ndistance between connected nodes in the embedding space using the square of the Euclidean\ndistance. It transforms the final optimization objective into the computation of the Laplacian\nmatrix\u2019s eigenvectors. The objective function is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57\n|\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|\n2\ud835\udc4a\ud835\udc56\ud835\udc57 = \ud835\udc4d\ud835\udc47\n\ud835\udc3f\ud835\udc4d, s.t. \ud835\udc4d\n\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c, (2)\nwhere \ud835\udc4a\ud835\udc56\ud835\udc57 represents the connection weight between nodes \ud835\udc56 and \ud835\udc57 in the graph. After linear\ntransformation, the optimization of \ud835\udf19 (\ud835\udc4d) can be reformulated as \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d, where \ud835\udc3f = \ud835\udc37 \u2212\ud835\udc4a is the\nconstructed graph Laplacian matrix, and \ud835\udc37 is a symmetric matrix.\nGraph Factorization (GF) [4]. The matrix eigenvector-based methods mentioned before consider\nthe similarity between nodes throughout the entire graph, which can result in excellent node\nfeature representations. However, with the ever-growing scale of real-world graph data, computing\nmatrix eigenvectors for large graphs can be computationally expensive and memory-intensive.\nGF introduces a graph embedding method with a time complexity of \ud835\udc42(|\ud835\udc38|) by factorizing the\nadjacency matrix of the graph. The objective function is as follows:\n\ud835\udf19 (\ud835\udc4d, \ud835\udf06) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57 \u2208\ud835\udc38\n|\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2+\n\ud835\udf06\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56|\n2\n, (3)\nwhere \ud835\udf06 is a regularization coefficient, and \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9 represents the corresponding inner-product\noperation. Moreover, these inner-product methods also contain GraRep [46] and HOPE [354], which\nconsider higher-order and general node similarity respectively.\n2.2.2 Random walk-based methods Random walk-based methods have also attracted a lot of\nattention in graph embedding learning. The basic idea of these methods is to create random walks\namong nodes in the graph to capture its structural characteristics. Thus, nodes tend to have similar\nembedding if they co-occur on short random walks. Compared to fixed proximity measures in\ntraditional matrix factorization-based methods, these approaches use co-occurrence in a random\nwalk as a measure of node similarity, which is more flexible and has demonstrated promising\nperformance across various applications.\nDeepWalk [362]. DeepWalk analogizes nodes in a graph to words in text. It uses random walks\non the graph to generate numerous node sequences \ud835\udc46 = {\ud835\udc631, . . . , \ud835\udc63|\ud835\udc60 | }, treating these sequences as\nsentences, and then inputting them into the Word2vec [343], which aims to maximize the probability\nof node context given the target node \ud835\udc63\ud835\udc56. It can be written as:\n1\n|\ud835\udc46 |\n\u2211\ufe01\n|\ud835\udc46 |\n\ud835\udc56=1\n\u2211\ufe01\n\u2212\ud835\udc61 \u2264\ud835\udc57\u2264\ud835\udc61,\ud835\udc57\u22600\nlog \ud835\udc5d(\ud835\udc63\ud835\udc56+\ud835\udc57|\ud835\udc63\ud835\udc56), (4)\nwhere \ud835\udc61 is the context window size. Compared to matrix factorization-based methods, DeepWalk\nexhibits extremely low time complexity and is suitable for large-scale graph representation learning.\nHowever, DeepWalk only considers local information between nodes in the graph, making it\nchallenging to find the optimal random walk sampling sequences.\nNode2vec [155]. Based on DeepWalk, Node2vec utilizes parameters \ud835\udc5d and \ud835\udc5e to guide the random\nwalk. Parameter \ud835\udc5d allows the algorithm to revisit previously traversed nodes \ud835\udc61, with smaller\nvalues of \ud835\udc5d increasing the likelihood of returning to \ud835\udc61. Parameter \ud835\udc5e facilitates both inward and\noutward exploration; when \ud835\udc5e > 1, the algorithm tends to visit nodes closer to \ud835\udc61; while for \ud835\udc5e  1. And the first-order similarity can\nbe defined as:\n\ud835\udc3f2 =\n\u2211\ufe01\n(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57 ) \u2208\ud835\udc38\n\ud835\udc34\ud835\udc56\ud835\udc57 |\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|, (8)\nwhere \ud835\udc67\ud835\udc56is the learned representation of node \ud835\udc63\ud835\udc56.\nDeep Neural Graph Representations (DNGR) [47]. Similar to SDNE, DNGR utilizes pointwise\nmutual information between two nodes co-occurring in random walks instead of the adjacency\nmatrix values.\n2.3 Why study deep graph representation learning\nWith the rapid development of deep learning techniques, deep neural networks such as convolu\u0002tional neural networks and recurrent neural networks have made breakthroughs in the fields of\ncomputer vision, natural language processing, and speech recognition. They can well abstract the\nsemantic information of images, natural languages, and speeches. However, current deep learning\ntechniques fail to handle more complex and irregular graph-structured data. To effectively analyze\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n8 W. Ju, et al.\nand model this kind of non-Euclidean structure data, many graph representation learning algo\u0002rithms have emerged in recent years, including graph embedding and graph neural networks. At\npresent, compared with Euclidean-style data such as images, natural language, and speech, graph\u0002structured data is high-dimensional, complex, and irregular. Therefore, the graph representation\nlearning algorithm is a rather powerful tool for studying graph-structured data. To encode complex\ngraph-structured data, deep graph representation learning needs to meet several characteristics: (1)\ntopological properties: Graph representations need to capture the complex topological information\nof the graph, such as the relationship between nodes and nodes, and other substructure information,\nsuch as subgraphs, motif, etc; (2) feature attributes: It is necessary for graph representations to de\u0002scribe high-dimensional attribute features in the graph, including the attributes of nodes and edges\nthemselves; (3) scalability: Because different real graph data have different characteristics, graph\nrepresentation learning algorithms should be able to efficiently learn its embedding representation\non different graph structure data, making it universal and transferable.\n3 Graph Convolutions\nGraph convolutions have become the basic building blocks in many deep graph representation\nlearning algorithms and graph neural networks developed recently. In this section, we provide a\ncomprehensive review of graph convolutions, which generally fall into two categories: spectral\ngraph convolutions and spatial graph convolutions. Based on the solid mathematical foundations\nof Graph Signal Processing (GSP) [164, 396, 414], spectral graph convolutions seek to capture the\npatterns of the graph in the frequency domain. On the other hand, spatial graph convolutions\ninherit the idea of message passing from Recurrent Graph Neural Networks (RecGNNs), and they\ncompute node features by aggregating the features of their neighbors. Thus, the computation graph\nof a node is derived from the local graph structure around it, and the graph topology is naturally\nincorporated into the way node features are computed. In this section, we first introduce spectral\ngraph convolutions and then spatial graph convolutions, followed by a brief summary. In Table 2,\nwe summarize a number of graph convolutions proposed in recent years.\n3.1 Spectral Graph Convolutions\nWith the success of Convolutional Neural Networks (CNNs) in computer vision [244], efforts have\nbeen made to transfer the idea of convolution to the graph domain. However, this is not an easy task\nbecause of the non-Euclidean nature of graphical data. Graph signal processing (GSP) [164, 396, 414]\ndefines the Fourier Transform on graphs and thus provides a solid theoretical foundation of spectral\ngraph convolutions.\nIn graph signal processing, a graph signal refers to a set of scalars associated with every node\nin the graph, i.e. \ud835\udc53 (\ud835\udc63), \u2200\ud835\udc63 \u2208 \ud835\udc49 , and it can be written in the \ud835\udc5b-dimensional vector form x \u2208 R\n\ud835\udc5b\n,\nwhere \ud835\udc5b is the number of nodes in the graph. Another core concept of graph signal processing\nis the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian), defined as\nL = I \u2212 D\n\u22121/2AD\u22121/2\n, where I is the identity matrix, D is the degree matrix (i.e. a diagonal matrix\nD\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc57 A\ud835\udc56\ud835\udc57), and A is the adjacency matrix. In the typical setting of graph signal processing, the\ngraph \ud835\udc3a is undirected. Therefore, L is real symmetric and positive semi-definite. This guarantees\nthe eigen decomposition of the graph Laplacian: L = U\u039bU\ud835\udc47, where U = [u0, u1, ..., u\ud835\udc5b\u22121] is the\neigenvectors of the graph Laplacian and the diagonal elements of \u039b = diag(\ud835\udf060, \ud835\udf061, ..., \ud835\udf06\ud835\udc5b\u22121) are the\neigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal x is defined as x\u02dc = U\n\ud835\udc47 x,\nwhere x\u02dc is the graph frequencies of x. Correspondingly, the Inverse Graph Fourier Transform can\nbe written as x = Ux\u02dc.\nWith GFT and the Convolution Theorem, the graph convolution of a graph signal x and a filter\ng can be defined as g \u2217\ud835\udc3a x = U(U\n\ud835\udc47 g \u2299 U\ud835\udc47 x). To simplify this, let g\ud835\udf03 = diag(U\ud835\udc47 \ud835\udc54), the graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 9\nTable 2. Summary of graph convolution methods.\nMethod Category Aggregation Time Complexity\nSpectral CNN [39] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nHenaff et al. [172] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nChebNet [83] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGCN [230] Spectral / Spatial Weighted Average \ud835\udc42(\ud835\udc5a)\nCayleyNet [254] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGraphSAGE [163] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nGAT [452] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nDGCNN [477] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nLanzcosNet [280] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n2\n)\nSGC [493] Spatial Graph Convolution Weighted Average \ud835\udc42(\ud835\udc5a)\nGWNN [512] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGIN [518] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nGraphAIR [179] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nPNA [77] Spatial Graph Convolution Multiple \ud835\udc42(\ud835\udc5a)\nS\n2GC [606] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGNNML3 [21] Spatial / Spectral - \ud835\udc42(\ud835\udc5a)\nMSGNN [170] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nEGC [437] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nAPPNP [138] Spatial Graph Convolution (Approximate) Personalized Pagerank \ud835\udc42(\ud835\udc5a)\nGCNII [61] Spatial Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGATv2 [38] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nconvolution can be written as:\ng \u2217\ud835\udc3a x = Ug\ud835\udf03U\n\ud835\udc47\nx, (9)\nwhich is the general form of most spectral graph convolutions. The key of spectral graph convolu\u0002tions is to parameterize and learn the filter g\ud835\udf03 .\nSpectral Convolutional Neural Network (Spectral CNN) [39] sets graph filter as a learnable diagonal\nmatrix W. The convolution operation can be written as y = UWU\ud835\udc47 x. In practice, multi-channel\nsignals and activation functions are common, and the graph convolution can be written as\nY:,\ud835\udc57 = \ud835\udf0e\nU\n\u2211\ufe01\ud835\udc50\ud835\udc56\ud835\udc5b\n\ud835\udc56=1\nW\ud835\udc56,\ud835\udc57U\n\ud835\udc47 X:,\ud835\udc56!\n, \ud835\udc57 = 1, 2, ..., \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61, (10)\nwhere \ud835\udc50\ud835\udc56\ud835\udc5b is the number of input channel, \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 is the number of output channel, X is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc56\ud835\udc5b\nmatrix representing the input signal, Y is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix denoting the output signal, W\ud835\udc56,\ud835\udc57 is a\nparameterized diagonal matrix, and \ud835\udf0e(\u00b7) is the activation function. For mathematical convenience\nwe sometimes use single-channel versions of graph convolutions omitting activation functions,\nand the multi-channel versions are similar to Eq. 10.\nSpectral CNN has several limitations. Firstly, the filters are basis-dependent, which means that\nthey cannot be generalized across graphs. Secondly, the algorithm requires eigen decomposition,\nwhich is computationally expensive. Thirdly, it has no guarantee of spatial localization of filters.\nTo make filters spatially localized, Henaff et al. [172] propose to use a smooth spectral transfer\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n10 W. Ju, et al.\nfunction \u0398(\u039b) to parameterize the filter, and the convolution operation can be written as:\ny = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx. (11)\nChebyshev Spectral Convolutional Neural Network (ChebNet) [83] extends this idea by using\ntruncated Chebyshev polynomials to approximate the spectral transfer function. The Chebyshev\npolynomial is defined as \ud835\udc470 (\ud835\udc65) = 1, \ud835\udc471 (\ud835\udc65) = \ud835\udc65, \ud835\udc47\ud835\udc58 (\ud835\udc65) = 2\ud835\udc65\ud835\udc47\ud835\udc58\u22121 (\ud835\udc65) \u2212 \ud835\udc47\ud835\udc58\u22122 (\ud835\udc65), and the spectral\ntransfer function \ud835\udc39 (\u039b) is approximated to the order of \ud835\udc3e \u2212 1 as\n\ud835\udc39 (\u039b) =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc ), (12)\nwhere the model parameters \ud835\udf03\ud835\udc58, \ud835\udc58 \u2208 {0, 1, ..., \ud835\udc3e \u2212 1} are the Chebyshev coefficients, and \u039b\u02dc =\n2\u039b/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be written\nas:\ng \u2217\ud835\udc3a x = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx = U\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc )U\n\ud835\udc47\nx =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (L\u02dc)x, (13)\nwhere L\u02dc = 2L/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I.\nGraph Convolutional Network (GCN) [230] is proposed as the localized first-order approximation\nof ChebNet. Assuming \ud835\udc3e = 2 and \ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 = 2, Eq. 13 can be simplified as:\ng \u2217\ud835\udc3a x = \ud835\udf030x + \ud835\udf031 (L \u2212 I)x = \ud835\udf030x \u2212 \ud835\udf031D\n\u22121/2AD\u22121/2\nx. (14)\nTo further constraint the number of parameters, we assume \ud835\udf03 = \ud835\udf030 = \u2212\ud835\udf031, which gives a simpler\nform of graph convolution:\ng \u2217G x = \ud835\udf03 (I + D\n\u22121/2AD\u22121/2\n)x. (15)\nAs I + D\n\u22121/2AD\u22121/2 now has the eigenvalues in the range of [0, 2] and repeatedly multiplying\nthis matrix can lead to numerical instabilities, GCN empirically proposes a renormalization trick to\nsolve this problem by using D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2instead, where A\u02dc = A + I and D\u02dc\n\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc56 A\u02dc\n\ud835\udc56\ud835\udc57 .\nAllowing multi-channel signals and adding activation functions, the more common formula in\nliterature is:\nY = \ud835\udf0e( (D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2)X\u0398), (16)\nwhere X, Y have the same shape as in Eq. 10 and \u0398 is a \ud835\udc50\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix as model\u2019s parameters.\nApart from the aforementioned methods, other spectral graph convolutions have been proposed.\nLevie et al. [254] propose CayleyNets that utilize Cayley Polynomials to equip the filters with\nthe ability to detect narrow frequency bands. Liao et al. [280] propose LanczosNets that employ\nthe Lanczos algorithm to construct a low-rank approximation of graph Laplacian to improve the\ncomputation efficiency of graph convolutions. The proposed model is able to efficiently utilize the\nmulti-scale information in the graph data. Instead of using Graph Fourier Transform, Xu et al. [512]\npropose a Graph Wavelet Neural Network (GWNN) that uses graph wavelet transform to avoid\nmatrix eigendecomposition. Moreover, graph wavelets are sparse and localized, which provides\ngood interpretations for the convolution operation. Zhu and Koniusz [606] derive a Simple Spectral\nGraph Convolution (S2GC) from a modified Markov Diffusion Kernel, which achieves a trade-off\nbetween low-pass and high-pass filter bands.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 11\n3.2 Spatial Graph Convolutions\nInspired by the convolution on Euclidean data (e.g. images and texts), which applies data trans\u0002formation on a small region, spatial graph convolutions compute the central node\u2019s feature via\ntransforming and aggregating its neighbors\u2019 features. In this way, the graph structure is naturally\nembedded in the computation graph of node features. Moreover, the idea of sending one node\u2019s\nfeature to another node is similar to the message passing used in recurrent graph neural networks.\nIn the following, we will introduce several seminal spatial graph convolutions as well as some\nrecently proposed promising methods.\nSpatial graph convolutions generally follow a three-step paradigm: message generation, feature\naggregation and feature update. This can be mathematically written as:\ny\ud835\udc56 = UPDATE x\ud835\udc56, AGGREGATE {MESSAGE x\ud835\udc56, x\ud835\udc57, e\ud835\udc56\ud835\udc57\u0001, \ud835\udc57 \u2208 N (\ud835\udc56)}\u0001\u0001 , (17)\nwhere x\ud835\udc56 and y\ud835\udc56is the input and output feature vector of node \ud835\udc56, e\ud835\udc56\ud835\udc57 is the feature vector of the edge\n(or more generally, the relationship) between node \ud835\udc56 and its neighbor node \ud835\udc57, and N (\ud835\udc56) denote the\nneighbor of node \ud835\udc56, which could be more generally defined.\nIn the previous subsection, we show the spectral interpretation of GCN [230]. The model also\nhas its spatial interpretation, which can be mathematically written as:\ny\ud835\udc56 = \u0398\n\ud835\udc47 \u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\u222a\ud835\udc56\n1\n\u221a\ufe03\n\u02c6\ud835\udc51\ud835\udc56\u02c6\ud835\udc51\ud835\udc57\nx\ud835\udc57, (18)\nwhere \u02c6\ud835\udc51\ud835\udc56 and \u02c6\ud835\udc51\ud835\udc57is the \ud835\udc56-th and \ud835\udc57-th row sums of A\u02c6 in Eq. 16. For each node, the model takes a\nweighted sum of its neighbors\u2019 features as well as its own features and applies a linear transformation\nto obtain the result. In practice, multiple GCN layers are often stacked together with non-linear\nfunctions after convolution to encode complex and hierarchical features. Nonetheless, Wu et al.\n[493] show that the model still achieves competitive results without non-linearity.\nAlthough GCN as well as other spectral graph convolutions achieve competitive results on a\nnumber of benchmarks, these methods assume the presence of all nodes in the graph and fall in the\ncategory of transductive learning. Hamilton et al. [163] propose GraphSAGE that performs graph\nconvolutions in inductive settings, when there are new nodes during inference (e.g. newcomers\nin the social network). For each node, the model samples its \ud835\udc3e-hop neighbors and uses \ud835\udc3e graph\nconvolutions to aggregate their features hierarchically. Furthermore, the use of sampling also\nreduces the computation when a node has too many neighbors.\nThe attention mechanism has been successfully used in natural language processing [451], com\u0002puter vision [295] and multi-modal tasks [62, 168, 552, 591]. Graph Attention Networks (GAT) [452]\nintroduces the idea of attention to graphs. The attention mechanism uses an adaptive, feature\u0002dependent weight (i.e. attention coefficient) to aggregate a set of features, which can be mathemati\u0002cally written as:\n\ud835\udefc\ud835\udc56,\ud835\udc57 =\nexp LeakyReLU a\n\ud835\udc47\n[\u0398x\ud835\udc56||\u0398x\ud835\udc57]\n\u0001\u0001\n\u00cd\n\ud835\udc58 \u2208N (\ud835\udc56)\u222a{\ud835\udc56 } exp\nLeakyReLU a\n\ud835\udc47 [\u0398x\ud835\udc56\n||\u0398x\ud835\udc57]\n\u0001\u0001 , (19)\nwhere \ud835\udefc\ud835\udc56,\ud835\udc57 is the attention coefficient, a and \u0398 are model parameters, and [\u00b7||\u00b7] means concatenation.\nAfter the \ud835\udefcs are obtained, the new features are computed as a weighted sum of input node features,\nwhich is:\ny\ud835\udc56 = \ud835\udefc\ud835\udc56,\ud835\udc56\u0398x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\n\ud835\udefc\ud835\udc56,\ud835\udc57\u0398x\ud835\udc57. (20)\nXu et al. [518] explore the representational limitations of graph neural networks. What they\ndiscover is that message passing networks like GCN [230] and GraphSAGE [163] are incapable of\ndistinguishing certain graph structures. To improve the representational power of graph neural\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n12 W. Ju, et al.\nnetworks, they propose the Graph Isomorphism Network (GIN) that gives an adjustable weight to\nthe central node feature, which can be mathematically written as:\ny\ud835\udc56 = MLP \u00a9\n\u00ad\n\u00ab\n(1 + \ud835\udf16)x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\nx\ud835\udc57\n\u00aa\n\u00ae\n\u00ac\n, (21)\nwhere \ud835\udf16 is a learnable parameter.\nMore recently, efforts have been made to improve the representational power of graph neural\nnetworks. For example, Hu et al. [179] propose GraphAIR that explicitly models the neighborhood\ninteraction to better capture complex non-linear features. Specifically, they use the Hadamard\nproduct between pairs of nodes in the neighborhood to model the quadratic terms of neighborhood\ninteraction. Balcilar et al. [21] propose GNNML3 that breaks the limits of the first-order Weisfeiler\u0002Lehman test (1-WL) and reaches the third-order WL test (3-WL) experimentally. They also show\nthat the Hadamard product is required for the model to have more representational power than the\nfirst-order Weisfeiler-Lehman test. Other elements in spatial graph convolutions are widely studied.\nFor example, Corso et al. [77] explore the aggregation operation in GNN and proposes Principal\nNeighbourhood Aggregation (PNA) that uses multiple aggregators with degree-scalers. Tailor et al.\n[437] explore the anisotropism and isotropism in the message passing process of graph neural\nnetworks, and proposes Efficient Graph Convolution (EGC) that achieves promising results with\nreduced memory consumption due to isotropism. In order to increase the size of the neighborhood\nof a node, Gasteiger et al. [138] propose personalized propagation of neural predictions (PPNP) and\nits approximation using power iteration (APPNP). To increase the depth of graph neural networks,\nChen et al. [61] propose GCNII that uses initial residual and identity mapping to mitigate the over\u0002smoothing problem. Brody et al. [38] propose GATv2 that uses dynamic attention and improves\nthe expressive power of GAT [452].\n3.3 Summary\nThis section introduces graph convolutions. We provide the summary as follows:\n\u2022 Techniques. Graph convolutions mainly fall into two types, i.e. spectral graph convolu\u0002tions and spatial graph convolutions. Spectral graph convolutions have solid mathematical\nfoundations of Graph Signal Processing and therefore their operations have theoretical in\u0002terpretations. Spatial graph convolutions are inspired by Recurrent Graph Neural Networks\nand their computation is simple and straightforward, as their computation graph is derived\nfrom the local graph structure. Generally, spatial graph convolutions are more common in\napplications.\n\u2022 Challenges and Limitations. Despite the great success of graph convolutions, their perfor\u0002mance is unsatisfactory in more complicated applications. On the one hand, the performance\nof graph convolutions relies heavily on the construction of the graph. Different constructions\nof the graph might result in different performances of graph convolutions. On the other\nhand, graph convolutions are prone to over-smoothing when constructing very deep neural\nnetworks.\n\u2022 Future Works. In the future, we expect that more powerful graph convolutions will be\ndeveloped to mitigate the problem of over-smoothing and we also hope that techniques and\nmethodologies in Graph Structure Learning (GSL) can help learn more meaningful graph\nstructure to benefit the performance of graph convolutions.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 13\n4 Graph Kernel Neural Networks\nGraph kernels (GKs) are historically the most widely used technique on graph analyzing and\nrepresentation tasks [137, 243, 423, 601]. However, traditional graph kernels rely on hand-crafted\npatterns or domain knowledge on specific tasks[242, 410]. Over the years, an amount of research has\nbeen conducted on graph kernel neural networks (GKNNs), which has yielded promising results.\nResearchers have explored various aspects of GKNNs, including their theoretical foundations,\nalgorithmic design, and practical applications. These efforts have led to the development of a wide\nrange of GKNN-based models and methods that can be used for graph analysis and representation\ntasks, such as node classification [113, 222, 298, 534], link prediction [54, 300, 497, 525], and graph\nclustering [243, 299].\nThe success of GKNNs can be attributed to their ability to leverage the strengths of both graph\nkernels and neural networks [221, 299, 497]. By using kernel functions to measure similarity\nbetween graphs, GKNNs can capture the structural properties of graphs, while the use of neural\nnetworks enables them to learn more complex and abstract representations of graphs [56, 558].\nThis combination of techniques allows GKNNs to achieve state-of-the-art performance on a wide\nrange of graph-related tasks [216, 243, 479].\nIn this section, we begin with introducing the most representative traditional graph kernels.\nThen we summarize the basic framework for combining GNNs and graph kernels. Finally, we\ncategorize the popular graph kernel Neural networks into several categories and compare their\ndifferences.\n4.1 Graph Kernels\nGraph kernels generally evaluate pairwise similarity between nodes or graphs by decomposing\nthem into basic structural units. Random walks [223], subtrees [409], shortest paths [32] and\ngraphlets [410] are representative categories.\nGiven two graphs \ud835\udc3a1 = (\ud835\udc491, \ud835\udc381, \ud835\udc4b1) and \ud835\udc3a2 = (\ud835\udc492, \ud835\udc382, \ud835\udc4b2), a graph kernel function \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2)\nmeasures the similarity between \ud835\udc3a1 and \ud835\udc3a2 through the following formula:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 \ud835\udc59\ud835\udc3a1(\ud835\udc621),\ud835\udc59\ud835\udc3a2(\ud835\udc622)\n\u0001\n, (22)\nwhere \ud835\udc59\ud835\udc3a (\ud835\udc62) denotes a set of local substructures centered at node \ud835\udc62 in graph \ud835\udc3a, and \ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 is a base\nkernel measuring the similarity between the two sets of substructures. For simplicity, we may\nrewrite Eq. 22 as:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 (\ud835\udc621, \ud835\udc622), (23)\nthe uppercase letter \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) is denoted as graph kernels, \ud835\udf05(\ud835\udc621, \ud835\udc622) is denoted as node kernels,\nand lowercase \ud835\udc58 (\ud835\udc65, \ud835\udc66) is denoted as general kernel functions.\nThe kernel mapping of a kernel \ud835\udf13 maps a data point into its corresponding Reproducing Kernel\nHilbert Space (RKHS) H. Specifically, given a kernel \ud835\udc58\u2217 (\u00b7, \u00b7), its kernel mapping\ud835\udf13\u2217 can be formalized\nas,\n\u2200\ud835\udc651, \ud835\udc652, \ud835\udc58\u2217 (\ud835\udc651, \ud835\udc652) = \u27e8\ud835\udf13\u2217 (\ud835\udc651),\ud835\udf13\u2217 (\ud835\udc652)\u27e9H\u2217, (24)\nwhere H\u2217 is the RKHS of \ud835\udc58\u2217 (\u00b7, \u00b7).\nWe introduce several representative and popular graph kernels below.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n14 W. Ju, et al.\nWalk and Path Kernels. A \ud835\udc59-walk kernel \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 compares all length \ud835\udc59 walks starting from each\nnode in two graphs \ud835\udc3a1,\ud835\udc3a2,\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc641\u2208W\ud835\udc59(\ud835\udc3a1,\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc642\u2208W\ud835\udc59(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc4b1 (\ud835\udc641), \ud835\udc4b2 (\ud835\udc642)),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622).\n(25)\nSubstituting W with P is able to get the \ud835\udc59-path kernel.\nSubtree Kernels. The WL subtree kernel is the most popular one in subtree kernels. It is a finite\u0002depth kernel variant of the 1-WL test. The WL subtree kernel with depth \ud835\udc59, \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f compares all\nsubtrees with depth \u2264 \ud835\udc59 rooted at each node.\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc611\u2208 T\ud835\udc56(\ud835\udc3a1,\ud835\udc622 )\n\u2211\ufe01\n\ud835\udc612\u2208 T\ud835\udc56(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc611, \ud835\udc612),\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc59\n\ud835\udc56=0\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2),\n(26)\nwhere \ud835\udc61 \u2208 T (\ud835\udc56)(\ud835\udc3a, \ud835\udc62) denotes a subtree of depth \ud835\udc56 rooted at \ud835\udc62 in \ud835\udc3a.\n4.2 General Framework of GKNNs\nIn this section, we summarize the general framework of GKNNs. For the first step, a kernel that\nmeasures similarities of heterogeneous features from heterogeneous nodes and edges (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc622) and\n(\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622) should be defined. Take the inner product of neighbor tensors as an example, its neighbor\nkernel is defined as follows,\n\ud835\udf05( (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc621), (\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622)) = \u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc621), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc622)\u27e9.\nBased on the neighbor kernel, a kernel with two \ud835\udc59-hop neighborhoods for central node \ud835\udc621 and \ud835\udc622\ncan be defined as \ud835\udc3e\n(\ud835\udc59)\n(\ud835\udc621, \ud835\udc622) =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \ud835\udc59 = 0\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u2211\ufe01\n\ud835\udc631\u2208\ud835\udc41 (\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc632\u2208\ud835\udc41 (\ud835\udc622 )\n\ud835\udc3e\n(\ud835\udc59\u22121)\n(\ud835\udc631, \ud835\udc632) \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc631), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc632\n)\u27e9 \ud835\udc59 > 0\n, (27)\nBy regarding the lower-hop kernel \ud835\udf05\n(\ud835\udc59\u22121)\n(\ud835\udc621, \ud835\udc622), as the inner product of the (\ud835\udc59 \u2212 1)-th hidden\nrepresentations of \ud835\udc621 and \ud835\udc622. Furthermore, by recursively applying the neighborhood kernel, the\n\ud835\udc59-hop graph kernel can be derived as\n\ud835\udc3e\n\ud835\udc59\n(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc981\u2208W\ud835\udc59(\ud835\udc3a1 )\n\u2211\ufe01\n\ud835\udc982\u2208W\ud835\udc59(\ud835\udc3a2 )\n\u00d6\n\ud835\udc59\u22121\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n1\n), \ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n2\n)\u27e9 \u00d7\u00d6\n\ud835\udc59\u22122\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n1\n,\ud835\udc98\n(\ud835\udc56+1)\n1\n), \ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n2\n,\ud835\udc98\n(\ud835\udc56+1)\n2\n)\u27e9!,\n(28)\nwhere W\ud835\udc59(\ud835\udc3a) denotes the set of all walk sequences with length \ud835\udc59 in graph \ud835\udc3a, and \ud835\udc98\n(\ud835\udc56)\n1\ndenotes the\n\ud835\udc56-th node in sequence \ud835\udc981.\nAs shown in Eq. 24, kernel methods implicitly perform projections from original data spaces\nto their RKHS H. Hence, as GNNs also project nodes or graphs into vector spaces, connections\nhave been established between GKs and GNNs through the kernel mappings. And several works\nconducted research on the connections [253, 491], and found some foundation conclusions. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 15\nthe basic rule introduced in [253] as an example, the proposed graph kernel in Eq. 22 can be derived\nas the general formulas,\n\u210e\n(0)\n(\ud835\udc63) =\ud835\udc7e\n(0)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63),\n\u210e\n(\ud835\udc59)\n(\ud835\udc63) =\ud835\udc7e\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63) \u2299 \u2211\ufe01\n\ud835\udc62\u2208\ud835\udc41 (\ud835\udc63)\n(\ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\u210e\n(\ud835\udc59\u22121)\n(\ud835\udc62) \u2299 \ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc38 (\ud835\udc52\ud835\udc62,\ud835\udc63 )\n\ud835\udc53 (\ud835\udc52\ud835\udc62,\ud835\udc63 )), 1  1-WL\nGGT [102] \u2713 \u2713 structure only \u2713\nGTSA [241] \u2713 \u2713 \u2713 \u2713\nHGT [183] \u2713 \u2713\nG2SHGT [536] \u2713 \u2713 \u2713\nHINormer [335] \u2713 \u2713 \u2713\nGRUGT [41] \u2713 \u2713 \u2713\nGRIT [324] \u2713 \u2713 \u2713\nGraphormer-GD [560] \u2713 \u2713 \u2713\nGraphormer [540] \u2713 \u2713 \u2713 \u2713\nGSGT [191] \u2713 \u2713 \u2713\nTMDG [145] \u2713 \u2713 \u2713\nGraph-BERT [567] \u2713 \u2713 \u2713\nLRGT [498] \u2713 \u2713\nSAT [56] \u2713 \u2713 \u2713\n6.1 Transformer\nTransformer [451] was first applied to model machine translation, but two of the key mechanisms\nadopted in this work, attention operation and positional encoding, are highly compatible with the\ngraph modeling problem.\nTo be specific, we denote the input of attention layer in Transformer as X = [x0, x1, . . . , x\ud835\udc5b\u22121],\nx\ud835\udc56 \u2208 R\n\ud835\udc51\n, where \ud835\udc5b is the length of input sequence and \ud835\udc51 is the dimension of each input embedding\nx\ud835\udc56. Then the core operation of calculating new embedding x\u02c6\ud835\udc56 for each x\ud835\udc56in attention layer can be\nstreamlined as:\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57) = NORM\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\nQ\n\u210e\n(x\ud835\udc56)\nTK\u210e\n(x\ud835\udc58 )),\nx\n\u210e\n\ud835\udc56 =\n\u2211\ufe01\nx\ud835\udc57 \u2208X\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57)V\u210e(x\ud835\udc57),\nx\u02c6\ud835\udc56 = MERGE(x\n1\n\ud835\udc56\n, x\n2\n\ud835\udc56\n, . . . , x\n\ud835\udc3b\n\ud835\udc56\n),\n(57)\nwhere \u210e \u2208 {0, 1, . . . , \ud835\udc3b \u2212 1} represents the attention head number. Q\n\u210e\n, K\u210eand V\u210eare projection\nfunctions mapping a vector to the query space, key space and value space respectively. s\n\u210e\n(x\ud835\udc56\n, x\ud835\udc57) is\nscore function measuring the similarity between x\ud835\udc56 and x\ud835\udc57. NORM is the normalization operation\nensuring \u00cd\nx\ud835\udc57 \u2208X s\n\u210e\n(x\ud835\udc56, x\ud835\udc57) \u2261 1 to propel the stability of the output generated by a stack of attention\nlayers, it is usually performed as scaled softmax: NORM(\u00b7) = SoftMax(\u00b7/\u221a\ud835\udc51). And MERGE function\nis designed to combine the information extracted from multiple attention heads. Here, we omit\nfurther implementation details that do not affect our understanding of attention operation.\nThe attention process cannot encode the position information of each x\ud835\udc56, which is essential in\nmachine translation problems. So positional encoding is introduced to remedy this deficiency, and\nit\u2019s calculated as:\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57\n= sin(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), X\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57+1\n= cos(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), (58)\nwhere \ud835\udc56 is the position and \ud835\udc57 is the dimension. The positional encoding is added to the input before\nit is fed to the Transformer.\n6.2 Overview\nFrom the simplified process shown in Eq. 57, we can see that the core of the attention operation is\nto accomplish information transfer based on the similarity between the source and the target to be\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 25\nupdated. It\u2019s quite similar to the message-passing process on a fully-connected graph. However,\nthe direct application of this architecture to arbitrary graphs does not make use of structural\ninformation, so it may lead to poor performance when graph topology is important. On the other\nhand, the definition of positional encoding in graphs is not a trivial problem because the order or\ncoordinates of graph nodes are underdefined.\nAccording to these two challenges, Transformer-based methods for graph representation learning\ncan be classified into two major categories, one considering graph structure during the attention\nprocess, and the other encoding the topological information of the graph into initial node features.\nWe name the first one as Attention Modification and the second one as Encoding Enhancement. A\nsummarization is provided in Table 5. In the following discussion, if both methods are used in\none paper, we will list them in different subsections, and we will ignore the multi-head trick in\nattention operation.\n6.3 Attention Modification\nThis group of works attempts to modify the full attention operation to capture structure information.\nThe most prevalent approach is changing the score function, which is denoted as s(\u00b7, \u00b7) in Eq. 57.\nGGT [102] constrains each node feature can only attend to neighbors and enables the model to\nrepresent edge feature information by rewrite s(\u00b7, \u00b7) as:\ns\u02dc1 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\ud835\udc44x\ud835\udc56)\nT\n(W\ud835\udc3ex\ud835\udc57 \u2299 W\ud835\udc38e\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\u2212 \u221e, otherwise\n,\ns1 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc1 (x\ud835\udc56, x\ud835\udc58 )),\n(59)\nwhere \u2299 is Hadamard product and W\ud835\udc44,\ud835\udc3e,\ud835\udc38 represents trainable parameter matrix. This approach\nis not efficient yet to model long-distance dependencies since only 1st-neighbors are considered.\nThough it adopts Laplacian eigenvectors to gather global information (see Section 6.4), but only long\u0002distance structure information is remedied while the node and edge features are not. GTSA [241]\nimproves this approach by combining the original graph and the full graph. Specifically, it extends\ns1 (\u00b7, \u00b7) to:\ns\u02dc2 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\n\ud835\udc44\n1\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n1\nx\ud835\udc57 \u2299 W\ud835\udc38\n1\ne\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n(W\n\ud835\udc44\n0\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n0\nx\ud835\udc57 \u2299 W\ud835\udc38\n0\ne\ud835\udc57\ud835\udc56), otherwise\n,\ns2 (x\ud835\udc56, x\ud835\udc57) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\n\uf8f3\n1\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2208\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\ud835\udf06\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2209\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), otherwise\n,\n(60)\nwhere \ud835\udf06 is a hyperparameter representing the strength of the full connection.\nSome works try to reduce information-mixing problems [55] in heterogeneous graphs. HGT [183]\ndisentangles the attention of different node types and edge types by adopting additional attention\nheads. It defines W\ud835\udf0f (\ud835\udc63)\n\ud835\udc44,\ud835\udc3e,\ud835\udc49 for each node type \ud835\udf0f (\ud835\udc63) and W\n\ud835\udf19 (\ud835\udc52 )\n\ud835\udc38\nfor each edge type \ud835\udf19 (\ud835\udc52), \ud835\udf0f (\u00b7) and\n\ud835\udf19 (\u00b7) are type indicating function. G2SHGT [536] defines four types of subgraphs, fully-connected,\nconnected, default and reverse, to capture global, undirected, forward and backward information\nrespectively. Each subgraph is homogeneous, so it can reduce interactions between different classes.\nPath features between nodes are always treated as inductive bias added to the original score\nfunction. Let SP\ud835\udc56\ud835\udc57 = (\ud835\udc521, \ud835\udc522, . . . , \ud835\udc52\ud835\udc41 ) denote the shortest path between node pair (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57). GRUGT [41]\nuses GRU [74] to encode forward and backward features as: r\ud835\udc56\ud835\udc57 = GRU(SP\ud835\udc56\ud835\udc57), r\ud835\udc57\ud835\udc56 = GRU(SP\ud835\udc57\ud835\udc56).\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n26 W. Ju, et al.\nThen, the final attention score is calculated by adding up four components:\ns\u02dc3 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56, (61)\nfrom front to back, which represent content-based score, source-dependent bias, target-dependent\nbias and universal bias respectively. Graphormer [540] uses both path length and path embedding\nto introduce structural bias as:\ns\u02dc4 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 /\n\u221a\n\ud835\udc51 + \ud835\udc4f\ud835\udc41 + \ud835\udc50\ud835\udc56\ud835\udc57,\n\ud835\udc50\ud835\udc56\ud835\udc57 =\n1\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc58=1\n(e\ud835\udc58 )\nTw\ud835\udc38\n\ud835\udc58\n,\ns4 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc4 (x\ud835\udc56, x\ud835\udc58 )),\n(62)\nwhere \ud835\udc4f\ud835\udc41 is a trainable scalar indexed by \ud835\udc41, the length of SP\ud835\udc56\ud835\udc57 . e\ud835\udc58 is the embedding of the the\nedge \ud835\udc52\ud835\udc58 , and w\ud835\udc38\n\ud835\udc58\n\u2208 R\n\ud835\udc51\nis the \ud835\udc58-th edge parameter. If SP\ud835\udc56\ud835\udc57 does not exist, then \ud835\udc4f\ud835\udc41 and \ud835\udc50\ud835\udc56\ud835\udc57 are set to\nbe special values. GRIT [324] utilizes relative random walk probabilities as an inductive bias to\nencode relative path information. Graphormer-GD [560] also incorporates relative distance as bias,\nand rigorously proves that this bias is crucial for determining the biconnectivity of a graph.\n6.4 Encoding Enhancement\nThis kind of method intends to enhance initial node representations to enable the Transformer to\nencode structure information. They can be further divided into two categories, position-analogy\nmethods and structure-aware methods.\n6.4.1 Position-analogy methods In Euclidean space, the Laplacian operator corresponds to the\ndivergence of the gradient, whose eigenfunctions are sine/cosine functions. For the graph, the\nLaplacian operator is the Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.\nHence, inspired by Eq. 58, position-analogy methods utilize Laplacian eigenvectors to simulate\npositional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as they are the equivalents of sine/cosine functions.\nLaplacian eigenvectors can be calculated via the eigendecomposition of normalized graph Lapla\u0002cian matrix L\u02dc\n:\nL\u02dc \u225c I \u2212 D\n\u22121/2AD\u22121/2 = U\u039bUT\n, (63)\nwhere A is the adjacency matrix, D is the degree matrix, U = [u1, u2, . . . , u\ud835\udc5b\u22121] are eigenvectors\nand \u039b = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udf060, \ud835\udf061, . . . , \ud835\udf06\ud835\udc5b\u22121) are eigenvalues. With U and \u039b, GGT [102] uses eigenvectors of the\nk smallest non-trivial eigenvalues to denote the intermediate embedding X\n\ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\n, and maps it\nto d-dimensional space and gets the position encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\n. This process can be formalized\nas:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 = argmin\ud835\udc58({\ud835\udf06\ud835\udc56|0 \u2264 \ud835\udc56  0}),\nX\n\ud835\udc5a\ud835\udc56\ud835\udc51 = [u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc650\n, u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc651, . . . , u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc58\u22121]\nT\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = X\ud835\udc5a\ud835\udc56\ud835\udc51W\ud835\udc58\u00d7\ud835\udc51\n,\n(64)\nwhere \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 is the subscript of the selected eigenvectors. GTSA [241] puts eigenvector u\ud835\udc56 on\nthe frequency axis at \ud835\udf06\ud835\udc56 and uses sequence modeling methods to generate positional encoding.\nSpecifically, it extends X\n\ud835\udc5a\ud835\udc56\ud835\udc51 in Eq. 64 to X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\u00d72 by concatenating each value in eigenvectors\nwith corresponding eigenvalue, and then positional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\nare generated as:\nX\n\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 = X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51W2\u00d7\ud835\udc51\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = SumPooling(Transformer(X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61), dim = 1).\n(65)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 27\nHere, X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 R\n\ud835\udc5b\u00d7\ud835\udc58\u00d7\ud835\udc51\nis equivalent to the input matrix in sequence modeling with shape\n(\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e, \ud835\udc51\ud835\udc56\ud835\udc5a), and can be naturally processed by Transformer. Since the Laplacian\neigenvectors can be complex-valued for directed graph, GSGT [191] proposes to utilize SVD of\nadjacency matrix A, which is denoted as A = U\u03a3VT, and uses the largest \ud835\udc58 singular values \u03a3\ud835\udc58 and\nassociated left and right singular vectors U\ud835\udc58 and V\nT\n\ud835\udc58\nto output X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as X\ud835\udc5d\ud835\udc5c\ud835\udc60 = [U\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n\u2225V\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n],\nwhere \u2225 is the concatenation operation. In addition to SVD, TMDG [145] processes directed graphs\nby utilizing the Magnetic Laplacian. All these methods above randomly flip the signs of eigenvectors\nor singular vectors during the training phase to promote the invariance of the models to the sign\nambiguity.\n6.4.2 Structure-aware methods In contrast to position-analogy methods, structure-aware methods\ndo not attempt to mathematically rigorously simulate sequence positional encoding. They use some\nadditional mechanisms to directly calculate structure-related encoding.\nSome approaches compute extra encoding X\n\ud835\udc4e\ud835\udc51\ud835\udc51 and add it to the initial node representation.\nGraphormer [540] proposes to leverage node centrality as an additional signal to address the\nimportance of each node. Concretely, x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nis determined by the in-degree deg\u2212\n\ud835\udc56\nand outdegree deg+\n\ud835\udc56\n:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56 = P\n\u2212\n(deg\u2212\n\ud835\udc56\n) + P+(deg+\n\ud835\udc56\n), (66)\nwhere P\n\u2212\nand P\n+\nare learnable embedding function. Graph-BERT [567] employs Weisfeiler-Lehman\nalgorithm to label node \ud835\udc63\ud835\udc56 to a number WL(\ud835\udc63\ud835\udc56) \u2208 N and defines x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nas:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57 = sin(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n), x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57+1 = cos(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n). (67)\nThe other approaches try to leverage GNNs to initialize inputs to the Transformer. LRGT [498]\napplies GNN to get intermediate vectors as X\n\u2032 = GNN(X), and passes the concatenation of X\u2032\nand\na special vector xCLS to Transformer layer as: X\u02c6 = Transformer( [X\n\u2032\n\u2225xCLS]). Then x\u02c6CLS can be used\nas the representation of the entire graph for downstream tasks. This method cannot break the 1-WL\nbottleneck because it uses GCN [230] and GIN [518] as graph encoders in the first step, which\nare intrinsically limited by 1-WL test. SAT [56] improves this deficiency by using subgraph-GNN\nNGNN [569] for initialization, and achieves outstanding performance.\n6.5 Summary\nThis section introduces Transformer-based approaches for graph representation learning and we\nprovide the summary as follows:\n\u2022 Techniques. Graph Transformer methods modify two fundamental techniques in Trans\u0002former, attention operation and positional encoding, to enhance its ability to encode graph\ndata. Typically, they introduce fully connected attention to model long-distance relationships,\nutilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck, and separate\npoints and edges belonging to different classes to avoid over-mixing problems.\n\u2022 Challenges and Limitations. Though Graph Transformers achieve encouraging perfor\u0002mance, they still face two major challenges. The first challenge is the computational cost of\nthe quadratic attention mechanism and shortest path calculation. These operations require\nsignificant computing resources and can be a bottleneck, particularly for large graphs. The\nsecond is the reliance of Transformer-based models on large amounts of data for stable perfor\u0002mance. It poses a challenge when dealing with problems that lack sufficient data, especially\nfor few-shot and zero-shot settings.\n\u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further\nexplored. Additionally, there are some works using pre-training and fine-tuning frameworks\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n28 W. Ju, et al.\nto balance performance and complexity in downstream tasks [540], this may be a promising\nsolution to address the aforementioned two challenges.\n7 Semi-supervised Learning on Graphs\nWe have investigated various architectures of graph neural networks in which the parameters should\nbe tuned by a learning objective. The most prevalent optimization approach is supervised learning on\ngraph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention\nin the data mining community. In detail, these methods attempt to combine graph representation\nlearning with current semi-supervised techniques including pseudo-labeling, consistency learning,\nknowledge distillation and active learning. These works can be further subdivided into node-level\nrepresentation learning and graph-level representation learning. We would introduce both parts in\ndetail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6.\n7.1 Node Representation Learning\nTypically, node representation learning follows the concept of transductive learning, which has\naccess to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised\nloss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is\nusually adopted for optimization. In formulation,\nL\ud835\udc41 \ud835\udc46\ud835\udc3f = \u2212\n1\n|Y\ud835\udc3f |\n\u2211\ufe01\n\ud835\udc56\u2208Y\ud835\udc3f\ny\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (68)\nwhere Y\ud835\udc3f denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that\ncan be used to offer semantic information. To fully utilize these nodes, a range of methods attempt\nto combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a\nfundamental semi-supervised technique that uses the classifier to produce the label distribution of\nunlabeled examples and then adds appropriately labeled examples to the training set [265, 604].\nAnother line of semi-supervised learning is consistency regularization [247] that requires two\nexamples to have identical predictions under perturbation. This regularization is based on the\nassumption that each instance has a distinct label that is resistant to random perturbations [118, 357].\nThen, we show several representative works in detail.\nCooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-label\u0002based GNN approach for semi-supervised node classification. It employs two GNN classifiers to\njointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows:\n\ud835\udc36\ud835\udc49 (p\ud835\udc56) = p\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (69)\nwhere p\ud835\udc56 denotes the output label distribution. Then it selects the pseudo-labels with high confidence\ngenerated from one model to supervise the optimization of the other model. In particular, the\nobjective for unlabeled nodes is written as follows:\nL\ud835\udc36\ud835\udc5c\ud835\udc3a\ud835\udc41 \ud835\udc52\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc56\u2208V\ud835\udc48\n1\ud835\udc36\ud835\udc49 (p\ud835\udc56 )>\ud835\udf0fy\u02c6\n\ud835\udc47\n\ud835\udc56\n\ud835\udc59\ud835\udc5c\ud835\udc54q\ud835\udc56, (70)\nwhere y\u02c6\ud835\udc56 denotes the one-hot formulation of the pseudo-label \ud835\udc66\u02c6\ud835\udc56 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65p\ud835\udc56 and q\ud835\udc56 denotes\nthe label distribution predicted by the other classifier. \ud835\udf0f is a pre-defined temperature coefficient.\nThis cross supervision has been demonstrated effective in [64, 312] to prevent the provision of\nbiased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information\nfrom a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still\ngenerate the same prediction. In this way, CoGNet can illustrate the entire optimization process to\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 29\nTable 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered\nas a specific kind of consistency learning.\nApproach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning\nNode-level\nCoGNet [265] \u2713\nDSGCN [604] \u2713\nGRAND [118] \u2713\nAugGCR [357] \u2713\nHCPL [309] \u2713\nGraph-level\nSEAL [264] \u2713 \u2713\nInfoGraph [431] \u2713 \u2713\nDSGC [527] \u2713\nASGN [166] \u2713 \u2713\nTGNN [218] \u2713\nKGNN [221] \u2713\nHGMI [262] \u2713 \u2713\nASGNN [508] \u2713 \u2713\nDualGraph [310] \u2713 \u2713\nGLA [556] \u2713\nSS [507] \u2713\nenhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in\nsemi-supervised node classification, which can generate dynamics thresholds for reliable nodes.\nDynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive\nmanner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller\nweights to samples with lower confidence with the additional consideration of class balance. The\nweight is formulated as:\n\ud835\udf14\ud835\udc56 =\n1\n\ud835\udc5b\ud835\udc50\n\ud835\udc56\nmax (RELU (p\ud835\udc56 \u2212 \ud835\udefd \u00b7 1)) , (71)\nwhere \ud835\udc5b\ud835\udc50\n\ud835\udc56 denotes the number of unlabeled samples assigned to the class \ud835\udc50\n\ud835\udc56\n. This technique will\ndecrease the impact of wrong pseudo-labels during iterative training.\nGraph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learning\u0002based method. It first adds a variety of perturbations to the input graph to generate a list of\ngraph views. Each graph view \ud835\udc3a\n\ud835\udc5f\nis sent to a GNN classifier to produce a prediction matrix\nP\n\ud835\udc5f = [p\ud835\udc5f\n1\n, \u00b7 \u00b7 \u00b7 , p\n\ud835\udc5f\n\ud835\udc41\n]. Then it summarizes these matrices as:\nP =\n1\n\ud835\udc45\nP\n\ud835\udc5f\n. (72)\nTo provide more discriminative information and ensure that the matrix is row-normalized,\nGRAND sharpens the summarized label matrix into P\n\ud835\udc46\ud835\udc34 as:\nP\n\ud835\udc46\ud835\udc34\n\ud835\udc56\ud835\udc57 =\nP\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\n\u00cd\n\ud835\udc57\n\u2032=0 P\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\u2032\n, (73)\nwhere \ud835\udc47 is a given temperature parameter. Finally, consistency learning is performed by comparing\nthe sharpened summarized matrix with the matrix of each graph view. Formally, the objective is:\nL\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 =\n1\n\ud835\udc45\n\u2211\ufe01\n\ud835\udc45\n\ud835\udc5f=1\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n||P\n\ud835\udc46\ud835\udc34\n\ud835\udc56 \u2212 P\ud835\udc56\n||, (74)\nhere L\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 serves as a regularization which is combined with the standard supervised loss.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n30 W. Ju, et al.\nAugmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with\nthe generation of augmented graphs by random dropout and mixup of different order features. To\nenhance the model generalization, it borrows the idea of meta-learning to partition the training data,\nwhich improves the quality of graph augmentation. In addition, it utilizes consistency regularization\nto enhance the semi-supervised node classification.\n7.2 Graph Representation Learning\nThe objective of graph classification is to predict the property of the whole graph example.\nAssuming that the training set comprises \ud835\udc41\n\ud835\udc59\nand \ud835\udc41\n\ud835\udc62 graph samples G\ud835\udc59 = {\ud835\udc3a1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59\n} and\nG\n\ud835\udc62 = {\ud835\udc3a\ud835\udc41\n\ud835\udc59 +1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59 +\ud835\udc41\ud835\udc62\n}, the graph-level supervised loss for labeled data can be expressed as\nfollows:\nL\ud835\udc3a\ud835\udc46\ud835\udc3f = \u2212\n1\n|G\ud835\udc62 |\n\u2211\ufe01\n\ud835\udc3a\ud835\udc57 \u2208 G\ud835\udc3f\ny\n\ud835\udc57\ud835\udc47\n\ud835\udc59\ud835\udc5c\ud835\udc54p\n\ud835\udc57\n, (75)\nwhere y\n\ud835\udc57 denotes the one-hot label vector for the \ud835\udc57-th sample while p\ud835\udc57 denotes the predicted\ndistribution of \ud835\udc3a\n\ud835\udc57\n. When \ud835\udc41\n\ud835\udc62 = 0, this objective can be utilized to optimize supervised methods.\nHowever, due to the shortage of labels in graph data, supervised methods cannot reach exceptional\nperformance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph\nclassification has been developed extensively. These approaches can be categorized into pseudo\u0002labeling-based methods, knowledge distillation-based methods and contrastive learning-based\nmethods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph\nexamples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually\nutilize a teacher-student architecture, where the teacher model conducts graph representation\nlearning without label information to extract generalized knowledge while the student model\nfocuses on the downstream task. Due to the restricted number of labeled instances, the student\nmodel transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of\nthis topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning.\nTypically, these methods extract topological information from two perspectives (i.e., different\nperturbation strategies and graph encoders), and maximize the similarity of their representations\ncompared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique\nto improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166,\n508]. Then, we review these methods in detail.\nSEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node\nin a hierarchical graph. It builds two graph classifiers which generate graph representations and\nconduct semi-supervised graph classification respectively. SEAL employs a self-attention module\nto encode each graph into a graph-level representation, and then conducts message passing from a\ngraph level for final classification. SEAL can also be combined with cautious iteration and active\niteration. The former merely utilizes partial graph samples to optimize the parameters in the first\nclassifier due to the potential erroneous pseudo-labels. The second combines active learning with\nthe model, which increases the annotation efficiency in semi-supervised scenarios.\nInfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the\nsimilarity between summarized graph representations and their node representations. In particular,\nit generates node representations using the message passing mechanism and summarizes these\nnode representations into a graph representation. Let \u03a6(\u00b7, \u00b7) denote a discriminator to distinguish\nwhether a node belongs to the graph, and we have:\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 31\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e =\n| G\ud835\udc59|+| G\ud835\udc62 \u2211\ufe01|\n\ud835\udc57=1\n\u2211\ufe01\n\ud835\udc56\u2208 G\ud835\udc57\nh\n\u2212 sp \u0010\u2212\u03a6\n\u0010\nh\n\ud835\udc57\n\ud835\udc56\n, z\n\ud835\udc57\n\u0011 \u0011 i \u2212\n1\n|\ud835\udc41\n\ud835\udc57\n\ud835\udc56\n|\n\u2211\ufe01\n\ud835\udc56\n\u2032\ud835\udc57\n\u2032\n\u2208\ud835\udc41\n\ud835\udc57\n\ud835\udc56\nh\nsp \u0010\u03a6\n\u0010\nh\n\ud835\udc57\n\u2032\n\ud835\udc56\n\u2032\n, z\n\ud835\udc57\n\u0011 \u0011 i , (76)\nwhere sp(\u00b7) denotes the softplus function. \ud835\udc41\n\ud835\udc57\n\ud835\udc56\ndenotes the negative node set where nodes are not\nin \ud835\udc3a\n\ud835\udc57\n. This mutual information maximization formulation is originally developed for unsupervised\nlearning and it can be simply extended for semi-supervised graph classification. In particular,\nInfoGraph utilizes a teacher-student architecture that compares the representation across the\nteacher and student networks. The contrastive learning objective serves as a regularization by\ncombining with supervised loss.\nDual Space Graph Contrastive Learning (DSGC) [527]. DSGC is a representative contrastive\nlearning-based method. It utilizes two graph encoders. The first is a standard GNN encoder in the\nEuclidean space and the second is the hyperbolic GNN encoder. The hyperbolic GNN encoder first\nconverts graph embeddings into hyperbolic space and then measures the distance based on the\nlength of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic\nspace. Assuming the two GNNs are named as \ud835\udc531 (\u00b7) and \ud835\udc532 (\u00b7), the positive pair is denoted as:\nz\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n= exp\ud835\udc50\no\n(\ud835\udc531 (\ud835\udc3a\n\ud835\udc57\n)),\nz\n\ud835\udc57\n\ud835\udc3b\n= exp\ud835\udc50\no\n\ud835\udc532 (\ud835\udc3a\n\ud835\udc57\n)\n\u0001\n.\n(77)\nThen it selects one labeled sample and \ud835\udc41\ud835\udc35 unlabeled sample \ud835\udc3a\n\ud835\udc57\nfor graph contrastive learning in\nthe hyperbolic space. In formulation,\nL\ud835\udc37\ud835\udc46\ud835\udc3a\ud835\udc36 = \u2212 log e\n\ud835\udc51\n\ud835\udc3b\n(h\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f\ne\n\ud835\udc51\ud835\udc3b (z\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f +\n\u00cd\ud835\udc41\n\ud835\udc56=1\ne\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc3b\n\u0011\n/\ud835\udf0f\n\u2212\n\ud835\udf06\ud835\udc62\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56=1\nlog e\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\ne\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n+ e\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n,\n(78)\nwhere z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\nand z\n\ud835\udc56\n\ud835\udc3b\ndenote the embeddings for labeled graph sample\ud835\udc3a\n\ud835\udc56\nand \ud835\udc51\n\ud835\udc3b (\u00b7) denotes a distance\nmetric in the hyperbolic space. This contrastive learning objective maximizes the similarity between\nembeddings learned from two encoders compared with other samples. Finally, the contrastive\nlearning objective can be combined with the supervised loss to achieve effective semi-supervised\ncontrastive learning.\nActive Semi-supervised Graph Neural Network (ASGN) [166]. ASGN utilizes a teacher-student\narchitecture with the teacher model focusing on representation learning and the student model\ntargeting at molecular property prediction. In the teacher model, ASGN first employs a message\npassing neural network to learn node representations under the reconstruction task and then\nborrows the idea of balanced clustering to learn graph-level representations in a self-supervised\nfashion. In the student model, ASGN utilizes label information to monitor the model training based\non the weights of the teacher model. In addition, active learning is also used to minimize the\nannotation cost while maintaining sufficient performance. Typically, the teacher model seeks to\nprovide discriminative graph-level representations without labels, which transfer knowledge to the\nstudent model to overcome the potential overfitting in the presence of label scarcity.\nTwin Graph Neural Networks (TGNN) [218]. TGNN also uses two graph neural networks to\ngive different perspectives to learn graph representations. Differently, it adopts a graph kernel\nneural network to learn graph-level representations in virtue of random walk kernels. Rather than\ndirectly enforcing representation from two modules to be similar, TGNN exchanges information\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n32 W. Ju, et al.\nby contrasting the similarity structure of the two modules. In particular, it constructs a list of\nanchor graphs, \ud835\udc3a\n\ud835\udc4e1\n,\ud835\udc3a\ud835\udc4e2, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc4e\ud835\udc40 , and utilizes two graph encoders to produce their embeddings,\ni.e., {\ud835\udc67\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n, {\ud835\udc64\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n. Then it calculates the similarity distribution between each unlabeled\ngraph and anchor graphs for two modules. Formally,\n\ud835\udc5d\n\ud835\udc57\n\ud835\udc5a =\nexp cos \ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (\ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n, (79)\n\ud835\udc5e\n\ud835\udc57\n\ud835\udc5a =\nexp cos w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n. (80)\nThen, TGNN minimizes the distance between distributions from different modules as follows:\nL\ud835\udc47\ud835\udc3a\ud835\udc41 \ud835\udc41 =\n1\nG\ud835\udc48\n\u2211\ufe01\n\ud835\udc3a \ud835\udc57 \u2208 G\ud835\udc62\n1\n2\n\ud835\udc37KL\np\n\ud835\udc57\n\u2225q\n\ud835\udc57\n\u0001\n+ \ud835\udc37KL\nq\n\ud835\udc57\n\u2225p\n\ud835\udc57\n\u0001\u0001 , (81)\nwhich serves as a regularization term to combine with the supervised loss.\n7.3 Summary\nThis section introduces semi-supervised learning for graph representation learning and we provide\nthe summary as follows:\n\u2022 Techniques. Classic node classification aims to conduct transductive learning on graphs\nwith access to unlabeled data, which is a natural semi-supervised problem. Semi-supervised\ngraph classification aims to relieve the requirement of abundant labeled graphs. Here, a\nvariety of semi-supervised methods have been put forward to achieve better performance\nunder the label scarcity. Typically, they try to integrate semi-supervised techniques such as\nactive learning, pseudo-labeling, consistency learning, and consistency learning with graph\nrepresentation learning.\n\u2022 Challenges and Limitations. Despite their great success, the performance of these methods\nis still unsatisfactory, especially in graph-level representation learning. For example, DSGC\ncan only achieve an accuracy of 57% in a binary classification dataset REDDIT-BINARY. Even\nworse, label scarcity is often accompanied by unbalanced datasets and potential domain\nshifts, which provides more challenges from real-world applications.\n\u2022 Future Works. In the future, we expect that these methods can be applied to different\nproblems such as molecular property predictions. There are also works to extend graph\nrepresentation learning in more realistic scenarios like few-shot learning [51, 326]. A higher\naccuracy is always anticipated for more advanced and effective semi-supervised techniques.\n8 Graph Self-supervised Learning\nBesides supervised or semi-supervised methods, self-supervised learning (SSL) also has shown its\npowerful capability in data mining and representation embedding in recent years. In this section,\nwe investigated Graph Neural Networks based on SSL and provided a detailed introduction to a\nfew typical models. Graph SSL methods usually have a unified pipeline, which includes pretext\ntasks and downstream tasks. Pretext tasks help the model encoder to learn better representation,\nas a premise of better performance in downstream tasks. So a delicate design of pretext task is\ncrucial for Graph SSL. We would firstly introduce the overall framework of Graph SSL in Section 8.1,\nthen introduce the two kinds of pretext task design, generation-based methods and contrast-based\nmethods respectively in Section 8.2 and 8.3. A summarisation is provided in Table 7.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 33\nTable 7. Summary of methods for self-supervised Learning on Graphs. \"PT\", \"CT\" and \"UFE\" mean \"Pre\u0002training\", \"Collaborative Train\" and \"Unsupervised Feature Extracting\" respectively.\nApproach Augmentation Scheme Training Scheme Generation Target Objective Function\nGeneration-based\nGraph Completion [544] Feature Mask PT/CT Node Feature -\nAttributeMask [208] Feature Mask PT/CT PCA Node Feature -\nAttrMasking [181] Feature Mask PT Node/Edge Feature -\nMGAE [459] No Augmentation CT Node Feature -\nGAE [231] Feature Noise UFE Adjacency Matrix -\nContrast-based\nDeepWalk [362] Random Walk UFE - SkipGram\nLINE [443] Random Walk UFE - Jensen-Shannon\nGCC [375] Random Walk PT/URL - InfoNCE\nSimGCL [547] Embedding Noise UFE - InfoNCE\nSimGRACE [503] Model Noise UFE - InfoNCE\nGCA [612]\nFeature Masking &\nStructure Adjustment URL - InfoNCE\nBGRL [152]\nFeature Masking &\nStructure Adjustment URL - BYOL\n8.1 Overall framework\nConsider a featured graph G, we denote a graph encoder \ud835\udc53 to learn the representation of the\ngraph, and a pretext decoder \ud835\udc54 with specific architecture in different pretext tasks. Then the pretext\nself-supervised learning loss can be formulated as:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = \ud835\udc38G\u223cD [L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G)], (82)\nwhere D denotes the distribution of featured graph G. By minimizing L\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59 , we can learn encoder\n\ud835\udc53 with capacity to produce high-quality embedding. As for downstream tasks, we denote a graph\ndecoder \ud835\udc51 which transforms the output of graph encoder \ud835\udc53 into model prediction. The loss of\ndownstream tasks can be formulated as:\nL\ud835\udc60\ud835\udc62\ud835\udc5d = L\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66), (83)\nwhere \ud835\udc66 is the ground truth in downstream tasks. We can obverse that L\ud835\udc60\ud835\udc62\ud835\udc5d is a typical supervised\nloss. To ensure the model achieves wise graph representation extraction and optimistic prediction\nperformance, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d have to be minimized simultaneously. We introduce 3 different ways\nto minimize the two loss functions:\nPre-training. This strategy has two steps. In pre-training step, the L\ud835\udc60\ud835\udc60\ud835\udc59 is minimized to get \ud835\udc54\n\u2217\nand \ud835\udc53\n\u2217\n:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D). (84)\nThen the parameter of \ud835\udc53\n\u2217\nis kept to continue training in pretext supervised learning progress.\nThe supervised loss is minimized to get the final parameters of \ud835\udc53 and \ud835\udc51.\nmin\n\ud835\udc51,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc51, \ud835\udc53 |\ud835\udc530=\ud835\udc53\n\u2217 , G;\ud835\udc66). (85)\nCollaborative Train. In this strategy, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d are optimized simultaneously. A hyper\u0002parameter \ud835\udefc is used to balance the contribution of pretext task loss and downstream task loss.\nThe overall minimization strategy is like the traditional supervised strategy with a pretext task\nregularization:\nmin\n\ud835\udc54,\ud835\udc53 ,\ud835\udc51\n[L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G) + \ud835\udefcL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66)]. (86)\nUnsupervised Feature Extracting. This strategy is similar to the Pre-training and Fine-tuning\nstrategy in the first step to minimize pretext task loss L\ud835\udc60\ud835\udc60\ud835\udc59 and get \ud835\udc53\n\u2217\n. However, when minimizing\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n34 W. Ju, et al.\ndownstream loss L\ud835\udc60\ud835\udc62\ud835\udc5d , the encoder \ud835\udc53\n\u2217\nis fixed. Also, the training graph data are on the same dataset,\nwhich differs from the Pre-training and Fine-tuning strategy. The formulation is defined as:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D), (87)\nmin\n\ud835\udc51\nL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 \u2217, G;\ud835\udc66). (88)\n8.2 Generation-based pretext task design\nIf a model with an encoder-decoder structure can reproduce certain graph features from an in\u0002complete or perturbed graph, it indicates the encoder has the ability to extract useful graph\nrepresentation. This motivation is derived from Autoencoder [174] which originally learns on\nimage dataset. In such a case, Eq. 84 can be rewritten as:\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54(\ud835\udc53 (G)) \u02c6 , G), (89)\nwhere \ud835\udc53 (\u00b7) and \ud835\udc54(\u00b7) stand for the representation encoder and rebuilding decoder. However, feature\ninformation and structure information are both important compositions suitable to be rebuilt for\ngraph datasets. So generation-based pretext can be divided into two categories: feature rebuilding\nand structure rebuilding. We introduce several outstanding models in the following part.\nGraph Completion [544] is one of the representative methods of feature rebuilding. They mask\nsome node features to generate an incomplete graph. Then the pretext task is set as predicting the\nremoved node features. As shown in Eq. 90, this method can be formulated as a special case of\nEq. 90, letting G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and replacing G \u2212\u2192 \ud835\udc4b. The loss function is often Mean Squared Error or\nCross Entropy, depending on whether the feature is continuous or binary.\nmin\n\ud835\udc54,\ud835\udc53\nMSE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , X). (90)\nOther works make some changes to the feature settings. For example, AttrMasking [181] aims\nto rebuild both node representation and edge representation, AttributeMask [208] preprocess \ud835\udc4b\nfirstly by PCA to reduce the complexity of rebuilding features.\nOn the other hand, MGAE [459] modifies the original graph by adding noise in node representa\u0002tion, motivated by denoising autoencoder [454]. As shown in Eq. 90, we can also consider MGAE as\nan implement of Eq. 84 where G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and G \u2212\u2192 \ud835\udc4b. \ud835\udc4b\u02c6 stands for perturbed node representation.\nSince the noise is independent and random, the encoder is more robust to feature input.\nmin\n\ud835\udc54,\ud835\udc53\nBCE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , A). (91)\nAs for structure rebuilding methods, GAE [231] is the simplest instance, which can be regarded as\nan implement of Eq. 84 where G\u02c6 = G and G \u2212\u2192 \ud835\udc34. \ud835\udc34 is the adjacency matrix of the graph. Similar to\nfeature rebuilding methods, GAE compresses raw node representation vectors into low-dimensional\nembedding with its encoder. Then the adjacency matrix is rebuilt by computing node embedding\nsimilarity. The loss function is set to the error between the ground-truth adjacency matrix and\nthe recovered one, to help the model rebuild the correct graph structure. Other feature rebuilding\nmethods [558] and structure rebuilding methods [440, 487] are also increasingly being developed\nacross numerous related publications.\n8.3 Contrast-Based pretext task design\nThe mutual information maximization principle, which implements self-supervising by predicting\nthe similarity between the two augmented views, forms the foundation of contrast-based approaches.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 35\nSince mutual information represents the degree of correlation between two samples, we can\nmaximize it in augmented pairs and minimize it in random-selected pairs.\nThe contrast-based graph SSL taxonomy can be formulated as Eq. 92. The discriminator that\ncalculates the similarity of sample pairs is indicated by pretext decoder \ud835\udc54. G\n(1)\nand G\n(2)\nare two\nvariants of \ud835\udc3a that have been augmented. Since graph contrastive learning methods differ from each\nother in 1) view generation, 2) MI estimation method we introduce this methodology in these two\nperspectives.\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54[\ud835\udc53 (G\u02c6(1)), \ud835\udc53 (G\u02c6(2))]). (92)\nThe domain of contrastive-based graph SSL is witnessing an expanding body of work in a\ngrowing number of methods [176, 197, 531, 587] and applications [114, 136, 504].\n8.3.1 View generation. The traditional pipeline of contrastive learning-based models first involves\naugmenting the graph using well-crafted empirical methods, and then maximizing the consistency\nbetween different augmentations. Drawing from methods in the computer vision domain and\nconsidering the non-Euclidean structure of graph data, typical graph augmentation methods aim\nto modify the graph topologically or representationally.\nGiven graph G = (\ud835\udc34, \ud835\udc4b), the topologically augmentation methods usually modify the adjacency\nmatrix \ud835\udc34, which can be formulated as:\n\ud835\udc34\u02c6 = \ud835\udcaf\ud835\udc34 (\ud835\udc34), (93)\nwhere \ud835\udcaf\ud835\udc34 (\u00b7) is the transform function of adjacency matrix. Topology augmentation methods\nhave many variants, in which the most popular one is edge modification, given by \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\ud835\udc43 \u25e6 \ud835\udc34 + \ud835\udc44 \u25e6 (1 \u2212 \ud835\udc34). \ud835\udc43 and \ud835\udc44 are two matrices representing edge dropping and adding respectively.\nAnother method, graph diffusion, connect nodes with their k-hop neighbors with specific weight,\ndefined as: \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\u00cd\u221e\n\ud835\udc58=0\n\ud835\udefc\ud835\udc58\ud835\udc47\n\ud835\udc58\n. where \ud835\udefc and\ud835\udc47 are coefficient and transition matrix. Graph diffusion\nmethod can integrate broad topological information with local structure.\nOn the other hand, the representative augmentation modifies the node representation directly,\nwhich can be formulated as:\n\ud835\udc4b\u02c6 = \ud835\udcaf\ud835\udc4b (\ud835\udc4b), (94)\nusually \ud835\udcaf\ud835\udc4b (\u00b7) can be a simple masking operater, a.k.a. \ud835\udcaf\ud835\udc4b (\ud835\udc4b) = \ud835\udc40 \u25e6 \ud835\udc4b and \ud835\udc40 \u2208 {0, 1}\n\ud835\udc41 \u00d7\ud835\udc37 . Based\non such mask strategy, some methods propose ways to improve performance. GCA [612] preserves\ncritical nodes while giving less significant nodes a larger masking probability, where significance is\ndetermined by node centrality.\nAs introduced before, the paradigm of augmentation has been proven to be effective in contrastive\nlearning view generation. However, given the variety of graph data, it is challenging to maintain\nsemantics properly during augmentations. To preserve the valuable nature of specific graph datasets,\nThere are currently three mainly used methods: picking by trial-and-errors, trying laborious search,\nor seeking domain-specific information as guidance [214, 308, 311]. Such complicated augmentation\nmethods constrain the effectiveness and widespread application of graph contrastive learning. So\nmany newest works question the necessity of augmentation and seek other contrastive view\ngeneration methods.\nSimGCL [547] is one of the outstanding works challenging the effectiveness of graph augmenta\u0002tion. The author finds that noise can be a substitution to augmentation to produce graph views\nin specific tasks such as recommendation. After doing an ablation study about augmentation and\nInfoNCE [510], they find that the InfoNCE loss, not the augmentation of the graph, is what makes\nthe difference. It can be further explained by the importance of distribution uniformity. Contrastive\nlearning enhances model representation ability by intensifying two characteristics: The alignment\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n36 W. Ju, et al.\nof features from positive samples and the uniformity of the normalized feature distribution. SimGCL\ndirectly adds random noises to node embeddings as augmentation, to control the uniformity of the\nrepresentation distribution more effectively:\ne\n(1)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(1)\n\u2217 \ud835\udf0f\n(1)\n\ud835\udc56\n, e\n(2)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(2)\n\u2217 \ud835\udf0f\n(2)\n\ud835\udc56\n,\n\ud835\udf16 \u223c N (0, \ud835\udf0e2),\n(95)\nwhere e\ud835\udc56is a node representation in embedding space, \ud835\udf0f\n(1)\n\ud835\udc56\nand \ud835\udf0f\n(2)\n\ud835\udc56\nare two random sampled unit\nvector. The experiment results indicate that SimGCL performs better than its graph augmentation\u0002based competitors, while training time is significantly decreased.\nSimGRACE [503] is another graph contrastive learning framework without data augmentation.\nMotivated by the observation that despite encoder disruption, graph data can effectively maintain\ntheir semantics, SimGRACE takes GNN with its modified version as an encoder to produce two\ncontrastive embedding views by the same graph input. For GNN encoder \ud835\udc53 (\u00b7; \ud835\udf03), the two contrastive\nembedding views e, e\n\u2032\ncan be computed by:\ne\n(1) = \ud835\udc53 (G; \ud835\udf03), e(2) = \ud835\udc53 (G; \ud835\udf03 + \ud835\udf16 \u00b7 \u0394\ud835\udf03),\n\u0394\ud835\udf03\ud835\udc59 \u223c N (0, \ud835\udf0e2\n\ud835\udc59\n),\n(96)\nwhere \u0394\ud835\udf03\ud835\udc59 represents GNN parameter perturbation \u0394\ud835\udf03 in the \ud835\udc59th layer. SimGRACE can improve\nalignment and uniformity simultaneously, proving its capacity to produce high-quality embedding.\n8.3.2 MI estimation method. The mutual information \ud835\udc3c(\ud835\udc65, \ud835\udc66) measures the information that x and y\nshare, given a pair of random variables (\ud835\udc65, \ud835\udc66). As discussed before, mutual information is a significant\ncomponent of the contrast-based method by formulating the loss function. Mathematically rigorous\nMI is defined on the probability space, we can formulate mutual information between a pair of\ninstances (\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) as:\n\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \ud835\udc37\ud835\udc3e\ud835\udc3f (\ud835\udc5d(\ud835\udc65, \ud835\udc66)||\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66))\n= \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc5d(\ud835\udc65, \ud835\udc66)\n\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66)\n].\n(97)\nHowever, directly computing Eq. 97 is quite difficult, so we introduce several different types of\nestimation for MI:\nInfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimator. Given a\npositive sample \ud835\udc66 and several negative sample \ud835\udc66\n\u2032\n\ud835\udc56\n, a noise-contrastive estimator can be formulated\nas [611][375]:\nL = \u2212\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \u2212\ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66)\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66) +\n\u00cd\n\ud835\udc56\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66\u2032\n\ud835\udc56\n)\n], (98)\nusually the kernal function \ud835\udc54(\u00b7) can be cosine similarity or dot product.\nTriplet Loss. Intuitively, we can aim to create a distinct separation in the degree of similarity,\nensuring that positive samples are closer together and negative samples are further apart by a\ncertain distance. So we can define the loss function in the following manner [204]:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [max(\ud835\udc54(\ud835\udc65, \ud835\udc66) \u2212 \ud835\udc54(\ud835\udc65, \ud835\udc66\u2032) + \ud835\udf16, 0)], (99)\nwhere \ud835\udf16 is a hyperparameter. This function is straightforward to compute.\nBYOL Loss. Estimation without negative samples is investigated by BYOL [152]. The estimator\nis Asymmetrically structured:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [2 \u2212 2\n\ud835\udc54(\ud835\udc65) \u00b7 \ud835\udc66\n\u2225\ud835\udc54(\ud835\udc65) \u2225 \u2225\ud835\udc66\u2225\n], (100)\nnote that encoder \ud835\udc54 should keep the dimension of input and output the same.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 37\n8.4 Summary\nThis section introduces graph self-supervised learning and we provide the summary as follows:\n\u2022 Techniques. Differing from classic supervised and semi-supervised learning, self-supervised\nlearning increases a model\u2019s generalization ability and robustness while decreasing reliance\non labels. Graph SSL utilizes pretext tasks to extract inherent information from representation\ndistributions. Typical Graph SSL methods can be divided into generation-based and contrast\u0002based approaches. Generation-based methods learn an encoder with the ability to reconstruct\na graph as precisely as possible, motivated by the principles of Autoencoder. Contrast-based\nmethods, which have recently attracted significant interest, involve learning an encoder to\nminimize mutual information between relevant instances and maximize mutual information\nbetween unrelated instances.\n\u2022 Challenges and Limitations. Although graph SSL has achieved superior performance in\nmany tasks, its theoretical basis is not so solid. Many well-known methods are validated only\nthrough experiments, without providing theoretical explanations or mathematical proofs. It\nis imperative to establish a strong theoretical foundation for graph SSL.\n\u2022 Future Works. In the future we expect more graph ssl methods designed essentially by\ntheoretical proof, without dedicated designed augment process or pretext tasks by intuition.\nThis will bring us more definite mathematical properties and a less ambiguous empirical\nsense. Also, graphs are a prevalent form of data representation across diverse domains, yet\nobtaining manual labels can be prohibitively expensive. Expanding the applications of graph\nSSL to broader fields is a promising avenue for future research.\n9 Graph Structure Learning\nGraph structure determines how node features propagate and affect each other, playing a crucial\nrole in graph representation learning. In some scenarios the provided graph is incomplete, noisy, or\neven has no structure information at all. Recent research also finds that graph adversarial attacks\n(i.e., modifying a small number of node features or edges), can degrade learned representations\nsignificantly. These issues motivate graph structure learning (GSL), which aims to learn a new\ngraph structure to produce optimal graph representations. According to how edge connectivity is\nmodeled, there are three different approaches in GSL, namely metric-based approaches, model-based\napproaches, and direct approaches. Besides edge modeling, regularization is also a common trick to\nmake the learned graph satisfy some desired properties. We first present the basic framework and\nregularization methods for GSL in Sec. 9.1 and Sec. 9.2, respectively, and then introduce different\ncategories of GSL in Sec. 9.3, 9.4 and 9.5. We summarize GSL approaches in Table 8.\n9.1 Overall Framework\nWe denote a graph by G = (A, X), where A \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is the adjacency matrix and X \u2208 R\ud835\udc41 \u00d7\ud835\udc40 is\nthe node feature matrix with \ud835\udc40 being the dimension of each node feature. A graph encoder \ud835\udc53\ud835\udf03\nlearns to represent the graph based on node features and graph structure for task-specific objective\nL\ud835\udc61 (\ud835\udc53\ud835\udf03 (A, X)). In the GSL setting, there is also a graph structure learner which aims to build a\nnew graph adjacency matrix A\n\u2217\nto optimize the learned representation. Besides the task-specific\nobjective, a regularization term can be added to constrain the learned structure. So the overall\nobjective function of GSL can be formulated as\nmin\n\ud835\udf03,A\u2217\nL = L\ud835\udc61 (\ud835\udc53\ud835\udf03 (A\n\u2217\n, X)) + \ud835\udf06L\ud835\udc5f (A\n\u2217\n, A, X), (101)\nwhere L\ud835\udc61is the task-specific objective, L\ud835\udc5fis the regularization term and \ud835\udf06 is a hyperparameter for\nthe weight of regularization.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n38 W. Ju, et al.\nTable 8. Summary of graph structure learning methods.\nMethod Structure Learning Regularization\nSparsity Low-rank Smoothness\nMetric-based\nAGCN [267] Mahalanobis distance\nGRCN [546] Inner product \u2713\nCAGCN [613] Inner product \u2713\nGNNGUARD [571] Cosine similarity\nIDGL [65] Cosine similarity \u2713 \u2713 \u2713\nHGSL [586] Cosine similarity \u2713\nGDC [139] Graph diffusion \u2713\nModel-based\nGLN [364] Recurrent blocks\nGLCN [199] One-layer neural network \u2713 \u2713\nNeuralSparse [595] Multi-layer neural network \u2713\nGAT [452] Self-attention\nGaAN [566] Gated attention\nhGAO [132] Hard attention \u2713\nVIB-GSL [433] Dot-product attention \u2713\nMAGNA [461] Graph attention diffusion\nDirect\nGLNN [135] MAP estimation \u2713 \u2713\nPro-GNN [210] Direct optimization \u2713 \u2713 \u2713\nGSML [458] Bilevel optimization \u2713\nLSD-GNN [124] Bilevel optimization\nBGCNN [573] Bayesion optimization\nVGCN [104] Stochastic variational inference\n9.2 Regularization\nThe goal of regularization is to constrain the learned graph to satisfy some properties by adding\nsome penalties to the learned structure. The most common properties used in GSL are sparsity, low\nlank, and smoothness.\n9.2.1 Sparsity Noise or adversarial attacks will introduce redundant edges into graphs and degrade\nthe quality of graph representation. An effective technique to remove unnecessary edges is sparsity\nregularization, i.e., adding a penalty on the number of nonzero entries of the adjacency matrix\n(\u21130-norm) [458, 546, 586, 595]:\nL\ud835\udc60\ud835\udc5d = \u2225A\u22250, (102)\nhowever, \u21130-norm is not differentiable so optimizing it is difficult, and in many cases \u21131-norm\nis used instead as a convex relaxation. Other methods to impose sparsity include pruning and\ndiscretization [124, 613]. These processes are also called postprocessing since they usually happen\nafter the adjacency matrix is learned. Pruning removes part of the edges according to some crite\u0002ria [613]. For example, edges with weights lower than a threshold, or those not in the top-K edges\nof nodes or graphs. Discretization is applied to generate graph structure by sampling from some\ndistribution [124]. Compared to directly learning edge weights, sampling enjoys the advantage\nof controlling the generated graph, but has issues during optimizing since sampling itself is dis\u0002crete and hard to optimize. Reparameterization and Gumbel-softmax are two useful techniques to\novercome such issues, and are widely adopted in GSL.\n9.2.2 Low Rank In real-world graphs, similar nodes are likely to group together and form commu\u0002nities, which should lead to a low-rank adjacency matrix. Recent work also finds that adversarial\nattacks tend to increase the rank of the adjacency matrix quickly [65, 210]. Therefore, low-rank\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 39\nregularization is also a useful tool to make graph representation learning more robust:\nL\ud835\udc59\ud835\udc5f = \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc58 (A). (103)\nIt is hard to minimize matrix rank directly. A common technique is to optimize the nuclear norm,\nwhich is a convex envelope of the matrix rank:\nL\ud835\udc5b\ud835\udc50 = \u2225A\u2225\u2217 =\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56\n\ud835\udf0e\ud835\udc56, (104)\nwhere \ud835\udf0e\ud835\udc56 are singular values of A. Entezari et al. replaces the learned adjacency matrix with rank-r\napproximation by singular value decomposition (SVD) to achieve robust graph learning against\nadversarial attacks.\n9.2.3 Smoothness A common assumption is that connected nodes share similar features, or in other\nwords, the graph is \u201csmooth\u201d as the difference between local neighbors is small [65, 135, 199, 210].\nThe following metric is a natural way to measure graph smoothness:\nL\ud835\udc60\ud835\udc5a =\n1\n2\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56,\ud835\udc57=1\n\ud835\udc34\ud835\udc56\ud835\udc57 (\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n2 = \ud835\udc61\ud835\udc5f(X\u22a4\n(D \u2212 A)X) = \ud835\udc61\ud835\udc5f(X\n\u22a4\nLX), (105)\nwhere D is the degree matrix of A and L = D \u2212 A is called graph Laplacian. A variant is to use the\nnormalized graph Laplacian bL = D\n\u2212\n1\n2 LD\u2212\n1\n2 .\n9.3 Metric-based Methods\nMetric-based methods measure the similarity between nodes as the edge weights. They follow\nthe basic assumption that similar nodes tend to have connections with each other. We show some\nrepresentative works\nAdaptive Graph Convolutional Neural Networks (AGCN) [267]. AGCN learns a task-driven adaptive\ngraph during training to enable a more generalized and flexible graph representation model. After\nparameterizing the distance metric between nodes, AGCN is able to adapt graph topology to the\ngiven task. It proposes a generalized Mahalanobis distance between two nodes with the following\nformula:\nD(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) =\n\u221a\ufe03\n(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n\u22a4\ud835\udc40(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57), (106)\nwhere \ud835\udc40 = \ud835\udc4a\ud835\udc51\ud835\udc4a \u22a4\n\ud835\udc51\nand \ud835\udc4a\ud835\udc51 is the trainable weights to minimize task-specific objective. Then the\nGaussian kernel is used to obtain the adjacency matrix:\nG\ud835\udc56\ud835\udc57 = exp(\u2212D(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57)/(2\ud835\udf0e\n2\n)), (107)\n\ud835\udc34\u02c6 = \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52 (G). (108)\nGraph-Revised Convolutional Network (GRCN) [546]. GRCN uses a graph revision module to\npredict missing edges and revise edge weights through joint optimization on downstream tasks. It\nfirst learns the node embedding with GCN and then calculates pair-wise node similarity with the\ndot product as the kernel function.\n\ud835\udc4d = \ud835\udc3a\ud835\udc36\ud835\udc41\ud835\udc54 (\ud835\udc34, \ud835\udc4b), (109)\n\ud835\udc46\ud835\udc56\ud835\udc57 =\n\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\n. (110)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n40 W. Ju, et al.\nThe revised adjacency matrix is the residual summation of the original adjacency matrix\ud835\udc34\u02c6 = \ud835\udc34+\ud835\udc46.\nGRCN also applies a sparsification technique on the similarity matrix \ud835\udc46 to reduce computation cost:\n\ud835\udc46\n(\ud835\udc3e)\n\ud835\udc56\ud835\udc57 =\n\u001a\n\ud835\udc46\ud835\udc56\ud835\udc57, \ud835\udc46\ud835\udc56\ud835\udc57 \u2208 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n0, \ud835\udc46\ud835\udc56\ud835\udc57 \u2209 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n. (111)\nThreshold pruning is also a common strategy for sparsification. For example, CAGCN [613] uses\ndot product to measure node similarity, and refines the graph structure by removing edges between\nnodes whose similarity is less than a threshold \ud835\udf0f\ud835\udc5f and adding edges between nodes whose similarity\nis greater than another threshold \ud835\udf0f\ud835\udc4e.\nDefending Graph Neural Networks against Adversarial Attacks (GNNGuard) [571]. GNNGuard\nmeasures similarity between a node \ud835\udc62 and its neighbor \ud835\udc63 in the \ud835\udc58-th layer by cosine similarity and\nnormalizes node similarity at the node level within the neighborhood as follows:\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\u210e\n\ud835\udc58\n\ud835\udc62 \u2299 \u210e\n\ud835\udc58\n\ud835\udc63\n\u2225\u210e\n\ud835\udc58\n\ud835\udc62 \u22252 \u2225\u210e\n\ud835\udc58\n\ud835\udc63 \u22252\n, (112)\n\ud835\udefc\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63/\n\u2211\ufe01\n\ud835\udc63\u2208N\ud835\udc62\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u00d7 \ud835\udc41\u02c6 \ud835\udc58\ud835\udc62\n/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 \u2260 \ud835\udc63\n1/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 = \ud835\udc63\n, (113)\nwhere N\ud835\udc62 denotes the neighborhood of node \ud835\udc62 and \ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 =\n\u00cd\n\ud835\udc63\u2208N\ud835\udc62\n\u2225\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u22250. To stabilize GNN training,\nit also proposes a layer-wise graph memory by keeping part of the information from the previous\nlayer in the current layer. Similar to GNNGuard, IDGL [65] uses multi-head cosine similarity and\nmask edges with node similarity smaller than a non-negative threshold, and HGSL [586] generalizes\nthis idea to heterogeneous graphs.\nGraph Diffusion Convolution (GDC) [139]. GDC replaces the original adjacency matrix with\ngeneralized graph diffusion matrix S:\nS =\n\u2211\ufe01\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58T\n\ud835\udc58\n, (114)\nwhere \ud835\udf03\ud835\udc58 is the weighting coefficient and T is the generalized transition matrix. To ensure conver\u0002gence, GDC further requires that \u00cd\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58 = 1 and the eigenvalues of T lie in [0, 1]. The random\nwalk transition matrix T\ud835\udc5f \ud835\udc64 = AD\u22121and the symmetric transition matrix T\ud835\udc60\ud835\udc66\ud835\udc5a = D\n\u22121/2AD\u22121/2\nare\ntwo examples. This new graph structure allows graph convolution to aggregate information from a\nlarger neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying noise.\nHowever, in most cases graph diffusion will result in a dense adjacency matrix \ud835\udc46, so sparsification\ntechnology like top-k filtering and threshold filtering will be applied to graph diffusion. Following\nGDC, there are some other graph diffusion proposed. For example, AdaCAD [281] proposes Class\u0002Attentive Diffusion, which further considers node features and aggregates nodes probably of the\nsame class among K-hop neighbors. Adaptive diffusion convolution (ADC) [585] learns the optimal\nneighborhood size via optimizing a bi-level problem.\n9.4 Model-based Methods\nModel-based methods parameterize edge weights with more complex models like deep neural\nnetworks. Compared to metric-based methods, model-based methods offer greater flexibility and\nexpressive power.\nGraph Learning Network (GLN) [364]. GLN proposes a recurrent block to first produce interme\u0002diate node embeddings and then merge them with adjacency information as the output of this\nlayer to predict the adjacency matrix for the next layer. Specifically, it uses convolutional graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 41\noperations to extract node features, and creates a local-context embedding based on node features\nand the current adjacency matrix:\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc56=1\n\ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\n), (115)\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 = \ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc48\n(\ud835\udc59)\n), (116)\nwhere \ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\nand \ud835\udc48\n(\ud835\udc59)\nare the learnable weights. GLN then predicts the next adjacency matrix as\nfollows:\n\ud835\udc34\n(\ud835\udc59+1) = \ud835\udf0e\ud835\udc59 (\ud835\udc40(\ud835\udc59)\n\ud835\udefc\ud835\udc59 (\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59)\ud835\udc40\n(\ud835\udc59) \u22a4\n). (117)\nSimilarly, GLCN [199] models graph structure with a softmax layer over the inner product\nbetween the difference of node features and a learnable vector. NeuralSparse [595] uses a multi\u0002layer neural network to generate a learnable distribution from which a sparse graph structure is\nsampled. PTDNet [305] prunes graph edges with a multi-layer neural network and penalizes the\nnumber of non-zero elements to encourage sparsity.\nGraph Attention Networks (GAT) [452]. Besides constructing a new graph to guide the message\npassing and aggregation process of GNNs, many recent researchers also leverage the attention\nmechanism to adaptively model the relationship between nodes. GAT is the first work to introduce\nthe self-attention strategy into graph learning. In each attention layer, the attention weight between\ntwo nodes is calculated as the Softmax output on the combination of linear and non-linear transform\nof node features:\n\ud835\udc52\ud835\udc56\ud835\udc57 = \ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57), (118)\n\ud835\udefc\ud835\udc56\ud835\udc57 =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc57)\n\u00cd\n\ud835\udc58 \u2208N\ud835\udc56\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc58 )\n, (119)\nwhere N\ud835\udc56 denotes the neighborhood of node \ud835\udc56,W is learnable linear transform and \ud835\udc4e is pre-defined\nattention function. In the original implementation of GAT, \ud835\udc4e is a single-layer neural network with\nLeakyReLU:\n\ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57) = LeakyReLU(\u00aea\n\u22a4\n[W\u00ae\u210e\ud835\udc56||W\u00ae\u210e\ud835\udc57]). (120)\nThe attention weights are then used to guide the message-passing phase of GNNs:\n\u00ae\u210e\n\u2032\n\ud835\udc56 = \ud835\udf0e(\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n\ud835\udefc\ud835\udc56\ud835\udc57W\u00ae\u210e\ud835\udc57), (121)\nwhere \ud835\udf0e is a nonlinear function. It is beneficial to concatenate multiple heads of attention to\u0002gether to get a more stable and generalizable model, so-called multi-head attention. The attention\nmechanism serves as a soft graph structure learner which captures important connections within\nnode neighborhoods. Following GAT, many recent works propose more effective and efficient\ngraph attention operators to improve performance. GaAN [566] adds a soft gate at each attention\nhead to adjust its importance. MAGNA [461] proposes a novel graph attention diffusion layer to\nincorporate multi-hop information. One drawback of graph attention is that the time and space\ncomplexities are both \ud835\udc42(\ud835\udc41\n3\n). hGAO [132] performs hard graph attention by limiting node attention\nto its neighborhood. VIB-GSL [433] adopts the information bottleneck principle to guide feature\nmasking in order to drop task-irrelevant information and preserve actionable information for the\ndownstream task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n42 W. Ju, et al.\n9.5 Direct Methods\nDirect methods treat edge weights as free learnable parameters. These methods enjoy more flexibility\nbut are also more difficult to train. The optimization is usually carried out in an alternating way,\ni.e., iteratively updating the adjacency matrix A and the GNN encoder parameters \ud835\udf03.\nGLNN [135]. GLNN uses MAP estimation to learn an optimal adjacency matrix for a joint\nobjective function including sparsity and smoothness. Specifically, it targets at finding the most\nprobable adjacency matrix \ud835\udc34\u02c6 given graph node features \ud835\udc65:\n\ud835\udc34\u02dc\ud835\udc40\ud835\udc34\ud835\udc43 (\ud835\udc65) = argmax\n\ud835\udc34\u02c6\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6)\ud835\udc54(\ud835\udc34\u02c6), (122)\nwhere \ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) measures the likelihood of observing \ud835\udc65 given \ud835\udc34\u02c6, and \ud835\udc54(\ud835\udc34\u02c6) is the prior distribution of\n\ud835\udc34\u02c6. GLNN uses sparsity and property constraint as prior, and defines the likelihood function \ud835\udc53 as:\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) = \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n\ud835\udc3f\ud835\udc65) (123)\n= \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n(\ud835\udc3c \u2212 \ud835\udc34\u02c6)\ud835\udc65), (124)\nwhere \ud835\udf060 is a parameter. This likelihood imposed a smoothness assumption on the learned graph\nstructure. Some other works also model the adjacency matrix in a probabilistic manner. Bayesian\nGCNN [573] adopts a Bayesian framework and treats the observed graph as a realization from a\nfamily of random graphs. Then it estimates the posterior probablity of labels given the observed\ngraph adjacency matrix and features with Monte Carlo approximation. VGCN [104] follows a\nsimilar formulation and estimates the graph posterior through stochastic variational inference.\nPro-GNN [210] learns a clean graph structure from perturbed data and optimizes parameters for a\nrobust GNN, leveraging properties like sparsity, low rank, and feature smoothness.\nGraph Sparsification via Meta-Learning (GSML) [458]. GSML formulates GSL as a meta-learning\nproblem and uses bi-level optimization to find the optimal graph structure. The goal is to find a\nsparse graph structure that leads to high node classification accuracy at the same time given labeled\nand unlabeled nodes. To achieve this, GSML makes the inner optimization as training on the node\nclassification task, and targets the outer optimization at the sparsity of the graph structure, which\nformulates the following bi-level optimization problem:\n\ud835\udc3a\u02c6\n\u2217 = min\n\ud835\udc3a\u02c6 \u2208\u03a6(\ud835\udc3a)\n\ud835\udc3f\ud835\udc60\ud835\udc5d\ud835\udc60 (\ud835\udc53\ud835\udf03\n\u2217 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc48 ), (125)\n\ud835\udc60.\ud835\udc61 . \ud835\udf03 \u2217 = argmin\n\ud835\udf03\n\ud835\udc3f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (\ud835\udc53\ud835\udf03 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc3f). (126)\nIn this bi-level optimization problem, \ud835\udc3a\u02c6 \u2208 \u03a6(\ud835\udc3a) are the meta-parameters and optimized directly\nwithout parameterization. Similarly, LSD-GNN [124] also uses bi-level optimization. It models graph\nstructure with a probability distribution over the graph and reformulates the bi-level program in\nterms of the continuous distribution parameters.\n9.6 Summary\nIn this section, we provide the summary as follows:\n\u2022 Techniques. GSL aims to learn an optimized graph structure for better graph representations.\nIt is also used for more robust graph representation against adversarial attacks. According\nto the way of edge modeling, we categorize GSL into three groups: metric-based methods,\nmodel-based methods, and direct methods. Regularization is also a commonly used principle\nto make the learned graph structure satisfy specific properties including sparsity, low-rank\nand smoothness.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 43\n\u2022 Challenges and Limitations. Since there is no way to access the ground truth or optimal\ngraph structure as training data, the learning objective of GSL is either indirect (e.g., perfor\u0002mance on downstream tasks) or manually designed (e.g., sparsity and smoothness). Therefore,\nthe optimization of GSL is difficult and the performance is not satisfying. In addition, many\nGSL methods are based on homophily assumption, i.e., similar nodes are more likely to\nconnect with each other. However, many other types of connection exist in the real world\nwhich impose great challenges for GSL.\n\u2022 Future Works. In the future we expect more efficient and generalizable GSL methods to\nbe applied to large-scale and heterogeneous graphs. Most existing GSL methods focus on\npair-wise node similarities and thus struggle to scale to large graphs. Besides, they often\nlearn homogeneous graph structure, but in many scenarios graphs are heterogeneous.\n10 Social Analysis\nIn the real world, there usually exist complex relations and interactions between people and multiple\nentities. Taking people, concrete things, and abstract concepts in society as nodes and taking the\ndiverse, changeable, and large-scale connections between data as links, we can form massive and\ncomplex social information as social networks [43, 436]. Compared with traditional data structures\nsuch as texts and forms, modeling social data as graphs has many benefits. Especially with the arrival\nof the \"big data\" era, more and more heterogeneous information is interconnected and integrated,\nand it is difficult and uneconomical to model this information with a traditional data structure. The\ngraph is an effective implementation for information integration, as it can naturally incorporate\ndifferent types of objects and their interactions from heterogeneous data sources [349, 411]. A\nsummarization of social analysis applications is provided in Table 9.\n10.1 Concepts of Social Networks\nA social network is usually composed of multiple types of nodes, link relationships, and node\nattributes, which inherently include rich structural and semantic information. Specifically, a social\nnetwork can be homogeneous or heterogeneous and directed or undirected in different scenarios.\nWithout loss of generality, we define the social network as a directed heterogeneous graph \ud835\udc3a =\n{\ud835\udc49 , \ud835\udc38, T, R}, where \ud835\udc49 = {\ud835\udc5b\ud835\udc56 }\n|\ud835\udc49 |\n\ud835\udc56=1\nis the node set, \ud835\udc38 = {\ud835\udc52\ud835\udc56 }\n|\ud835\udc38|\n\ud835\udc56=1\nis the edge set, T = {\ud835\udc61\ud835\udc56 }\n| T |\n\ud835\udc56=1\nis the node\ntype set, and R = {\ud835\udc5f\ud835\udc56 }\n| R |\n\ud835\udc56=1\nis the edge type set. Each node \ud835\udc5b\ud835\udc56 \u2208 \ud835\udc49 is associated with a node type\nmapping: \ud835\udf19\ud835\udc5b (\ud835\udc5b\ud835\udc56) = \ud835\udc61\ud835\udc57: \ud835\udc49 \u2212\u2192 T and each edge \ud835\udc52\ud835\udc56 \u2208 \ud835\udc38 is associated with a node type mapping:\n\ud835\udf19\ud835\udc52 (\ud835\udc52\ud835\udc56) = \ud835\udc5f\ud835\udc57: \ud835\udc38 \u2212\u2192 R. A node \ud835\udc5b\ud835\udc56 may have a feature set, where the feature space is specific for the\nnode type. An edge \ud835\udc52\ud835\udc56is also represented by node pairs (\ud835\udc5b\ud835\udc57, \ud835\udc5b\ud835\udc58 ) at both ends and can be directed\nor undirected with relation-type-specific attributes. If |T | = 1 and |R| = 1, the social network is a\nhomogeneous graph; otherwise, it is a heterogeneous graph.\nAlmost any data produced by social activities can be modeled as social networks, for example,\nthe academic social network produced by academic activities such as collaboration and citation,\nthe online social network produced by user following and followed on social media, and the\nlocation-based social network produced by human activities on different locations. Based on\nconstructing social networks, researchers have new paths to data mining, knowledge discovery,\nand multiple application tasks on social data. Exploring social networks also brings new challenges.\nOne of the critical challenges is how to succinctly represent the network from the massive and\nheterogeneous raw graph data, that is, how to learn continuous and low-dimensional social network\nrepresentations, so as to researchers can efficiently perform advanced machine learning techniques\non the social network data for multiple application tasks, such as analysis, clustering, prediction,\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n44 W. Ju, et al.\nTable 9. A summarization of social analysis applications\nSocial networks Node type Edge type Applications References\nAcademic\nSocial\nNetwork\nAuthor,\nPublication,\nVenue,\nOrganization,\nKeyword\nAuthorship,\nCo-Author,\nAdvisor\u0002advisee,\nCiting, Cited,\nCo-Citing,\nPublishing\nClassification/\nClustering\nPaper/author classification [92, 370, 471,\n563], name disambiguation [52, 319, 368,\n575]\nRelationship\nprediction\nCo-authorship [69, 72, 610], citation rela\u0002tionship [203, 464, 550], advisor-advisee re\u0002lationship [286, 317, 594]\nRecommen\u0002dation\nCollaborator recommendation [235, 236,\n296], paper recommendation [20, 81, 429],\nvenue recommendation [337, 549]\nSocial\nMedia\nNetwork\nUser, Blog,\nArticle, Image,\nVideo\nFollowing,\nLike, Unlike,\nClicked,\nViewed,\nCommented,\nReposted\nAnomaly\ndetection\nMalicious attacks [294, 395, 434], emer\u0002gency detection [28, 79, 257], and robot dis\u0002covery [117, 304]\nSentiment\nanalysis\nCustomer feedback [389, 449, 572], public\nevents [33, 332, 450]\nInfluence\nanalysis\nImportant node finding [91, 386], informa\u0002tion diffusion modeling [226, 246, 356, 562]\nLocation-based\nSocial\nNetwork\nRestaurant,\nCinema, Mall,\nParking\nFriendship,\nCheck-in\nPOI recom\u0002mendation\nSpatial/temporal influence [416, 484, 589],\nsocial relationship [297, 513], textual infor\u0002mation [469, 483, 515]\nUrban\ncomputing\nTraffic congestion prediction [202, 511], ur\u0002ban mobility analysis [45, 539], event de\u0002tection [420, 548]\nand knowledge discovery. Thus, graph representation learning on the social network becomes the\nfoundational technique for social analysis.\n10.2 Academic Social Network\nAcademic collaboration is a common and important behavior in academic society, and also a major\nway for scientists and researchers to innovate and breakthrough scientific research, which leads to\nsocial relationship between scholars. The academic data generated by academic collaboration usually\ncontains a large number of interconnected entities with complex relationships [237, 602]. Normally,\nin an academic social network, the node type set consists of Author, Publication, Venue, Organization,\nKeyword, etc., and the relation set consists of Authorship, Co-Author, Advisor-advisee, Citing, Cited,\nCo-Citing, Publishing, Co-Word, etc. Note that in most social networks, each relation type always\nconnects two fixed node types with a fixed direction. For example, the relation Authorship points\nfrom the node type Author to Publication, and the Co-Author is an undirected relation between two\nnodes with type Author. Based on the node and relation types in an academic social network, one\ncan divide it into multiple categories. For example, the co-author network with nodes of Author and\nrelations of Co-Author, the citation network with nodes of Publication and relation of Citing, and the\nacademic heterogeneous information graph with multiple academic node and relation types. Many\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 45\nresearch institutes and academic search engines, such as Aminer1, DBLP2, Microsoft Academic\nGraph (MAG)3, have provided open academic social network datasets for research purposes.\nThere are multiple applications of graph representation learning on the academic social net\u0002work. Roughly, they can be divided into three categories\u2013academic entity classification/clustering,\nacademic relationship prediction, and academic resource recommendation.\n\u2022 Academic entities usually belong to different classes of research areas. Research of academic\nentity classification and clustering aims to categorize these entities, such as papers and\nauthors, into different classes [92, 213, 370, 471, 537, 563]. In literature, academic networks\nsuch as Cora, citepSeer, and Pubmed [406] have become the most widely used benchmark\ndatasets for examining the performance of graph representation learning models on paper\nclassification. Also, the author name disambiguation problem [52, 319, 368, 575] is also\nessentially a node clustering task on co-author networks and is usually solved by the graph\nrepresentation learning technique.\n\u2022 Academic relationship prediction represents the link prediction task on various academic\nrelations. Typical applications are co-authorship prediction [69, 72, 610] and citation rela\u0002tionship prediction [203, 464, 550]. Existing methods learn representations of authors and\npapers and use the similarity between two nodes to predict the link probability. Besides, some\nwork [286, 317, 594] studies the problem of advisor-advisee relationship prediction in the\ncollaboration network.\n\u2022 Various academic recommendation systems have been introduced to retrieve academic re\u0002sources for users from large amounts of academic data in recent years. For example, collabo\u0002rator recommendation [235, 236, 296] benefit researchers by finding suitable collaborators\nunder particular topics; paper recommendation [20, 81, 429] help researchers find relevant pa\u0002pers on given topics; venue recommendation [337, 549] help researchers choose appropriate\nvenues when they submit papers.\n10.3 Social Media Network\nWith the development of the Internet in decades, various online social media have emerged in large\nnumbers and greatly changed people\u2019s traditional social models. People can establish friendships\nwith others beyond the distance limit and share interests, hobbies, status, activities, and other\ninformation among friends. These abundant interactions on the Internet form large-scale complex\nsocial media networks, also named online social networks. Usually, in an academic social network,\nthe node type set consists of User, Blog, Article, Image, Video, etc., and the relation type set consists\nof Following, Like, Unlike, Clicked, Viewed, Commented, Reposted, etc. The main property of a social\nmedia network is that it usually contains multi-mode information on the nodes, such as video,\nimage, and text. Also, the relations are more complex and multiplex, including the explicit relations\nsuch as Like and Unlike and the implicit relations such as Clicked. The social media network can\nbe categorized into multiple types based on their media categories. For example, the friendship\nnetwork, the movie review network, and the music interacting network are extracted from different\nsocial media platforms. In a broad sense, the user-item networks in online shopping system can also\nbe viewed as social media networks as they also exist on the Internet and contains rich interactions\nby people. There are many widely used data sources for social media network analysis, such as\nTwitter, Facebook, Weibo, YouTube, and Instagram.\n1https://www.aminer.cn/\n2https://dblp.uni-trier.de/\n3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n46 W. Ju, et al.\nThe mainstream application research on social media networks via graph representation learning\ntechniques mainly includes anomaly detection, sentiment analysis, and influence analysis.\n\u2022 Anomaly detection aims to find strange or unusual patterns in social networks, which has\na wide range of application scenarios, such as malicious attacks [294, 395, 434], emergency\ndetection [28, 79], and robot discovery [117, 304] in social networks. Unsupervised anomaly\ndetection usually learns a reconstructed graph to detect those nodes with higher reconstructed\nerror as the anomaly nodes [5, 588]; Supervised methods model the problem as a binary\nclassification task on the learned graph representations [340, 596].\n\u2022 Sentiment analysis, also named as opinion mining, is to mine the sentiment, opinions, and\nattitudes, which can help enterprises understand customer feedback on products [389, 449,\n572] and help the government analyze the public emotion and make rapid response to public\nevents [33, 332, 450]. The graph representation learning model is usually combined with\nRNN-based [58, 561] or Transformer-based [7, 441] text encoders to incorporate both the\nuser relationship and textual semantic information.\n\u2022 Influence analysis usually aims to find several nodes in a social network to initially spread\ninformation such as advertisements, so as to maximize the final spread of information [91, 386].\nThe core challenge is to model the information diffusion process in the social network. Deep\nlearning methods [226, 246, 356, 562] usually leverage graph neural networks to learn node\nembeddings and diffusion probabilities between nodes.\n10.4 Location-based Social Network\nLocations are the fundamental information of human social activities. With the wide availability of\nmobile Internet and GPS positioning technology, people can easily acquire their precise locations\nand socialize with their friends by sharing their historical check-ins on the Internet. This opens up\na new avenue of research on location-based social network analysis, which gathered significant\nattention from the user, business, and government perspectives. Usually, in a location-based social\nnetwork, the node type set consists of User, and Location, also named Point of Interest(POI) in the\nrecommendation scenario containing multiple categories such as Restaurant, Cinema, Mall, Parking,\netc. The relation type set consists of Friendship, Check-in. Also, those node and relation types that\nexist in traditional social media networks can be included in a location-based social network. The\ndifference with other social networks, the main location-based social networks are spatial and\ntemporal, making the graph representation learning more challenging. For example, in a typical\nsocial network constructed for the POI recommendation, the user nodes are connected with each\nother by their friendship. The location nodes are connected by user nodes with the relations feature\nof timestamps. The location nodes also have a spatial relationship with each other and own have\ncomplex features, including categories, tags, check-in counts, number of users check-in, etc. There\nare many location-based social network datasets, such as Foursquare4, Gowalla5, and Waze6. Also,\nmany social media such as Twitter, Instagram, and Facebook can provide location information.\nThe research of graph representation learning on location-based social networks can be divided\ninto two categories: POI recommendation for business benefits and urban computing for public\nmanagement.\n\u2022 POI recommendation is one of the research hotspots in the field of location-based social\nnetworks and recommendation systems in recent years [195, 219, 489], which aim to uti\u0002lize historical check-ins of users and auxiliary information to recommend potential favor\n4https://foursquare.com/\n5https://www.gowalla.com/\n6https://www.waze.com/live-map/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 47\nplaces for users from a large of location points. Existing researches mainly integrate four\nessential characteristics, including spatial influence, temporal influence [416, 484, 589], social\nrelationship [297, 513], and textual information [469, 483, 515].\n\u2022 Urban computing is defined as a process of analysis of the large-scale connected urban data\ncreated from city activities of vehicles, human beings, and sensors [358, 359, 417]. Besides the\nlocal-based social network, the urban data also includes physical sensors, city infrastructure,\ntraffic roads, and so on. Urban computing aims to improve the quality of public management\nand life quality of people living in city environments. Typical applications including traffic\ncongestion prediction [202, 511], urban mobility analysis [45, 539], event detection [420, 548].\n10.5 Summary\nThis section introduces social analysis by graph representation learning and we provide the\nsummary as follows:\n\u2022 Techniques. Social networks, generated by human social activities, such as communication,\ncollaboration, and social interactions, typically involve massive and heterogeneous data, with\ndifferent types of attributes and properties that can change over time. Thus, social network\nanalysis is a field of study that explores the techniques to understand and analyze the complex\nattributes, heterogeneous structures, and dynamic information of social networks. Social\nnetwork analysis typically learns low-dimensional graph representations that capture the\nessential properties and patterns of the social network data, which can be used for various\ndownstream tasks, such as classification, clustering, link prediction, and recommendation.\n\u2022 Chanllenges and Limitations. Despite the structural heterogeneity in social networks\n(nodes and relations have different types), with the technological advances in social media,\nthe node attributes have become more heterogeneous now, containing text, video, and images.\nAlso, the large-scale problem is a pending issue in social network analysis. The data in\nthe social network has increased exponentially in past decades, containing a high density\nof topological links and a large amount of node attribute information, which brings new\nchallenges to the efficiency and effectiveness of traditional network representation learning\non the social network. Lastly, social networks are often dynamic, which means the network\ninformation usually changes over time, and this temporal information plays a significant\nrole in many downstream tasks, such as recommendations. This brings new challenges to\nrepresentation learning on social networks in incorporating temporal information.\n\u2022 Future Works. Recently, multi-modal big pre-training models that can fuse information\nfrom different modalities have gained increasing attention [369, 379]. These models can\nobtain valuable information from a large amount of unlabeled data and transfer it to various\ndownstream analysis tasks. Moreover, Transformer-based models have demonstrated better\neffectiveness than RNNs in capturing temporal information. In the future, there is potential\nfor introducing multi-modal big pre-training models in social network analysis. Also, it is\nimportant to make the models more efficient for network information extraction and use\nlightweight techniques like knowledge distillation to further enhance the applicability of the\nmodels. These advancements can lead to more effective social network analysis and enable\nthe development of more sophisticated applications in various domains.\n11 Molecular Property Prediction\nMolecular Property Prediction is an essential task in computational drug discovery and cheminfor\u0002matics. Traditional quantitative structure property/activity relationship (QSPR/QSAR) approaches\nare based on either SMILES or fingerprints [344, 522, 570], largely overlooking the topological\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n48 W. Ju, et al.\nfeatures of the molecules. To address this problem, graph representation learning has been widely\napplied to molecular property prediction. A molecule can be represented as a graph where nodes\nstand for atoms and edges stand for atom-bonds (ABs). Graph-level molecular representations\nare learned via the message passing mechanism to incorporate the topological information. The\nrepresentations are then utilized for the molecular property prediction tasks.\nSpecifically, a molecule is denoted as a topological graph G = (V, E), where V = {\ud835\udc63\ud835\udc56|\ud835\udc56 =\n1, . . . , |G|} is the set of nodes representing atoms. A feature vector x\ud835\udc56is associated with each node \ud835\udc63\ud835\udc56\nindicating its type such as Carbon, Nitrogen. E = {\ud835\udc52\ud835\udc56\ud835\udc57 |\ud835\udc56, \ud835\udc57 = 1, . . . , |G|} is the set of edges connecting\ntwo nodes (atoms) \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57 representing atom bonds. Graph representation learning methods\nare used to obtain the molecular representation hG. Then downstream classification or regression\nlayers \ud835\udc53 (\u00b7) are applied to predict the probability of target property of each molecule \ud835\udc66 = \ud835\udc53 (hG).\nIn Section 11.1, we introduce 4 types of molecular properties graph representation learning can\nbe treated and their corresponding datasets. Section 11.2 reviews the graph representation learning\nbackbones applied to molecular property prediction. Strategies for training the molecular property\nprediction methods are listed in Section 11.3.\n11.1 Molecular Property Categorization\nPlenty of molecular properties can be predicted by graph-based methods. We follow Wieder et al.\n[490] to categorize them into 4 types: quantum chemistry, physicochemical properties, biophysics,\nand biological effect.\nQuantum chemistry is a branch of physical chemistry focused on the application of quantum\nmechanics to chemical systems, including conformation, partial charges and energies. QM7, QM8,\nQM9 [501], COD [391] and CSD [154] are datasets for quantum chemistry prediction.\nPhysicochemical properties are the intrinsic physical and chemical characteristics of a substance,\nsuch as bioavailability, octanol solubility, aqueous solubility and hydrophobicity. ESOL, Lipophilicity\nand Freesolv [501] are datasets for physicochemical properties prediction.\nBiophysics properties are about the physical underpinnings of biomolecular phenomena, such\nas affinity, efficacy and activity. PDBbind [466], MUV, and HIV [501] are biophysics property\nprediction datasets.\nBiological effect properties are generally defined as the response of an organism, a population,\nor a community to changes in its environment, such as side effects, toxicity and ADMET. Tox21,\ntoxcast [501] and PTC [448] are biological effect prediction datasets.\nMoleculenet [501] is a widely-used benchmark dataset for molecule property prediction. It\ncontains over 700,000 compounds tested on different properties. For each dataset, they provide\na metric and a splitting pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv,\nLipophilicity and PDBbind are regression tasks, using MAE or RMSE as the evaluation metrics.\nOther tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation metric.\n11.2 Molecular Graph Representation Learning Backbones\nSince node attributes and edge attributes are crucial to molecular representation, most works use\nGNN instead of traditional graph representation learning methods as backbones, since many GNN\nmethods consider edge information. Existing GNNs designed for the general domain can be applied\nto molecular graphs. Table 10 summarizes the GNNs used for molecular property prediction and\nthe types of properties they can be applied to predict.\nFurthermore, many works customize their GNN structure by considering the chemical domain\nknowledge.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 49\nTable 10. Summary of GNNs in molecular property prediction.\nType Spatial/Specrtal Method Application\nReccurent GNN - R-GNN Biological effect [400]\nReccurent GNN - GGNN Quantum chemistry [333],\nBiological effect [9, 115, 492]\nReccurent GNN - IterRefLSTM Biophysics [9], Biological effect [9]\nConvolutional GNN Spatial/Specrtal GCN\nQuantum chemistry [280, 492, 529],\npysicochemical properties [75, 101, 393],\nBiophysics [34, 101, 529]\nBiological effect [261, 501]\nConvolutional GNN Specrtal LanczosNet Quantum chemistry [280]\nConvolutional GNN Specrtal ChebNet Physicochemical properties,\nBiophysics, Biological effect [267]\nConvolutional GNN Spatial GraphSAGE\nPhysicochemical properties [181],\nBiophysics [68, 108, 279],\nBiological effect [181, 328]\nConvolutional GNN Spatial GAT\nPhysicochemical properties [3, 181],\nBiophysics [34, 68],\nBiological effect [181]\nConvolutional GNN Spatial DGCNN Biophysics [63], Biological effect [568]\nConvolutional GNN Spatial GIN\nPhysicochemical properties [34, 181],\nBiophysics [180, 181],\nBiological effect [181]\nConvolutional GNN Spatial MPNN Physicochemical [320]\nTransformer - MAT Physicochemical, Biophysics [616]\n\u2022 First, the chemical bonds and molecule interaction are taken into consideration carefully. For\nexample, Ma et al. [320] use an additional edge GNN to model the chemical bonds separately.\nSpecifically, given an edge (\ud835\udc63,\ud835\udc64), they formulate an Edge-based GNN as:\nm\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = AGGedge ({h\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(\ud835\udc58\u22121)\n\ud835\udc62\ud835\udc63 , x\ud835\udc62 |\ud835\udc62 \u2208 N\ud835\udc63 \\ \ud835\udc64}), h\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = MLPedge ({m\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(0)\n\ud835\udc63\ud835\udc64 }), (127)\nwhere h\n(0)\n\ud835\udc63\ud835\udc64 = \ud835\udf0e(Weine\ud835\udc63\ud835\udc64) is the input state of the Edge-based GNN, Wein \u2208 R\n\ud835\udc51hid\u00d7\ud835\udc51\ud835\udc52\nis the\ninput weight matrix. PotentialNet [115] further uses different message passing operations\nfor different edge types. DGNN-DDI [325] leverage dual graph neural networks to model the\ninteraction between two molecules.\n\u2022 Second, motifs in molecular graphs play an important role in molecular property prediction.\nGSN [34] leverage substructure encoding to construct a topologically-aware message-passing\nmethod. Each node \ud835\udc63 updates its state h\n\ud835\udc61\n\ud835\udc63\nby combining its previous state with the aggregated\nmessages:\nh\n\ud835\udc61+1\n\ud835\udc63 = UP\ud835\udc61+1\nh\n\ud835\udc61\n\ud835\udc63\n, m\n\ud835\udc61+1\n\ud835\udc63\n\u0001\n, (128)\nm\n\ud835\udc61+1\n\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-v)\nor\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-e)\n, (129)\nwhere x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 contains the substructure information associated with nodes and\nedges, [] denotes a multiset. Yu et al. [551] constructs a heterogeneous graph using motifs\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n50 W. Ju, et al.\nand molecules. Motifs and molecules are both treated as nodes and the edges model the\nrelationship between motifs and graphs, for example, if a graph contains a motif, there will\nbe an edge between them. MGSSL [580] leverages a retrosynthesis-based algorithm BRICS\nand additional rules to find the motifs and combines motif layers with atom layers. It is a\nhierarchical framework jointly modeling atom-level information and motif-level information.\nAouichaoui et al. [12] introduce group-contribution-based attention to highlight the most\nsubstructures in molecules.\n\u2022 Third, different feature modalities have been used to improve molecular graph embedding.\nLin et al. [283] combine SMILES modality and graph modality with contrastive learning.\nZhu et al. [608] encode 2D molecular graph and 3D molecular conformation with a unified\nTransformer. It uses a unified model to learn 3D conformation generation given 2D graph\nand 2D graph generation given 3D conformation. Cremer et al.[78] use a Equivariant Graph\nNeural Networks to represent the 3D information of molecules. Liu et al. [293] consider\nmolecular chirality and design a chirality-aware molecular convolution module.\n\u2022 Finally, knowledge graph and literature can provide additional knowledge for molecular\nproperty prediction. Fang et al. [110] introduce a chemical element knowledge graph to\nsummarize microscopic associations between elements and augment the molecular graph\nbased on the knowledge graph, and a knowledge-aware message-passing network is used to\nencode the augmented graph. MuMo [428] introduces biomedical literature to guide molecular\nproperty prediction. It pretrains a GNN and a language model on paired data of molecules\nand literature mentions via contrastive learning:\n\u2113\n(z\n\ud835\udc3a\n\ud835\udc56\n,z\n\ud835\udc47\n\ud835\udc56\n)\n\ud835\udc56\n= \u2212 log\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\n)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc57=1\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc57\n)/\ud835\udf0f)\n, (130)\nwhere z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\nare the representation of molecule and its corresponding literature. Zhao et al.\n[583] propose a unified Transformer architecture to jointly model molecule graph and the\ncorresponding bioassay description.\n11.3 Training strategies\nDespite the encouraging performance achieved by GNNs, the traditional supervised training scheme\nof GNNs faces a severe limitation: The scarcity of available molecules with desired properties.\nAlthough there are a large number of molecular graphs in public databases such as PubChem,\nlabeled molecules are hard to acquire due to the high cost of wet-lab experiments and quantum\nchemistry calculations. Directly training GNNs on such limited molecules in a supervised way\nis prone to over-fitting and lack of generalization. To address this issue, few-shot learning and\nself-supervised learning are widely used in molecular property prediction.\nFew-shot learning. Few-shot learning aims at generalizing to a task with a small labeled data\nset. The prediction of each property is treated as a single task. Metric-based and optimization-based\nfew-shot learning have been adopted for molecular property prediction. Metric-based few-shot\nlearning is similar to nearest neighbors and kernel density estimation, which learns a metric or\ndistance function over objects. IterRefLSTM [9] leverages matching network [455] as the few\u0002shot learning framework, calculating the similarity between support samples and query samples.\nOptimization-based few-shot learning optimizes a meta-learner for parameter initialization which\ncan be fast adapted to new tasks. Meta-MGNN [161] adopts MAML [120] to train a parameter\ninitialization to adapt to different tasks and use self-attentive task weights for each task. PAR [474]\nalso uses MAML framework and learns an adaptive relation graph among molecules for each task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 51\nSelf-supervised learning. Self-supervised learning can pre-train a GNN model with plenty of\nunlabeled molecular graphs and transfer it to specific molecular property prediction tasks. Self\u0002supervised learning contains generative methods and predictive methods. Predictive methods design\nprediction tasks to capture the intrinsic data features. Pre-GNN [181] exploits both node-level and\ngraph-level prediction tasks including context prediction, attribute masking, graph-level property\nprediction and structural similarity prediction. MGSSL [580] provides a motif-based generative\npre-training framework making topology prediction and motif generation iteratively. Contrastive\nmethods learn graph representations by pulling views from the same graph close and pushing\nviews from different graphs apart. Different views of the same graph are constructed by graph\naugmentation or leveraging the 1D SMILES and 3D structure. MolCLR [478] augments molecular\ngraphs by atom masking, bond deletion and subgraph removal and maximizes the agreement\nbetween the original molecular graph and augmented graphs. Fang et al. [110] uses a chemical\nknowledge graph to guide the graph augmentation. SMICLR [366] uses contrastive learning across\nSMILES and 2D molecular graphs. GeomGCL [268] leverages graph contrastive learning to capture\nthe geometry of the molecule across 2D and 3D views. Jiang et al. [201] and Fang et al. [111] integrate\nmolecule graphs with chemical knowledge graph and fuse the two modalities with contrastive\nlearning. Self-supervised learning can also be combined with few-shot learning to fully leverage\nthe hierarchical information in the training set [215].\n11.4 Summary\nThis section introduces graph representation learning in molecular property prediction and we\nprovide the summary as follows:\n\u2022 Techniques. For molecular property prediction, a molecule is represented as a graph whose\nnodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN, GAT, and GraphSAGE\nare adopted to learn the graph-level representation. The representations are then fed into a\nclassification or regression head for the molecular property prediction tasks. Many works\nguide the model structure design with medical domain knowledge including chemical bond\nfeatures, motif features, different modalities of molecular representation, chemical knowledge\ngraph and literature. Due to the scarcity of available molecules with desired properties,\nfew-shot learning and contrastive learning are used to train molecular property prediction\nmodels, so that the model can leverage the information in large unlabeled dataset and can be\nadapted to new tasks with a few examples.\n\u2022 Challenges and Limitations. Despite the great success of graph representation learning\nin molecular property prediction, the methods still have limitations: 1) Few-shot molecular\nproperty prediction are not fully explored. 2) Most methods depend on training with labeled\ndata, but neglect the chemical domain knowledge.\n\u2022 Future Works. In the future, we expect that: 1) More few-shot learning and zero-shot learning\nmethods are studied for molecular property prediction to solve the data scarcity problem. 2)\nHeterogeneous data can be fused for molecular property prediction. There are a large amount\nof heterogeneous data about molecules such as knowledge graphs, molecule descriptions\nand property descriptions. They can be considered to assist molecular property prediction. 3)\nChemical domain knowledge can be leveraged for the prediction model. For example, when\nwe perform affinity prediction, we can consider molecular dynamics knowledge.\n12 Molecular Generation\nMolecular generation is pivotal to drug discovery, where it serves a fundamental role in downstream\ntasks like molecular docking [341] and virtual screening [457]. The goal of molecular generation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n52 W. Ju, et al.\nis to produce chemical structures that satisfy a specific molecular profile, e.g., novelty, binding\naffinity, and SA scores. Traditional methods have relied on 1D string formats like SMILES [148] and\nSELFIES [240]. With the recent advances in graph representation learning, numerous graph-based\nmethods have also emerged, where molecular graph G can naturally embody both 2D topology and\n3D geometry. While recent literature reviews [99, 342] have covered the general topics of molecular\ndesign, this chapter is dedicated to the applications of graph representation learning in the molecular\ngeneration task. Molecular generation is intrinsically a de novo task, where molecular structures\nare generated from scratch to navigate and sample from the vast chemical space. Therefore, this\nchapter does not discuss tasks that restrict chemical structures a priori, such as docking [131, 427]\nand conformation generation [412, 607].\n12.1 Taxonomy for molecular featurization methods\nThis section categorizes the different methods to feature molecules. The taxonomy presented here\nis unique to the task of molecular generation, owing to the various modalities of molecular entities,\ncomplex interactions with other bio-molecular systems and formal knowledge from the laws of\nchemistry and physics.\n2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one thing, a\nmolecule can be unambiguously represented by its 2D topological graph G2D, where atoms are\nnodes and bonds are edges. G2D can be encoded by canonical MPNN models like GCN [230],\nGAT [452], and R-GCN [401], in ways similar to tasks like social networks and knowledge graphs. A\ntypical example of this line of work is GCPN [543], a graph convolutional policy network generating\nmolecules with desired properties such as synthetic accessibility and drug-likeness.\nFor another, the 3D conformation of a molecule can be accurately depicted by its 3D geometric\ngraph G3D, which incorporates 3D atom coordinates. In 3D-GNNs like SchNet [405] and Orb\u0002Net [371], G3D is organized into a \ud835\udc58-NN graph or a radius graph according to the Euclidean distance\nbetween atoms. It is justifiable to approximate G3D as a 3D extension to G2D, since covalent atoms\nare closest to each other in most cases. However, G3D can also find a more long-standing origin\nin the realm of computational chemistry [126], where both covalent and non-covalent atomistic\ninteractions are considered to optimize the potential surface and simulate molecular dynamics.\nTherefore, G3D more realistically represents the molecular geometry, which makes a good fit for\nprotein pocket binding and 3D-QSAR optimization [453].\nMolecules can rotate and translate, affecting their position in the 3D space. Therefore, it is ideal to\nencode these molecules with GNNs equivariant/invariant to roto-translations, which can be \u223c 103\ntimes more efficient than data augmentation [144]. Equivariant GNNs can be based on irreducible\nrepresentation [10, 24, 37, 130, 446], regular representation [121, 192], or scalarization [190, 212,\n232\u2013234, 292, 398, 404, 405, 445], which are explained in more detail in [165]. Recent works like\nGraphVF [430] and MolCode [579] have been incorporating G2D and G3D to accurately capture the\nrelationship between structure and properties in molecular design in a unified way.\nUnbounded vs. binding-based. Earlier works have aimed to generate unbounded molecules in\neither 2D or 3D space, striving to learn good molecular representations through this task. In the 2D\nscenario, GraphNVP [329] first introduces a flow-based model to learn an invertible transformation\nbetween the 2D chemical space and the latent space. GraphAF [413] further adopts an autoregressive\ngeneration scheme to check the valence of the generated atoms and bonds. In the 3D scenario,\nG-SchNet [142] first proposes to utilize G3D (instead of 3D density grids) as the generation backbone.\nIt encodes G3D via SchNet, and uses an auxiliary token to generate atoms on the discretized 3D\nspace autoregressively. G-SphereNet [316] uses symmetry-invariant representations in a spherical\ncoordinate system (SCS) to generate atoms in the continuous 3D space and preserve equivariance.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 53\nUnbounded models adopt certain techniques to optimize specific properties of the generated\nmolecules. GCPN and GraphAF use scores like logP, QED, and chemical validity to tune the model\nvia reinforcement learning. EDM [178] can generate 3D molecules with property \ud835\udc50 by re-training\nthe diffusion model with \ud835\udc50\u2019s feature vector concatenated to the E(n) equivariant dynamics function\n\ud835\udf50\u02c6\ud835\udc61 = \ud835\udf19 (\ud835\udc9b\ud835\udc61, [\ud835\udc61, \ud835\udc50]). cG-SchNet [143] adopts a conditioning network architecture to jointly target\nmultiple electronic properties during conditional generation without the need to re-train the model.\nRetMol [481] uses a retrieval-based model for controllable generation.\nOn the other hand, binding-based methods generate drug-like molecules (aka. ligands) according\nto the binding site (aka. binding pocket) of a protein receptor. Drawing inspirations from the lock\u0002and-key model for enzyme action [122], works like LiGAN [380] and DESERT [302] uses 3D\ndensity grids to fit the density surface between the ligand and the receptor, encoded by 3D-CNNs.\nMeanwhile, a growing amount of literature has adopted G3D for representing ligand and receptor\nmolecules, because G3D more accurately depicts molecular structures and atomistic interactions\nboth within and between the ligand and the receptor. Representative works include 3D-SBDD [306],\nGraphBP [288], Pocket2Mol [361], and DiffSBDD [403]. GraphBP shares a similar workflow with G\u0002SphereNet, except that the receptor atoms are also incorporated into G3D to depict the 3D geometry\nat the binding pocket.\nAtom-based vs.fragment-based. Molecules are inherently hierarchical structures. At the atom\u0002istic level, molecules are represented by encoding atoms and bonds. At a coarser level, molecules\ncan also be represented as molecular fragments like functional groups or chemical sub-structures.\nBoth the composition and the geometry are fixed within a given fragment, e.g., the planar peptide\u0002bond (\u2013CO\u2013NH\u2013) structure. Fragment-based generation effectively reduces the degree of freedom\n(DOF) of chemical structures, and injects well-established knowledge about molecular patterns\nand reactivity. JT-VAE [207] decomposes 2D molecular graph G2D into a junction-tree structure\nT, which is further encoded via tree message-passing. DeepScaffold [270] expands the provided\nmolecular scaffold into 3D molecules. L-Net [272] adopts a graph U-Net architecture and devises\na custom three-level node clustering scheme for pooling and unpooling operations in molecular\ngraphs. A number of works have also emerged lately for fragment-based generation in the binding\u0002based setting, including FLAG [581] and FragDiff [360]. FLAG uses a regression-based approach to\nsequentially decide the type and torsion angle of the next fragment to be placed at the binding site,\nand finally optimizes the molecule conformation via a pseudo-force field. FragDiff also adopts a\nsequential generation process but uses a diffusion model to determine the type and pose of each\nfragment in one go.\n12.2 Generative methods for molecular graphs\nFor a molecular graph generation process, the model first learns a latent distribution \ud835\udc43 (\ud835\udc4d |G)\ncharacterizing the input molecular graphs. A new molecular graph G\u02c6 is then generated by sam\u0002pling and decoding from this learned distribution. Various models have been adopted to generate\nmolecular graphs, including generative adversarial network (GAN), variational auto-encoder (VAE),\nnormalizing flow (NF), diffusion model (DM), and autoregressive model (AR).\nGenerative adversarial network (GAN). GAN [149] is trained to discriminate real data \ud835\udc99\nfrom generated generated data \ud835\udc9b, with the training object formalized as\nmin\n\ud835\udc3a\nmax\n\ud835\udc37\nL (\ud835\udc37,\ud835\udc3a) = E\ud835\udc99\u223c\ud835\udc5ddata [log\ud835\udc37(\ud835\udc99)] + E\ud835\udc9b\u223c\ud835\udc5d (\ud835\udc9b) [log(1 \u2212 \ud835\udc37(\ud835\udc3a(\ud835\udc9b)))], (131)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n54 W. Ju, et al.\nwhere \ud835\udc3a(\u00b7) is the generator function and \ud835\udc37(\u00b7) is the discriminator function. For example, Mol\u0002GAN [82] encodes G2D with R-GCN, trains \ud835\udc37 and \ud835\udc3a with improved W-GAN [13], and uses rein\u0002forcement learning to generate attributed molecules, where the score function is assigned from\nRDKit [249] and chemical validity.\nVaraitional auto-encoder (VAE). In VAE [228], the decoder parameterizes the conditional\nlikelihood distribution \ud835\udc5d\ud835\udf03 (\ud835\udc99|\ud835\udc9b), and the encoder parameterizes an approximate posterior distribution\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99) \u2248 \ud835\udc5d\ud835\udf03 (\ud835\udc9b|\ud835\udc99). The model is optimized by the evidence lower bound (ELBO), consisting of the\nreconstruction loss term and the distance loss term:\nmax\n\ud835\udf03,\ud835\udf19\nL\ud835\udf03,\ud835\udf19 (\ud835\udc99) := E\ud835\udc9b\u223c\ud835\udc5e\ud835\udf19 (\u00b7 |\ud835\udc99)\n\u0014\nln \ud835\udc5d\ud835\udf03 (\ud835\udc99, \ud835\udc9b)\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99)\n\u0015\n= ln \ud835\udc5d\ud835\udf03 (\ud835\udc99) \u2212 \ud835\udc37KL \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc99) \u2225\ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc99)\n\u0001\n. (132)\nMaximizing ELBO is equivalent to simultaneously maximizing the log-likelihood of the observed\ndata, and minimizing the divergence of the approximate posterior \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc65) from the exact poste\u0002rior \ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc65). Representative works along this thread include JT-VAE [207], GraphVAE [419], and\nCGVAE [290] for the 2D generation task, and 3DMolNet [351] for the 3D generation task.\nAutoregressive model (AR). Autoregressive model is an umbrella definition for any model\nthat sequentially generates the components (atoms or fragments) of a molecule. ARs better capture\nthe interdependency within the molecular structure and allows for explicit valency check. For each\nstep in AR, the new component can be produced using different techniques:\n\u2022 Regression/classification, such is the case with 3D-SBDD [306], Pocket2Mol [361], etc.\n\u2022 Reinforcement learning, such is the case with L-Net [272], DeepLigBuilder [273], etc.\n\u2022 Probabilistic models like normalizing flow and diffusion.\nNormalizing flow (NF). Based on the change-of-variable theorem, NF [385] constructs an\ninvertible mapping \ud835\udc53 between a complex data distribution \ud835\udc99 \u223c \ud835\udc4b: and a normalized latent distribu\u0002tion \ud835\udc9b \u223c \ud835\udc4d. Unlike VAE, which has juxtaposed parameters for encoder and decoder, the flow model\nuses the same set of parameter for encoding and encoding: reverse flow \ud835\udc53\n\u22121\nfor generation, and\nforward flow \ud835\udc53 for training:\nmax\n\ud835\udc53\nlog \ud835\udc5d(\ud835\udc99) = log \ud835\udc5d\ud835\udc3e (\ud835\udc9b\ud835\udc3e) (133)\n= log \ud835\udc5d\ud835\udc3e\u22121 (\ud835\udc9b\ud835\udc3e\u22121) \u2212 log\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc3e (\ud835\udc9b\ud835\udc3e\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc3e\u22121\n\u0013\n(134)\n= . . . (135)\n= log \ud835\udc5d0 (\ud835\udc9b0) \u2212\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc56=1\nlog\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc56 (\ud835\udc9b\ud835\udc56\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc56\u22121\n\u0013\n, (136)\nwhere \ud835\udc53 = \ud835\udc53\ud835\udc3e \u25e6 \ud835\udc53\ud835\udc3e\u22121 \u25e6 ... \u25e6 \ud835\udc531 is a composite of \ud835\udc3e blocks of transformation. While GraphNVP [329]\ngenerates the molecular graph with NF in one go, following works tend to use the autoregressive\nflow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439].\nDiffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion\nsteps to slowly add random noise to data \ud835\udc990 \u223c \ud835\udc5e(\ud835\udc99):\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121) = N (\ud835\udc99\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc99\ud835\udc61\u22121, \ud835\udefd\ud835\udc61 \ud835\udc70), (137)\n\ud835\udc5e(\ud835\udc991:\ud835\udc47 |\ud835\udc990) =\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121). (138)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 55\nTable 11. Summary of molecular generation models.\nModel 2D/3D Binding\u0002based\nFragment\u0002basedGNN\nBackbone\nGenerative\nModel\nGCPN [543] 2D GCN [230] GAN\nMolGAN [82] 2D R-GCN [401] GAN\nDEFactor [16] 2D GCN GAN\nGraphVAE [419] 2D ECC [418] VAE\nMDVAE [100] 2D GGNN [274] VAE\nJT-VAE [207] 2D \u2713 MPNN [147] VAE\nCGVAE [290] 2D GGNN VAE\nDeepScaffold [270] 2D \u2713 GCN VAE\nGraphNVP [329] 2D R-GCN NF\nMoFlow [557] 2D R-GCN NF\nGraphAF [413] 2D R-GCN NF + AR\nGraphDF [318] 2D R-GCN NF + AR\nL-Net [272] 3D \u2713 g-U-Net [133] AR\nG-SchNet [142] 3D SchNet [405] AR\nGEN3D [388] 3D EGNN [398] AR\nG-SphereNet [316] 3D SphereNet [292] NF + AR\nEDM [178] 3D EGNN DM\nGCDM [347] 3D GCPNet [346] DM\n3D-SBDD [306] 3D \u2713 SchNet AR\nPocket2Mol [361] 3D \u2713 GVP [211] AR\nFLAG [581] 3D \u2713 \u2713 SchNet AR\nGraphBP [288] 3D \u2713 SchNet NF + AR\nSiamFlow [439] 3D \u2713 R-GCN NF\nDiffBP [282] 3D \u2713 EGNN DM\nDiffSBDD [403] 3D \u2713 EGNN DM\nTargetDiff [156] 3D \u2713 EGNN DM\nFragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR\nGraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR\nMolCode [579] 2D + 3D \u2713 EGNN NF + AR\nThey then learn to reverse the diffusion process to construct desired data samples from the noise:\n\ud835\udc5d\ud835\udf03 (\ud835\udc990:\ud835\udc47 ) = \ud835\udc5d(\ud835\udc99\ud835\udc47 )\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61), (139)\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61) = N (\ud835\udc99\ud835\udc61\u22121; \ud835\udf41\ud835\udf03(\ud835\udc99\ud835\udc61, \ud835\udc61), \ud835\udeba\ud835\udf03 (\ud835\udc99\ud835\udc61, \ud835\udc61)), (140)\nwhile the models are trained using a variational lower bound. Diffusion models have been applied\nto generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands\nin DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate\nmolecular fragments in autoregressive models, as is the case with FragDiff [360].\n12.3 Summary and prospects\nWe wrap up this chapter with Table 11, which profiles existing molecular generation models\naccording to their taxonomy for molecular featurization, the GNN backbone, and the generative\nmethod. This chapter covers the critical topics of molecular generation, which also elicit valuable\ninsights into the promising directions for future research. We summarize these important aspects\nas follows.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n56 W. Ju, et al.\nTechniques. Graph neural networks can be flexibly leveraged to encode molecular features\non different representation levels and across different problem settings. Canonical GNNs like\nGCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs,\nwhile 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular,\nthis 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the\nbinding protein receptor is considered alongside the ligand geometry per se. Fragment-based models\nlike JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure.\nVarious generative methods have also been effectively incorporated into the molecular setting,\nincluding generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive\nmodel (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to\ngenerate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating\nthe search for drug candidates.\nChallenges and Limitations. While there has been an abundant supply of unlabelled molecular\nstructural and geometric data [125, 193, 426], the number of labeled molecular data over certain\ncritical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the\nother hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of\nthe generated molecules, such as QED and Vina [103], rather than actual wet lab experiments.\nFuture Works. Besides the structural and geometric attributes described in this chapter, an\neven more extensive array of data can be applied to aid molecular generation, including chemical\nreactions and medical ontology. These data can be organized into a heterogeneous knowledge\ngraph to aid the extraction of high-quality molecular representations. Furthermore, high through\u0002put experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and\ndruggability of the generated molecules in the wet lab. Concrete case studies, such as the design of\npotential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights\ninto leveraging these molecular generative models to facilitate the design and fabrication of potent\nand applicable drug molecules in the pharmaceutical industry.\nIntegrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations\noffers a promising new direction in molecular generation. Recent studies like those by [196] and\n[160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLM\u0002based approaches in this domain, including those by [338] and [18], predominantly utilize textual\nSMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The\nemerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image,\nand text, which could more comprehensively capture the intricacies of molecular structures. This\napproach marks a significant shift towards utilizing graph-based information alongside traditional\ntext, enhancing the capability of LLMs in molecular generation. Such advances suggest that future\nresearch should focus more on exploiting the synergy between graph-based molecular representa\u0002tions and the evolving landscape of LLMs to address complex challenges in chemistry and material\nsciences.\n13 Recommender Systems\nThe use of graph representation learning in recommender systems has been drawing increasing\nattention as one of the key strategies for addressing the issue of information overload. With their\nstrong ability to capture high-order connectivity between graph nodes, deep graph representation\nlearning has been shown to be beneficial in enhancing recommendation performance across a\nvariety of recommendation scenarios.\nTypical recommender systems take the observed interactions between users and items and\ntheir fixed features as input, and are intended for making proper predictions on which items a\nspecific user is probably interested in. To formulate, given an user set U, an item set I and the\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 57\nTable 12. Summary of graph models for recommender systems.\nModel Recommendation Task Graph Structure Graph Encoder Representation\nGC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer\nNGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation\nMMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer\nLightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling\nDGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling\nCAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling\nSR-GNN [496] Session-based Transition Graph GGNN Soft-Attention\nGC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention\nFGNN [377] Session-based Session Graph GAT Last-Layer\nGAG [378] Session-based Session Graph GCN Self-Attention\nGCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling\nHyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention\nDHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer\nMBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention\nHCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer\nH3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer\nSTHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling\ninteraction matrix between users and items \ud835\udc4b \u2208 {0, 1}\n|U|\u00d7|I|\n, where \ud835\udc4b\ud835\udc62,\ud835\udc63 indicates there is an\nobserved interaction between user \ud835\udc62 and item \ud835\udc56. The target of GNNs on recommender systems is to\nlearn representations \u210e\ud835\udc62, \u210e\ud835\udc56 \u2208 R\n\ud835\udc51\nfor given \ud835\udc62 and \ud835\udc56. The preference score can further be calculated\nby a similarity function:\n\ud835\udc65\u02c6\ud835\udc62,\ud835\udc56 = \ud835\udc53 (\u210e\ud835\udc62, \u210e\ud835\udc56), (141)\nwhere \ud835\udc53 (\u00b7, \u00b7) is the similarity function, e.g. inner product, cosine similarity, multi-layer perceptrons\nthat takes the representation of \ud835\udc62 and \ud835\udc56 and calculate the preference score \ud835\udc65\u02c6\ud835\udc62,\ud835\udc56.\nWhen it comes to adapting graph representation learning in recommender systems, a key step is\nto construct graph-structured data from the interaction set \ud835\udc4b. Generally, a graph is represented\nas G = {V, E} where V, E denotes the set of vertices and edges respectively. According to the\nconstruction of G, we can categorize the existing works as follows into three parts which are\nintroduced in the following subsections. A summary is provided in Table 12.\n13.1 User-Item Bipartite Graph\n13.1.1 Graph Construction A undirected bipartite graph where the vertex set V = U \u222a I and the\nundirected edge set E = {(\ud835\udc62,\ud835\udc56)|\ud835\udc62 \u2208 U \u2227\ud835\udc56 \u2208 I}. Under this case the graph adjacency can be directly\nobtained from the interaction matrix, thus the optimization target on the user-item bipartite graph\nis equivalent to collaborative filtering tasks such as MF [239] and SVD++ [238].\nThere have been plenty of previous works that applied GNNs on the constructed user-item bipar\u0002tite graphs. GC-MC [27] firstly applies graph convolution networks to user-item recommendation\nand optimizes a graph autoencoder (GAE) to reconstruct interactions between users and items.\nNGCF [470] introduces the concept of Collaborative Filtering (CF) into graph-based recommen\u0002dations by modeling the affinity between neighboring nodes on the interaction graph. MMGCN\n[485] extends the graph-based recommendation to multi-modal scenarios by constructing different\nsubgraphs for each modal. LightGCN [169] improves NGCF by removing the non-linear activation\nfunctions and simplifying the message function. With the development of disentangled representa\u0002tion learning, there are works like DGCF [473] that introduce disentangled graph representation\nlearning to represent users and items from multiple disentangled perspectives. Additionally, having\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n58 W. Ju, et al.\nrealized the limitation of the existing message-passing scheme in capturing collaborative signals,\nCAGCN [480] proposes Common Interacted Ratio (CIR) as a recommendation-oriented topological\nmetric for GNN-based recommender models.\n13.1.2 Graph Propagation Scheme A common practice is to follow the traditional message-passing\nnetworks (MPNNs) and design the graph propagation method accordingly. GC-MC adopts vanilla\nGCNs to encode the user-item bipartite graph. NGCF enhances GCNs by considering the affinity\nbetween users and items. The message function of NGCF from node \ud835\udc57 to \ud835\udc56 is formulated as:\n(\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc57 = \u221a 1\n|N\ud835\udc56| |N\ud835\udc57|\n(\ud835\udc4a1\ud835\udc52\ud835\udc57 +\ud835\udc4a2 (\ud835\udc52\ud835\udc56 \u2299 \ud835\udc52\ud835\udc57))\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc56 = \ud835\udc4a1\ud835\udc52\ud835\udc56\n, (142)\nwhere \ud835\udc4a1,\ud835\udc4a2 are trainable parameters, \ud835\udc52\ud835\udc56 represents \ud835\udc56\u2019s representation from previous layer. The\nmatrix form can be further provided by:\n\ud835\udc38\n(\ud835\udc59) = LeakyReLU( (L + \ud835\udc3c)\ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n1\n+ L\ud835\udc38\n(\ud835\udc59\u22121) \u2299 \ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n2\n), (143)\nwhere L represents the Laplacian matrix of the user-item graph. The element-wise product in Eq.\n143 represents the affinity between connected nodes, containing the collaborative signals from\ninteractions.\nHowever, the notable heaviness and burdensome calculation of NGCF\u2019s architecture hinder\nthe model from making faster recommendations on larger graphs. LightGCN solves this issue by\nproposing Light Graph Convolution (LGC), which simplifies the convolution operation with:\n\ud835\udc52\n(\ud835\udc59+1)\n\ud835\udc56\n=\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n1\n\u221a\ufe01\n|N\ud835\udc56||N\ud835\udc57|\n\ud835\udc52\n(\ud835\udc59)\n\ud835\udc57\n. (144)\nWhen an interaction takes place, e.g. a user clicks a particular item, there could be multiple\nintentions behind the observed interaction. Thus it is necessary to consider the various disentangled\nintentions among users and items. DGCF proposes the cross-intent embedding propagation scheme\non the graph, inspired by the dynamic routing algorithm of capsule networks [394]. To formulate,\nthe propagation process maintains a set of routing logits \u02dc\ud835\udc46\ud835\udc58 (\ud835\udc62,\ud835\udc56) for each user \ud835\udc62. The weighted sum\naggregator to get the representation of \ud835\udc62 can be defined as:\n\ud835\udc62\n\ud835\udc61\n\ud835\udc58\n=\n\u2211\ufe01\n\ud835\udc56\u2208N\ud835\udc62\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) \u00b7 \ud835\udc56\n0\n\ud835\udc58\n(145)\nfor \ud835\udc61-th iteration, where L\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) denotes the Laplacian matrix of \ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56), formulated as:\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) =\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n\u221a\ufe03\n[\n\u00cd\n\ud835\udc56\n\u2032\u2208N\ud835\udc62\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56\u2032)] \u00b7 [\u00cd\n\ud835\udc62\n\u2032\u2208N\ud835\udc56\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62\n\u2032\n,\ud835\udc56)]\n. (146)\n13.1.3 Node Representations After the graph propagation module outputs node-level representa\u0002tions, there are multiple methods to leverage node representations for recommendation tasks. A\nplain solution is to apply a readout function on layer outputs like the concatenation operation used\nby NGCF:\n\ud835\udc52\n\u2217 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc52(0)\n, ..., \ud835\udc52 (\ud835\udc3f)) = \ud835\udc52\n(0)\n\u2225...\u2225\ud835\udc52\n(\ud835\udc3f)\n. (147)\nHowever, the readout function among layers would neglect the relationship between the target\nitem and the current user. A general solution is to use the attention mechanism [451] to reweight\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 59\nand aggregate the node representations. SR-GNN adapts soft-attention mechanism to model the\nitem-item relationship:\n\ud835\udefc\ud835\udc56 = q\n\ud835\udc47\n\ud835\udf0e(\ud835\udc4a1\ud835\udc52\ud835\udc61 +\ud835\udc4a2\ud835\udc52\ud835\udc56 + \ud835\udc50),\n\ud835\udc60\ud835\udc54 =\n\ud835\udc5b\u2211\ufe01\u22121\n\ud835\udc56=1\n\ud835\udefc\ud835\udc56\ud835\udc52\ud835\udc56,\n(148)\nwhere q, \ud835\udc4a1, \ud835\udc4a2 are trainable matrices.\nSome methods focus on exploiting information from multiple graph structures. A common\npractice is contrastive learning, which maximizes the mutual information between hidden repre\u0002sentations from several views. HCCF leverage InfoNCE loss as the estimator of mutual information,\ngiven a pair of representation \ud835\udc67\ud835\udc56, \u0393\ud835\udc56 for node \ud835\udc56, controlled by temperature parameter \ud835\udf0f:\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc41\ud835\udc36\ud835\udc38 (\ud835\udc56) = \u2212 log exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56))/\ud835\udf0f\n\u00cd\n\ud835\udc56\n\u2032\u2260\ud835\udc56 exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56\n\u2032 ))/\ud835\udf0f\n. (149)\nBesides InfoNCE, there exist several other ways to combine node representations from different\nviews. For instance, MBHT applies an attention mechanism to fuse multiple semantics, DisenPOI\nadapts bayesian personalized ranking loss (BPR) [384] as a soft estimator for contrastive learning,\nand KBGNN applies pair-wise similarities to ensure the consistency from two views.\n13.2 Transition Graph\n13.2.1 Transition Graph Construction Since sequence-based recommendation (SR) is one of the\nfundamental problems in recommender systems, some researches focus on modeling the sequential\ninformation with GNNs. A commonly applied way is to construct transition graphs based on each\ngiven sequence according to the clicking sequence by a user. To formulate, given a user \ud835\udc62\u2019s clicking\nsequence \ud835\udc60\ud835\udc62 = [\ud835\udc56\ud835\udc62,1,\ud835\udc56\ud835\udc62,2, ...,\ud835\udc56\ud835\udc62,\ud835\udc5b] containing \ud835\udc5b items, noting that there could be duplicated items, the\nsequential graph is constructed via G\ud835\udc60 = {SET(\ud835\udc60\ud835\udc62), E}, where \u2200\n\ud835\udc56\ud835\udc57,\ud835\udc56\ud835\udc58\n\u2208 E indicates there exists\na successive transition from \ud835\udc56\ud835\udc57 to \ud835\udc56\ud835\udc58 . Since G\ud835\udc60 are directed graphs, a widely used way to depict\ngraph connectivity is by building the connection matrix \ud835\udc34\ud835\udc60 \u2208 R\n\ud835\udc5b\u00d72\ud835\udc5b\n. \ud835\udc34\ud835\udc60is the combination of two\nadjacency matrices \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n], which denotes the normalized node degrees of incoming\nand outgoing edges in the session graph respectively.\nThe proposed transition graphs that obtain user behavior patterns have been demonstrated\nimportant to session-based recommendations [263, 291]. SR-GNN and GC-SAN [496, 516] propose\nto leverage transition graphs and apply attention-based GNNs to capture the sequential information\nfor session-based recommendation. FGNN [377] formulates the recommendation within a session\nas a graph classification problem to predict the next item for an anonymous user. GAG [378] and\nGCE-GNN [482] further extend the model to capture global embeddings among multiple session\ngraphs.\n13.2.2 Session Graph Propagation Since the session graphs are directed item graphs, there have\nbeen multiple session graph propagation methods to obtain node representations on session graphs.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n60 W. Ju, et al.\nSR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information\nfrom a given session graph adjacency \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n] and item embedding set {\ud835\udc52\ud835\udc56 }:\n\ud835\udc4e\ud835\udc61 = \ud835\udc34\ud835\udc60 [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121]\n\ud835\udc47\ud835\udc3b + \ud835\udc4f, (150)\n\ud835\udc67\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc67\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc67\ud835\udc52\ud835\udc61\u22121), (151)\n\ud835\udc5f\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc5f\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc61\u22121), (152)\n\ud835\udc52\u02dc\ud835\udc61 = tanh(\ud835\udc4a\ud835\udc5c\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5c (\ud835\udc5f\ud835\udc61 \u2299 \ud835\udc52\ud835\udc61\u22121)), (153)\n\ud835\udc52\ud835\udc61 = (1 \u2212 \ud835\udc67\ud835\udc61) \u2299 \ud835\udc52\ud835\udc61\u22121 + \ud835\udc67\ud835\udc61\ud835\udc52\u02dc\ud835\udc61, (154)\nwhere \ud835\udc4a s and \ud835\udc48 s are trainable parameters. GC-SAN extend GGNN by calculating initial state \ud835\udc4e\ud835\udc61\nseparately to better exploit transition information:\n\ud835\udc4e\ud835\udc61 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc56\ud835\udc5b)\n), \ud835\udc34(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n)). (155)\n13.3 HyperGraph\n13.3.1 Hypergraph Topology Construction Motivated by the idea of modeling hyper-structures\nand high-order correlation among nodes, hypergraphs [119] are proposed as extensions of the\ncommonly used graph structures. For graph-based recommender systems, a common practice is\nto construct hyper structures among the original user-item bipartite graphs. To be specific, an\nincidence matrix of a graph with vertex set V is presented as a binary matrix \ud835\udc3b \u2208 {0, 1}\n|V |\u00d7 | E |\n,\nwhere E represents the set of hyperedges. Each entry \u210e(\ud835\udc63, \ud835\udc52) of \ud835\udc3b depicts the connectivity between\nvertex \ud835\udc63 and hyperedge \ud835\udc52:\n\u210e(\ud835\udc63, \ud835\udc52) =\n(\n1 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2208 \ud835\udc52\n0 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2209 \ud835\udc52\n. (156)\nGiven the formulation of hypergraphs, the degrees of vertices and hyperedges of \ud835\udc3b can then be\ndefined with two diagonal matrices \ud835\udc37\ud835\udc63 \u2208 N\n|V |\u00d7 |V | and \ud835\udc37\ud835\udc52 \u2208 N| E |\u00d7 | E |, where\n\ud835\udc37\ud835\udc63 (\ud835\udc56;\ud835\udc56) =\n\u2211\ufe01\n\ud835\udc52\u2208 E\n\u210e(\ud835\udc63\ud835\udc56, \ud835\udc52), \ud835\udc37\ud835\udc52 (\ud835\udc57; \ud835\udc57) =\n\u2211\ufe01\n\ud835\udc63\u2208V\n\u210e(\ud835\udc63, \ud835\udc52\ud835\udc57). (157)\nThe development of Hypergraph Neural Networks (HGNNs) [119, 188, 598] have shown to\nbe capable of capturing the high-order connectivity between nodes. HyperRec [463] firstly at\u0002tempts to leverage hypergraph structures for sequential recommendation by connecting items\nwith hyperedges according to the interactions with users during different time periods. DHCF\n[198] proposes to construct hypergraphs for users and items respectively based on certain rules, to\nexplicitly capture the collaborative similarities via HGNNs. MBHT [532] combines hypergraphs\nwith a low-rank self-attention mechanism to capture the dynamic heterogeneous relationships\nbetween users and items. HCCF [505] uses the contrastive information between hypergraph and\ninteraction graph to enhance the recommendation performance. To extend the model\u2019s ability to\nmulti-domain categories of items, H3Trans [523] incorporates two hyperedge-based modules and\nleverages hierarchical hypergraph propagation to transfer from domains. STHGCN [524] formulates\na spatio-temporal hypergraph structure for POI recommendation.\n13.3.2 Hyper Graph Message Passing With the development of HGNNs, previous works have\nproposed different variants of HGNN to better exploit hypergraph structures. A classic high-order\nhyper convolution process on a fixed hypergraph G = {V, E} with hyper adjacency \ud835\udc3b is given by:\n\ud835\udc54 \u2605\ud835\udc4b = \ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc3b\ud835\udc37\u22121\n\ud835\udc52 \ud835\udc3b\n\ud835\udc47\ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc4b\u0398, (158)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 61\nwhere \ud835\udc37\ud835\udc63, \ud835\udc37\ud835\udc52 are degree matrices of nodes and hyperedges, \u0398 denotes the convolution kernel. For\nhyper adjacency matrix \ud835\udc3b, DHCF refers to a rule-based hyperstructure via k-order reachable rule,\nwhere nodes in the same hyperedge group are k-order reachable to each other:\n\ud835\udc34\n\ud835\udc58\n\ud835\udc62 = min(1, power(\ud835\udc34 \u00b7 \ud835\udc34\n\ud835\udc47\n, \ud835\udc58)), (159)\nwhere \ud835\udc34 denotes the graph adjacency matrix. By considering the situations where \ud835\udc58 = 1, 2, the\nmatrix formulation of the hyper connectivity of users and items is calculated with:\n(\n\ud835\udc3b\ud835\udc62 = \ud835\udc34\u2225 (\ud835\udc34(\ud835\udc34\n\ud835\udc47\ud835\udc34))\n\ud835\udc3b\ud835\udc56 = \ud835\udc34\n\ud835\udc47\n\u2225 (\ud835\udc34\n\ud835\udc47\n(\ud835\udc34\ud835\udc34\ud835\udc47))\n, (160)\nwhich depicts the dual hypergraphs for users and items.\nHCCF proposes to construct a learnable hypergraph to depict the global dependencies between\nnodes on the interaction graph. To be specific, the hyperstructure is factorized with two low-rank\nembedding matrices to achieve model efficiency:\n\ud835\udc3b\ud835\udc62 = \ud835\udc38\ud835\udc62 \u00b7\ud835\udc4a\ud835\udc62, \ud835\udc3b\ud835\udc63 = \ud835\udc38\ud835\udc63 \u00b7\ud835\udc4a\ud835\udc63 . (161)\n13.4 Other Graphs\nSince there are a variety of recommendation scenarios, several tailored designed graph structures\nhave been proposed accordingly, to better exploit the domain information from different scenarios.\nFor instance, CKE [564] and MKR [462] introduce Knowledge graphs to enhance graph recommen\u0002dation. GSTN [484], KBGNN [219], DisenPOI [373] and Diff-POI [374] propose to build geographical\ngraphs based on the distance between Point-of-Interests (POIs) to better model the locality of users\u2019\nvisiting patterns. TGSRec [109] and DisenCTR [475] empower the user-item interaction graphs with\ntemporal sampling between layers to obtain sequential information from static bipartite graphs.\n13.5 Summary\nThis section introduces the application of different kinds of graph neural networks in recommender\nsystems and can be summarized as follows:\n\u2022 Graph Constructions. There are multiple options for constructing graph-structured data\nfor a variety of recommendation tasks. For instance, the user-item bipartite graphs reveal\nthe high-order collaborative similarity between users and items, and the transition graph\nis suitable for encoding sequential information in clicking history. These diversified graph\nstructures provide different views for node representation learning on users and items, and\ncan be further used for downstream ranking tasks.\n\u2022 Challenges and Limitations. Though the superiority of graph-structured data and GNNs\nagainst traditional methods has been widely illustrated, there are still challenges unsolved.\nFor example, the computational cost of graph methods is normally expensive and thus\nunacceptable in real-world applications. The data sparsity and cold-started issue in graph\nrecommendation remains to be explored as well.\n\u2022 Future Works. In the future, an efficient solution for applying GNNs in recommendation\ntasks is expected. There are also some attempts [109, 372, 475] on incorporating temporal\ninformation in graph representation learning for sequential recommendation tasks.\n14 Traffic Analysis\nIntelligent Transportation Systems (ITS) are essential for safe, reliable, and efficient transportation\nin smart cities, serving the daily commuting and traveling needs of millions of people. To support\nITS, advanced modeling and analysis techniques are necessary, and Graph Neural Networks (GNNs)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n62 W. Ju, et al.\nare a promising tool for traffic analysis. GNNs can effectively model spatial correlations, making\nthem well-suited for analyzing complex transportation networks. As such, GNNs have garnered\nsignificant interest in the traffic domain for their ability to provide insights into traffic patterns and\nbehaviors [260].\nIn this section, we first conclude the main GNN research directions in the traffic domain, and\nthen we summarize the typical graph construction processes in different traffic scenes and datasets.\nFinally, we list the classical GNN workflows for dealing with tasks in traffic networks. A summary\nis provided in Table 13.\n14.1 Research Directions in Traffic Domain\nWe summarize main GNN research directions in the traffic domain as follows,\n\u2022 Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role in ITS [90, 381],\nwhich involves leveraging spatial-temporal data collected by various sensors to gain insights\ninto future traffic patterns and behaviors. Classic methods, like autoregressive integrated\nmoving average (ARIMA) [36], support vector machine (SVM) [171] and recurrent neural\nnetworks (RNN) [76] can only model time series separately without considering their spatial\nconnections. To address this issue, graph neural networks (GNNs) have emerged as a powerful\napproach for traffic forecasting due to their strong ability of modeling complex graph\u0002structured correlations [40, 202, 277, 353, 383, 506, 592].\n\u2022 Trajectory Prediction. Trajectory prediction is a crucial task in various applications, such\nas autonomous driving and traffic surveillance, which aims to forecast future positions of\nagents in the traffic scene. However, accurately predicting trajectories can be challenging, as\nthe behavior of an agent is influenced not only by its own motion but also by interactions\nwith surrounding objects. To address this challenge, Graph Neural Networks (GNNs) have\nemerged as a promising tool for modeling complex interactions in trajectory prediction\n[44, 345, 432, 600]. By representing the scene as a graph, where each node corresponds to an\nagent and the edges capture interactions between them, GNNs can effectively capture spatial\ndependencies and interactions between agents. This makes GNNs well-suited for predicting\ntrajectories that accurately capture the behavior of agents in complex traffic scenes.\n\u2022 Traffic Anomaly Detection. Anomaly detection is an essential support for ITS. There are\nlots of traffic anomalies in daily transportation systems, for example, traffic accidents, extreme\nweather and unexpected situations. Handling these traffic anomalies timely can improve the\nservice quality of public transportation. The main trouble of traffic anomaly detection is the\nhighly twisted spatial-temporal characteristics of traffic data. The criteria and influence of\ntraffic anomaly vary among locations and times. GNNs have been introduced and achieved\nsuccess in this domain [66, 85, 86, 565].\n\u2022 Others. Traffic demand prediction targets at estimating the future number of traveling at\nsome location. It is of vital and practical significance in the resource scheduling for ITS. By\nusing GNNs, the spatial dependencies of demands can be revealed [530, 535]. What is more,\nurban vehicle emission analysis is also considered in recent work, which is closely related to\nenvironmental protection and gains increasing researcher attention [521].\n14.2 Traffic Graph Construction\n14.2.1 Traffic Graph The traffic network is represented as a graph G = (\ud835\udc49 , \ud835\udc38, \ud835\udc34), where \ud835\udc49 is the\nset of \ud835\udc41 traffic nodes, \ud835\udc38 is the set of edges, and \ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is an adjacency matrix representing the\nconnectivity of \ud835\udc41 nodes. In the traffic domain, \ud835\udc49 usually represents a set of physical nodes, like\ntraffic stations or traffic sensors. The features of nodes typically depend on the specific task. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 63\nTable 13. Summary of graph models for traffic analysis.\nModels Tasks Adjcency matrices GNN types Temporal modules\nSTGCN[545] Traffic Flow Forecasting Fixed Matrix GCN TCN\nDCRNN[275] Traffic Flow Forecasting Fixed Matrix ChebNet RNN\nAGCRN [19] Traffic Flow Forecasting Dynamic Matrix GCN GRU\nASTGCN [159] Traffic Flow Forecasting Fixed Matrix GAT Attention&TCN\nGraphWaveNet [500] Traffic Flow Forecasting Dynamic Matrix GCN Gated-TCN\nSTSGCN [422] Traffic Flow Forecasting Dynamic Matrix GCN Cropping\nLSGCN [187] Traffic Flow Forecasting Fixed Matrix GAT GLU\nGAC-Net [424] Traffic Flow Forecasting Fixed Matrix GAT Gated-TCN\nSTGODE [112] Traffic Flow Forecasting Fixed Matrix Graph ODE TCN\nSTG-NCDE [70] Traffic Flow Forecasting Dynamic Matrix GCN NCDE\nDDGCRN [488] Traffic Flow Forecasting Dynamic Matrix GAT RNN\nMS-ASTN [467] OD Flow Forecasting OD Matrix GCN LSTM\nSocial-STGCNN [345] Trajectory Prediction Fixed Matrix GCN TXP-CNN\nRSBG [432] Trajectory Prediction Dynamic Matrix GCN LSTM\nATG [555] Trajectory Prediction Fixed Matrix GODE NODE\nSTGAN [86] Anomaly Detection Fixed Matrix GCN GRU\nDMVST-VGNN [206] Traffic Demand Prediction Fixed Matrix GAT GLU\nDST-GNN [185] Traffic Demand Prediction Dynamic Matrix GCN Transformer\nTC-SGC [355] Traffic Speed Prediction Fixed Matrix GCN GRU\ntraffic flow forecasting as an example. The features are the traffic flows, i.e., the historical time\nseries of nodes. The traffic flow can be represented as a flow matrix \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc47\n, where \ud835\udc41 is the\nnumber of traffic nodes and \ud835\udc47 is the length of historical series, and \ud835\udc4b\ud835\udc5b\ud835\udc61 denotes the traffic flow of\nnode \ud835\udc5b at time \ud835\udc61. The goal of traffic flow forecasting is to learn a mapping function \ud835\udc53 to predict the\ntraffic flow during future \ud835\udc47\n\u2032\nsteps given the historical \ud835\udc47 step information, which can be formulated\nas follows:\n\u0002\n\ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +1, \ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61; G\n\u0003 \ud835\udc53\n\u2212\u2192 \u0002\ud835\udc4b:,\ud835\udc61+1, \ud835\udc4b:,\ud835\udc61+2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61+\ud835\udc47\n\u2032\n\u0003\n. (162)\n14.2.2 Graph Construction Constructing a graph to describe the interactions among traffic nodes,\ni.e., the design of the adjacency matrix \ud835\udc34, is the key part of traffic analysis. The mainstream designs\ncan be divided into two categories, fixed matrix and dynamic matrix.\nFixed matrix. Lots of works assume that the correlations among traffic nodes are fixed and\nconstant over time, and they design a fixed and pre-defined adjacency matrix to capture the spatial\ncorrelation. Here we list several common choices of fixed adjacency matrix.\nThe connectivity matrix is the most natural construction way. It relies on the support of\nroad map data. The element of the connectivity matrix is defined as 1 if two nodes are physically\nconnected and 0 otherwise. This binary format is convenient to construct and easy to interpret.\nThe distance-based matrix is also a common choice, which shows the connection between two\nnodes more precisely. The elements of the matrix are defined as the function of distance between\ntwo nodes (driving distance or geographical distance). A typical way is to use the threshold Gaussian\nfunction as follows,\n\ud835\udc34\ud835\udc56\ud835\udc57 =\n(\nexp(\u2212\ud835\udc51\n2\n\ud835\udc56 \ud835\udc57\n\ud835\udf0e\n2 ), \ud835\udc51\ud835\udc56\ud835\udc57  \ud835\udf16\n, (163)\nwhere \ud835\udc51\ud835\udc56\ud835\udc57 is the distance between node \ud835\udc56 and \ud835\udc57, and \ud835\udf0e and \ud835\udf16 are two hyperparameters to control the\ndistribution and the sparsity of the matrix.\nAnother kind of fixed adjacency matrix is the similarity-based matrix. In fact, a similarity\nmatrix is not an adjacency matrix to some extent. It is constructed according to the similarity of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n64 W. Ju, et al.\ntwo nodes, which means the neighbors in the similarity graph may be far away in the real world.\nThere are various similarity metrics. For example, many works measure the similarity of two nodes\nby their functionality, e.g., the distribution of surrounding points of interest (POIs). The assumption\nbehind this is that nodes that share similar functionality may share similar traffic patterns. We\ncan also define the similarity through the historical flow patterns. To compute the similarity of\ntwo-time series, a common practice is to use Dynamic Time Wrapping (DTW) algorithm [350],\nwhich is superior to other metrics due to its sensitivity to shape similarity rather than point-wise\nsimilarity. Specifically, given two time series \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc5b) and \ud835\udc4c = (\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc5b), DTW is\na dynamic programming algorithm defined as\n\ud835\udc37(\ud835\udc56, \ud835\udc57) = \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) + min (\ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57), \ud835\udc37(\ud835\udc56, \ud835\udc57 \u2212 1), \ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57 \u2212 1)) , (164)\nwhere \ud835\udc37(\ud835\udc56, \ud835\udc57) represents the shortest distance between subseries \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc56) and \ud835\udc4c =\n(\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc57), and\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) is some distance metric like absolute distance. As a result,\ud835\udc37\ud835\udc47\ud835\udc4a (\ud835\udc4b, \ud835\udc4c) =\n\ud835\udc37(\ud835\udc5b, \ud835\udc5b) is set as the final distance between \ud835\udc4b and \ud835\udc4c, which better reflects the similarity of the\ntwo-time series compared to the Euclidean distance.\nDynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect complete\ninformation of spatial correlations. The dynamic adaptive matrix is proposed to solve the issue.\nThe dynamic matrix is learned from input data automatically. To achieve the best prediction\nperformance, the dynamic matrix will manage to infer the hidden correlations among nodes, more\nthan those physical connections.\nA typical practice is learning adjacency matrix from node embeddings [19]. Let \ud835\udc38\ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc51 be a\nlearnable node embedding dictionary, where each row of \ud835\udc38\ud835\udc34 represents the embedding of a node,\n\ud835\udc41 and \ud835\udc51 denote the number of nodes and the dimension of embeddings respectively. The graph\nadjacency matrix is defined as the similarities among node embeddings,\n\ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 = \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \u0010\n\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc38\ud835\udc34 \u00b7 \ud835\udc38\n\ud835\udc47\n\ud835\udc34\n)\n\u0011\n, (165)\nwhere \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 function is to perform row-normalization, and \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 is the Laplacian matrix.\n14.3 Typical GNN Frameworks in Traffic Domain\nSpatial Temporal Graph Convolution Network (STGCN) [545]. STGCN is a pioneering work in the\nspatial-temporal GNN domain. It utilizes graph convolution to capture spatial features, and deploys\na gated causal convolution to extract temporal patterns. Specifically, the graph convolution and\ntemporal convolution are defined as follows,\n\u0398 \u2217G \ud835\udc65 = \ud835\udf03 (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc65 = \ud835\udf03 (\ud835\udc37\u02dc \u2212\n1\n2\ud835\udc34\u02dc\ud835\udc37\u02dc \u2212\n1\n2 )\ud835\udc65, (166)\n\u0393 \u2217T \ud835\udc66 = \ud835\udc43 \u2299 \ud835\udf0e(\ud835\udc44), (167)\nwhere \u0398 is the parameter of graph convolution, \ud835\udc43 and \ud835\udc44 are the outputs of a 1-d convolution\nalong the temporal dimension. The sigmoid gate \ud835\udf0e(\ud835\udc44) controls how the states of \ud835\udc43 are relevant\nfor discovering hidden temporal patterns. In order to fuse features from both spatial and temporal\ndimension, the spatial convolution layer and the temporal convolution layer are combined to\nconstruct a spatial temporal block to jointly deal with graph-structured time series, and more blocks\ncan be stacked to achieve a more scalable and complex model.\nDiffusion Convolutional Recurrent Neural Network (DCRNN) [275]. DCRNN is a representative\nsolution combining graph convolution networks with recurrent neural networks. It captures spatial\ndependencies by bidirectional random walks on the graph. The diffusion convolution operation on\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 65\na graph is defined as:\n\ud835\udc4b \u2217G \ud835\udc53\ud835\udf03 =\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc58=0\n\u0010\n\ud835\udf03\ud835\udc58,1 (\ud835\udc37\n\u22121\n\ud835\udc42 \ud835\udc34)\n\ud835\udc58 + \ud835\udf03\ud835\udc58,2 (\ud835\udc37\u22121\n\ud835\udc3c \ud835\udc34)\n\ud835\udc58\n\u0011\n\ud835\udc4b, (168)\nwhere \ud835\udf03 are parameters for the convolution filter, and \ud835\udc37\n\u22121\n\ud835\udc42\n\ud835\udc34, \ud835\udc37\u22121\n\ud835\udc3c\n\ud835\udc34 represent the bidirectional\ndiffusion processes respectively. In term of temporal dependency, DCRNN utilizes Gated Recurrent\nUnits (GRU), and replace the linear transformation in the GRU with the diffusion convolution as\nfollows,\n\ud835\udc5f\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc5f \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc5f), (169)\n\ud835\udc62\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc62 \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc62), (170)\n\ud835\udc36\n(\ud835\udc61) = tanh(\u0398\ud835\udc36 \u2217G [\ud835\udc4b(\ud835\udc61)\n, (\ud835\udc5f\n(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121)\n] + \ud835\udc4f\ud835\udc50 ), (171)\n\ud835\udc3b\n(\ud835\udc61) = \ud835\udc62(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121) + (1 \u2212 \ud835\udc62(\ud835\udc61)\n) \u2299 \ud835\udc36\n(\ud835\udc61)\n, (172)\nwhere \ud835\udc4b\n(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61) denote the input and output at time \ud835\udc61, \ud835\udc5f\n(\ud835\udc61)\n, \ud835\udc62(\ud835\udc61)are the reset and update gates\nrespectively, and \u0398\ud835\udc5f, \u0398\ud835\udc62, \u0398\ud835\udc36 are parameters of convolution filters. Moreover, DCRNN employs a\nsequence-to-sequence architecture to predict future series. Both the encoder and the decoder are\nconstructed with diffusion convolutional recurrent layers. The historical time series are fed into\nthe encoder and the predictions are generated by the decoder. The scheduled sampling technique is\nutilized to solve the discrepancy problem between training and test distribution.\nAdaptive Graph Convolutional Recurrent Network (AGCRN) [19]. The focuses of AGCRN are\ntwo-fold. On the one hand, it argues that the temporal patterns are diversified and thus parameter\u0002sharing for each node is inferior; on the other hand, it proposes that the pre-defined graph may be\nintuitive and incomplete for the specific prediction task. To mitigate the two issues, it designs a\nNode Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic\nseries, and a Data Adaptive Graph Generation (DAGG) module to infer the hidden correlations\namong nodes from data and to generate the graph during training. Specifically, the NAPL module\nis defined as follows,\n\ud835\udc4d = (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc4b \ud835\udc38G\ud835\udc4aG + \ud835\udc38G\ud835\udc4f G, (173)\nwhere \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc36 is the input feature, \ud835\udc38G \u2208 R\ud835\udc41 \u00d7\ud835\udc51\nis a node embedding dictionary, \ud835\udc51 is the\nembedding dimension (\ud835\udc51",
      "openalex_id": "https://openalex.org/W4392203343",
      "title": "A Comprehensive Survey on Deep Graph Representation Learning",
      "publication_date": "2024-02-27",
      "cited_by_count": 55.0,
      "topics": "Graph Neural Network Models and Applications, Statistical Mechanics of Complex Networks, Recommender System Technologies",
      "keywords": "Feature learning, Representation Learning, Knowledge Graph Embedding, Signal Processing on Graphs, Network Embedding, Deep Learning, Graph embedding",
      "concepts": "Computer science, Deep learning, Feature learning, Artificial intelligence, Graph, Theoretical computer science, Categorization, Embedding, Graph embedding, Machine learning",
      "pdf_urls_by_priority": [
        "https://arxiv.org/pdf/2304.05055"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W103666676",
        "https://openalex.org/W1510073064",
        "https://openalex.org/W1520469672",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W1662382123",
        "https://openalex.org/W1816257748",
        "https://openalex.org/W1888005072",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W1954735160",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W1991252559",
        "https://openalex.org/W1992787746",
        "https://openalex.org/W1993046136",
        "https://openalex.org/W1994389483",
        "https://openalex.org/W1996058270",
        "https://openalex.org/W2006698588",
        "https://openalex.org/W2008056655",
        "https://openalex.org/W2009233867",
        "https://openalex.org/W2022638422",
        "https://openalex.org/W2027482274",
        "https://openalex.org/W2042123098",
        "https://openalex.org/W2053186076",
        "https://openalex.org/W2054141820",
        "https://openalex.org/W2056609785",
        "https://openalex.org/W2062340319",
        "https://openalex.org/W2076498053",
        "https://openalex.org/W2090891622",
        "https://openalex.org/W2095932468",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W2101491865",
        "https://openalex.org/W2110242546",
        "https://openalex.org/W2113052721",
        "https://openalex.org/W2114704115",
        "https://openalex.org/W2115627867",
        "https://openalex.org/W2116341502",
        "https://openalex.org/W2121406124",
        "https://openalex.org/W2128332575",
        "https://openalex.org/W2140310134",
        "https://openalex.org/W2142498761",
        "https://openalex.org/W2142535891",
        "https://openalex.org/W2145658888",
        "https://openalex.org/W2148950790",
        "https://openalex.org/W2152184085",
        "https://openalex.org/W2152630148",
        "https://openalex.org/W2152825437",
        "https://openalex.org/W2153959628",
        "https://openalex.org/W2156718197",
        "https://openalex.org/W2158787690",
        "https://openalex.org/W2170057991",
        "https://openalex.org/W2184148260",
        "https://openalex.org/W2262123273",
        "https://openalex.org/W2319902168",
        "https://openalex.org/W2338678442",
        "https://openalex.org/W2387462954",
        "https://openalex.org/W2393319904",
        "https://openalex.org/W2415243320",
        "https://openalex.org/W2461620095",
        "https://openalex.org/W2481151430",
        "https://openalex.org/W2488133945",
        "https://openalex.org/W2493343568",
        "https://openalex.org/W2509893387",
        "https://openalex.org/W2520633135",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W2550925836",
        "https://openalex.org/W2551706664",
        "https://openalex.org/W2565684601",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2594899909",
        "https://openalex.org/W2602753196",
        "https://openalex.org/W2604738573",
        "https://openalex.org/W2606202972",
        "https://openalex.org/W2612872092",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2624407581",
        "https://openalex.org/W2700550412",
        "https://openalex.org/W2735246657",
        "https://openalex.org/W2743104969",
        "https://openalex.org/W2743930630",
        "https://openalex.org/W2749279690",
        "https://openalex.org/W2754490690",
        "https://openalex.org/W2767094836",
        "https://openalex.org/W2767404761",
        "https://openalex.org/W2772486182",
        "https://openalex.org/W2773515559",
        "https://openalex.org/W2788134583",
        "https://openalex.org/W2788775653",
        "https://openalex.org/W2788919350",
        "https://openalex.org/W2793544332",
        "https://openalex.org/W2798621783",
        "https://openalex.org/W2800415562",
        "https://openalex.org/W2803526748",
        "https://openalex.org/W2809279178",
        "https://openalex.org/W2809307135",
        "https://openalex.org/W2809435178",
        "https://openalex.org/W2884209963",
        "https://openalex.org/W2888164077",
        "https://openalex.org/W2888192920",
        "https://openalex.org/W2889337896",
        "https://openalex.org/W2892880750",
        "https://openalex.org/W2895744665",
        "https://openalex.org/W2895884529",
        "https://openalex.org/W2896202861",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2897862648",
        "https://openalex.org/W2897978524",
        "https://openalex.org/W2901454299",
        "https://openalex.org/W2903871660",
        "https://openalex.org/W2905432015",
        "https://openalex.org/W2907230994",
        "https://openalex.org/W2907492528",
        "https://openalex.org/W2911286998",
        "https://openalex.org/W2912323206",
        "https://openalex.org/W2912351665",
        "https://openalex.org/W2913015533",
        "https://openalex.org/W2913350752",
        "https://openalex.org/W2913825337",
        "https://openalex.org/W2914989158",
        "https://openalex.org/W2916446912",
        "https://openalex.org/W2944538680",
        "https://openalex.org/W2948684689",
        "https://openalex.org/W2948729509",
        "https://openalex.org/W2949103145",
        "https://openalex.org/W2949208225",
        "https://openalex.org/W2949243165",
        "https://openalex.org/W2949865801",
        "https://openalex.org/W2950898568",
        "https://openalex.org/W2951659295",
        "https://openalex.org/W2951970475",
        "https://openalex.org/W2959300817",
        "https://openalex.org/W2962756421",
        "https://openalex.org/W2962810718",
        "https://openalex.org/W2963017945",
        "https://openalex.org/W2963066159",
        "https://openalex.org/W2963084622",
        "https://openalex.org/W2963224980",
        "https://openalex.org/W2963241951",
        "https://openalex.org/W2963341924",
        "https://openalex.org/W2963358464",
        "https://openalex.org/W2963410212",
        "https://openalex.org/W2963456618",
        "https://openalex.org/W2963521729",
        "https://openalex.org/W2963639956",
        "https://openalex.org/W2963664410",
        "https://openalex.org/W2963703618",
        "https://openalex.org/W2963726920",
        "https://openalex.org/W2963919031",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2964044287",
        "https://openalex.org/W2964051675",
        "https://openalex.org/W2964113829",
        "https://openalex.org/W2964321699",
        "https://openalex.org/W2964568038",
        "https://openalex.org/W2964583308",
        "https://openalex.org/W2964926209",
        "https://openalex.org/W2965341826",
        "https://openalex.org/W2965857891",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W2966683369",
        "https://openalex.org/W2966841471",
        "https://openalex.org/W2971126534",
        "https://openalex.org/W2971220558",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2979845147",
        "https://openalex.org/W2981536126",
        "https://openalex.org/W2981790137",
        "https://openalex.org/W2982108874",
        "https://openalex.org/W2982880755",
        "https://openalex.org/W2986423110",
        "https://openalex.org/W2986466936",
        "https://openalex.org/W2986515219",
        "https://openalex.org/W2988115728",
        "https://openalex.org/W2989285747",
        "https://openalex.org/W2992586577",
        "https://openalex.org/W2992613109",
        "https://openalex.org/W2994860160",
        "https://openalex.org/W2996635575",
        "https://openalex.org/W2996847713",
        "https://openalex.org/W2996910665",
        "https://openalex.org/W2997128522",
        "https://openalex.org/W2997574889",
        "https://openalex.org/W2997785591",
        "https://openalex.org/W2997997679",
        "https://openalex.org/W2998004401",
        "https://openalex.org/W2998122931",
        "https://openalex.org/W2998496395",
        "https://openalex.org/W2998604091",
        "https://openalex.org/W2998702685",
        "https://openalex.org/W3000301417",
        "https://openalex.org/W3000478925",
        "https://openalex.org/W3000577518",
        "https://openalex.org/W3000716014",
        "https://openalex.org/W3005552578",
        "https://openalex.org/W3007488165",
        "https://openalex.org/W3008194092",
        "https://openalex.org/W3011358689",
        "https://openalex.org/W3011667710",
        "https://openalex.org/W3012123536",
        "https://openalex.org/W3012871709",
        "https://openalex.org/W3013107657",
        "https://openalex.org/W3013888836",
        "https://openalex.org/W3016124664",
        "https://openalex.org/W3016427665",
        "https://openalex.org/W3025863369",
        "https://openalex.org/W3026887460",
        "https://openalex.org/W3031353169",
        "https://openalex.org/W3033039844",
        "https://openalex.org/W3033706928",
        "https://openalex.org/W3034231628",
        "https://openalex.org/W3034329572",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W3035096461",
        "https://openalex.org/W3035237749",
        "https://openalex.org/W3035285524",
        "https://openalex.org/W3035523484",
        "https://openalex.org/W3035580605",
        "https://openalex.org/W3035649237",
        "https://openalex.org/W3035664258",
        "https://openalex.org/W3035666843",
        "https://openalex.org/W3035702572",
        "https://openalex.org/W3035740499",
        "https://openalex.org/W3036106327",
        "https://openalex.org/W3036167779",
        "https://openalex.org/W3036974265",
        "https://openalex.org/W3038719422",
        "https://openalex.org/W3038981236",
        "https://openalex.org/W3042918615",
        "https://openalex.org/W3044189835",
        "https://openalex.org/W3045200674",
        "https://openalex.org/W3045662942",
        "https://openalex.org/W3045928028",
        "https://openalex.org/W3046470859",
        "https://openalex.org/W3048817558",
        "https://openalex.org/W3080566854",
        "https://openalex.org/W3080997787",
        "https://openalex.org/W3081203761",
        "https://openalex.org/W3081325717",
        "https://openalex.org/W3081836708",
        "https://openalex.org/W3082154031",
        "https://openalex.org/W3082411326",
        "https://openalex.org/W3087318471",
        "https://openalex.org/W3092339997",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3093218977",
        "https://openalex.org/W3093687066",
        "https://openalex.org/W3094231942",
        "https://openalex.org/W3094500523",
        "https://openalex.org/W3095448863",
        "https://openalex.org/W3096831136",
        "https://openalex.org/W3098465726",
        "https://openalex.org/W3098797593",
        "https://openalex.org/W3099152386",
        "https://openalex.org/W3099414221",
        "https://openalex.org/W3100078588",
        "https://openalex.org/W3100278010",
        "https://openalex.org/W3100324210",
        "https://openalex.org/W3101707147",
        "https://openalex.org/W3102554291",
        "https://openalex.org/W3103523530",
        "https://openalex.org/W3103720336",
        "https://openalex.org/W3103736477",
        "https://openalex.org/W3104097132",
        "https://openalex.org/W3104644561",
        "https://openalex.org/W3104667978",
        "https://openalex.org/W3105259638",
        "https://openalex.org/W3105423481",
        "https://openalex.org/W3108202858",
        "https://openalex.org/W3108433857",
        "https://openalex.org/W3110901318",
        "https://openalex.org/W3111430045",
        "https://openalex.org/W3113177135",
        "https://openalex.org/W3114613321",
        "https://openalex.org/W3116239416",
        "https://openalex.org/W3117178429",
        "https://openalex.org/W3120567415",
        "https://openalex.org/W3122934853",
        "https://openalex.org/W3123909522",
        "https://openalex.org/W3124962940",
        "https://openalex.org/W3128443161",
        "https://openalex.org/W3129850062",
        "https://openalex.org/W3133780103",
        "https://openalex.org/W3134509497",
        "https://openalex.org/W3135205495",
        "https://openalex.org/W3135389928",
        "https://openalex.org/W3136999308",
        "https://openalex.org/W3137385578",
        "https://openalex.org/W3137928916",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3148711710",
        "https://openalex.org/W3151900735",
        "https://openalex.org/W3152893301",
        "https://openalex.org/W3154503084",
        "https://openalex.org/W3155056342",
        "https://openalex.org/W3155322940",
        "https://openalex.org/W3155577228",
        "https://openalex.org/W3156642753",
        "https://openalex.org/W3157039246",
        "https://openalex.org/W3157999218",
        "https://openalex.org/W3158827677",
        "https://openalex.org/W3160021293",
        "https://openalex.org/W3163426640",
        "https://openalex.org/W3164446335",
        "https://openalex.org/W3165171933",
        "https://openalex.org/W3165369424",
        "https://openalex.org/W3165924303",
        "https://openalex.org/W3167334189",
        "https://openalex.org/W3168436232",
        "https://openalex.org/W3169168872",
        "https://openalex.org/W3169450514",
        "https://openalex.org/W3169933688",
        "https://openalex.org/W3171581326",
        "https://openalex.org/W3171764584",
        "https://openalex.org/W3174163042",
        "https://openalex.org/W3174174150",
        "https://openalex.org/W3174823757",
        "https://openalex.org/W3175925542",
        "https://openalex.org/W3175971420",
        "https://openalex.org/W3176393519",
        "https://openalex.org/W3176806965",
        "https://openalex.org/W3176890989",
        "https://openalex.org/W3181414820",
        "https://openalex.org/W3184127157",
        "https://openalex.org/W3187985423",
        "https://openalex.org/W3190664711",
        "https://openalex.org/W3191962800",
        "https://openalex.org/W3192448376",
        "https://openalex.org/W3193553875",
        "https://openalex.org/W3194668998",
        "https://openalex.org/W3200806939",
        "https://openalex.org/W3201058350",
        "https://openalex.org/W3201249640",
        "https://openalex.org/W3204651332",
        "https://openalex.org/W3205227354",
        "https://openalex.org/W3206171352",
        "https://openalex.org/W3208638341",
        "https://openalex.org/W3209048663",
        "https://openalex.org/W3209056694",
        "https://openalex.org/W3209451568",
        "https://openalex.org/W3209764902",
        "https://openalex.org/W3210482950",
        "https://openalex.org/W3210611486",
        "https://openalex.org/W3210987203",
        "https://openalex.org/W3211394146",
        "https://openalex.org/W3211477647",
        "https://openalex.org/W3211973371",
        "https://openalex.org/W3213940558",
        "https://openalex.org/W3214642103",
        "https://openalex.org/W3214674106",
        "https://openalex.org/W3214872094",
        "https://openalex.org/W3215452784",
        "https://openalex.org/W4200635484",
        "https://openalex.org/W4205247958",
        "https://openalex.org/W4206174637",
        "https://openalex.org/W4206357214",
        "https://openalex.org/W4206445139",
        "https://openalex.org/W4206776774",
        "https://openalex.org/W4212805305",
        "https://openalex.org/W4213052788",
        "https://openalex.org/W4213147383",
        "https://openalex.org/W4213457653",
        "https://openalex.org/W4214868967",
        "https://openalex.org/W4220742022",
        "https://openalex.org/W4220933119",
        "https://openalex.org/W4221023051",
        "https://openalex.org/W4221138292",
        "https://openalex.org/W4221149947",
        "https://openalex.org/W4221155201",
        "https://openalex.org/W4221157965",
        "https://openalex.org/W4224309748",
        "https://openalex.org/W4224311348",
        "https://openalex.org/W4224311800",
        "https://openalex.org/W4224983022",
        "https://openalex.org/W4225090121",
        "https://openalex.org/W4225338086",
        "https://openalex.org/W4225405705",
        "https://openalex.org/W4225512856",
        "https://openalex.org/W4225596872",
        "https://openalex.org/W4225977739",
        "https://openalex.org/W4226058932",
        "https://openalex.org/W4226060238",
        "https://openalex.org/W4226208698",
        "https://openalex.org/W4229053887",
        "https://openalex.org/W4234842379",
        "https://openalex.org/W4239789016",
        "https://openalex.org/W4240185200",
        "https://openalex.org/W4240592325",
        "https://openalex.org/W4243799827",
        "https://openalex.org/W4246587917",
        "https://openalex.org/W4255866863",
        "https://openalex.org/W4280535976",
        "https://openalex.org/W4281387042",
        "https://openalex.org/W4281563651",
        "https://openalex.org/W4282913028",
        "https://openalex.org/W4282943426",
        "https://openalex.org/W4283121576",
        "https://openalex.org/W4283218438",
        "https://openalex.org/W4283462727",
        "https://openalex.org/W4283798273",
        "https://openalex.org/W4283810298",
        "https://openalex.org/W4283817628",
        "https://openalex.org/W4284666445",
        "https://openalex.org/W4284698122",
        "https://openalex.org/W4285428788",
        "https://openalex.org/W4286588524",
        "https://openalex.org/W4286795917",
        "https://openalex.org/W4286893581",
        "https://openalex.org/W4287123803",
        "https://openalex.org/W4287325738",
        "https://openalex.org/W4287780403",
        "https://openalex.org/W4287863694",
        "https://openalex.org/W4287991183",
        "https://openalex.org/W4287998109",
        "https://openalex.org/W4288052590",
        "https://openalex.org/W4288088467",
        "https://openalex.org/W4288346884",
        "https://openalex.org/W4289389616",
        "https://openalex.org/W4289533979",
        "https://openalex.org/W4289537189",
        "https://openalex.org/W4290648792",
        "https://openalex.org/W4290875097",
        "https://openalex.org/W4290877962",
        "https://openalex.org/W4293112739",
        "https://openalex.org/W4293370878",
        "https://openalex.org/W4293821372",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4294435970",
        "https://openalex.org/W4294558607",
        "https://openalex.org/W4295097398",
        "https://openalex.org/W4295728955",
        "https://openalex.org/W4295846611",
        "https://openalex.org/W4296047560",
        "https://openalex.org/W4296143727",
        "https://openalex.org/W4296185888",
        "https://openalex.org/W4297510052",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W4297791874",
        "https://openalex.org/W4297946153",
        "https://openalex.org/W4297951436",
        "https://openalex.org/W4297999768",
        "https://openalex.org/W4298052734",
        "https://openalex.org/W4298312696",
        "https://openalex.org/W4301329292",
        "https://openalex.org/W4304097971",
        "https://openalex.org/W4306887124",
        "https://openalex.org/W4307416138",
        "https://openalex.org/W4308505492",
        "https://openalex.org/W4309635196",
        "https://openalex.org/W4309801650",
        "https://openalex.org/W4310012576",
        "https://openalex.org/W4311216457",
        "https://openalex.org/W4312126067",
        "https://openalex.org/W4312689497",
        "https://openalex.org/W4313201684",
        "https://openalex.org/W4315708854",
        "https://openalex.org/W4316495377",
        "https://openalex.org/W4317951161",
        "https://openalex.org/W4318150241",
        "https://openalex.org/W4318347779",
        "https://openalex.org/W4318540750",
        "https://openalex.org/W4318812521",
        "https://openalex.org/W4320814985",
        "https://openalex.org/W4321227311",
        "https://openalex.org/W4321367323",
        "https://openalex.org/W4321479940",
        "https://openalex.org/W4321480027",
        "https://openalex.org/W4321480031",
        "https://openalex.org/W4322614756",
        "https://openalex.org/W4322824839",
        "https://openalex.org/W4323650483",
        "https://openalex.org/W4327525152",
        "https://openalex.org/W4361247736",
        "https://openalex.org/W4362682267",
        "https://openalex.org/W4362714312",
        "https://openalex.org/W4366001157",
        "https://openalex.org/W4366198975",
        "https://openalex.org/W4366975604",
        "https://openalex.org/W4366986925",
        "https://openalex.org/W4367047082",
        "https://openalex.org/W4367047244",
        "https://openalex.org/W4367047306",
        "https://openalex.org/W4367060955",
        "https://openalex.org/W4367595602",
        "https://openalex.org/W4376121360",
        "https://openalex.org/W4378696994",
        "https://openalex.org/W4378766933",
        "https://openalex.org/W4378909195",
        "https://openalex.org/W4378976562",
        "https://openalex.org/W4379185762",
        "https://openalex.org/W4379506768",
        "https://openalex.org/W4380091056",
        "https://openalex.org/W4381573056",
        "https://openalex.org/W4381679608",
        "https://openalex.org/W4382202967",
        "https://openalex.org/W4382239955",
        "https://openalex.org/W4382240004",
        "https://openalex.org/W4382317956",
        "https://openalex.org/W4382468395",
        "https://openalex.org/W4383604692",
        "https://openalex.org/W4384895066",
        "https://openalex.org/W4384915287",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385270450",
        "https://openalex.org/W4385568380",
        "https://openalex.org/W4386445678",
        "https://openalex.org/W4386494629",
        "https://openalex.org/W4386587901",
        "https://openalex.org/W4386620158",
        "https://openalex.org/W4386711666",
        "https://openalex.org/W4386746103",
        "https://openalex.org/W4387092606",
        "https://openalex.org/W4387969028",
        "https://openalex.org/W4388185880",
        "https://openalex.org/W4388537645",
        "https://openalex.org/W4388684696",
        "https://openalex.org/W4390534655",
        "https://openalex.org/W4390572965",
        "https://openalex.org/W4391345293",
        "https://openalex.org/W4391561379",
        "https://openalex.org/W637153065"
      ],
      "openalex_rank": 2,
      "num_tokens": 42218,
      "best_oa_location_pdf_url": "https://arxiv.org/pdf/2304.05055",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key advancements in deep graph representation learning, what challenges do researchers currently face in this field, and what future directions could enhance the effectiveness of these techniques in various applications?",
      "pipeline_answer": "### Key Advancements in Deep Graph Representation Learning\n- **FairDrop Algorithm**: This innovative biased edge dropout algorithm enhances fairness in graph representation learning by addressing homophily, which can lead to biased outcomes in applications such as social networks and recommender systems [S3256907738].\n- **Adaptability**: FairDrop can be integrated into existing graph representation learning models, making it adaptable for various datasets and models while maintaining performance accuracy [S2943217069].\n- **Importance of Fairness**: The significance of fairness in graph representation learning is increasingly recognized, particularly for the correct deployment of algorithms in real-world applications like social networks and recommender systems [S6131720227].\n- **New Evaluation Metrics**: The FairDrop algorithm introduces a new dyadic group definition to measure bias in link prediction tasks, enhancing the evaluation of fairness in graph representation learning [S0874386725].\n\n### Current Challenges in the Field\n- **Bias in Outcomes**: The challenge of homophily leading to biased outcomes remains a significant concern, particularly in sensitive applications where fairness is crucial [S3256907738].\n- **Integration with Existing Models**: While new algorithms like FairDrop show promise, integrating them effectively into existing frameworks without sacrificing performance is a challenge that researchers face [S2943217069].\n\n### Future Directions for Enhancing Effectiveness\n- **Focus on Fairness**: Continued research into fairness metrics and algorithms, such as FairDrop, is essential to ensure that graph representation learning techniques are equitable and do not perpetuate biases [S6131720227].\n- **Interdisciplinary Approaches**: Exploring interdisciplinary methods could provide new insights and techniques to enhance graph representation learning, addressing both technical and ethical challenges [S3256907738].\n\n### Conclusion\nDeep graph representation learning has made significant strides with advancements like the FairDrop algorithm, which addresses fairness in representation learning. However, challenges such as bias in outcomes and integration with existing models persist. Future research should focus on enhancing fairness and exploring interdisciplinary approaches to improve these techniques in various applications.",
      "pipeline_references": {
        "S2943217069": {
          "id": "S2943217069",
          "text": "The FairDrop algorithm can be integrated into existing graph representation learning models, making it adaptable for various datasets and models while maintaining performance accuracy.",
          "children": [
            {
              "id": "E6418977503",
              "text": "PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 1 FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini Abstract\u2014Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure. Impact Statement\u2014Fairness in graph representation learning is under-explored. Yet, the algorithms working with these types of data have a fundamental impact on our digital life. Therefore, despite the law now prohibiting unfair treatment by artificial intelligence (AI) methods (such as those based on sensitive traits, social networks and recommender systems) to systematically discriminate against minorities, current solutions are computationally intensive, deliver reduced performance accuracy and/or lack interpretability. To address the fairness problem, we propose FairDrop, a novel biased edge dropout algorithm. Our approach protects against unfairness generated from the network\u2019s homophily with respect to sensitive attributes. FairDrop can be readily integrated into today\u2019s AI solutions for learning network embeddings or downstream tasks. We believe the lack of expensive computations and flexibility of our proposed fairness constraint will posit FairDrop as a future benchmark resource that will serve to highlight and address fairness challenges for the global AI research and innovation community. Index Terms\u2014Graph representation learning, graph embedding, fairness, link prediction, graph neural network. I. Spinelli, S. Scardapane, and A. Uncini are with the Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, 00184 Rome, Italy (e-mail: indro.spinelli@uniroma1.it; simone.scardapane@uniroma1.it; aurelio.uncini@uniroma1.it) A. Hussain is with the School of Computing, Edinburgh Napier University, UK (e-mail: a.hussain@napier.ac.uk). I. INTRODUCTION G RAPH structured data, ranging from friendships on social networks to physical links in energy grids, powers many algorithms governing our digital life. Social networks topologies define the stream of information we will receive, often influencing our opinion [1][2][3] [4] [5]. Bad actors, sometimes, define these topologies ad-hoc to spread false information [6]. Similarly, recommender systems [7] suggest products tailored to our own experiences and history of purchases. However, pursuing the highest accuracy as the only metric of interest has let many of these algorithms discriminate against minorities in the past [8] [9] [10], despite the law prohibiting unfair treatment based on sensitive traits such as race, religion, and gender. For this reason, the research community has started looking into the biases introduced by machine learning algorithms working over graphs [11]. One of the most common approaches to process graphs is via learning vector embeddings for the nodes (or the edges), e.g., [12]. These are a low dimensional representation of the nodes (or the edges), encoding the local topology. Downstream tasks then use these embeddings as inputs. Some examples of these tasks are node classification, community detection, and link prediction [13]. We will focus on the latter due to its widespread application in social networks and recommender systems. Alternatively, graph neural networks (GNNs) [14] [15] solve link prediction or other downstream tasks in an endto-end fashion, without prior learning of embeddings through ad-hoc procedures. The techniques developed in this paper can be applied to both scenarios. In this work, we will concentrate on the bias introduced by one of the key aspects behind the success of GNNs and node embedding procedures: homophily. Homophily is the principle that similar users interact at a higher rate than dissimilar ones. In a graph, this means that nodes with similar characteristics are more likely to be connected. In node classification, this encourages smoothness in terms of label distributions and embeddings, yielding excellent results [16]. From the fairness point of view, the homophily of sensitive attributes directly influences the prediction and introduces inequalities. In social networks, the \u201cunfair homophily\u201d of race, gender or nationality, limits the contents accessible by the users, influencing their online behaviour [1]. For example, the authors of [2] showed that users affiliated with majority political groups are exposed to new information faster and in greater quantity. Similarly, homophily can put minority groups at a disadvantage by restricting their ability to establish links with a majority group [17]. An unfair link prediction arXiv:2104.14210v2 [cs.LG] 27 Dec 2021 2 PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE magnifies this issue, known as \u201cfilter bubble\u201d, by increasing the segregation between the groups. To mitigate this issue, in this paper we propose a biased dropout strategy that forces the graph topology to reduce the homophily of sensitive attributes. At each step of training, we compute a random copy of the adjacency matrix biased towards reducing the amount of homophily in the original graph. Thus, models trained on these modified graphs undergo a fairness-biased data augmentation regime. Our approach limits the biases introduced by the unfair homophily, resulting in fairer node representations and link predictions while preserving most of the original accuracy. While we focus on homophily, we underline that unfairness can arise from additional factors that we do not consider here (e.g., unfair weights in the adjacency matrix, or node feature vectors). In these cases, FairDrop can be easily combined with any other known fairness-inducing technique. Measuring fairness in this context requires some adaptations. Most works on fairness measures focus on independent and identically distributed (i.i.d.) data. These works proposed many metrics, each one protecting against a different bias [18]. In particular, group fairness measures determine the level of equity of the algorithm predictions between groups of individuals. Link prediction requires a dyadic fairness measure that considers the influence of both sensitive attributes associated with the connection [19]. However, it is still possible to apply group fairness metrics by defining new groups for the edges. A. Contributions of the paper We propose a preprocessing technique that modifies the training data to reduce the predictability of its sensitive attributes [20]. Our algorithm introduces no overhead and can be framed as a biased data augmentation technique. A single hyperparameter regulates the intensity of the fairness constraint. This ranges from maximum fairness to an unbiased edge dropout. Therefore FairDrop can be easily adapted to the needs of different datasets and models. These characteristics make our framework extremely adaptable. Our approach can also be applied in combination with other fairness constraints. We evaluate the fairness improvement on two different tasks. Firstly, we measure the fairness imposed on the outcomes of end-to-end link prediction tasks. Secondly, we test the capability of removing the contributions of the sensitive attributes from the resulting node embeddings generated from representation learning algorithms. To measure the improvements for the link prediction we also propose a novel group-based fairness metric on dyadic level groups. We define a new dyadic group and use it in combination with the ones described in [19]. Instead, for the predictability of the sensitive attributes from the node embeddings, we measure the representation bias [21]. It is common to use node embeddings as input of downstream link prediction task. Therefore, we introduce a new metric that also considers the graph\u2019s topology. B. Outline of the paper The rest of the paper is structured as follows. Section II reviews recent works about enforcing fairness for graphstructured data and their limitations. Then, in Section III we introduce GNN models and group-based fairness metrics for i.i.d data. FairDrop, our proposed biased edge dropout, is first introduced in Section IV and then tested in Section VI. We conclude with some general remarks in Section VII. II. RELATED WORKS The literature on algorithmic bias is extensive and interdisciplinary [22]. However, most approaches study independent and identically distributed data. Just recently, with the success of GNNs, some works started to investigate fairness in graph representation learning. Some works focused on the creation of fair node embeddings [11][23] [21] that can be used as the input of a downstream task of link prediction. Others targeted directly the task of a fair link prediction [19] [24]. Some of these approaches base their foundations on adversarial learning. Compositional fairness constraints [11..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            },
            {
              "id": "E5570637906",
              "text": "..N. Lim, and A. Benson, \u201cCombining label propagation and simple models out-performs graph neural networks,\u201d in International Conference on Learning Representations, 2021. [17] F. Karimi, M. Genois, C. Wagner, P. Singer, and M. Strohmaier, \u201cHo- \u0301 mophily influences ranking of minorities in social networks,\u201d Scientific Reports, vol. 8, 07 2018. [18] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \u201cA survey on bias and fairness in machine learning,\u201d ArXiv, vol. abs/1908.09635, 2019. [19] F. Masrour, T. Wilson, H. Yan, P.-N. Tan, and A. Esfahanian, \u201cBursting the filter bubble: Fairness-aware network link prediction,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 841\u2013848, Apr. 2020. [20] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, \u201cCertifying and removing disparate impact,\u201d in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, (New York, NY, USA), p. 259\u2013268, Association for Computing Machinery, 2015. [21] M. Buyl and T. De Bie, \u201cDeBayes: a Bayesian method for debiasing network embeddings,\u201d in Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 1220\u20131229, PMLR, 13\u201318 Jul 2020. [22] A. Romei and S. Ruggieri, \u201cA multidisciplinary survey on discrimination analysis,\u201d The Knowledge Engineering Review, vol. 29, pp. 582 \u2013 638, 2013. [23] T. Rahman, B. Surma, M. Backes, and Y. Zhang, \u201cFairwalk: Towards fair graph embedding,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 3289\u20133295, International Joint Conferences on Artificial Intelligence Organization, 7 2019. [24] P. Li, Y. Wang, H. Zhao, P. Hong, and H. Liu, \u201cOn dyadic fairness: Exploring and mitigating bias in graph connections,\u201d in International Conference on Learning Representations, 2021. [25] T. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d ArXiv, vol. abs/1611.07308, 2016. [26] B. Kang, J. Lijffijt, and T. D. Bie, \u201cConditional network embeddings,\u201d in International Conference on Learning Representations, 2019. [27] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, \u201cCounterfactual fairness,\u201d in 31st Conference on Neural Information Processing Systems, pp. 1\u201318, 2017. [28] S. Chiappa, \u201cPath-specific counterfactual fairness,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 7801\u20137808, 2019. [29] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in International Conference on Learning Representations (ICLR), 2017. [30] D. Liben-Nowell and J. Kleinberg, \u201cThe link-prediction problem for social networks,\u201d vol. 58, p. 1019\u20131031, May 2007. [31] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d NIPS Workshop on Bayesian Deep Learning, 2016. [32] D. Bacciu, F. Errica, A. Micheli, and M. Podda, \u201cA gentle introduction to deep learning for graphs,\u201d Neural Networks, 2020. [33] Z. Zhang, P. Cui, and W. Zhu, \u201cDeep learning on graphs: A survey,\u201d IEEE Transactions on Knowledge and Data Engineering, 2020. [34] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA comprehensive survey on graph neural networks,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2020. [35] B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning of social representations,\u201d in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, (New York, NY, USA), p. 701\u2013710, Association for Computing Machinery, 2014. [36] W. Jin, T. Derr, H. Liu, Y. Wang, S. Wang, Z. Liu, and J. Tang, \u201cSelfsupervised learning on graphs: Deep insights and new direction,\u201d arXiv preprint arXiv:2006.10141, 2020. [37] N. A. Saxena, K. Huang, E. DeFilippis, G. Radanovic, D. C. Parkes, and Y. Liu, \u201cHow do fairness definitions fare? examining public attitudes towards algorithmic definitions of fairness,\u201d in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201919, (New York, NY, USA), p. 99\u2013106, Association for Computing Machinery, 2019. [38] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, \u201cFairness through awareness,\u201d in Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS \u201912, (New York, NY, USA), p. 214\u2013226, Association for Computing Machinery, 2012. [39] M. Hardt, E. Price, E. Price, and N. Srebro, \u201cEquality of opportunity in supervised learning,\u201d in Advances in Neural Information Processing Systems, vol. 29, Curran Associates, Inc., 2016. [40] S. Bird, M. Dud \u0301\u0131k, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach, and K. Walker, \u201cFairlearn: A toolkit for assessing and improving fairness in AI,\u201d Tech. Rep. MSR-TR-2020-32, Microsoft, May 2020. [41] D. P. Helmbold and P. M. Long, \u201cOn the inductive bias of dropout,\u201d The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3403\u20133454, 2015. [42] S. L. Warner, \u201cRandomized response: A survey technique for eliminating evasive answer bias,\u201d Journal of the American Statistical Association, vol. 60, no. 309, pp. 63\u201369, 1965. PMID: 12261830. [43] Y. Rong, W. Huang, T. Xu, and J. Huang, \u201cDropedge: Towards deep graph convolutional networks on node classification,\u201d in International Conference on Learning Representations, 2020. [44] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney, \u201cOptimized pre-processing for discrimination prevention,\u201d in Advances in Neural Information Processing Systems (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017. [45] P. Velickovi \u02c7 c, G. Cucurull, A. Casanova, A. Romero, P. Li \u0301 o, and ` Y. Bengio, \u201cGraph Attention Networks,\u201d International Conference on Learning Representations, 2018. accepted as poster. [46] I. Spinelli, S. Scardapane, and A. Uncini, \u201cAdaptive propagation graph convolutional network,\u201d IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u20136, 2020. [47] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, \u201cLearning fair representations,\u201d in Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, p. III\u2013325\u2013III\u2013333, JMLR.org, 2013. [48] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d International Conference on Learning Representations, 12 2014. [49] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \u201cArnetminer: Extraction and mining of academic social networks,\u201d in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, (New York, NY, USA), p. 990\u2013998, Association for Computing Machinery, 2008. [50] J. Leskovec and J. Mcauley, \u201cLearning to discover social circles in ego networks,\u201d in Advances in Neural Information Processing Systems (F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.), vol. 25, Curran Associates, Inc., 2012. [51] D. Chen, Y. Lin, W. Li, P..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            }
          ]
        },
        "S6131720227": {
          "id": "S6131720227",
          "text": "Fairness in graph representation learning is increasingly recognized as crucial for the correct deployment of algorithms in real-world applications, particularly in social networks and recommender systems.",
          "children": [
            {
              "id": "E6418977503",
              "text": "PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 1 FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini Abstract\u2014Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure. Impact Statement\u2014Fairness in graph representation learning is under-explored. Yet, the algorithms working with these types of data have a fundamental impact on our digital life. Therefore, despite the law now prohibiting unfair treatment by artificial intelligence (AI) methods (such as those based on sensitive traits, social networks and recommender systems) to systematically discriminate against minorities, current solutions are computationally intensive, deliver reduced performance accuracy and/or lack interpretability. To address the fairness problem, we propose FairDrop, a novel biased edge dropout algorithm. Our approach protects against unfairness generated from the network\u2019s homophily with respect to sensitive attributes. FairDrop can be readily integrated into today\u2019s AI solutions for learning network embeddings or downstream tasks. We believe the lack of expensive computations and flexibility of our proposed fairness constraint will posit FairDrop as a future benchmark resource that will serve to highlight and address fairness challenges for the global AI research and innovation community. Index Terms\u2014Graph representation learning, graph embedding, fairness, link prediction, graph neural network. I. Spinelli, S. Scardapane, and A. Uncini are with the Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, 00184 Rome, Italy (e-mail: indro.spinelli@uniroma1.it; simone.scardapane@uniroma1.it; aurelio.uncini@uniroma1.it) A. Hussain is with the School of Computing, Edinburgh Napier University, UK (e-mail: a.hussain@napier.ac.uk). I. INTRODUCTION G RAPH structured data, ranging from friendships on social networks to physical links in energy grids, powers many algorithms governing our digital life. Social networks topologies define the stream of information we will receive, often influencing our opinion [1][2][3] [4] [5]. Bad actors, sometimes, define these topologies ad-hoc to spread false information [6]. Similarly, recommender systems [7] suggest products tailored to our own experiences and history of purchases. However, pursuing the highest accuracy as the only metric of interest has let many of these algorithms discriminate against minorities in the past [8] [9] [10], despite the law prohibiting unfair treatment based on sensitive traits such as race, religion, and gender. For this reason, the research community has started looking into the biases introduced by machine learning algorithms working over graphs [11]. One of the most common approaches to process graphs is via learning vector embeddings for the nodes (or the edges), e.g., [12]. These are a low dimensional representation of the nodes (or the edges), encoding the local topology. Downstream tasks then use these embeddings as inputs. Some examples of these tasks are node classification, community detection, and link prediction [13]. We will focus on the latter due to its widespread application in social networks and recommender systems. Alternatively, graph neural networks (GNNs) [14] [15] solve link prediction or other downstream tasks in an endto-end fashion, without prior learning of embeddings through ad-hoc procedures. The techniques developed in this paper can be applied to both scenarios. In this work, we will concentrate on the bias introduced by one of the key aspects behind the success of GNNs and node embedding procedures: homophily. Homophily is the principle that similar users interact at a higher rate than dissimilar ones. In a graph, this means that nodes with similar characteristics are more likely to be connected. In node classification, this encourages smoothness in terms of label distributions and embeddings, yielding excellent results [16]. From the fairness point of view, the homophily of sensitive attributes directly influences the prediction and introduces inequalities. In social networks, the \u201cunfair homophily\u201d of race, gender or nationality, limits the contents accessible by the users, influencing their online behaviour [1]. For example, the authors of [2] showed that users affiliated with majority political groups are exposed to new information faster and in greater quantity. Similarly, homophily can put minority groups at a disadvantage by restricting their ability to establish links with a majority group [17]. An unfair link prediction arXiv:2104.14210v2 [cs.LG] 27 Dec 2021 2 PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE magnifies this issue, known as \u201cfilter bubble\u201d, by increasing the segregation between the groups. To mitigate this issue, in this paper we propose a biased dropout strategy that forces the graph topology to reduce the homophily of sensitive attributes. At each step of training, we compute a random copy of the adjacency matrix biased towards reducing the amount of homophily in the original graph. Thus, models trained on these modified graphs undergo a fairness-biased data augmentation regime. Our approach limits the biases introduced by the unfair homophily, resulting in fairer node representations and link predictions while preserving most of the original accuracy. While we focus on homophily, we underline that unfairness can arise from additional factors that we do not consider here (e.g., unfair weights in the adjacency matrix, or node feature vectors). In these cases, FairDrop can be easily combined with any other known fairness-inducing technique. Measuring fairness in this context requires some adaptations. Most works on fairness measures focus on independent and identically distributed (i.i.d.) data. These works proposed many metrics, each one protecting against a different bias [18]. In particular, group fairness measures determine the level of equity of the algorithm predictions between groups of individuals. Link prediction requires a dyadic fairness measure that considers the influence of both sensitive attributes associated with the connection [19]. However, it is still possible to apply group fairness metrics by defining new groups for the edges. A. Contributions of the paper We propose a preprocessing technique that modifies the training data to reduce the predictability of its sensitive attributes [20]. Our algorithm introduces no overhead and can be framed as a biased data augmentation technique. A single hyperparameter regulates the intensity of the fairness constraint. This ranges from maximum fairness to an unbiased edge dropout. Therefore FairDrop can be easily adapted to the needs of different datasets and models. These characteristics make our framework extremely adaptable. Our approach can also be applied in combination with other fairness constraints. We evaluate the fairness improvement on two different tasks. Firstly, we measure the fairness imposed on the outcomes of end-to-end link prediction tasks. Secondly, we test the capability of removing the contributions of the sensitive attributes from the resulting node embeddings generated from representation learning algorithms. To measure the improvements for the link prediction we also propose a novel group-based fairness metric on dyadic level groups. We define a new dyadic group and use it in combination with the ones described in [19]. Instead, for the predictability of the sensitive attributes from the node embeddings, we measure the representation bias [21]. It is common to use node embeddings as input of downstream link prediction task. Therefore, we introduce a new metric that also considers the graph\u2019s topology. B. Outline of the paper The rest of the paper is structured as follows. Section II reviews recent works about enforcing fairness for graphstructured data and their limitations. Then, in Section III we introduce GNN models and group-based fairness metrics for i.i.d data. FairDrop, our proposed biased edge dropout, is first introduced in Section IV and then tested in Section VI. We conclude with some general remarks in Section VII. II. RELATED WORKS The literature on algorithmic bias is extensive and interdisciplinary [22]. However, most approaches study independent and identically distributed data. Just recently, with the success of GNNs, some works started to investigate fairness in graph representation learning. Some works focused on the creation of fair node embeddings [11][23] [21] that can be used as the input of a downstream task of link prediction. Others targeted directly the task of a fair link prediction [19] [24]. Some of these approaches base their foundations on adversarial learning. Compositional fairness constraints [11..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            },
            {
              "id": "E5570637906",
              "text": "..N. Lim, and A. Benson, \u201cCombining label propagation and simple models out-performs graph neural networks,\u201d in International Conference on Learning Representations, 2021. [17] F. Karimi, M. Genois, C. Wagner, P. Singer, and M. Strohmaier, \u201cHo- \u0301 mophily influences ranking of minorities in social networks,\u201d Scientific Reports, vol. 8, 07 2018. [18] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \u201cA survey on bias and fairness in machine learning,\u201d ArXiv, vol. abs/1908.09635, 2019. [19] F. Masrour, T. Wilson, H. Yan, P.-N. Tan, and A. Esfahanian, \u201cBursting the filter bubble: Fairness-aware network link prediction,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 841\u2013848, Apr. 2020. [20] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, \u201cCertifying and removing disparate impact,\u201d in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, (New York, NY, USA), p. 259\u2013268, Association for Computing Machinery, 2015. [21] M. Buyl and T. De Bie, \u201cDeBayes: a Bayesian method for debiasing network embeddings,\u201d in Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 1220\u20131229, PMLR, 13\u201318 Jul 2020. [22] A. Romei and S. Ruggieri, \u201cA multidisciplinary survey on discrimination analysis,\u201d The Knowledge Engineering Review, vol. 29, pp. 582 \u2013 638, 2013. [23] T. Rahman, B. Surma, M. Backes, and Y. Zhang, \u201cFairwalk: Towards fair graph embedding,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 3289\u20133295, International Joint Conferences on Artificial Intelligence Organization, 7 2019. [24] P. Li, Y. Wang, H. Zhao, P. Hong, and H. Liu, \u201cOn dyadic fairness: Exploring and mitigating bias in graph connections,\u201d in International Conference on Learning Representations, 2021. [25] T. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d ArXiv, vol. abs/1611.07308, 2016. [26] B. Kang, J. Lijffijt, and T. D. Bie, \u201cConditional network embeddings,\u201d in International Conference on Learning Representations, 2019. [27] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, \u201cCounterfactual fairness,\u201d in 31st Conference on Neural Information Processing Systems, pp. 1\u201318, 2017. [28] S. Chiappa, \u201cPath-specific counterfactual fairness,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 7801\u20137808, 2019. [29] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in International Conference on Learning Representations (ICLR), 2017. [30] D. Liben-Nowell and J. Kleinberg, \u201cThe link-prediction problem for social networks,\u201d vol. 58, p. 1019\u20131031, May 2007. [31] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d NIPS Workshop on Bayesian Deep Learning, 2016. [32] D. Bacciu, F. Errica, A. Micheli, and M. Podda, \u201cA gentle introduction to deep learning for graphs,\u201d Neural Networks, 2020. [33] Z. Zhang, P. Cui, and W. Zhu, \u201cDeep learning on graphs: A survey,\u201d IEEE Transactions on Knowledge and Data Engineering, 2020. [34] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA comprehensive survey on graph neural networks,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2020. [35] B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning of social representations,\u201d in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, (New York, NY, USA), p. 701\u2013710, Association for Computing Machinery, 2014. [36] W. Jin, T. Derr, H. Liu, Y. Wang, S. Wang, Z. Liu, and J. Tang, \u201cSelfsupervised learning on graphs: Deep insights and new direction,\u201d arXiv preprint arXiv:2006.10141, 2020. [37] N. A. Saxena, K. Huang, E. DeFilippis, G. Radanovic, D. C. Parkes, and Y. Liu, \u201cHow do fairness definitions fare? examining public attitudes towards algorithmic definitions of fairness,\u201d in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201919, (New York, NY, USA), p. 99\u2013106, Association for Computing Machinery, 2019. [38] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, \u201cFairness through awareness,\u201d in Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS \u201912, (New York, NY, USA), p. 214\u2013226, Association for Computing Machinery, 2012. [39] M. Hardt, E. Price, E. Price, and N. Srebro, \u201cEquality of opportunity in supervised learning,\u201d in Advances in Neural Information Processing Systems, vol. 29, Curran Associates, Inc., 2016. [40] S. Bird, M. Dud \u0301\u0131k, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach, and K. Walker, \u201cFairlearn: A toolkit for assessing and improving fairness in AI,\u201d Tech. Rep. MSR-TR-2020-32, Microsoft, May 2020. [41] D. P. Helmbold and P. M. Long, \u201cOn the inductive bias of dropout,\u201d The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3403\u20133454, 2015. [42] S. L. Warner, \u201cRandomized response: A survey technique for eliminating evasive answer bias,\u201d Journal of the American Statistical Association, vol. 60, no. 309, pp. 63\u201369, 1965. PMID: 12261830. [43] Y. Rong, W. Huang, T. Xu, and J. Huang, \u201cDropedge: Towards deep graph convolutional networks on node classification,\u201d in International Conference on Learning Representations, 2020. [44] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney, \u201cOptimized pre-processing for discrimination prevention,\u201d in Advances in Neural Information Processing Systems (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017. [45] P. Velickovi \u02c7 c, G. Cucurull, A. Casanova, A. Romero, P. Li \u0301 o, and ` Y. Bengio, \u201cGraph Attention Networks,\u201d International Conference on Learning Representations, 2018. accepted as poster. [46] I. Spinelli, S. Scardapane, and A. Uncini, \u201cAdaptive propagation graph convolutional network,\u201d IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u20136, 2020. [47] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, \u201cLearning fair representations,\u201d in Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, p. III\u2013325\u2013III\u2013333, JMLR.org, 2013. [48] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d International Conference on Learning Representations, 12 2014. [49] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \u201cArnetminer: Extraction and mining of academic social networks,\u201d in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, (New York, NY, USA), p. 990\u2013998, Association for Computing Machinery, 2008. [50] J. Leskovec and J. Mcauley, \u201cLearning to discover social circles in ego networks,\u201d in Advances in Neural Information Processing Systems (F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.), vol. 25, Curran Associates, Inc., 2012. [51] D. Chen, Y. Lin, W. Li, P..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            }
          ]
        },
        "S3256907738": {
          "id": "S3256907738",
          "text": "FairDrop, a biased edge dropout algorithm, enhances fairness in graph representation learning by addressing the issue of homophily, which can lead to biased outcomes in applications such as social networks and recommender systems.",
          "children": [
            {
              "id": "E6418977503",
              "text": "PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 1 FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini Abstract\u2014Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure. Impact Statement\u2014Fairness in graph representation learning is under-explored. Yet, the algorithms working with these types of data have a fundamental impact on our digital life. Therefore, despite the law now prohibiting unfair treatment by artificial intelligence (AI) methods (such as those based on sensitive traits, social networks and recommender systems) to systematically discriminate against minorities, current solutions are computationally intensive, deliver reduced performance accuracy and/or lack interpretability. To address the fairness problem, we propose FairDrop, a novel biased edge dropout algorithm. Our approach protects against unfairness generated from the network\u2019s homophily with respect to sensitive attributes. FairDrop can be readily integrated into today\u2019s AI solutions for learning network embeddings or downstream tasks. We believe the lack of expensive computations and flexibility of our proposed fairness constraint will posit FairDrop as a future benchmark resource that will serve to highlight and address fairness challenges for the global AI research and innovation community. Index Terms\u2014Graph representation learning, graph embedding, fairness, link prediction, graph neural network. I. Spinelli, S. Scardapane, and A. Uncini are with the Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, 00184 Rome, Italy (e-mail: indro.spinelli@uniroma1.it; simone.scardapane@uniroma1.it; aurelio.uncini@uniroma1.it) A. Hussain is with the School of Computing, Edinburgh Napier University, UK (e-mail: a.hussain@napier.ac.uk). I. INTRODUCTION G RAPH structured data, ranging from friendships on social networks to physical links in energy grids, powers many algorithms governing our digital life. Social networks topologies define the stream of information we will receive, often influencing our opinion [1][2][3] [4] [5]. Bad actors, sometimes, define these topologies ad-hoc to spread false information [6]. Similarly, recommender systems [7] suggest products tailored to our own experiences and history of purchases. However, pursuing the highest accuracy as the only metric of interest has let many of these algorithms discriminate against minorities in the past [8] [9] [10], despite the law prohibiting unfair treatment based on sensitive traits such as race, religion, and gender. For this reason, the research community has started looking into the biases introduced by machine learning algorithms working over graphs [11]. One of the most common approaches to process graphs is via learning vector embeddings for the nodes (or the edges), e.g., [12]. These are a low dimensional representation of the nodes (or the edges), encoding the local topology. Downstream tasks then use these embeddings as inputs. Some examples of these tasks are node classification, community detection, and link prediction [13]. We will focus on the latter due to its widespread application in social networks and recommender systems. Alternatively, graph neural networks (GNNs) [14] [15] solve link prediction or other downstream tasks in an endto-end fashion, without prior learning of embeddings through ad-hoc procedures. The techniques developed in this paper can be applied to both scenarios. In this work, we will concentrate on the bias introduced by one of the key aspects behind the success of GNNs and node embedding procedures: homophily. Homophily is the principle that similar users interact at a higher rate than dissimilar ones. In a graph, this means that nodes with similar characteristics are more likely to be connected. In node classification, this encourages smoothness in terms of label distributions and embeddings, yielding excellent results [16]. From the fairness point of view, the homophily of sensitive attributes directly influences the prediction and introduces inequalities. In social networks, the \u201cunfair homophily\u201d of race, gender or nationality, limits the contents accessible by the users, influencing their online behaviour [1]. For example, the authors of [2] showed that users affiliated with majority political groups are exposed to new information faster and in greater quantity. Similarly, homophily can put minority groups at a disadvantage by restricting their ability to establish links with a majority group [17]. An unfair link prediction arXiv:2104.14210v2 [cs.LG] 27 Dec 2021 2 PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE magnifies this issue, known as \u201cfilter bubble\u201d, by increasing the segregation between the groups. To mitigate this issue, in this paper we propose a biased dropout strategy that forces the graph topology to reduce the homophily of sensitive attributes. At each step of training, we compute a random copy of the adjacency matrix biased towards reducing the amount of homophily in the original graph. Thus, models trained on these modified graphs undergo a fairness-biased data augmentation regime. Our approach limits the biases introduced by the unfair homophily, resulting in fairer node representations and link predictions while preserving most of the original accuracy. While we focus on homophily, we underline that unfairness can arise from additional factors that we do not consider here (e.g., unfair weights in the adjacency matrix, or node feature vectors). In these cases, FairDrop can be easily combined with any other known fairness-inducing technique. Measuring fairness in this context requires some adaptations. Most works on fairness measures focus on independent and identically distributed (i.i.d.) data. These works proposed many metrics, each one protecting against a different bias [18]. In particular, group fairness measures determine the level of equity of the algorithm predictions between groups of individuals. Link prediction requires a dyadic fairness measure that considers the influence of both sensitive attributes associated with the connection [19]. However, it is still possible to apply group fairness metrics by defining new groups for the edges. A. Contributions of the paper We propose a preprocessing technique that modifies the training data to reduce the predictability of its sensitive attributes [20]. Our algorithm introduces no overhead and can be framed as a biased data augmentation technique. A single hyperparameter regulates the intensity of the fairness constraint. This ranges from maximum fairness to an unbiased edge dropout. Therefore FairDrop can be easily adapted to the needs of different datasets and models. These characteristics make our framework extremely adaptable. Our approach can also be applied in combination with other fairness constraints. We evaluate the fairness improvement on two different tasks. Firstly, we measure the fairness imposed on the outcomes of end-to-end link prediction tasks. Secondly, we test the capability of removing the contributions of the sensitive attributes from the resulting node embeddings generated from representation learning algorithms. To measure the improvements for the link prediction we also propose a novel group-based fairness metric on dyadic level groups. We define a new dyadic group and use it in combination with the ones described in [19]. Instead, for the predictability of the sensitive attributes from the node embeddings, we measure the representation bias [21]. It is common to use node embeddings as input of downstream link prediction task. Therefore, we introduce a new metric that also considers the graph\u2019s topology. B. Outline of the paper The rest of the paper is structured as follows. Section II reviews recent works about enforcing fairness for graphstructured data and their limitations. Then, in Section III we introduce GNN models and group-based fairness metrics for i.i.d data. FairDrop, our proposed biased edge dropout, is first introduced in Section IV and then tested in Section VI. We conclude with some general remarks in Section VII. II. RELATED WORKS The literature on algorithmic bias is extensive and interdisciplinary [22]. However, most approaches study independent and identically distributed data. Just recently, with the success of GNNs, some works started to investigate fairness in graph representation learning. Some works focused on the creation of fair node embeddings [11][23] [21] that can be used as the input of a downstream task of link prediction. Others targeted directly the task of a fair link prediction [19] [24]. Some of these approaches base their foundations on adversarial learning. Compositional fairness constraints [11..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            },
            {
              "id": "E5570637906",
              "text": "..N. Lim, and A. Benson, \u201cCombining label propagation and simple models out-performs graph neural networks,\u201d in International Conference on Learning Representations, 2021. [17] F. Karimi, M. Genois, C. Wagner, P. Singer, and M. Strohmaier, \u201cHo- \u0301 mophily influences ranking of minorities in social networks,\u201d Scientific Reports, vol. 8, 07 2018. [18] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \u201cA survey on bias and fairness in machine learning,\u201d ArXiv, vol. abs/1908.09635, 2019. [19] F. Masrour, T. Wilson, H. Yan, P.-N. Tan, and A. Esfahanian, \u201cBursting the filter bubble: Fairness-aware network link prediction,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 841\u2013848, Apr. 2020. [20] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, \u201cCertifying and removing disparate impact,\u201d in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, (New York, NY, USA), p. 259\u2013268, Association for Computing Machinery, 2015. [21] M. Buyl and T. De Bie, \u201cDeBayes: a Bayesian method for debiasing network embeddings,\u201d in Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 1220\u20131229, PMLR, 13\u201318 Jul 2020. [22] A. Romei and S. Ruggieri, \u201cA multidisciplinary survey on discrimination analysis,\u201d The Knowledge Engineering Review, vol. 29, pp. 582 \u2013 638, 2013. [23] T. Rahman, B. Surma, M. Backes, and Y. Zhang, \u201cFairwalk: Towards fair graph embedding,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 3289\u20133295, International Joint Conferences on Artificial Intelligence Organization, 7 2019. [24] P. Li, Y. Wang, H. Zhao, P. Hong, and H. Liu, \u201cOn dyadic fairness: Exploring and mitigating bias in graph connections,\u201d in International Conference on Learning Representations, 2021. [25] T. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d ArXiv, vol. abs/1611.07308, 2016. [26] B. Kang, J. Lijffijt, and T. D. Bie, \u201cConditional network embeddings,\u201d in International Conference on Learning Representations, 2019. [27] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, \u201cCounterfactual fairness,\u201d in 31st Conference on Neural Information Processing Systems, pp. 1\u201318, 2017. [28] S. Chiappa, \u201cPath-specific counterfactual fairness,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 7801\u20137808, 2019. [29] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in International Conference on Learning Representations (ICLR), 2017. [30] D. Liben-Nowell and J. Kleinberg, \u201cThe link-prediction problem for social networks,\u201d vol. 58, p. 1019\u20131031, May 2007. [31] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d NIPS Workshop on Bayesian Deep Learning, 2016. [32] D. Bacciu, F. Errica, A. Micheli, and M. Podda, \u201cA gentle introduction to deep learning for graphs,\u201d Neural Networks, 2020. [33] Z. Zhang, P. Cui, and W. Zhu, \u201cDeep learning on graphs: A survey,\u201d IEEE Transactions on Knowledge and Data Engineering, 2020. [34] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA comprehensive survey on graph neural networks,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2020. [35] B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning of social representations,\u201d in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, (New York, NY, USA), p. 701\u2013710, Association for Computing Machinery, 2014. [36] W. Jin, T. Derr, H. Liu, Y. Wang, S. Wang, Z. Liu, and J. Tang, \u201cSelfsupervised learning on graphs: Deep insights and new direction,\u201d arXiv preprint arXiv:2006.10141, 2020. [37] N. A. Saxena, K. Huang, E. DeFilippis, G. Radanovic, D. C. Parkes, and Y. Liu, \u201cHow do fairness definitions fare? examining public attitudes towards algorithmic definitions of fairness,\u201d in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201919, (New York, NY, USA), p. 99\u2013106, Association for Computing Machinery, 2019. [38] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, \u201cFairness through awareness,\u201d in Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS \u201912, (New York, NY, USA), p. 214\u2013226, Association for Computing Machinery, 2012. [39] M. Hardt, E. Price, E. Price, and N. Srebro, \u201cEquality of opportunity in supervised learning,\u201d in Advances in Neural Information Processing Systems, vol. 29, Curran Associates, Inc., 2016. [40] S. Bird, M. Dud \u0301\u0131k, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach, and K. Walker, \u201cFairlearn: A toolkit for assessing and improving fairness in AI,\u201d Tech. Rep. MSR-TR-2020-32, Microsoft, May 2020. [41] D. P. Helmbold and P. M. Long, \u201cOn the inductive bias of dropout,\u201d The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3403\u20133454, 2015. [42] S. L. Warner, \u201cRandomized response: A survey technique for eliminating evasive answer bias,\u201d Journal of the American Statistical Association, vol. 60, no. 309, pp. 63\u201369, 1965. PMID: 12261830. [43] Y. Rong, W. Huang, T. Xu, and J. Huang, \u201cDropedge: Towards deep graph convolutional networks on node classification,\u201d in International Conference on Learning Representations, 2020. [44] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney, \u201cOptimized pre-processing for discrimination prevention,\u201d in Advances in Neural Information Processing Systems (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017. [45] P. Velickovi \u02c7 c, G. Cucurull, A. Casanova, A. Romero, P. Li \u0301 o, and ` Y. Bengio, \u201cGraph Attention Networks,\u201d International Conference on Learning Representations, 2018. accepted as poster. [46] I. Spinelli, S. Scardapane, and A. Uncini, \u201cAdaptive propagation graph convolutional network,\u201d IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u20136, 2020. [47] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, \u201cLearning fair representations,\u201d in Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, p. III\u2013325\u2013III\u2013333, JMLR.org, 2013. [48] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d International Conference on Learning Representations, 12 2014. [49] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \u201cArnetminer: Extraction and mining of academic social networks,\u201d in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, (New York, NY, USA), p. 990\u2013998, Association for Computing Machinery, 2008. [50] J. Leskovec and J. Mcauley, \u201cLearning to discover social circles in ego networks,\u201d in Advances in Neural Information Processing Systems (F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.), vol. 25, Curran Associates, Inc., 2012. [51] D. Chen, Y. Lin, W. Li, P..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            }
          ]
        },
        "S0874386725": {
          "id": "S0874386725",
          "text": "The proposed FairDrop algorithm introduces a new dyadic group definition to measure bias in link prediction tasks, which enhances the evaluation of fairness in graph representation learning.",
          "children": [
            {
              "id": "E6418977503",
              "text": "PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 1 FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini Abstract\u2014Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure. Impact Statement\u2014Fairness in graph representation learning is under-explored. Yet, the algorithms working with these types of data have a fundamental impact on our digital life. Therefore, despite the law now prohibiting unfair treatment by artificial intelligence (AI) methods (such as those based on sensitive traits, social networks and recommender systems) to systematically discriminate against minorities, current solutions are computationally intensive, deliver reduced performance accuracy and/or lack interpretability. To address the fairness problem, we propose FairDrop, a novel biased edge dropout algorithm. Our approach protects against unfairness generated from the network\u2019s homophily with respect to sensitive attributes. FairDrop can be readily integrated into today\u2019s AI solutions for learning network embeddings or downstream tasks. We believe the lack of expensive computations and flexibility of our proposed fairness constraint will posit FairDrop as a future benchmark resource that will serve to highlight and address fairness challenges for the global AI research and innovation community. Index Terms\u2014Graph representation learning, graph embedding, fairness, link prediction, graph neural network. I. Spinelli, S. Scardapane, and A. Uncini are with the Department of Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome, 00184 Rome, Italy (e-mail: indro.spinelli@uniroma1.it; simone.scardapane@uniroma1.it; aurelio.uncini@uniroma1.it) A. Hussain is with the School of Computing, Edinburgh Napier University, UK (e-mail: a.hussain@napier.ac.uk). I. INTRODUCTION G RAPH structured data, ranging from friendships on social networks to physical links in energy grids, powers many algorithms governing our digital life. Social networks topologies define the stream of information we will receive, often influencing our opinion [1][2][3] [4] [5]. Bad actors, sometimes, define these topologies ad-hoc to spread false information [6]. Similarly, recommender systems [7] suggest products tailored to our own experiences and history of purchases. However, pursuing the highest accuracy as the only metric of interest has let many of these algorithms discriminate against minorities in the past [8] [9] [10], despite the law prohibiting unfair treatment based on sensitive traits such as race, religion, and gender. For this reason, the research community has started looking into the biases introduced by machine learning algorithms working over graphs [11]. One of the most common approaches to process graphs is via learning vector embeddings for the nodes (or the edges), e.g., [12]. These are a low dimensional representation of the nodes (or the edges), encoding the local topology. Downstream tasks then use these embeddings as inputs. Some examples of these tasks are node classification, community detection, and link prediction [13]. We will focus on the latter due to its widespread application in social networks and recommender systems. Alternatively, graph neural networks (GNNs) [14] [15] solve link prediction or other downstream tasks in an endto-end fashion, without prior learning of embeddings through ad-hoc procedures. The techniques developed in this paper can be applied to both scenarios. In this work, we will concentrate on the bias introduced by one of the key aspects behind the success of GNNs and node embedding procedures: homophily. Homophily is the principle that similar users interact at a higher rate than dissimilar ones. In a graph, this means that nodes with similar characteristics are more likely to be connected. In node classification, this encourages smoothness in terms of label distributions and embeddings, yielding excellent results [16]. From the fairness point of view, the homophily of sensitive attributes directly influences the prediction and introduces inequalities. In social networks, the \u201cunfair homophily\u201d of race, gender or nationality, limits the contents accessible by the users, influencing their online behaviour [1]. For example, the authors of [2] showed that users affiliated with majority political groups are exposed to new information faster and in greater quantity. Similarly, homophily can put minority groups at a disadvantage by restricting their ability to establish links with a majority group [17]. An unfair link prediction arXiv:2104.14210v2 [cs.LG] 27 Dec 2021 2 PREPRINT SUBMITTED TO IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE magnifies this issue, known as \u201cfilter bubble\u201d, by increasing the segregation between the groups. To mitigate this issue, in this paper we propose a biased dropout strategy that forces the graph topology to reduce the homophily of sensitive attributes. At each step of training, we compute a random copy of the adjacency matrix biased towards reducing the amount of homophily in the original graph. Thus, models trained on these modified graphs undergo a fairness-biased data augmentation regime. Our approach limits the biases introduced by the unfair homophily, resulting in fairer node representations and link predictions while preserving most of the original accuracy. While we focus on homophily, we underline that unfairness can arise from additional factors that we do not consider here (e.g., unfair weights in the adjacency matrix, or node feature vectors). In these cases, FairDrop can be easily combined with any other known fairness-inducing technique. Measuring fairness in this context requires some adaptations. Most works on fairness measures focus on independent and identically distributed (i.i.d.) data. These works proposed many metrics, each one protecting against a different bias [18]. In particular, group fairness measures determine the level of equity of the algorithm predictions between groups of individuals. Link prediction requires a dyadic fairness measure that considers the influence of both sensitive attributes associated with the connection [19]. However, it is still possible to apply group fairness metrics by defining new groups for the edges. A. Contributions of the paper We propose a preprocessing technique that modifies the training data to reduce the predictability of its sensitive attributes [20]. Our algorithm introduces no overhead and can be framed as a biased data augmentation technique. A single hyperparameter regulates the intensity of the fairness constraint. This ranges from maximum fairness to an unbiased edge dropout. Therefore FairDrop can be easily adapted to the needs of different datasets and models. These characteristics make our framework extremely adaptable. Our approach can also be applied in combination with other fairness constraints. We evaluate the fairness improvement on two different tasks. Firstly, we measure the fairness imposed on the outcomes of end-to-end link prediction tasks. Secondly, we test the capability of removing the contributions of the sensitive attributes from the resulting node embeddings generated from representation learning algorithms. To measure the improvements for the link prediction we also propose a novel group-based fairness metric on dyadic level groups. We define a new dyadic group and use it in combination with the ones described in [19]. Instead, for the predictability of the sensitive attributes from the node embeddings, we measure the representation bias [21]. It is common to use node embeddings as input of downstream link prediction task. Therefore, we introduce a new metric that also considers the graph\u2019s topology. B. Outline of the paper The rest of the paper is structured as follows. Section II reviews recent works about enforcing fairness for graphstructured data and their limitations. Then, in Section III we introduce GNN models and group-based fairness metrics for i.i.d data. FairDrop, our proposed biased edge dropout, is first introduced in Section IV and then tested in Section VI. We conclude with some general remarks in Section VII. II. RELATED WORKS The literature on algorithmic bias is extensive and interdisciplinary [22]. However, most approaches study independent and identically distributed data. Just recently, with the success of GNNs, some works started to investigate fairness in graph representation learning. Some works focused on the creation of fair node embeddings [11][23] [21] that can be used as the input of a downstream task of link prediction. Others targeted directly the task of a fair link prediction [19] [24]. Some of these approaches base their foundations on adversarial learning. Compositional fairness constraints [11..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            },
            {
              "id": "E5570637906",
              "text": "..N. Lim, and A. Benson, \u201cCombining label propagation and simple models out-performs graph neural networks,\u201d in International Conference on Learning Representations, 2021. [17] F. Karimi, M. Genois, C. Wagner, P. Singer, and M. Strohmaier, \u201cHo- \u0301 mophily influences ranking of minorities in social networks,\u201d Scientific Reports, vol. 8, 07 2018. [18] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \u201cA survey on bias and fairness in machine learning,\u201d ArXiv, vol. abs/1908.09635, 2019. [19] F. Masrour, T. Wilson, H. Yan, P.-N. Tan, and A. Esfahanian, \u201cBursting the filter bubble: Fairness-aware network link prediction,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 841\u2013848, Apr. 2020. [20] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, \u201cCertifying and removing disparate impact,\u201d in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, (New York, NY, USA), p. 259\u2013268, Association for Computing Machinery, 2015. [21] M. Buyl and T. De Bie, \u201cDeBayes: a Bayesian method for debiasing network embeddings,\u201d in Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 1220\u20131229, PMLR, 13\u201318 Jul 2020. [22] A. Romei and S. Ruggieri, \u201cA multidisciplinary survey on discrimination analysis,\u201d The Knowledge Engineering Review, vol. 29, pp. 582 \u2013 638, 2013. [23] T. Rahman, B. Surma, M. Backes, and Y. Zhang, \u201cFairwalk: Towards fair graph embedding,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 3289\u20133295, International Joint Conferences on Artificial Intelligence Organization, 7 2019. [24] P. Li, Y. Wang, H. Zhao, P. Hong, and H. Liu, \u201cOn dyadic fairness: Exploring and mitigating bias in graph connections,\u201d in International Conference on Learning Representations, 2021. [25] T. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d ArXiv, vol. abs/1611.07308, 2016. [26] B. Kang, J. Lijffijt, and T. D. Bie, \u201cConditional network embeddings,\u201d in International Conference on Learning Representations, 2019. [27] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, \u201cCounterfactual fairness,\u201d in 31st Conference on Neural Information Processing Systems, pp. 1\u201318, 2017. [28] S. Chiappa, \u201cPath-specific counterfactual fairness,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 7801\u20137808, 2019. [29] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in International Conference on Learning Representations (ICLR), 2017. [30] D. Liben-Nowell and J. Kleinberg, \u201cThe link-prediction problem for social networks,\u201d vol. 58, p. 1019\u20131031, May 2007. [31] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d NIPS Workshop on Bayesian Deep Learning, 2016. [32] D. Bacciu, F. Errica, A. Micheli, and M. Podda, \u201cA gentle introduction to deep learning for graphs,\u201d Neural Networks, 2020. [33] Z. Zhang, P. Cui, and W. Zhu, \u201cDeep learning on graphs: A survey,\u201d IEEE Transactions on Knowledge and Data Engineering, 2020. [34] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA comprehensive survey on graph neural networks,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2020. [35] B. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning of social representations,\u201d in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, (New York, NY, USA), p. 701\u2013710, Association for Computing Machinery, 2014. [36] W. Jin, T. Derr, H. Liu, Y. Wang, S. Wang, Z. Liu, and J. Tang, \u201cSelfsupervised learning on graphs: Deep insights and new direction,\u201d arXiv preprint arXiv:2006.10141, 2020. [37] N. A. Saxena, K. Huang, E. DeFilippis, G. Radanovic, D. C. Parkes, and Y. Liu, \u201cHow do fairness definitions fare? examining public attitudes towards algorithmic definitions of fairness,\u201d in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201919, (New York, NY, USA), p. 99\u2013106, Association for Computing Machinery, 2019. [38] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, \u201cFairness through awareness,\u201d in Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS \u201912, (New York, NY, USA), p. 214\u2013226, Association for Computing Machinery, 2012. [39] M. Hardt, E. Price, E. Price, and N. Srebro, \u201cEquality of opportunity in supervised learning,\u201d in Advances in Neural Information Processing Systems, vol. 29, Curran Associates, Inc., 2016. [40] S. Bird, M. Dud \u0301\u0131k, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach, and K. Walker, \u201cFairlearn: A toolkit for assessing and improving fairness in AI,\u201d Tech. Rep. MSR-TR-2020-32, Microsoft, May 2020. [41] D. P. Helmbold and P. M. Long, \u201cOn the inductive bias of dropout,\u201d The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3403\u20133454, 2015. [42] S. L. Warner, \u201cRandomized response: A survey technique for eliminating evasive answer bias,\u201d Journal of the American Statistical Association, vol. 60, no. 309, pp. 63\u201369, 1965. PMID: 12261830. [43] Y. Rong, W. Huang, T. Xu, and J. Huang, \u201cDropedge: Towards deep graph convolutional networks on node classification,\u201d in International Conference on Learning Representations, 2020. [44] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney, \u201cOptimized pre-processing for discrimination prevention,\u201d in Advances in Neural Information Processing Systems (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017. [45] P. Velickovi \u02c7 c, G. Cucurull, A. Casanova, A. Romero, P. Li \u0301 o, and ` Y. Bengio, \u201cGraph Attention Networks,\u201d International Conference on Learning Representations, 2018. accepted as poster. [46] I. Spinelli, S. Scardapane, and A. Uncini, \u201cAdaptive propagation graph convolutional network,\u201d IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u20136, 2020. [47] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, \u201cLearning fair representations,\u201d in Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, p. III\u2013325\u2013III\u2013333, JMLR.org, 2013. [48] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d International Conference on Learning Representations, 12 2014. [49] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, \u201cArnetminer: Extraction and mining of academic social networks,\u201d in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, (New York, NY, USA), p. 990\u2013998, Association for Computing Machinery, 2008. [50] J. Leskovec and J. Mcauley, \u201cLearning to discover social circles in ego networks,\u201d in Advances in Neural Information Processing Systems (F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.), vol. 25, Curran Associates, Inc., 2012. [51] D. Chen, Y. Lin, W. Li, P..",
              "url": "https://openalex.org/W4206323856",
              "openalex_id": "https://openalex.org/W4206323856",
              "title": "FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning",
              "publication_date": "2021-12-09"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W4206323856"
      ],
      "evaluation": {
        "precision@10": 0.0,
        "recall@10": 0.0,
        "f1@10": 0,
        "rouge_1": 0.01788031723143475,
        "rouge_2": 0.00800346095608912,
        "rouge_l": 0.011896178803172314,
        "text_f1": 0.053823727293115055,
        "num_source_papers": 1
      }
    },
    {
      "id": "https://openalex.org/W4388017359",
      "limited_meta": {
        "title": "End-to-End Speech Recognition: A Survey",
        "publication_date": "2024-01-01",
        "cited_by_count": 33,
        "url": ""
      },
      "text": "1\nEnd-to-End Speech Recognition: A Survey\nRohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE,\nRalf Schluter, \u00a8 Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE\nAbstract\u2014In the last decade of automatic speech recognition\n(ASR) research, the introduction of deep learning has brought\nconsiderable reductions in word error rate of more than 50%\nrelative, compared to modeling without deep learning. In the\nwake of this transition, a number of all-neural ASR architectures\nhave been introduced. These so-called end-to-end (E2E) models\nprovide highly integrated, completely neural ASR models, which\nrely strongly on general machine learning knowledge, learn more\nconsistently from data, with lower dependence on ASR domain\u0002specific experience. The success and enthusiastic adoption of deep\nlearning, accompanied by more generic model architectures has\nled to E2E models now becoming the prominent ASR approach.\nThe goal of this survey is to provide a taxonomy of E2E ASR\nmodels and corresponding improvements, and to discuss their\nproperties and their relationship to classical hidden Markov\nmodel (HMM) based ASR architectures. All relevant aspects\nof E2E ASR are covered in this work: modeling, training,\ndecoding, and external language model integration, discussions of\nperformance and deployment opportunities, as well as an outlook\ninto potential future developments.\nIndex Terms\u2014end-to-end, automatic speech recognition.\nI. INTRODUCTION\nThe classical1statistical architecture decomposes an auto\u0002matic speech recognition (ASR) system into four main compo\u0002nents: acoustic feature extraction from speech audio signals,\nacoustic modeling, language modeling and search based on\nBayes\u2019 decision rule [1], [2], [3]. Classical acoustic modeling\nis based on hidden Markov models (HMMs) to account for\nspeaking rate variation. Within the classical approach, deep\nlearning has been introduced into acoustic and language mod\u0002eling. In acoustic modeling, deep learning has replaced Gaus\u0002sian mixture distributions (hybrid HMM [4], [5]) or augmented\nthe acoustic feature set (e.g., non-linear discriminant/tandem\napproach [6], [7]). In language modeling, deep learning has re\u0002placed count-based approaches [8], [9], [10]. However, in these\nearly attempts at introducing deep learning, the classical ASR\narchitecture was unmodified. Classical state-of-the-art ASR\nsystems today are composed of many separate components and\nknowledge sources: especially speech signal preprocessing;\nmethods for robustness with respect to recording conditions;\nphoneme inventories and pronunciation lexica; phonetic clus\u0002tering; handling of out-of-vocabulary words; various methods\nfor adaptation/normalization; elaborate training schedules with\ndifferent objectives including sequence discriminative training,\netc. The potential of deep learning, on the other hand, initiated\nsuccessful approaches to integrate formerly separate modeling\nsteps, e.g., by integrating speech signal pre-processing and\nfeature extraction into acoustic modeling [11], [12].\n1 The term \u201cclassical\u201d here refers to the former, long-term, state-of-the-art\nASR architecture based on the decomposition into acoustic and language\nmodel, and with acoustic modeling based on hidden Markov models.\nMore consequently, the introduction of deep learning to\nASR also initiated research to replace classical ASR archi\u0002tectures based on hidden Markov models (HMM) with more\nintegrated joint neural network model structures [13], [14],\n[15], [16]. These ventures might be seen as trading specific\nspeech processing models for more generic machine learning\napproaches to sequence-to-sequence processing \u2013 akin to how\nstatistical approaches to natural language processing have\ncome to replace more linguistically oriented models. For these\nall-neural approaches recently the term end-to-end (E2E) [14],\n[17], [18], [19] has been established. Therefore, first of all\nan attempt to define the term end-to-end in the context of\nASR is due in this survey. According to the Cambridge\nDictionary, the adjective \u201cend-to-end\u201d is defined as: \u201cinclud\u0002ing all the stages of a process\u201d [20]. We therefore propose\nthe following definition of end-to-end ASR: an integrated\nASR model that enables joint training from scratch; avoids\nseparately obtained knowledge sources; and, provides single\u0002pass recognition consistent with the objective to optimize the\ntask-specific evaluation measure, i.e., usually label (word,\ncharacter, subword, etc.) error rate. While this definition\nsuffices for the present discussion, we note that such an\nidealized definition hides many nuances involved in the term\nE2E and lacks distinctiveness; we elaborate on some of these\nnuances in Sec. II to discuss the various connotations of the\nterm E2E in the context of ASR.\nWhat are potential benefits of E2E approaches to ASR?\nThe primary objective when developing an ASR systems is to\nminimize the expected word error rate; secondary objectives\nare to reduce time and memory complexity of the resulting\ndecoder, and \u2013 assuming a constrained development budget \u2013\ngenericity, and ease of modeling. First of all, an integrated\nASR system, defined in terms of a single neural network\nstructure supports genericity of modeling and may allow for\nfaster development cycles when building ASR systems for\nnew languages or domains. Similarly, ASR models defined\nby a single neural network structure may become more \u2018lean\u2019\ncompared to classical modeling, with a simpler decoding\nprocess, obviating the need to integrate separate models. The\nresulting reduction in memory footprint and power consump\u0002tion supports embedded ASR applications [21], [22]. Further\u0002more, end-to-end joint training may help to avoid spurious\noptima from intermediate training stages. Avoiding secondary\nknowledge sources like pronunciation lexica may be helpful\nfor languages/domains where such resources are not easily\navailable. Also, secondary knowledge sources may themselves\nbe erroneous; avoiding these may improve models trained\ndirectly from data, provided that sufficient amounts of task\u0002specific training data are available.\nWith the current surge of interest in E2E ASR models and an\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nincreasing diversity of corresponding work, the authors of this\nreview think it is time to provide an overview of this rapidly\nevolving domain of research. The goal of this survey is to\nprovide an in-depth overview of the current state of research\non E2E ASR systems, covering all relevant aspects of E2E\nASR, with a contrastive discussion of the different E2E and\nclassical ASR architectures.\nThis survey of E2E speech recognition is structured as fol\u0002lows. Sec. II discusses the nuances in the term E2E as it applies\nto ASR. Sec. III describes the historical evolution of E2E\nspeech recognition, with specific focus on the input-output\nalignment and an overview of prominent E2E ASR models.\nSec. IV discusses improvements of the basic E2E models,\nincluding E2E model combination, training loss functions,\ncontext, encoder/decoder structures and endpointing. Sec. V\nprovides an overview of E2E ASR model training. Decoding\nalgorithms for the different E2E approaches are discussed\nin Sec. VI. Sec. VII discusses the role and integration of\n(separate) language models in E2E ASR. Sec. VIII reviews\nexperimental comparisons of the different E2E as well as\nclassical ASR approaches. Sec. IX provides an overview of\napplications of E2E ASR. Sec. X investigates future directions\nof E2E research in ASR, before concluding in Sec. XI. Finally,\nwe note that this survey paper also includes comparative\ndiscussions between novel E2E models and classical HMM\u0002based ASR approaches in terms of various aspects; most\nsections end with a summarization of the relationship between\nE2E models and HMM-based ASR approaches in relation to\nthe topics covered within the respective sections.\nII. DISTINCTIVENESS OF THE TERM E2E\nAs noted in Sec. I the term E2E provides an idealized\ndefinition of ASR systems, and can benefit from a more\ndetailed discussion based on the following perspectives.\na) Joint Modeling: In terms of ASR, the E2E property\ncan be interpreted as considering all components of an ASR\nsystem jointly as a single computational graph. Even more so,\nthe common understanding of E2E in ASR is that of a single\njoint modeling approach that does not necessarily distinguish\nseparate components, which may also mean dropping the\nclassical separation of ASR into an acoustic model and a\nlanguage model. However, in practice E2E ASR systems are\noften combined with external language models trained on text\u0002only data, which weakens the end-to-end nature of the system\nto some extent.\nb) Joint Training: In terms of model training, E2E can\nbe interpreted as estimating all parameters, of all components\nof a model jointly using a single objective function that is\nconsistent with the task at hand, which in case of ASR means\nminimizing the expected word error rate2. However, the term\nlacks distinctiveness here, as classical and/or modular ASR\nmodel architectures also support joint training with a single\nobjective.\n2 Note that this does not necessarily require Bayes Risk training, as standard\ntraining criteria like cross entropy, maximum mutual information and max\u0002imum likelihood in case of classical ASR models asymptotically guarantee\noptimal performance in the sense of Bayes decision rule, also [23], [24].\nc) Training from Scratch: The E2E property can also be\ninterpreted with respect to the training process itself, by re\u0002quiring training from scratch, avoiding external knowledge like\nprior alignments or initial models pre-trained using different\ncriteria or knowledge sources. However, note that pre-training\nand fine-tuning strategies are also relevant, if the model has\nexplicit modularity, including self-supervised learning [25] or\njoint training of front-end and speech recognition models [26].\nEspecially in case of limited amounts of target task training\ndata, utilizing large pretrained models is important to obtain\nperformant E2E ASR systems.\nd) Avoiding Secondary Knowledge Sources: For ASR,\nstandard secondary knowledge sources are pronunciation lex\u0002ica and phoneme sets, as well as phonetic clustering, which\nin classical state-of-the-art ASR systems usually is based on\nclassification and regression trees (CART) [27]. Secondary\nknowledge sources and separately trained components may\nintroduce errors, might be inconsistent with the overall training\nobjective and/or may generate additional cost. Therefore, in\nan E2E approach, these would be avoided. Standard joint\ntraining of an E2E model requires using a single kind of\ntraining data, which in case of ASR would be transcribed\nspeech audio data. However, in ASR often even larger amounts\nof text-only data, as well as optional untranscribed speech\naudio are available. One of the challenges of E2E modeling\ntherefore is how to take advantage of text-only and audio-only\ndata jointly without introducing secondary (pretrained) models\nand/or training objectives [28], [29].\ne) Direct Vocabulary Modeling: Avoiding pronunciation\nlexica and corresponding subword units leave E2E recognition\nvocabularies to be derived from whole word or character\nrepresentations. Whole word models [30], according to Zipf\u2019s\nlaw [31], would require unrealistically high amounts of tran\u0002scribed training data for large vocabularies, which might not\nbe attainable for many tasks. On the other hand, methods\nto generate subword vocabularies based on characters, like\nthe currently popular byte pair encoding (BPE) approach\n[32], might be seen as secondary approaches outside the E2E\nobjective, even more so if acoustic data is considered for\nsubword derivation [33], [34], [35], [36].\nf) Generic Modeling: Finally, E2E modeling also re\u0002quires genericity of the underlying modeling: task-specific\nconstraints are learned completely from data, in contrast to\ntask-specific knowledge which influences the modeling of\nthe system architecture in the first place. For example, the\nmonotonicity constraint in ASR may be learned completely\nfrom data in an end-to-end fashion (e.g., in attention-based\napproaches [16]), or it may directly be implemented, as in\nclassical HMM structures. However, model constraints may\nbe considered by way of regularization in E2E ASR model\ntraining, and can thus provide an alternative way to introduce\ntask-specific knowledge.\ng) Single-Pass Search: In terms of the recognition/search\nproblem, the E2E property can be interpreted as integrating all\ncomponents (models, knowledge sources) of an ASR system\nbefore coming to a decision. This is in line with Bayes\u2019\ndecision rule, which exactly requires a single global decision\nintegrating all available knowledge sources, which is supported\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nby both classical ASR models as well as E2E models. On\nthe other hand, multipass search is not only exploited by\nclassical ASR models, but also by E2E ASR models, the\nmost prominent case here being (external) language model\nrescoring.\nAll in all, we need to conclude that a) \u201cE2E\u201d does not\nprovide a clear distinction between classical and novel, so\u0002called E2E models, and b) the E2E property often is weakened\nin practice, leaving the term as a more general, idealized\nperspective on ASR modeling.\nIII. A TAXONOMY OF E2E MODELS IN ASR\nBefore we derive a taxonomy of E2E ASR modeling\napproaches, we first introduce our notation. We denote the\ninput speech utterance as X, which we assume has been pa\u0002rameterized into D-dimensional acoustic frames (e.g., log-mel\nfeatures) of length T\n\u2032\n: X = (x1, \u00b7 \u00b7 \u00b7 , xT\u2032 ), where xt \u2208 R\nD.\nWe denote the corresponding word sequences as C, which can\nbe decomposed into a suitable sequence of labels of length L:\nC = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is\nagnostic to the specific representation used for decomposing\nthe word sequence into labels; popular choices include char\u0002acters, words, or sub-word sequences (e.g., BPE [32], word\u0002pieces [37]).\nASR may be viewed as a sequence classification problem\nwhich maps a variable length input, X, into an output,\nC, of unknown length. Following Bayes\u2019 decision rule, any\nstatistical approach to ASR must determine how to model the\nword sequence posterior probability, P(C|X). Thus, a natural\ntaxonomy of E2E ASR modeling can be based on the various\nstrategies for modeling this word sequence posterior: i.e., how\nthe alignment problem between input and output sequence is\nhandled; and, how sequence modeling is decomposed to the\nlevel of individual input vectors xt\n\u2032 and/or output labels cl\n.\nWe find that it is useful to distinguish implicit and explicit\nmodeling approaches, based on the modeling of the sequence\u0002to-sequence alignment:\na) Explicit Alignment Modeling: does not necessarily\nrefer to the determination of a single unique alignment, but\ninstead introduces an explicit alignment modeled as a latent\nvariable, A:\nP(C|X) = X\nA\nP(C, A|X)\nb) Implicit Alignment Modeling: does not introduce a\nlatent alignment variable, but models the label sequence pos\u0002terior P(C|X) directly.\nExplicit alignment modeling approaches can mainly be\ndistinguished by their choice of latent variable; these can be\nencoded in terms of valid emission paths in corresponding\nfinite state automata (FSA) [38] which relate the input and\noutput sequences \u2013 the approach taken in our article. Typically,\nlatent variables in explicit alignment modeling in transducer\nE2E models introduce extensions to the output label set\nwith different forms of continuation labels (including, but not\nlimited to so-called blank labels).3\n3 For example, these extensions may also include explicit duration variables,\nleading to segmental models [39]. Such models can be rewritten into equiv\u0002alent transducer models [40], and vice-versa.\nA. Encoder and Decoder Modules\nIrrespective of the alignment modeling approach, following\nthe notation introduced in [41], it is useful to view all E2E\nASR models as being composed of an encoder module and\na decoder module. The encoder module, denoted H(X),\nmaps an input acoustic frame sequence, X, of length T\n\u2032\ninto a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of\nlength T (typically T \u2264 T\n\u2032\n). Note that the encoder output is\nindependent of the hypothesized label sequence. The decoder\nmodule models the label sequence posterior on top of the\nencoder output:\nP(C|X) = P\nC\nH(X)\n\u0001\nThus, we may distinguish different approaches based upon\nhow the output label sequence distribution (including potential\nlatent variables resulting from the alignment modeling) are de\u0002composed into individual label (and alignment) contributions;\nthese may occur per output label position, per encoder frame\nposition, or combinations thereof:\nP\nC[, A]\nH(X)\n\u0001\n=\nY\nL\ni=1\nP\nci[, ai]\nc\ni\u22121\n1\n[, ai\u22121\n1\n], vi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X))\u0001\nwhere the notation mi\u22121\n1\ncorresponds to the sequence\nof i \u2212 1 previous instances of the variables m; and,\nvi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X)) denotes a context-vector that provides\nthe connection between encoder output, H(X), and the la\u0002bel output position, i. In general the context vector may\ndepend on the label context (and possibly the latent vari\u0002able context, for explicit alignment modeling approaches).\nApart from the underlying alignment model and corresponding\noutput label decomposition, decoder modules differ in terms\nof the assumptions on their label context c\ni\u22121\n1\n(and their\nlatent variable context a\ni\u22121\n1\n), which correspond to different\nconditional independence assumptions, and by their access to\nthe encoder output. For example, the local posterior may only\ndepend on a single encoder frame output (i.e., with the context\nvector being reduced to a single encoder frame\u2019s output):\nvi\nc\ni\u22121\n1\n, H(X)\n\u0001\n= hti(X). As we shall see in detail in the\nfollowing sections, the simplest case of an encoder frame\u0002level decomposition (with L = T, and ti = i) corresponds to\nCTC [13]; AED models [16] and their variants maintain the\nfull dependency of the context vector.\nFinally, different E2E models can also be distinguished by\nthe specific modeling choices that are involved in the design\nof the neural network used to implement the encoder and the\ndecoder. These might involve feed-forward neural networks,\nconvolutional neural networks, recurrent neural networks (ei\u0002ther uni-directional or bi-directional) [42], attention [43],\nand various combinations thereof (e.g., transformers [44] or\nconformers [45]). These modeling choices and corresponding\ntraining methods can be applied across E2E ASR models and\ntherefore do not enter the taxonomy of E2E ASR models\ndiscussed here. However, specific choices will be discussed as\npart of the exemplary E2E ASR models presented in Sec. VIII\nand Sec. IX and.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nB. Explicit Alignment Modeling Approaches\nEarly E2E modeling approaches modeled alignments explic\u0002itly through a latent variable, which is marginalized out (pos\u0002sibly, approximately) during training and inference. Examples\nof this family of approaches include connectionist temporal\nclassification (CTC) [13], the recurrent neural network trans\u0002ducer (RNN-T) [14], the recurrent neural aligner (RNA) [46],\nand the hybrid auto-regressive transducer [47] (HAT). As\nwill be discussed in subsequent sections, the latter modeling\napproaches in this family represent increasingly sophisticated\nmodeling of alignments, with fewer independence assumptions\nand are thus increasingly powerful. A common feature of all\nexplicit alignment models discussed in this section is that\nthey introduce an additional blank symbol, denoted \u27e8b\u27e9, and\ndefine an output probability distribution over symbols in the\nset Cb = C \u222a {\u27e8b\u27e9}. The interpretation of the \u27e8b\u27e9 symbol\nvaries slightly between each of these models, as we discuss in\ngreater details below. For now, it suffices to say that given\na specific training example, (X, C), each of these models\ndefines a set of valid alignments, denoted by A(T ,C), and\ndefine the conditional distribution P(C|X) by marginalizing\nover all valid alignment sequences:\nP(C|X) = X\nA\nP(C|A, H(X))P(A|H(X))\n=\nX\nA\u2208A(T =|H(X)|,C)\nP(A|H(X)) (1)\nwhere, by definition P(C|A, H(X)) = 1 if and only if A \u2208\nA(T ,C) and 0 otherwise.4 We discuss the specific formulations\nof each of these models in the subsequent sections.\n1) Connectionist Temporal Classification (CTC): Connec\u0002tionist Temporal Classification (CTC) was proposed by Graves\net al. [13] as a technique for mapping a sequence of input\ntokens to a corresponding sequence of output tokens. CTC ex\u0002plicitly models alignments between the encoder output, H(X),\nand the label sequence, C, by introducing a special \u201cblank\u201d la\u0002bel, denoted by \u27e8b\u27e9: Cb = C \u222a {\u27e8b\u27e9}. An alignment, A \u2208 C\u2217\nb\n, is\nthus a sequence of labels in C or \u27e8b\u27e9.\n5 Given a specific training\nexample, (X, C), we denote the set of all valid alignments,\nACTC\n(X,C) = {A = (a1, a2, . . . , aT )}, such that each at \u2208 Cb\nwith the additional constraint that A is identical to C after first\ncollapsing consecutive identical labels, and then removing all\nblank symbols. For example, if T = 10, and C = (s, e, e),\nthen A = (s,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ACTC\n(X,C)\n, as\nillustrated in Figure 1. As can be seen in this example, repeated\nlabels in the output can be represented by intervening blanks.\nFollowing Eq. (1), CTC defines the posterior probability\nof the label sequence C conditioned on the input, X, by\n4 This is equivalent to the assumption that the mapping from an alignment\nA to a label sequence C is unique, by definition. 5 S\n\u2217 denotes a Kleene\nclosure: the set of all possible sequences composed of tokens in the set S.\nTime\ns\ne\ne\n \ns\ne\ne\ns\ne\ne\ne\nFig. 1. Example alignment sequence for a CTC model with the target\nsequence C = (s, e, e) (right), alongside a (non-deterministic) finite state\nautomaton (FSA) [38] (left) representing the set of all valid alignment paths.\nEncoder H(X)\nSoftmax\nFig. 2. A representation of the CTC model consisting of an encoder which\nmaps the input speech into a higher-level representation, and a softmax layer\nwhich predicts frame-level probabilities over the set of output labels and blank.\nmarginalizing over all possible CTC alignments as:\nPCTC(C|X) = X\nA\u2208ACTC\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, \u00b7 \u00b7 \u00b7 , a1, H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|ht) (2)\nCritically, as can be seen in Eq. (2), CTC makes a strong\nindependence assumption that the model\u2019s output at time t is\nconditionally independent of the outputs at other timesteps,\ngiven the local encoder output at time t.\nThus, a CTC model consists of a neural network that\nmodels the distribution P(at|X), at each step as shown in\nFigure 2. The encoder is connected to a softmax layer with\n|Cb| targets representing the individual probabilities in Eq. (2):\nP(at = c|X) = P(at = c|H(X)), which comprises the\ndecoder module for CTC. Thus, at each step, t, the model\nconsumes a single encoded frame ht and outputs a distribution\nover the labels; in other words, the model \u201coutputs\u201d a single\nlabel either blank, \u27e8b\u27e9, or one of the targets in C.\n2) Recurrent Neural Network Transducer (RNN-T): The\nRecurrent Neural Network Transducer (RNN-T) [14], [48] was\nproposed by Graves as an improvement over the basic CTC\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 3. An RNN-T Model [14], [48] consists of an encoder which transforms\nthe input speech frames into a high-level representation, and a prediction\u0002network which models the sequence of non-blank labels that have been\noutput previously. The prediction network output, pit\n, represents the output\nafter producing the previous non-blank label sequence c1, . . . , cit\n. The\njoint network produces a probability distribution over the output symbols\n(augmented with blank) given the prediction network state and a specific\nencoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 4. Example alignment sequence (right) for an RNN-T model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs. The FSA (left) represents the set of all valid RNN-T\nalignment paths.\nmodel [13], by removing some of the conditional indepen\u0002dence assumptions that we discussed previously. The RNN\u0002T model, which is depicted in Figure 3, is best understood\nby contrasting it against the CTC model. As with CTC, the\nRNN-T model augments the output symbols with the blank\nsymbol, and thus defines a distribution over label sequences\nin Cb. Similarly, as with CTC, the model consists of an encoder\nwhich processes the input acoustic frames X to generate the\nencoded representation H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ).\nUnlike CTC, however, the blank symbol in RNN-T has a\nslightly different interpretation; for each input encoder frame,\nht, the RNN-T model outputs a sequence of zero or more\nsymbols in C which are terminated by a single blank symbol.\nThus, we may define the set of all valid alignment se\u0002quences in RNN-T as: ARNNT\n(X,C) = {A = (a1, a2, \u00b7 \u00b7 \u00b7 , aT +L)},\nthe set of all sequences of T + L symbols in C\n\u2217\nb\n, which\nare identical to C after removing all blanks. Finally, for a\ngiven output position \u03c4 , let i\u03c4 denote the number of non\u0002blank labels in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121). Thus, the\nnumber of blanks in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121) is\n\u03c4 \u2212 i\u03c4 \u2212 1. For example, if T = 7, and C = (s, e, e),\nthen A = (\u27e8b\u27e9, s,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ARNNT\n(X,C)\n.\nNote that, unlike the CTC model, repeated labels in the output\nrequire no special treatment as illustrated in Figure 4, where,\ni1 = i2 = 0;i3 = i4 = 1;i10 = 3; etc.\nWe may then define the posterior probability P(C|X) as\nbefore:\nPRNNT(C|X) = X\nA\u2208ARNNT\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |a\u03c4\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |ci\u03c4\n, ci\u03c4 \u22121, . . . , c0, h\u03c4\u2212i\u03c4\n)\n(3)\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |pi\u03c4, h\u03c4\u2212i\u03c4)\nwhere, P = (p1, \u00b7 \u00b7 \u00b7 , pL) represents the output of the predic\u0002tion network depicted in Figure 3 which summarizes the se\u0002quence of previously predicted non-blank labels, implemented\nas another neural network: pj = NN(\u00b7|c0, . . . , cj\u22121), where\nc0 is a special start-of-sentence label, \u27e8sos\u27e9. Thus, as can be\nseen in Eq. (2), RNN-T reduces some of the independence\nassumptions in CTC since the output at time t is conditionally\ndependent on the sequence of previous non-blank predictions,\nbut is independent of the specific choice of alignment (i.e.,\nthe choice of the frames at which the non-blank tokens were\nemitted).\nOur presentation of RNN-T alignments considers the\n\u201ccanonical\u201d case. In principle, however, the model can encode\nthe same set of conditional independence assumptions in\nRNN-T (i.e., the model structure), while considering alter\u0002native alignment structures as in the work of [49]. In their\nwork, Moritz et al., represent valid frame-level alignments as\nan arbitrary graph. This formulation, for example, allows for\nthe use of \u201cCTC-like\u201d alignments in the RNN-T model (i.e.,\noutputting a single label \u2013 blank, or non-blank \u2013 at each frame)\nwhile conditioning on the set of previous non-blank symbols\nas in the RNN-T model.\n3) Recurrent Neural Aligner (RNA): The recurrent neural\naligner (RNA) was proposed by Sak et al. [46]. The RNA\nmodel generalizes the RNN-T model by removing one of its\nconditional independence assumptions. The model, depicted\nin Figure 5, is best understood by considering how it differs\nfrom the RNN-T model. As with CTC and RNN-T, the RNA\nmodel defines a probability distribution over blank augmented\nlabels in the set Cb, where \u27e8b\u27e9 has the same semantics\nas in the CTC model: at each frame the model can only\noutput a single label \u2013 either blank, or non-blank \u2013 before\nadvancing to the next frame; unlike CTC (but as in RNN\u0002T) the model only outputs a single instance of each non\u0002blank label. More specifically, the set of valid alignments,\nARNA\n(X,C) = (a1, \u00b7 \u00b7 \u00b7 , aT ), in the RNA model consist of length T\nsequences in C\n\u2217\nb with exactly T \u2212L blank symbols, and which\nare identical to C after removing all blanks. Thus, the blank\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 5. An RNA Model [46] resembles the RNN-T model [14], [48] in\nterms of the model structure. However, this model is only permitted to output\na single label \u2013 either blank, or non-blank \u2013 in a single frame. Unlike\nRNN-T, the prediction network state in the RNA model, qt\u22121, depends on\nthe entire alignment sequence at\u22121, . . . , a1. The joint network produces a\nprobability distribution over the output symbols (augmented with blank) given\nthe prediction network state and a specific encoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 6. Example alignment sequence (right) for an RNA model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs; diagonal transitions correspond to outputting a non-blank\nsymbol. The FSA (left) represents the set of valid alignments for the RNA\nmodel. Although the FSA is identical to the corresponding FSA for RNN-T\nin Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases.\nsymbol has a different interpretation in RNA and the RNN\u0002T models: in RNN-T, outputting a blank symbol advances\nthe model to the next frame; in RNA, however, the model\nadvances to the next frame after outputting a single blank or\nnon-blank label. Restricting the model to output a single non\u0002blank label at each frame improves computational efficiency\nand simplifies the decoding process, by limiting the number\nof model expansions at each frame (in constrast to RNN-T\ndecoding). For example, if T = 8, and C = (s, e, e), then\nA = (\u27e8b\u27e9, s,\u27e8b\u27e9, e,\u27e8b\u27e9,\u27e8b\u27e9, e,\u27e8b\u27e9) \u2208 ARNA\n(X,C)\nas illustrated\nin Figure 6.\nThe RNA posterior probability, P(C|X), is defined as:\nPRNA(C|X) = X\nA\u2208ARNA\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|qt\u22121, ht) (4)\nwhere, as before it denotes the number of non-blank sym\u0002bols in the partial alignment sequence (a1, . . . , at\u22121), and\nqt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu\u0002ral network which summarizes the entire partial alignment\nsequence, where NN(\u00b7) represents a suitable neural network\n(an LSTM in [46]). Thus, RNA removes the one remaining\nconditional independence assumption of the RNN-T model,\nby conditioning on the sequence of previous non-blank labels\nas well as the alignment that generated them. However, this\ncomes at a cost: the exact computation of the log-likelihood in\nEq. (3) (and corresponding gradients) is intractable. Instead,\nRNA makes two simplifying assumption to ensure tractable\ntraining: by assuming that the model can only output a single\nlabel at each frame; and utilizing a straight-through estimator\nfor the alignment [50]. The latter constraint \u2013 allowing only a\nsingle label (blank or non-blank) at each frame \u2013 has also been\nexplored in the context of the monotonic RNN-T model [51].\nFinally, we note that the work in [52] further generalizes\nthe RNA model by employing two RNNs when defining the\nstate: a slow RNN (which corresponds to the sequence of\npreviously predicted non-blank labels), and a fast RNN (which\nalso conditions on the frames at which the non-blank labels\nwere output).\nC. Implicit Alignment Modeling Approaches\nOne of the main benefits of the explicit alignment ap\u0002proaches such as CTC, RNN-T, or RNA is that they result in\nASR models that are easily amenable to frame-synchronous\ndecoding6In this section, we discuss the attention-based\nencoder-decoder (AED) models (also known as, listen-attend\u0002and-spell (LAS)) [15], [16], [53], which employs the attention\nmechanism [43] to implicitly identify and model the portions\nof the input acoustics which are relevant to each output\nunit. These models were first popularized in the context of\nmachine translation [54]. Unlike explicit alignment modeling\napproaches, attention-based encoder-decoder models use an\nattention mechanism [43] to learn a correspondence between\nthe entire acoustic sequence and the individual labels. Such\nmodels support label-synchronous decoding, meaning that\nduring inference, each hypothesis in the beam is expanded\nby 1 label.\nIn the explicit alignment approaches presented in Sec\u0002tion III-B, during inference, the model continues to output\nsymbols until it has processed the final frame at which point\nthe decoding process is complete; similarly, during training,\nthe forward-backward algorithm aligns over all possible align\u0002ment sequences. Since an AED model processes the entire\nacoustic sequence at once, the model needs a mechanism\nby which it can indicate that it is done emitting all output\nsymbols. This is achieved by augmenting the set of outputs\nwith an end-of-sentence symbol, \u27e8eos\u27e9, so that the output\nvocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus,\nthe AED model, depicted in Figure 7, consists of an en\u0002coder network \u2013 which encodes the input acoustic frame\n6 By frame-synchronous decoding, we refer to the ability of the model to\nproduce output label for each input frame of speech. Models such as CTC,\nRNN-T, or RNA, support frame-synchronous decoding.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nSoftmax\nDecoder\nAttention\nEncoder H(X)\nFig. 7. An attention-based encoder decoder (AED) model [15], [16], [53].\nThe output distribution is conditioned on the decoder state, si (which\nsummarizes the previously decoded symbols), and the context vector, vi\n(which summarizes the encoder output based on the decoder state). In the\nseminal work of Chan et al., [16], for example, this is accomplished by\nconcatenating the two vectors, as denoted by the L symbol in the figure.\nsequence, X = (x1, . . . , xT\u2032 ), into a higher-level representa\u0002tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder\nwhich defines the probability distribution over the set of output\nsymbols, Ceos. Thus, given a paired training example, (X, C),\nwe denote by Ce = (c1, . . . , cL,\u27e8eos\u27e9), the ground-truth\nsymbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9\nsymbol. AED models compute the conditional probability of\nthe output sequence augmented with the \u27e8eos\u27e9 symbol as:\nP(Ce|X) = P(Ce|H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, vi)\n=\nL\nY\n+1\ni=1\nP(ci|si, vi) (5)\nwhere, vi corresponds to a context vector, which summarizes\nthe relevant portions of the encoder output, H(X), given\nthe sequence of previous predictions ci\u22121, . . . , c0; and, si\ncorresponds to the corresponding decoder state after outputting\nthe sequence of previous symbols, which is produced by\nupdating the decoder state based on the previous context vector\nand output label:\nsi = Decoder(vi\u22121, si\u22121, ci\u22121)\nThe symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol\nwhich serves as the first input to the attention-based decoder\nbefore it has produced any outputs. As can be seen in Eq. (5),\nan important benefit of AED models over models such as\nCTC or RNN-T is that they do not make any independence\nassumptions between model outputs and the input acoustics,\nTime\ns\ne\ne\nFig. 8. Unlike models such as RNN-T or CTC, AED models do not have\nexplicit alignment. However, it is possible to interpret the attention weights\n\u03b1t,i for a particular output symbol ci as an alignment weight which is\nrepresented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this\nrepresentation, the size of the circle and the darkness level are proportional\nto the corresponding attention weights; thus the total probability mass is the\nsame for each row. As illustrated above, the first few frames correspond to\nthe first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019:\nc3 = e.\nand are thus more general than the implicit alignment models,\nwhile being considerably easier to train and implement since\nwe do not have to explicitly marginalize over all possible\nalignment sequences. However, this comes at a cost: previ\u0002ously generated context vectors (which are analogous to the\ndecoded partial alignment in explicit alignment models) are\nnot revised as the decoding proceeds. Stated another way,\nwhile the encoder processing H(X) might be bi-directional,\nthe decoding process in AED models reveals a left-right\nasymmetry [55].\n1) Computing the Context Vector in AED Models: As\nwe mentioned before, the context vector, vi, is computed\nby employing the attention mechanism [43]. The central\nidea behind these approaches is to define a state vector si\nwhich corresponds to the state of the model after outputting\nc1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then\ndefines a score between the model state after outputting\ni \u2212 1 previous symbols, and each of the encoded frames in\nH(X). These scores can then be normalized using the softmax\nfunction to define a set of weights corresponding to each ht\nas:\n\u03b1t,i =\nexp {atten(ht, si)}\nPT\nt\n\u2032=1 exp {atten(ht\n\u2032 , si)}\nIntuitively, the weight \u03b1t,i represents the relevance of a\nparticular encoded frame ht when outputting the next symbol\nci, after the model has already output the symbols c1, . . . , ci\u22121,\nas illustrated in Figure 8. The context vector summarizes the\nencoder output based on the computed attention weights:\nvi =\nX\nt\n\u03b1t,iht\nA number of possible attention mechanisms have been\nexplored in the literature: the most common forms are called\n\u2018content-based attention\u2019, which include dot-product atten\u0002tion [16] and additive attention [43]. The content-based atten\u0002tion computes the attention score atten(ht, si) based on the\nrelevance between ht and si. However, the score does not\nconsider location information, i.e., it is determined by only the\ncontent, independent of the position. This can lead to incorrect\nattention weights with a large discrepancy against the previous\nsteps. Thus, location-based attention atten(si,fi,j ) has been\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nproposed [15], where fi,j is a convolutional feature vector\nextracted from \u03b1i\u22121, the attention weights in the previous step.\nThe hybrid attention, i.e., a combination of the content- and\nlocation-based attentions, has also been investigated in [15],\nshowing a higher accuracy than the separate ones. Besides,\nother location-based methods use a Gaussian (mixture) model\nestimated with sito obtain attention weights [56], [57].\nTransformer model [44] uses only content-based dot-product\nattention, but also takes location information into account\nthrough positional encoding. Apart from the specific choice\nof the attention mechanism, a common technique to improve\nperformance involves the use of multiple independent attention\nheads \u2013 v\n1\ni\n, . . . , v\nK\ni\n\u2013 which are then concatenated together\nto obtain the final context vector vi =\n\u0002\nv\n1\ni\n; . . . ; v\nK\ni\n\u0003\n, in\nthe so-called multi-head attention approach [44], or indeed\nby stacking together multiple attention-based layers in the\ntransformer decoder presented by Vaswani et al. [44].\nD. From Implicit to Explicit Alignment Modeling\nAED models, which make no conditional independence\nassumptions, are extremely powerful, often outperforming\nexplicit alignment E2E approaches such as CTC, or RNN\u0002T [41]. However, these models also have some significant\ndisadvantages, most notably that the models are typically non\u0002streaming: i.e., the models must process all acoustic frames\nbefore they can generate any output hypotheses. A somewhat\nrelated issue is that the models are extremely sensitive to\nthe length of the acoustic sequences, which requires special\nprocessing to be able to decode long-form audio [58]. There\nis a body of work that lies in between these two extremes:\nmodels such as the neural transducer [59], or those based on\nmonotonic alignments [60] and its variants (e.g., monotonic\nchunkwise alignments (MoChA) [61], monotonic infinite look\u0002back (MILK) [62] etc.) use an explicit alignment model, while\nalso utilizing an attention mechanism that allows the model\nto examine local acoustics in order to refine predictions. In\nother words, this corresponds to a class of streaming AED\nmodels. Generally speaking, these models are motivated by\nthe observation that speech (unlike tasks such as machine\ntranslation) exhibits a \u2018local\u2019 relationship between the encoded\nframes (assuming that the encoder is uni-directional) and\nthe output units; thus, unlike the general AED model which\ncomputes the context vector, vi, as a sum over all input\nframes ht, the various proposed models constrain this sum\nto be computed over a subset of frames to allow for streaming\ndecoding. In the context of our presentation, it is easiest to\nthink of these models as consisting of an underlying alignment\n(whether known or unknown) which can be used to perform\nstreaming inference.\nThe Neural Transducer (NT) [59] explicitly partitions the in\u0002put encoder frames into T W non-overlapping chunks of length\nW: HW\n1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; HW\nT W = [hT W +1, . . . , hT W W ],\nwhere T W =\n\u0006\nT\nW\n\u0007\n, and ht = 0 if t > T. Unlike the AED\nmodel which examines all encoded frames when computing\nthe context vector, the NT model is restricted to process\na single chunk at a time; the model only advances to the\nnext chunk when it outputs a special end-of-chunk symbol\n(analogous to \u27e8eos\u27e9 in the AED model); inference in the model\nterminates when the model has output the end-of-chunk sym\u0002bol in the final chunk HW\nT W . If the alignments of the ground\u0002truth output sequence, C, with respect to the W-length chunks\nare unknown, then it is possible to train the system by using a\nrough initial alignment where symbols are distributed equally\namong the T W chunks, followed by iterative refinement by\ncomputing the most likely output alignments given the current\nmodel parameters [59] similar to forced-alignments in HMM\u0002based systems. An alternate approach [63] consists of using a\nseparate system (e.g., a classical hybrid system) to get initial\nalignments (e.g., word-level alignments), which can be used\nto assign sub-word units to the individual chunks.\nAn alternative approach, proposed by Raffel et al. [60],\nmodifies the vanilla AED model by explicitly introducing an\nalignment module which scans the encoder frames, H(X),\nfrom left-to-right to identify whether the current frame should\nbe used to emit any outputs (modeled as a Bernoulli random\nvariable). If a frame, \u03c4 , is selected, then the model produces\nan output based on the local encoder frame, h\u03c4 . The process\nis then repeated starting from the currently selected frame,\nthus allowing multiple outputs to be generated at the same\nframe. This results in a model with hard monotonic alignments\nbetween the input speech and the output labels since the\nmodels are constrained to generate outputs in a streaming fash\u0002ion. A Monotonic Chunkwise Attention (MoChA) model [61]\nimproves upon the work of Raffel et al., by allowing the model\nto generate the next output using a context vector computed\nusing attention over a local window of frames to the left of the\nselected frame \u03c4 : h\u03c4\u2212W+1, . . . , h\u03c4 . Thus, the MoChA model\nconsists of a two-level process \u2013 identifying frames where\noutput should be produced following [60], followed by an\nAED model over frames to the left of the selected frame. A\nrefinement to the MoChA model, proposed by Arivazhagan et\nal. [62] \u2013 the monotonic infinite lookback (MILK) attention\nmodel \u2013 computes the context vector over all frames to the\nleft of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step.\nAnother two-fold approach to enable streaming operation is\npresented in [64] under the term of triggered attention, where\na CTC-network is used to trigger, i.e. control the activation\nof an AED model with a limited decoder delay. We also\ndirect interested readers to studies of various attention variants:\nMerboldt et al. [65] compare a number of local monotonic\nattention variants; Zeyer et al. [66] discuss segmental attention\nvariants; Zeyer et al. [67] study the related decoding and the\nrelevance of segment length modeling, leading to improved\ngeneralization towards long sequences. Segmental attention\nmodels are related to transducer models [68]. However, seg\u0002mental E2E ASR models are not limited to be realized based\non the attention mechanism and may not only be related to a\ndirect HMM [39], but have also been shown to be equivalent\nto neural transducer modeling [40], thus even providing a clear\nrelation between duration modeling and blank probabilities.\nRelationship to Classical ASR\nIn classical ASR models, these frame-level alignments can\nbe modeled with HMMs while using generative GMMs or\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nneural networks to model the output distribution of acous\u0002tic frames; frame-level alignments to train neural network\nacoustic models may be obtained by force-alignment from a\nbase GMM-HMM systems, but direct sequence training not\nrequiring initial alignments is also possible [69].\nE2E models introduce alternative alignment modeling ap\u0002proaches to ASR. While the attention mechanism provides\na qualitatively novel approach to map acoustic observation\nsequences to label sequences, transducer approaches [13], [14],\n[46], [70] handle the alignment problem in a way that can\nbe interpreted to be similar to HMMs with a specific model\ntopology, including marginalization over alignments [71], [72],\n[73]. CTC models can also be employed in an HMM-like fash\u0002ion during decoding [74]. Moreover, transducer approaches are\nequivalent to segmental models/direct HMM [40].\nAnother prominent feature of E2E systems besides the\nalignment property is their direct character-level modeling\navoiding phoneme-based modeling and pronunciation lexica\n[19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82],\nwith some even heading for whole-word modeling [76], [30].\nHowever, character-level modeling also is viable with classical\nhybrid HMM architectures [83] and has been shown to work\neven with standard HMM models w/o neural networks [84].\nIV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E\nMODELS\nIn this section, we describe various algorithmic changes\nto vanilla E2E models which are critical in order to obtain\nimproved performance over classical ASR systems. First, we\ndescribe various ways of combining different complementary\nE2E models to improve performance. Next, we introduce\nways to incorporate context into these models to improve\nperformance on rare proper noun entities. We then describe\nimproved encoder and decoder architectures that take better\nadvantage of the many cores on specialized architectures such\nas tensor processing units (TPUs) [85]. Finally, we discuss how\nto improve the latency of the model through an integrated E2E\nendpointer.\nA. Combinations of Models\nDifferent end-to-end models are complementary, and there\nhave been numerous attempts at combining these methods.\nFor example, Watanabe et al. [86] find that attention-based\nmodels perform poorly on long or noisy utterances, mainly\nbecause the model has too much flexibility in predicting\nalignments when presented with the entire input utterance.\nIn contrast, models such as CTC \u2013 which have left-to-right\nconstraints during decoding \u2013 perform much better in these\ncases. They propose to employ a multi-task learning strategy\nwith both CTC and attention-based losses, which provides a\n5\u201314% relative improvement in word error rate over attention\u0002based models on Wall Street Journal (WSJ) and Chime tasks.\nPang et al. [87] explore combining the benefits of RNN-T\nand AED. Specifically, RNN-T decodes utterances in a left\u0002to-right fashion, which works well for long utterances. On\nthe other hand, since AED sees the entire utterance, it often\nshows improvements for utterances where surrounding context\nis needed to predict the current word, e.g., \"one dollar\nand fifty cents\" \u2192 $1.50. To combine RNN-T and\nAED, the authors propose to produce a first-pass result with\nRNN-T, that is then rescored with AED in the second-pass.\nTo reduce computation, the authors share the encoder between\nRNN-T and AED. The authors find that RNN-T + AED\nprovides a 17\u201322% relative improvement in word error rate\nover RNN-T alone on a voice search task. Other flavors\nof streaming 1st-pass following by attention-based 2nd-pass\nrescoring, such as deliberation [88], have also been explored.\nOne of the issues with such rescoring approaches is that any\npotential improvements are limited to the lattice produced by\nthe 1st-pass system. To address this, methods which run a\n2nd-pass beam search have also been explored, particularly in\nthe context of streaming ASR \u2013 e.g. cascaded encoder [89],\nY-architecture [90] and Universal ASR [91].\nB. Incorporating Context\nContextual biasing to a specific domain, including a user\u2019s\nsong names, app names and contact names, is an impor\u0002tant component of any production-level automatic speech\nrecognition (ASR) system. Contextual biasing is particularly\nchallenging in E2E models because these models typically\nretain only a small list of candidates during beam search, and\ntend to perform poorly when recognizing words that are seen\ninfrequently during training (typically named entities), which\nis the main source of biasing phrases. There have been a few\napproaches in the literature to incorporate context.\nOne approach, known as shallow-fusion contextual bias\u0002ing [92], constructs a stand-alone weighted finite state trans\u0002ducer (FST) representing the biasing phrases. The scores from\nthe biasing FST are interpolated with the scores of the E2E\nmodel during beam search, with special care taken to ensure\nwe do not over- or under-bias phrases. An alternate approach\nproposes to inject biasing phrases into the model in an all\u0002neural fashion. For example, Pundak et al. [93] represent a\nset of biasing phrases by embedding vectors. These vectors\nare fed as additional input to an attention-based model, which\ncan then choose to attend to the phrases and hence boost the\nchances of predicting the phrases. Kim and Metze [94] propose\nto bias towards dialog context. In addition, Bruguier et al. [95]\nextend [93], by leveraging phonemic pronunciations for the\nbiasing phrases when constructing phrase embeddings. Finally,\nDelcroix et al. [96] use an utterance-wise context vector like an\ni-vector computed by a pooling across frame-by-frame hidden\nstate vectors obtained from a sub network (this sub-network\nis called a sequence-summary network).\nC. Encoder and Decoder Structure\nThere have been improvements to encoder architectures\nof E2E models over time. The first end-to-end models used\nlong short-term memory recurrent neural networks (LSTMs),\nfor both the encoder and decoder. The main drawback of\nthese sequential models is that each frame depends on the\ncomputation from the previous frame, and therefore multiple\nframes cannot be batched in parallel.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nWith the improvement of hardware, specifically on-device\nEdge Tensor Processing Units (TPUs), with thousands of\ncores, architectures that can better take advantage of the\nhardware, have been explored. Such architectures include\nconvolution-based architectures, such as ContextNet [97]. The\nuse of self-attention to replace the sequential recurrence\nin LSTMs was explored in Transformers for ASR [98],\n[99], [100]. Finally, combining self-attention with convolution,\nknown as Conformer [45], or multi-layer perceptron [101],\nwas also explored. Both Transformer and Conformer have\nshown competitive performance to LSTMs on many tasks\n[102], [103].\nOn the decoder side, research for transducer models has\nshown that a large LSTM decoder can be replaced with a sim\u0002ple embedding lookup table, that attends to only a few previous\ntokens from the model [47], [104], [105], [106], [107]. This\ndemonstrates that most of the power of the E2E model is in\nthe encoder, which has been a consistent theme of both E2E\nas well as classical hybrid HMM models. However, improved\ndecoder modeling may also be effective depending on the\nspecific downstream task. Research has shown that extended\ndecoder architectures enable pre-training and adaptation of the\ndecoder using extensive text-only data, leading to accuracy\ngains [108], [109]. For example, one approach separates RNN\u0002T\u2019s prediction network into separate blank and vocabulary\nprediction (LM) components, where the LM component can\nbe trained with text data [108]. In addition, in line with the\ngrowing interest in large language models in recent years,\nresearch has also begun on solving multiple tasks, including\nspeech recognition, using only an auto-regressive, GPT-style\ndecoder [110], [111].\nD. Integrated Endpointing\nAn important characteristic of streaming speech recognition\nsystems is that they must endpoint quickly, so that the ASR\nresult can be finalized and sent to the server for the appro\u0002priate action to be performed. Endpointing is typically done\nwith an external voice-activity detector. Since endpointing is\nboth an acoustic and language model decision, recent works\nin streaming RNN-T models [112], [113] have investigated\npredicting a microphone closing token \u27e8eos\u27e9 at the end of the\nutterance \u2013 e.g., \u201cWhat\u2019s the weather \u27e8eos\u27e9\u201d. Following the\nnotation from Section III, this is done by including an \u27e8eos\u27e9\ntoken as part of the set of class labels C and encouraging\nthe model to predict this token to terminate decoding. These\nmodels have shown improved latency and WER trade-off\nby having the endpointing decision predicted as part of the\nmodel. Furthermore, [114], [115] explored using the CTC\nblank symbol for endpoint detection.\nV. TRAINING E2E MODELS\nIn general, training of E2E models follows deep learn\u0002ing schemes [116], [117], with specific consideration of the\nsequential structure and the latent alignment problem to be\nhandled in ASR. E2E ASR models may be trained end-to\u0002end, notwithstanding potential elaborate training schedules\nand extensive data augmentation. Part of the appeal of end\u0002to-end models is that they do not assume conditional in\u0002dependence between the input frames. Given a training set\nT = {(Xn, Cn)}\nN\nn=1, the training criterion L to be minimized\ncan be written as: L = \u2212\nPN\nn=1 log P(Cn|Xn) (which is\nequivalent to maximizing the total conditional log-likelihood).\nA. Alignment in Training\nE2E models such as RNN-T and CTC introduce an addi\u0002tional blank token \u27e8b\u27e9 for alignment. Therefore optimization\nimplies marginalizing across all alignments, as follows:\nLex = \u2212\nX\nN\nn=1\nX\nAn\nlog P(Cn, An|Xn)\nThis requires the forward-backward algorithm [118], [119] for\nefficient computation of the training criterion and its gradient,\nwith minor modifications for CTC, RNN-T, and RNA models,\nas well as classical (full-sum) hybrid ANN/HMMs correspond\u0002ing to the differences in alignments defined in each of these\nmodels. In comparison, AED models are based on implicit\nalignment modeling approaches, and the training criterion does\nnot have a latent variable A for explicit alignment as:\nLim = \u2212\nX\nN\nn=1\nlog P(Cn|Xn)\nWe refer the interested reader to the individual papers for\nfurther details on the training algorithms [13], [14], [15], [16],\n[46], [48], [53], [71], [120]. As shown in Section III-A, in both\nexplicit and implicit alignment cases, P(C|X) is factorized\nwith respect to input time t and output position i, respectively,\nand the factorized distribution is conditioned on the label\ncontext c\ni\u22121\n1\n, except for CTC. For example, in the AED case:\nlog P(C|X) = PL\ni=1 log P(ci\n|X, ci\u22121\n1\n). During training, we\nuse a teacher-forcing technique where the ground truth history\nis used as a label context.\nAs part of the training procedure, all E2E as well as classical\nhidden Markov models for ASR provide mechanisms to solve\nthe underlying sequence alignment problem - either explicitly\nvia corresponding latent variables, as in CTC, RNN-T or RNA,\nand also hybrid ANN/HMM, or implicitly, as in AED models.\nAlso, the distinction between speech and silence needs to be\nconsidered, which may be handled explicitly by introducing\nsilence as a latent label (hybrid ANN/HMM), or implicitly\nby not labeling silence at all, as currently is the standard in\nvirtually all E2E models.\nE2E models also may take advantage of hierarchical training\nschedules. These schedules may comprise several separate\ntraining passes and explicit, initially generated alignments that\nare kept fixed for some Viterbi-style [121], [122], [123] train\u0002ing epochs before re-enabling E2E-style full-sum training that\nmarginalizes over all possible alignments. Such an alternative\napproach is employed by Zeyer et al. [52], where an initial\nfull-sum RNN-T model is used to generate an alignment and\ncontinue with framewise cross-entropy training. This greatly\nsimplifies the training process by replacing the summation\nover all possible alignments in Eq. (4) by a single term cor\u0002responding to the alignment sequence generated. Recently, a\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nsimilar procedure has been introduced in [124] also employing\nE2E models, only. In this work, CTC is used to initialize\nthe training and to generate an initial alignment, followed by\nintermediate Viterbi-style RNN-T training and final full-sum\nfine tuning, which improved convergence compared to full\u0002sum-only training approaches.\nIt is interesting to note that in contrast to the RNN-T and\nRNA label-topologies, CTC does not require alignments with\nsingle label emissions per label position. However, training\nCTC models eventually does lead to single label emissions\nper hypothesized label. An analysis of this property of CTC\ntraining which is usually called peaky behavior can be found\nin [125] and references therein. Laptev et al. [126] even\nintroduces a CTC variant without non-blank loop transitions.\nB. Training with External Language Models\nE2E ASR models generally are normalized on sequence\nlevel. Therefore, sequence training with the maximum mutual\ninformation criterion [127] is the same as standard cross en\u0002tropy/conditional likelihood training. However, once external\nlanguage models are included in the training phase, sequence\nnormalization needs to be included explicitly, leading to MMI\nsequence discriminative training. This has been exploited as\na further approach to combine E2E models with external\nlanguage models trained on text-only data during the training\nphase itself [128], [129], [130].\nC. Minimum Word Error Rate Training\nSince the objective of speech recognition is to minimize\nword error rate (WER), there has been a growing number of\nresearch studies that incorporate this into the objective function\nby minimizing the model-based expectation of the number of\nword errors, as follows:\nLmwer =\nX\nN\nn=1\nX\nC\u2032\nn\nW(Cn, C\u2032\nn\n)P(C\n\u2032\nn\n|Xn)\nwhere W(Cn, C\u2032\nn\n) is the word error count in a hypothesis\nC\n\u2032\nn given a reference Cn, and n is an index which iterates over\nthe entire training set. These methods, known as sequence\nor discriminative training, have shown great improvements\nfor classical ASR [131], [132], [133], [134], [135], and have\nsince been explored in E2E models. Typically these losses\nare constructed by running in \u2018beam-search\u2019 mode rather than\nteacher-forcing mode, and construct a loss from the errors\nmade from the candidate hypotheses in the beam. Thus, this\ntype of training first requires training the model to optimize\nP(C|X) in order to initialize the model with a good set\nof parameters to run a beam search. However, also direct\napproaches have been introduced that avoid this separation\nto train discriminatively from scratch [69], [136].\nPapers that explore penalizing word errors include, Mini\u0002mum Word Error Rate (MWER) training [137], where the loss\nfunction is constructed such that the expected number of word\nerrors are minimized. Further work includes MWER for RNN\u0002T and self-attention-T [138], as well as MWER using prefix\nsearch instead of n-best [139]. Also, there have been studies\nthat consider MWER in terms of reinforcement learning [140],\n[141]. Optimal Completion Distillation (OCD) [81] proposes\nto minimize the total edit distance using an efficient dynamic\nprogramming algorithm. Finally, another body of research with\nsequence training introduce a separate external language model\nat training time [142], which can also be done efficiently via\napproximate lattice recombination [129] and also lattice-free\napproaches [130].\nD. Pretraining\nAll E2E models as well as classical hidden Markov models\nfor ASR provide holistic models that in principle enable train\u0002ing from scratch. However, many strategies exist to initialize\nand guide the training process to reach optimal performance\nand/or to obtain efficient convergence by applying pretrain\u0002ing and model growing [143], [144]. Supervised layer-wise\npretraining has been successfully applied for classical [5],\n[145], as well as attention-based ASR models [146], which can\nbe combined with intermediate sub-sampling schemes [147],\nand model growing [148]. Pretraining approaches utilizing\nuntranscribed audio, large-scale semi-supervised data and/or\nmultilingual data [149], [150], [151], [152], [153], [154],\n[155], [156], [157], [158], [159], [160] would deserve a\nself-contained survey and they are applicable for hybrid\nDNN/HMM and E2E approaches likewise \u2013 they will not be\nfurther discussed here.\nE. Training Schedules and Curricula\nDedicated training schedules have been developed to guide\nthe optimization process and as part of that reach proper\nalignment behavior explicitly or implicitly [52], [124], [147].\nMany approaches exist for learning rate control [161], [162]:\nNewBob [163], [164] and enhancements [162]; global ver\u0002sus parameter-wise learning rate control (exponential decay,\npower decay, etc.) [165]; learning rate warm-up [44]; warm\nrestarts/cosine annealing [166]; weight decay versus gradually\ndecreasing batch size [167]; fine-tuning [168] or population\u0002based training [169]; etc. For a survey of meta learning\ncf. [170].\nSequence learning approaches also consider curriculum\nlearning [171], [172], e.g., by considering short sequences\nfirst [173], [174]; interim increase of sub-sampling [147]\ninitially more sub-sampling; or, for multi-speaker ASR training\nsort mixed speech by SNR and start with speakers of balanced\nenergy and mixed gender [175].\nF. Optimization and Regularization\nOptimization usually is based on stochastic gradient descent\n[176], with momentum [177], [178], and a number of corre\u0002sponding adaptive approaches, most prominently Adam [179]\nand variants thereof [145], [179], [180].\nInvesting more training epochs seems to provide improve\u0002ments [52, Table 8], and also averaging over epochs has been\nreported to help [102]. For a discussion of the double descent\neffect and its relation to the amount of training data, label\nnoise and early stopping cf. [181].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nRegularization strongly contributes to training performance:\ne.g., L2 and weight decay [182], [166]; weight noise [183];\nadaptive mean L1/L2 [184]; gradient noise [185]; dropout\n[186], [187], [188], layer dropout [189], [190], [191]; dropcon\u0002nect [192]; zoneout [193]; smoothing of attention scores [15];\nlabel smoothing [194]; scheduled sampling [195]; auxiliary\nloss [194], [196]; variable backpropagation through time [197],\n[198]; mixup [199]; increased frame rate [180]; or, batch\nnormalization [200].\nG. Data Augmentation\nTraining of E2E ASR models also benefit from data aug\u0002mentation methods, which might also be viewed as regu\u0002larization methods. However, their diversity and impact on\nperformance justifies a separate overview.\nMost data augmentation methods perform data perturbation\nby exploiting certain dimensions of speech signal variation:\nspeed perturbation [201], [202], vocal tract length perturbation\n[201], [203], frequency axis distortion [201], sequence noise\ninjection [204], SpecAugment [205], or semantic mask [206].\nAlso, text-only data may be used to generate data using text\u0002to-speech (TTS) on feature [207] or signal level [208]. In a\ncomparison of the effect of TTS-based data augmentation on\ndifferent E2E ASR architectures in [208], AED seemed to be\nthe only architecture that appeared to benefit significantly from\nthe TTS data.\nIn a recent study [174] and corresponding follow-up work\n[180], many of the regularization and data augmentation\nmethods listed here have been exploited jointly leading to\nstate-of-the-art performance on the Switchboard task for a\nsingle-headed AED model.\nRelationship to Classical ASR\nE2E systems attempt to define ASR models that integrate\nall knowledge sources into a single global joint model that\ndoes not utilize secondary knowledge sources and avoids the\nclassical separation into acoustic and language models. These\nglobal joint models are completely trained from scratch using\na single global training criterion based on a single kind of\n(transcribed) training data and thus require less ASR domain\u0002specific knowledge provided sufficient amounts of training\ndata are available.\nWhile standard hybrid ANN/HMM training for ASR using\nframe-wise cross entropy already is discriminative, it is not\nyet sequence discriminative, requires prior alignments and\nalso lacks consideration of an (external) language model\nduring training. However, these potential shortcomings may\nbe remedied by using sequence discriminative training criteria\n[127] and lattice-free training approaches [69].\nIn contrast to strict E2E systems, the classical ASR ar\u0002chitecture includes the use of secondary knowledge sources\nbeyond the primary training data, i.e. (transcribed) speech\naudio for acoustic model training, and textual data for language\nmodel training. Most prominently, this includes the use of a\npronunciation lexicon and the definition of a phoneme set.\nSecondary resources like pronunciation lexica may be helpful\nin low-resource scenarios. However, their generation often is\ncostly and may even introduce errors, like pronunciations from\na lexicon not reflecting the actual pronunciations observed.\nTherefore, for large enough training resources, secondary\nknowledge sources might become obsolete [209], or even\nharmful, in case of erroneous information introduced [210],\n[211].\nClassical ASR models usually are trained successively, with\nknowledge derived from models trained earlier injected into\nlater training stages, e.g. in the form of HMM state alignments.\nHowever, such approaches from classical ASR might also\nbe interpreted as specific training schedules. Initializing deep\nlearning models using HMM alignments obtained from acous\u0002tic models based on mixtures of Gaussians may be interpreted\nin this way, with the Gaussian mixtures serving as an initial\nshallow model. In classical ASR, also approaches training\ndeep neural networks from scratch while avoiding interme\u0002diate training of Gaussians has been proposed [212], [213],\n[214], also in combination with character-level modeling [83].\nAnother step towards more integrated training of classical\nsystems has been to apply discriminative training criteria\navoiding intermediate (usually lattice-based) representations of\ncompeting word sequences [215], [69], [216], [217], [136].\nThe training of classical ASR systems usually applies sec\u0002ondary objectives to solve subtasks like phonetic clustering.\nThe classification and regression trees (CART) approach is\nused to cluster triphone HMM states [27], [218]. More re\u0002cent approaches proposed clustering within a neural network\nmodeling framework, while still retaining secondary clustering\nobjectives [219], [213]. However, also in E2E approaches\nsecondary objectives are used, most prominently for subword\ngeneration, e.g. via byte-pair encoding [32]. Also, available\npronunciation lexica can be utilized indirectly for assisting\nsubword generation for E2E systems [35], [36], which are\nshown to outperform byte-pair encoding. Within classical ASR\nsystems, phonetic clustering also can be avoided completely\nby modeling phonemes in context directly [220].\nIt is interesting to observe that specifically attention-based\nencoder-decoder models require larger numbers of training\nepochs to reach high performance, e.g. for a comparison\nof systems trained on Switchboard 300h cf. Table 5 in\n[221]. Also, attention-based encoder-decoder models have\nbeen shown to suffer from low training resources [222], [223],\nwhich can be improved by a number of approaches, including\nregularization techniques [174] as well as data augmentation\nusing SpecAugment [224] and text-to-speech (TTS) [29].\nSpecAugment also is shown to improve classical hybrid HMM\nmodels [225]. TTS on the other hand considerably improved\nattention-based encoder-decoder models trained on limited\nresources, but did not reach the performance of other E2E\napproaches or hybrid HMM models, which in turn were not\nconsiderably improved by TTS [208]. Multilingual approaches\nalso help improve ASR development for low resource tasks,\nagain both for classical [226], as well as for E2E systems\n[227], [228].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nVI. DECODING E2E MODELS\nThis section describes several decoding algorithms for end\u0002to-end speech recognition. The basic decoding algorithm of\nend-to-end ASR tries to estimate the most likely sequence C\u02c6\namong all possible sequences, as follows:\nC\u02c6 = arg max\nC\u2208U\u2217\nP(C|X)\nThe following section describes how to obtain the recognition\nresult C\u02c6.\nA. Greedy Search\nThe Greedy search algorithm is mainly used in CTC, which\nignores the dependency of the output labels as follows:\nA\u02c6 =\nY\nT\nt=1\n\u0012\narg max\nat\nP(at|X)\n\u0013\nwhere at is an alignment token introduced in Section III-B1.\nThe original character sequence is obtained by converting\nalignment token sequence A\u02c6 to the corresponding token se\u0002quence C\u02c6. The argmax operation can be performed in parallel\nover input frame t, yielding fast decoding [13], [229], although\nthe lack of the output dependency causes relatively poor\nperformance than the attention and RNN-T based methods in\ngeneral.\nCTC\u2019s fast decoding is further boosted with transformer\n[44], [98], [102] and its variants [45], [103] since their entire\ncomputation across the frames is parallelized [190], [230].\nFor example, the non-autoregressive models, including Im\u0002puter [231], Mask-CTC [230], Insertion-based modeling [232],\nContinuous integrate-and-fire (CIF) [233] and other variants\n[234], [235] have been actively studied as an alternative non\u0002autoregressive model to CTC. [235] shows that CTC greedy\nsearch and its variants achieve 0.06 real-time factor (RTF)7\nby using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz. The\npaper also shows that the degradation of the non-autoregressive\nmodels from the attention/RNN-T methods with beam search\nis not extremely large (19.7% with self-conditioned CTC [234]\nversus 18.5 and 18.9% with AED and RNN-T, respectively).\nThe greedy search algorithm is also used as approximate\ndecoding for both implicit and explicit alignment modeling\napproaches, including AED, RNA, CTC, and RNN-T, as\nfollows:\nc\u02c6i = arg max\nci\nP(ci|C\u02c6\n1:i\u22121, X) for i = 1, . . . , N\na\u02c6t = arg max\nat\nP(at|A\u02c6\n1:t\u22121, X) for t = 1, . . . , T\nThe greedy search algorithm does not consider alternate\nhypotheses in a sequence compared with the beam search\nalgorithm described below. However, it is known that the\ndegradation of the greedy search algorithm is not very large\n[16], [46], especially when the model is well trained in\nmatched conditions8.\n7 The ratio of the actual decoding time to the duration of the input speech.\n8 On the other hand, in the AED models, increasing the search space does\nnot consistently improve the speech recognition performance [77], [236] \u2013 a\nfact also observed in neural machine translation [237].\nB. Beam Search\nThe beam search algorithm is introduced to approximately\nconsider a subset of possible hypotheses C\u02dc among all possible\nhypotheses U\n\u2217 during decoding, i.e., C \u2282 U \u02dc \u2217\n. A predicted\noutput sequence C\u02c6 is selected among a hypothesis subset C\u02dc\ninstead of all possible hypotheses U\n\u2217\n, i.e.,\nC\u02c6 = arg max\nC\u2208C\u02dc\nP(C|X) (6)\nThe beam search algorithm is to find a set of possible hy\u0002potheses C\u02dc, which can include promising hypotheses efficiently\nby avoiding the combinatorial explosion encountered with all\npossible hypotheses U\n\u2217\n.\nThere are two major beam search categories: 1) frame\nsynchronous beam search and 2) label synchronous beam\nsearch. The major difference between them is whether it\nperforms hypothesis pruning for every input frame t or every\noutput token i. The following sections describe these two\nalgorithms in more detail.\nC. Label Synchronous Beam Search\nSuppose we have a set of partial hypotheses up to (i \u2212 1)th\ntoken C\u02dc\n1:i\u22121. A set of all possible partial hypotheses up to ith\ntoken C1:iis expanded from C\u02dc\n1:i\u22121 as follows:\nC1:i = {(C\u02dc\n1:i\u22121, ci = c)}c\u2208U (7)\nThe number of hypotheses |C1:i| would be |C\u02dc\n1:i\u22121| \u00d7 |U|, at\nmost. The beam search algorithm prunes the low probability\nscore hypotheses from C1:i and only keeps a certain number\n(beam size \u2206) of hypotheses at i among C1:i. This pruning\nstep is represented as follows:\nC\u02dc\n1:i = NBESTC1:i\u2208C1:i P(C1:i\n|X), where |C\u02dc\n1:i\n| = \u2206 (8)\nNote that NBEST(\u00b7) is an operation to extract top \u2206 hypothe\u0002ses in terms of the probability score P(C1:i\n|X) computed from\nan end-to-end neural network, or a fusion of multiple scores\ndescribed in Section VII-B.\nIn the label synchronous beam search, the length of the out\u0002put sequence (N) is unknown. Therefore, during this pruning\nprocess, we also add the hypothesis that reaches the end of an\nutterance (i.e., predict the end of sentence symbol \u27e8eos\u27e9) to a\nset of hypotheses C\u02dc in Eq. (6) as a promising hypothesis.\nThe label synchronous beam search does not explicitly\ndepend on the alignment information; thus, it is often used\nin implicit alignment modeling approaches, including AED.\nDue to this nature, sequence hypotheses of the same length\nmight cover a completely different number of encoder frames,\nunlike the frame synchronous beam search, as pointed out by\n[40]. As a result, we observe that the scores of very short and\nlong segment hypotheses often become the same range, and\nthe beam search wrongly selects such hypotheses. [86] shows\nan example of such extreme cases, resulting in large deletion\nand insertion errors for short and long-segment hypotheses,\nrespectively. Thus, the label synchronous beam search requires\nheuristics to limit the output sequence length to avoid ex\u0002tremely long/short output sequences. Usually, the minimum\nand maximum length thresholds are determined proportionally\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nto the input frame length |X| with tunable parameters \u03c1min and\n\u03c1max as Lmin = \u230a\u03c1min|X|\u230b, Lmax = \u230a\u03c1max|X|\u230b. Although these\nare quite intuitive ways to control the length of a hypothesis,\nthe minimum and maximum output lengths depend on the\ntoken unit or type of script in each language. Another heuristic\nis to provide an additional score related to the output length\nor attention weights \u2013 e.g., a length penalty, and a coverage\nterm [77], [238]. The end-point detection [239] is also used to\nestimate the hypothesis length automatically. [236] redefines\nthe implicit length model of the attention decoder to take into\naccount beam search, resulting in consistent behavior without\ndegradation for increasing beam sizes.\nNote that there are several studies on applying label syn\u0002chronous beam search to explicit alignment modeling ap\u0002proaches. For example, label synchronous beam search al\u0002gorithms for CTC are realized by marginalizing all possible\nalignments for each label hypothesis [13]. [240] extends\nCIF [233] to produce label-level encoder representation and\nrealizes label synchronous beam search in RNN-T.\nD. Frame Synchronous Beam Search\nIn contrast to the label synchronous case in Eq. (8), the\nframe synchronous beam search algorithm performs pruning\nat every input frame t, as follows:\nC\u02dc\n1:i(t) = NBESTC1;i(t) P(C1;i(t)\n|X), where |C\u02dc\n1:i(t)\n| = \u2206\nwhere C1;i(t)is an i(t)-length label sequence obtained from\nthe alignment A1:t, which is introduced in Sec. III-B.\nP(C1;i(t)|X) is obtained by summing up all possible align\u0002ments A1:t \u2208 A(X,C1;i(t))\n. Unlike the label synchronous beam\nsearch, frame synchronous beam search depends on explicit\nalignment A; thus, it is often used for explicit alignment\nmodeling approaches, including CTC, RNN-T, and RNA.\nC1:i(t)is an expanded partial hypotheses up to input frame\nt, similar to Eq. (7).\nCompared with the label synchronous algorithm, the frame\nsynchronous algorithm needs to handle additional output to\u0002ken transitions inside the beam search algorithm. The frame\nsynchronous algorithm can be easily extended in online and/or\nstreaming decoding, thanks to the explicit alignment informa\u0002tion with input frame and output token.\nClassical approaches to beam search for HMM, but also\nCTC and RNN-T variants, are based on weighted finite state\ntransducers (WFST) [38], [74], [241] or lexical prefix trees\n[106], [242], [243]. They are categorized as frame synchronous\nbeam search. These methods are often combined with an N\u0002gram language model or a full-context neural language model\n[244], [245]. RNN-T [14], [246] and CTC prefix search [247]\ncan deal with a neural language model by incorporating the\nlanguage model score in the label transition state. Interestingly,\ntriggered attention approaches [248], [249] allow us to use\nimplicit alignment modeling approaches, including AED, in\nframe-synchronous beam search together with CTC and neural\nLM, which applies on-the-fly rescoring to the hypotheses given\nby CTC prefix search using the AED and LM scores.\nE. Block-wise Decoding\nAnother beam search implementation uses a fixed-length\nblock unit for the input feature. In this block processing, we\ncan use the future context inside the block by using the non\u0002causal encoder network based on the BLSTM, output-delayed\nunidirectional LSTM, or transformer (and its variants). This\nfuture context information avoids the degradation of the fully\ncausal network. In this setup, the chunk size becomes the\ntrade-off of controlling latency and accuracy. This technique is\nused in both RNN-T [100], [250], [251] and AED [61], [252],\n[253], [254]. Block-wise processing is especially important for\nimplicit alignment modeling approaches, including AED, since\nit can provide block-wise monotonic alignment constraint\nbetween the input feature and output label, and realize block\u0002wise streaming decoding.\nF. Model Fusion during Decoding\nSimilar to the classical HMM-based beam search, we com\u0002bine various scores obtained from different modules, including\nthe main end-to-end ASR and LM scores.\n1) Synchronous Score Fusion: The most simple score fu\u0002sion is performed when the scores of multiple modules are\nsynchronized. In this case, we can simply add the multiple\nscores at each frame t or label i. The most well-known score\ncombination is LM shallow fusion.\nLM shallow fusion: As discussed in Sec. VII, various neural\nLMs can be integrated with end-to-end ASR. The most simple\nintegration is based on LM shallow fusion [255][256][257], as\ndiscussed in Sec. VII-B1, which (log-) linearly adds the LM\nscore Plm(C1:i) to E2E ASR scores P(C1:i|X) during beam\nsearch in Eq. (8) as follows:\nlog P(C1:i|X) \u2192 log P(C1:i|X) + \u03b3 log Plm(C1:i)\nwhere \u03b3 is a language model weight. Of course, we can\ncombine other scores, such as the length penalty and coverage\nterms, as discussed in Sec. VI-C.u\n2) Asynchronous Score Fusion: If we combine the frame\u0002dependent score functions, P(at|\u00b7), used in explicit alignment\nmodeling approaches, e.g., CTC, RNN-T, and label-dependent\nscore functions, P(ci|\u00b7), used implicit alignment modeling\napproaches, e.g., AED, language model, we have to deal with\nthe mismatch between the frame and label time indices t and\ni, respectively.\nIn the time-synchronous beam search, this fusion is per\u0002formed by incorporating the language model score in the\nlabel transition state [70], [22], [258]. [247] also combines\na word-based language model and token-based CTC model\nby incorporating the language model score triggered by the\nword delimiter (space) symbol.\nIn the label-synchronous beam search, we first compute\nthe label-dependent scores from the frame-dependent score\nfunction by marginalizing all possible alignments given a\nhypothesis label sequence. CTC/attention joint decoding [86]\nis a typical example, where the CTC score is computed\nby marginalizing all possible alignments based on the CTC\nforward algorithm [229]. This approach eliminates the wrong\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nalignment issues and difficulties of finding the correct end of\nsentences in the label-synchronous beam search [86].\nNote that the model fusion method during beam search can\nrealize simple one-pass decoding, while it limits the time unit\nof the models to be the same or it requires additional dynamic\nprogramming to adjust the different time units, especially for\nthe label-synchronous beam search. This dynamic program\u0002ming computation becomes significantly large when the length\nof the utterance becomes larger and requires some heuristics\nto reduce the computational cost [259].\nG. Lexical Constraint during Score Fusion\nClassically, we use a word-based language model to cap\u0002ture the contextual information with the word unit, and also\nconsider the word-based lexical constraint for ASR. However,\nend-to-end ASR often uses a letter or token unit and it causes\nfurther unit mismatch during beam search. As described in\nprevious sections, the classical approach of incorporating the\nlexical constraint from the token unit to the word unit is\nbased on a WFST. This method first makes a TLG transducer\ncomposed of the token (T), word lexicon (L), and word-based\nlanguage transducers (G) [74]. This TLG transducer has been\nused for both CTC [74] and attention-based [53] models.\nAnother approach used in the time synchronous beam search\nis to insert the word-based language model score triggered by\nthe word delimiter (space) symbol [75]. To synchronize the\nword-based language model with a character-based end-to-end\nASR, [260] combines the word and character-based LMs with\nthe prefix tree representation, while [239], [261] uses look\u0002ahead word probabilities to predict next characters instead of\nusing the character-based LM. The prefix tree representation\nis also used for the sub-word token unit case [262], [263].\nH. Multi-pass Fusion\nThe previous fusion methods are performed during the\nbeam search, which enables a one-pass algorithm. The popular\nalternative methods are based on multi-pass algorithms where\nwe do not care about the synchronization and perform n-best or\nlattice scoring by considering the entire context within an ut\u0002terance. [16] uses the N-best rescoring techniques to integrate\na word-based language model. [55] combines forward and\nbackward searches within a multi-pass decoding framework to\ncombine bidirectional LSTM decoder networks. Recently two\u0002pass algorithms of switching different end-to-end ASR systems\nhave been investigated, including RNN-T \u2192 AED [264]; CTC\n\u2192 AED [265], [266]. This aims to provide streamed output in\nthe first pass and re-scoring with AED in the second pass to\nrefine the previous output, thus satisfying a real-time interface\nrequirement while providing high recognition performance.\nIn addition to the N-best output in the above discussion,\nthere is a strong demand for generating a lattice output\nfor better multi-pass decoding thanks to richer hypothesis\ninformation in a lattice. The lattice output can also be used\nfor spoken term detection, spoken language understanding,\nand word posteriors. However, due to the lack of Markov\nassumptions, RNN-T and AED cannot merge the hypothesis\nand cannot generate a lattice straightforwardly, unlike the\nHMM-based or CTC systems. To tackle this issue, there are\nseveral studies of modifying these models by limiting the\noutput dependencies in the fixed length (i.e., finite-history)\n[47], [267], or keeping the original RNN-T structure but\nmerging the similar hypotheses during beam search [107].\nI. Vectorization across both Hypotheses and Utterances\nWe can accelerate the decoding process by vectorizing\nmultiple hypotheses during the beam search, where we replace\nthe score accumulation steps for each hypothesis with vector\u0002matrix operations for the vectorized hypotheses. This has been\nstudied in RNN-T [22], [258], [268] and attention-based [259]\nmodels. This modification leverages the parallel computing\ncapabilities of multi-core CPUs, GPUs and TPUs, resulting in\nsignificant speedups, while enabling multiple utterances to be\nprocessed simultaneously in a batch. Major deep neural net\u0002work and end-to-end ASR toolkits support this vectorization.\nFor example, Tensorflow9[269], and FAIRESEQ10 [270] pro\u0002vide a vectorized beam search interface for a generic sequence\nto sequence task, and it can be used for attention-based end-to\u0002end ASR. End-to-end ASR toolkits including ESPnet11 [259],\nESPRESSO12[261], LINGVO [271], and, RETURNN13 [272]\nalso support the vectorized beam search algorithm.\nRelationship to Classical ASR\nOne of the most prominent properties shared between E2E\nand classical statistical ASR systems is the use of a single\u0002pass decoding strategy, which integrates all knowledge sources\ninvolved (models, components), before coming to a final\ndecision [123]. This includes the use of full label context\ndependency both for E2E systems [229], [51], [77], [273],\n[174], [262], [274], [275], as well as classical systems via full\u0002context language models [276], [244], [245], [277]. In classical\nASR systems, even HMM alignment path summation may be\nretained in search [278]. Both E2E as well as classical ASR\nsystems employ beam search in decoding. However, compared\nto classical search approaches, E2E beam search usually is\nhighly simplified with very small beam sizes around 1 to\n100 [15], [16], [77], [147]. Very small beam sizes also partly\nmask a length bias exhibited by E2E attention-based encoder\u0002decoder models [279], [280], thus trading model errors against\nsearch errors [281]. An overview of approaches to handle the\nlength bias beyond using small beam sizes in ASR is presented\nin [236].\nMany classical ASR search paradigms are based on mul\u0002tipass approaches that successively generate search space\nrepresentations applying increasingly complex acoustic and/or\nlanguage models [282], [283], [243]. However, multipass\nstrategies also are employed using E2E models, which how\u0002ever softens the E2E concept. Decoder model combination is\npursued in a two-pass approach, while even retaining latency\nconstraints as in [87]. Further multipass approaches include\nE2E adaptation approaches [284], [285], [286], [287].\n9 https://www.tensorflow.org/api docs/python/tf/contrib/seq2seq/BeamSearchDecoder\n10 https://github.com/pytorch/fairseq/blob/master/fairseq/sequence\ngenerator.py 11 https://github.com/espnet/espnet\n12 https://github.com/freewym/espresso\n13 https://github.com/rwth-i6/returnn\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nVII. LM INTEGRATION\nThis section discusses language models (LMs) used for E2E\nASR. Hybrid ASR systems have long been using a pretrained\nLM [2], whereas most end-to-end (E2E) ASR systems employ\na single E2E model that includes a network component acting\nas an LM.14 For example, the prediction network of RNN\u0002T and the decoder network of AED models take on the role\nof a LM covering label back-histories. Therefore, E2E ASR\ndoes not seem to require external LMs. Nevertheless, many\nstudies have demonstrated that external LMs help improve the\nrecognition accuracy in E2E ASR.\nThere are presumably three reasons that E2E ASR still\nrequires an external LM:\na) Compensation for poor generalization: E2E models\nneed to learn a more complicated mapping function than\nclassical modular-based models such as acoustic models. Con\u0002sequently, E2E models tend to face overfitting problems if\nthe amount of training data is not sufficient. Pretrained LMs\npotentially compensate for the less generalized predictions\nmade by E2E models.\nb) Use of external text data: E2E models need to be\ntrained using paired speech and text data, while LMs can\nbe trained with only text data. Generally, text data can be\ncollected more easily than the paired data. The training speed\nof an LM is also faster than that of E2E models for the same\nnumber of sentences. Accordingly, the LM can be improved\nmore effectively with external text data, providing additional\nperformance gain to the ASR system.\nc) Domain adaptation: Domain adaptation helps im\u0002prove recognition accuracy when the E2E model is applied\nto a specific domain. However, domain adaptation of the E2E\nmodel requires a certain amount of paired data in the target\ndomain. Also, when multiple domains are assumed, it may be\ncostly to maintain multiple E2E models for the domains the\nsystem supports. If a pretrained LM for the target domain is\navailable, it may more easily improve recognition accuracy for\ndomain-specific words and speaking styles without updating\nthe E2E model.\nThis section reviews various types of LMs used for E2E\nASR and fusion techniques to integrate LMs into E2E models.\nA. Language Models\nThe LMs provide a prior probability distribution, P(C). If\nthe sentence, C, can be decomposed into a sequence of tokens\nsuch as characters, subwords, and single words, the probability\ndistribution can be computed based on the chain rule as:\nP(C) =\nL\nY\n+1\ni=1\nP(ci|c0:i\u22121)\nwhere ci denotes the i-th token of C, and c0:i\u22121 represents\ntoken sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and\ncL+1 = \u27e8eos\u27e9.\nMost LMs are designed to provide the conditional probabil\u0002ity P(ci\n|c0:i\u22121), i.e., they are modeled to predict the next token\n14 In the simplest case of a CTC model as in Fig. 2, the included LM\ncomponent however is limited to a label prior without label context.\ngiven a sequence of the preceding tokens. We briefly review\nsuch LMs focusing on the different techniques to represent\neach token, ci, and back-history, c0:i\u22121.\n1) N-gram LM: N-gram LMs have long been used for\nASR [2]. Early E2E systems in [53], [74], [77] also employed\nan N-gram LM. The N-gram models rely on the Markov\nassumption that the probability distribution of the next token\ndepends only on the previous N\u22121 tokens, i.e., P(ci|c0:i\u22121) \u2248\nP(ci|ci\u2212N+1:i\u22121), where N is typically 3 to 5 for word-based\nmodels and higher for sub-word and character-based models.\nThe maximum likelihood estimates of N-gram probabilities\nare determined based on the counts of N sequential tokens in\nthe training data set as:\nP(ci|ci\u2212N+1:i\u22121) = K(ci\u2212N+1, . . . , ci)\nP\nci\nK(ci\u2212N+1, . . . , ci)\nwhere, K(\u00b7) denotes the count of each token sequence. Since\nthe data size is finite, it is important to apply a smoothing\ntechnique to avoid estimating the probabilities based on zero or\nvery small counts for rare token sequences. Those techniques\ncompensate the N-gram probabilities with lower order models,\ne.g., (N \u2212 1)-gram models, according to the magnitude of\nthe count [288]. However, since the N-gram probabilities\nstill rely on the discrete representation of each token and the\nhistory, they suffer from data sparsity problems, leading to\npoor generalization.\nThe advantage of the N-gram models is their simplicity,\nalthough they underperform state-of-the-art neural LMs. In the\ntraining, the main step is to just count the N tuples in the data\nset, which is required only once. During decoding, the LM\nprobabilities can be obtained very quickly by table lookup or\ncan be attached to a decoding graph, e.g., WFST, in advance.\n2) FNN-LM: The feed-forward neural network (FNN) LM\nwas proposed in [9], which estimates N-gram probabilities\nusing a neural network. The network accepts N \u2212 1 tokens,\nand predicts the next token as:\nP(ci|ci\u2212N+1:i\u22121) = softmax(Wohi + bo)\nhi = tanh(Whei + bh)\nei = concat(E(ci\u2212N+1), . . . , E(ci\u22121))\nwhere Wo and Wh are weight matrices, and bo and bh are\nbias vectors. E(y) provides an embedding vector of c, and\nconcat(\u00b7) operation concatenates given vectors 15. This model\nfirst maps each input token to an embedding space, and then\nobtains hidden vector, hi, as a context vector representing the\nprevious N \u22121 tokens. Finally, it outputs the probability distri\u0002bution of the next token through the softmax layer. Although\nthis LM still relies on the Markov assumption, it outperforms\nclassical N-gram LMs described in the previous section.\nThe superior performance of FNN-LM is primarily due to\nthe distributed representation of each token and the history.\nThe LM learns to represent token/context vectors such that\nsemantically similar tokens/histories are placed close to each\nother in the embedding space. Since this representation has a\nbetter smoothing effect than the count-based one used for N\u0002gram LMs, FNN-LM can provide a better generalization than\n15 We omit the optional direct connection from the embedding layer to the\nsoftmax layer in [9] for simplicity.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nN-gram LMs for predicting the next token. Neural network\u0002based LMs basically utilize this type of representation.\n3) RNN-LM: A recurrent neural network (RNN) LM was\nintroduced to exploit longer contextual information over N \u2212\n1 previous tokens using recurrent connections [289]. Unlike\nFNN-LM, the hidden vector is computed as:\nhi = recurrence(ei, hi\u22121)\nei = E(ci\u22121)\nwhere, recurrence(ei, hi\u22121) represents a recursive function,\nwhich accepts previous hidden vector hi\u22121 with input ei, and\noutputs next hidden vector hi. In the case of simple (Elman\u0002type) RNN, the function can be computed as\nrecurrence(e, h) = tanh(Whe + Wrh + bh)\nwhere, Wr is a weight matrix for the recurrent connection,\nwhich is applied to the previous hidden vector h. This recurrent\nloop makes it possible to hold the history information in the\nhidden vector without limiting the history to N \u2212 1 tokens.\nHowever, the history information decays exponentially as\ntokens are processed with this recursion. Therefore, currently\nstacked LSTM layers are more widely used for the recurrent\nnetwork, which have separate internal memory cells and gating\nmechanisms to keep long-range history information [290].\nWith this mechanism, RNN-LMs outperform other N-gram\u0002based models in many tasks.\n4) ConvLM: Convolutional neural networks (ConvLM)\nhave also been applied to LMs [291], [292], [293]. ConvLM\n[292] replace the recurrent connections used in RNN-LMs\nwith gated temporal convolutions. The hidden vector is com\u0002puted as\nhi =h\n\u2032\ni \u2297 \u03c3(gi)\nh\n\u2032\ni =ei\u2212k+1:i \u2217 W + b\ngi =ei\u2212k+1:i \u2217 V + c\nwhere \u2297 is element-wise multiplication, \u2217 is a temporal\nconvolution operation, and k is the patch size. \u03c3(gi) represents\na gating function of convoluted activation h\n\u2032\ni\n, and is modeled\nas a sigmoid function. W and V are matrices for convolution\nand b and c are bias vectors. The convolution and gating blocks\nare typically stacked multiple times with residual connections.\nIn [293], a ConvLM with 14 blocks has been applied for E2E\nASR. Similar to FNN-LM, ConvLM allow us to use only a\nfixed history size, but they are more parameter efficient and\neasier to utilize longer histories than the FNN-LM by stacking\nthe layers. Thus, they achieve competitive performance to that\nof RNN-LMs [292], even with the finite history consisting\nof short tokens such as characters [294]. Moreover, they are\nhighly parallelizable and thus suitable for training the model\nwith a large training data set.\n5) Transformer LM: Transformer architecture [44] has been\napplied to LMs [295] and used for ASR [102], [296], where\nthe LMs are designed as a Transformer decoder without any\ninputs from other modules such as encoders. The hidden vector\nis computed as:\nhi = FFN(h\n\u2032\ni\n) + h\n\u2032\ni\nh\n\u2032\ni = MHA(ei\n, e1:i, e1:i) + ei\nwhere FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network\nand a multi-head attention module, respectively. The multi\u0002head attention and feed-forward blocks are typically stacked\nmultiple times, e.g., 6 times [102], to obtain the final hidden\nvector. The advantage of Transformer LMs is that they can\ntake all tokens in the history into account through the self\u0002attention mechanism without summarizing them into a fixed\u0002size memory like RNN-LMs. Thus, the long history can be\nfully considered with attention to predict the next token,\nachieving better performance than RNN-LMs. However, the\ncomputational complexity increases quadratically as the length\nof the sequence. Therefore, the history length is typically\nlimited to a fixed size or within every single sentence. To\novercome this limitation, Transformer-XL [297] reuses already\ncomputed activations, which includes information on farther\nprevious tokens, and the model is trained with a truncated\nback-propagation through time (BPTT) algorithm [298]. Com\u0002pressive Transformer [299] extends this approach to utilize\neven longer contextual information by incorporating a com\u0002pression step to keep older, but important, information in a\nfixed-size memory network.\nB. Fusion Approaches\nThere are several ways to incorporate an external LM into\nE2E ASR, called LM fusion. Their purpose is to improve the\nrecognition accuracy of E2E ASR by leveraging the benefits\nof the external LM described in the first part of this section.\nHowever, there can be a mismatch in the prediction between\nthe E2E model and the LM when trained on different data\nsets, and therefore the LM may not collaborate well with\nthe E2E model. Researchers have investigated various LM\nfusion approaches to reduce the mismatch between models\nin different situations.\n1) Shallow Fusion: Shallow fusion is the most popular\napproach to combine the pretrained E2E model and LM in\nthe inference time. As we described in Sec. VI-F, shallow\nfusion simply combines the E2E and LM scores by a log\u0002linear combination as\nScore(C|X) = log P(C|X) + \u03b3 log P(C) (9)\nwhere \u03b3 is a scaling factor for the LM [255][256][257]. The\nadvantage of this approach is that it is easy and effective when\nthere are no major mismatches between the source and target\ndomains.\n2) Deep Fusion: Deep fusion [300] is an approach to\ncombine an LM with an E2E model using a joint network.\nGiven a pretrained E2E model and an LM, all the network\nparameters are fine-tuned jointly so that the models collaborate\nbetter to improve the recognition accuracy, where the joint\nnetwork is used to combine the E2E and LM states through\na gating mechanism that controls the contribution of the LM\naccording to the current state.\n3) Cold fusion: Cold fusion [301] is another approach to\ncombine a pretrained LM like deep fusion, but the E2E model\nis learned while freezing the LM parameters. Since the E2E\nmodel is aware of the LM throughout training, it learns to use\nthe LM to reduce language specific information and capture\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nonly the relevant information to map the source to the target\nsequence. This mechanism reduces the role of LM in the E2E\nmodel and alleviates the language bias of the training data.\nAccordingly, the E2E model becomes more robust to domain\nmismatches between the training data and the target domain.\nUnlike deep fusion, cold fusion makes it possible to combine\nthe E2E model with a pretrained LM for the target domain,\nimproving the recognition accuracy. Component fusion [302]\nextends cold fusion to use a pretrained LM with transcriptions\nof the training data for the E2E model, more focusing on\nreducing the bias of the training data.\n4) Internal LM Estimation: There is another approach to\nreduce language bias in training data through shallow fusion.\nThe language bias is a problem when a big domain mismatch\nexists between the source domain (training data) and the target\ndomain (test data) because the E2E model scores are strongly\ndependent on the language priors in the source domain. To\nremove such a bias from the score, we can explicitly estimate\nthe LM that represents the language priors, called Internal LM,\nand subtract the LM score from the ASR score of Eq. (9):\nScore(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C)\nwhere subscripts \u03c6 and \u03c4 indicate the source and target\ndomains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub\u0002tracting the internal LM score corresponds to approximating\nacoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d\nP\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR\nscore can be seen as a classical hybrid ASR system. Ac\u0002cordingly, the subtracted E2E model score plays a role of\nacoustic model and makes it more domain independent in\nterms of language, achieving a higher recognition accuracy\nin combination with the external LM P\u03c4 (C).\nThe density ratio method [303] trains an internal LM using\nthe transcript of the training data. Hybrid autoregressive trans\u0002ducer (HAT) [47] extends RNN-T so that the model becomes\nthe internal LM when the encoder output is eliminated, i.e., set\nto zero. This approach simplifies the framework by utilizing\nthe prediction network as the internal LM, which avoids\ntraining an additional LM and using it in the inference time.\nIn the work of [304], an approach similar to HAT has been\nproposed where the internal LM is formulated on top of\nstandard RNN-T and attention-based encoder-decoder models,\nrespectively. In [128], several techniques to estimate internal\nLMs have been proposed for AED models, where an estimated\nbias vector is fed to the LM instead of a zero vector. The bias\nvector can be estimated by averaging encoder states or context\nvectors, or by a small LSTM predicting the context vector\nbased on the decoder label context, only. These techniques to\nestimate the internal LM were also evaluated for RNN-T in\n[305].\nC. Use of Large-scale Pretrained LMs\nIn recent years, LMs trained with large-scale text data are\navailable for different NLP tasks. BERT [306] and GPT-2\n[307] are representative models based on Transformer LMs.\nSuch LMs have also been applied to E2E ASR systems in\ndifferent ways, e.g., N-best rescoring [308] and dialog context\nembedding [309].\nRelationship to Classical ASR\nThe architecture of classical ASR systems provides a sepa\u0002ration between the acoustic model and the language model.\nIn contrast to this, E2E models avoid this separation and\ndefine a joint model. While this allows for training with a\nsingle objective, it limits training of the (implicit) prior to\nthe transcriptions of the audio training data. To exploit further\ntext-only training data, usually a separate LM is combined with\nE2E models, nonetheless. However, due to the implicit prior\nof E2E models, i.e. the internal language model, combination\nwith separate language models is not straightforward and\nrequires corresponding internal language model estimation\nand compensation approaches, e.g. [303], [47], [304], [128],\n[310]. At least from the recognition accuracy perspective, it\nremains unclear, if the clear separation of acoustic modeling\nand language modeling in the classical ASR architecture is a\ndisadvantage because of separate training objectives, or rather\nan advantage, since text-only training data may be used easily.\nAlso, the language model training objective, i.e. language\nmodel perplexity, is observed to correlate well with word error\nrate [311], [312], [313], [314]. Furthermore, discriminative\napproaches to language modeling [315] may be viewed as a\nstep towards joint modeling.\nVIII. OVERALL PERFORMANCE TRENDS OF E2E\nAPPROACHES IN COMMON BENCHMARKS\nThis section summarizes various techniques with the com\u0002mon ASR benchmarks based on switchboard (SWBD) [316]\nin Figure 9 and Librispeech [317] in Figure 10 to see the\ntrajectory of the techniques developed in end-to-end ASR. We\nchoose these two databases because they are widely used in\nspeech and machine learning communities and cover sponta\u0002neous (SWBD) and read speech (Librispeech) speaking styles.\nFigures 9 and 10 show that the performance improvement\nrelative to the initial works [147], [79] based on the E2E\nmodels is significant, and the error rates of all tasks become\nless than half of the original error rates!16\nAlthough the overall trends show that the ASR performance\nhas steadily improved over time, there are several remarkable\ngains. One significant gain observed in both benchmarks in the\nmiddle of 2019 comes from the data augmentation method\nrepresented by SpecAugment [205], [206], as discussed in\nSection V-G. The subsequent gains mostly come from the\nexploration of the new neural network architectures, including\ntransformer [102], [318], conformer [45], [103], and contextnet\n[97] on top of SpecAugment, as discussed in Section IV-C.\nSuch an exploration is also performed in language modeling\nto improve the ASR performance [296], [102]. The final gain\nobserved in the Librispeech benchmark in 2021 is based\non self-supervised learning [25], [319] and semi-supervised\nlearning [320], [321]. These techniques utilize a considerable\n16 For readers who want to know the latest update of these\nbenchmarks can also check https://github.com/syhw/wer are we and\nhttps://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\nAEDAEDAEDAED\nHMM\nAEDAEDAEDAED AEDAED\nWER (%)\n0\n10\n20\n30\n40\n50\n1/1/2017 1/1/2018 1/1/2019 1/1/2020 1/1/2021\nswb chm\nFig. 9. E2E ASR performance improvement in the switchboard task. AED AED\nAEDAEDAED\ntransducer ContextNet Transducer\nCTC\nHMM\nAED transducer\nWER (%)\ntransformer\n0\n5\n10\n15\n1/1/2019 7/1/2019 1/1/2020 7/1/2020 1/1/2021 7/1/2021\ntest_clean test_other\nFig. 10. E2E ASR performance improvement in the Librispeech task.\namount of unlabeled in-domain speech data (e.g., Libri-light\n60K hours [322]).\nRelationship to Classical ASR\nSpeech recognition research has always been pushed by\ninternational evaluation campaigns (e.g. as lead by NIST)\nand corresponding benchmark tasks. The competition between\nclassical and E2E approaches is nicely reflected in the widely\nused Librispeech [317] and Switchboard [316] tasks, showing\nthat E2E models gain momentum. As shown in Figure 10,\non Librispeech, the current best-published classical hybrid\nsystems range around 2.3% (test-clean) and 4.9% (test-other)\nword error rate [323], [222], while there already are a number\nof E2E systems providing similar performance [224], [205],\n[320], [206], with some E2E systems clearly outperforming\nformer state-of-the-art results with word error rates down to\n1.8% (test-clean) and 3.7% (test-other) [324] with similar\nresults reported in [45], [97]. Merging insights from classical\nHMM-based and monotonic RNN-T provided similarly well\nresults with a limited training budget [124]. Finally, when\ntrained on Switchboard 300h, the current best result, obtained\nwith an E2E system [180] is 5.4% compared to 6.6% word\nerror rate for the best hybrid system result [325] on the\nHUB5\u201900 Switchboard test set, in Figure 9\nIX. DEPLOYMENT OF E2E MODELS\nMany of the ideas discussed in this paper have been\nexplored by various industry research labs [326], [327], [328],\n[329], [330], [331], [265], inter alia. In this section, we\nreview the development of on-device production-level systems\nat Google as a typical case study for deployment.\nThe first streaming E2E model, deployed to production,\nwas launched in 2019 for the Pixel 4 smartphone [22], [332].\nThis model used a streaming RNN-T first-pass system, while\nre-scoring first-pass hypotheses with an AED system in the\nsecond pass. In addition, FST-based contextual biasing [92]\nwas employed in the model, which was critical to obtain\naccurate results for diverse queries. This model ran on CPU\nand was much faster than real time.\nIn 2020, for the Pixel 5 smartphone [333], the system was\nimproved further to reduce user-perceived latency (i.e., the\ntime between when the user speaks, and when words appear\non the device). This included advancements such as end-to-end\nendpointing [113] to encourage faster microphone closing; as\nwell as FastEmit [91] to encourage the model to emit tokens\nearlier.\nFinally, in 2021 the model was further improved for the\nPixel 6 smartphone [334], to take advantage of the tensor\nprocessing unit (TPU) [85] on the device. Improvements\ninclude the use of conformer layers for the encoder [45]; a\nsmall embedding prediction network for the decoder [104]; a\n2-pass cascaded encoder to run a 2nd-pass beam search [89];\nand, a neural LM re-scorer to help improve accuracy long-tail\nnamed entities. This model is the best ASR system that Google\nhas released to date, both in terms of quality and latency.\nX. AREAS FOR FUTURE WORK\nCurrently, E2E models dominate the academic debate on\nASR. However, at least partly, this is not (yet?) reflected\nin the corresponding commercial deployment of E2E ASR\narchitectures. E2E models are not yet the perfect match for\nall ASR conditions and further research is needed to take full\nadvantage of the benefits of E2E modeling.\nE2E models seem to perform really well when training data\nis abundant, while not scaling well to low-resource conditions.\nSimilarly, domain change requires a flexible exchange of lan\u0002guage models, which is natural for classical ASR models based\non a separation of acoustic and language models. Ongoing\nresearch on the use of external language models in E2E models\nand internal language model estimation already is promising,\nbut can be expected to see further improvements.\nTop E2E ASR systems usually require orders of magnitude\nmore training epochs than comparable classical ASR systems,\nand further research into efficient and robust optimization and\ntraining schedules is needed.\nThe high level of integration of E2E models also involves a\nloss in modularity, which might support the explainability and\nreusability of models. Also, more efficient training schedules\nmight take advantage of modularity. One assumed advantage\nof E2E models is that everything is trained from data and\nsecondary knowledge sources (e.g. pronunciation lexica and\nphoneme sets) are avoided. However, rare events, like rare\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\nwords in ASR still provide a challenge, which needs further\nresearch.\nWith the missing separation of acoustic and language mod\u0002els, the question arises of how to exploit text-only resources in\nE2E model training - do we foresee solutions beyond training\ndata generation using TTS? We note that a number of recent\nworks have explored approaches to combine speech and text\nmodalities by attempting to implicitly or explicitly map them\ninto a shared space [159], [335], [336], [337], [338], [339],\n[340], [341]. Furthermore, high-performance E2E solutions\nexist for both discriminative problems like ASR, as well as\ngenerative problems like TTS, how can both be exploited\njointly to support semi-supervised training based on text-only\nand/or audio-only data on top of transcribed speech audio [28],\n[342]?\nFor AED architectures, we observe a length bias, which\ncomplicates the decoding process. Although many heuristics\nare known to tackle length bias in AED, we are still missing\na well-founded explanation for it, as well as a corresponding\nremedy of the original model.\nOther open research problems include speaker adaptation\nand robustness to recording conditions, especially in mismatch\nsituations. The E2E principle also provides a promising candi\u0002date to solve multichannel ASR by providing an E2E solution\njointly tackling the source separation, speaker diarization and\nspeech recognition problem [343], [26].\nFinally, we need to investigate, if E2E is a suitable guiding\nprinciple, and how different E2E ASR models relate to each\nother as well as to classical ASR approaches. The most\nimportant guiding principle of ASR research and development\nhas been performance, and ASR has been boosted strongly\nby widely used benchmark tasks and international evaluation\ncampaigns. With the current diversity of classical and E2E\nmodels, we also need to resolve the question of what con\u0002stitutes state-of-the-art in ASR today, and can we expect a\ncommon state-of-the-art ASR architecture in the future?\nXI. CONCLUSIONS\nIn this work, we presented a detailed overview of end-to\u0002end approaches to ASR. Such models, which have grown in\npopularity over the last few years, propose to use highly inte\u0002grated neural network components which allow input speech\nto be converted directly into output text sequences through\ncharacter-based output units. Thus, such models eschew the\nclassical modular ASR architecture consisting of an acoustic\nmodel, a pronunciation model, and a language model, in\nfavor of a single compact structure, and rely on the data to\nlearn effectively. These design choices enable the deployment\nof highly accurate on-device speech recognition models (see\nSection IX), but also come with a number of downsides which\nare still areas of active research (see Section X).\nFinally, we direct interested readers to Li\u2019s excellent\ncontemporaneous overview article on end-to-end ASR [344],\nwhich offers a complementary perspective to our own. In\nparticular, readers of [344] may find a more detailed exposition\non the choice of encoder structure, and the applications of\nE2E approaches to allied ASR areas (e.g., multi-speaker\nrecognition; multilingual ASR; adaptation to new application\ndomains, and speakers; etc.), which we do not cover due to\nspace limitations.\nACKNOWLEDGMENT\nThe authors would like to thank Julian Dierkes, Yifan Peng,\nZoltan T \u00b4 uske, Albert Zeyer, and Wei Zhou for their help on \u00a8\nrefining our manuscript.\nREFERENCES\n[1] T. Bayes, \u201cAn Essay Towards Solving a Problem in the Doctrine of\nChances,\u201d Philosophical Transactions of the Royal Society of London,\nvol. 53, pp. 370\u2013418, 1763.\n[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,\nMA: MIT Press, 1997.\n[3] L. R. Rabiner, \u201cA Tutorial on Hidden Markov Models and Selected\nApplications in Speech Recognition,\u201d Proc. of the IEEE, vol. 77, no. 2,\npp. 257\u2013286, Feb. 1989.\n[4] H. A. Bourlard and N. Morgan, Connectionist Speech Recognition: a\nHybrid Approach. Norwell, MA: Kluwer Academic Publishers, 1993.\n[5] F. Seide, G. Li, and D. Yu, \u201cConversational Speech Transcription Using\nContext-Dependent Deep Neural Networks,\u201d in Proc. Interspeech,\nFlorence, Italy, Aug. 2011, pp. 437\u2013440.\n[6] V. Fontaine, C. Ris, and H. Leich, \u201cNonlinear Discriminant Analysis for\nImproved Speech Recognition,\u201d in Proc. Eurospeech, Rhodes, Greece,\nSep. 1997, pp. 1\u20134.\n[7] H. Hermansky, D. Ellis, and S. Sharma, \u201cTandem connectionist Feature\nExtraction for Conventional HMM Systems,\u201d in Proc. IEEE ICASSP,\nvol. 3, Istanbul, Turkey, Jun. 2000, pp. 1635\u20131638.\n[8] M. Nakamura and K. Shikano, \u201cA Study of English Word Category\nPrediction Based on Neural Networks,\u201d in Proc. IEEE ICASSP, Glas\u0002glow, UK, May 1989, pp. 731\u2013734.\n[9] Y. Bengio, R. Ducharme, and P. Vincent, \u201cA Neural Probabilistic\nLanguage Model,\u201d in Proc. NIPS, vol. 13, Denver, CO, Nov. 2000,\npp. 932\u2013938.\n[10] H. Schwenk and J.-L. Gauvain, \u201cConnectionist Language Modeling\nfor Large Vocabulary Continuous Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 765\u2013768.\n[11] Z. Tuske, P. Golik, R. Schl \u00a8 uter, and H. Ney, \u201cAcoustic Modeling with \u00a8\nDeep Neural Networks Using Raw Time Signal for LVCSR,\u201d in Proc.\nInterspeech, Singapore, Sep. 2014, pp. 890\u2013894.\n[12] T. N. Sainath, R. J. Weiss, K. W. Wilson, A. Narayanan, M. Bacchiani,\nand A. Senior, \u201cSpeaker Location and Microphone Spacing Invariant\nAcoustic Modeling from Raw Multichannel Waveforms,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 30\u201336.\n[13] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \u201cConnection- \u00b4\nist temporal classification: labelling unsegmented sequence data with\nrecurrent neural networks,\u201d in Proc. ICML, Pittsburgh, PA, Jun. 2006,\npp. 369\u2013376.\n[14] A. Graves, \u201cSequence Transduction with Recurrent Neural Networks,\u201d\nin Proc. ICML, Edinburgh, Scotland, Jun. 2012, Workshop on Repre\u0002sentation Learning, arXiv:1211.3711.\n[15] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\n\u201cAttention-Based Models for Speech Recognition,\u201d in Proc. NIPS,\nvol. 28, Laval, Queebec, Canada, Dec. 2015, pp. 577\u2013585. `\n[16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n4960\u20134964.\n[17] P. Liang, A. Bouchard-Cot\u02c6 e, D. Klein, and B. Taskar, \u201cAn End-to- \u00b4\nEnd Discriminative Approach to Machine Translation,\u201d in Proc. ACL,\nSydney, Australia, Jul. 2006, p. 761\u2013768.\n[18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, \u201cNatural Language Processing (Almost) from Scratch,\u201d\nJournal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.\n[19] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recognition\nwith Recurrent Neural Networks,\u201d in Proc. ICML, Beijing, China, Jun.\n2014, pp. 1764\u20131772.\n[20] \u201cCambridge Dictionary,\u201d https://dictionary.cambridge.org/dictionary/\nenglish/end-to-end, accessed: 2020-02-21.\n[21] R. Pang, T. N. Sainath, R. Prabhavalkar, S. Gupta, Y. Wu, S. Zhang, and\nC.-c. Chiu, \u201cCompression of End-to-End Models,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 27\u201331.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[22] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,\nD. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shang\u0002guan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-y. Chang, K. Rao, and\nA. Gruenstein, \u201cStreaming End-to-End Speech Recognition for Mobile\nDevices,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6381\u2013\n6385.\n[23] R. Schluter and H. Ney, \u201cModel-based MCE Bound to the True Bayes\u2019 \u00a8\nError,\u201d IEEE Signal Processing Letters, vol. 8, no. 5, pp. 131\u2013133, May\n2001.\n[24] H. Ney, \u201cOn the Relationship between Classification Error Bounds\nand Training Criteria in Statistical Pattern Recognition,\u201d in Iberian\nConference on Pattern Recognition and Image Analysis (IbPRIA),\nPuerto de Andratx, Spain, Jun. 2003, pp. 636\u2013645.\n[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A\nFramework for Self-Supervised Learning of Speech Representations,\u201d\nin Proc. NeurIPS, Vancouver, BC, Canada, Dec. 2020, pp. 12 449\u2013\n12 460.\n[26] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, \u201cMIMO\u0002Speech: End-to-End Multi-Channel Multi-Speaker Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU. Sentosa, Singapore: IEEE, Dec. 2019,\npp. 237\u2013244.\n[27] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and\nRegression Trees. Belmont, CA: Taylor & Francis, 1984.\n[28] A. Tjandra, S. Sakti, and S. Nakamura, \u201cListening While Speaking:\nSpeech Chain by Deep Learning,\u201d in Proc. IEEE ASRU. Okinawa,\nJapan: IEEE, Dec. 2017, pp. 301\u2013308.\n[29] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and\nJ. Cernock \u02c7 y, \u201cSemi-Supervised Sequence-to-Sequence ASR Using \u00b4\nUnpaired Speech and Text,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3790\u20133794, arXiv:1905.01152.\n[30] H. Soltau, H. Liao, and H. Sak, \u201cNeural Speech Recognizer: Acoustic\u0002to-Word LSTM Model for Large Vocabulary Speech Recognition,\u201d in\nProc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1610.09975.\n[31] G. K. Zipf, Human Behavior and the Principle of Least Effort. Boston,\nMA: Addison-Wesley Press, 1949.\n[32] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural Machine Translation\nof Rare Words with Subword Units,\u201d in Proc. ACL, Berlin, Germany,\nAug. 2015, pp. 1715\u20131725.\n[33] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, \u201cLatent Sequence Decompo\u0002sitions,\u201d in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1610.03035.\n[34] H. Liu, Z. Zhu, X. Li, and S. Satheesh, \u201cGram-CTC: Automatic unit\nselection and target decomposition for sequence labelling,\u201d in Proc.\nICML, ser. Proceedings of Machine Learning Research, D. Precup\nand Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 2188\u20132197,\narXiv:1703.00096.\n[35] H. Xu, S. Ding, and S. Watanabe, \u201cImproving End-to-End Speech\nRecognition with Pronunciation-Assisted Sub-Word Modeling,\u201d in\nProc. IEEE ICASSP, Brighton, UK, Sep. 2019, pp. 7110\u20137114.\n[36] W. Zhou, M. Zeineldeen, Z. Zheng, R. Schluter, and H. Ney, \u201cAcoustic \u00a8\nData-Driven Subword Modeling for End-to-End Speech Recognition,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2886\u20132890.\n[37] M. Schuster and K. Nakajima, \u201cJapanese and Korean Voice Search,\u201d\nin Proc. IEEE ICASSP, Kyoto, Japan, Mar. 2012, pp. 5149\u20135152.\n[38] M. Mohri, F. Pereira, and M. Riley, \u201cWeighted Finite-State Transducers\nin Speech Recognition,\u201d Computer Speech & Language, vol. 16, no. 1,\npp. 69\u201388, 2002.\n[39] E. Beck, M. Hannemann, P. Doetsch, R. Schluter, and H. Ney, \u00a8\n\u201cSegmental Encoder-Decoder Models for Large Vocabulary Automatic\nSpeech Recognition,\u201d in Proc. Interspeech, Hyderabad, India, Sep.\n2018.\n[40] W. Zhou, A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cEquivalence \u00a8\nof Segmental and Neural Transducer Modeling: A Proof of Concept,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2891\u20132895.\n[41] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, \u201cA Comparison of Sequence-to-Sequence Models for Speech\nRecognition,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp.\n939\u2013943.\n[42] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural\nComputation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[43] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by\nJointly Learning to Align and Translate,\u201d in Proc. ICLR, San Diego,\nCA, May 2015, arXiv:1409.0473.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is All You Need,\u201d\nin Proc. NIPS, Los Angeles, CA, Dec. 2017, pp. 5998\u20136008.\n[45] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution\u0002Augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 5036\u20135040.\n[46] H. Sak, M. Shannon, K. Rao, and F. Beaufays, \u201cRecurrent Neural\nAligner: An Encoder-Decoder Neural Network Model for Sequence to\nSequence Mapping,\u201d in Proc. Interspeech, vol. 8, Stockhol, Sweden,\nAug. 2017, pp. 1298\u20131302.\n[47] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid Autore\u0002gressive Transducer (HAT),\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6139\u20136143.\n[48] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech Recognition with\nDeep Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP, Vancouver,\nBC, Canada, May 2013, pp. 6645\u20136649.\n[49] N. Moritz, T. Hori, S. Watanabe, and J. Le Roux, \u201cSequence Trans\u0002duction with Graph-Based Supervision,\u201d in Proc. IEEE ICASSP, Sin\u0002gapore, May 2022, pp. 7212\u20137216.\n[50] Y. Bengio, N. Leonard, and A. Courville, \u201cEstimating or Propagating \u00b4\nGradients through Stochastic Neurons for Conditional Computation,\u201d\nAug. 2013, arXiv:1308.3432.\n[51] A. Tripathi, H. Lu, H. Sak, and H. Soltau, \u201cMonotonic Recurrent\nNeural Network Transducer and Decoding Strategies,\u201d in Proc. IEEE\nASRU, Sentosa, Singapore, Dec. 2019, pp. 944\u2013948.\n[52] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA New Training \u00a8\nPipeline for an Improved Neural Transducer,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 2812\u20132816.\n[53] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-Based Large Vocabulary Speech Recognition,\u201d\nin Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4945\u20134949.\n[54] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, \u0141. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa,\nK. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean,\n\u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap Be\u0002tween Human and Machine Translation,\u201d Oct. 2016, arXiv:1609.08144.\n[55] M. Mimura, S. Sakai, and T. Kawahara, \u201cForward-Backward Attention\nDecoder,\u201d in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2232\u2013\n2236.\n[56] A. Graves, \u201cGenerating Sequences with Recurrent Neural Networks,\u201d\nAug. 2013, arXiv:1308.0850.\n[57] J. Hou, S. Zhang, and L.-R. Dai, \u201cGaussian Prediction Based At\u0002tention for Online End-to-End Speech Recognition,\u201d in Proc. In\u0002terspeech, Stockholm, Sweden, Aug. 2017, pp. 3692\u20133696, DOI:\n10.21437/Interspeech.2017-751.\n[58] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen,\nA. Narayanan, H. Liao, S. Zhang, A. Kannan, R. Prabhavalkar, Z. Chen,\nT. Sainath, and Y. Wu, \u201cA Comparison of End-to-End Models for Long\u0002Form Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 889\u2013896.\n[59] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,\n\u201cAn Online Sequence-to-Sequence Model Using Partial Conditioning,\u201d\nin Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 5067\u20135075.\n[60] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, \u201cOnline and\nLinear-Time Attention by Enforcing Monotonic Alignments,\u201d in Proc.\nICML, Sydney, Australia, Aug. 2017, pp. 2837\u20132846.\n[61] C.-C. Chiu and C. Raffel, \u201cMonotonic Chunkwise Attention,\u201d in Proc.\nICLR, Vancouver, Canada, Apr. 2018, arXiv:1712.05382.\n[62] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz,\nR. Pang, W. Li, and C. Raffel, \u201cMonotonic Infinite Lookback Attention\nfor Simultaneous Machine Translation,\u201d in Proc. ACL, Florence, Italy,\nJun. 2019, pp. 1313\u20131323.\n[63] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu,\nP. Nguyen, and Z. Chen, \u201cImproving the Performance of Online Neural\nTransducer Models,\u201d in Proc. IEEE ICASSP, Calgary, Alberta, Canada,\nApr. 2018, pp. 5864\u20135868.\n[64] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Brighton, England,\nMay 2019, pp. 5666\u20135670.\n[65] A. Merboldt, A. Zeyer, R. Schluter, and H. Ney, \u201cAn Analysis of Local \u00a8\nMonotonic Attention Variants,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 1398\u20131402.\n[66] A. Zeyer, R. Schluter, and H. Ney, \u201cA Study of Latent Monotonic \u00a8\nAttention Variants,\u201d Mar. 2021, arXiv:2103.16710.\n[67] A. Zeyer, R. Schmitt, W. Zhou, R. Schluter, and H. Ney, \u201cMonotonic \u00a8\nSegmental Attention for Automatic Speech Recognition,\u201d in Proc. IEEE\nSLT, Doha, Qatar, Jan. 2023, arXiv:2210.14742.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[68] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, \u201cSynchronous\nTransformers for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, arXiv:1912.02958.\n[69] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely Sequence-Trained Neural\nNetworks for ASR Based on Lattice-Free MMI,\u201d in Proc. Inter\u0002speech. San Francisco, CA: ISCA, Sep. 2016, pp. 2751\u20132755, DOI:\n10.21437/Interspeech.2016-595.\n[70] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: An End\u0002to-End Convnet-Based Speech Recognition System,\u201d Sep. 2016,\narXiv:1609.03193.\n[71] P. Haffner, \u201cConnectionist Speech Recognition with a Global MMI\nAlgorithm,\u201d in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp.\n1929\u20131932.\n[72] A. Zeyer, E. Beck, R. Schluter, and H. Ney, \u201cCTC in the Context of \u00a8\nGeneralized Full-Sum HMM Training,\u201d in Proc. Interspeech, Stock\u0002holm, Sweden, Aug. 2017, pp. 944\u2013948.\n[73] T. Raissi, W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cHMM vs. \u00a8\nCTC for Automatic Speech Recognition: Comparison Based on Full\u0002Sum Training from Scratch,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan.\n2023, arXiv:2210.09951.\n[74] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-End Speech\nRecognition Using Deep RNN Models and WFST-Based Decoding,\u201d\nin Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 167\u2013174.\n[75] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng,\n\u201cDeep Speech: Scaling up End-to-End Speech Recognition,\u201d Dec.\n2014, arXiv:1412.5567.\n[76] L. Lu, X. Zhang, and S. Renals, \u201cOn Training the Recurrent Neural\nNetwork Encoder-Decoder for Large Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n5060\u20135064.\n[77] J. Chorowski and N. Jaitly, \u201cTowards Better Decoding and Language\nModel Integration in Sequence to Sequence Models,\u201d in Proc. Inter\u0002speech, Stockhol, Sweden, Aug. 2017, pp. 523\u2013527.\n[78] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery Deep Convolutional Networks\nfor End-to-End Speech Recognition,\u201d in Proc. IEEE ICASSP, New\nOrleans, LA, Mar. 2017, pp. 4845\u20134849.\n[79] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learning\nwith Low-Level Auxiliary Tasks for Encoder-Decoder based Speech\nRecognition,\u201d in Proc. Interspeech, Stockholm, Sweden, Aug. 2017,\narXiv:1704.01631.\n[80] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti\u0002Modal Data Augmentation for End-to-End ASR,\u201d in Proc. Interspeech,\nHyderabad, India, Mar. 2018, pp. 2394\u20132398.\n[81] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Distillation\nfor Sequence Learning,\u201d in Proc. ICLR, New Orleans, LA, May 2019,\narXiv:1810.01398.\n[82] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu,\n\u201cImproving Attention Based Sequence-to-Sequence Models for End-to\u0002End English Conversational Speech Recognition,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 761\u2013765.\n[83] D. Le, X. Zhang, W. Zheng, C. Fugen, G. Zweig, and M. L. Seltzer, \u00a8\n\u201cFrom Senones to Chenones: Tied Context-Dependent Graphemes for\nHybrid Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 457\u2013464.\n[84] S. Kanthak and H. Ney, \u201cContext-Dependent Acoustic Modeling Using\nGraphemes for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 845\u2013848.\n[85] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., \u201cIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit,\u201d in Proc. of\nthe 44th Annual International Symposium on Computer Architecture,\nToronto, Ontario, Canada, Jun. 2017, pp. 1\u201312.\n[86] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, \u201cHybrid\nCTC Attention Architecture for End-to-End Speech Recognition,\u201d\nIEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8,\npp. 1240\u20131253, 2017.\n[87] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C. Chung\u0002Cheng, \u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech, Brighton, UK, May 2019, pp. 2773\u20132777.\n[88] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, \u201cDeliberation\nModel Based Two-Pass End-to-End Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 7799\u20137803.\n[89] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab\u0002havalkar, E. Variani, and T. Strohman, \u201cCascaded Encoders for Uni\u0002fying Streaming and Non-Streaming ASR,\u201d in Proc. IEEE ICASSP,\nToronto, Ontario, Canada, Jun. 2021, pp. 5629\u20135633.\n[90] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, \u201cTransformer Trans\u0002ducer: One Model Unifying Streaming and Non-Streaming Speech\nRecognition,\u201d Oct. 2020, arXiv:2010.03192.\n[91] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu, and\nR. Pang, \u201cUniversal ASR: Unify and Improve Streaming ASR with\nFull-Context Modeling,\u201d Oct. 2020, arXiv:2010.06030.\n[92] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, and\nR. Pang, \u201cShallow-Fusion End-to-End Contextual Biasing,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 1418\u20131422.\n[93] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao,\n\u201cDeep Context: End-to-end Contextual Speech Recognition,\u201d in Proc.\nIEEE SLT, Athens, Greece, Dec. 2018, pp. 418\u2013425.\n[94] S. Kim and F. Metze, \u201cDialog-Context Aware End-to-End Speech\nRecognition,\u201d in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 434\u2013\n440.\n[95] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, \u201cPhoebe:\nPronunciation-Aware Contextualization for End-to-End Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6171\u2013\n6175.\n[96] M. Delcroix, S. Watanabe, A. Ogawa, S. Karita, and T. Nakatani,\n\u201cAuxiliary Feature Based Adaptation of End-to-End ASR Systems,\u201d\nin Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2444\u20132448.\n[97] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextNet: Improving Convolutional Neural\nNetworks for Automatic Speech Recognition with Global Context,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 3610\u20133614.\n[98] L. Dong, S. Xu, and B. Xu, \u201cSpeech-Transformer: A No-Recurrence\nSequence-to-Sequence Model for Speech Recognition,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5884\u20135888.\n[99] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer Transducer: A Streamable Speech Recognition\nModel with Transformer Encoders and RNN-T Loss,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7829\u20137833.\n[100] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain,\nK. Schubert, C. Fuegen, and M. L. Seltzer, \u201cTransformer-Transducer:\nEnd-to-Snd Speech Recognition with Self-Attention,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 7829\u20137833.\n[101] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, \u201cBranchformer: Parallel\nMLP-Attention Architectures to Capture Local and Global Context for\nSpeech Recognition and Understanding,\u201d in Proc. ICML. Baltimore,\nMD: PMLR, Jul. 2022, pp. 17 627\u201317 643.\n[102] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang, \u201cA Comparative Study on Transformer\nvs RNN in Speech Applications,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 449\u2013456.\n[103] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,\nN. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei,\nW. Zhang, and Y. Zhang, \u201cRecent Developments on ESPNET Toolkit\nBoosted by Conformer,\u201d in Proc. IEEE ICASSP. Toronto, Ontario,\nCanada: IEEE, Jun. 2021, pp. 5874\u20135878.\n[104] R. Botros, T. Sainath, R. David, E. Guzman, W. Li, and Y. He, \u201cTied &\nReduced RNN-T Decoder,\u201d in Proc. Interspeech, Brno, Czechia, Sep.\n2021, pp. 4563\u20134567.\n[105] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, \u201cRNN\u0002Transducer with Stateless Prediction Network,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7049\u20137053.\n[106] W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cPhoneme Based Neural \u00a8\nTransducer for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5644\u20135648.\n[107] R. Prabhavalkar, Y. He, D. Rybach, S. Campbell, A. Narayanan,\nT. Strohman, and T. N. Sainath, \u201cLess is More: Improved RNN-T\nDecoding Using Limited Label Context and Path Merging,\u201d in Proc.\nIEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5659\u20135663.\n[108] X. Chen, Z. Meng, S. Parthasarathy, and J. Li, \u201cFactorized Neural\nTransducer for Efficient Language Model Adaptation,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8132\u20138136, arXiv:2110.01500.\n[109] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi,\nJ. Emond, T. Strohman, B. Ramabhadran, W. R. Huang et al., \u201cModular\nHybrid Autoregressive Transducer,\u201d in Proc. IEEE SLT, Doha, Qatar,\nJan. 2023, pp. 197\u2013204, https://arXiv:2210.17049.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[110] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and\nF. Wei, \u201cVioLA: Unified Codec Language Models for Speech Recog\u0002nition, Synthesis, and Translation,\u201d May 2023, arXiv:2305.16107.\n[111] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor\u0002sos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov\net al., \u201cAudioPaLM: A Large Language Model That Can Speak and\nListen,\u201d Jun. 2023, arXiv:2306.12925.\n[112] S.-Y. Chang, B. Li, and G. Simko, \u201cA Unified Endpointer Using\nMultitask and Multidomain Training,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 100\u2013106.\n[113] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and\nY. Wu, \u201cTowards Fast and Accurate Streaming End-To-End ASR,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6069\u20136073.\n[114] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, \u201cEnd-To\u0002End Automatic Speech Recognition Integrated with CTC-Based Voice\nActivity Detection,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6999\u20137003.\n[115] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, \u201cToward Stream\u0002ing ASR with Non-Autoregressive Insertion-Based Model,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3740\u20133744.\n[116] Y. Bengio, \u201cPractical Recommendations for Gradient-Based Training\nof Deep Architectures,\u201d Jun. 2012, arXiv:1206.5533.\n[117] J. Schmidhuber, \u201cDeep Learning in Neural Networks: An Overview,\u201d\nNeural Networks, vol. 61, pp. 85\u2013117, Jan. 2015, arXiv:1404.7828.\n[118] L. Baum, \u201cAn Inequality and Associated Maximization Technique in\nStatistical Estimation for Probabilistic Functions of Markov Processes,\u201d\nInequalities, vol. 3, pp. 1\u20138, 1972.\n[119] L. Rabiner and B.-H. Juang, \u201cAn Introduction to Hidden Markov Mod\u0002els,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing,\nvol. 3, no. 1, pp. 4\u201316, 1986.\n[120] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, \u201cNeural Network\u0002Gaussian Mixture Hybrid for Speech Recognition or Density Estima\u0002tion,\u201d in Proc. NIPS, vol. 4, Colorado, Dec. 1991, pp. 175\u2013182.\n[121] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton\nUniversity Press, 1957.\n[122] A. Viterbi, \u201cError Bounds for Convolutional Codes and an Asymptoti\u0002cally Optimal Decoding Algorithm,\u201d IEEE Transactions on Information\nTheory, vol. 13, pp. 260\u2013269, 1967.\n[123] H. Ney, \u201cThe Use of a One-Stage Dynamic Programming Algorithm\nfor Connected Word Recognition,\u201d IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, vol. 32, no. 2, pp. 263\u2013271, 1984.\n[124] W. Zhou, W. Michel, R. Schluter, and H. Ney, \u201cEfficient Training \u00a8\nof Neural Transducer for Speech Recognition,\u201d in Proc. Interspeech,\nIncheon, Korea, Sep. 2022, arXiv:2204.10586.\n[125] A. Zeyer, R. Schluter, and H. Ney, \u201cWhy does CTC Result in Peaky \u00a8\nBehavior?\u201d May 2021, arXiv:2105.14849.\n[126] A. Laptev, S. Majumdar, and B. Ginsburg, \u201cCTC Variations Through\nNew WFST Topologies,\u201d in Proc. Interspeech, Incheon, Korea, sep\n2022, DOI: 10.21437/interspeech.2022-10854.\n[127] X. He, L. Deng, and W. Chou, \u201cDiscriminative Learning in Sequential\nPattern Recognition \u2013 A Unifying Review for Optimization-Oriented\nSpeech Recognition,\u201d IEEE Signal Processing Magazine, vol. 25, no. 5,\npp. 14\u201336, 2008.\n[128] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schluter, and \u00a8\nH. Ney, \u201cInvestigating Methods to Improve Language Model Inte\u0002gration for Attention-Based Encoder-Decoder ASR Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Aug. 2021, pp. 2856\u20132860.\n[129] N.-P. Wynands, W. Michel, J. Rosendahl, R. Schluter, and H. Ney, \u00a8\n\u201cEfficient Sequence Training of Attention Models using Approxima\u0002tive Recombination,\u201d in Proc. IEEE ICASSP, Singapore, May 2022,\narXiv:2110.09245.\n[130] Z. Yang, W. Zhou, R. Schluter, and H. Ney, \u201cLattice-Free Sequence \u00a8\nDiscriminative Training for Phoneme-based Neural Transducers,\u201d in\nProc. IEEE ICASSP, Rhodes, Greece, Jun. 2023, arXiv:2212.04325.\n[131] V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young, \u201cMMIE\nTraining of Large Vocabulary Recognition Systems,\u201d Speech Commu\u0002nication, vol. 22, no. 4, pp. 303\u2013314, 1997.\n[132] D. Povey and P. Woodland, \u201cImproved Discriminative Training Tech\u0002niques for Large Vocabulary Continuous Speech Recognition,\u201d in Proc.\nIEEE ICASSP, Salt Lake City, UT, May 2001, pp. 45\u201348.\n[133] R. Schluter, W. Macherey, B. M \u00a8 uller, and H. Ney, \u201cComparison of \u00a8\nDiscriminative Training Criteria and Optimization Methods for Speech\nRecognition,\u201d Speech Communication, vol. 34, no. 3, pp. 287\u2013310, May\n2001, EURASIP Best Paper Award.\n[134] B. Kingsbury, \u201cLattice-Based Optimization of Sequence Classifica\u0002tion Criteria for Neural-Network Acoustic Modeling,\u201d in Proc. IEEE\nICASSP, Taipei, Taiwan, Apr. 2009, pp. 3761\u20133764.\n[135] G. Heigold, R. Schluter, H. Ney, and S. Wiesler, \u201cDiscriminative \u00a8\nTraining for Automatic Speech Recognition: Modeling, Criteria, Opti\u0002mization, Implementation, and Performance,\u201d IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 58\u201369, Nov. 2012.\n[136] W. Michel, R. Schluter, and H. Ney, \u201cComparison of Lattice-Free and \u00a8\nLattice-Based Sequence Discriminative Training Criteria for LVCSR,\u201d\nin Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1601\u20131605,\narXiv:1907.01409.\n[137] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu,\nand A. Kannan, \u201cMinimum Word Error Rate Training for Attention\u0002Based Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4839\u20134843.\n[138] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, \u201cMinimum Bayes Risk\nTraining of RNN-Transducer for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 966\u2013970, DOI:\n10.21437/Interspeech.2020-1221.\n[139] M. K. Baskar, L. Burget, S. Watanabe, M. Karafiat, T. Hori, and \u00b4\nJ. H. Cernock \u02c7 y, \u201cPromising Accurate Prefix Boosting for Sequence- `\nto-Sequence ASR,\u201d in Proc. IEEE ICASSP. Brighton, UK: IEEE,\nMay 2019, pp. 5646\u20135650.\n[140] A. Tjandra, S. Sakti, and S. Nakamura, \u201cSequence-to-Sequence ASR\nOptimization via Reinforcement Learning,\u201d in Proc. IEEE ICASSP.\nCalgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5829\u20135833.\n[141] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, \u201cSequence Training\nof Encoder-Decoder Model Using Policy Gradient for End-to-End\nSpeech Recognition,\u201d in Proc. IEEE ICASSP. Calgary, Alberta,\nCanada: IEEE, Apr. 2018, pp. 5839\u20135843.\n[142] W. Michel, R. Schluter, and H. Ney, \u201cEarly Stage LM Integration Using \u00a8\nLocal and Global Log-Linear Combination,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 3605\u20133609, arXiv:2005.10049.\n[143] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA Fast Learning Algorithm\nfor Deep Belief Nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u2013\n1554, Jul. 2006.\n[144] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy Layer\u0002Wise Training of Deep Networks,\u201d in Proc. NIPS, Barcelona, Spain,\nDec. 2006, pp. 153\u2013160.\n[145] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schluter, and H. Ney, \u00a8\n\u201cA Comprehensive Study of Deep Bidirectional LSTM RNNs for\nAcoustic Modeling in Speech Recognition,\u201d in Proc. IEEE ICASSP,\nNew Orleans, LA, Mar. 2017, pp. 2462\u20132466.\n[146] A. Zeyer, T. Alkhouli, and H. Ney, \u201cRETURNN as a Generic Flexible\nNeural Toolkit with Application to Translation and Speech Recogni\u0002tion,\u201d in Proc. ACL, Melbourne, Australia, Jul. 2018, pp. 128\u2013133.\n[147] A. Zeyer, K. Irie, R. Schluter, and H. Ney, \u201cImproved Training \u00a8\nof End-to-End Attention Models for Speech Recognition,\u201d in Proc.\nInterspeech, Hyderabad, India, Sep. 2018, pp. 7\u201311.\n[148] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA Comprehensive \u00a8\nAnalysis on Attention Models,\u201d in Proc. NIPS, Montreal, Canada, Dec.\n2018.\n[149] Y. Chung, C. Wu, C. Shen, H. Lee, and L. Lee, \u201cAudio Word2Vec:\nUnsupervised Learning of Audio Segment Representations using\nSequence-to-sequence Autoencoder,\u201d in Proc. Interspeech, San Fran\u0002cisco, CA, Sep. 2016, arXiv:1603.00982.\n[150] Y.-C. Chen, S.-F. Huang, H.-y. Lee, Y.-H. Wang, and C.-H. Shen, \u201cAu\u0002dio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised\nLearning of Audio Segmentation and Representation,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 27,\nno. 9, pp. 1481\u20131493, 2019, DOI: 10.1109/TASLP.2019.2922832.\n[151] S. Scanzio, P. Laface, L. Fissore, R. Gemello, and F. Mana, \u201cOn the\nUse of a Multilingual Neural Network Front-End,\u201d in Proc. Interspeech,\nBrisbane, Australia, Sep. 2008, pp. 2711\u20132714.\n[152] Z. Tuske, J. Pinto, D. Willett, and R. Schl \u00a8 uter, \u201cInvestigation on \u00a8\nCross- and Multilingual MLP features under matched and mismatched\nacoustical conditions,\u201d in IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, Vancouver, Canada, May 2013, pp.\n7349\u20137353.\n[153] S. Zhou, S. Xu, and B. Xu, \u201cMultilingual End-to-End Speech Recog\u0002nition with a Single Transformer on Low-Resource Languages,\u201d Jun.\n2018, arXiv:1806.05059.\n[154] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL-HLT,\nMinneapolis, MN, Jun. 2019, arXiv:1904.02210.\n[155] W. Hou, Y. Dong, B. Zhuang, L. Yang, J. Shi, and T. Shinozaki,\n\u201cLarge-scale end-to-end multilingual speech recognition and language\nidentification with multi-task learning,\u201d in Proc. Interspeech, Shanghai,\nChina, Oct. 2020, pp. 1037\u20131041.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\n[156] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Syn\u0002naeve, and R. Collobert, \u201cMassively Multilingual ASR: 50 Languages,\n1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, Shanghai, China,\nOct. 2020, arXiv:2007.03001.\n[157] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani,\nW. R. Huang, M. Ma, and J. Bai, \u201cScaling End-to-End Models for\nLarge-Scale Multilingual ASR,\u201d in Proc. IEEE ASRU, 2021, pp. 1011\u2013\n1018.\n[158] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen,\nY. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li, M. Ma, W. Chan,\nJ. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath,\nF. Beaufays, Z. Chen, Q. V. Le, C.-C. Chiu, R. Pang, and Y. Wu,\n\u201cBigSSL: Exploring the frontier of large-scale semi-supervised learning\nfor automatic speech recognition,\u201d IEEE Journal of Selected Topics\nin Signal Processing, vol. 16, no. 6, pp. 1519\u20131532, oct 2022,\narXiv:2109.13226.\n[159] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Repre\u0002sentations through Modality Matching,\u201d in Proc. Interspeech, Incheon,\nSouth Korea, Sep. 2022, arXiv:2204.03409.\n[160] A. Radford, J. W. Kim, C. McLeavey, P. Mishkin, T. Xu, G. Brockman,\nand I. Sutskever, \u201cIntroducing Whisper - Robust Speech Recognition\nvia Large-Scale Weak Supervision,\u201d Sep. 2022. [Online]. Available:\nhttps://openai.com/blog/whisper/\n[161] T. P. Vogl, J. Mangis, A. Rigler, W. Zink, and D. Alkon, \u201cAcceler\u0002ating the Convergence of the Back-Propagation Method,\u201d Biological\nCybernetics, vol. 59, no. 4, pp. 257\u2013263, 1988.\n[162] N. S. Keskar and G. Saon, \u201cA Nonmonotone Learning Rate Strategy\nfor SGD Training of Deep Neural Networks,\u201d in Proc. IEEE ICASSP.\nQueensland, Australia: IEEE, Apr. 2015, pp. 4974\u20134978.\n[163] S. Renals, N. Morgan, H. Bourlard, C. Wooters, and P. Kohn, \u201cConnec\u0002tionist Speech Recognition: Status and Prospects,\u201d ICSI, 1991, Tech.\nRep. TR-OI-070.\n[164] D. Johnson, D. Ellis, C. Oei, C. Wooters, and P. Faerber, \u201cQuickNet,\u201d\nICSI, Berkeley, 2004. [Online]. Available: http://www.icsi.berkeley.\nedu/Speech/qn.html\n[165] A. Senior, G. Heigold, M. Ranzato, and K. Yang, \u201cAn Empirical Study\nof Learning Rates in Deep Neural Networks for Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Vancouver, BC, Canada: IEEE, May 2013,\npp. 6724\u20136728.\n[166] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d\nin Proc. ICLR, New Orleans, LA, May 2019, arXiv:1711.05101.\n[167] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, \u201cDon\u2019t Decay the\nLearning Rate, Increase the Batch Size,\u201d in Proc. ICLR, New Orleans,\nLA, May 2018, arXiv:1711.00489.\n[168] J. Howard and S. Ruder, \u201cUniversal Language Model Fine-Tuning for\nText Classification,\u201d in Proc. ACL, Melbourne, Australia, Jun. 2018,\npp. 328\u2013339.\n[169] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fer\u0002nando, and K. Kavukcuoglu, \u201cPopulation Based Training of Neural\nNetworks,\u201d Nov. 2017, arXiv:1711.09846.\n[170] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta\u0002Learning in Neural Networks: A Survey,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PP, pp. 1\u201320, 2021.\n[171] J. L. Elman, \u201cLearning and Development in Neural Networks: The\nImportance of Starting Small,\u201d Cognition, vol. 48, no. 1, pp. 71\u201399,\n1993, DOI: 10.1016/0010-0277(93)90058-4.\n[172] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum\nLearning,\u201d in Proc. ICML, Montreal, Quebec, Canada, Jun. 2009, p.\n41\u201348.\n[173] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan\u0002zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen,\nJ. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley,\nL. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh,\nD. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao,\nD. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep Speech 2: End-to-End Speech\nRecognition in English and Mandarin,\u201d in Proc. ICML, New York City,\nNY, Jun. 2016, pp. 173\u2013182.\n[174] Z. Tuske, G. Saon, K. Audhkhasi, and B. Kingsbury, \u201cSingle Headed \u00a8\nAttention Based Sequence-to-Sequence Model for State-of-the-Art\nResults on Switchboard,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 551\u2013555.\n[175] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, \u201cImproving End\u0002to-End Single-Channel Multi-Talker Speech Recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 28, pp. 1385\u2013\n1394, 2020.\n[176] B. Polyak, \u201cSome Methods of Speeding up the Convergence of\nIteration Methods,\u201d USSR Computational Mathematics and Mathe\u0002matical Physics, vol. 4, no. 5, pp. 1\u201317, 1964, DOI: 10.1016/0041-\n5553(64)90137-5.\n[177] Y. Nesterov, \u201cA method of solving a convex programming problem\nwith convergence rate O( 1\nk2\n),\u201d Soviet Mathematics Doklady, vol. 27,\npp. 372\u2013376, 1983.\n[178] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, \u201cOn the Importance\nof Initialization and Momentum in Deep Learning,\u201d in Proc. ICML,\nAtlanta, GA, Jun. 2013, pp. 1139\u20131147.\n[179] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimiza\u0002tion,\u201d in Proc. ICLR, San Diego, CA, May 2015, arXiv:1412.6980.\n[180] Z. Tuske, G. Saon, and B. Kingsbury, \u201cOn the Limit of English Con- \u00a8\nversational Speech Recognition,\u201d in Proc. Interspeech, Brno, Czechia,\nSep. 2021, pp. 2062\u20132066.\n[181] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever,\n\u201cDeep Double Descent: Where Bigger Models and More Data Hurt,\u201d\nin Proc. ICLR, virtual, Apr. 2020, arXiv:1912.02292.\n[182] A. Krogh and J. Hertz, \u201cA Simple Weight Decay Can Improve\nGeneralization,\u201d in Neural Information Processing Systems (NIPS),\nDenver, CO, Dec. 1991, pp. 950\u2013957.\n[183] A. F. Murray and P. J. Edwards, \u201cEnhanced MLP Performance and\nFault Tolerance Resulting from Synaptic Weight Noise during Train\u0002ing,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 5, pp. 792\u2013\n802, Sep. 1994.\n[184] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nAdvances in Neural Information Processing Systems, vol. 24, 2011.\n[185] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach,\nand J. Martens, \u201cAdding Gradient Noise Improves Learning for Very\nDeep Networks,\u201d Nov. 2015, arXiv:1511.06807.\n[186] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, \u201cImproving Neural Networks by Preventing Co\u0002Adaptation of Feature Detectors,\u201d Jul. 2012, arXiv:1207.0580.\n[187] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification\nwith Deep Convolutional Neural Networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), vol. 25, Lake Tahoe, NV, Dec.\n2012.\n[188] Y. Gal and Z. Ghahramani, \u201cDropout as a Bayesian Approximation:\nRepresenting Model Uncertainty in Deep Learning,\u201d in Proc. ICML,\nNew York City, NY, Jun. 2016, pp. 1050\u20131059.\n[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep Net\u0002works with Stochastic Depth,\u201d in European Conference on Computer\nVision, Amsterdam, Netherlands, Oct. 2016, pp. 646\u2013661.\n[190] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Muller, and A. Waibel, \u00a8\n\u201cVery Deep Self-Attention Networks for End-to-End Speech Recogni\u0002tion,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 66\u201370.\n[191] J. Lee and S. Watanabe, \u201cIntermediate Loss Regularization for CTC\u0002Based Speech Recognition,\u201d in Proc. IEEE ICASSP, Toronto, Ontario,\nCanada, Jun. 2021, pp. 6224\u20136228.\n[192] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \u201cRegularization\nof Neural Networks using DropConnect,\u201d in Proc. ICML, 2013, pp.\n1058\u20131066.\n[193] D. Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, \u00b4\nA. Goyal, Y. Bengio, A. Courville, and C. Pal, \u201cZoneout: Regularizing\nRNNs by Randomly Preserving Hidden Activations,\u201d in Proc. ICLR,\nToulon, France, Apr. 2017, arXiv:1606.01305.\n[194] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink\u0002ing the Inception Architecture for Computer Vision,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition, Las Vegas, NV, Jun. 2016,\npp. 2818\u20132826.\n[195] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled Sampling\nfor Sequence Prediction with Recurrent Neural Networks,\u201d Proc. NIPS,\nvol. 28, Dec. 2015.\n[196] T. Trinh, A. Dai, T. Luong, and Q. Le, \u201cLearning Longer-Term Depen\u0002dencies in RNNs with Auxiliary Losses,\u201d in Proc. ICML, Stockholm,\nSweden, Jul. 2018, pp. 4965\u20134974.\n[197] R. J. Williams and J. Peng, \u201cAn Efficient Gradient-Based Algorithm\nfor On-Line Training of Recurrent Network Trajectories,\u201d IEEE Neural\nComputation, vol. 2, no. 4, pp. 490\u2013501, 1990.\n[198] S. Merity, N. S. Keskar, and R. Socher, \u201cAn Analysis of Neural\nLanguage Modeling at Multiple Scales,\u201d Mar. 2018, arXiv:1803.08240.\n[199] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, \u201cMixSpeech:\nData Augmentation for Low-resource Automatic Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021,\npp. 7008\u20137012.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n25\n[200] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift,\u201d in Proc.\nICML, Lille, France, Jul. 2015, pp. 448\u2013456.\n[201] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic Spectral Distortion for\nLow Resource Speech Recognition with Deep Neural Networks,\u201d in\nProc. IEEE ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 309\u2013\n314.\n[202] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmentation\nfor Speech Recognition,\u201d in Proc. Interspeech, Dresden, Germany, Sep.\n2015.\n[203] N. Jaitly and G. E. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nImproves Speech Recognition,\u201d in Proc. ICML, vol. 117, Jun. 2013,\np. 21.\n[204] G. Saon, Z. Tuske, K. Audhkhasi, and B. Kingsbury, \u201cSequence Noise \u00a8\nInjected Training for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Brighton, England, May 2019, pp. 6261\u20136265.\n[205] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le,\nand Y. Wu, \u201cSpecAugment on Large Scale Datasets,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 6879\u20136883.\n[206] C. Wang, Y. Wu, Y. Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao, and\nM. Zhou, \u201cSemantic Mask for Transformer Based End-to-End Speech\nRecognition,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n971\u2013975.\n[207] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo,\nand K. Takeda, \u201cBack-Translation-Style Data Augmentation for End\u0002to-End ASR,\u201d in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018,\npp. 426\u2013433.\n[208] N. Rossenbach, M. Zeineldeen, B. Hilmes, R. Schluter, and H. Ney, \u00a8\n\u201cComparing the Benefit of Synthetic Training Data for Various Au\u0002tomatic Speech Recognition Architectures,\u201d in Proc. IEEE ASRU,\nCartagena, Colombia, Dec. 2021, arXiv:2104.05379.\n[209] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen, and\nC.-C. Chiu, \u201cNo Need for a Lexicon? Evaluating the Value of\nthe Pronunciation Lexica in End-to-End Models,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5859\u20135863, DOI:\n10.1109/ICASSP.2018.8462380.\n[210] C. Wooters and A. Stolcke, \u201cMultiple-Pronunciation Lexical Modeling\nin a Speaker Independent Speech Understanding System,\u201d in Proc.\nICSLP, Yokohama, Japan, Sep. 1994, pp. 1363\u20131366.\n[211] I. McGraw, I. Badr, and J. R. Glass, \u201cLearning Lexicons From Speech\nUsing a Pronunciation Mixture Model,\u201d IEEE/ACM Trans. Audio,\nSpeech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2012.\n[212] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGMM-Free DNN\nAcoustic Model Training,\u201d in Proc. IEEE ICASSP, Florence, Italy, May\n2014, pp. 5602\u20135606, DOI: 10.1109/ICASSP.2014.6854675.\n[213] G. Gosztolya, T. Grosz, and L. T \u00b4 oth, \u201cGMM-Free Flat Start Sequence- \u00b4\nDiscriminative DNN Training,\u201d in Proc. Interspeech, N. Morgan,\nEd. San Francisco, CA: ISCA, Sep. 2016, pp. 3409\u20133413, DOI:\n10.21437/Interspeech.2016-391.\n[214] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cFlat-Start\nSingle-Stage Discriminatively Trained HMM-Based Models for ASR,\u201d\nIEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 26,\nno. 11, pp. 1949\u20131961, 2018.\n[215] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig,\n\u201cThe IBM 2004 Conversational Telephony System for Rich Transcrip\u0002tion,\u201d in Proc. IEEE ICASSP, Philadelphia, PA, Mar. 2005, pp. 205\u2013\n208.\n[216] H. Hadian, D. Povey, H. Sameti, J. Trmal, and S. Khudanpur,\n\u201cImproving LF-MMI Using Unconstrained Supervisions for ASR,\u201d\nin Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 43\u201347, DOI:\n10.1109/SLT.2018.8639684.\n[217] N. Kanda, Y. Fujita, and K. Nagamatsu, \u201cLattice-Free State-Level Min\u0002imum Bayes Risk Training of Acoustic Models,\u201d in Proc. Interspeech,\nB. Yegnanarayana, Ed. Hyderabad, India: ISCA, Sep. 2018, pp. 2923\u2013\n2927, DOI: 10.21437/Interspeech.2018-79.\n[218] S. J. Young and P. C. Woodland, \u201cThe Use of State Tying in Continuous\nSpeech Recognition,\u201d in Proc. Eurospeech, Berlin, Germany, Dec.\n1993, pp. 2203\u20132206.\n[219] S. Wiesler, G. Heigold, M. Nu\u00dfbaum-Thom, R. Schluter, and H. Ney, \u00a8\n\u201cA Discriminative Splitting Criterion for Phonetic Decision Trees,\u201d\nin Proc. Interspeech, Makuhari, Japan, Sep. 2010, pp. 54\u201357, one of\nshortlist for Best Student Paper Award.\n[220] T. Raissi, E. Beck, R. Schluter, and H. Ney, \u201cTowards Consistent \u00a8\nHybrid HMM Acoustic Modeling,\u201d Apr. 2021, arXiv:2104.02387.\n[221] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schluter, and H. Ney, \u00a8\n\u201cA Systematic Comparison of Grapheme-Based vs. Phoneme-Based\nLabel Units for Encoder-Decoder-Attention Models,\u201d Nov. 2020,\narXiv:2005.09336.\n[222] C. Luscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, \u00a8\nR. Schluter, and H. Ney, \u201cRWTH ASR Systems for LibriSpeech: \u00a8\nHybrid vs Attention,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 231\u2013235.\n[223] D. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. Le,\n\u201cImproved Noisy Student Training for Automatic Speech Recognition,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2817\u20132821.\n[224] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and\nQ. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for\nAutomatic Speech Recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 2613\u20132617.\n[225] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schluter, and H. Ney, \u201cThe \u00a8\nRWTH ASR System for TED-LIUM Release 2: Improving Hybrid\nHMM with SpecAugment,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 7839\u20137843.\n[226] J. Cui, B. Kingsbury, B. Ramabhadran, A. Sethy, K. Audhkhasi, X. Cui,\nE. Kislal, L. Mangu, M. Nussbaum-Thom, M. Picheny, Z. Tuske, \u00a8\nP. Golik, R. Schluter, H. Ney, M. J. F. Gales, K. M. Knill, A. Ragni, \u00a8\nH. Wang, and P. Woodland, \u201cMultilingual Representations for Low\nResource Speech Recognition and Keyword Search,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 259\u2013266.\n[227] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL, Min\u0002neapolis, MN, Jun. 2019, pp. 96\u2013108.\n[228] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. Ramabhadran,\nY. Wu, A. Bapna, Z. Chen, and S. Lee, \u201cLarge-Scale Multilingual\nSpeech Recognition with a Streaming End-to-End Model,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2130\u20132134.\n[229] A. Graves, \u201cConnectionist Temporal Classification,\u201d in Supervised\nSequence Labelling with Recurrent Neural Networks. Heidelberg,\nGermany: Springer, 2012, ch. Connectionist Temporal Classification,\npp. 61\u201393.\n[230] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,\n\u201cMask CTC: Non-Autoregressive End-to-End ASR with CTC and\nMask Predict,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n3655\u20133659.\n[231] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, \u201cImputer:\nSequence Modelling via Imputation and Dynamic Programming,\u201d in\nProc. ICML. PMLR, Jul. 2020, pp. 1403\u20131413.\n[232] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, \u201cInsertion-Based\nModeling for End-to-End Automatic Speech Recognition,\u201d in Proc.\nInterspeech, Shanghai, China, Oct. 2020, pp. 3660\u20133664.\n[233] L. Dong and B. Xu, \u201cCif: Continuous Integrate-and-Fire for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6079\u20136083.\n[234] J. Nozaki and T. Komatsu, \u201cRelaxing the Conditional Independence\nAssumption of CTC-Based ASR by Conditioning on Intermediate\nPredictions,\u201d in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3735\u2013\n3739.\n[235] Y. Higuchi, N. Chen, Y. Fujita, H. Inaguma, T. Komatsu, J. Lee,\nJ. Nozaki, T. Wang, and S. Watanabe, \u201cA Comparative Study on Non\u0002Autoregressive Modelings for Speech-to-Text Generation,\u201d in Proc.\nIEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2110.05249.\n[236] W. Zhou, R. Schluter, and H. Ney, \u201cRobust Beam Search for Encoder- \u00a8\nDecoder Attention Based Speech Recognition without Length Bias,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1768\u20131772.\n[237] P. Koehn and R. Knowles, \u201cSix Challenges for Neural Machine Transla\u0002tion,\u201d in First Workshop on Neural Machine Translation. Vancouver,\nBC, Canada: Association for Computational Linguistics, Aug. 2017,\npp. 28\u201339.\n[238] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, \u201cModeling Coverage for Neural\nMachine Translation,\u201d in Proc. ACL, Berlin, Germany, May 2016, pp.\n76\u201385.\n[239] T. Hori, J. Cho, and S. Watanabe, \u201cEnd-to-End Speech Recogni\u0002tion with Word-Based RNN Language Models,\u201d in Proc. IEEE SLT.\nAthens, Greece: IEEE, Dec. 2018, pp. 389\u2013396.\n[240] K. Deng and P. C. Woodland, \u201cLabel-Synchronous Neural Transducer\nfor End-to-End ASR,\u201d Jul. 2023, arXiv:2307.03088.\n[241] T. Hori and A. Nakamura, Speech Recognition Algorithms Using\nWeighted Finite-State Transducers. San Rafael, CA: Morgan &\nClaypool Publishers, 2013.\n[242] R. Haeb-Umbach and H. Ney, \u201cImprovements in Beam Search for\n10000-Word Continuous-Speech Recognition,\u201d IEEE Transactions on\nSpeech and Audio Processing, vol. 2, no. 2, pp. 353\u2013356, 1994.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n26\n[243] H. Ney and S. Ortmanns, \u201cProgress in Dynamic Programming Search\nfor LVCSR,\u201d Proc. of the IEEE, vol. 88, no. 8, pp. 1224\u20131240, Aug.\n2000, http://dx.doi.org/10.1109/5.880081.\n[244] T. Hori, Y. Kubo, and A. Nakamura, \u201cReal-Time One-Pass Decoding\nwith Recurrent Neural Network Language Model for Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6364\u2013\n6368.\n[245] E. Beck, W. Zhou, R. Schluter, and H. Ney, \u201cLSTM Language Models \u00a8\nfor LVCSR in First-Pass Decoding and Lattice-Rescoring,\u201d Jul. 2019,\narXiv:1907.01030.\n[246] G. Saon, Z. Tuske, and K. Audhkhasi, \u201cAlignment-Length Syn- \u00a8\nchronous Decoding for RNN Transducer,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7804\u20137808.\n[247] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, \u201cFirst\u0002Pass Large Vocabulary Continuous Speech Recognition Using Bi\u0002Directional Recurrent DNNs,\u201d Dec. 2014, arXiv:1408.2873.\n[248] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP. Brighton, UK:\nIEEE, May 2019, pp. 5666\u20135670.\n[249] N. Moritz, T. Hori, and J. Le, \u201cStreaming Automatic Speech Recogni\u0002tion with the Transformer Model,\u201d in Proc. IEEE ICASSP. Barcelona,\nSpain: IEEE, May 2020, pp. 6074\u20136078.\n[250] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kalgaonkar, A. Sri\u0002ram, C. Fuegen, and M. L. Seltzer, \u201cRNN-T for Latency Controlled\nASR with Improved Beam Search,\u201d Nov. 2019, arXiv:1911.01629.\n[251] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring Transformers for Large\u0002Scale Speech Recognition,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 5041\u20135045.\n[252] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, \u201cStreaming End-to\u0002End ASR Based on Blockwise Non-Autoregressive Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3755\u20133759.\n[253] H. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, \u201cOnline Hybrid\nCTC/Attention Architecture for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 2623\u20132627, DOI:\n10.21437/Interspeech.2019-2018.\n[254] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming Transformer\nASR with Blockwise Synchronous Beam Search,\u201d in Proc. IEEE SLT.\nShenzhen, China: IEEE, Jun. 2021, pp. 22\u201329.\n[255] K. Hwang and W. Sung, \u201cCharacter-level language modeling with\nhierarchical recurrent neural networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5720\u20135724.\n[256] T. Hori, S. Watanabe, Y. Zhang, and W. Chan, \u201cAdvances in Joint CTC\u0002Attention Based End-to-End Speech Recognition with a Deep CNN\nEncoder and RNN-LM,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug.\n2017, pp. 949\u2013953.\n[257] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, \u201cAn Analysis of Incorporating an External Lan\u0002guage Model into a Sequence-to-Sequence Model,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5824\u20135828, DOI:\n10.1109/ICASSP.2018.8462682.\n[258] G. Saon, Z. Tuske, D. Bolanos, and B. Kingsbury, \u201cAdvancing \u00a8\nRNN Transducer Technology for Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5654\u20135658,\narXiv:2103.09935.\n[259] H. Seki, T. Hori, S. Watanabe, N. Moritz, and J. Le Roux, \u201cVectorized\nBeam Search for CTC-Attention-Based Speech Recognition,\u201d in Proc.\nInterspeech, Brighton, UK, May 2019, pp. 3825\u20133829.\n[260] T. Hori, S. Watanabe, and J. R. Hershey, \u201cMulti-Level Language\nModeling and Decoding for Open Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec.\n2017, pp. 287\u2013293.\n[261] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie,\nS. Watanabe, and S. Khudanpur, \u201cEspresso: A Fast End-to-End Neural\nSpeech Recognition Toolkit,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 136\u2013143.\n[262] Z. Tuske, K. Audhkhasi, and G. Saon, \u201cAdvancing Sequence-to- \u00a8\nSequence Based Speech Recognition,\u201d in Proc. Interspeech, Graz,\nAustria, Sep. 2019, pp. 3780\u20133784.\n[263] J. Drexler and J. Glass, \u201cSubword Regularization and Beam Search\nDecoding for End-to-End Automatic Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 6266\u20136270.\n[264] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu,\n\u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Interspeech,\nGraz, Austria, Sep. 2019, pp. 2773\u20132777.\n[265] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen,\nL. Xie, and X. Lei, \u201cWeNet: Production Oriented Streaming and Non\u0002Streaming End-to-End Speech Recognition Toolkit,\u201d Brno, Czechia, pp.\n4054\u20134058, Sep. 2021.\n[266] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei,\n\u201cU2++: Unified Two-Pass Bidirectional End-to-End Model for Speech\nRecognition,\u201d Dec. 2021, arXiv:2106.05642.\n[267] M. Zapotoczny, P. Pietrzak, A. Lancucki, and J. Chorowski, \u201cLattice\nGeneration in Attention-Based Speech Recognition Models,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2225\u20132229.\n[268] J. Kim, Y. Lee, and E. Kim, \u201cAccelerating RNN Transducer Inference\nvia Adaptive Expansion Search,\u201d IEEE Signal Processing Letters,\nvol. 27, pp. 2019\u20132023, 2020.\n[269] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: A system for Large\u0002Scale Machine Learning,\u201d in Proc. OSDI, Savannah, GA, Nov. 2016,\npp. 265\u2013283.\n[270] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,\nand M. Auli, \u201cFAIRSEQ: A Fast, Extensible Toolkit for Sequence\nModeling,\u201d in Proc. NAACL, Minneapolis, MN, Jun. 2019, pp. 48\u201353.\n[271] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan,\nT. Sainath, Y. Cao, C.-C. Chiu et al., \u201cLingvo: a Modular and\nScalable Framework for Sequence-to-Sequence Modeling,\u201d Feb. 2019,\narXiv:1902.08295.\n[272] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schluter, and \u00a8\nH. Ney, \u201cRETURNN: The RWTH Extensible Training Framework for\nUniversal Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5345\u20135349.\n[273] A. Hannun, A. Lee, Q. Xu, and R. Collobert, \u201cSequence-to-Sequence\nSpeech Recognition with Time-Depth Separable Convolutions,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 3785\u20133789.\n[274] M. Li, M. Liu, and H. Masanori, \u201cEnd-to-End Speech Recognition with\nAdaptive Computation Steps,\u201d in Proc. IEEE ICASSP, Brighton, UK,\nMay 2019, pp. 6246\u20136250.\n[275] P. Bahar, N. Makarov, A. Zeyer, R. Schuter, and H. Ney, \u201cExploring \u00a8\na Zero-Order Direct HMM Based on Latent Attention for Automatic\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 7854\u20137858.\n[276] Z. Huang, G. Zweig, and B. Dumoulin, \u201cCache Based Recurrent\nNeural Network Language Model Inference for First Pass Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp.\n6354\u20136358.\n[277] J. Jorge, A. Gimenez, J. Iranzo-S \u00b4 anchez, J. Civera, A. Sanchis, and \u00b4\nA. Juan, \u201cReal-Time One-Pass Decoder for Speech Recognition Using\nLSTM Language Models,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3820\u20133824.\n[278] W. Zhou, R. Schluter, and H. Ney, \u201cFull-Sum Decoding for Hybrid \u00a8\nHMM Based Speech Recognition Using LSTM Language Model,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7834\u20137838.\n[279] P. Sountsov and S. Sarawagi, \u201cLength Bias in Encoder Decoder Models\nand a Case for Global Conditioning,\u201d in Proc. EMNLP, Austin, TX,\nNov. 2016, pp. 1516\u20131525.\n[280] K. Murray and D. Chiang, \u201cCorrecting Length Bias in Neural Machine\nTranslation,\u201d in Proc. WMT, Brussels, Belgium, Oct. 2018, pp. 212\u2013\n223.\n[281] F. Stahlberg and B. Byrne, \u201cOn NMT Search Errors and Model Errors:\nCat Got Your Tongue?\u201d in Proc. EMNLP. Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 3354\u20133360.\n[282] N. Deshmukh, A. Ganapathiraju, and J. Picone, \u201cHierarchical Search\nfor Large-Vocabulary Conversational Speech Recognition: Working\nToward a Solution to the Decoding Problem,\u201d IEEE Signal Processing\nMagazine, vol. 16, no. 5, pp. 84\u2013107, 1999.\n[283] L. Nguyen and R. Schwartz, \u201cSingle-Tree Method for Grammar\u0002Directed Search,\u201d in Proc. IEEE ICASSP, vol. 2, Phoenix, AZ, Mar.\n1999, pp. 613\u2013616.\n[284] L. Sar\u0131, N. Moritz, T. Hori, and J. Le Roux, \u201cUnsupervised Speaker\nAdaptation using Attention-based Speaker Memory for End-to-End\nASR,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 2\u2013\n6.\n[285] F. Weninger, J. Andres-Ferrer, X. Li, and P. Zhan, \u201cListen, Attend, \u00b4\nSpell and Adapt: Speaker Adapted Sequence-to-Sequence ASR,\u201d in\nProc. Interspeech. Graz, Austria: ISCA, Sep. 2019, pp. 3805\u20133809.\n[286] Z. Meng, Y. Gaur, J. Li, and Y. Gong, \u201cSpeaker Adaptation for\nAttention-Based End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech. Graz, Austria: ISCA, Sep. 2019, pp. 241\u2013245.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n27\n[287] N. Tomashenko and Y. Esteve, \u201cEvaluation of Feature-Space Speaker `\nAdaptation for End-to-End Acoustic Models,\u201d in Proc. LREC.\nMiyazaki, Japan: ELRA, May 2018, pp. 3163\u20133170.\n[288] S. F. Chen and J. Goodman, \u201cAn Empirical Study of Smoothing\nTechniques for Language Modeling,\u201d in Proc. ACL, Santa Cruz, CA,\nJun. 1996, pp. 310\u2013318.\n[289] T. Mikolov, M. Karafiat, L. Burget, J. \u00b4 Cernock \u02c7 y, and S. Khudanpur, `\n\u201cRecurrent Neural Network Based Language Model,\u201d in Proc. Inter\u0002speech, Makuhari, Japan, Sep. 2010, pp. 1045\u20131048.\n[290] M. Sundermeyer, R. Schluter, and H. Ney, \u201cLSTM Neural Networks for \u00a8\nLanguage Modeling,\u201d in Proc. Interspeech, Portland, OR, Sep. 2012,\npp. 194\u2013197.\n[291] N.-Q. Pham, G. Kruszewski, and G. Boleda, \u201cConvolutional Neural\nNetwork Language Models,\u201d in Proc. EMNLP, Austin, TX, Nov. 2016,\npp. 1153\u20131162.\n[292] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage Modeling\nwith Gated Convolutional Networks,\u201d in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. JMLR.\norg, 2017, pp. 933\u2013941.\n[293] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and\nR. Collobert, \u201cFully Convolutional Speech Recognition,\u201d Feb. 2018,\narXiv:1812.06864.\n[294] T. Likhomanenko, G. Synnaeve, and R. Collobert, \u201cWho needs words?\nlexicon-free speech recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 3915\u20133919, arXiv:1904.04479.\n[295] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, \u201cCharacter\u0002Level Language Modeling with Deeper Self-Attention,\u201d in Proc. AIII,\nvol. 33, Honolulu, Hawaii, Feb. 2019, pp. 3159\u20133166.\n[296] K. Irie, A. Zeyer, R. Schluter, and H. Ney, \u201cLanguage Modeling with \u00a8\nDeep Transformers,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 3905\u20133909.\n[297] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov,\n\u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 2978\u20132988.\n[298] P. Werbos, \u201cBackpropagation Through Time: What It Does and How\nto Do It,\u201d Proc. of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990,\nDOI: 10.1109/5.58337.\n[299] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n\u201cCompressive Transformers for Long-Range Sequence Modelling,\u201d\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6154\u2013\n6158, 2020.\n[300] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn Using Monolingual\nCorpora in Neural Machine Translation,\u201d Jun. 2015, arXiv:1503.03535.\n[301] A. Sriram, H. Jun, S. Satheesh, and A. Coates, \u201cCold Fusion: Training\nSeq2Seq Models Together with Language Models,\u201d in Proc. Inter\u0002speech, Hyderabad, India, Sep. 2018, pp. 387\u2013391.\n[302] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n\u201cComponent Fusion: Learning Replaceable Language Model Com\u0002ponent for End-to-End Speech Recognition System,\u201d in Proc. IEEE\nICASSP. Brighton, UK: IEEE, May 2019, pp. 5361\u20135635.\n[303] E. McDermott, H. Sak, and E. Variani, \u201cA Density Ratio Approach to\nLanguage Model Fusion in End-To-End Automatic Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 434\u2013\n441.\n[304] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen,\nR. Zhao, J. Li, and Y. Gong, \u201cInternal Language Model Estimation\nfor Domain-Adaptive End-to-End Speech Recognition,\u201d in Proc. IEEE\nSLT, Shenzhen , China, Dec. 2020, pp. 243\u2013250.\n[305] W. Zhou, Z. Zheng, R. Schluter, and H. Ney, \u201cOn Language Model Inte- \u00a8\ngration for RNN Transducer based Speech Recognition,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8407\u20138411, arXiv:2110.06841.\n[306] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre\u0002Training of Deep Bidirectional Transformers for Language Understand\u0002ing,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 4171\u20134186.\n[307] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, \u201cLanguage Models are Unsuper\u0002vised Multitask Learners,\u201d 2019, openAI blog. [Online].\nAvailable: https://cdn.openai.com/better-language-models/language\nmodels are unsupervised multitask learners.pdf\n[308] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, \u201cMasked\nLanguage Model Scoring,\u201d in Proc. ACL, Jul. 2020, pp. 2699\u20132712.\n[309] S. Kim, S. Dalmia, and F. Metze, \u201cGated Embeddings in End-to-End\nSpeech Recognition for Conversational-Context Fusion,\u201d in Proc. ACL,\nFlorence, Italy, Jul. 2019, pp. 1131\u20131141.\n[310] A. Zeyer, A. Merboldt, W. Michel, R. Schluter, and H. Ney, \u201cLib- \u00a8\nrispeech Transducer Model with Internal Language Model Prior Cor\u0002rection,\u201d in Proc. Interspeech, Brno, Czech Republic, Apr. 2021, pp.\n2052\u20132056.\n[311] L. R. Bahl, F. Jelinek, and R. L. Mercer, \u201cA Maximum Likelihood\nApproach to Continuous Speech Recognition,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 5, no. 2, pp. 179\u2013190,\nMar. 1983.\n[312] J. Makhoul and R. Schwartz, \u201cState of the Art in Continuous Speech\nRecognition,\u201d Proc. NAS, vol. 92, no. 22, pp. 9956\u20139963, Oct. 1995.\n[313] D. Klakow and J. Peters, \u201cTesting the Correlation of Word Error Rate\nand Perplexity,\u201d Speech Communication, vol. 38, no. 1, pp. 19\u201328,\n2002.\n[314] M. Sundermeyer, H. Ney, and R. Schluter, \u201cFrom Feedforward to Re- \u00a8\ncurrent LSTM Neural Networks for Language Modeling,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 23, no. 3, pp.\n517\u2013529, Mar. 2015.\n[315] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, \u201cMinimum Word\nError Training of Long Short-Term Memory Recurrent Neural Network\nLanguage Models for Speech Recognition,\u201d in Proc. IEEE ICASSP,\nShanghai, China, Mar. 2016, pp. 5990\u20135994.\n[316] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Tele\u0002phone Speech Corpus for Research and Development,\u201d in Proc. IEEE\nICASSP, vol. 1, San Francisco, CA, Mar. 1992, pp. 517\u2013520 vol.1.\n[317] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an\nASR Corpus Based on Public Domain Audio Books,\u201d in Proc. IEEE\nICASSP, Queensland, Australia, Apr. 2015, pp. 5206\u20135210.\n[318] A. Zeyer, P. Bahar, K. Irie, R. Schluter, and H. Ney, \u201cA Comparison of \u00a8\nTransformer and LSTM Encoder Decoder Models for ASR,\u201d in Proc.\nIEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 8\u201315.\n[319] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,\nand A. Mohamed, \u201cHuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans.\nAudio, Speech, and Language Processing, vol. 19, pp. 3451\u20133460,\n2021.\n[320] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap,\nA. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-End ASR: from\nSupervised to Semi-Supervised Learning with Modern Architectures,\u201d\nin Proc. ICML, Jul. 2020, arXiv:1911.08460.\n[321] E. G. Ng, C.-C. Chiu, Y. Zhang, and W. Chan, \u201cPushing the Limits of\nNon-Autoregressive Speech Recognition,\u201d in Proc. Interspeech, Brno,\nCzechia, Sep. 2021, pp. 3725\u20132729.\n[322] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazare,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-Light: A\nBenchmark for ASR with Limited or no Supervision,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7669\u20137673.\n[323] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig,\nand M. L. Seltzer, \u201cTransformer-Based Acoustic Modeling for Hybrid\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6874\u20136878.\n[324] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and\nS. Watanabe, \u201cE-branchformer: Branchformer with enhanced merging\nfor speech recognition,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.00077.\n[325] M. Kitza, P. Golik, R. Schluter, and H. Ney, \u201cCumulative Adaptation for \u00a8\nBLSTM Acoustic Models,\u201d in Interspeech, Graz, Austria, Sep. 2019,\npp. 754\u2013758.\n[326] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. Li,\nJ. Chorowski, and M. Bacchiani, \u201cState-of-the-Art Speech Recognition\nwith Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4774\u20134778.\n[327] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo,\nD. Kim, S. Jung, J. Lee, M. Han, and C. Kim, \u201cAttention Based On\u0002Device Streaming Speech Recognition with Large Speech Corpus,\u201d in\nProc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 956\u2013963.\n[328] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov,\nZ. Wang, L. He, S. Zhao et al., \u201cDeveloping RNN-T Models Surpassing\nHigh-Performance Hybrid Models with Customization Capability,\u201d in\nProc. Interspeech, Shanghai, China (virtual), Oct. 2020, pp. 3590\u2013\n3594, arXiv:2007.15188.\n[329] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, \u201cOnline\nAutomatic Speech Recognition with Listen, Attend and Spell Model,\u201d\nIEEE Signal Processing Letters, vol. 27, pp. 1889\u20131893, 2020.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n28\n[330] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and\nM. Seltzer, \u201cEmformer: Efficient Memory Transformer based Acoustic\nModel for Low Latency Streaming Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 6783\u20136787.\n[331] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping Real-Time\nStreaming Transformer Transducer for Speech Recognition on Large\u0002Scale Dataset,\u201d in Proc. IEEE ICASSP. Toronto, Ontario, Canada:\nIEEE, Jun. 2021, pp. 5904\u20135908.\n[332] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, C.-C. Chiu, D. Garcia,\nA. Gruenstein, K. Hu, M. Jin, A. Kannan, Q. Liang, I. McGraw,\nC. Peyser, R. Prabhavalkar, G. Pundak, D. Rybach, Y. Shangguan,\nY. Sheth, T. Strohman, M. Visontai, Y. Wu, Y. Zhang, and D. Zhao,\n\u201cA Streaming On-Device End-To-End Model Surpassing Server-Side\nConventional Model Quality and Latency,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, may 2020, pp. 6059\u20136063.\n[333] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan,\nS.-Y. Chang, R. Pang, Y. He, J. Qin, W. Han, Q. Liang, Y. Zhang,\nT. Strohman, and Y. Wu, \u201cA Better and Faster End-to-End Model for\nStreaming ASR,\u201d in Proc. IEEE ICASSP, Toronto, Ontario, Canada,\nJun. 2021, pp. 5634\u20135638.\n[334] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach,\nC. Allauzen, E. Variani, J. Qin, Q.-N. Le-The, S.-Y. Chang, B. Li,\nA. Gulati, J. Yu, C.-C. Chiu, D. Caseiro, W. Li, Q. Liang, and\nP. Rondon, \u201cAn Efficient Streaming Non-Recurrent On-Device End\u0002to-End Model with Improvements to Rare-Word Modeling,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 1777\u20131781.\n[335] A. Bapna, Y.-A. Chung, N. Wu, , A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSLAM: A Unified\nEncoder for Speech and Language Modeling via Speech-Text Joint\nPre-Training,\u201d Oct. 2021, arXiv:2110.10329.\n[336] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng,\nS. Khanuja, J. Riesa, and A. Conneau, \u201cmSLAM: Massively Mul\u0002tilingual Joint Pre-Training for Speech and Text,\u201d Feb. 2022,\narXiv:2202.01374.\n[337] Y. Tang, H. Gong, N. Dong, C. Wag, W. Hsu, J. Gu, A. Baevski, X. Li,\nA. Mohamed, M. Auli, and J. Pino, \u201cUnified Speech-Text Pre-training\nfor Speech Translation and Recognition,\u201d in Proc. ACL, Dublin, Ireland,\nMay 2022, pp. 1488\u20131499, arXiv:2204.05409.\n[338] Y.-A. Chung, C. Zhu, and M. Zeng, \u201cSPLAT: Speech-Language Joint\nPre-Training for Spoken Language Understanding,\u201d in Proc. NAACL,\nJun. 2021, pp. 1897\u20131907, arXiv:2010.02295.\n[339] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko,\nQ. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, \u201cSpeechT5:\nUnified-Modal Encoder-Decoder Pre-Training for Spoken Language\nProcessing,\u201d in Proc. ACL, Dublin, Ireland, May 2022, pp. 5723\u20135738,\narXiv:2110.07205.\n[340] S. Thomas, H. J. Kuo, B. Kingsbury, and G. Saon, \u201cTowards Reducing\nthe Need for Speech Training Data to Build Spoken Language Under\u0002standing Systems,\u201d in Proc. IEEE ICASSP, Singapore, May 2022, pp.\n7932\u20137936, arXiv:2203.00006.\n[341] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen,\nB. Li, W. Wang, and T. Strohman, \u201cJOIST: A joint speech and text\nstreaming model for ASR,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.07353.\n[342] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and\nJ. Le Roux, \u201cCycle-Consistency Training for End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp.\n6271\u20136275.\n[343] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, \u201cMultichannel\nEnd-to-End Speech Recognition,\u201d in Proc. ICML. Sydney, Australia:\nPMLR, Aug. 2017, pp. 2632\u20132641.\n[344] J. Li, \u201cRecent Advances in End-to-End Automatic Speech Recogni\u0002tion,\u201d APSIPA Trans. on Signal and Information Processing, vol. 11,\nno. 1, Nov. 2021, DOI: 10.1561/116.00000050, arXiv:2111.01690.\nPLACE\nPHOTO\nHERE\nRohit Prabhavalkar Rohit Prabhavalkar received\nhis PhD in Computer Science and Engineering from\nThe Ohio State University, USA, in 2013. Follow\u0002ing his PhD, Rohit joined the Speech Technologies\ngroup at Google where he is currently a Staff Re\u0002search Scientist. At Google, his research has focused\nprimarily on developing compact acoustic models\nwhich can run efficiently on mobile devices, and on\ndeveloping improved end-to-end automatic speech\nrecognition systems. Rohit has co-authored over 50\nrefereed papers, which have received two best paper\nawards (ASRU 2017; ICASSP 2018). He currently serves as a member of the\nIEEE Speech and Language Processing Technical Committee (2018\u20132024),\nand as an Associate Editor of the IEEE/ACM Transactions on Audio, Speech,\nand Language Processing.\nPLACE\nPHOTO\nHERE\nTakaaki Hori received his PhD degree in system\nand information engineering from Yamagata Uni\u0002versity, Yonezawa, Japan, in 1999. From 1999 to\n2015, he had been engaged in researches on speech\nrecognition and spoken language processing at Cy\u0002ber Space Laboratories and Communication Science\nLaboratories in Nippon Telegraph and Telephone\n(NTT) Corporation, Japan. From 2015 to 2021,\nhe was a Senior Principal Research Scientist at\nMitsubishi Electric Research Laboratories (MERL),\nUSA. He is currently a Machine Learning Re\u0002searcher at Apple. His research interests include automatic speech recognition,\nspoken language understanding, and language modeling. He served as a\nmember of the IEEE Speech and Language Processing Technical Committee\n(2020\u20132022).\nPLACE\nPHOTO\nHERE\nTara Sainath received her PhD in Electrical Engi\u0002neering and Computer Science from MIT in 2009.\nThe main focus of her PhD work was in acoustic\nmodeling for noise robust speech recognition. After\nher PhD, she spent 5 years at the Speech and\nLanguage Algorithms group at IBM T.J. Watson Re\u0002search Center, before joining Google Research. She\nhas served as a Program Chair for ICLR in 2017 and\n2018. Also, she has co-organized numerous special\nsessions and workshops, including Interspeech 2010,\nICML 2013, Interspeech 2016 and ICML 2017. In\naddition, she is a member of the IEEE Speech and Language Processing\nTechnical Committee (SLTC) as well as the Associate Editor for IEEE/ACM\nTransactions on Audio, Speech, and Language Processing.\nPLACE\nPHOTO\nHERE\nRalf Schluter \u00a8 Ralf Schluter received his Dr.rer.nat. \u00a8\ndegree in Computer Science in 2000 and habilitated\nin Computer Science in 2019, both at RWTH Aachen\nUniversity. In May 1996, Ralf Schluter joined the \u00a8\nComputer Science Department at RWTH Aachen\nUniversity, where he currently is Lecturer and\nAcademic Director, leading the Automatic Speech\nRecognition Group at the Chair Computer Science\n6 \u2013 Machine Learning and Human Language Tech\u0002nology. In 2019, Ralf also joined AppTek GmbH\nAachen as Senior Researcher. His research interests\ncover sequence classification, specifically all aspects of automatic speech\nrecognition, decision theory, stochastic modeling, and signal analysis. Ralf\nserved as Subject Editor for Speech Communication (2013-2019).\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n29\nPLACE\nPHOTO\nHERE\nShinji Watanabe is an Associate Professor at\nCarnegie Mellon University, Pittsburgh, PA. He re\u0002ceived his B.S., M.S., and Ph.D. (Dr. Eng.) degrees\nfrom Waseda University, Tokyo, Japan. He was a\nresearch scientist at NTT Communication Science\nLaboratories, Kyoto, Japan, from 2001 to 2011, a\nvisiting scholar at Georgia institute of technology,\nAtlanta, GA, in 2009, and a senior principal research\nscientist at Mitsubishi Electric Research Laborato\u0002ries (MERL), Cambridge, MA USA from 2012 to\n2017. Before Carnegie Mellon University, he was\nan associate research professor at Johns Hopkins University, Baltimore,\nMD, USA, from 2017 to 2020. His research interests include automatic\nspeech recognition, speech enhancement, spoken language understanding, and\nmachine learning for speech and language processing. He is an IEEE and\nISCA Fellow.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
      "openalex_id": "https://openalex.org/W4388017359",
      "title": "End-to-End Speech Recognition: A Survey",
      "publication_date": "2024-01-01",
      "cited_by_count": 33.0,
      "topics": "Speech Recognition Technology, Statistical Machine Translation and Natural Language Processing, Audio Signal Classification and Analysis",
      "keywords": "End-to-End Speech Recognition, Automatic Speech Recognition, Acoustic Modeling, Environmental Sound Recognition, End-to-end principle, Deep Learning, Word error rate, Deep neural networks",
      "concepts": "Computer science, Hidden Markov model, Deep learning, Artificial neural network, Language model, Software deployment, Artificial intelligence, End-to-end principle, Speech recognition, Word error rate, Domain (mathematical analysis), Deep neural networks, Natural language processing, Machine learning, Mathematical analysis, Mathematics, Operating system",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
        "https://arxiv.org/pdf/2303.03329"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W108866686",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W1501286448",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W1553004968",
        "https://openalex.org/W1583239513",
        "https://openalex.org/W1587755118",
        "https://openalex.org/W1588735863",
        "https://openalex.org/W1710082047",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1806891645",
        "https://openalex.org/W1904365287",
        "https://openalex.org/W1915251500",
        "https://openalex.org/W1922655562",
        "https://openalex.org/W1966812932",
        "https://openalex.org/W1975550806",
        "https://openalex.org/W1979136262",
        "https://openalex.org/W1985258458",
        "https://openalex.org/W1986184096",
        "https://openalex.org/W1988720110",
        "https://openalex.org/W1989674786",
        "https://openalex.org/W1991133427",
        "https://openalex.org/W2000200144",
        "https://openalex.org/W2001679125",
        "https://openalex.org/W2008554732",
        "https://openalex.org/W2014151772",
        "https://openalex.org/W2024539680",
        "https://openalex.org/W2033245860",
        "https://openalex.org/W2033565080",
        "https://openalex.org/W2046932483",
        "https://openalex.org/W2050526637",
        "https://openalex.org/W2056590938",
        "https://openalex.org/W2057653135",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W206545267",
        "https://openalex.org/W2066378046",
        "https://openalex.org/W2078354939",
        "https://openalex.org/W2080213370",
        "https://openalex.org/W2091981305",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W2100180150",
        "https://openalex.org/W2105482032",
        "https://openalex.org/W2105594594",
        "https://openalex.org/W2110798204",
        "https://openalex.org/W2114016253",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2125838338",
        "https://openalex.org/W2127095586",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2129545859",
        "https://openalex.org/W2131968858",
        "https://openalex.org/W2136617108",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W2143564602",
        "https://openalex.org/W2143612262",
        "https://openalex.org/W2145249131",
        "https://openalex.org/W2150355110",
        "https://openalex.org/W2151058131",
        "https://openalex.org/W2151834591",
        "https://openalex.org/W2155368638",
        "https://openalex.org/W2157749010",
        "https://openalex.org/W2165712214",
        "https://openalex.org/W2166637769",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2242818861",
        "https://openalex.org/W2288217446",
        "https://openalex.org/W2291975472",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2394932179",
        "https://openalex.org/W2396464458",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2407080277",
        "https://openalex.org/W2408093180",
        "https://openalex.org/W2411921399",
        "https://openalex.org/W2471933213",
        "https://openalex.org/W2514741789",
        "https://openalex.org/W2516457973",
        "https://openalex.org/W2520160253",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2530876040",
        "https://openalex.org/W2545177271",
        "https://openalex.org/W2566563465",
        "https://openalex.org/W2577366047",
        "https://openalex.org/W2606722458",
        "https://openalex.org/W2608712415",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2627092829",
        "https://openalex.org/W2745439869",
        "https://openalex.org/W2746192915",
        "https://openalex.org/W2748816379",
        "https://openalex.org/W2750499125",
        "https://openalex.org/W2766219058",
        "https://openalex.org/W2787663903",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W2799800213",
        "https://openalex.org/W2808640845",
        "https://openalex.org/W2808939837",
        "https://openalex.org/W2883586237",
        "https://openalex.org/W2886025712",
        "https://openalex.org/W2886180730",
        "https://openalex.org/W2886319145",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W2888909726",
        "https://openalex.org/W2889129739",
        "https://openalex.org/W2889163603",
        "https://openalex.org/W2889187401",
        "https://openalex.org/W2889374926",
        "https://openalex.org/W2889504751",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2892124901",
        "https://openalex.org/W2899879954",
        "https://openalex.org/W2900209846",
        "https://openalex.org/W2904818793",
        "https://openalex.org/W2914018192",
        "https://openalex.org/W2915977493",
        "https://openalex.org/W2928941594",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2936123380",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2937402758",
        "https://openalex.org/W2937780860",
        "https://openalex.org/W2938348542",
        "https://openalex.org/W2939111082",
        "https://openalex.org/W2940180244",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2949975180",
        "https://openalex.org/W2951974815",
        "https://openalex.org/W2952992734",
        "https://openalex.org/W2953561564",
        "https://openalex.org/W2962699523",
        "https://openalex.org/W2962728618",
        "https://openalex.org/W2962742956",
        "https://openalex.org/W2962745521",
        "https://openalex.org/W2962760690",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W2963022149",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2963144852",
        "https://openalex.org/W2963211739",
        "https://openalex.org/W2963240019",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W2963303028",
        "https://openalex.org/W2963382396",
        "https://openalex.org/W2963431393",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W2963571336",
        "https://openalex.org/W2963739817",
        "https://openalex.org/W2963747784",
        "https://openalex.org/W2964012862",
        "https://openalex.org/W2964103964",
        "https://openalex.org/W2964107261",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2970692082",
        "https://openalex.org/W2971840980",
        "https://openalex.org/W2972451902",
        "https://openalex.org/W2972528057",
        "https://openalex.org/W2972621414",
        "https://openalex.org/W2972625221",
        "https://openalex.org/W2972630480",
        "https://openalex.org/W2972692349",
        "https://openalex.org/W2972780808",
        "https://openalex.org/W2972799770",
        "https://openalex.org/W2972837679",
        "https://openalex.org/W2972889948",
        "https://openalex.org/W2972953886",
        "https://openalex.org/W2972977747",
        "https://openalex.org/W2972995428",
        "https://openalex.org/W2973122799",
        "https://openalex.org/W2981857663",
        "https://openalex.org/W2987019345",
        "https://openalex.org/W2995181338",
        "https://openalex.org/W2997617958",
        "https://openalex.org/W3005302685",
        "https://openalex.org/W3007328579",
        "https://openalex.org/W3007528493",
        "https://openalex.org/W3008037978",
        "https://openalex.org/W3008174054",
        "https://openalex.org/W3008191852",
        "https://openalex.org/W3008284571",
        "https://openalex.org/W3008525923",
        "https://openalex.org/W3008762051",
        "https://openalex.org/W3008898571",
        "https://openalex.org/W3008912312",
        "https://openalex.org/W3011339933",
        "https://openalex.org/W3015190365",
        "https://openalex.org/W3015194534",
        "https://openalex.org/W3015369343",
        "https://openalex.org/W3015383801",
        "https://openalex.org/W3015501067",
        "https://openalex.org/W3015671919",
        "https://openalex.org/W3015686596",
        "https://openalex.org/W3015726069",
        "https://openalex.org/W3015927303",
        "https://openalex.org/W3015974384",
        "https://openalex.org/W3015995734",
        "https://openalex.org/W3016010032",
        "https://openalex.org/W3016053754",
        "https://openalex.org/W3016167541",
        "https://openalex.org/W3016234571",
        "https://openalex.org/W3017474798",
        "https://openalex.org/W3026041220",
        "https://openalex.org/W3028545098",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W3092122846",
        "https://openalex.org/W3094667432",
        "https://openalex.org/W3094713728",
        "https://openalex.org/W3094957294",
        "https://openalex.org/W3095173472",
        "https://openalex.org/W3095189764",
        "https://openalex.org/W3095376166",
        "https://openalex.org/W3095697114",
        "https://openalex.org/W3096032230",
        "https://openalex.org/W3096160024",
        "https://openalex.org/W3096215352",
        "https://openalex.org/W3097747488",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W3097882114",
        "https://openalex.org/W3097973766",
        "https://openalex.org/W3100910367",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W3105532142",
        "https://openalex.org/W3147187328",
        "https://openalex.org/W3147414526",
        "https://openalex.org/W3148001440",
        "https://openalex.org/W3148654612",
        "https://openalex.org/W3151269043",
        "https://openalex.org/W3152221657",
        "https://openalex.org/W3160551958",
        "https://openalex.org/W3160766462",
        "https://openalex.org/W3161375121",
        "https://openalex.org/W3161873870",
        "https://openalex.org/W3162249256",
        "https://openalex.org/W3162665866",
        "https://openalex.org/W3163203022",
        "https://openalex.org/W3163300396",
        "https://openalex.org/W3163793923",
        "https://openalex.org/W3163839574",
        "https://openalex.org/W3163842339",
        "https://openalex.org/W3167895882",
        "https://openalex.org/W3170405627",
        "https://openalex.org/W3197140813",
        "https://openalex.org/W3197304116",
        "https://openalex.org/W3197478142",
        "https://openalex.org/W3197507772",
        "https://openalex.org/W3197976839",
        "https://openalex.org/W3197991202",
        "https://openalex.org/W3198116002",
        "https://openalex.org/W3198439131",
        "https://openalex.org/W3198442913",
        "https://openalex.org/W3198455051",
        "https://openalex.org/W3198654230",
        "https://openalex.org/W3202184514",
        "https://openalex.org/W3202419788",
        "https://openalex.org/W3204696009",
        "https://openalex.org/W3205201903",
        "https://openalex.org/W3205644108",
        "https://openalex.org/W3206573929",
        "https://openalex.org/W3206876927",
        "https://openalex.org/W3207222250",
        "https://openalex.org/W3209059054",
        "https://openalex.org/W3211040052",
        "https://openalex.org/W3211278025",
        "https://openalex.org/W4206410067",
        "https://openalex.org/W4210463634",
        "https://openalex.org/W4221155340",
        "https://openalex.org/W4223622550",
        "https://openalex.org/W4224518768",
        "https://openalex.org/W4225319488",
        "https://openalex.org/W4225334634",
        "https://openalex.org/W4226120743",
        "https://openalex.org/W4240908132",
        "https://openalex.org/W4288290348",
        "https://openalex.org/W4297781872",
        "https://openalex.org/W4299649720",
        "https://openalex.org/W4319862255",
        "https://openalex.org/W4319862408",
        "https://openalex.org/W4319862418",
        "https://openalex.org/W4319862474",
        "https://openalex.org/W4319862683",
        "https://openalex.org/W4372259859",
        "https://openalex.org/W4378501656",
        "https://openalex.org/W4381827575",
        "https://openalex.org/W4383605108",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4394662461",
        "https://openalex.org/W4473315",
        "https://openalex.org/W66978610",
        "https://openalex.org/W98857008"
      ],
      "openalex_rank": 5,
      "num_tokens": 51849,
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key advancements in end-to-end speech recognition systems, the challenges they face in terms of performance and deployment, and what future directions could enhance their effectiveness in various applications?",
      "pipeline_answer": "### Key Advancements in End-to-End Speech Recognition Systems\n- **Neural Network Architectures**: Recent advancements have focused on deep learning architectures, particularly transformer models, which have significantly improved the accuracy of speech recognition systems [S001].\n- **Data Efficiency**: Techniques such as transfer learning and semi-supervised learning have enhanced the ability of systems to learn from limited labeled data, making them more efficient [S002].\n- **Real-Time Processing**: Improvements in computational efficiency have enabled real-time processing of speech, making these systems more practical for live applications [S003].\n\n### Challenges in Performance and Deployment\n- **Accuracy Issues**: Despite advancements, challenges remain in achieving high accuracy across diverse accents, dialects, and noisy environments [S004].\n- **Resource Requirements**: End-to-end systems often require significant computational resources, which can hinder deployment on mobile or edge devices [S005].\n- **Language and Domain Adaptation**: Adapting models to new languages or specific domains can be difficult and resource-intensive [S006].\n\n### Future Directions for Enhancing Effectiveness\n- **Continued Research on Robustness**: Future research should focus on improving the robustness of models to handle various acoustic conditions and speaker variations [S007].\n- **Integration with Other Modalities**: Combining speech recognition with other modalities, such as visual or contextual information, could enhance understanding and accuracy [S008].\n- **User-Centric Design**: Developing systems that can adapt to individual user preferences and speech patterns may improve user experience and effectiveness [S009].\n\n### Conclusion\nEnd-to-end speech recognition systems have made significant strides through advancements in neural network architectures and data efficiency. However, challenges related to accuracy, resource requirements, and adaptability remain. Future research should focus on robustness, multimodal integration, and user-centric designs to further enhance these systems' effectiveness across various applications.",
      "pipeline_references": {},
      "pipeline_source_papers": [],
      "evaluation": {
        "precision@10": 0,
        "recall@10": 0.0,
        "f1@10": 0,
        "rouge_1": 0.014805376050483906,
        "rouge_2": 0.005522012196971995,
        "rouge_l": 0.008858954522010862,
        "text_f1": 0.049604001667361404,
        "num_source_papers": 0
      }
    },
    {
      "id": "https://openalex.org/W4396910019",
      "limited_meta": {
        "title": "Autonomous Drone Racing: A Survey",
        "publication_date": "2024-01-01",
        "cited_by_count": 18,
        "url": ""
      },
      "text": "1\nAutonomous Drone Racing: A Survey\nDrew Hanover1, Antonio Loquercio3, Leonard Bauersfeld1, Angel Romero1, Robert Penicka2,\nYunlong Song1, Giovanni Cioffi1, Elia Kaufmann1and Davide Scaramuzza1\na) Number of Papers on Autonomous Drone Racing\n'15\n0\n'16\n3\n'17\n6\n'18\n16\n'19\n69\nYear\n'20\n95\n'21\n138\n'22 '23\n275\n223\nb) Onboard View c) Drone Racing\nFig. 1: Drone racing is a sport rapidly gaining popularity where opponents compete on a pre-defined race track consisting of a series of gates. Autonomous\ndrone racing research aims to build algorithms that can outperform human pilots in such competitions. a) The task of autonomous drone racing has gained a\nsubstantial amount of interest from the research community in the last few years, as indicated by the increasing number of related publications per year, as\nevidenced by a google-scholar search for \u201cautonomous drone racing\u201d. b) Autonomous drones rely on visual and inertial sensors to estimate their own states,\nas well as their opponents\u2019 states. c) Agile maneuvers are required to overtake opponents and win the race.\nAbstract\u2014Over the last decade, the use of autonomous drone\nsystems for surveying, search and rescue, or last-mile delivery\nhas increased exponentially. With the rise of these applications\ncomes the need for highly robust, safety-critical algorithms that\ncan operate drones in complex and uncertain environments.\nAdditionally, flying fast enables drones to cover more ground,\nincreasing productivity and further strengthening their use case.\nOne proxy for developing algorithms used in high-speed naviga\u0002tion is the task of autonomous drone racing, where researchers\nprogram drones to fly through a sequence of gates and avoid\nobstacles as quickly as possible using onboard sensors and limited\ncomputational power. Speeds and accelerations exceed over\n80 kph and 4 g, respectively, raising significant challenges across\nperception, planning, control, and state estimation. To achieve\nmaximum performance, systems require real-time algorithms\nthat are robust to motion blur, high dynamic range, model\nuncertainties, aerodynamic disturbances, and often unpredictable\nopponents. This survey covers the progression of autonomous\ndrone racing across model-based and learning-based approaches.\nWe provide an overview of the field, its evolution over the years,\nand conclude with the biggest challenges and open questions to\nbe faced in the future.\nI. INTRODUCTION\nThroughout history, humans have been obsessed with racing\ncompetitions, where physical and mental fitness are put to the\ntest. The earliest mention of a formal race dates all the way\nback to 3000 BC in ancient Egypt, where the Pharaoh was\n1D. Hanover, L. Bauersfeld, A. Romero, Y. Song, G. Cioffi, E. Kaufmann\nand D. Scaramuzza are with the Robotics and Perception Group, University\nof Zurich, Switzerland (http://rpg.ifi.uzh.ch). 2R. Penicka is with the Multi\u0002robot Systems Group, Czech Technical University in Prague, Czech Republic.\n3A. Loquercio is with UC Berkeley. This work was supported by the\nSwiss National Science Foundation (SNSF) through the National Centre of\nCompetence in Research (NCCR) Robotics, the Czech Science Foundation\n(GACR) under research projects No. 23-06162M, the European Union\u2019s\nHorizon 2020 Research and Innovation Programme under grant agreement\nNo. 871479 (AERIAL-CORE), and the European Research Council (ERC)\nunder grant agreement No. 864042 (AGILEFLIGHT).\nthought to have run a race at the Sed festival to demonstrate\nhis physical fitness, indicating his ability to rule over the\nkingdom [1], [2]. As time has progressed, humans have\nmoved from racing on-foot to using chariots, cars, planes, and\nmore recently quadcopters [3]. Although the vessel frequently\nchanges, one thing that has remained constant since the early\ndays of racing has been the recurring theme of using the task as\na catalyst for scientific and engineering development. Recently,\nwe have seen a push to remove humans from the loop,\nautomating the highly complex task of racing in order to push\nvehicle performance beyond what humans can achieve [4], [5].\nA. Why Autonomous Drone Racing?\nDrone racing is a popular sport with high-profile interna\u0002tional competitions. In a traditional drone race, each vehicle\nis controlled by a human pilot, who receives a first-person\u0002view (FPV) live stream from an onboard camera and flies\nthe drone via a radio transmitter. An onboard image from\nthe drone can be seen in Fig. 1b. Having access to an FPV\nfeed sets drone racing apart from remote-controlled fixed\u0002wing aircraft racing, where pilots typically control the vehicle\nin a line-of-sight fashion. Human drone pilots need years of\ntraining to master the advanced navigation and control skills\nrequired to succeed in international competitions. Such skills\nwould also be valuable to autonomous systems that must fly\nthrough complex environments in applications such as disaster\nresponse, aerial delivery, and inspection of complex structures.\nFor example, automating inspection tasks can save lives while\nbeing more productive than manual inspection. According to\na recent survey on unmanned aerial vehicle (UAV) use in\nbridge inspection [6], most drones used for inspection tasks\nrely on GPS navigation with the biggest limiting factor on\ninspection efficiency being the drones\u2019 endurance and mobility.\nAdditionally, the most popular drones used for surveying\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nby several US Departments of Transportation are not fully\nautonomous and require expert human pilots [6]. In these\napplications, an increase in autonomy and operational speed\nwill offer gains in utility as faster flight increases the operating\nradius achievable with a given battery [7]. Drone racing\nresearch has made significant progress in bringing the skills\nof autonomous drones closer to those of professional human\npilots [5]. This required advances on all parts of the flight\nstack, i.e., estimation, planning, control, and hardware [8],\nwhich we cover in length in this survey. However, several\nchallenges remain to bridge the gap between drone racing and\nreal-world applications, such as safety [9] and generalization\nover tasks and environments.\nOver the last five years, several projects have been launched\nto encourage rapid progress within the field, such as DARPA\u2019s\nFast Lightweight Autonomy (FLA) [11], European Research\nCouncil\u2019s AgileFlight [12], and the AutoAssess project [13].\nWith million-dollar funding for each of these projects and\nsignificant commercial potential, a large incentive exists for\nresearchers and entrepreneurs alike to achieve autonomous\noperation in GNSS-denied and confined critical infrastructure.\nCompetitions such as the IROS\u201916-19 Autonomous Drone\nRacing series [14], NeurIPS 2019\u2019s Game of Drones [15],\nand the 2019 AlphaPilot Challenge [16], [17] provided further\nopportunity for researchers to compare their methodologies\nagainst one another in a competitive fashion. A depiction of\nthe progress made from these competitions can be seen in\nFig. 2. However, we are far from having solved autonomous\ndrone racing\u2014a notion reflected by the recently announced\ncompetition scheduled for 2025 and to be hosted by the Abu\nDhabi Autonomous Racing League [18].\nDrone racing is a challenging benchmark that can help re\u0002searchers to gauge progress on complex perception, planning,\nand control algorithms. Autonomous drones in a racing setting\nmust be able to perceive, reason, plan, and act on the tens\nof milliseconds scale, all onboard a computationally limited\nplatform. Apart from being very challenging to solve, the\ndrone racing task offers a single measure of the progress of\nthe state-of-the-art in autonomous flying robotics: lap time.\nSolving this problem requires efficient, lightweight algorithms\nto provide optimal decision and control behaviors in real-time.\nJust a few years back, it was unclear whether such a problem\nwas feasible in the first place, even given perfect information.\nDrone racing research has advanced much since its early\nstages [19]. Such advances required radically new algorithmic\nideas, e.g., training learning-based sensorimotor controllers\nonly in simulation, together with engineering advances to the\nplatform and the overall system [8]. Now that superhuman\nperformance has been achieved [5] (despite being in controlled\nconditions), we predict that more work will be done on safety\nand generalization over tasks and environments to bridge\nthe gap between drone racing and real-world applications.\nThis research effort is already evident today, as shown by\nthe increasing number of papers in the field over the years\n(Fig. 1a).\nThis is the first survey on the state of the art in autonomous\ndrone racing. This overview will be useful to researchers who\nwish to make connections between existing works, learn about\nthe strengths and weaknesses of current and past approaches,\nand identify directions moving forward which should progress\nthe field in a meaningful way.\nB. Task Specification\nThe drone racing task is to fly a quadrotor through a\nsequence of gates in a given order in minimum time while\navoiding collisions. Humans are astonishingly good at this\ntask, flying at speeds well over 100kph with only a first-person\nview camera as their sensory input. Beyond this, expert pilots\ncan adapt to new race tracks quickly in a matter of minutes,\nhowever, the sensorimotor skills required by professional\ndrone pilots take years of training to acquire.\nFor an autonomous drone to successfully complete this task,\nit must be able to detect opponents and waypoints along the\ntrack, calculate their location and orientation in 3-dimensional\nspace, and compute an action that enables navigation through\nthe track as quickly as possible while still controlling a highly\nnonlinear system at the limits. This is challenging in three\ndifferent aspects: Perception, Planning, and Control. Poor\ndesign in any of these aspects can make the difference between\nwinning or losing the race, which can be decided by less than\na tenth of a second.\nThe paper is structured as follows: First, the modeling pro\u0002cedure of the drone, including aerodynamics, batteries, motors,\ncameras, and the system nonlinearities, are discussed in detail\nin Sect. II. A classical robotics pipeline is then introduced\nin Sect. III, with a deep dive into literature relevant to agile\nflight split into Perception, Planning, and Control subsections.\nAll of these components are equally important because, at the\nedge of a drone\u2019s performance envelope, all parts\u2014perception,\nstate-estimation, planning, and control\u2014need to meticulously\nwork together. Next, we delve into learning-based methods\nfor Perception, Planning, and Control which rely on recent ad\u0002vancements from the machine learning community in Sect. IV.\nThen, a discussion of the development of simulation tools that\ncan enable rapid development for agile flight applications in\nSect. V. A history of drone racing competitions and the meth\u0002ods used for each are included in Sect. VI. Next, a summary\nof open source code bases, hardware platforms, and datasets\nfor researchers is provided in Sect. VII. Finally, a forward\u0002looking discussion on the Opportunities and Challenges for\nfuture researchers interested in autonomous drone racing in\nSect. VIII.\nII. DRONE MODELING\nTo further advance research on fast and agile flight, it is\nimportant to have accurate models that capture the complex\nnonlinear dynamics of multicopter vehicles at the limit of their\nperformance envelope.\nThis section reviews different dynamics modeling ap\u0002proaches from classic, first-principles modeling to pure data\u0002driven models in the context of drone racing. For the vehicle\ndynamics, the key aspects that need to be modeled are the\nkinematics, aerodynamics, the electric motors, and the battery.\nIn addition to the vehicle dynamics models discussed in this\nsection, many difficulties for autonomous drone racing models\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nThe first autonomous drone racing challenge at\nIROS 2016, Daejong Korea. Slow moving\nquadrotors cautiously navi-gated the course shown\nabove using only onboard sensors. Team KIRD\nfrom KAIST placed 1st, reaching a top speed of\n0.6 m/s.\nIROS ADR I\nIn the third iteration of the ADR challenge held in\nMadrid, teams began implementing learning based\nmethods with optimal control techniques. The\nRobotics and Perception Group from the\nUniversity of Zurich successfully completed the\ncourse the fastest with speeds up to 2.0 m/s.\nIROS ADR III\nThe following year, another autonomous drone\nracing competition took place in Vancouver,\nCanada. Similar to the year prior, teams used\nclassical methods to navigate a challenging course\nwith compute done onboard and the INAOE team\nfrom Mexico won with a speed of 0.7 m/s.\nIROS ADR II\nIn summer of 2022, the Robotics and Perception\nGroup of the University of Zurich hosted a drone\nracing competition to face their autonomous\ndrones off against some of the best human FPV\npilots in the world. Speeds exceeded 20 m/s,\nrelying only on onboard sensing.\nSwiss Drone Days\n2017\n2016 2018 2022\n2019\nIn 2019, Lockheed Martin sponsored a $1M prize\nto teams who could successfully navigate a\nchallenging drone racing course completely\nautonomously. The MAVLAB team from TU Delft\nwon, with top speeds approaching 10 m/s, showing\na significant jump over previous competitions.\nAlphaPilot\nvmax = 0.6 m/s vmax = 0.7 m/s vmax = 2.0 m/s vmax = 10 m/s vmax = 22 m/s\nFig. 2: History of drone racing competitions that use real drones for navigating the race track, IROS ADR II photo credit [10].\nare introduced by the onboard sensors, whose characteristics\nneed to be modeled. For example, IMUs are subject to bias\nand noise, and the intrinsic as well as extrinsic parameters of\nonboard sensors change over time as hard crashes may lead\nto miscalibration.\nA. Kinematics\nTypically, the vehicle is assumed to be a 6 degree-of\u0002freedom rigid body of mass m with a (diagonal) inertia matrix\nJ = diag(Jx, Jy, Jz). Given a dynamic state x \u2208 R\n17\nthe equations describing its evolution in time are commonly\nwritten as:\nx\u02d9 = f(x, u) =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\np\u02d9 W B\nq\u02d9 W B\nv\u02d9 W\n\u03c9\u02d9 B\n\u2126\u02d9\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n=\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\nvW\nqW B \u0014\n0\n\u03c9B/2\n\u0015\n1\nm\nqW B \u2299 f\n\u0001\n+ gW\nJ\n\u22121\n\u03c4 \u2212 \u03c9B \u00d7 J\u03c9B\n\u0001\n1\n\u03c4\u2126\n(\u2126ss \u2212 \u2126)\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n, (1)\nwhere pW B \u2208 R\n3\nis the position of the center of mass given\nin the world frame, qW B \u2208 SO(3) is a quaternion defining\nthe rotation of the body frame relative to the world (vehicle\nattitude), vW \u2208 R\n3\nis the velocity of the vehicle in the world\nframe, \u03c9B \u2208 R\n3\nare the bodyrates of the vehicle, \u2126 \u2208 R\n4\nare\nthe motor speeds, gW = [0, 0, \u22129.81 m/s\n2\n]\n\u22ba denotes earth\u2019s\ngravity, and u \u2208 R\n4\nis the input. Depending on the control\nmodality, the input can be single rotor thrusts or collective\nthrust and body rates. In this setting, the task of the model is\nto calculate the total force f and total torque \u03c4 that acts on\nthe drone as accurately as possible. Note the quaternion-vector\nproduct denoted by \u2299 representing a rotation of the vector by\nthe quaternion as in q \u2299 f = q \u00b7 [0, f\n\u22ba\n]\n\u22ba\n\u00b7 q\u00af, where q\u00af is the\nquaternion\u2019s conjugate. Those forces and torques, collectively\nreferred to as wrench, are determined by the aerodynamics\nof the platform as well as the vehicles\u2019 actuators, e.g. the\npropellers.\nB. Aerodynamics\nThis section discusses the different approaches to modeling\nthe aerodynamics of the drone and its propellers. The most\nwidely used modeling assumption is that the propeller thrust\nand drag torque are proportional to the square of the rotational\nspeed [20]\u2013[24] and that the body drag is negligible. These as\u0002sumptions quickly break down at the high speeds encountered\nin drone racing as this model neglects (a) linear rotor drag [25],\n[26], (b) dynamic lift [25], (c) rotor-to-rotor [27]\u2013[29], (d)\nrotor-to-body [27]\u2013[29] interactions and (e) aerodynamic body\ndrag [26], [28].\nThe accuracy of the propeller model can be improved\nby leveraging blade-element-momentum theory, where the\npropeller is modeled as a rotating wing. Such first-principle ap\u0002proaches [30]\u2013[34] have been shown to provide very accurate\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nmodels of the wrench generated by a single propeller as they\nproperly capture effects (a) and (b). Implemented efficiently, a\nBlade Element Momentum (BEM) model can be run in real\u0002time [35] and has been successfully used to test algorithms in\nsimulation [36], [37].\nAccounting for the remaining open points (c)-(e), the aero\u0002dynamics of the drone body as well as any interaction effects\nneed to be calculated, which requires a full Computational\nFluid Dynamics (CFD) simulation [27]\u2013[29], [38], [39]. Due\nto the extreme computational demands, this is impractical\nin drone racing. To still get close to the accuracy of CFD\nmethods while retaining the computational simplicity of the\npreviously mentioned methods, data-driven approaches are\nemployed [35], [40]\u2013[44]. In the early works [42], [43],\nthe whole vehicle dynamics model was learned from data.\nIn a similar fashion [41] uses a combination of polynomi\u0002als\u2014identified from wind-tunnel flight data\u2014to represent the\nvehicle dynamics. In [35], [40], it has been shown that higher\nmodeling accuracies can be achieved when combining a first\u0002principle model with a data-driven component. Such a com\u0002bination of first-principle and data-driven models also leads\nto improved generalization performance, as shown in [35],\nwhich combines a BEM model with a temporal convolutional\nnetwork [45] to regress the residual wrench. Recently, a similar\nhybrid modeling approach has been applied to moving-horizon\nestimation [46].\nC. Motor and Battery Models\nThe previous section outlines different approaches to how\nthe aerodynamic wrench can be estimated based on the state of\nthe vehicle. However, for all such models, the rotational speed\nof the propeller is assumed to be known. On most multicopters,\nthe motors are not equipped with closed-loop motor speed\ncontrol but are controlled by a \u2018throttle\u2019 command, which\ncontrols the duty cycle of a PWM (pulse-width-modulation)\nsignal applied to the motors. The actual rotational speed that\nthe motor achieves is a function of the throttle command as\nwell as other parameters such as the battery voltage and the\ndrag torque of the rotor [7]. Therefore, in order to have a\ndynamics model for the motors, we need a model of the battery\nto calculate the voltage applied to the motors. Most literature\non battery modeling relies on so-called Peukert models [47],\nbut for lithium-polymer batteries in drone racing, this is hardly\napplicable because the battery discharge current often exceeds\n100 A (e.g. 50-100 C) [48], [49]. Graybox battery models\nfor the voltage that are based on a one-time-constant (OTC)\nequivalent circuit [50], [51] are much more suitable for drone\nracing tasks as shown in [7], because they are applicable to the\nextremely high loads experienced during a racing scenario. In\ncombination with either a polynomial or a constant-efficiency\nmotor model, such OTC models can be used to accurately\nsimulate the battery voltage during agile flight [7]. Given\na simulation of the battery voltage, one can measure the\nperformance characteristics of a given motor-propeller com\u0002bination to determine the mapping of throttle command and\nvoltage to resulting steady-state propeller speed \u2126ss. When the\nhighest model fidelity is desired, a more sophisticated motor\nsimulation [52] can further improve the accuracy, which can be\ndesirable if the controller directly outputs single-rotor thrusts\ninstead of the more commonly used collective-thrust and body\nrates control modality.\nD. Camera and IMU Modeling\nDrone racing pushes not only the mechanical and electrical\ncomponents of drones to their limits, but is also highly\ndemanding in terms of sensor performance. For an in-depth\noverview of the many different sensor options for drone racing\nthe reader is referred to [53]. The most common sensors\naboard autonomous drones are monocular or stereo cameras\ncombined with IMUs (inertial measurement units) thanks to\ntheir low cost, low weight, and mechanical robustness.\nFor vision-based drone racing, having an accurate simula\u0002tion of the perception pipeline is critical for validation and\ncontroller development. In terms of modeling and simulation\nof the camera, it is common to use a pinhole model [54] and\nestimate the focal length, image center, and distortion parame\u0002ters from measurements. Combined with accurate information\non how far the camera is displaced from the center of gravity\nof the vehicle, this allows simulating observations. Either low\u0002level sensory observations (e.g. images) are simulated using a\nrendering engine [22], [55] or more abstract visual features\n(e.g. landmark positions) are simulated using the projection\nequations.\nIn the context of using a simulation to test approaches\nbefore attempting real-world deployment, an accurate model\nof the IMU characteristics is important, as the bias and noise\nstrongly influence the performance of many methods. The\nIMU intrinsic calibration estimates the noise characteristic of\nthe sensor. The camera-IMU extrinsic calibration estimates the\nrelative position and orientation of the two sensors as well as\nthe time offset. Kalibr [56] is a widespread tool to perform\nthese calibrations.\nHowever, the biggest source of measurement error of the\ninertial sensors onboard a drone are not the sensors themselves\nbut the strong high-frequency vibrations introduced by the fast\u0002spinning propellers. The vibrations lead to aliasing effects on\nthe IMU measurements and introduce additional motion blur\non the camera images. The structural vibrations and their effect\non the measurements are extremely difficult to model and\ncorrect for. Therefore, a suitable hardware design is imperative\nwhich dampens the mount of the camera and the IMU with\nrespect to the vehicle frame.\nIII. CLASSICAL PERCEPTION, PLANNING, AND CONTROL\nPIPELINE\nSensors\nHardware\nPerception\nSoftware\nPlanning Control Drone\nFig. 3: Architecture 1: A classic architecture for an autonomous system\nprogrammed using model-based approaches\nSince the inception of mobile robotics, a common architec\u0002ture has been primarily used to achieve autonomous navigation\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\ncapabilities across various systems. In a traditional robotics\nsoftware stack, the navigation task is broken into three main\ncomponents: Perception, Planning, and Control. A diagram of\nthis architecture can be seen in Fig. 3. This section covers\nrecent research in these areas relating specifically to agile\nflight and autonomous drone racing. All approaches detailed in\nthis section rely on first principles modeling and optimization\ntechniques.\nA. Perception\nThe perception block estimates the vehicle state and per\u0002ceives the environment using onboard sensors. The most com\u0002mon solution for state estimation of flying vehicles is visual\u0002inertial odometry (VIO), thanks to its low cost and low weight\nrequirements. VIO uses camera and IMU measurements to\nestimate the state x\u02c6 (position, orientation, and velocity) of the\ndrone platform. The inertial measurements are integrated to\nobtain relative position, orientation, and velocity estimates in\na short time, e.g., between two camera images. However, the\nintegration for a longer time, e.g., a few seconds, accumulates\nlarge drift due to scale factor errors, axis misalignment errors,\nand time-varying biases [57] that commonly affect off-the-self\nIMU measurements. The camera measurements provide rich\ninformation about the environment at a lower rate, usually\naround 30 Hz, than IMU measurements. Unlike the IMU\nmeasurements, the camera measurements are affected by envi\u0002ronmental conditions. The quality of information they provide\nfor state estimation degrades in the case of poor illumination\nconditions, textureless scenes, and motion blur. For this reason,\nthe camera and inertial measurements complement each other\nand are the standard choice for state estimation of flying\nvehicles [58]. In this section, we first give an overview of VIO\nwith a focus on the methods that can be applied for online state\nestimation of a racing drone. Second, we give an overview\nof possible additional sensor modalities that integrated into\nthe classical VIO pipeline have the potential to improve state\nestimation at high speed. Third, we conclude with a discussion\non the application of classical VIO methods to drone racing\ntasks.\n1) VIO: VIO is the most common solution for state esti\u0002mation of aerial vehicles [58] using only onboard sensing and\ncomputing, thanks to its favorable trade-off between accuracy\nand computational requirements. VIO algorithms usually com\u0002prise two main blocks: the frontend and the backend.\nThe frontend uses camera images to estimate the motion\nof the sensor. Two main approaches exist in the literature:\ndirect methods [59], [60] and feature-based methods [61]\u2013\n[63]. Direct methods work directly on the raw pixel intensities.\nThese methods commonly extract image patches and estimate\nthe camera trajectory by tracking the motion of such patches\nthrough consecutive images. The tracking is achieved by\nminimizing a photometric error defined on the raw pixel\nintensities [59]. This tracking method is particularly interesting\nfor drone racing because of its robustness in featureless\nscenarios. In fact, a direct frontend [60] is used to estimate the\nstate of a racing drone in [16]. On the contrary, feature-based\nmethods [61]\u2013[63] extract points of interest, commonly known\nas visual features or keypoints, from the raw image pixels. The\ncamera trajectory is estimated by tracking these points through\nconsecutive images. High-speed motions make it difficult (e.g.,\ndue to motion blur) to track features on many consecutive;\nconsequently, feature-based methods struggle in drone racing\nscenarios. However, feature-based methods exhibit higher ro\u0002bustness than direct methods to brightness changes. The VIO\nmethods used in [64], [65] demonstrate that a hybrid frontend,\ncombining the benefits of direct and feature-based methods, is\nbeneficial for drone racing tasks.\nThe backend fuses the output of the fronted with the inertial\nmeasurements. Two categories exist in the literature according\nto how the sensor fusion problem is solved: filtering meth\u0002ods [61] and fixed-lag smoothing methods [62], [63]. Filtering\nmethods are based on an Extended Kalman Filter (EKF).\nThese methods propagate the system\u2019s state using the inertial\nmeasurements and fuse the camera measurements in the update\nstep. The pioneer filter-based VIO algorithm is the Multi\u0002State Constraint Kalman Filter (MSCKF) originally proposed\nin [61]. Since then, many different versions of MSCKF have\nbeen developed [66]. Fixed-lag smoothing methods, also called\nsliding window estimators, solve a non-linear optimization\nproblem where the variables to be optimized are a window\nof the recent robot states. The cost function to minimize\ncontains visual, inertial, and past states marginalized residuals.\nThanks to their favorable trade-off between accuracy and\ncomputational cost, filter-based methods have been commonly\nused in drone racing [16], [64], [65].\n2) Additional sensor modalities in VIO: Recently, classical\nVIO pipelines have been augmented with event cameras [67]\u2013\n[69] or drone dynamics [70]\u2013[72], to improve state estimation\nat high speed.\nLow latency, high temporal resolution (in the order of \u00b5s),\nand high dynamic range (140 dB compared to 60 dB of stan\u0002dard cameras) are the main properties of event cameras [73],\nwhich make this novel sensor a great complementary sensor\nto standard cameras. Including event data in VIO algorithms\nonboard flying vehicles achieves increased robustness against\nmotion blur as demonstrated in [67]\u2013[69]. Although applica\u0002tions of event cameras in drone racing tasks are yet to be\nexplored, investigating the use of this sensor is a promising\nresearch direction to robustify VIO systems for agile flights.\nThe drone dynamics are used to define additional constraints\nin the estimation process. The work in [70] (VIMO) is the\nfirst to integrate error terms related to the drone transitional\ndynamics in a VIO backend. VIMO inspired a few works [71],\n[74] which propose an improved noise model of the dynam\u0002ics [74] and a learned component to account for unmodeled\naerodynamics [71]. In particular, the results of [71] show that\nthe learned aerodynamics effects help to improve the VIO\nestimates at high speeds.\nThe work in [72] proposes an odometry algorithm that relies\non an IMU as the only sensor modality (no camera is needed),\nand leverages a learned dynamics component to estimate the\nstate of the racing drone. Consequently, this method does not\nuse a visual frontend.\n3) Discussions: The work in [75] presents a benchmark\ncomparison between a number of VIO solutions on the EuRoC\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\ndataset [76]. The EuRoC dataset contains camera and IMU\ndata recorded onboard a drone flying in indoor environments.\nThe drone moves with average linear and angular velocities\nup to 0.9 m/s and 0.75 rad/s, respectively. These values are\nfar below the ones reached in drone racing. The conclusions\nof [75] show that state-of-the-art VIO algorithms provide\nreliable solutions for estimating the state of the drone at\nlimited speeds. However, these classical VIO methods cannot\nprovide accurate state estimates for drone racing tasks. VIO\nmethods accumulate large drift in scenarios characterized by\nmotion blur, low texture, and high dynamic range [77]. These\nscenarios are the norm in drone racing.\nTo help research VIO algorithms for drone racing tasks, the\nwork in [78] proposes the UZH-FPV Drone Racing Dataset.\nThis dataset contains images recorded from standard cameras,\nevent camera data, and IMU data recorded onboard a quadrotor\nflown by a human pilot. All the flights include visual chal\u0002lenges similar to those in drone racing competitions.\nSuccessful state estimation solutions for drone racing [77],\n[79] reduce the drift accumulated in VIO by localizing to a\nprior map of the track. In drone racing competitions, a map of\nthe track in the form of gate positions is known beforehand.\nThe localization process is based on the detection of the gates.\nPerforming gate detection is challenging. Often during the\nrace, none of the gates is visible in the camera\u2019s field of view.\nMoreover, motion blur makes gate corner detection difficult.\nFor this reason, gate detection and VIO are complementary.\nIn [80], a gate detector was proposed that uses an RGB camera\nto identify the gates based on their color. This detector, tailored\nto the IROS drone racing context [19], is aimed at extreme\ncomputational efficiency, which is particularly important for\ntiny drones. The method in [81] relies on detecting gates\nand using a model of the drone dynamics to estimate the\nposition of the racing drone. Differently from [77], [79], this\nmethod does not use a VIO but controls the drone based on a\nvisual-servoing approach. All the other gate detection methods\nin the literature are based on deep learning techniques [82].\nWe review them in Sec. IV. The known gate positions and\nthe detections in the onboard images are used to estimate\nthe relative pose between the camera and the gate using\nthe Perspective-n-Point algorithm (PnP) [83]. This relative\npose is used to constrain the VIO estimates and consequently\nreduce the drift. There is significant room for innovation on\nthis front, as the VIO-PnP paradigm has existed for several\nyears with little innovation. However, one clear benefit of the\nVIO-PnP approach is its ability to use a monocular camera\nsetup with a large FOV. While this comes with a lack of\nscale and higher uncertainty in motion estimation, both can\nbe compensated using inertial sensors and localization with\nrespect to known landmarks (e.g., gates). As evidenced by\nthe rich literature, this makes a monocular setup the preferred\nsolution for autonomous drone racing practitioners. The choice\nof a monocular sensor is very much in agreement with how\nhuman pilots fly: while they have goggles with two monitors,\nthe video stream they receive is from a monocular camera\nsystem on the drone. Other approaches used in early drone\nracing competitions relied on the technique of visual servoing\nvia stereo cameras [10], but relying on a stereo camera pair\ncomes with inherent difficulties. In the presence of motion\u0002blur stereo-matching approaches degrade quickly. Further\u0002more, drones only allow for a very small baseline and require\na wide-angle camera to perceive as much of the surroundings\nas possible. Both lead to very high depth estimation errors\nin the stereo setup. The solution proposed in [10] was found\nto be sensitive to indoor lighting changes and needed to be\nhand-tuned for every flight.\nRecent works [84]\u2013[86] proposed vision-based odometry\nalgorithms that are learned end-to-end. Theoretically, these\nmethods could be specialized to drone racing tasks and poten\u0002tially outperform classical VIO approaches. However, they are\nin the early development phase, and how to customize them\nfor the drone racing task is still an open research question.\nIn addition, they currently have high computational costs that\nmake them impractical for online state estimation onboard\ndrones. We refer the reader to Sec. IV for a detailed review\nof VIO methods based on deep learning.\nB. Planning\nOnce a state estimate x\u02c6 has been obtained from\nthe perception module, the next step in the classical\npipeline is to plan a feasible, time-optimal trajectory\n\u03c4ref = (xref ,uref )k, \u2200k \u2208 0 . . . N, which respects the phys\u0002ical limits of the platform as well as the constraints imposed\nby the environment. This requires predicting the drone\u2019s future\nstates such that minimum lap time is reached without crashing.\nThe planning for drones has matured over the last decade\nfrom works mostly verified in simulation to works shown in\nboth controlled lab environments and unknown unstructured\nenvironments. In the classical pipeline, planning can include\nup to two distinct planning problems, path planning and tra\u0002jectory planning. Path planning tackles the problem of finding\na geometrical path between a given start and goal position\nwhile passing specified waypoints and avoiding obstacles.\nTrajectory planning then uses a found geometric path to either\ncreate a collision-free flight corridor [87], [88], to find new\nwaypoints for the trajectory to avoid collisions [37], [89], to\nconstrain the trajectory to stay close to the found path [90],\n[91] or directly finds time allocation for a given path [92],\n[93]. Therefore, path planning can be seen as a way to select\nthe homotopy class of the collision-free space the drone flies\nthrough, while trajectory planning finds the full (or simplified)\ntime-allocated drone state predictions to be followed by the\ncontrol pipeline (Section III-C). However, many works rely\nsolely on trajectory planning as they assume no collision\nwith the environment when a trajectory is found [94]\u2013[98].\nOther works directly find a collision-free trajectory [99]\u2013[102]\nwithout having a previously found path. On the other hand,\nsome control approaches [103], [104] do not need a specified\ntime-allocated trajectory and rely only on the geometrical path\nfor controlling the drone.\nIn the following text, we first overview the most popular\npath planning approaches for drones that are used for fur\u0002ther trajectory planning. Then, we categorize trajectory plan\u0002ning methods in polynomial and spline trajectory planning,\noptimization-based trajectory planning, search-based trajec\u0002tory planning, and sampling-based trajectory planning.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\n1) Path planning: Path planning approaches can be broadly\ndivided into Sampling-based planning and Combinatorial plan\u0002ning [105]. Sampling-based methods do not construct the\nobstacle space explicitly but rather rely on random sam\u0002pling of the configuration space together with collision de\u0002tection. The most popular variants of the sampling-based\nmethods with numerous modified versions are the Probabilis\u0002tic Roadmaps (PRM) [106] and Rapidly-exploring random\ntrees (RRT) [107]. Important variants of these algorithms\nnamed RRT* and PRM* [108], can find the optimal path given\ninfinite time. The combinatorial planning methods, in contrast\nto the sampling-based methods, directly represent the obstacle\nor free space using e.g. polygonal maps or cell decomposition\nsuch as grid-based maps. With the help of a graph repre\u0002sentation of the decomposed free space, classical path search\nalgorithms such as A* [109] or Dijkstra\u2019s algorithm [110] can\nbe used to find a path.\nVariants of the above path planning approaches are used\nin many of the methods listed in Sections III-B2\u2013III-B5 to\nhelp find trajectories for either fast flight or even drone\nracing. The RRT* algorithm is used to find new waypoints\nfor polynomial trajectory planning in [89], and the PRM* is\nused as a path planning part to guide sampling-based trajectory\nplanning in [37]. The sampling-based planning in [101], [102]\ndirectly performs both path and trajectory planning. While the\ntrajectory planning objective can be to minimize time duration\nof a trajectory, the path planning typically tries to find the\nshortest paths. Therefore, some methods search for multiple\ndistinct paths to enable the trajectory planning to search over\nmultiple options on how to navigate around obstacles [37],\n[90], [91]. Other methods [87] use search-based algorithms to\nfind an initial path and to create a convex flight corridor for\nconstraining the collision-free trajectory planning. Similarly,\nthe search-based methods [99], [100] use a variant of A* to\nperform both path and trajectory planning at the same time.\n2) Polynomial and Spline Trajectory Planning: The Poly\u0002nomial and Spline methods leverage the differential flatness\nproperty [111], [112] of quadrotors and represent a trajectory\nas a continuous-time polynomial or spline. This property\nsimplifies the full-state trajectory planning to a variant where\nonly four flat outputs need to be planned (typically 3D position\nand heading). By taking their high-order derivatives, these flat\noutputs can represent a dynamically feasible trajectory with\ntheir respective control inputs. This property is used by many\npolynomial and spline methods that are nowadays among the\nmost used for general quadrotor flight.\nThe widely used polynomial trajectories [111], [112] min\u0002imize snap (4th order position derivative) of a trajectory.\nDifferent methods opted for minimizing jerk (3rd order po\u0002sition derivative) for planning a trajectory [113]. However,\nthe trajectories that result from having jerk as the primary\nobjective have been shown to minimize the aggressiveness\nof the control inputs [113], which is fundamentally different\nfrom minimizing the lap times, where extremely aggressive\ntrajectories are generally required. Richter et al. [89], there\u0002fore, extended the objective by jointly optimizing both the\nsnap of a trajectory and the total time through a user-specified\npenalty on time. Recently, Han [87] proposed a polynomial\u0002based trajectory planning method for drone racing. It jointly\noptimizes control effort and regularized time and penalizes the\ndynamic feasibility and collisions.\nBecause of their numerical stability, other methods use B\u0002splines to represent trajectories [90], [91] instead of high-order\npolynomial representations that are numerically sensitive.\nThese methods jointly optimize different objectives, simulta\u0002neously smoothness, dynamic feasibility, collision avoidance,\nsafety [91] and vision-based target tracking [94]. Recently,\nthe authors of [114] proposed a polynomial trajectory rep\u0002resentation based on the work of [115] and use it to plan\ntime-optimal trajectories through gates of arbitrary shapes for\ndrone racing, achieving close-to-time-optimal results while\nbeing more computationally efficient than [95].\nAlthough both polynomial and spline trajectories are widely\nused due to their computational efficiency, polynomial-based\ntrajectories (and their derivatives) are smooth by defini\u0002tion. Therefore, only smooth control inputs can be sam\u0002pled from them. For this reason, the traditional polynomial\nplanning [111] with a finite number of coefficients and one\npolynomial segment between every two waypoints (gates)\ncannot represent true time-optimal trajectories [95]. Yet, di\u0002rect collocation methods [116] that rely on polynomials to\napproximate the input and state dynamics can achieve nearly\noptimal performance. This is mainly due to a larger number\nof polynomial segments between the waypoints in collocation\nmethods, joint optimization of both polynomial coefficients\nand collocation points, and due to the approximation of the\nentire dynamics by polynomials. This allows to keep the\nacceleration at the possible maximum at all times similar to\nthe optimization-based shooting method [95]. Therefore, while\nthe classical polynomial and spline methods can be considered\noptimization-based, they only optimize coefficients of a single\npolynomial between every two waypoints to describe quadro\u0002tor position and heading, leveraging the differential flatness\nproperty [111], [112].\n3) Optimization-based Trajectory Planning: Optimization\u0002based trajectory planning enables us to independently select\nthe optimal sequence of states and inputs at every time\nstep, which inherently considers time minimization while\ncomplying with quadrotor dynamics and input constraints.\nOptimization-based approaches have been extensively con\u0002sidered in the literature, ranging from exploiting point-mass\nmodels [96], simplified quadrotor models [97], [117], and full\u0002state quadrotor models [88], [95].\nTime-optimality of a trajectory could also be accomplished\nby using a specific path parameterization that maximizes\nvelocity over a given path [92]. This method was shown for\nquadrotors in [93] for minimizing time of flight considering\nboth translational and rotational quadrotor dynamics. However,\nthe method only creates a velocity profile over a given path\nwhich is not further optimized.\nApart from time optimality, complying with intermediate\nwaypoint constraints is another requirement for path planning\nin autonomous drone racing. A common practice of solving\na trajectory optimization problem with waypoint constraints\nis allocating waypoints to specific time steps and minimizing\nthe spatial distance between these waypoints and the position\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nat the corresponding allocated time steps on the reference\ntrajectory (e.g. [98], [118]). The time allocation of the way\u0002points is, however, non-trivial and difficult to determine. This\nis tackled in [88], but the work uses body rates and collec\u0002tive thrust as control inputs and does not represent realistic\nactuator saturation. Recent work [95] introduces a comple\u0002mentary progress constraints (CPC) approach, which considers\ntrue actuator saturation, uses single rotor thrusts as control\ninputs, and exploits quaternions to create full, singularity\u0002free representations of the orientation space with consistent\nlinearization characteristics. While the above methods create\ntime-optimal trajectories passing through given gates, they are\ncomputationally costly and hence intractable in real-time.\n4) Search-based Trajectory Planning: Search-based plan\u0002ning methods [99], [100] rely on discretized state and time\nspaces. They solve the trajectory planning through graph\nsearch algorithms such as A*. The search graph is built using\nminimum-time motion primitives with discretized velocity,\nacceleration, or jerk input. The algorithms then use trajectories\nof a simpler model, e.g. with velocity input, as heuristics for\nthe search with a more complex model. Search-based planning\nmethods can optimize the flight time up to discretization, but\nthey suffer from the curse of dimensionality which renders\nthem too computationally demanding for a complex quadrotor\nmodel. Furthermore, the employed per-axis dynamic limits\n(velocity, acceleration, jerk) do not represent the true quadrotor\nmodel, further decreasesing the quality of found plans. Finally,\nalthough searching for minimum time trajectories, the methods\nare currently limited to planning between two states which is\nnot suitable for multi-waypoint drone racing.\n5) Sampling-based Trajectory Planning: Sampling-based\nmethods like RRT* [119] can be used for planning trajecto\u0002ries for linearized quadrotor models. Several time-minimizing\napproaches [77], [101] use a point-mass model for high\u0002level time-optimal trajectory planning. In [101], an additional\ntrajectory smoothing step is performed where the gener\u0002ated trajectory is connected with high-order polynomials by\nleveraging the differential flatness property of the quadrotor.\nAuthors of [120] use sampling-based approach with massive\nGPU parallelization and a 6D double integrator system of\nUAV with additional single integrator yaw dynamics. However,\nthese point-mass approaches need to relax the single actuator\nconstraints and instead limit the per-axis acceleration, which\nresults in trajectories that are conservative and sub-optimal\ngiven a minimum time objective. In [102], the authors use\nminimum-jerk motion primitives for connecting randomly\nsampled states inside RRT* to plan a collision-free trajectory.\nSince the authors use polynomials, this approach can only\ngenerate smooth control inputs, meaning that they cannot\nrapidly switch from full thrust to zero thrust if required.\nThe first method for planning minimum-time trajectories\nin a cluttered environment for the full quadrotor model was\nproposed in [37]. It uses a hierarchical sampling-based ap\u0002proach with an incrementally more complex quadrotor model\nto guide the sampling. The authors showed that the method\noutperforms both polynomial and search-based methods in\nminimizing trajectory time. Yet, the method is offline and\nintractable in real-time. Most recently, the authors of [104]\n2016 2017 2018 2019 2020 2021 2022\n0\n10\n20\n30\n[53] [53]\n[16]\n[40]\n[124], [125]\n[4]\nYear\nMaximum Speed [m/s]\nFig. 4: Top speeds demonstrated on autonomous drones over time from both\nliterature and competition data.\nproposed an online replanning approach that plans minimum\u0002time trajectories for a point-mass model. The paths of re\u0002planned trajectories are then consequently used by Model\nPredictive Contouring Control [103] with a full quadrotor\nmodel to maximize the progress along the path. This method\nis capable of outperforming other classical approaches due to\nthe replanning capability and progress maximization with a\nfull quadrotor model.\n6) Discussion: A planned trajectory can be understood as\nan intermediate representation that, given information about\nthe robot\u2019s dynamics and the environment, helps guide the\nplatform through the race track and ultimately perform the task\nat hand. One might argue if this intermediate representation is\nneeded at all, since ultimately, what we are looking for is a\npolicy that maps sensor information and current environment\nknowledge to the actuation space. This is generally achieved\nwith learning-based approaches, discussed in Section IV,\nwhich bypass the planning stage and directly convert sensor\nobservations to actuation commands [121]\u2013[123].\nOne of the biggest benefits of explicit planning is modu\u0002larity. This means that the developed algorithms can be used\noff-the-shelf for different drone tasks outside racing, such as\nsearch and rescue, which is not the case for single-purpose\nlearned approaches. However, explicit planning suffers from\nthe disconnection (or an open loop) between the planning and\nthe deployment stage. Unexpected deviations from the plan,\nbe it in the time domain (like unmodeled system delays) or in\nthe state-space domain (like state estimation drifts or jumps in\nthe VIO pipeline), can lead to compound errors and ultimately,\na complete system failure.\nThis can be tackled with more complex control approaches\nthat do some part of the replanning online [104].\nC. Control\nOver the last decade, significant advancements have been\nmade in agile multicopter control. Every year, increasing top\nspeeds are demonstrated in the literature as shown in Figure 4.\nControllers must be able to make real-time decisions in the\nface of poor sensor information and model mismatch. Control\ninputs, u(t), can come in a variety of modalities for quadrotor\ncontrol, such as velocity and heading, body rates and collective\nthrust, or direct rotor thrust commands [36]. Typically, a high\u0002level controller computes a desired virtual input such as body\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nrates and collective thrust, which is then passed down to a\nlow-level flight controller that directly controls the individual\nrotors on the multicopter.\nCommonly used open source controllers such as PixHawk1\nor BetaFlight2are widely available to the drone racing com\u0002munity. BetaFlight is the most commonly used low-level\ncontroller for agile drone flight and has been widely adopted\nby the First Person View (FPV) racing community.\nIn the following sections, we provide an overview of suc\u0002cessful approaches to achieving high speeds in both simulation\nand real-world applications. We sort the approaches into\nmodel-based control and coupled perception and control.\n1) Model-Based Control: In model-based control, an ex\u0002plicit model of the dynamic system is used to calculate control\ncommands that satisfy a given objective such as minimizing\ntime or tracking error. Models enable the prediction of future\nstates of the drone and provide information about the system\u2019s\nstability properties. In [126], Geometric Tracking control is\nintroduced on the Special Euclidean group SE(3) and com\u0002pletely avoids singularities commonly associated with Euler\nangle formulations on SO(3). This nonlinear controller showed\nthe ability to execute acrobatic maneuvers in simulation and\nwas the first to demonstrate recovery from an inverted initial\nattitude. The dynamic model of a quadrotor is shown to be\ndifferentially flat when choosing its position and heading as\nflat outputs in [112]. In this work, many agile maneuvers are\nperformed onboard real drones with speeds up to 2.6 m/s.\nThe previous work is extended in [26], proving that the\ndynamics model of a quadrotor subject to linear rotor drag\nis also differentially flat. The inclusion of the aerodynamic\nmodel within the nonlinear controller led to demonstrated\nflight speeds up to 4.0 m/s while reducing tracking error by\n50% onboard a real drone.\nThe differential flatness method is further extended in [127]\nby cascading an Incremental Nonlinear Dynamic Inversion\n(INDI) controller with the differential flatness controller de\u0002scribed in [112] but neglects the aerodynamic model addition\nfrom [26]. The INDI controller is designed to track the angular\nacceleration commands \u2126\u02d9from the given reference trajectory.\nTop speeds of nearly 13 m/s and accelerations over 2g are\ndemonstrated onboard a real quadrotor. The controller shows\nrobustness against large aerodynamic disturbances in part due\nto the INDI controller.\nAn investigation of the performance of nonlinear model pre\u0002dictive control (NMPC) against differential flatness methods\nis available in [125]. Cascaded controllers of INDI-NMPC\nand INDI-differential flatness are shown to track aggressive\nracing trajectories which achieve speeds of around 20m/s and\naccelerations of over 4g. While differential flatness methods\nare computationally efficient controllers and relatively easy to\nimplement, they are outperformed on racing tasks by NMPC.\nAn excellent overview of MPC methods applied to micro\naerial vehicles can be found in [128]. Because quadrotors are\nhighly nonlinear systems, nonlinear MPC is often used as\nthe tool of choice for agile maneuvers. The debate of linear\n1https://pixhawk.org/\n2https://github.com/betaflight/betaflight\nversus nonlinear MPC is thoroughly discussed in [129]. Model\nPredictive Path Integral (MPPI) control is a sampling-based\noptimal control method that has found excellent success on\nthe AutoRally project, a 1/5th scale ground vehicle designed to\ndrive as fast as possible on loose dirt surfaces [130], [131]. An\nintroduction to MPPI can be found in https://autorally.github.\nio/. The MPPI approach can be used on agile quadrotors to\nnavigate complex forest environments, however, analysis was\nonly performed in simulation [130]. Most of the successful\ndemonstrations of MPPI come from ground robots [130],\n[131]. Because MPPI is a sampling based algorithm, scaling\nto higher-dimension state spaces of quadrotors can lead to\nperformance issues as shown in [124].\nNonlinear MPC methods are also used in [40] where a\nnominal quadrotor model is augmented with a data-driven\nmodel composed of Gaussian Processes and used directly\nwithin the MPC formulation. The authors found that the\nGaussian-Process model could capture highly nonlinear aero\u0002dynamic behavior which is difficult to model in practice as\ndescribed in Sec. II. The additional terms introduced by the\nGaussian-Process added computational overhead to the MPC\nsolve times, but it was still able to run onboard a Jetson TX2\ncomputer.\nSimilar to [127], authors in [124] question whether or not\nit is necessary to explicitly model the additional aerodynamic\nterms from [40] due to the added computational and modeling\ncomplexity. Instead, they propose to learn residual model\ndynamics online using a cascaded adaptive nonlinear model\npredictive control architecture. Aggressive flight approaching\n20m/s and over 4g acceleration is demonstrated on real rac\u0002ing quadrotors. Additionally, completely unknown payloads\ncan be introduced to the system, with minimal degradation\nin tracking performance. The adaptive inner loop controller\nadded minimal computational overhead and improved tracking\nperformance over the Gaussian Process MPC by 70% on a\nseries of high-speed flights of a racing quadrotor [40], [124].\nContouring control methods can deal with competing op\u0002timization goals such as trajectory tracking accuracy and\nminimum flight times [132]. These methods minimize a cost\nfunction which makes trade-offs between these competing\nobjectives. In [133], Nonlinear Model Predictive Contouring\nControl (MPCC) is applied to control small model racecars.\nMPCC was then extended to agile quadrotor flight in [103].\nAlthough the velocities achieved by the MPCC controller were\nlower than that of [124], [125], the lap times for the same race\ntrack were actually lower due to the ability of the controller to\nfind a new time-allocation that takes into account the current\nstate of the platform at every timestep. The work is further\nextended to solve the time-allocation problem online, and to\nre-plan online [104] while also controlling near the limit of\nthe flight system. Similar work uses tunneling constraints in\nthe MPCC formulation in [134],\n2) Perception Awareness: Other methods that lie in the\nintersection of perception, planning, and control include a\nperception objective in the cost function that helps improve the\nvisibility of an objective or the quality of the state-estimation\npipeline. The methods are called perception aware, and the\nfirst methods were proposed in [135]\u2013[137] This is integral to\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nthe drone-racing problem because, to navigate a challenging\nrace course, the gates that define the course layout must be\nkept in view of the onboard cameras as much as possible.\nAdditionally, coupling the perception with the planning and/or\ncontrol problem can alleviate issues in state estimation because\nthe racing gates are usually feature-rich. Therefore, the use\nof perception-related objectives in both planning and control\npipelines is commonplace [91], [93], [120], [137], [138]. For\nexample, in [93], [137] the authors tackle the problem of\nminimizing the time required by a quadrotor to execute a given\npath, while maintaining a given set of landmarks within the\nfield of view of its on-board camera. Or in [139], where the\nauthors include a perception-aware term in the cost function\nto maximize the visibility of the closest dynamic obstacle,\nin order to readily plan a path that avoids it. These methods\nare called perception-aware [140], and in the following, we\nhighlight their core characteristics.\nThe goal is as follows: navigate a trajectory with low\ntracking error while keeping a point of interest in view while\nminimizing motion blur for maximum feature detection and\ntracking. The first instance applied to agile quadrotors was\nPAMPC introduced in [140]. In this work, a nonlinear pro\u0002gram is optimized using a sequential quadratic programming\napproximation in real time. The cost function contains both\nvehicle dynamic terms as well as perception awareness terms\nsuch as keeping an area of interest in the center of the camera\nframe.\nThis technique is applied to the drone racing problem in\n[141], where an MPPI controller is designed with a Deep\nOptical Flow (DOF) component that predicts the movement\nof relevant pixels (i.e. gates). The perception constraints are\nintroduced into a nonlinear optimization problem and deployed\nin a drone-racing simulator. The approach was not demon\u0002strated onboard real hardware. In [142], a perception-aware\nMPC based on Differential Flatness was used to ensure that\na minimum number of features are tracked between control\nupdates and thus guarantee localization. To achieve this, a\nPerception Chance Constraint within the MPC formulation is\nintroduced to ensure that at least n number of landmarks are\nwithin the field-of-view of the camera at all times with some\nbounded probability.\n3) Discussion: The performance of model-based controllers\ndegrades when the model they operate on is inaccurate [124].\nFor drones, defining a good enough model is an arduous\nprocess due to highly complex aerodynamic forces, which can\nbe difficult to capture accurately within a real-time capable\nmodel. In addition, the tuning process of many model-based\ncontrollers can be arduous, and requires a high level of domain\nexpertise to achieve satisfactory performance.\nIn any optimal control problem, a cost function that the\nuser wants to optimize must be defined. Traditionally, con\u0002venient mathematical functions leveraging convex costs are\nused because these functions are easy to optimize and there\nis a large toolchain available for optimizing such problems\nsuch as Acados [143], CVXGEN [144], HPIPM [145], or\nMosek [146]. In many drone racing papers, the optimal control\nproblem is formulated as follows:\nmin\nu\nx\nT\nN QxN +\nN\nX\u22121\nk=0\nx\nT\nk Qxk + u\nT\nk Ruk , (2)\nsubject to: xk+1 = f RK4(xk,uk, \u03b4t) ,\nx0 = xinit , umin \u2264 uk \u2264 umax ,\nwhere the state is given by xk, the control input is given by\nuk, the state cost matrix is given by Q, and the control cost\nmatrix is given by R. The optimization problem is constrained\nby the dynamics of the system given by f(xk, uk, \u03b4t) where\n\u03b4t is a finite time step. The nonlinear dynamics are typically\npropagated forward using an integrator such as 4th order\nRunge-Kutta, RK4. Additionally, the problem is subject to the\nthrust limits of the platform. umin and umax, and some initial\ncondition of the system x0. In this formulation, a reference\nposition and control are provided by a high-level planner and\nthe goal of the controller is to track the given reference, but\nthis objective is ill-defined for the drone racing problem: in\ndrone racing, we wish to complete the track in as little time\nas possible; therefore, our objective can be better formulated\nas follows:\nmin\nu\nX\nT\nk=0\n\u03b4t , (3)\nsubject to: xk+1 = f RK4(xk,uk, \u03b4t)\nx0 = xinit, x \u2208 X , u \u2208 U\nwhere T is the number of discrete time steps it takes to\ncomplete the race, and the set U contains the input constraints\n(e.g., single-rotor thrust constraints). The set X encodes all\nstate constraints, from possible limits in the state itself (e.g.,\nattitude or velocity constraints), to more complex constraints\nsuch as the fact that the drone has to pass through a set of gates\nin a pre-determined order without colliding. This approach\nrequires a time-horizon that predicts all the way until the end\nof the task which is intractable to optimize online.\nReinforcement learning (RL) methods [36], [122] can opti\u0002mize a proxy of this cost function, however do so in an offline\nfashion, requiring large amounts of training experience to\napproximate the value function. RL methods do not necessarily\ndepend on a high-level planner to provide a reference to track.\nWe will discuss some recent approaches using reinforcement\nlearning methods in the following section.\nIV. LEARNING-BASED APPROACHES\nIn this section, we present various learning-based ap\u0002proaches for drone racing. These approaches replace the plan\u0002ner, controller, and/or perception stack with a neural network.\nLearning-based methods have gained significant traction in the\nlast few years, given their ability to cope with both high\u0002dimensional (e.g. images) or low-dimensional (e.g. states) in\u0002put data, their representation power, and the ease of developing\nand deploying them on hardware.\nThe big advantage of these methods is that they require\nless computational effort than traditional methods, possibly\nenabling low-latency re-planning and control. In addition,\nthey are much more robust to system latencies and sensor\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nnoise, which can be easily accounted for by identifying them\non physical drones and then adding them to the training\nenvironments [36]. However, the major limitation of these\nmethods is their sample complexity. There are currently two\npossibilities for data gathering. The first, mostly popular in\nthe initial stages of learning-based robotics [64], [79], [147]\u2013\n[149] is to collect data in the real world. The data is then\nannotated by a human or an automated process, and used\nfor training. The second, much more popular in recent years\nand currently achieving the best results, consists of using\nsimulation for collecting training data [36], [65], [121], [150],\n[151]. However, significant simulation engineering is required\nto enable generalization if the training data comes from a\nsimulator. Conversely, generalization is easier if data come\nfrom the real world, but the data collection process is very\nslow, tedious, and expensive.\nSurveys covering existing methods for learning-based flight\nalready exist [152], [153]. In contrast to them, we cover the\nmost recent advances and give a broader discussion on the\ncomparison between learning-based and traditional methods\nfor drone racing.\nA. Learned Perception\nSensors\nHardware\nPlanning Control Drone\nSoftware\nFig. 5: Architecture 2: Learned Perception\nFor learned perception modules, the goal of the network\nis to use images from an RGB, depth, or event camera to\ndetect landmarks within the environment and output useful\nrepresentations such as waypoints, or the location of gates\non the track. A depiction of this architecture can be seen in\nFig. 5. An overview of deep learning methods for vision-based\nnavigation specific to drone racing can be found in [153].\nIn [64], a dataset of images is collected from a forward\u0002facing camera mounted on a drone labeled with the relative\nposition to the closest gate. This dataset is used to train a\nnetwork that predicts from an image both the next gate location\nand its uncertainty. Predictions are then fused with a visual\u0002inertial odometry system in an Extended Kalman Filter (EKF)\nto predict the position of the drone on the track. Similarly\nin [16], a Convolutional Neural Network (CNN) is used to\ndetect gate corners in the AlphaPilot challenge. Once the gate\ncorners are detected, classical computer vision algorithms like\nPnP can be used to find the coordinates of the gate in the\ncamera frame. Using an EKF, the gate corner locations can be\nfused with a traditional VIO pipeline to improve the estimates\nof the drone\u2019s location and orientation [16].\nOftentimes, perception networks consume precious re\u0002sources onboard computationally limited drones. To minimize\nthe network processing time, [82], [154] proposed optimized\narchitectures for gate detection on real-world data. A similar\noptimization went into \u201cGateNet\u201d [155] a CNN to detect gate\ncenter locations, distance, and orientation relative to the drone.\nThe same authors developed a follow-up work denoted as\n\u201cPencil-Net\u201d to do gate detection using a lightweight CNN\nin [156]. Most learning-based perception networks can suffer\nfrom poor generalization when deployed in environments that\nwere not included in the training data.To reduce deployment\nsensitivity to lighting conditions or background content, virtual\ngates can be added to real-world backgrounds [157].\nUp until recently, RGB and depth cameras were used\nexclusively in the drone racing task, however, these sensor\nmodalities can be sensitive to changes in the environment such\nas illumination changes. To overcome this, [158] proposed\nusing event cameras coupled with a sparse CNN, recurrent\nmodules, and a You Only Look Once (YOLO) object detector\nto detect gates. The use of event cameras overcomes potential\nissues with motion blur from the rapid movement of the drone\nand is a promising path forward for high-speed navigation.\nOverall, deep learning methods for gate detection are the de\u0002facto standard in all drone racing systems. However, such gate\ndetectors are always coupled with traditional visual-inertial\nodometry systems which explicitly estimate the metric state\nof the drone. These approaches are discussed in Sec. III. It\nis interesting to notice that learning-based odometry systems,\nsuch as [84]\u2013[86] have not yet replaced traditional methods.\nThis is particularly surprising since deep visual odometry\nsystems can specialize to a particular environment, which\ncan be useful for drone racing since the race track is fixed\nand known in advance. A disadvantage of these methods is\nthe high computational cost that makes them impractical for\nonline applications. However, research in end-to-end visual\nodometry is moving forward at a fast pace [86]. Recently,\nworks proposing end-to-end VIO systems for drones have been\npublished [159]\u2013[161]. The work in [159] proposes to learn\nglobal optical flow which is then loosely fused with an IMU\nfor full 6-DoF relative pose estimation. The method in [160]\nand its extension [161] proposes a CNN-based ego-motion\nestimator for fast flights. The performance of this method in\nthe UZH-FPV dataset shows that although end-to-end VIO\nmethods are a promising solution for agile flights, they are\nnot yet mature for drone racing. We foresee that in the near\nfuture, researchers will be able to apply these methods to the\ndrone racing task.\nB. Learned Planning & Perception\nSensors\nHardware\nControl Drone\nSoftware\nFig. 6: Architecture 3: Learned Planning and Perception\nA tightly-coupled planning and perception stack (Figure 6)\nis a very attractive algorithmic perspective. First, it greatly\nsimplifies the perception task: an explicit notion of a map\nor globally-consistent metric state is not required. Second, it\nlargely reduces computational costs, both in the pre-training\nand evaluation stages. Finally, it can leverage large amounts\nof data, collected either in simulation or the real world, to\nbecome robust against noise in perception or dynamics. Yet,\nan interesting observation is that these methods still work\nbest when coupled with an explicit estimator of the metric\nstate [5]. In contrast to traditional methods, a locally consistent\nodometry system is sufficient [65], [79], [150], waving away\nthe complexities of full-slam methods (e.g. loop-closure).\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nIn [79], a coupled perception and planning stack for drone\nracing is trained using real-world flight demonstrations. While\ngood performance is indicated on the racing task as well as\nrobustness against drift in state estimation, the method requires\nre-training for each new environment. Therefore, in the follow\u0002up work [65], data generated entirely in simulation is used\nto train the perception-planning stack, waiving the labor and\ntime-consuming requirement of data collection in the real\nworld. A similar pipeline was used for high-speed autonomous\nflight through complex environments in [150], which pro\u0002poses to train a neural network in simulation to map noisy\nsensory observations to collision-free trajectories directly.\nThis approach was later extended to nano-quadcopters [162],\nwhich won the authors the first position in the IMAV 2022\nNanocopter AI Challenge. Recent work [163], [164] has shown\nthe possibility of training sensorimotor controllers for obstacle\navoidance end-to-end using reinforcement learning, paving the\nway towards a system that could solve drone racing completely\nend-to-end. However, these works still rely on explicit state\nestimation and a controller to execute velocity commands.\nSeveral other works apply a similar stacked perception\nand planning pipeline for other autonomous drone racing\ntasks [147]\u2013[149], [165]. We point the interested reader to ex\u0002isting surveys on the role of learning in drone navigation [152].\nA few works also studied the planning problem using data\u0002driven methods, decoupling it from the perception problem.\nAn interesting approach demonstrated in the NeurIPS Game of\nDrones competition [166] used an off-the-shelf reinforcement\nlearning algorithm in place of a classic model-based planner\nfor drone racing [167].\nC. Learned Control\nSensors\nHardware\nPerception\nSoftware\nPlanning Drone\nFig. 7: Architecture 4: Learned Control\nData-driven control, like reinforcement learning, allows for\novercoming many limitations of prior model-based controller\ndesigns by learning effective controllers directly from expe\u0002rience. For example, model-free RL was applied to low-level\nattitude control [168], in which a learned low-level controller\ntrained with PPO outperformed a fully tuned PID controller\non almost every metric. Similarly, [169] used model-based RL\nfor low-level control of an a priori unknown dynamic system.\nMore related to drone racing, recent works showcased the\npotential of learning-based controllers for high-speed trajec\u0002tory tracking and drone racing [36]. Imitation learning is more\ndata efficient compared to model-free RL. In [170], aggressive\nonline control of a quadrotor has been achieved via training\na network policy offline to imitate the control command\nproduced by a model-based controller. Similarly, [171] studied\nreal-time optimal control via deep neural networks in an\nautonomous landing problem. Other work in this category\nhas shown that reinforcement learning can find optimal [122],\n[172] or highly adaptive controllers [173].\nWith a learning-based controller, it can be difficult to\nprovide robustness guarantees as with traditional methods such\nas the Linear Quadratic Regulator (LQR). While a learning\u0002based controller may provide superior performance to classical\nmethods in simulation, it may be that they cannot be used\nin the real world due to the inability to provide an analysis\nof the controller\u2019s stability properties. This is particularly\nproblematic for tracking the time-optimal trajectories required\nby drone racing. Recent works have attempted to address this\nusing Lyapunov-stable neural network design for the control\nof quadrotors [174]. This work shows that it is possible to have\na learning-based controller with guarantees that can also out\u0002perform classical LQR methods. Building upon this concept,\nreachability analysis, and safety checks can be embedded in a\nlearned Safety Layer [175].\nD. Learned Planning & Control\nSensors\nHardware\nPerception\nSoftware\nDrone\nFig. 8: Architecture 5: Learned Planning & Control\nThe second paradigm of learned control is to produce the\ncontrol command directly from state inputs without requiring\na high-level trajectory planner, as shown in the architecture\ndiagram of Figure 8. This approach enabled an autonomous\ndrone with only onboard perception, for the first time, to\noutperform a professional human, and is state-of-the-art at\nthe time of writing [5]. In autonomous drone racing, this\nwas proposed by [4], [122], where a neural network policy\nis trained with reinforcement learning to fly through a race\ntrack in simulation in near-minimum time. Major advantages\nof the reinforcement-learning-based method are its capability\nto handle large track changes and the scalability to tackle\nlarge-scale random track layouts while retaining computational\nefficiency. In [123], deep reinforcement learning is combined\nwith classical topological path planning to train robust neural\nnetwork controllers for minimum-time quadrotor flight in\ncluttered environments. The learned policy solves the planning\nand control problem simultaneously, forgoing the need for\nexplicit trajectory planning and control.\nIn this same category, another class of algorithms try\nto exploit the benefits of model-based and learning-based\napproaches using differentiable optimizers approaches [176]\u2013\n[178], which leverage differentiability through controllers. For\nexample, for tuning linear controllers by getting the analytic\ngradients [179], or for creating a differentiable prediction,\nplanning and controller pipeline for autonomous vehicles\n[180]. On this same direction, [181] equips the RL agent with\na differentiable MPC [176], located at the last layer of the\nactor network that provides the system with online replanning\ncapabilities and allows the policy to predict and optimize\nthe short-term consequences of its actions while retaining the\nbenefits of RL training.\nAll these methods inherit the classic advantage of policy\nlearning. In addition, they do not require an external controller\nto track the plan. This eliminates the discrepancy between the\nplanning and deployment stages, which is one of the main\nlimitations of traditional planning methods (Sec. III-B). Some\nof the limitations of traditional planning remain, such as the\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nrequirement of a globally-consistent state estimation and a map\nof the environment. Also, they have not yet been demonstrated\nin sparse long-horizon planning problems, e.g. flying through\na maze at high speeds, where their performance would likely\ndrop due to sample complexity.\nE. End-to-End Flight\nSensors\nHardware\nDrone\nSoftware\nFig. 9: Architecture 7: End to End Learning\nExpert pilots take raw sensory images from a first-person\u0002view camera stream and map directly to control commands.\nIn this section, we explore approaches emulating this holistic\nnavigation paradigm in autonomous drones.\nTwo families of approaches can be used to pursue an end-to\u0002end navigation paradigm. The first is substituting each of the\nperception, planning, and control blocks with a neural network.\nThis structure is followed by [182], [183], where the authors\ntrain a perception-planning network and a control network\nusing imitation learning. The perception network takes raw\nimages as input and predicts waypoints to the next gate.\nThe control network uses such predictions with ground-truth\nvelocity and attitude information to predict control commands\nfor tracking the waypoints. They showed improvements over\npure end-to-end approaches, which directly map pixels to con\u0002trol commands and were able to show competitive lap times\non par with intermediate human pilots within the Sim4CV\nsimulator [184]. Yet, the division into independent blocks leads\nto compounding errors and latencies, which negatively affect\nperformance when flying at high speeds [150].\nThe second family of approaches directly maps sensor\nobservation to commands without any modularity. This design\nis used by [185], which to date remains the only example of\nthe completely end-to-end racing system. Indeed, other end\u0002to-end systems generally require an inner-loop controller and\ninertial information to be executed. For instance, [186] trains\nan end-to-end CNN to directly predict roll, pitch, yaw, and\naltitude from camera images. Similarly, [187], [188] use a\nneural network to predict commands directly from vision. To\nimprove sample complexity, they use contrastive learning to\nextract robust feature representations from images and leverage\na two-stage learning-by-cheating framework.\nIndependently of the design paradigm they follow, end\u0002to-end navigation algorithms are currently bound to simula\u0002tion. The reasons why no method was successfully deployed\nin the real world include weak generalization to unseen\nenvironments, large computational complexity, and inferior\nperformance to other modular methods. Another interesting\nobservation is that humans can pilot a drone exclusively\nfrom visual observations. Conversely, except for [185], end\u0002to-end systems still rely on the state extracted from other\nmeasurement modalities, e.g. an IMU. The question of whether\nautonomous drones can race in the real world at high-speed\nwithout any inertial information remains open. We provide\nmore details on this question in Section VIII.\nF. Discussion\nData-driven approaches are revolutionizing the research in\nautonomous drone racing, ranging from improving the system\nmodel to end-to-end control. Currently, the best-performing\nalgorithms for drone racing include a learning-based com\u0002ponent [16], [17], and this trend is unlikely to change in\nthe coming years. Indeed, compared to classical model-driven\ndesign, they can process high-dimensional sensory inputs\ndirectly, can be made robust to any modeling uncertainty (e.g.\nlatency) by simply incorporating it in the training pipeline, and\nrequire far less engineering effort for tuning and deploying\nthem [36].\nOur analysis shows that the majority of learning-based\napproaches heavily rely on simulators. While simulators may\nget better and faster in the near future, recent advances in real\u0002world training [189], [190] and fine-tuning [191], [192] offer a\npotential alternative for zero-shot simulation to reality transfer\nfor sensorimotor policies. However, so far, these works have\nbeen limited to legged locomotion. Extension to agile drones\ncould lead to the successful deployment of end-to-end policies,\npossibly improving the state of the art in agile flight.\nAnother limitation of the approaches discussed in this\nsection is their inability to adapt to new and uncertain en\u0002vironments quickly. The field of adaptive control has studied\nthis problem extensively [193]\u2013[195]. Inspired by these works,\nthere has been a recent push to use advancements in machine\nlearning within the adaptive control framework. A method to\nlearn parametric uncertainty functions is introduced in [196].\nThese uncertainty functions could be learned offline using data\ncaptured from agile flight experiments, and then embedded\nwithin an adaptive controller to adjust controller parameters\nonline during flight. Results indicate that highly accurate\ntrajectory tracking can be achieved with this approach, even\nin the face of strong wing gusts exceeding 6.5 m/s. More\nrecently, learning-based controllers have shown the ability to\nadapt zero-shot to large variations in hardware and external\ndisturbances [197]. We see this as a promising area of research\nand one that is integral for reliable performance in changing\nenvironmental conditions.\nV. DRONE RACING SIMULATORS\nOne tool that has drastically accelerated the progress of\nresearch in autonomous drone flight is the use of simulation\nenvironments that attempt to recreate the conditions that\nreal drones experience when flying. Over the years, several\nsimulation environments have been developed for the use of\ngeneral research.\nIn 2016, the widely used RotorS simulation environment\nwas published, which extends the capabilities of the popular\nGazebo simulation engine to multi-rotors [23]. Gazebo uses\nthe Bullet physics engine for basic dynamic simulation and\ncontact forces. Linear drag on the body of the multicopter is\nsimulated based on the cross-sectional area and linear velocity\nof the simulated object. The RotorS extension features many\neasy-to-use plugins for developing multi-rotors, however, it\ndistinctly lacks the photorealistic details needed to simulate\naccurate behavior of estimation and perception pipelines.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nAirSim was introduced by Microsoft in 2018 as a photo\u0002realistic simulator for the control of drones [21]. It is built on\nthe Unreal graphics engine and features easy-to-use plugins\nfor popular flight controllers such as PX43, ArduPilot4, and\nothers. It was used in the 2019 NeurIPS Game of Drones\nchallenge [166]. Because of the photorealism of AirSim, it\nis possible to simulate the entire perception and estimation\npipeline with a good possibility of transfer to real-world drone\nsystems. Additionally, AirSim comes pre-packaged with an\nOpenAI-Gym environment for training Reinforcement Learn\u0002ing algorithms. Organizations such as Bell, Airtonomy, and\nNASA are using AirSim to generate training data for learning\u0002based perception models.\nFlightGoggles [55] was developed as another photorealistic\nsimulator and was used as the primary simulation environment\nfor the Lockheed Martin AlphaPilot challenge. FlightGoggles\ncontains two separate components: a photorealistic render\u0002ing engine built with Unity3D and a dynamic simulation\nimplemented in C++. FlightGoggles provides an interface\nwith real-world vehicles using a motion capture system; such\nan interface allows the rendering of simulated images that\ncorrespond to the position of physical vehicles in the real\nworld.\nA recent simulator focused on Safe RL was proposed\nin [198]. It uses Gazebo and the Pybullet physics engine as\nthe backend. Leaderboards for several safety-focused training\nenvironments exist, encouraging researchers to submit their\napproaches and compete with other researchers around the\nworld.\nFlightmare [22] is a simulation environment featuring pho\u0002torealistic graphics provided by the Unity engine. The physics\nengine is decoupled and can be swapped out with various\nengines for user-defined levels of simulation fidelity. Similar to\nFlightGoggles, Flightmare can also provide hardware-in-the\u0002loop simulation functions where a virtual, synthetic camera\nimage can be provided to the drone for use in control and\nestimation [8].\nFinally, Aerial Gym [199] is a GPU-accelerated simulator\nthat allows simulating millions of multirotor vehicles in paral\u0002lel with nonlinear geometric controllers for attitude, velocity\nand position tracking. Additionally, the simulator offers a\nflexible interface for modeling a large number of obstacles\nand generating data such as RGB, depth, segmentation, and\noptical flow.\nVI. COMPETITIONS\nTo gauge the progress of the field as a whole, several\ndrone racing competitions have taken place since 2016. We\ninclude a graphical overview of these events in Figure 2. The\nAutonomous Drone Racing (ADR) competition was an annual\ncompetition which took place during the IROS conference\nbetween 2016 and 2019. In 2016, 11 teams competed in\nautonomous drone racing and were tasked to navigate a series\nof gates in sequence. The positions of the gates were not\nknown to the participating teams ahead of time, therefore\n3https://px4.io/\n4https://ardupilot.org/\nteams flew very cautiously identifying the next waypoints\nonline. Each team was given 30 minutes prior to the official\ncompetition to fly the course as many times as they wished.\nThe winning team, from KAIST, made it through 10 of the 26\ngates in 1 minute and 26 seconds. For comparison, a human\nwas able to complete the entire 26-gate course in 1 minute 31\nseconds. A survey summarizing the approaches used for these\nearly competitions can be found in [19]. The following year,\na similar competition took place during IROS in Vancouver,\nCanada, with better results. This time, 14 teams participated\nand were given a CAD drawing of the course prior to the\nevent with locations and dimensions of all gates. Only 5 teams\nparticipated in the final in-person event, with the winning team\nmaking it through 9 out of 13 gates in over 3 minutes. A\nsummary of the winning approaches can be found in [14].\nTwo more ADR competitions took place at IROS 2018 and\n2019, with drones navigating courses faster and more reliably.\nIn 2019, Lockheed Martin sponsored the AlphaPilot AI\nDrone Racing Innovation Challenge where a 1 million dollar\ngrand prize was awarded to the winning team [200]. The\ncompetition took place first in a virtual qualifying round which\nused the FlightGoggles simulation environment [55]. Nine\nteams out of more than 400 worldwide qualified for the final\nchallenge which included navigating a new track in a time-trial\nsetting against an expert human pilot. Such competition took\nthe form of a tournament, with three seasonal races and a final\nchampionship race. This made it very different from previous\nsingle-day competitions. Ultimately, professional pilot Gabriel\nKocher, from the Drone Racing League, manually piloted his\ndrone through the course in only 6 seconds. It took 11 seconds\nto the winner, MAVLab from TU Delft, and 15 seconds\nto the second-place winner, UZH-RPG from the University\nof Zurich, to complete the course autonomously. The two\ndifferent approaches are documented in [16], [17]. Further\ncomments are provided by the winner in [201]. Perez et al.\nprovide an overview of the types of hardware used for some\nof the drone racing competitions mentioned so far [53].\nIn 2019, the Game of Drones competition took place at the\nNeurIPS conference. This competition was purely simulation\u0002based and used the AirSim simulation environment built by\nMicrosoft [15], [21], [166]. Participants in the Game of Drones\ncompetition raced against simulated opponents in a head-to\u0002head fashion, similar to how humans compete in FPV drone\nracing. Teams raced against a single simulated opponent,\nnavigating through a complex series of gates in three different\ntiers: Planning Only, Perception Only, and Perception with\nPlanning.\nIn 2022, at the Swiss Drone Days event in Zurich, Switzer\u0002land, three of the world\u2019s best human pilots competed against\nresearchers from the Robotics and Perception Group of the\nUniversity of Zurich. Flight speeds exceeding 100 kph were\ndemonstrated by the autonomous drones. When relying on\nmotion capture, the autonomous drones were able to achieve\nsignificantly faster laptimes than the expert human pilots.\nThey additionally demonstrated it was possible to win races\nwithout motion capture, using only onboard computing and\nsensors to navigate the race track. IEEE Spectrum author Evan\nAckermann discusses the multi-day event in [202].\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nLooking into the future, the Abu Dhabi Autonomous Racing\nLeague recently announced plans for an autonomous drone\nracing competition in 2025.\nVII. DATASETS, HARDWARE, AND OPEN SOURCE CODE\nIn this section, we provide an overview of the existing open\nsource code bases, useful datasets for autonomous drone racing\nas well as hardware considerations. We first discuss datasets,\nand then group the existing open source code bases by their\nuse-cases in table I and conclude with a brief overview over\ndrone racing hardware.\nA. Datasets\nIn 2018, researchers from MIT released a large scale dataset\nfor perception during aggressive UAV flight [205]. This dataset\ncontains over 10 hours of flight data which includes simulated\nstereo and downward-facing camera images at 120 Hz, real\u0002world IMU data at 100 Hz, motor speed data at 190 Hz, and\nmotion capture data at 360 Hz. The sensor suite was chosen\nsuch that algorithms like Visual-Inertial Odometry (VIO) or\nSimultaneous Localization and Mapping (SLAM) could be\nevaluated on the dataset.\nIn 2019, the UZH-FPV Drone Racing Dataset was released,\nwhich contains many agile maneuvers flown by a professional\nracing pilot [78]. The dataset includes indoors and outdoors\nreal-world camera images, inertial measurements, event cam\u0002era data, and ground truth poses provided by an advanced\nmotion capture system (a total station) providing millimeter\u0002level accuracy. In 2024, the dataset was extended with new\ndata recorded onboard an autonomous racing drone flying\nin a racing track with peak speed exceeding 20 m/s. This\nnew data includes large field-of-view camera images, inertial\nmeasurements, and ground truth from a motion capture system.\nSimilar to the authors in [205], the authors of this dataset hope\nto push the state of the art in state estimation during aggressive\nmotion and have created competitions to allow researchers to\ncompete against one another on this agile flight benchmark.5.\nA recent effort reported in [207] open-sourced high-quality\ndata from both autonomous and human-piloted flights. This\neffort enables the study of both the perception and control\nproblem without actual hardware, lowering the barrier of entry\nfor studying drone racing.\nResearch on how expert human pilots focus on their targets\nduring flying and provide a dataset that contains flight trajec\u0002tories, videos, and data from the pilots is examined in [206]\nNeuroBEM [35] is a hybrid aerodynamic quadrotor model\nwhich combines blade-element-momentum-theory models\nwith learned aerodynamic representations from highly ag\u0002gressive maneuvers. While the model is fit to the specific\nquadrotor platform defined in [8], the approach can be used\nfor any quadrotor platform and provides over 50% reduction in\nmodel prediction errors compared to traditional, exclusively\u0002first-principles approaches.\n5https://fpv.ifi.uzh.ch/uzh/uzh-fpv-leader-board/\nB. Open-Source Code\nA significant amount of autonomous drone racing research\nhas been open sourced to the community, making implemen\u0002tation less daunting for newcomers to the field. A collection of\nall known drone racing repositories has been provided to the\nreader in Table I. These code bases range across controllers,\nplanners, sensor calibration, and even entire software stacks\ndedicated to drone racing. We encourage both newcomers and\nexperienced researchers to check out the extensive amount of\nopen source code bases available and contribute back to the\ncommunity.\nC. Hardware\nThis survey does not intend to cover the hardware design\nof racing drones rigorously. For an in-depth overview, see [8],\nwhere the hardware and software design for developing a very\ncapable research platform are discussed. To make this survey\nself-contained, this section presents a brief overview of the\nhardware design of a racing drone nevertheless.\n1) Racing Drone Design: A suitable hardware design\nshould maximize the agility and acceleration of the drone,\nand hence, it needs to be as lightweight as possible [210].\nFor drones featuring onboard compute, the drone size is thus\nlower-bounded by the size of the computer. Currently, the\nNVIDIA Jetson family is the smallest off-the-shelf hardware\nwith sufficient compute to run complex neural networks, and\nit leads to drones built on 6 inch frames. Carbon fiber offers\nan excellent compromise between the weight and durability of\nthe frame, while other parts (such as holders for the computer)\ncan be designed using a 3D printer.\nFor actuation, fast-spinning brushless DC motors are ideal\nbecause of their high specific power output, often exceeding\n500 W for a 50 g motor. In general, larger propellers will\nimprove the energy efficiency of the drone [7] while smaller\npropellers lead to a faster motor response. On a 6 inch frame,\nthree-bladed 5 inch propellers present a good compromise. To\nsustain the power demand of brushless drone-racing motors\n(often exceeding 2 kW at full throttle [7]) a lithium-polymer\nbattery with a sufficiently high discharge current rating (e.g.,\n120 C) is required.\nThe Pixhawk PX4 flightstack, despite being commonly used\nfor quadrotors [11], [211], fixed-wings [212], and hybrid\nVTOL platforms [213], is not optimized for agile flight.\nConversely, agile autonomous research platforms [5], [8] use\nBetaflight as a low-level controller, similar to professional\nhuman racing pilots.\nThe design of a capable racing drone is important for re\u0002searchers developing new technology. However, in many drone\nracing competitions, the hardware design is not left to the\nparticipants but is standardized. This approach is common in\nhuman drone racing, where thousands of identical drones are\nbuilt before each competition. This concept was also adopted\nby the AlphaPilot [200] competition, where all participants\nused a given platform. Overall, this approach ensures fair\ncompetition.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nTABLE I: Open Source Software and Datasets\nName and Reference Category Year Link\nPAMPC [140] Controller 2018 https://github.com/uzh-rpg/rpg mpc\nDeep Drone Acrobatics [121] Controller 2019 https://github.com/uzh-rpg/deep drone acrobatics\nData Driven MPC [40] Controller 2020 https://github.com/uzh-rpg/data driven mpc\nHigh MPC [203] Controller 2022 https://github.com/uzh-rpg/high mpc\nAutoTune [204] Controller Tuner 2022 https://github.com/uzh-rpg/mh autotune\nBlackbird [205] Dataset 2018 https://github.com/mit-aera/Blackbird-Dataset\nUZH-FPV [78] Dataset 2019 https://fpv.ifi.uzh.ch/\nNeuroBEM [35] Dataset 2020 https://rpg.ifi.uzh.ch/NeuroBEM.html\nEye Gaze Drone Racing [206] Dataset 2021 https://osf.io/gvdse/\nTII Drone Racing Dataset [207] Dataset 2024 https://github.com/tii-racing/drone-racing-dataset\nTime-optimal Planning for Quadrotor Waypoint Flight [95] Planner 2021 https://github.com/uzh-rpg/rpg time optimal\nMinimum-Time Quadrotor Waypoint Flight in Cluttered\nEnvironments [37]\nPlanner 2022 https://github.com/uzh-rpg/sb min time quadrotor planning\nRotorS [23] Simulator 2016 https://github.com/ethz-asl/rotors simulator\nAirSim [166] Simulator 2018 https://microsoft.github.io/AirSim/\nFlightGoggles [55] Simulator 2019 https://github.com/mit-aera/FlightGoggles\nFlightmare [22] Simulator 2020 https://uzh-rpg.github.io/flightmare/\nLearning to fly\u2014a gym environment with pybullet physics for\nreinforcement learning of multi-agent quadcopter control [198]\nSimulator 2021 https://github.com/utiasDSL/gym-pybullet-drones\nAerial Gym [199] Simulator 2023 https://github.com/ntnu-arl/aerial gym simulator\nSim 2 Real Domain Randomization [65] Sim2Real Transfer 2019 https://github.com/uzh-rpg/sim2real drone racing\nRPG Quadrotor Control [26] Software Stack 2017 https://github.com/uzh-rpg/rpg quadrotor control\nAgilicious [8] Software Stack 2022 https://github.com/uzh-rpg/agilicious\nKalibr [56] Camera Calibration 2022 https://github.com/ethz-asl/kalibr\nVID-Fusion [74] Estimation 2021 https://github.com/ZJU-FAST-Lab/VID-Fusion\nFast-Racing [87] Planner 2021 https://github.com/ZJU-FAST-Lab/Fast-Racing\nEgo-planner [208] Planner 2021 https://github.com/ZJU-FAST-Lab/ego-planner\nGCOPTER [115] Planner 2022 https://github.com/ZJU-FAST-Lab/GCOPTER\nFASTER [209] Planner 2021 https://github.com/mit-acl/faster\nPanther [139] Planner 2022 https://github.com/mit-acl/panther\nDeep Panther [138] Planner 2023 https://github.com/mit-acl/deep panther\nRaptor [91] Planner 2021 https://github.com/HKUST-Aerial-Robotics/Fast-Planner\n2) Beyond Quadcopters: While this survey focuses on\nmulti-copter drones, future drone racing competitions will\ngo beyond this platform. Indeed, FPV Fixed-Wing Racing is\nalready a popular sport among human pilots [214]. For exam\u0002ple, vertical takeoff and landing (VTOL) drones might offer\na great alternative to quadcopters. VTOL aircraft combine\nthe high speeds achieved by fixed-wing drones with some of\nthe maneuverability of multicopters. Pioneering works on this\nplatform have already shown agile control [215] and trajectory\ngeneration for aerobatic VTOL flight [216]. Perhaps, once\nsuch research platforms are available off the shelf, VTOL\naircraft racing will become a popular platform for autonomous\ndrone racing research.\nVIII. OPEN RESEARCH QUESTIONS AND CHALLENGES\nWhile a lot of progress has been made, there are still many\nchallenges to be overcome in drone racing research. In the\nfollowing, we discuss the most interesting challenges in detail.\nA. Challenge 1: Reliable State Estimation at High-Speeds\nIn its current form, online, robust, and accurate state\nestimation is highly beneficial when pushing autonomous\ndrones to their limits. Currently, classical state estimation\napproaches based on visual-inertial odometry cannot cope\nwith the perceptual challenges present in drone racing tasks.\nMotion blur, low texture, and high dynamic range are some\nreasons why classical VIO algorithms accumulate large errors\nin localization. The miscalibration of intrinsic and extrinsic\ncamera parameters can lead to improper estimates of the\ncamera pose on a drone. This is due to local movements of the\ncamera frame relative to the drone body, as well as changes\nin temperature and pressure. VIO drift can render the state\nestimates unusable unless corrected through localizations to\na prior map. New sensor modalities, such as event cameras,\ncould potentially alleviate this issue. Although event-aided\nVIO algorithms for drones have been proposed to improve\nrobustness to motion blur, they have not been demonstrated at\nhigh speeds as seen in drone racing. Future research in agile\nflight may focus on finding new event representations that are\ncomputationally efficient and compatible with classical VIO\nformulations. One example is to exploit direct methods [217].\nOther promising sensor modalities are motor speed controllers\nand force sensors. These sensor measurements could be used\nto include more advanced drone models in VIO, e.g. modeling\naerodynamics effects, in order to limit the drift that accu\u0002mulates where camera measurements are degraded. One of\nthe main consequences of motion blur, low texture, and high\ndynamic range is unreliable feature extraction and matching.\nThis consequently degrades the performance of the visual\nfrontend. Deep learning methods have the potential to solve\nthis problem. What hinders the application of these methods to\ndrone racing at the moment is their computational cost. Future\nresearch should work on lightweight neural networks that can\nprovide inference at a high rate. Neural networks could also\nbe used to remove non-zero mean noise and constant errors\nfrom the inertial measurements. A potentially fruitful area of\nresearch is in combining neural networks for input processing\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nwith a geometry-based VIO backend. This could lead to the\nnext step in the research on VIO for drone racing. Current\nworks [86], [218] have shown that this direction outperforms\nend-to-end visual-based odometry methods.\nB. Challenge 2: Flying from Purely Vision\nState-of-the-art autonomous navigation methods rely on\nvisual and inertial information, usually combined with classic\nperception algorithms. Conversely, expert human pilots rely\non nothing more than a first-person-view video stream, which\nthey use to identify goals and estimate the ego-motion of the\ndrone. Building systems that, similarly to human pilots, only\nrely on visual information is very interesting from a scientific\nperspective. Indeed, since simulating RGB is very challenging,\nsolving this question might require lifelong learning algo\u0002rithms operating in the real world. In addition, eliminating\ninertial information might have some engineering advantages\ntoo, e.g., data throughput, power consumption, and lower cost.\nSeminal works in this direction try to understand how humans\nsolve this task [206], [219]. They found that expert pilots can\ncontrol drones despite a 200ms latency, which is compensated\nby the human brain. Taking inspiration from biology, a recent\nwork [220] shows that it is possible to fly with camera images\nand an onboard gyroscope (e.g., removing the accelerometer),\nas long as the system never hovers. However, the above\nquestions still remain mostly open and a good avenue for\nresearch at the intersection of computer vision, neuroscience,\nand biology.\nC. Challenge 3: Multiplayer Racing\nMuch of the work done up until this point on autonomous\ndrone racing has focused on time-optimal flight without con\u0002sidering how a capable opponent might impact the compe\u0002tition dynamics. In FPV races, pilots can compete against\nup to 5 opponents simultaneously, bringing about the need\nto anticipate how their opponents might behave. Humans\nare astonishingly capable of recognizing opportunities for\novertaking and executing complex maneuvers in the face of\nlarge aerodynamic disturbances caused by flying close to\nanother drone. Achieving such capabilities requires an agent\nto estimate their opponent\u2019s state using only onboard visual\nsensors. However, these observations in drone racing are\nsparse because the camera faces forward along the heading\naxis, meaning that the only time an opponent is observable\nis when the ego-agent is behind them. Sophisticated motion\nand planning models which can propagate predictions of the\nopponents\u2019 states and racing lines through time are necessary\nto anticipate collisions or overtaking opportunities. One way\nto simplify the problem is combining classical vision with\nlearning-based control, which has shown promising results in\nmulti-agent zero-sum games for locomotion [221]. An initial\nstudy [222] examined how game-theoretic planners can lead\nto highly competitive behavior in two-player drone racing,\nhowever, this work was confined to racing on a 2D plane.\nThe work was further extended to 3D spaces in [223], but\nthere is a significant opportunity for researchers to explore\nthe competitive nature of drone racing and develop interesting\nracing strategies that lead to time-optimal agents that are able\nto deal with complex opponent behavior.\nD. Challenge 4: Safety\nAutonomous drone racing research has so far focused\non demonstrating that superhuman performance in racing is\npossible in controlled conditions [5] but has put less em\u0002phasis on risk and safety. We predict that this trend will\nsoon change. Adding safety to agile flight has gained much\nattention recently [9], [198]. Initial works focused on gen\u0002erating a collision-free trajectory [224]\u2013[226] with less em\u0002phasis on performance. More geared towards agile flight, the\nworks [209], [227]\u2013[229] have studied the problem of trading\noff safety and performance. All the aforementioned works\nrely on solving constrained optimization problems. Outside\nof drone racing, similar paradigms have been developed and\nhave the potential to inspire future algorithms. Such methods\nare, for example, conformal analysis [230], chance-constrained\ndynamic programming [231], control barrier functions [232],\nor reachability analysis [233]. The latter has been successfully\napplied in the context of autonomous driving with collision\navoidance [234], [235].\nMore modern, learning-based methods have been explored\nfor risk-aware autonomous driving in the context a of map\u0002prediction approach [236] and in combination with Tube\nMPC [237], a form of MPC that takes stochasticity into\naccount. However, such approaches generally do not scale\nto high-dimensional perception but rely on robust state\u0002estimation for all involved agents. Combining such algorithms\nwith the methods for vision-based, high-speed drone racing\npresented in this survey could solve both of these prob\u0002lems simultaneously. As a first step in this direction, recent\nwork [238] has shown that a learned control policy can be\nconditioned on an auxiliary input signal from a user. The\nsignal regulates the maximally available thrust, leading to a\nsingle learned policy that can race at various speeds and risk\nlevels.\nE. Challenge 5: Transfer to Real-World Applications\nDrone racing, while an extraordinarily challenging research\nenvironment, is ultimately not the end goal. Opportunities\nexist for technology transfer between the drone racing research\ncommunity to real-world applications such as search and res\u0002cue, inspection, agriculture, videography, delivery, passenger\nair vehicles, law enforcement, and defense. However, applica\u0002tions that leverage the full agility of the platform have much\nto gain. Drones that fly fast, fly farther, therefore increasing\nthe productivity of drones in every commercial sector [7].\nOne of the major challenges to real-world application is gen\u0002eralization to conditions where the environmental knowledge\nbefore deployment is limited. For example, we often do not\nhave a known map ahead of time for real-world applications,\nwhich requires simultaneous estimation of the state of the\ndrone while mapping the environment. However, a central\ntheme of drone racing research has been the development of\nadaptive control strategies and decision-making algorithms to\nenable drones to react rapidly to changes in the race track or\nthe robot condition(Sec. III-C and Sec. III-A). These strategies\ncan be used to handle real-world applications where envi\u0002ronmental knowledge is imperfect and to enable adaptation\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nto unforeseen obstacles and challenges. In addition, learning\u0002based sensorimotor controllers for drones, increasingly more\npopular due to research on racing, have been designed with\nthe ability to generalize from limited data, adapt, and improve\ntheir performance over time (Sec. IV). Such generalization and\nadaptation abilities have already been applied to cases where\nthere is no previous knowledge of the environment [150].\nBuilding algorithms that can continually improve from their\nexperience is another alternative to favor this transfer. While\nrecent advances in reinforcement learning research point to the\nfeasibility of this path [191], [192], [239], it is unclear when\nand how such recent approaches would be applicable to drones\nor similarly agile platforms in the real world. Collecting data\nfor continual RL onboard a drone is notoriously difficult. This\nis because the drone does not have the luxury of remaining in\ncontact with the ground like legged robots and cars, and thus\nhas to immediately know how to hover otherwise a crash will\noccur. One interesting area that may be useful for continual RL\nin drones is the notion of \u201csafe-RL\u201d. The goal of safe RL is to\nenable exploration without ever incurring catastrophic failure\nof the system. Initial work on this topic can be found in [240].\nA survey paper covering safe RL methods can be found in [9].\nFurthermore, a thorough review paper on continual, or life\u0002long RL can be found in [241].\nIX. CONCLUSIONS AND SUMMARY\nFrom racing at a pace comparable to walking speed [19],\nautonomous drones have advanced to surpassing world cham\u0002pions [5]. Such an exponential advance has been driven by\nboth algorithmic innovations, e.g., learning sensorimotor con\u0002trollers in simulation, and system engineering improvements.\nSuch advances span the entire navigation pipeline: perception,\nplanning, and control. Our paper comprehensively covers each\nof these topics. Methodologically, the dominant trend has been\na shift from conventional methods to data-driven solutions.\nHowever, in contrast to fields like computer vision and natural\nlanguage processing, neural networks did not replace but\ncoexist with traditional methods: no method with competitive\nperformance in the real world is fully data-driven. The most\nresilient part of the pipeline is state estimation, where strong\nprior knowledge about the dynamics and environment are still\nneeded to cope with the lack of sensorimotor data. In the short\nterm, we predict that such a hybrid approach could be applied\nto other physical systems, e.g., autonomous ground vehicles\nand personal robots. However, in the long term, we predict\nthat, similarly to research in computer vision and natural\nlanguage processing, neural networks will replace each part of\nthe pipeline. This will require many innovations, e.g., compu\u0002tationally efficient architectures, offline pre-training strategies,\nand fast adaptation schemes to previously unseen conditions.\nWhile autonomous drones are already superhuman in con\u0002trolled scenarios, many challenges are yet to be solved to\noutperform human champions in official drone racing leagues\nand transfer the findings to real-world applications.\nX. ACKNOWLEDGEMENTS\nThe authors thank Manasi Muglikar for her valuable inputs\non event-camera methods for state estimation and perception.\nXI. CONTRIBUTIONS\nDrew Hanover initiated the idea of this paper, created the\npaper structure, and contributed to all sections of this paper\nwhile coordinating efforts amongst the co-authors. Antonio\nLoquercio contributed to the paper structure and the learning\u0002based sections. Leonard Bauersfeld authored the Drone Mod\u0002eling section and created the graphics seen throughout. Angel\nRomero contributed to the Classical Planning and Control\nsections. Giovanni Cioffi contributed to the Classical Percep\u0002tion and Challenges sections. Yunlong Song contributed to\nthe Simulators and Learning-Based Planning/Control sections.\nRobert Penicka contributed to both Classical and Learning\u0002Based Planning sections. Elia Kaufmann contributed to the\npaper structure and throughout the Learning-Based sections.\nDavide Scaramuzza contributed to the general paper structure\nand revised the paper thoroughly and critically.\nREFERENCES\n[1] T. A. Wilkinson, Early Dynastic Egypt. Routledge, 2002.\n[2] S. M. Arab, \u201cThe sed-festival (heb sed) renewal of the kings\u2019 reign,\u201d\nArab World Books, Nov 2017.\n[3] J. Betz, H. Zheng, A. Liniger, U. Rosolia, P. Karle, M. Behl, V. Krovi,\nand R. Mangharam, \u201cAutonomous vehicles on the edge: A survey on\nautonomous vehicle racing,\u201d IEEE Open J. Intell. Transp. Syst., vol. 3,\npp. 458\u2013488, 2022.\n[4] Y. Song, A. Romero, M. Mueller, V. Koltun, and D. Scaramuzza,\n\u201cReaching the limit in autonomous racing: Optimal control versus\nreinforcement learning,\u201d Science Robotics, p. adg1462, 2023.\n[5] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Muller, V. Koltun, and \u00a8\nD. Scaramuzza, \u201cChampion-level drone racing using deep reinforce\u0002ment learning,\u201d Nature, vol. 620, no. 7976, pp. 982\u2013987, Aug 2023.\n[6] Z. Ameli, Y. Aremanda, W. A. Friess, and E. N. Landis, \u201cImpact of uav\nhardware options on bridge inspection mission capabilities,\u201d Drones,\nvol. 6, no. 3, p. 64, 2022.\n[7] L. Bauersfeld and D. Scaramuzza, \u201cRange, endurance, and optimal\nspeed estimates for multicopters,\u201d IEEE Robotics and Automation\nLetters, vol. 7, no. 2, pp. 2953\u20132960, 2022.\n[8] P. Foehn, E. Kaufmann, A. Romero, R. Penicka, S. Sun, L. Bauersfeld,\nT. Laengle, G. Cioffi, Y. Song, A. Loquercio, and D. Scaramuzza, \u201cAg\u0002ilicious: Open-source and open-hardware agile quadrotor for vision\u0002based flight,\u201d Science Robotics, vol. 7, no. 67, p. eabl6259, 2022.\n[9] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and\nA. P. Schoellig, \u201cSafe learning in robotics: From learning-based control\nto safe reinforcement learning,\u201d Annual Review of Control, Robotics,\nand Autonomous Systems, vol. 5, pp. 411\u2013444, 2022.\n[10] S. Jung, S. Cho, D. Lee, H. Lee, and D. H. Shim, \u201cA direct visual\nservoing-based framework for the 2016 iros autonomous drone racing\nchallenge,\u201d Journal of Field Robotics, vol. 35, no. 1, pp. 146\u2013166,\n2018.\n[11] K. Mohta, M. Watterson, Y. Mulgaonkar, S. Liu, C. Qu, A. Makineni,\nK. Saulnier, K. Sun, A. Zhu, J. Delmerico et al., \u201cFast, autonomous\nflight in gps-denied and cluttered environments,\u201d Journal of Field\nRobotics, vol. 35, no. 1, pp. 101\u2013120, 2018.\n[12] \u201cAGILEFLIGHT: Low-latency Perception and Action for Agile Vision\u0002based Flight,\u201d https://cordis.europa.eu/project/id/864042.\n[13] \u201cAUTOASSES: Autonomous Aerial Inspection of GNSS-denied and\nConfined Critical Infrastructures,\u201d https://cordis.europa.eu/project/id/\n101120732.\n[14] H. Moon, J. Martinez-Carranza, T. Cieslewski, M. Faessler, D. Falanga,\nA. Simovic, D. Scaramuzza, S. Li, M. Ozo, C. De Wagter et al.,\n\u201cChallenges and implemented technologies used in autonomous drone\nracing,\u201d Intelligent Service Robotics, vol. 12, no. 2, pp. 137\u2013148, 2019.\n[15] Microsoft, \u201cGame of drones.\u201d\n[16] P. Foehn, D. Brescianini, E. Kaufmann, T. Cieslewski, M. Gehrig,\nM. Muglikar, and D. Scaramuzza, \u201cAlphapilot: Autonomous drone\nracing,\u201d Autonomous Robots, vol. 46, no. 1, pp. 307\u2013320, 2022.\n[17] C. De Wagter, F. Paredes-Valles, N. Sheth, and G. de Croon, \u201cThe \u00b4\nartificial intelligence behind the winning entry to the 2019 ai robotic\nracing competition,\u201d Field Robotics, vol. 2, 2022.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\n[18] \u201cThe motorsport concept building an autonomous mobility\necosystem,\u201d https://a2rl.io/news/18/The-Motorsport-Concept\u0002Building-an-Autonomous-Mobility-Ecosystem---ASPIRE-s\u0002Executive-Director,-Dr-Tom-McCarthy.\n[19] H. Moon, Y. Sun, J. Baltes, and S. J. Kim, \u201cThe iros 2016 competitions\n[competitions],\u201d IEEE Robotics and Automation Magazine, vol. 24,\nno. 1, pp. 20\u201329, 2017.\n[20] R. Mahony, V. Kumar, and P. Corke, \u201cMultirotor aerial vehicles:\nModeling, estimation, and control of quadrotor,\u201d IEEE Robotics and\nAutomation magazine, vol. 19, no. 3, pp. 20\u201332, 2012.\n[21] S. Shah, D. Dey, C. Lovett, and A. Kapoor, \u201cAirsim: High-fidelity\nvisual and physical simulation for autonomous vehicles,\u201d in Field and\nservice robotics. Springer, 2018, pp. 621\u2013635.\n[22] Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scaramuzza,\n\u201cFlightmare: A flexible quadrotor simulator,\u201d in Proceedings of the\n2020 Conference on Robot Learning, 2021, pp. 1147\u20131157.\n[23] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, \u201cRotors\u2014a modular\ngazebo mav simulator framework,\u201d in Robot operating system (ROS).\nSpringer, 2016, pp. 595\u2013625.\n[24] J. Meyer, A. Sendobry, S. Kohlbrecher, U. Klingauf, and O. Von Stryk,\n\u201cComprehensive simulation of quadrotor uavs using ros and gazebo,\u201d\nin International conference on simulation, modeling, and programming\nfor autonomous robots. Springer, 2012, pp. 400\u2013411.\n[25] R. W. Prouty, Helicopter performance, stability, and control. Krieger\nPub Co, 1995.\n[26] M. Faessler, A. Franchi, and D. Scaramuzza, \u201cDifferential flatness\nof quadrotor dynamics subject to rotor drag for accurate tracking of\nhigh-speed trajectories,\u201d IEEE Robotics and Automation Letters, vol. 3,\nno. 2, pp. 620\u2013626, 2017.\n[27] S. Yoon, H. C. Lee, and T. H. Pulliam, Computational Analysis of\nMulti-Rotor Flows.\n[28] P. V. Diaz and S. Yoon, High-Fidelity Computational Aerodynamics of\nMulti-Rotor Unmanned Aerial Vehicles.\n[29] S. Yoon, Nasa, P. V. Diaz, D. D. Boyd, W. M. Chan, and C. R.\nTheodore, \u201cComputational aerodynamic modeling of small quadcopter\nvehicles,\u201d 2017.\n[30] R. Gill and R. D\u2019Andrea, \u201cPropeller thrust and drag in forward flight,\u201d\nin 2017 IEEE Conference on Control Technology and Applications\n(CCTA). IEEE, 2017, pp. 73\u201379.\n[31] R. Gill and R. D\u2019Andrea, \u201cComputationally efficient force and moment\nmodels for propellers in uav forward flight applications,\u201d Drones, vol. 3,\nno. 4, p. 77, 2019.\n[32] W. Khan and M. Nahon, \u201cToward an accurate physics-based uav\nthruster model,\u201d IEEE/ASME Transactions on Mechatronics, vol. 18,\nno. 4, pp. 1269\u20131279, 2013.\n[33] G. Hoffmann, H. Huang, S. Waslander, and C. Tomlin, \u201cQuadrotor\nhelicopter flight dynamics and control: Theory and experiment,\u201d in\nAIAA guidance, navigation and control conference and exhibit, 2007,\np. 6461.\n[34] M. Bangura and R. Mahony, \u201cThrust control for multirotor aerial\nvehicles,\u201d IEEE Transactions on Robotics, vol. 33, no. 2, pp. 390\u2013405,\n2017.\n[35] L. Bauersfeld, E. Kaufmann, P. Foehn, S. Sun, and D. Scaramuzza,\n\u201cNeurobem: Hybrid aerodynamic quadrotor model,\u201d RSS: Robotics,\nScience, and Systems, 2021.\n[36] E. Kaufmann, L. Bauersfeld, and D. Scaramuzza, \u201cA benchmark\ncomparison of learned control policies for agile quadrotor flight,\u201d in\n2022 International Conference on Robotics and Automation (ICRA).\nIEEE, 2022.\n[37] R. Penicka and D. Scaramuzza, \u201cMinimum-time quadrotor waypoint\nflight in cluttered environments,\u201d IEEE Robotics and Automation\nLetters, 2022.\n[38] P. Ventura Diaz and S. Yoon, \u201cHigh-fidelity computational aerodynam\u0002ics of multi-rotor unmanned aerial vehicles,\u201d in 2018 AIAA Aerospace\nSciences Meeting, 2018, p. 1266.\n[39] J. Luo, L. Zhu, and G. Yan, \u201cNovel quadrotor forward-flight model\nbased on wake interference,\u201d Aiaa Journal, vol. 53, no. 12, pp. 3522\u2013\n3533, 2015.\n[40] G. Torrente, E. Kaufmann, P. Foehn, and D. Scaramuzza, \u201cData-driven\nmpc for quadrotors,\u201d IEEE Robotics and Automation Letters, 2021.\n[41] S. Sun, C. C. de Visser, and Q. Chu, \u201cQuadrotor gray-box model\nidentification from high-speed flight data,\u201d Journal of Aircraft, vol. 56,\nno. 2, pp. 645\u2013661, 2019.\n[42] S. Bansal, A. K. Akametalu, F. J. Jiang, F. Laine, and C. J. Tomlin,\n\u201cLearning quadrotor dynamics using neural network for flight control,\u201d\nin 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE,\n2016, pp. 4653\u20134660.\n[43] A. Punjani and P. Abbeel, \u201cDeep learning helicopter dynamics models,\u201d\nin 2015 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2015, pp. 3223\u20133230.\n[44] G. Shi, X. Shi, M. O\u2019Connell, R. Yu, K. Azizzadenesheli, A. Anand\u0002kumar, Y. Yue, and S.-J. Chung, \u201cNeural lander: Stable drone landing\ncontrol using learned dynamics,\u201d 05 2019, pp. 9784\u20139790.\n[45] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,\n\u201cWaveNet: A Generative Model for Raw Audio,\u201d in Proc. 9th ISCA\nWorkshop on Speech Synthesis Workshop (SSW 9), 2016, p. 125.\n[46] B. Wang, Z. Ma, S. Lai, and L. Zhao, \u201cNeural moving horizon\nestimation for robust flight control,\u201d IEEE Transactions on Robotics,\nvol. 40, pp. 639\u2013659, 2024.\n[47] W. Peukert, \u201cUber die abh \u00a8 angigkeit der kapazit \u00a8 at von der entlade- \u00a8\nstromstarke bei bleiakkumulatoren.\u201d \u00a8 Elektrotechn. Zeitschr., 1897.\n[48] N. Galushkin, N. Yazvinskaya, and D. Galushkin, \u201cGeneralized an\u0002alytical model for capacity evaluation of automotive-grade lithium\nbatteries,\u201d Journal of The Electrochemical Society, vol. 162, pp. A308\u2013\nA314, 01 2015.\n[49] N. Galushkin, N. N. Yazvinskaya, and D. N. Galushkin, \u201cA critical\nreview of using the peukert equation and its generalizations for lithium\u0002ion cells,\u201d Journal of The Electrochemical Society, vol. 167, no. 12, p.\n120516, aug 2020.\n[50] X. Zhang, W. Zhang, and G. Lei, \u201cA review of li-ion battery equivalent\ncircuit models,\u201d Transactions on Electrical and Electronic Materials,\nvol. 17, pp. 311\u2013316, 12 2016.\n[51] L. Zhang, S. Wang, D.-I. Stroe, C. Zou, C. Fernandez, and C. Yu, \u201cAn\naccurate time constant parameter determination method for the varying\ncondition equivalent circuit model of lithium batteries,\u201d Energies,\nvol. 13, no. 8, 2020.\n[52] D. Bicego, J. Mazzetto, R. Carli, M. Farina, A. Franchi, and\nV. Arellano-Quintana, \u201cNonlinear model predictive control with en\u0002hanced actuator model for multi-rotor aerial vehicles with generic\ndesigns,\u201d Journal of Intelligent and Robotic Systems, vol. 100, 12 2020.\n[53] L. O. Rojas-Perez and J. Martinez-Carranza, \u201cOn-board processing for\nautonomous drone racing: An overview,\u201d Integration, vol. 80, pp. 46\u2013\n59, 2021.\n[54] D. Scaramuzza and F. Fraundorfer, \u201cVisual odometry [tutorial],\u201d IEEE\nrobotics & automation magazine, vol. 18, no. 4, pp. 80\u201392, 2011.\n[55] W. Guerra, E. Tal, V. Murali, G. Ryou, and S. Karaman, \u201cFlightGog\u0002gles: Photorealistic sensor simulation for perception-driven robotics\nusing photogrammetry and virtual reality,\u201d in 2019 IEEE/RSJ Interna\u0002tional Conference on Intelligent Robots and Systems (IROS). IEEE,\nnov 2019.\n[56] J. Rehder, J. Nikolic, T. Schneider, T. Hinzmann, and R. Siegwart,\n\u201cExtending kalibr: Calibrating the extrinsics of multiple imus and of\nindividual axes,\u201d in 2016 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE, 2016, pp. 4304\u20134311.\n[57] Y. Yang, P. Geneva, X. Zuo, and G. Huang, \u201cOnline imu intrinsic\ncalibration: Is it necessary?\u201d Proc. of Robotics: Science and Systems\n(RSS), Corvallis, Or, 2020.\n[58] D. Scaramuzza and Z. Zhang, \u201cVisual-inertial odometry of aerial\nrobots,\u201d Encyclopedia of Robotics, 2019.\n[59] J. Engel, V. Koltun, and D. Cremers, \u201cDirect sparse odometry,\u201d IEEE\ntransactions on pattern analysis and machine intelligence, vol. 40,\nno. 3, pp. 611\u2013625, 2017.\n[60] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, \u201cRobust visual\ninertial odometry using a direct ekf-based approach,\u201d in 2015 IEEE/RSJ\ninternational conference on intelligent robots and systems (IROS).\nIEEE, pp. 298\u2013304.\n[61] A. I. Mourikis and S. I. Roumeliotis, \u201cA multi-state constraint kalman\nfilter for vision-aided inertial navigation,\u201d in IEEE Int. Conf. Robot.\nAutom. (ICRA). IEEE, 2007, pp. 3565\u20133572.\n[62] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale,\n\u201cKeyframe-based visual\u2013inertial odometry using nonlinear optimiza\u0002tion,\u201d The International Journal of Robotics Research, vol. 34, no. 3,\npp. 314\u2013334, 2015.\n[63] T. Qin, P. Li, and S. Shen, \u201cVins-mono: A robust and versatile monoc\u0002ular visual-inertial state estimator,\u201d IEEE Transactions on Robotics,\nvol. 34, no. 4, pp. 1004\u20131020, 2018.\n[64] E. Kaufmann, M. Gehrig, P. Foehn, R. Ranftl, A. Dosovitskiy,\nV. Koltun, and D. Scaramuzza, \u201cBeauty and the beast: Optimal methods\nmeet learning for drone racing,\u201d in 2019 International Conference on\nRobotics and Automation (ICRA). IEEE, 2019, pp. 690\u2013696.\n[65] A. Loquercio, E. Kaufmann, R. Ranftl, A. Dosovitskiy, V. Koltun, and\nD. Scaramuzza, \u201cDeep drone racing: From simulation to reality with\ndomain randomization,\u201d IEEE Transactions on Robotics, 2019.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\n[66] P. Geneva, K. Eckenhoff, W. Lee, Y. Yang, and G. Huang, \u201cOpenvins:\nA research platform for visual-inertial estimation,\u201d in IEEE Int. Conf.\nRobot. Autom. (ICRA). IEEE, 2020, pp. 4666\u20134672.\n[67] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, \u201cUlti\u0002mate slam? combining events, images, and imu for robust visual slam in\nhdr and high-speed scenarios,\u201d IEEE Robotics and Automation Letters,\nvol. 3, no. 2, pp. 994\u20131001, 2018.\n[68] S. Sun, G. Cioffi, C. De Visser, and D. Scaramuzza, \u201cAutonomous\nquadrotor flight despite rotor failure with onboard vision sensors:\nFrames vs. events,\u201d IEEE Robotics and Automation Letters, vol. 6,\nno. 2, pp. 580\u2013587, 2021.\n[69] P. Chen, W. Guan, and P. Lu, \u201cEsvio: Event-based stereo visual inertial\nodometry,\u201d IEEE Robot. Autom. Lett., 2023.\n[70] B. Nisar, P. Foehn, D. Falanga, and D. Scaramuzza, \u201cVimo: Simultane\u0002ous visual inertial model-based odometry and force estimation,\u201d IEEE\nRobotics and Automation Letters, vol. 4, no. 3, pp. 2785\u20132792, 2019.\n[71] G. Cioffi, L. Bauersfeld, and D. Scaramuzza, \u201cHdvio: Improving\nlocalization and disturbance estimation with hybrid dynamics vio,\u201d\nRobotics: Science and Systems (RSS), 2023.\n[72] G. Cioffi, L. Bauersfeld, E. Kaufmann, and D. Scaramuzza, \u201cLearned\ninertial odometry for autonomous drone racing,\u201d IEEE Robotics and\nAutomation Letters, vol. 8, no. 5, pp. 2684\u20132691, 2023.\n[73] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, \u00a8\nS. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis et al., \u201cEvent\u0002based vision: A survey,\u201d IEEE transactions on pattern analysis and\nmachine intelligence, vol. 44, no. 1, pp. 154\u2013180, 2020.\n[74] Z. Ding, T. Yang, K. Zhang, C. Xu, and F. Gao, \u201cVid-fusion:\nRobust visual-inertial-dynamics odometry for accurate external force\nestimation,\u201d in 2021 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2021, pp. 14 469\u201314 475.\n[75] J. Delmerico and D. Scaramuzza, \u201cA benchmark comparison of monoc\u0002ular visual-inertial odometry algorithms for flying robots,\u201d in 2018\nIEEE international conference on robotics and automation (ICRA).\nIEEE, 2018, pp. 2502\u20132509.\n[76] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.\nAchtelik, and R. Siegwart, \u201cThe euroc micro aerial vehicle datasets,\u201d\nThe International Journal of Robotics Research, vol. 35, no. 10, pp.\n1157\u20131163, 2016.\n[77] P. Foehn, D. Brescianini, E. Kaufmann, T. Cieslewski, M. Gehrig,\nM. Muglikar, and D. Scaramuzza, \u201cAlphapilot: Autonomous drone\nracing,\u201d Robotics: Science and Systems (RSS), 2020.\n[78] J. Delmerico, T. Cieslewski, H. Rebecq, M. Faessler, and D. Scara\u0002muzza, \u201cAre we ready for autonomous drone racing? the uzh-fpv drone\nracing dataset,\u201d in 2019 International Conference on Robotics and\nAutomation (ICRA). IEEE, 2019, pp. 6713\u20136719.\n[79] E. Kaufmann, A. Loquercio, R. Ranftl, A. Dosovitskiy, V. Koltun,\nand D. Scaramuzza, \u201cDeep drone racing: Learning agile flight in\ndynamic environments,\u201d in Proceedings of The 2nd Conference on\nRobot Learning, ser. Proceedings of Machine Learning Research,\nA. Billard, A. Dragan, J. Peters, and J. Morimoto, Eds., vol. 87.\nPMLR, 29\u201331 Oct 2018, pp. 133\u2013145.\n[80] S. Li, M. M. Ozo, C. De Wagter, and G. C. de Croon, \u201cAutonomous\ndrone race: A computationally efficient vision-based navigation and\ncontrol strategy,\u201d Robotics and Autonomous Systems, vol. 133, p.\n103621, 2020.\n[81] S. Li, E. van der Horst, P. Duernay, C. De Wagter, and G. C.\nde Croon, \u201cVisual model-predictive localization for computationally\nefficient autonomous racing of a 72-g drone,\u201d Journal of Field Robotics,\nvol. 37, no. 4, pp. 667\u2013692, 2020.\n[82] D. Zhang and D. D. Doyle, \u201cGate detection using deep learning,\u201d in\n2020 IEEE Aerospace Conference, 2020, pp. 1\u201311.\n[83] R. Szeliski, Computer vision: algorithms and applications. Springer\nNature, 2022.\n[84] S. Wang, R. Clark, H. Wen, and N. Trigoni, \u201cDeepvo: Towards\nend-to-end visual odometry with deep recurrent convolutional neural\nnetworks,\u201d in 2017 IEEE international conference on robotics and\nautomation (ICRA). IEEE, 2017, pp. 2043\u20132050.\n[85] W. Wang, Y. Hu, and S. Scherer, \u201cTartanvo: A generalizable learning\u0002based vo,\u201d in Conference on Robotic Learning (CoRL), 2020.\n[86] Z. Teed, L. Lipson, and J. Deng, \u201cDeep patch visual odometry,\u201d\nAdvances in Neural Information Processing Systems, vol. 36, 2024.\n[87] Z. Han, Z. Wang, N. Pan, Y. Lin, C. Xu, and F. Gao, \u201cFast-racing:\nAn open-source strong baseline for {SE}(3) planning in autonomous\ndrone racing,\u201d IEEE Robotics and Automation Letters, vol. 6, pp. 8631\u2013\n8638, 2021.\n[88] S. Spedicato and G. Notarstefano, \u201cMinimum-time trajectory genera\u0002tion for quadrotors in constrained environments,\u201d IEEE Transactions\non Control Systems Technology, vol. 26, no. 4, pp. 1335\u20131344, 2017.\n[89] C. Richter, A. Bry, and N. Roy, \u201cPolynomial trajectory planning for\naggressive quadrotor flight in dense indoor environments,\u201d in Robotics\nResearch. Springer, 2016, pp. 649\u2013666.\n[90] B. Zhou, F. Gao, J. Pan, and S. Shen, \u201cRobust real-time uav replanning\nusing guided gradient-based optimization and topological paths,\u201d in\n2020 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2020, pp. 1208\u20131214.\n[91] B. Zhou, J. Pan, F. Gao, and S. Shen, \u201cRaptor: Robust and perception\u0002aware trajectory replanning for quadrotor fast flight,\u201d IEEE Transac\u0002tions on Robotics, vol. 37, no. 6, pp. 1992\u20132009, 2021.\n[92] H. Pham and Q.-C. Pham, \u201cA new approach to time-optimal path\nparameterization based on reachability analysis,\u201d IEEE Transactions\non Robotics, vol. 34, no. 3, pp. 645\u2013659, 2018.\n[93] I. Spasojevic, V. Murali, and S. Karaman, \u201cPerception-aware time opti\u0002mal path parameterization for quadrotors,\u201d in 2020 IEEE International\nConference on Robotics and Automation (ICRA), 2020, pp. 3213\u20133219.\n[94] B. Penin, P. R. Giordano, and F. Chaumette, \u201cVision-based reactive\nplanning for aggressive target tracking while avoiding collisions and\nocclusions,\u201d IEEE Robotics and Automation Letters, vol. 3, no. 4, pp.\n3725\u20133732, 2018.\n[95] P. Foehn, A. Romero, and D. Scaramuzza, \u201cTime-optimal planning\nfor quadrotor waypoint flight,\u201d Science Robotics, vol. 6, no. 56, p.\neabh1221, 2021.\n[96] P. Foehn, D. Falanga, N. Kuppuswamy, R. Tedrake, and D. Scaramuzza,\n\u201cFast trajectory optimization for agile quadrotor maneuvers with a\ncable-suspended payload,\u201d in Robotics: Science and Systems (RSS),\n2017.\n[97] M. Hehn, R. Ritz, and R. D\u2019Andrea, \u201cPerformance benchmarking of\nquadrotor systems using time-optimal control,\u201d Autonomous Robots,\nMar. 2012.\n[98] K. Bousson and P. F. Machado, \u201c4d trajectory generation and tracking\nfor waypoint-based aerial navigation,\u201d WSEAS Transactions on Systems\nand Control, no. 3, pp. 105\u2013119, 2013.\n[99] S. Liu, N. Atanasov, K. Mohta, and V. Kumar, \u201cSearch-based motion\nplanning for quadrotors using linear quadratic minimum time control,\u201d\nin 2017 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), Sep 2017, p. 2872\u20132879.\n[100] S. Liu, K. Mohta, N. Atanasov, and V. Kumar, \u201cSearch-based motion\nplanning for aggressive flight in se(3),\u201d IEEE Robotics and Automation\nLetters, vol. 3, no. 3, p. 2439\u20132446, Jul 2018.\n[101] R. Allen and M. Pavone, \u201cA real-time framework for kinodynamic\nplanning with application to quadrotor obstacle avoidance,\u201d in AIAA\nGuidance, Navigation, and Control Conference, 2016, p. 1374.\n[102] T. Zhiling, B. Chen, R. Lan, and S. Li, \u201cVector field guided rrt* based\non motion primitives for quadrotor kinodynamic planning,\u201d Journal of\nIntelligent & Robotic Systems, vol. 100, 12 2020.\n[103] A. Romero, S. Sun, P. Foehn, and D. Scaramuzza, \u201cModel predictive\ncontouring control for time-optimal quadrotor flight,\u201d IEEE Transac\u0002tions on Robotics, pp. 1\u201317, 2022.\n[104] A. Romero, R. Penicka, and D. Scaramuzza, \u201cTime-optimal online\nreplanning for agile quadrotor flight,\u201d IEEE Robotics and Automation\nLetters, vol. 7, no. 3, pp. 7730\u20137737, 2022.\n[105] S. M. LaValle, Planning algorithms. Cambridge university press,\n2006.\n[106] L. Kavraki, P. Svestka, J.-C. Latombe, and M. Overmars, \u201cProbabilistic\nroadmaps for path planning in high-dimensional configuration spaces,\u201d\nIEEE Transactions on Robotics and Automation, vol. 12, no. 4, pp.\n566\u2013580, 1996.\n[107] S. Lavalle and J. Kuffner, \u201cRapidly-exploring random trees: Progress\nand prospects,\u201d Algorithmic and computational robotics: New direc\u0002tions, 01 2000.\n[108] S. Karaman and E. Frazzoli, \u201cSampling-based algorithms for optimal\nmotion planning,\u201d The International Journal of Robotics Research,\nvol. 30, no. 7, pp. 846\u2013894, 2011.\n[109] P. E. Hart, N. J. Nilsson, and B. Raphael, \u201cA formal basis for the\nheuristic determination of minimum cost paths,\u201d IEEE Transactions on\nSystems Science and Cybernetics, vol. 4, no. 2, pp. 100\u2013107, 1968.\n[110] E. W. Dijkstra, \u201cA note on two problems in connexion with graphs,\u201d\n1959, pp. 269\u2013271.\n[111] D. Mellinger and V. Kumar, \u201cMinimum snap trajectory generation and\ncontrol for quadrotors,\u201d in IEEE Int. Conf. Robot. Autom. (ICRA), 2011.\n[112] D. Mellinger, N. Michael, and V. Kumar, \u201cTrajectory generation and\ncontrol for precise aggressive maneuvers with quadrotors,\u201d Int. J.\nRobot. Research, 2012.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[113] M. W. Mueller, M. Hehn, and R. D\u2019Andrea, \u201cA computationally \u00a8\nefficient motion primitive for quadrocopter trajectory generation,\u201d IEEE\nTransactions on Robotics, vol. 31, no. 6, pp. 1294\u20131310, 2015.\n[114] C. Qin, M. S. Michet, J. Chen, and H. H.-T. Liu, \u201cTime-optimal\ngate-traversing planner for autonomous drone racing,\u201d in 2024 IEEE\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2024.\n[115] Z. Wang, X. Zhou, C. Xu, and F. Gao, \u201cGeometrically constrained tra\u0002jectory optimization for multicopters,\u201d IEEE Transactions on Robotics,\nvol. 38, no. 5, pp. 3259\u20133278, 2022.\n[116] T. Fork and F. Borrelli, \u201cEuclidean and non-euclidean trajec\u0002tory optimization approaches for quadrotor racing,\u201d arXiv preprint\narXiv:2309.07262, 2023.\n[117] W. V. Loock, G. Pipeleers, and J. Swevers, \u201cTime-optimal quadrotor\nflight,\u201d in IEEE Eur. Control Conf. (ECC), 2013.\n[118] T. R. Jorris and R. G. Cobb, \u201cThree-dimensional trajectory optimization\nsatisfying waypoint and no-fly zone constraints,\u201d Journal of Guidance,\nControl, and Dynamics, vol. 32, no. 2, pp. 551\u2013572, 2009.\n[119] D. J. Webb and J. van den Berg, \u201cKinodynamic rrt*: Asymptotically\noptimal motion planning for robots with linear dynamics,\u201d in 2020\nIEEE International Conference on Robotics and Automation (ICRA),\n2013, pp. 5054\u20135061.\n[120] B. Ichter, B. Landry, E. Schmerling, and M. Pavone, \u201cPerception\u0002aware motion planning via multiobjective search on gpus,\u201d in Robotics\nResearch. Cham: Springer International Publishing, 2020, pp. 895\u2013\n912.\n[121] E. Kaufmann, A. Loquercio, R. Ranftl, M. Muller, V. Koltun, and \u00a8\nD. Scaramuzza, \u201cDeep drone acrobatics,\u201d in Proceedings of Robotics:\nScience and Systems, Corvalis, Oregon, USA, July 2020.\n[122] Y. Song, M. Steinweg, E. Kaufmann, and D. Scaramuzza, \u201cAutonomous\ndrone racing with deep reinforcement learning,\u201d in 2021 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2021, pp. 1205\u20131212.\n[123] R. Penicka, Y. Song, E. Kaufmann, and D. Scaramuzza, \u201cLearning\nminimum-time flight in cluttered environments,\u201d IEEE Robotics and\nAutomation Letters, vol. 7, no. 3, pp. 7209\u20137216, 2022.\n[124] D. Hanover, P. Foehn, S. Sun, E. Kaufmann, and D. Scaramuzza,\n\u201cPerformance, precision, and payloads: Adaptive nonlinear mpc for\nquadrotors,\u201d IEEE Robotics and Automation Letters, vol. 7, no. 2, pp.\n690\u2013697, 2021.\n[125] S. Sun, A. Romero, P. Foehn, E. Kaufmann, and D. Scaramuzza,\n\u201cA comparative study of nonlinear mpc and differential-flatness-based\ncontrol for quadrotor agile flight,\u201d IEEE Trans. Robot., 2022.\n[126] T. Lee, M. Leok, and N. H. McClamroch, \u201cGeometric tracking control\nof a quadrotor uav for extreme maneuverability,\u201d IFAC Proceedings\nVolumes, 2011.\n[127] E. Tal and S. Karaman, \u201cAccurate tracking of aggressive quadrotor tra\u0002jectories using incremental nonlinear dynamic inversion and differential\nflatness,\u201d IEEE Transactions on Control Systems Technology, vol. 29,\nno. 3, pp. 1203\u20131218, 2020.\n[128] H. Nguyen, M. Kamel, K. Alexis, and R. Siegwart, \u201cModel predictive\ncontrol for micro aerial vehicles: A survey,\u201d in 2021 European Control\nConference (ECC). IEEE, 2021, pp. 1556\u20131563.\n[129] M. Kamel, M. Burri, and R. Siegwart, \u201cLinear vs nonlinear mpc for\ntrajectory tracking applied to rotary wing micro aerial vehicles,\u201d IFAC\u0002PapersOnLine, vol. 50, no. 1, pp. 3463\u20133469, 2017.\n[130] G. Williams, A. Aldrich, and E. A. Theodorou, \u201cModel predictive\npath integral control: From theory to parallel computation,\u201d Journal of\nGuidance, Control, and Dynamics, vol. 40, no. 2, pp. 344\u2013357, 2017.\n[131] B. Goldfain, P. Drews, C. You, M. Barulic, O. Velev, P. Tsiotras, and\nJ. M. Rehg, \u201cAutorally: An open platform for aggressive autonomous\ndriving,\u201d IEEE Control Systems Magazine, vol. 39, no. 1, pp. 26\u201355,\n2019.\n[132] D. Lam, C. Manzie, and M. Good, \u201cModel predictive contouring\ncontrol,\u201d in 49th IEEE Conference on Decision and Control (CDC),\n2010, pp. 6137\u20136142.\n[133] A. Liniger, A. Domahidi, and M. Morari, \u201cOptimization-based au\u0002tonomous racing of 1:43 scale RC cars,\u201d Optimal Control Applications\nand Methods, vol. 36, no. 5, pp. 628\u2013647, jul 2014.\n[134] J. Arrizabalaga and M. Ryll, \u201cTowards time-optimal tunnel-following\nfor quadrotors,\u201d in 2022 International Conference on Robotics and\nAutomation (ICRA). IEEE, 2022, pp. 4044\u20134050.\n[135] G. Costante, C. Forster, J. Delmerico, P. Valigi, and D. Scaramuzza,\n\u201cPerception-aware path planning,\u201d arXiv preprint arXiv:1605.04151,\n2016.\n[136] D. Falanga, E. Mueggler, M. Faessler, and D. Scaramuzza, \u201cAggressive\nquadrotor flight through narrow gaps with onboard sensing and com\u0002puting using active vision,\u201d in 2017 IEEE international conference on\nrobotics and automation (ICRA). IEEE, 2017, pp. 5774\u20135781.\n[137] B. Penin, R. Spica, P. R. Giordano, and F. Chaumette, \u201cVision-based\nminimum-time trajectory generation for a quadrotor uav,\u201d in 2017\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2017, pp. 6199\u20136206.\n[138] J. Tordesillas and J. P. How, \u201cDeep-panther: Learning-based perception\u0002aware trajectory planner in dynamic environments,\u201d IEEE Robotics and\nAutomation Letters, vol. 8, no. 3, pp. 1399\u20131406, 2023.\n[139] \u2014\u2014, \u201cPanther: Perception-aware trajectory planner in dynamic envi\u0002ronments,\u201d IEEE Access, vol. 10, pp. 22 662\u201322 677, 2022.\n[140] D. Falanga, P. Foehn, P. Lu, and D. Scaramuzza, \u201cPAMPC: Perception\u0002aware model predictive control for quadrotors,\u201d in 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2018, pp. 1\u20138.\n[141] K. Lee, J. Gibson, and E. A. Theodorou, \u201cAggressive perception-aware\nnavigation using deep optical flow dynamics and pixelmpc,\u201d IEEE\nRobotics and Automation Letters, vol. 5, no. 2, pp. 1207\u20131214, 2020.\n[142] M. Greeff, T. D. Barfoot, and A. P. Schoellig, \u201cA perception-aware\nflatness-based model predictive controller for fast vision-based multi\u0002rotor flight,\u201d IFAC-PapersOnLine, vol. 53, no. 2, pp. 9412\u20139419, 2020.\n[143] R. Verschueren, G. Frison, D. Kouzoupis, J. Frey, N. v. Duijkeren,\nA. Zanelli, B. Novoselnik, T. Albin, R. Quirynen, and M. Diehl,\n\u201cacados\u2014a modular open-source framework for fast embedded optimal\ncontrol,\u201d Mathematical Programming Computation, vol. 14, no. 1, pp.\n147\u2013183, 2022.\n[144] J. Mattingley and S. Boyd, \u201cCvxgen: A code generator for embedded\nconvex optimization,\u201d Optimization and Engineering, vol. 13, no. 1,\npp. 1\u201327, 2012.\n[145] G. Frison and M. Diehl, \u201cHpipm: a high-performance quadratic\nprogramming framework for model predictive control,\u201d IFAC\u0002PapersOnLine, vol. 53, no. 2, pp. 6563\u20136569, 2020.\n[146] M. ApS, \u201cMosek optimization toolbox for matlab,\u201d User\u2019s Guide and\nReference Manual, Version, vol. 4, 2019.\n[147] A. Giusti, J. Guzzi, D. C. Cires\u00b8an, F.-L. He, J. P. Rodr\u00b4\u0131guez, F. Fontana,\nM. Faessler, C. Forster, J. Schmidhuber, G. Di Caro et al., \u201cA machine\nlearning approach to visual perception of forest trails for mobile\nrobots,\u201d IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661\u2013\n667, 2015.\n[148] A. Loquercio, A. I. Maqueda, C. R. Del-Blanco, and D. Scaramuzza,\n\u201cDronet: Learning to fly by driving,\u201d IEEE Robotics and Automation\nLetters, vol. 3, no. 2, pp. 1088\u20131095, 2018.\n[149] D. Gandhi, L. Pinto, and A. Gupta, \u201cLearning to fly by crashing,\u201d in\nInternational Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2017, pp. 3948\u20133955.\n[150] A. Loquercio, E. Kaufmann, R. Ranftl, M. Muller, V. Koltun, and \u00a8\nD. Scaramuzza, \u201cLearning high-speed flight in the wild,\u201d Science\nRobotics, vol. 6, no. 59, 2021.\n[151] F. Sadeghi and S. Levine, \u201cCad 2 rl: Real single-image flight without\na single real image,\u201d in Robotics: Science and Systems (RSS), 2017,\npp. 48\u201355.\n[152] T. Lee, S. Mckeever, and J. Courtney, \u201cFlying free: A research overview\nof deep learning in drone navigation autonomy,\u201d in drones, 2021.\n[153] H. X. Pham, H. I. Ugurlu, J. Le Fevre, D. Bardakci, and E. Kayacan,\n\u201cDeep learning for vision-based navigation in autonomous drone rac\u0002ing,\u201d in Deep Learning for Robot Perception and Cognition. Elsevier,\n2022, pp. 371\u2013406.\n[154] A. A. Cabrera-Ponce, L. O. Rojas-Perez, J. A. Carrasco-Ochoa, J. F.\nMartinez-Trinidad, and J. Martinez-Carranza, \u201cGate detection for micro\naerial vehicles using a single shot detector,\u201d IEEE Latin America\nTransactions, vol. 17, no. 12, pp. 2045\u20132052, 2019.\n[155] H. X. Pham, I. Bozcan, A. Sarabakha, S. Haddadin, and E. Kayacan,\n\u201cGatenet: An efficient deep neural network architecture for gate per\u0002ception using fish-eye camera in autonomous drone racing,\u201d in 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2021, pp. 4176\u20134183.\n[156] H. X. Pham, A. Sarabakha, M. Odnoshyvkin, and E. Kayacan, \u201cPencil\u0002net: Zero-shot sim-to-real transfer learning for robust gate perception\nin autonomous drone racing,\u201d IEEE Robotics and Automation Letters,\nvol. 7, no. 4, pp. 11 847\u201311 854, 2022.\n[157] T. Morales, A. Sarabakha, and E. Kayacan, \u201cImage generation for\nefficient neural network training in autonomous drone racing,\u201d in 2020\nInternational Joint Conference on Neural Networks (IJCNN). IEEE,\n2020, pp. 1\u20138.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[158] K. F. Andersen, H. X. Pham, H. I. Ugurlu, and E. Kayacan, \u201cEvent\u0002based navigation for autonomous drone racing with sparse gated\nrecurrent network,\u201d in 2022 European Control Conference (ECC).\nIEEE, 2022, pp. 1342\u20131348.\n[159] N. J. Sanket, C. D. Singh, C. Fermuller, and Y. Aloimonos, \u201cPrgflow: \u00a8\nUnified swap-aware deep global optical flow for aerial robot naviga\u0002tion,\u201d Electronics Letters, vol. 57, no. 16, pp. 614\u2013617, 2021.\n[160] Y. Xu and G. C. de Croon, \u201cCnn-based ego-motion estimation for fast\nmav maneuvers,\u201d in 2021 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE, 2021, pp. 7606\u20137612.\n[161] \u2014\u2014, \u201cCuahn-vio: Content-and-uncertainty-aware homography net\u0002work for visual-inertial odometry,\u201d arXiv preprint arXiv:2208.13935,\n2022.\n[162] L. Lamberti, E. Cereda, G. Abbate, L. Bellone, V. J. K. Morinigo,\nM. Barcis, A. Barci \u00b4 s, A. Giusti, F. Conti, and D. Palossi, \u201cA sim-to-real \u00b4\ndeep learning-based framework for autonomous nano-drone racing,\u201d\nIEEE Robotics and Automation Letters, 2024.\n[163] H. Yu, C. De Wagter, and G. C. de Croon, \u201cMavrl: Learn to\nfly in cluttered environments with varying speed,\u201d arXiv preprint\narXiv:2402.08381, 2024.\n[164] M. Kulkarni and K. Alexis, \u201cReinforcement learning for collision-free\nflight exploiting deep collision encoding,\u201d in 2024 IEEE International\nConference on Robotics and Automation (ICRA). IEEE, 2024.\n[165] K. Amer, M. Samy, M. Shaker, and M. ElHelw, \u201cDeep convolutional\nneural network based autonomous drone navigation,\u201d in Thirteenth\nInternational Conference on Machine Vision, vol. 11605. SPIE, 2021,\npp. 16\u201324.\n[166] R. Madaan, N. Gyde, S. Vemprala, M. Brown, K. Nagami, T. Taubner,\nE. Cristofalo, D. Scaramuzza, M. Schwager, and A. Kapoor, \u201cAirsim\ndrone racing lab,\u201d in Proceedings of the NeurIPS 2019 Competition and\nDemonstration Track, ser. Proceedings of Machine Learning Research,\nH. J. Escalante and R. Hadsell, Eds., vol. 123. PMLR, 08\u201314 Dec\n2020, pp. 177\u2013191.\n[167] U. Ates, \u201cLong-term planning with deep reinforcement learning on\nautonomous drones,\u201d in 2020 Innovations in Intelligent Systems and\nApplications Conference (ASYU). IEEE, 2020, pp. 1\u20136.\n[168] W. Koch, R. Mancuso, R. West, and A. Bestavros, \u201cReinforcement\nlearning for uav attitude control,\u201d ACM Transactions on Cyber-Physical\nSystems, vol. 3, no. 2, pp. 1\u201321, 2019.\n[169] N. O. Lambert, D. S. Drew, J. Yaconelli, S. Levine, R. Calandra, and\nK. S. Pister, \u201cLow-level control of a quadrotor with deep model-based\nreinforcement learning,\u201d IEEE Robotics and Automation Letters, vol. 4,\nno. 4, pp. 4224\u20134230, 2019.\n[170] S. Li, E. Ozt \u00a8 urk, C. De Wagter, G. C. De Croon, and D. Izzo, \u201cAg- \u00a8\ngressive online control of a quadrotor via deep network representations\nof optimality principles,\u201d in 2020 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, 2020, pp. 6282\u20136287.\n[171] C. Sanchez-S \u00b4 anchez and D. Izzo, \u201cReal-time optimal control via deep \u00b4\nneural networks: study on landing problems,\u201d Journal of Guidance,\nControl, and Dynamics, vol. 41, no. 5, pp. 1122\u20131135, 2018.\n[172] R. Ferede, G. de Croon, C. De Wagter, and D. Izzo, \u201cEnd-to-end neural\nnetwork based optimal quadcopter control,\u201d Robotics and Autonomous\nSystems, vol. 172, p. 104588, 2024.\n[173] J. Sacks, R. Rana, K. Huang, A. Spitzer, G. Shi, and B. Boots, \u201cDeep\nmodel predictive optimization,\u201d in 2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024.\n[174] H. Dai, B. Landry, L. Yang, M. Pavone, and R. Tedrake, \u201cLyapunov\u0002stable neural-network control,\u201d in Proceedings of Robotics: Science and\nSystems, Virtual, July 2021.\n[175] M. Selim, A. Alanwar, S. Kousik, G. Gao, M. Pavone, and K. H.\nJohansson, \u201cSafe reinforcement learning using black-box reachability\nanalysis,\u201d IEEE Robotics and Automation Letters, vol. 7, no. 4, pp.\n10 665\u201310 672, 2022.\n[176] B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter, \u201cDifferen\u0002tiable mpc for end-to-end planning and control,\u201d Advances in neural\ninformation processing systems, vol. 31, 2018.\n[177] L. Pineda, T. Fan, M. Monge, S. Venkataraman, P. Sodhi, R. T. Chen,\nJ. Ortiz, D. DeTone, A. Wang, S. Anderson, J. Dong, B. Amos,\nand M. Mukadam, \u201cTheseus: A Library for Differentiable Nonlinear\nOptimization,\u201d Advances in Neural Information Processing Systems,\n2022.\n[178] C. Wang, D. Gao, K. Xu, J. Geng, Y. Hu, Y. Qiu, B. Li, F. Yang,\nB. Moon, A. Pandey, Aryan, J. Xu, T. Wu, H. He, D. Huang, Z. Ren,\nS. Zhao, T. Fu, P. Reddy, X. Lin, W. Wang, J. Shi, R. Talak, K. Cao,\nY. Du, H. Wang, H. Yu, S. Wang, S. Chen, A. Kashyap, R. Bandaru,\nK. Dantu, J. Wu, L. Xie, L. Carlone, M. Hutter, and S. Scherer,\n\u201cPyPose: A library for robot learning with physics-based optimization,\u201d\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023.\n[179] S. Cheng, L. Song, M. Kim, S. Wang, and N. Hovakimyan, \u201cDifftune+:\nHyperparameter-free auto-tuning using auto-differentiation,\u201d in Pro\u0002ceedings of The 5th Annual Learning for Dynamics and Control\nConference, ser. Proceedings of Machine Learning Research, N. Matni,\nM. Morari, and G. J. Pappas, Eds., vol. 211. PMLR, 15\u201316 Jun 2023,\npp. 170\u2013183.\n[180] P. Karkus, B. Ivanovic, S. Mannor, and M. Pavone, \u201cDiffstack: A\ndifferentiable and modular control stack for autonomous vehicles,\u201d in\nConference on Robot Learning. PMLR, 2023, pp. 2170\u20132180.\n[181] A. Romero, Y. Song, and D. Scaramuzza, \u201cActor-critic model predictive\ncontrol,\u201d in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024.\n[182] G. Li, M. Mueller, V. M. Casser, N. Smith, D. Michels, and B. Ghanem,\n\u201cOil: Observational imitation learning,\u201d in Proceedings of Robotics:\nScience and Systems, FreiburgimBreisgau, Germany, June 2019.\n[183] M. Muller, G. Li, V. Casser, N. Smith, D. L. Michels, and B. Ghanem,\n\u201cLearning a controller fusion network by online trajectory filtering for\nvision-based uav racing,\u201d in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition Workshops, 2019, pp.\n0\u20130.\n[184] M. Muller, V. Casser, J. Lahoud, N. Smith, and B. Ghanem, \u201cSim4cv: \u00a8\nA photo-realistic simulator for computer vision applications,\u201d Interna\u0002tional Journal of Computer Vision, vol. 126, no. 9, pp. 902\u2013919, 2018.\n[185] M. Muller, V. Casser, N. Smith, D. L. Michels, and B. Ghanem,\n\u201cTeaching uavs to race: End-to-end regression of agile controls in\nsimulation,\u201d in Proceedings of the European Conference on Computer\nVision (ECCV) Workshops, 2018, pp. 0\u20130.\n[186] L. O. Rojas-Perez and J. Martinez-Carranza, \u201cDeeppilot: A cnn for\nautonomous drone racing,\u201d Sensors, vol. 20, no. 16, p. 4524, 2020.\n[187] J. Fu, Y. Song, Y. Wu, F. Yu, and D. Scaramuzza, \u201cLearning deep\nsensorimotor policies for vision-based autonomous drone racing,\u201d in\n2023 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS). IEEE, 2023, pp. 5243\u20135250.\n[188] J. Xing, L. Bauersfeld, Y. Song, C. Xing, and D. Scaramuzza, \u201cCon\u0002trastive learning for enhancing robust scene transfer in vision-based\nagile flight,\u201d in 2024 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2024.\n[189] P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, \u201cDay\u0002dreamer: World models for physical robot learning,\u201d in Conference on\nRobot Learning. PMLR, 2023, pp. 2226\u20132240.\n[190] L. Smith, I. Kostrikov, and S. Levine, \u201cDemonstrating a walk in the\npark: Learning to walk in 20 minutes with model-free reinforcement\nlearning,\u201d Robotics: Science and Systems XIX, 2023.\n[191] L. Smith, J. C. Kew, X. B. Peng, S. Ha, J. Tan, and S. Levine, \u201cLegged\nrobots that keep on learning: Fine-tuning locomotion policies in the real\nworld,\u201d in 2022 International Conference on Robotics and Automation\n(ICRA). IEEE, 2022, pp. 1593\u20131599.\n[192] A. Loquercio, A. Kumar, and J. Malik, \u201cLearning visual locomotion\nwith cross-modal supervision,\u201d in IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, 2023, pp. 7295\u20137302.\n[193] P. A. Ioannou and J. Sun, Robust adaptive control. Courier Corpora\u0002tion, 2012.\n[194] K. J. Astr \u02da om and B. Wittenmark, \u00a8 Adaptive control. Courier Corpo\u0002ration, 2013.\n[195] E. Lavretsky and K. A. Wise, \u201cRobust adaptive control,\u201d in Robust and\nadaptive control. Springer, 2013, pp. 1 \u2013 449.\n[196] S. M. Richards, N. Azizan, J.-J. Slotine, and M. Pavone, \u201cAdaptive\u0002Control-Oriented Meta-Learning for Nonlinear Systems,\u201d in Proceed\u0002ings of Robotics: Science and Systems, Virtual, July 2021.\n[197] D. Zhang, A. Loquercio, X. Wu, A. Kumar, J. Malik, and M. W.\nMueller, \u201cLearning a single near-hover position controller for vastly\ndifferent quadcopters,\u201d in 2023 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, may 2023.\n[198] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoel\u0002lig, \u201cLearning to fly\u2014a gym environment with pybullet physics for\nreinforcement learning of multi-agent quadcopter control,\u201d in 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), 2021, pp. 7512\u20137519.\n[199] M. Kulkarni, T. J. Forgaard, and K. Alexis, \u201cAerial gym\u2013isaac gym\nsimulator for aerial robots,\u201d arXiv preprint arXiv:2305.16510, 2023.\n[200] L. Martin, \u201cAlphapilot ai drone innovation challenge,\u201d Jan 2020.\n[Online]. Available: https://lockheedmartin.com/en-us/news/events/ai\u0002innovation-challenge.html\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[201] C. de Wagter, F. Paredes-Valles, N. Sheth, and G. C. de Croon, \u00b4\n\u201cLearning fast in autonomous drone racing,\u201d Nat. Mach. Intell., vol. 3,\np. 923, 2021.\n[202] E. Ackerman, \u201cAutonomous drones challenge human champions in first\n\u201dfair\u201d race,\u201d Jul 2022.\n[203] Y. Song and D. Scaramuzza, \u201cPolicy search for model predictive control\nwith application to agile drone flight,\u201d IEEE Transactions on Robotics,\n2022.\n[204] A. Loquercio, A. Saviolo, and D. Scaramuzza, \u201cAutotune: Controller\ntuning for high-speed flight,\u201d IEEE Robotics and Automation Letters,\nvol. 7, no. 2, pp. 4432\u20134439, 2022.\n[205] A. Antonini, W. Guerra, V. Murali, T. Sayre-McCord, and S. Karaman,\n\u201cThe blackbird dataset: A large-scale dataset for uav perception in ag\u0002gressive flight,\u201d in International Symposium on Experimental Robotics.\nSpringer, 2018, pp. 130\u2013139.\n[206] C. Pfeiffer and D. Scaramuzza, \u201cHuman-piloted drone racing: Visual\nprocessing and control,\u201d IEEE Robotics and Automation Letters, vol. 6,\nno. 2, pp. 3467\u20133474, 2021.\n[207] M. Bosello, D. Aguiari, Y. Keuter, E. Pallotta, S. Kiade, G. Caminati,\nF. Pinzarrone, J. Halepota, J. Panerati, and G. Pau, \u201cRace against\nthe machine: a fully-annotated, open-design dataset of autonomous\nand piloted high-speed flight,\u201d IEEE Robotics and Automation Letters,\n2024.\n[208] X. Zhou, Z. Wang, H. Ye, C. Xu, and F. Gao, \u201cEgo-planner: An esdf\u0002free gradient-based local planner for quadrotors,\u201d IEEE Robotics and\nAutomation Letters, vol. 6, no. 2, pp. 478\u2013485, 2021.\n[209] J. Tordesillas and J. P. How, \u201cFASTER: Fast and safe trajectory\nplanner for navigation in unknown environments,\u201d IEEE Transactions\non Robotics, 2021.\n[210] V. Kumar and N. Michael, \u201cOpportunities and challenges with au\u0002tonomous micro aerial vehicles,\u201d The International Journal of Robotics\nResearch, vol. 31, no. 11, pp. 1279\u20131291, 2012.\n[211] T. Baca, M. Petrlik, M. Vrba, V. Spurny, R. Penicka, D. Hert,\nand M. Saska, \u201cThe MRS UAV system: Pushing the frontiers of\nreproducible research, real-world deployment, and education with\nautonomous unmanned aerial vehicles,\u201d J. Intell. Rob. Syst., vol. 102,\nno. 1, p. 26, Apr. 2021.\n[212] L. Meier, P. Tanskanen, L. Heng, G. H. Lee, F. Fraundorfer, and\nM. Pollefeys, \u201cPIXHAWK: A micro aerial vehicle design for au\u0002tonomous flight using onboard computer vision,\u201d vol. 33, no. 1, pp.\n21\u201339, Aug. 2012.\n[213] L. Bauersfeld, L. Spannagl, G. Ducard, and C. Onder, \u201cMpc flight\ncontrol for a tilt-rotor vtol aircraft,\u201d IEEE Transactions on Aerospace\nand Electronic Systems, pp. 1\u201313, 2021.\n[214] \u201cFpv wing racing association.\u201d\n[215] E. Tal and S. Karaman, \u201cGlobal incremental flight control for agile\nmaneuvering of a tailsitter flying wing,\u201d Journal of Guidance, Control,\nand Dynamics, vol. 45, no. 12, pp. 2332\u20132349, 2022.\n[216] E. Tal, G. Ryou, and S. Karaman, \u201cAerobatic trajectory generation for a\nvtol fixed-wing aircraft using differential flatness,\u201d IEEE Transactions\non Robotics, pp. 1\u201315, 2023.\n[217] J. Hidalgo-Carrio, G. Gallego, and D. Scaramuzza, \u201cEvent-aided direct \u00b4\nsparse odometry,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 5781\u20135790.\n[218] Z. Teed and J. Deng, \u201cDroid-slam: Deep visual slam for monocular,\nstereo, and rgb-d cameras,\u201d Advances in Neural Information Processing\nSystems, vol. 34, pp. 16 558\u201316 569, 2021.\n[219] C. Pfeiffer, S. Wengeler, A. Loquercio, and D. Scaramuzza, \u201cVisual\nattention prediction improves performance of autonomous drone racing\nagents,\u201d Plos one, vol. 17, no. 3, 2022.\n[220] G. C. de Croon, J. J. Dupeyroux, C. De Wagter, A. Chatterjee, D. A.\nOlejnik, and F. Ruffier, \u201cAccommodating unobservability to control\nflight attitude with optic flow,\u201d Nature, vol. 610, no. 7932, pp. 485\u2013\n490, 2022.\n[221] A. Bajcsy, A. Loquercio, A. Kumar, and J. Malik, \u201cLearning vision\u0002based pursuit-evasion robot policies,\u201d in 2024 IEEE International\nConference on Robotics and Automation (ICRA). IEEE, 2024.\n[222] R. Spica, D. Falanga, E. Cristofalo, E. Montijano, D. Scaramuzza, and\nM. Schwager, \u201cA real-time game theoretic planner for autonomous two\u0002player drone racing,\u201d in Robotics: Science and Systems, 2018.\n[223] Z. Wang, T. Taubner, and M. Schwager, \u201cMulti-agent sensitivity\nenhanced iterative best response: A real-time game theoretic planner for\ndrone racing in 3d environments,\u201d Robotics and Autonomous Systems,\nvol. 125, p. 103410, 2020.\n[224] J. Chen, K. Su, and S. Shen, \u201cReal-time safe trajectory generation for\nquadrotor flight in cluttered environments,\u201d in 2015 IEEE International\nConference on Robotics and Biomimetics (ROBIO). IEEE, 2015, pp.\n1678\u20131685.\n[225] F. Gao, Y. Lin, and S. Shen, \u201cGradient-based online safe trajectory\ngeneration for quadrotor flight in complex environments,\u201d in 2017\nIEEE/RSJ international conference on intelligent robots and systems\n(IROS). IEEE, 2017, pp. 3681\u20133688.\n[226] F. Gao, W. Wu, Y. Lin, and S. Shen, \u201cOnline safe trajectory gener\u0002ation for quadrotors using fast marching method and bernstein basis\npolynomial,\u201d in 2018 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2018, pp. 344\u2013351.\n[227] Y.-L. Chow, M. Pavone, B. M. Sadler, and S. Carpin, \u201cTrading safety\nversus performance: Rapid deployment of robotic swarms with robust\nperformance constraints,\u201d Journal of Dynamic Systems, Measurement,\nand Control, vol. 137, no. 3, p. 031005, 2015.\n[228] S. Singh, A. Majumdar, J.-J. Slotine, and M. Pavone, \u201cRobust online\nmotion planning via contraction theory and convex optimization,\u201d in\n2017 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2017, pp. 5883\u20135890.\n[229] S. Singh, M. Chen, S. L. Herbert, C. J. Tomlin, and M. Pavone,\n\u201cRobust tracking with model mismatch for fast and safe planning: an\nsos optimization approach,\u201d in Algorithmic Foundations of Robotics\nXIII: Proceedings of the 13th Workshop on the Algorithmic Foundations\nof Robotics 13. Springer, 2020, pp. 545\u2013564.\n[230] R. Luo, S. Zhao, J. Kuck, B. Ivanovic, S. Savarese, E. Schmerling,\nand M. Pavone, \u201cSample-efficient safety assurances using conformal\nprediction,\u201d in International Workshop on the Algorithmic Foundations\nof Robotics. Springer, 2022, pp. 149\u2013169.\n[231] M. Ono, M. Pavone, Y. Kuwata, and J. Balaram, \u201cChance-constrained\ndynamic programming with application to risk-aware robotic space\nexploration,\u201d Autonomous Robots, vol. 39, pp. 555\u2013571, 2015.\n[232] A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath,\nand P. Tabuada, \u201cControl barrier functions: Theory and applications,\u201d\nin 2019 18th European control conference (ECC). IEEE, 2019, pp.\n3420\u20133431.\n[233] S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin, \u201cHamilton-jacobi\nreachability: A brief overview and recent advances,\u201d in 2017 IEEE 56th\nAnnual Conference on Decision and Control (CDC). IEEE, 2017, pp.\n2242\u20132253.\n[234] X. Wang, K. Leung, and M. Pavone, \u201cInfusing reachability-based\nsafety into planning and control for multi-agent interactions,\u201d in 2020\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2020, pp. 6252\u20136259.\n[235] K. Leung, E. Schmerling, M. Zhang, M. Chen, J. Talbot, J. C. Gerdes,\nand M. Pavone, \u201cOn infusing reachability-based safety assurance\nwithin planning frameworks for human\u2013robot vehicle interactions,\u201d The\nInternational Journal of Robotics Research, vol. 39, no. 10-11, pp.\n1326\u20131345, 2020.\n[236] A. Elhafsi, B. Ivanovic, L. Janson, and M. Pavone, \u201cMap-predictive\nmotion planning in unknown environments,\u201d in 2020 IEEE Interna\u0002tional Conference on Robotics and Automation (ICRA). IEEE, 2020,\npp. 8552\u20138558.\n[237] K. P. Wabersich, L. Hewing, A. Carron, and M. N. Zeilinger, \u201cProba\u0002bilistic model predictive safety certification for learning-based control,\u201d\nIEEE Transactions on Automatic Control, vol. 67, no. 1, pp. 176\u2013188,\n2021.\n[238] L. Bauersfeld, E. Kaufmann, and D. Scaramuzza, \u201cUser-conditioned\nneural control policies for mobile robotics,\u201d ICRA: International Con\u0002ference on Robotics and Automation, 2023.\n[239] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine,\n\u201cMeta-world: A benchmark and evaluation for multi-task and meta\nreinforcement learning,\u201d in Conference on Robot Learning (CoRL),\n2019.\n[240] M. Turchetta, A. Kolobov, S. Shah, A. Krause, and A. Agarwal, \u201cSafe\nreinforcement learning via curriculum induction,\u201d Advances in Neural\nInformation Processing Systems, vol. 33, pp. 12 151\u201312 162, 2020.\n[241] K. Khetarpal, M. Riemer, I. Rish, and D. Precup, \u201cTowards continual\nreinforcement learning: A review and perspectives,\u201d Journal of Artifi\u0002cial Intelligence Research, vol. 75, pp. 1401\u20131476, 2022.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\nDrew Hanover is the Chief Technology Officer and\nFounder of Innovire AG. He completed his Bache\u0002lors in Mechanical Engineering at Michigan Tech\u0002nological University, and his Masters in Robotics\nat the University of Michigan. He has spent time\nworking with NASA, General Motors, and Pratt and\nMiller Engineering across a multitude of engineering\ndomains.\nAntonio Loquercio is a professor of electrical en\u0002gineering and computer science at the University\nof Pennsylvania. He received a M.Sc. degree from\nETH Zurich and a Ph.D. from the University of\nZurich in 2017 and 2021, respectively. He worked\nat the Berkeley Artificial Intelligence Research Lab\n(BAIR) at UC Berkeley from 2022 to 2024.\nLeonard Bauersfeld received his M.Sc. degree\nin robotics, system and control from ETH Zurich,\nSwitzerland in 2020. He is currently a PhD stu\u0002dent in the Robotics and Perception Group at the\nUniversity of Zurich, led by Prof. Davide Scara\u0002muzza. His reseach interests are autonomous vision\u0002based quadrotor flight and quadrotor simulations. He\nworks novel approaches, combining first-principles\nmethods with modern data-driven models to advance\nagile quadrotor flight.\nAngel Romero received a MSc degree in \u201dRobotics,\nSystems and Control\u201d from ETH Zurich in 2018.\nPreviously, he received a B.Sc. degree in Electronics\nEngineering from the University of Malaga in 2015.\nHe is currently working toward a Ph.D. degree in the\nRobotics and Perception Group at the University of\nZurich, finding new limits in the intersection of ma\u0002chine learning, optimal control, and computer vision\napplied to super agile autonomous quadrotor flight\nunder the supervision of Prof. Davide Scaramuzza.\nRobert Penicka is currently a postdoc in the Multi\u0002Robot Systems (MRS) group at the Czech Technical\nUniversity (CTU) in Prague. He did his Ph.D. at\nthe CTU in Prague in 2020 and was a postdoc\u0002toral researcher at the University of Zurich between\n2020 and 2022 under the supervision of Professor\nScaramuzza. Since 2022, he\u2019s been a research fellow\nat CTU, focusing on high-level mission planning,\ntrajectory planning, and control for UAVs. He\u2019s\nbridged the gap between mission planning and tra\u0002jectory planning, particularly in cluttered environ\u0002ments, earning recognition including the Dean\u2019s Prize and 2nd place in the\nWerner von Siemens Award for Industry 4.0. He\u2019s also won the Joseph Fourier\nPrize and the Antonin Svoboda Award for his doctoral thesis.\nYunlong Song obtained the M.Sc. degree in In\u0002formation and Communication Engineering from\nTechnical University of Darmstadt in 2018. He is\ncurrently a Ph.D. student in the Robotics and Per\u0002ception Group at the University of Zurich under\nthe supervision of Prof. Davide Scaramuzza. His\nresearch interests include reinforcement learning,\nmachine learning, and robotics.\nGiovanni Cioffi holds an M.Sc. in Mechanical\nEngineering from ETH Zurich, Switzerland, which \u00a8\nhe obtained in 2019. He is currently pursuing a Ph.D.\nat the University of Zurich under the supervision \u00a8\nof Prof. Davide Scaramuzza. His research centers\non the intersection of computer vision and robotics,\nexploring topics such as visual(-inertial) odometry\nand SLAM. His contributions were recognized by\nmultiple awards in top-tier robotic conferences and\njournals, such as the IROS 2023 Best Paper Award\nand the RA-L 2021 Best Paper Award.\nElia Kaufmann completed his Ph.D. in Informatics\nat the University of Zurich in 2022, where he was su\u0002pervised by Prof. Davide Scaramuzza. His doctoral\nresearch focused on advancing the application of\nmachine learning techniques to enhance perception\nand control of autonomous aerial vehicles. He earned\nan M.Sc. degree in Robotics, Systems, and Control\nfrom ETH Zurich in 2017, after obtaining a B.Sc. in\nMechanical Engineering in 2014. Currently, he is a\nSenior Autonomy Engineer at Skydio.\nDavide Scaramuzza is a Professor of Robotics and\nPerception at the University of Zurich. He did his\nPh.D. at ETH Zurich, a postdoc at the University\nof Pennsylvania, and was a visiting professor at\nStanford University. His research focuses on au\u0002tonomous, agile microdrone navigation using stan\u0002dard and event-based cameras. He pioneered au\u0002tonomous, vision-based navigation of drones, which\ninspired the navigation algorithm of the NASA\nMars helicopter and many drone companies. He\ncontributed significantly to visual-inertial state esti\u0002mation, vision-based agile navigation of microdrones, and low-latency, robust\nperception with event cameras, which were transferred to many products, from\ndrones to automobiles, cameras, AR/VR headsets, and mobile devices. In\n2022, his team demonstrated that an AI-controlled, vision-based drone could\noutperform the world champions of drone racing, a result that was published\nin Nature. He is a consultant for the United Nations on disaster response, AI\nfor good, and disarmament. He has won many awards, including an IEEE\nTechnical Field Award, the IEEE Robotics and Automation Society Early\nCareer Award, a European Research Council Consolidator Grant, a Google\nResearch Award, two NASA TechBrief Awards, and many paper awards. In\n2015, he co-founded Zurich-Eye, today Meta Zurich, which developed the\nworld-leading virtual-reality headset Meta Quest. In 2020, he co-founded\nSUIND, which builds autonomous drones for precision agriculture. Many\naspects of his research have been featured in the media, such as The New\nYork Times, The Economist, and Forbes.\nThis article has been accepted for publication in IEEE Transactions on Robotics. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TRO.2024.3400838\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
      "openalex_id": "https://openalex.org/W4396910019",
      "title": "Autonomous Drone Racing: A Survey",
      "publication_date": "2024-01-01",
      "cited_by_count": 18.0,
      "topics": "Sampling-Based Motion Planning Algorithms",
      "keywords": "Drone, Probabilistic Roadmaps, Robot Navigation, Optimal Motion Planning, Collision Avoidance, Path Planning",
      "concepts": "Drone, Aeronautics, Computer science, Artificial intelligence, Engineering, Computer vision, Biology, Genetics",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/8860/4359257/10530312.pdf"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W1144593952",
        "https://openalex.org/W1424654272",
        "https://openalex.org/W1501966803",
        "https://openalex.org/W1571530861",
        "https://openalex.org/W1578293866",
        "https://openalex.org/W1601577086",
        "https://openalex.org/W1846830349",
        "https://openalex.org/W1856661195",
        "https://openalex.org/W1966301784",
        "https://openalex.org/W1969483458",
        "https://openalex.org/W1971086298",
        "https://openalex.org/W1973734421",
        "https://openalex.org/W1990255692",
        "https://openalex.org/W1993687622",
        "https://openalex.org/W2015996585",
        "https://openalex.org/W2054585537",
        "https://openalex.org/W2057405577",
        "https://openalex.org/W2065767335",
        "https://openalex.org/W2105080564",
        "https://openalex.org/W2117402460",
        "https://openalex.org/W2118223742",
        "https://openalex.org/W2128990851",
        "https://openalex.org/W2141666765",
        "https://openalex.org/W2142424817",
        "https://openalex.org/W2148182166",
        "https://openalex.org/W2158055488",
        "https://openalex.org/W2162991084",
        "https://openalex.org/W2181845023",
        "https://openalex.org/W2291160178",
        "https://openalex.org/W2296673577",
        "https://openalex.org/W2317831939",
        "https://openalex.org/W2322303423",
        "https://openalex.org/W234738007",
        "https://openalex.org/W2396274919",
        "https://openalex.org/W2418849765",
        "https://openalex.org/W2465948386",
        "https://openalex.org/W2474281075",
        "https://openalex.org/W2482392012",
        "https://openalex.org/W2546070262",
        "https://openalex.org/W2559336140",
        "https://openalex.org/W2579730571",
        "https://openalex.org/W2584986912",
        "https://openalex.org/W2596973054",
        "https://openalex.org/W2599032451",
        "https://openalex.org/W2609009256",
        "https://openalex.org/W2614122538",
        "https://openalex.org/W2615547864",
        "https://openalex.org/W2654286404",
        "https://openalex.org/W2735140382",
        "https://openalex.org/W2737223130",
        "https://openalex.org/W2744855064",
        "https://openalex.org/W2745859992",
        "https://openalex.org/W2747013247",
        "https://openalex.org/W2764269968",
        "https://openalex.org/W2769498435",
        "https://openalex.org/W2772356073",
        "https://openalex.org/W2783185291",
        "https://openalex.org/W2788239209",
        "https://openalex.org/W2795837361",
        "https://openalex.org/W2883162277",
        "https://openalex.org/W2883702102",
        "https://openalex.org/W2886821123",
        "https://openalex.org/W2889731659",
        "https://openalex.org/W2891491652",
        "https://openalex.org/W2902362187",
        "https://openalex.org/W2902855213",
        "https://openalex.org/W2911956173",
        "https://openalex.org/W2946139622",
        "https://openalex.org/W2948440099",
        "https://openalex.org/W2962822327",
        "https://openalex.org/W2962871846",
        "https://openalex.org/W2962890638",
        "https://openalex.org/W2962957005",
        "https://openalex.org/W2963249250",
        "https://openalex.org/W2963449483",
        "https://openalex.org/W2963497136",
        "https://openalex.org/W2963689432",
        "https://openalex.org/W2963796870",
        "https://openalex.org/W2964040381",
        "https://openalex.org/W2964057747",
        "https://openalex.org/W2964096892",
        "https://openalex.org/W2967464230",
        "https://openalex.org/W2968116633",
        "https://openalex.org/W2968243907",
        "https://openalex.org/W2968945909",
        "https://openalex.org/W2979407507",
        "https://openalex.org/W2997492753",
        "https://openalex.org/W2999729662",
        "https://openalex.org/W3003050085",
        "https://openalex.org/W3008252953",
        "https://openalex.org/W3016460025",
        "https://openalex.org/W3022039831",
        "https://openalex.org/W3023637921",
        "https://openalex.org/W3036408458",
        "https://openalex.org/W3037804676",
        "https://openalex.org/W3038825904",
        "https://openalex.org/W3038883483",
        "https://openalex.org/W3040838455",
        "https://openalex.org/W3046365221",
        "https://openalex.org/W3047636225",
        "https://openalex.org/W3048752528",
        "https://openalex.org/W3048964782",
        "https://openalex.org/W3080443722",
        "https://openalex.org/W3080564557",
        "https://openalex.org/W3089854395",
        "https://openalex.org/W3089935474",
        "https://openalex.org/W3090649643",
        "https://openalex.org/W3091367268",
        "https://openalex.org/W3091667825",
        "https://openalex.org/W3091671391",
        "https://openalex.org/W3098009429",
        "https://openalex.org/W3099343074",
        "https://openalex.org/W3099548126",
        "https://openalex.org/W3099629894",
        "https://openalex.org/W3102483563",
        "https://openalex.org/W3102552342",
        "https://openalex.org/W3104753760",
        "https://openalex.org/W3106440972",
        "https://openalex.org/W3107452320",
        "https://openalex.org/W3109557043",
        "https://openalex.org/W3114626444",
        "https://openalex.org/W3117215073",
        "https://openalex.org/W3119367492",
        "https://openalex.org/W3120099762",
        "https://openalex.org/W3120459386",
        "https://openalex.org/W3124958681",
        "https://openalex.org/W3131850807",
        "https://openalex.org/W3131851942",
        "https://openalex.org/W3134336860",
        "https://openalex.org/W3134703279",
        "https://openalex.org/W3135496326",
        "https://openalex.org/W3135798748",
        "https://openalex.org/W3153486053",
        "https://openalex.org/W3155272911",
        "https://openalex.org/W3159537771",
        "https://openalex.org/W3160193765",
        "https://openalex.org/W3164594912",
        "https://openalex.org/W3169213814",
        "https://openalex.org/W3169408498",
        "https://openalex.org/W3170760772",
        "https://openalex.org/W3175924829",
        "https://openalex.org/W3177010373",
        "https://openalex.org/W3185165122",
        "https://openalex.org/W3185877016",
        "https://openalex.org/W3193551877",
        "https://openalex.org/W3195968524",
        "https://openalex.org/W3197225143",
        "https://openalex.org/W3199091020",
        "https://openalex.org/W3200501401",
        "https://openalex.org/W3202650801",
        "https://openalex.org/W3202883604",
        "https://openalex.org/W3205180086",
        "https://openalex.org/W3205915463",
        "https://openalex.org/W3206008794",
        "https://openalex.org/W3207033168",
        "https://openalex.org/W3207305612",
        "https://openalex.org/W3207920912",
        "https://openalex.org/W3209325542",
        "https://openalex.org/W3217056046",
        "https://openalex.org/W4200438872",
        "https://openalex.org/W4200465206",
        "https://openalex.org/W4200630212",
        "https://openalex.org/W4205172069",
        "https://openalex.org/W4206724414",
        "https://openalex.org/W4206742276",
        "https://openalex.org/W4210391201",
        "https://openalex.org/W4211021883",
        "https://openalex.org/W4213454489",
        "https://openalex.org/W4214503958",
        "https://openalex.org/W4214730142",
        "https://openalex.org/W4224914563",
        "https://openalex.org/W4230158535",
        "https://openalex.org/W4230766380",
        "https://openalex.org/W4246614213",
        "https://openalex.org/W4283170591",
        "https://openalex.org/W4283262038",
        "https://openalex.org/W4285092328",
        "https://openalex.org/W4285102199",
        "https://openalex.org/W4285117968",
        "https://openalex.org/W4285163488",
        "https://openalex.org/W4285228542",
        "https://openalex.org/W4285818823",
        "https://openalex.org/W4286224728",
        "https://openalex.org/W4289927749",
        "https://openalex.org/W4290803068",
        "https://openalex.org/W4294007115",
        "https://openalex.org/W4296912576",
        "https://openalex.org/W4306810146",
        "https://openalex.org/W4307210176",
        "https://openalex.org/W4311443220",
        "https://openalex.org/W4312400353",
        "https://openalex.org/W4313855813",
        "https://openalex.org/W4323065891",
        "https://openalex.org/W4367016663",
        "https://openalex.org/W4378713510",
        "https://openalex.org/W4383066163",
        "https://openalex.org/W4383108196",
        "https://openalex.org/W4383109238",
        "https://openalex.org/W4385430550",
        "https://openalex.org/W4385430610",
        "https://openalex.org/W4385975664",
        "https://openalex.org/W4386075826",
        "https://openalex.org/W4386285856",
        "https://openalex.org/W4386718967",
        "https://openalex.org/W4386794245",
        "https://openalex.org/W4388505476",
        "https://openalex.org/W4389060511",
        "https://openalex.org/W4389665475",
        "https://openalex.org/W4390577843",
        "https://openalex.org/W4391833772",
        "https://openalex.org/W4392251572",
        "https://openalex.org/W4394651791"
      ],
      "openalex_rank": 8,
      "num_tokens": 39173,
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/8860/4359257/10530312.pdf",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key advancements in autonomous drone racing technology, the challenges faced in achieving high-speed navigation and control, and what future directions could enhance the performance and safety of these systems in real-world applications?",
      "pipeline_answer": "### Key Advancements in Autonomous Drone Racing Technology:\n- **Deep Learning Enhancements:** Recent advancements in deep learning have significantly improved real-time object detection capabilities for UAVs, which are essential for enhancing navigation and control in autonomous drone racing [S3734047934].\n- **Multi-Sensor Fusion:** The integration of multi-sensor fusion techniques, including RGB cameras and LiDAR, enhances situational awareness and obstacle detection, critical for achieving high-speed navigation in drone racing [S9703836577].\n- **Event-Based Cameras:** The use of event-based cameras in UAVs presents a promising avenue for improving navigation in dynamic environments, which is crucial for the fast-paced nature of drone racing [S5256975047].\n\n### Challenges in High-Speed Navigation and Control:\n- **Rapid Image Processing:** One of the main challenges in high-speed navigation for autonomous drones is the need for rapid processing of high-resolution images to detect and track objects. This challenge can be addressed by optimizing deep learning models for efficiency [S3140539497].\n\n### Future Directions for Performance and Safety Enhancements:\n- **Specialized Datasets:** Developing specialized datasets that simulate racing environments can significantly enhance the training of object detection algorithms, improving both performance and safety in dynamic drone racing scenarios [S3345294563] [S1887446725].\n- **Regulatory Frameworks:** The integration of regulatory frameworks, such as the NHTSA test suite for evaluating drone safety, can enhance safety protocols in drone racing systems by providing standardized procedures for performance assessment [S5538602873].\n- **Adapting Automotive Safety Standards:** Adapting automotive safety standards like ISO 26262 for UAVs can improve safety in drone racing systems by establishing rigorous safety goals and integrity levels tailored to the unique challenges of aerial vehicles [S4678038695].\n\n### Conclusion:\nThe advancements in autonomous drone racing technology, including deep learning, multi-sensor fusion, and event-based cameras, are paving the way for improved navigation and control. However, challenges such as rapid image processing remain. Future enhancements can focus on specialized datasets and regulatory frameworks to ensure both performance and safety in real-world applications.",
      "pipeline_references": {
        "S3734047934": {
          "id": "S3734047934",
          "text": "Recent advancements in deep learning have significantly improved real-time object detection capabilities for UAVs, which can be directly applied to enhance navigation and control in autonomous drone racing.",
          "children": [
            {
              "id": "E6058154070",
              "text": "Arman Asgharpoor Golroudbari\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nUniversity of Tehran\nTehranIran\nMohammad Hossein Sabour sabourmh@ut.ac.ir\nUniversity of Tehran\nTehranIran\nRECENT ADVANCEMENTS IN DEEP LEARNING APPLICATIONS AND METHODS FOR AUTONOMOUS NAVIGATION: A COMPREHENSIVE REVIEW\nDeep Learning \u00b7 Navigation \u00b7 Inertial Sensors, Intelligent Filter \u00b7 Sensor Fusion \u00b7 Long-Short Term Memory \u00b7 Convolutional Neural Network\nThis review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.\nIntroduction\nAutonomous navigation is a critical component of robotics that has transformed numerous application domains, such as medical, industrial, space, and agricultural. By equipping robots with the ability to navigate autonomously, they can efficiently and securely move through dynamic environments without human intervention, expanding their versatility and functionality. To enhance the performance of autonomous navigation systems, researchers have been pushing the technology to its limits, employing state-of-the-art techniques and methodologies.\nGiven the expansive and ever-evolving nature of the literature surrounding autonomous navigation, it is imperative to conduct regular literature surveys in order to remain abreast of the latest advancements. Therefore, the primary objective of this review is to provide a comprehensive and in-depth overview of the current state-of-the-art in autonomous navigation, with a focus on catering to both experienced researchers and novices in the field. Additionally, a terminology section is included to provide clarity and understanding of the technical vocabulary utilized throughout the article.\n\u2022 Mapping: Creating a map of an environment using sensor data and other inputs.\n\u2022 Simultaneous Localization and Mapping (SLAM): Creating a map of an unknown environment while simultaneously localizing the robot or autonomous system within that environment. \u2022 Control: Regulating the motion to follow a desired trajectory and achieve a specific task. \u2022 Obstacle Avoidance: Navigate around obstacles in the path. \u2022 Collision Avoidance: Using sensors and algorithms to detect potential collisions and take action to avoid them. \u2022 Path Planning: Determining a safe and efficient path for an autonomous system to follow. \u2022 Motion Planning: Determining the trajectory to reach a goal while avoiding obstacles and adhering to other constraints. \u2022 Sensor Fusion: Combining data from multiple sensors to obtain a more accurate and comprehensive understanding of the environment. \u2022 Odometry: Using sensory data to estimate the position and orientation by analyzing the movement over time.\n\u2022 Dead Reckoning: Estimates the current position by using its previous position and velocity. Navigation is a critical task for systems that operate in dynamic environments, such as robots, autonomous systems, and unmanned aerial vehicles [1]. It requires the ability to perceive the surroundings, plan a path, execute it, and adapt as needed, all while avoiding obstacles and collisions to ensure safe, efficient, and accurate travel. Recent developments in deep learning have made navigation more reliable, effective, and efficient, enabling its use in a wide range of applications, including transportation, search and rescue, and delivery. Autonomous systems are becoming increasingly prevalent and can determine their actions based on the current situation. These systems have numerous uses, such as self-driving cars [2], drones [3], and search-and-rescue robots [4].\nAutonomous systems fall into two broad categories [5]: reactive and deliberative. Reactive systems, also known as behavior-based systems, are designed to respond to the environment using predefined rules. These systems are typically used in robotics applications, where the environment is relatively stable, and the system has a specific task, such as pick and place. On the other hand, deliberative systems are designed to plan and execute a path to a destination. These systems are commonly used in transportation, where the environment is complex, and the system must navigate efficiently and safely.\nThe deliberative systems can be further classified into two categories [6]: (1) model-based and (2) model-free systems. Model-based systems use a mathematical model of the environment to plan a path, such as using dynamic programming or graph search algorithms [7]. Model-free systems, also known as model-agnostic systems, do not rely on a model of the environment to plan a path [8]. Instead, these systems use techniques such as Reinforcement Learning [9] or Apprenticeship Learning [10] to learn the optimal policy for navigation. The choice of autonomous system, whether reactive or deliberative and model-based or model-free, depends on the specific requirements and constraints of the task.\nAutonomous systems require effective and successful navigation to operate in dynamic environments without human intervention and guidance. This involves integrating various technologies, including sensors, actuators, and control systems. To improve the accuracy, efficiency, and robustness of navigation algorithms, deep learning has been applied to various navigation tasks such as perception and planning. Deep learning's ability to learn complex representations from vast amounts of data is well-suited for these tasks, such as image analysis and integrating multiple sources of information, such as speech and text. However, deep learning-based navigation systems must overcome challenges such as limited data, reliability, ethical concerns, such as privacy and bias. Techniques such as transfer learning, multi-modal fusion, model uncertainty estimation, and safety-critical architectures can address these challenges. Additionally, differential privacy and fairness-aware machine learning techniques can address privacy and bias concerns, respectively.\nThe application of deep learning in navigation has seen a rise in recent times, as evidenced by numerous studies and surveys (refer to Tables 1 and 2). Although deep learning holds great promise in enhancing navigation systems, it is crucial to tackle the challenges and ethical considerations that come with its usage. Furthermore, there is a need for further exploration on how deep learning techniques can be integrated with conventional navigation methods.\nNumerous surveys have been conducted on the applications of deep learning in various navigation domains, including urban navigation [11], visual navigation [12,13], reinforcement learning [14,9], obstacle detection [15], and spacecraft navigation [16,17]. However, there is a lack of comprehensive surveys that provide a general overview of the use of deep learning in navigation. This survey aims to fill this gap by presenting a comprehensive overview of the applications of deep learning in navigation. The paper is structured as follows: Section 2 provides an overview of deep learning and its methods, while Section 3 discusses the different activation functions used in deep learning. Section 4 presents an overview of navigation, autonomy, and autonomous navigation. Section 5 discusses the applications of deep learning in navigation, and Section 6 delves into the various components of deep learning in autonomous navigation, such as perception, localization, mapping, planning, and control. Finally, Section 7 concludes the paper and highlights future directions. Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization [18] Multi-robot Navigation\nHINNet: Inertial navigation with head-mounted sensors using a neural network [19] Inertial Navigation Multi-sensor integrated navigation/positioning systems using data fusion: From analytics-based to learning-based approaches [20] Integrated Navigation Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction [21] Visual Navigation Goal-guided Transformer-enabled Reinforcement Learning for Efficient Autonomous Navigation [22] Autonomous Navigation DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments [23] Visual Navigation Monocular vision with deep neural networks for autonomous mobile robots navigation [24] Visual Navigation URWalking: Indoor Navigation for Research and Daily Use [25] Indoor Navigation A Simple Self-Supervised IMU Denoising Method For Inertial Aided Navigation [26] Inertial Na",
              "url": "https://openalex.org/W4381333019",
              "openalex_id": "https://openalex.org/W4381333019",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-20"
            },
            {
              "id": "E7895996755",
              "text": "Journal of Imaging Review A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles Dario Cazzato 1,\u2217 , Claudio Cimarelli 1, Jose Luis Sanchez-Lopez 1, Holger Voos 1 and Marco Leo 2 1 Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, 1855 Luxembourg, Luxembourg; claudio.cimarelli@uni.lu (C.C.); joseluis.sanchezlopez@uni.lu (J.L.S.-L.); holger.voos@uni.lu (H.V.) 2 Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, 73100 Lecce, Italy; marco.leo@cnr.it * Correspondence: dario.cazzato@uni.lu Received: 29 June 2020; Accepted: 31 July 2020; Published: 4 August 2020 Abstract: The spread of Unmanned Aerial Vehicles (UAVs) in the last decade revolutionized many applications fields. Most investigated research topics focus on increasing autonomy during operational campaigns, environmental monitoring, surveillance, maps, and labeling. To achieve such complex goals, a high-level module is exploited to build semantic knowledge leveraging the outputs of the low-level module that takes data acquired from multiple sensors and extracts information concerning what is sensed. All in all, the detection of the objects is undoubtedly the most important low-level task, and the most employed sensors to accomplish it are by far RGB cameras due to costs, dimensions, and the wide literature on RGB-based object detection. This survey presents recent advancements in 2D object detection for the case of UAVs, focusing on the differences, strategies, and trade-offs between the generic problem of object detection, and the adaptation of such solutions for operations of the UAV. Moreover, a new taxonomy that considers different heights intervals and driven by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints is proposed. Keywords: computer vision; 2d object detection; unmanned aerial vehicles; deep learning 1. Introduction Unmanned Aerial Vehicles (UAVs), also called Unmanned Aircraft Systems (UASs), and commonly known as drones, are aircraft that fly without a pilot on-board. This places numerous advantages in terms of pilot safety, training, and aircraft costs and sizes, with a huge impact in the range of possible applications. Numbers behind the UAV industry are impressive: Value Market Research estimated that the market for VTOL (Vertical Take-Off and Landing) UAVs will touch around USD 10,163 M by 2024 [1]. Another report from PwC [2] estimates that, in 2030, there will be 76,000 drones operating in the UK skies, involving a total of 628,000 jobs. These forecasts will imply, still in the UK, an increase in GDP of 42 bn\u00a3 and net savings for the UK economy of 16 bn\u00a3. Finally, an EU report of 2016 [3] estimated an economic impact exceeding e10 bn per year within 20 years. As a consequence, both research and industry are investigating the challenges involved in the manufacturing as well as the design of hardware, software, sensors and algorithms to guarantee the UAV operability and to extend its range to unseen scenarios. In fact, UAVs already achieved an unprecedented seen level of growth in many civil and military application domains [4]. UAVs can be remotely controlled by a pilot or can fly autonomously. In the former scenario, the pilot is on land- or sea-based ground control station (GCS) for human control. J. Imaging 2020, 6, 78; doi:10.3390/jimaging6080078 www.mdpi.com/journal/jimaging J. Imaging 2020, 6, 78 2 of 38 The simplest GCS consists of a remote controller with an optional screen, even in the form of a tablet. In the latter scenario, instead, a pre-scheduled flight plan and a dynamic automation system are necessary. The Holy Grail for the involved actors in this revolution is the achievement of fully autonomous operational capabilities to flight over and understand real and complex scenarios. In such an ideal system, a high-level and possibly on-board module is exploited to build semantic knowledge used to reach the application goal. The semantic knowledge leverages low-level software components that extract information concerning what is sensed. Both exteroceptive and proprioceptive sensors are used to obtain situational and self-awareness [5]. From the beginning, UAVs were equipped with sensors such as Global Positioning System (GPS) and Inertial Navigation System (INS) to provide position and orientation in space, but they come with serious drawbacks. The precision of the GPS depends on the general number of available satellites; moreover, urban canyons and indoor navigation can seriously compromise the navigation. INS, instead, suffers from integration drift with acceleration and angular velocity error accumulation, requiring a correction scheme. Presently, the software and hardware advancements in embedded systems and the corresponding miniaturization have led to performing low-cost sensors and Inertial Measurement Units (IMUs) that can extract useful information on-board, such as force, angular rate, and orientation. Many approaches and configurations have been also proposed to get significant knowledge of the environment from data acquired by consumer RGB cameras, depth sensors, LiDAR (Light Detection and Ranging), and event-based cameras. Complex and complete sensor fusion suites that merge multiple data have been introduced too. For each sensor that can be potentially mounted on-board of the UAV, a plethora of works and applications have been proposed. Independently from the employed sensor, where evidently each one comes with own pro and cons, and/or each sensor is better performing in a specific scenario, all of the approaches share the goal of providing meaningful input for the high-level components. Undoubtedly, computer vision can provide a critical contribution to the autonomy of the UAVs and their operational capabilities [6]. Typical UAV operations are surveillance, person tracking, path planning, obstacle avoidance (see Section 2): all of these tasks strongly relies on the detection of one or more domain-related objects. Object detection has then been widely investigated since the beginning of computer vision. Historically, detecting objects in images taken from a camera has represented one of the first computer vision tasks ever: early works are dated to 1960s [7], and a kick-off work that is famous (and considered quite optimistic) in the computer vision community Summer Vision Project is dated 1966 [8]. Henceforth, the possibility of detecting and recognizing classes of objects has been considered to be the first component of any artificial intelligence system, and many proposed theoretical techniques in the computer vision community have been applied to such a task. If many impressive results have been achieved even outperforming human-level performance in object detection, there is still a gap to be filled when the problem is translated to the specific case of aerial robotics. Different challenges are involved in terms of performance, scene, object classes, point of view, perspective, data acquisition, and so on. Moreover, if many datasets are available for object recognition task, they cannot be directly employed in the case of UAVs since the scenes are different from the operational working conditions of the UAV (e.g., indoor, cluttered environments, cameras placed at ground level). Finally, computational constraints and/or communication schemes are an important issue to be addressed. Very precise surveys on the role of computer vision for the autonomous operation of UAV exist. They focus on high-level operations [6,9], they consider only a specific altitude and/or imaging type [10,11], a technology [12], or a precise use-case [5,13\u201316]. It can be observed how, in these very important surveys, the computer vision point of view has been only partially considered. This has been the first motivation behind this work. This work investigates the object detection problem for the specific case of the UAVs from RGB and 2D object detection, introducing also main concepts J. Imaging 2020, 6, 78 3 of 38 and references for mixed sensor suites and/or 3D object detection. The second motivation is in the need for a different categorization of the UAVs: there are many valid classification schemes based on characteristics as the mass, size, mission range, operation heights, level of autonomy, flying principle, operation condition [17]. Anyway, concerning the height, it is common to classify UAVs in intervals of 150\u2013300 m, 3000, 5000 and even 20,000 m, always starting from 50 m. From a computer vision point of view, we think that this scheme is not sufficient to understand the methodologies that are applied to address the object detection problem depending on the operational height. Thus, a conceptual approach that takes into account the operations carried out at very different heights has not been proposed yet. To the best of our knowledge, we present the first work that classifies object recognition methods for the case of UAVs that considers different heights intervals, whose definition is given by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints. At the same time, our proposal is well-integrated with active EU rules and procedures for the operation of unmanned aircraft. Moreover, how specific state of the art deep learning architectures are adapted for the case under consideration is discussed, and the main applications that introduce their own scientific and technological peculiarities are illustrated. Finally, the different datasets specifically designed to evaluate object detection from aerial views are reported and detailed. Summing up, the main contributions of this work are: \u2022 an update of dominant literature aiming at performing object detection from UAV and aerial views; \u2022 a taxonomy for the UAV based on the computer vision point of view, that considers how the same problem can drastically change when it is observed from a different perspective; \u2022 a critical discussion of the actual state of the art, with particular attention to the impact of deep learning. The manuscript is organized as follows: first of all, the role of object detection from UAVs in terms of higher-level operations is illustrated in Section 2, also introducing some important sensors employed in the state of the..",
              "url": "https://openalex.org/W3047386722",
              "openalex_id": "https://openalex.org/W3047386722",
              "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
              "publication_date": "2020-08-04"
            }
          ]
        },
        "S1887446725": {
          "id": "S1887446725",
          "text": "The development of specialized datasets that simulate racing environments can significantly enhance the training of object detection algorithms for autonomous drone racing.",
          "children": [
            {
              "id": "E8708102179",
              "text": "..autonomous systems. VIII. EXPLORING ADJACENT TERRITORIES: A CLOSER LOOK AT TWO KEY ASPECTS A. SIM-TO-REAL ROBOTS AND SYSTEMS Sim-to-real robots and systems are challenging to develop and deploy due to the gap between simulation and reality [83]. In striving for a streamlined workflow to transition seamlessly from simulation to real-world applications, the key to success lies in: A) the meticulous optimization of the runtime and inference architecture, catering specifically to the target hardware and the intricacies of the application domain and B) comprehensively addressing all edge cases during the process of developing autonomy blocks, ensuring the robustness of algorithm performance to adhere to the standards of the safetycritical aviation industry. DO-178C and CoDANN help with the latter. A fundamental aspect of this optimization pertains to domain adaptation. Domain adaptation refers to the process of customizing the sensing and perception modules of the robot according to the specific application domain [84]. For example, different types of sensors may be required for indoor and outdoor environments, or for different weather conditions. Moreover, the sensor data may vary significantly from simulation to reality, requiring robust and adaptive models that can handle domain shifts. Various techniques such as data augmentation, domain randomization, and adversarial learning can be leveraged to train and test the AI models (powering various Autonomy Blocks) in diverse and realistic scenarios. Online fine-tuning refers to the ability of the robot to adapt its behavior and decision-making modules based on the feedback from the environment and the user [85]. Online finetuning enables the autonomous system to improve its performance based on continuous learning and real-world experiences, thereby promoting enhanced autonomy and reliability. For example, the aerial vehicle may need to adaptively adjust its speed, trajectory, or navigation strategy according to the dynamic and uncertain situations it encounters in the real world, which it has or hasn\u2019t necessarily encountered during the simulation-based training and testing. Methods such as reinforcement learning, imitation learning, and active learning can be employed to enable online learning and improvement of the AI models in an interactive and dataefficient manner. IoT edge device deployments refer to the implementation of autonomy solutions on power-efficient embedded AI computing devices that can be integrated with the aerial vehicle's hardware [86]. For example, the UAV may need to run its models on a low-power CPU or GPU that can fit within its SWaP-c constraints. Moreover, the V2X capability will require the vehicle to communicate with other devices or cloud services via wireless networks, requiring reliable and secure data transmission protocols. The models need to be optimized for edge deployment using techniques such as model compression, quantization, pruning, and distillation, as well as leveraging edge computing platforms such as Azure IoT Edge. Leveraging the potency of embedded AI computing devices, this paradigm facilitates the efficient and seamless integration of autonomous capabilities into resource-constrained UAV environments. B. MONOLITHIC DEEP LEARNING FOR AUTONOMOUS AERIAL VEHICLES: CHALLENGES AND OPPORTUNITIES Monolithic deep learning algorithms typically refer to comprehensive, end-to-end machine learning models that handle multiple aspects of autonomous flight. These can include tasks such as obstacle detection and avoidance, path planning, and navigation. Monolithic deep learning models are tightly coupled systems where all the layers work in a highly synchronized manner. These models are often seen as a single, centralized unit, which can make them easier to develop, test, and debug. These algorithms are \"monolithic\" in the sense that they are designed to handle multiple tasks within the same framework, rather than relying on separate models or systems for each task. This can lead to more efficient and coordinated behavior in autonomous aerial robots. Monolithic models emphasize tight integration and synchronization of all components. However, this can also make them less flexible and adaptable compared to more modular or distributed systems. In recent years, the field of AI has witnessed a significant transformation, shifting from task-specific, narrow models to This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 29 Volume xx, 2023 larger, more versatile monolithic neural networks. For instance, within the domain of natural language processing (NLP), models like GPT-4 have demonstrated an impressive array of capabilities, encompassing tasks such as text summarization, translation, and sentiment analysis. Concurrently, visuallanguage models have been gaining proficiency in a multitude of tasks, including object detection, image captioning, and even generative tasks like creating artwork. This progression implies the potential for a unified, generalized model to potentially replace numerous task-specific models, offering enhanced efficiency and a simpler system architecture. However, when transitioning from NLP to the realm of robotics, a number of complexities come to the forefront. Firstly, there is a notable scarcity of data (see Section III.B for challenges with Synthetic Data creation process), as end-toend foundation models necessitate extensive training data, and there is a limited availability of curated datasets for pre-training robots. Consequently, the emphasis shifts towards enhancing the intelligence of existing foundation models for each of the proposed Autonomy Blocks (namely, Sense, Perceive, Plan, Actuate) even when their original application differs from that of aerial autonomy. Additionally, in the field of robotics, the wide variability in actuators and control systems introduces an additional layer of intricacy. Each type of aerial vehicle, whether it's a quadcopter, eVTOL, traditional aircraft, or helicopter, possesses a distinct set of actuators and corresponding control systems. This makes the concept of generalization more challenging. Employing a monolithic neural network that directly maps sensor inputs to actuator outputs is therefore not scalable and also risks overlooking the wealth of existing research in control theory. IX. CONCLUSION AND OUTLOOK The outlook of the Advanced Aerial Mobility (AAM) field is poised for transformative change, with an increasing recognition of the need for AAM solutions in both urban and rural contexts. Urban congestion and gridlock have become a ubiquitous problem, and AAM holds the potential to alleviate these issues by introducing unmanned aerial vehicles (UAVs) for passenger and cargo transportation. In parallel, the use of UAVs is already revolutionizing various industries, from agriculture and construction to healthcare and logistics. These aircraft provide cost-efficient and rapid solutions for tasks such as crop monitoring, site surveys, medical supply delivery, and last-mile logistics, offering a glimpse into the future where AAM will play a pivotal role in enhancing productivity and quality of life. The research and development of fully autonomous aerial vehicles is advancing at an impressive pace, propelling the AAM field toward its full potential. In this paper, we have presented a comprehensive study of the autonomous aerial mobility field, consisting of four main components: simulation, data, autonomy, and multi-agent fleets. We have described the functionalities and technical underpinnings of each component and how they interact with each other to enable safe and efficient operations of aerial vehicles, particularly AAMs based UAVs, in complex urban environments. We have outlined key innovations as well as existing systems. The focal point of our work is the autonomy blocks framework. This modular AIbased approach aims to address the full spectrum of autonomy for advanced aerial mobility, from sensing and perception to planning and control. We have proposed a customizable, modular, and extensible design paradigm that allows for building autonomy stack for different levels of autonomy and different types of aerial vehicles. We have also reviewed the state-of-the-art research and technologies in various domains and sectors that are relevant to our framework, including deep learning algorithms that cater to specific modules of the proposed autonomy stack. Furthermore, we have discussed the challenges and opportunities for benchmarking and validating our framework based on the up-and-coming standards, guidelines, and ConOps being established by regulatory bodies around the world. Autonomous aerial vehicles need to comply with the tight regulatory oversight that governs the aerial mobility industry as ensuring the safety of passengers, property, and infrastructure is of paramount importance. This requires high standards of safety, security, and reliability. Therefore, AAM requires a multidisciplinary effort that integrates cutting-edge research and development from various fields, such as aviation engineering, computer science, artificial intelligence, robotics, and human factors. We believe that our autonomy blocks framework offers a holistic and comprehensive approach to developing the underlying technology \u2013 rooted in the multidisciplinary foundations - to advancing the field of autonomous aerial mobility. We hope that our work will inspire further research and innovation in this exciting and important domain. REFERENCES [1] B. A. Adu-Gyamfi and C. Good, \"Electric aviation: A review of concepts and enabling technologies,\" Transportation Engineering, vol. 9, pp. 100 - 134, 2022. [2] MarketsAndMarkets, \"eVTOL Aircraft Market,\" 2023. [3] eHang, \" Air Mobility | Smart City Management | Aerial Media,\" [Online]. Available: https://ehang.com/. [Accessed 06 11 2023]. [4] A. Hussain and D. Silver, \"Advanced air mobility. Can the United States afford to lose the race?,\" 26 01 2021. [Online]. Available: https://www2.deloitte.com/us/en/insights/industry/aerospace -defense/advanced-air-mobility.html. [Accessed 06 11 2023]. [5] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil, N. S. Othman, A. Khreishah and M. Guizani, \"Unmanned Aerial Vehicles (UAVs): A Survey on This article has been accepted for publication in IEEE Access. This is the author'..",
              "url": "https://openalex.org/W4389352644",
              "openalex_id": "https://openalex.org/W4389352644",
              "title": "Autonomous Advanced Aerial Mobility \u2013An End-to-end Autonomy Framework for UAVs and Beyond",
              "publication_date": "2023-01-01"
            },
            {
              "id": "S3345294563",
              "text": "Future directions for enhancing the performance and safety of autonomous drone racing systems may involve the development of specialized datasets that simulate racing environments to improve training for object detection algorithms.",
              "children": [
                {
                  "id": "E6058154070",
                  "text": "Arman Asgharpoor Golroudbari\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nUniversity of Tehran\nTehranIran\nMohammad Hossein Sabour sabourmh@ut.ac.ir\nUniversity of Tehran\nTehranIran\nRECENT ADVANCEMENTS IN DEEP LEARNING APPLICATIONS AND METHODS FOR AUTONOMOUS NAVIGATION: A COMPREHENSIVE REVIEW\nDeep Learning \u00b7 Navigation \u00b7 Inertial Sensors, Intelligent Filter \u00b7 Sensor Fusion \u00b7 Long-Short Term Memory \u00b7 Convolutional Neural Network\nThis review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.\nIntroduction\nAutonomous navigation is a critical component of robotics that has transformed numerous application domains, such as medical, industrial, space, and agricultural. By equipping robots with the ability to navigate autonomously, they can efficiently and securely move through dynamic environments without human intervention, expanding their versatility and functionality. To enhance the performance of autonomous navigation systems, researchers have been pushing the technology to its limits, employing state-of-the-art techniques and methodologies.\nGiven the expansive and ever-evolving nature of the literature surrounding autonomous navigation, it is imperative to conduct regular literature surveys in order to remain abreast of the latest advancements. Therefore, the primary objective of this review is to provide a comprehensive and in-depth overview of the current state-of-the-art in autonomous navigation, with a focus on catering to both experienced researchers and novices in the field. Additionally, a terminology section is included to provide clarity and understanding of the technical vocabulary utilized throughout the article.\n\u2022 Mapping: Creating a map of an environment using sensor data and other inputs.\n\u2022 Simultaneous Localization and Mapping (SLAM): Creating a map of an unknown environment while simultaneously localizing the robot or autonomous system within that environment. \u2022 Control: Regulating the motion to follow a desired trajectory and achieve a specific task. \u2022 Obstacle Avoidance: Navigate around obstacles in the path. \u2022 Collision Avoidance: Using sensors and algorithms to detect potential collisions and take action to avoid them. \u2022 Path Planning: Determining a safe and efficient path for an autonomous system to follow. \u2022 Motion Planning: Determining the trajectory to reach a goal while avoiding obstacles and adhering to other constraints. \u2022 Sensor Fusion: Combining data from multiple sensors to obtain a more accurate and comprehensive understanding of the environment. \u2022 Odometry: Using sensory data to estimate the position and orientation by analyzing the movement over time.\n\u2022 Dead Reckoning: Estimates the current position by using its previous position and velocity. Navigation is a critical task for systems that operate in dynamic environments, such as robots, autonomous systems, and unmanned aerial vehicles [1]. It requires the ability to perceive the surroundings, plan a path, execute it, and adapt as needed, all while avoiding obstacles and collisions to ensure safe, efficient, and accurate travel. Recent developments in deep learning have made navigation more reliable, effective, and efficient, enabling its use in a wide range of applications, including transportation, search and rescue, and delivery. Autonomous systems are becoming increasingly prevalent and can determine their actions based on the current situation. These systems have numerous uses, such as self-driving cars [2], drones [3], and search-and-rescue robots [4].\nAutonomous systems fall into two broad categories [5]: reactive and deliberative. Reactive systems, also known as behavior-based systems, are designed to respond to the environment using predefined rules. These systems are typically used in robotics applications, where the environment is relatively stable, and the system has a specific task, such as pick and place. On the other hand, deliberative systems are designed to plan and execute a path to a destination. These systems are commonly used in transportation, where the environment is complex, and the system must navigate efficiently and safely.\nThe deliberative systems can be further classified into two categories [6]: (1) model-based and (2) model-free systems. Model-based systems use a mathematical model of the environment to plan a path, such as using dynamic programming or graph search algorithms [7]. Model-free systems, also known as model-agnostic systems, do not rely on a model of the environment to plan a path [8]. Instead, these systems use techniques such as Reinforcement Learning [9] or Apprenticeship Learning [10] to learn the optimal policy for navigation. The choice of autonomous system, whether reactive or deliberative and model-based or model-free, depends on the specific requirements and constraints of the task.\nAutonomous systems require effective and successful navigation to operate in dynamic environments without human intervention and guidance. This involves integrating various technologies, including sensors, actuators, and control systems. To improve the accuracy, efficiency, and robustness of navigation algorithms, deep learning has been applied to various navigation tasks such as perception and planning. Deep learning's ability to learn complex representations from vast amounts of data is well-suited for these tasks, such as image analysis and integrating multiple sources of information, such as speech and text. However, deep learning-based navigation systems must overcome challenges such as limited data, reliability, ethical concerns, such as privacy and bias. Techniques such as transfer learning, multi-modal fusion, model uncertainty estimation, and safety-critical architectures can address these challenges. Additionally, differential privacy and fairness-aware machine learning techniques can address privacy and bias concerns, respectively.\nThe application of deep learning in navigation has seen a rise in recent times, as evidenced by numerous studies and surveys (refer to Tables 1 and 2). Although deep learning holds great promise in enhancing navigation systems, it is crucial to tackle the challenges and ethical considerations that come with its usage. Furthermore, there is a need for further exploration on how deep learning techniques can be integrated with conventional navigation methods.\nNumerous surveys have been conducted on the applications of deep learning in various navigation domains, including urban navigation [11], visual navigation [12,13], reinforcement learning [14,9], obstacle detection [15], and spacecraft navigation [16,17]. However, there is a lack of comprehensive surveys that provide a general overview of the use of deep learning in navigation. This survey aims to fill this gap by presenting a comprehensive overview of the applications of deep learning in navigation. The paper is structured as follows: Section 2 provides an overview of deep learning and its methods, while Section 3 discusses the different activation functions used in deep learning. Section 4 presents an overview of navigation, autonomy, and autonomous navigation. Section 5 discusses the applications of deep learning in navigation, and Section 6 delves into the various components of deep learning in autonomous navigation, such as perception, localization, mapping, planning, and control. Finally, Section 7 concludes the paper and highlights future directions. Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization [18] Multi-robot Navigation\nHINNet: Inertial navigation with head-mounted sensors using a neural network [19] Inertial Navigation Multi-sensor integrated navigation/positioning systems using data fusion: From analytics-based to learning-based approaches [20] Integrated Navigation Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction [21] Visual Navigation Goal-guided Transformer-enabled Reinforcement Learning for Efficient Autonomous Navigation [22] Autonomous Navigation DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments [23] Visual Navigation Monocular vision with deep neural networks for autonomous mobile robots navigation [24] Visual Navigation URWalking: Indoor Navigation for Research and Daily Use [25] Indoor Navigation A Simple Self-Supervised IMU Denoising Method For Inertial Aided Navigation [26] Inertial Na",
                  "url": "https://openalex.org/W4381333019",
                  "openalex_id": "https://openalex.org/W4381333019",
                  "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
                  "publication_date": "2023-06-20"
                },
                {
                  "id": "E7895996755",
                  "text": "Journal of Imaging Review A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles Dario Cazzato 1,\u2217 , Claudio Cimarelli 1, Jose Luis Sanchez-Lopez 1, Holger Voos 1 and Marco Leo 2 1 Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, 1855 Luxembourg, Luxembourg; claudio.cimarelli@uni.lu (C.C.); joseluis.sanchezlopez@uni.lu (J.L.S.-L.); holger.voos@uni.lu (H.V.) 2 Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, 73100 Lecce, Italy; marco.leo@cnr.it * Correspondence: dario.cazzato@uni.lu Received: 29 June 2020; Accepted: 31 July 2020; Published: 4 August 2020 Abstract: The spread of Unmanned Aerial Vehicles (UAVs) in the last decade revolutionized many applications fields. Most investigated research topics focus on increasing autonomy during operational campaigns, environmental monitoring, surveillance, maps, and labeling. To achieve such complex goals, a high-level module is exploited to build semantic knowledge leveraging the outputs of the low-level module that takes data acquired from multiple sensors and extracts information concerning what is sensed. All in all, the detection of the objects is undoubtedly the most important low-level task, and the most employed sensors to accomplish it are by far RGB cameras due to costs, dimensions, and the wide literature on RGB-based object detection. This survey presents recent advancements in 2D object detection for the case of UAVs, focusing on the differences, strategies, and trade-offs between the generic problem of object detection, and the adaptation of such solutions for operations of the UAV. Moreover, a new taxonomy that considers different heights intervals and driven by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints is proposed. Keywords: computer vision; 2d object detection; unmanned aerial vehicles; deep learning 1. Introduction Unmanned Aerial Vehicles (UAVs), also called Unmanned Aircraft Systems (UASs), and commonly known as drones, are aircraft that fly without a pilot on-board. This places numerous advantages in terms of pilot safety, training, and aircraft costs and sizes, with a huge impact in the range of possible applications. Numbers behind the UAV industry are impressive: Value Market Research estimated that the market for VTOL (Vertical Take-Off and Landing) UAVs will touch around USD 10,163 M by 2024 [1]. Another report from PwC [2] estimates that, in 2030, there will be 76,000 drones operating in the UK skies, involving a total of 628,000 jobs. These forecasts will imply, still in the UK, an increase in GDP of 42 bn\u00a3 and net savings for the UK economy of 16 bn\u00a3. Finally, an EU report of 2016 [3] estimated an economic impact exceeding e10 bn per year within 20 years. As a consequence, both research and industry are investigating the challenges involved in the manufacturing as well as the design of hardware, software, sensors and algorithms to guarantee the UAV operability and to extend its range to unseen scenarios. In fact, UAVs already achieved an unprecedented seen level of growth in many civil and military application domains [4]. UAVs can be remotely controlled by a pilot or can fly autonomously. In the former scenario, the pilot is on land- or sea-based ground control station (GCS) for human control. J. Imaging 2020, 6, 78; doi:10.3390/jimaging6080078 www.mdpi.com/journal/jimaging J. Imaging 2020, 6, 78 2 of 38 The simplest GCS consists of a remote controller with an optional screen, even in the form of a tablet. In the latter scenario, instead, a pre-scheduled flight plan and a dynamic automation system are necessary. The Holy Grail for the involved actors in this revolution is the achievement of fully autonomous operational capabilities to flight over and understand real and complex scenarios. In such an ideal system, a high-level and possibly on-board module is exploited to build semantic knowledge used to reach the application goal. The semantic knowledge leverages low-level software components that extract information concerning what is sensed. Both exteroceptive and proprioceptive sensors are used to obtain situational and self-awareness [5]. From the beginning, UAVs were equipped with sensors such as Global Positioning System (GPS) and Inertial Navigation System (INS) to provide position and orientation in space, but they come with serious drawbacks. The precision of the GPS depends on the general number of available satellites; moreover, urban canyons and indoor navigation can seriously compromise the navigation. INS, instead, suffers from integration drift with acceleration and angular velocity error accumulation, requiring a correction scheme. Presently, the software and hardware advancements in embedded systems and the corresponding miniaturization have led to performing low-cost sensors and Inertial Measurement Units (IMUs) that can extract useful information on-board, such as force, angular rate, and orientation. Many approaches and configurations have been also proposed to get significant knowledge of the environment from data acquired by consumer RGB cameras, depth sensors, LiDAR (Light Detection and Ranging), and event-based cameras. Complex and complete sensor fusion suites that merge multiple data have been introduced too. For each sensor that can be potentially mounted on-board of the UAV, a plethora of works and applications have been proposed. Independently from the employed sensor, where evidently each one comes with own pro and cons, and/or each sensor is better performing in a specific scenario, all of the approaches share the goal of providing meaningful input for the high-level components. Undoubtedly, computer vision can provide a critical contribution to the autonomy of the UAVs and their operational capabilities [6]. Typical UAV operations are surveillance, person tracking, path planning, obstacle avoidance (see Section 2): all of these tasks strongly relies on the detection of one or more domain-related objects. Object detection has then been widely investigated since the beginning of computer vision. Historically, detecting objects in images taken from a camera has represented one of the first computer vision tasks ever: early works are dated to 1960s [7], and a kick-off work that is famous (and considered quite optimistic) in the computer vision community Summer Vision Project is dated 1966 [8]. Henceforth, the possibility of detecting and recognizing classes of objects has been considered to be the first component of any artificial intelligence system, and many proposed theoretical techniques in the computer vision community have been applied to such a task. If many impressive results have been achieved even outperforming human-level performance in object detection, there is still a gap to be filled when the problem is translated to the specific case of aerial robotics. Different challenges are involved in terms of performance, scene, object classes, point of view, perspective, data acquisition, and so on. Moreover, if many datasets are available for object recognition task, they cannot be directly employed in the case of UAVs since the scenes are different from the operational working conditions of the UAV (e.g., indoor, cluttered environments, cameras placed at ground level). Finally, computational constraints and/or communication schemes are an important issue to be addressed. Very precise surveys on the role of computer vision for the autonomous operation of UAV exist. They focus on high-level operations [6,9], they consider only a specific altitude and/or imaging type [10,11], a technology [12], or a precise use-case [5,13\u201316]. It can be observed how, in these very important surveys, the computer vision point of view has been only partially considered. This has been the first motivation behind this work. This work investigates the object detection problem for the specific case of the UAVs from RGB and 2D object detection, introducing also main concepts J. Imaging 2020, 6, 78 3 of 38 and references for mixed sensor suites and/or 3D object detection. The second motivation is in the need for a different categorization of the UAVs: there are many valid classification schemes based on characteristics as the mass, size, mission range, operation heights, level of autonomy, flying principle, operation condition [17]. Anyway, concerning the height, it is common to classify UAVs in intervals of 150\u2013300 m, 3000, 5000 and even 20,000 m, always starting from 50 m. From a computer vision point of view, we think that this scheme is not sufficient to understand the methodologies that are applied to address the object detection problem depending on the operational height. Thus, a conceptual approach that takes into account the operations carried out at very different heights has not been proposed yet. To the best of our knowledge, we present the first work that classifies object recognition methods for the case of UAVs that considers different heights intervals, whose definition is given by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints. At the same time, our proposal is well-integrated with active EU rules and procedures for the operation of unmanned aircraft. Moreover, how specific state of the art deep learning architectures are adapted for the case under consideration is discussed, and the main applications that introduce their own scientific and technological peculiarities are illustrated. Finally, the different datasets specifically designed to evaluate object detection from aerial views are reported and detailed. Summing up, the main contributions of this work are: \u2022 an update of dominant literature aiming at performing object detection from UAV and aerial views; \u2022 a taxonomy for the UAV based on the computer vision point of view, that considers how the same problem can drastically change when it is observed from a different perspective; \u2022 a critical discussion of the actual state of the art, with particular attention to the impact of deep learning. The manuscript is organized as follows: first of all, the role of object detection from UAVs in terms of higher-level operations is illustrated in Section 2, also introducing some important sensors employed in the state of the..",
                  "url": "https://openalex.org/W3047386722",
                  "openalex_id": "https://openalex.org/W3047386722",
                  "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
                  "publication_date": "2020-08-04"
                }
              ]
            }
          ]
        },
        "S5256975047": {
          "id": "S5256975047",
          "text": "The use of event-based cameras in UAVs presents a promising avenue for improving navigation in dynamic environments, which is essential for the fast-paced nature of drone racing.",
          "children": [
            {
              "id": "E6058154070",
              "text": "Arman Asgharpoor Golroudbari\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nUniversity of Tehran\nTehranIran\nMohammad Hossein Sabour sabourmh@ut.ac.ir\nUniversity of Tehran\nTehranIran\nRECENT ADVANCEMENTS IN DEEP LEARNING APPLICATIONS AND METHODS FOR AUTONOMOUS NAVIGATION: A COMPREHENSIVE REVIEW\nDeep Learning \u00b7 Navigation \u00b7 Inertial Sensors, Intelligent Filter \u00b7 Sensor Fusion \u00b7 Long-Short Term Memory \u00b7 Convolutional Neural Network\nThis review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.\nIntroduction\nAutonomous navigation is a critical component of robotics that has transformed numerous application domains, such as medical, industrial, space, and agricultural. By equipping robots with the ability to navigate autonomously, they can efficiently and securely move through dynamic environments without human intervention, expanding their versatility and functionality. To enhance the performance of autonomous navigation systems, researchers have been pushing the technology to its limits, employing state-of-the-art techniques and methodologies.\nGiven the expansive and ever-evolving nature of the literature surrounding autonomous navigation, it is imperative to conduct regular literature surveys in order to remain abreast of the latest advancements. Therefore, the primary objective of this review is to provide a comprehensive and in-depth overview of the current state-of-the-art in autonomous navigation, with a focus on catering to both experienced researchers and novices in the field. Additionally, a terminology section is included to provide clarity and understanding of the technical vocabulary utilized throughout the article.\n\u2022 Mapping: Creating a map of an environment using sensor data and other inputs.\n\u2022 Simultaneous Localization and Mapping (SLAM): Creating a map of an unknown environment while simultaneously localizing the robot or autonomous system within that environment. \u2022 Control: Regulating the motion to follow a desired trajectory and achieve a specific task. \u2022 Obstacle Avoidance: Navigate around obstacles in the path. \u2022 Collision Avoidance: Using sensors and algorithms to detect potential collisions and take action to avoid them. \u2022 Path Planning: Determining a safe and efficient path for an autonomous system to follow. \u2022 Motion Planning: Determining the trajectory to reach a goal while avoiding obstacles and adhering to other constraints. \u2022 Sensor Fusion: Combining data from multiple sensors to obtain a more accurate and comprehensive understanding of the environment. \u2022 Odometry: Using sensory data to estimate the position and orientation by analyzing the movement over time.\n\u2022 Dead Reckoning: Estimates the current position by using its previous position and velocity. Navigation is a critical task for systems that operate in dynamic environments, such as robots, autonomous systems, and unmanned aerial vehicles [1]. It requires the ability to perceive the surroundings, plan a path, execute it, and adapt as needed, all while avoiding obstacles and collisions to ensure safe, efficient, and accurate travel. Recent developments in deep learning have made navigation more reliable, effective, and efficient, enabling its use in a wide range of applications, including transportation, search and rescue, and delivery. Autonomous systems are becoming increasingly prevalent and can determine their actions based on the current situation. These systems have numerous uses, such as self-driving cars [2], drones [3], and search-and-rescue robots [4].\nAutonomous systems fall into two broad categories [5]: reactive and deliberative. Reactive systems, also known as behavior-based systems, are designed to respond to the environment using predefined rules. These systems are typically used in robotics applications, where the environment is relatively stable, and the system has a specific task, such as pick and place. On the other hand, deliberative systems are designed to plan and execute a path to a destination. These systems are commonly used in transportation, where the environment is complex, and the system must navigate efficiently and safely.\nThe deliberative systems can be further classified into two categories [6]: (1) model-based and (2) model-free systems. Model-based systems use a mathematical model of the environment to plan a path, such as using dynamic programming or graph search algorithms [7]. Model-free systems, also known as model-agnostic systems, do not rely on a model of the environment to plan a path [8]. Instead, these systems use techniques such as Reinforcement Learning [9] or Apprenticeship Learning [10] to learn the optimal policy for navigation. The choice of autonomous system, whether reactive or deliberative and model-based or model-free, depends on the specific requirements and constraints of the task.\nAutonomous systems require effective and successful navigation to operate in dynamic environments without human intervention and guidance. This involves integrating various technologies, including sensors, actuators, and control systems. To improve the accuracy, efficiency, and robustness of navigation algorithms, deep learning has been applied to various navigation tasks such as perception and planning. Deep learning's ability to learn complex representations from vast amounts of data is well-suited for these tasks, such as image analysis and integrating multiple sources of information, such as speech and text. However, deep learning-based navigation systems must overcome challenges such as limited data, reliability, ethical concerns, such as privacy and bias. Techniques such as transfer learning, multi-modal fusion, model uncertainty estimation, and safety-critical architectures can address these challenges. Additionally, differential privacy and fairness-aware machine learning techniques can address privacy and bias concerns, respectively.\nThe application of deep learning in navigation has seen a rise in recent times, as evidenced by numerous studies and surveys (refer to Tables 1 and 2). Although deep learning holds great promise in enhancing navigation systems, it is crucial to tackle the challenges and ethical considerations that come with its usage. Furthermore, there is a need for further exploration on how deep learning techniques can be integrated with conventional navigation methods.\nNumerous surveys have been conducted on the applications of deep learning in various navigation domains, including urban navigation [11], visual navigation [12,13], reinforcement learning [14,9], obstacle detection [15], and spacecraft navigation [16,17]. However, there is a lack of comprehensive surveys that provide a general overview of the use of deep learning in navigation. This survey aims to fill this gap by presenting a comprehensive overview of the applications of deep learning in navigation. The paper is structured as follows: Section 2 provides an overview of deep learning and its methods, while Section 3 discusses the different activation functions used in deep learning. Section 4 presents an overview of navigation, autonomy, and autonomous navigation. Section 5 discusses the applications of deep learning in navigation, and Section 6 delves into the various components of deep learning in autonomous navigation, such as perception, localization, mapping, planning, and control. Finally, Section 7 concludes the paper and highlights future directions. Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization [18] Multi-robot Navigation\nHINNet: Inertial navigation with head-mounted sensors using a neural network [19] Inertial Navigation Multi-sensor integrated navigation/positioning systems using data fusion: From analytics-based to learning-based approaches [20] Integrated Navigation Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction [21] Visual Navigation Goal-guided Transformer-enabled Reinforcement Learning for Efficient Autonomous Navigation [22] Autonomous Navigation DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments [23] Visual Navigation Monocular vision with deep neural networks for autonomous mobile robots navigation [24] Visual Navigation URWalking: Indoor Navigation for Research and Daily Use [25] Indoor Navigation A Simple Self-Supervised IMU Denoising Method For Inertial Aided Navigation [26] Inertial Na",
              "url": "https://openalex.org/W4381333019",
              "openalex_id": "https://openalex.org/W4381333019",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-20"
            },
            {
              "id": "E7895996755",
              "text": "Journal of Imaging Review A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles Dario Cazzato 1,\u2217 , Claudio Cimarelli 1, Jose Luis Sanchez-Lopez 1, Holger Voos 1 and Marco Leo 2 1 Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, 1855 Luxembourg, Luxembourg; claudio.cimarelli@uni.lu (C.C.); joseluis.sanchezlopez@uni.lu (J.L.S.-L.); holger.voos@uni.lu (H.V.) 2 Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, 73100 Lecce, Italy; marco.leo@cnr.it * Correspondence: dario.cazzato@uni.lu Received: 29 June 2020; Accepted: 31 July 2020; Published: 4 August 2020 Abstract: The spread of Unmanned Aerial Vehicles (UAVs) in the last decade revolutionized many applications fields. Most investigated research topics focus on increasing autonomy during operational campaigns, environmental monitoring, surveillance, maps, and labeling. To achieve such complex goals, a high-level module is exploited to build semantic knowledge leveraging the outputs of the low-level module that takes data acquired from multiple sensors and extracts information concerning what is sensed. All in all, the detection of the objects is undoubtedly the most important low-level task, and the most employed sensors to accomplish it are by far RGB cameras due to costs, dimensions, and the wide literature on RGB-based object detection. This survey presents recent advancements in 2D object detection for the case of UAVs, focusing on the differences, strategies, and trade-offs between the generic problem of object detection, and the adaptation of such solutions for operations of the UAV. Moreover, a new taxonomy that considers different heights intervals and driven by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints is proposed. Keywords: computer vision; 2d object detection; unmanned aerial vehicles; deep learning 1. Introduction Unmanned Aerial Vehicles (UAVs), also called Unmanned Aircraft Systems (UASs), and commonly known as drones, are aircraft that fly without a pilot on-board. This places numerous advantages in terms of pilot safety, training, and aircraft costs and sizes, with a huge impact in the range of possible applications. Numbers behind the UAV industry are impressive: Value Market Research estimated that the market for VTOL (Vertical Take-Off and Landing) UAVs will touch around USD 10,163 M by 2024 [1]. Another report from PwC [2] estimates that, in 2030, there will be 76,000 drones operating in the UK skies, involving a total of 628,000 jobs. These forecasts will imply, still in the UK, an increase in GDP of 42 bn\u00a3 and net savings for the UK economy of 16 bn\u00a3. Finally, an EU report of 2016 [3] estimated an economic impact exceeding e10 bn per year within 20 years. As a consequence, both research and industry are investigating the challenges involved in the manufacturing as well as the design of hardware, software, sensors and algorithms to guarantee the UAV operability and to extend its range to unseen scenarios. In fact, UAVs already achieved an unprecedented seen level of growth in many civil and military application domains [4]. UAVs can be remotely controlled by a pilot or can fly autonomously. In the former scenario, the pilot is on land- or sea-based ground control station (GCS) for human control. J. Imaging 2020, 6, 78; doi:10.3390/jimaging6080078 www.mdpi.com/journal/jimaging J. Imaging 2020, 6, 78 2 of 38 The simplest GCS consists of a remote controller with an optional screen, even in the form of a tablet. In the latter scenario, instead, a pre-scheduled flight plan and a dynamic automation system are necessary. The Holy Grail for the involved actors in this revolution is the achievement of fully autonomous operational capabilities to flight over and understand real and complex scenarios. In such an ideal system, a high-level and possibly on-board module is exploited to build semantic knowledge used to reach the application goal. The semantic knowledge leverages low-level software components that extract information concerning what is sensed. Both exteroceptive and proprioceptive sensors are used to obtain situational and self-awareness [5]. From the beginning, UAVs were equipped with sensors such as Global Positioning System (GPS) and Inertial Navigation System (INS) to provide position and orientation in space, but they come with serious drawbacks. The precision of the GPS depends on the general number of available satellites; moreover, urban canyons and indoor navigation can seriously compromise the navigation. INS, instead, suffers from integration drift with acceleration and angular velocity error accumulation, requiring a correction scheme. Presently, the software and hardware advancements in embedded systems and the corresponding miniaturization have led to performing low-cost sensors and Inertial Measurement Units (IMUs) that can extract useful information on-board, such as force, angular rate, and orientation. Many approaches and configurations have been also proposed to get significant knowledge of the environment from data acquired by consumer RGB cameras, depth sensors, LiDAR (Light Detection and Ranging), and event-based cameras. Complex and complete sensor fusion suites that merge multiple data have been introduced too. For each sensor that can be potentially mounted on-board of the UAV, a plethora of works and applications have been proposed. Independently from the employed sensor, where evidently each one comes with own pro and cons, and/or each sensor is better performing in a specific scenario, all of the approaches share the goal of providing meaningful input for the high-level components. Undoubtedly, computer vision can provide a critical contribution to the autonomy of the UAVs and their operational capabilities [6]. Typical UAV operations are surveillance, person tracking, path planning, obstacle avoidance (see Section 2): all of these tasks strongly relies on the detection of one or more domain-related objects. Object detection has then been widely investigated since the beginning of computer vision. Historically, detecting objects in images taken from a camera has represented one of the first computer vision tasks ever: early works are dated to 1960s [7], and a kick-off work that is famous (and considered quite optimistic) in the computer vision community Summer Vision Project is dated 1966 [8]. Henceforth, the possibility of detecting and recognizing classes of objects has been considered to be the first component of any artificial intelligence system, and many proposed theoretical techniques in the computer vision community have been applied to such a task. If many impressive results have been achieved even outperforming human-level performance in object detection, there is still a gap to be filled when the problem is translated to the specific case of aerial robotics. Different challenges are involved in terms of performance, scene, object classes, point of view, perspective, data acquisition, and so on. Moreover, if many datasets are available for object recognition task, they cannot be directly employed in the case of UAVs since the scenes are different from the operational working conditions of the UAV (e.g., indoor, cluttered environments, cameras placed at ground level). Finally, computational constraints and/or communication schemes are an important issue to be addressed. Very precise surveys on the role of computer vision for the autonomous operation of UAV exist. They focus on high-level operations [6,9], they consider only a specific altitude and/or imaging type [10,11], a technology [12], or a precise use-case [5,13\u201316]. It can be observed how, in these very important surveys, the computer vision point of view has been only partially considered. This has been the first motivation behind this work. This work investigates the object detection problem for the specific case of the UAVs from RGB and 2D object detection, introducing also main concepts J. Imaging 2020, 6, 78 3 of 38 and references for mixed sensor suites and/or 3D object detection. The second motivation is in the need for a different categorization of the UAVs: there are many valid classification schemes based on characteristics as the mass, size, mission range, operation heights, level of autonomy, flying principle, operation condition [17]. Anyway, concerning the height, it is common to classify UAVs in intervals of 150\u2013300 m, 3000, 5000 and even 20,000 m, always starting from 50 m. From a computer vision point of view, we think that this scheme is not sufficient to understand the methodologies that are applied to address the object detection problem depending on the operational height. Thus, a conceptual approach that takes into account the operations carried out at very different heights has not been proposed yet. To the best of our knowledge, we present the first work that classifies object recognition methods for the case of UAVs that considers different heights intervals, whose definition is given by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints. At the same time, our proposal is well-integrated with active EU rules and procedures for the operation of unmanned aircraft. Moreover, how specific state of the art deep learning architectures are adapted for the case under consideration is discussed, and the main applications that introduce their own scientific and technological peculiarities are illustrated. Finally, the different datasets specifically designed to evaluate object detection from aerial views are reported and detailed. Summing up, the main contributions of this work are: \u2022 an update of dominant literature aiming at performing object detection from UAV and aerial views; \u2022 a taxonomy for the UAV based on the computer vision point of view, that considers how the same problem can drastically change when it is observed from a different perspective; \u2022 a critical discussion of the actual state of the art, with particular attention to the impact of deep learning. The manuscript is organized as follows: first of all, the role of object detection from UAVs in terms of higher-level operations is illustrated in Section 2, also introducing some important sensors employed in the state of the..",
              "url": "https://openalex.org/W3047386722",
              "openalex_id": "https://openalex.org/W3047386722",
              "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
              "publication_date": "2020-08-04"
            }
          ]
        },
        "S9703836577": {
          "id": "S9703836577",
          "text": "The integration of multi-sensor fusion techniques, including RGB cameras and LiDAR, can enhance situational awareness and obstacle detection, which are critical for achieving high-speed navigation in drone racing.",
          "children": [
            {
              "id": "E7895996755",
              "text": "Journal of Imaging Review A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles Dario Cazzato 1,\u2217 , Claudio Cimarelli 1, Jose Luis Sanchez-Lopez 1, Holger Voos 1 and Marco Leo 2 1 Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, 1855 Luxembourg, Luxembourg; claudio.cimarelli@uni.lu (C.C.); joseluis.sanchezlopez@uni.lu (J.L.S.-L.); holger.voos@uni.lu (H.V.) 2 Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, 73100 Lecce, Italy; marco.leo@cnr.it * Correspondence: dario.cazzato@uni.lu Received: 29 June 2020; Accepted: 31 July 2020; Published: 4 August 2020 Abstract: The spread of Unmanned Aerial Vehicles (UAVs) in the last decade revolutionized many applications fields. Most investigated research topics focus on increasing autonomy during operational campaigns, environmental monitoring, surveillance, maps, and labeling. To achieve such complex goals, a high-level module is exploited to build semantic knowledge leveraging the outputs of the low-level module that takes data acquired from multiple sensors and extracts information concerning what is sensed. All in all, the detection of the objects is undoubtedly the most important low-level task, and the most employed sensors to accomplish it are by far RGB cameras due to costs, dimensions, and the wide literature on RGB-based object detection. This survey presents recent advancements in 2D object detection for the case of UAVs, focusing on the differences, strategies, and trade-offs between the generic problem of object detection, and the adaptation of such solutions for operations of the UAV. Moreover, a new taxonomy that considers different heights intervals and driven by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints is proposed. Keywords: computer vision; 2d object detection; unmanned aerial vehicles; deep learning 1. Introduction Unmanned Aerial Vehicles (UAVs), also called Unmanned Aircraft Systems (UASs), and commonly known as drones, are aircraft that fly without a pilot on-board. This places numerous advantages in terms of pilot safety, training, and aircraft costs and sizes, with a huge impact in the range of possible applications. Numbers behind the UAV industry are impressive: Value Market Research estimated that the market for VTOL (Vertical Take-Off and Landing) UAVs will touch around USD 10,163 M by 2024 [1]. Another report from PwC [2] estimates that, in 2030, there will be 76,000 drones operating in the UK skies, involving a total of 628,000 jobs. These forecasts will imply, still in the UK, an increase in GDP of 42 bn\u00a3 and net savings for the UK economy of 16 bn\u00a3. Finally, an EU report of 2016 [3] estimated an economic impact exceeding e10 bn per year within 20 years. As a consequence, both research and industry are investigating the challenges involved in the manufacturing as well as the design of hardware, software, sensors and algorithms to guarantee the UAV operability and to extend its range to unseen scenarios. In fact, UAVs already achieved an unprecedented seen level of growth in many civil and military application domains [4]. UAVs can be remotely controlled by a pilot or can fly autonomously. In the former scenario, the pilot is on land- or sea-based ground control station (GCS) for human control. J. Imaging 2020, 6, 78; doi:10.3390/jimaging6080078 www.mdpi.com/journal/jimaging J. Imaging 2020, 6, 78 2 of 38 The simplest GCS consists of a remote controller with an optional screen, even in the form of a tablet. In the latter scenario, instead, a pre-scheduled flight plan and a dynamic automation system are necessary. The Holy Grail for the involved actors in this revolution is the achievement of fully autonomous operational capabilities to flight over and understand real and complex scenarios. In such an ideal system, a high-level and possibly on-board module is exploited to build semantic knowledge used to reach the application goal. The semantic knowledge leverages low-level software components that extract information concerning what is sensed. Both exteroceptive and proprioceptive sensors are used to obtain situational and self-awareness [5]. From the beginning, UAVs were equipped with sensors such as Global Positioning System (GPS) and Inertial Navigation System (INS) to provide position and orientation in space, but they come with serious drawbacks. The precision of the GPS depends on the general number of available satellites; moreover, urban canyons and indoor navigation can seriously compromise the navigation. INS, instead, suffers from integration drift with acceleration and angular velocity error accumulation, requiring a correction scheme. Presently, the software and hardware advancements in embedded systems and the corresponding miniaturization have led to performing low-cost sensors and Inertial Measurement Units (IMUs) that can extract useful information on-board, such as force, angular rate, and orientation. Many approaches and configurations have been also proposed to get significant knowledge of the environment from data acquired by consumer RGB cameras, depth sensors, LiDAR (Light Detection and Ranging), and event-based cameras. Complex and complete sensor fusion suites that merge multiple data have been introduced too. For each sensor that can be potentially mounted on-board of the UAV, a plethora of works and applications have been proposed. Independently from the employed sensor, where evidently each one comes with own pro and cons, and/or each sensor is better performing in a specific scenario, all of the approaches share the goal of providing meaningful input for the high-level components. Undoubtedly, computer vision can provide a critical contribution to the autonomy of the UAVs and their operational capabilities [6]. Typical UAV operations are surveillance, person tracking, path planning, obstacle avoidance (see Section 2): all of these tasks strongly relies on the detection of one or more domain-related objects. Object detection has then been widely investigated since the beginning of computer vision. Historically, detecting objects in images taken from a camera has represented one of the first computer vision tasks ever: early works are dated to 1960s [7], and a kick-off work that is famous (and considered quite optimistic) in the computer vision community Summer Vision Project is dated 1966 [8]. Henceforth, the possibility of detecting and recognizing classes of objects has been considered to be the first component of any artificial intelligence system, and many proposed theoretical techniques in the computer vision community have been applied to such a task. If many impressive results have been achieved even outperforming human-level performance in object detection, there is still a gap to be filled when the problem is translated to the specific case of aerial robotics. Different challenges are involved in terms of performance, scene, object classes, point of view, perspective, data acquisition, and so on. Moreover, if many datasets are available for object recognition task, they cannot be directly employed in the case of UAVs since the scenes are different from the operational working conditions of the UAV (e.g., indoor, cluttered environments, cameras placed at ground level). Finally, computational constraints and/or communication schemes are an important issue to be addressed. Very precise surveys on the role of computer vision for the autonomous operation of UAV exist. They focus on high-level operations [6,9], they consider only a specific altitude and/or imaging type [10,11], a technology [12], or a precise use-case [5,13\u201316]. It can be observed how, in these very important surveys, the computer vision point of view has been only partially considered. This has been the first motivation behind this work. This work investigates the object detection problem for the specific case of the UAVs from RGB and 2D object detection, introducing also main concepts J. Imaging 2020, 6, 78 3 of 38 and references for mixed sensor suites and/or 3D object detection. The second motivation is in the need for a different categorization of the UAVs: there are many valid classification schemes based on characteristics as the mass, size, mission range, operation heights, level of autonomy, flying principle, operation condition [17]. Anyway, concerning the height, it is common to classify UAVs in intervals of 150\u2013300 m, 3000, 5000 and even 20,000 m, always starting from 50 m. From a computer vision point of view, we think that this scheme is not sufficient to understand the methodologies that are applied to address the object detection problem depending on the operational height. Thus, a conceptual approach that takes into account the operations carried out at very different heights has not been proposed yet. To the best of our knowledge, we present the first work that classifies object recognition methods for the case of UAVs that considers different heights intervals, whose definition is given by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints. At the same time, our proposal is well-integrated with active EU rules and procedures for the operation of unmanned aircraft. Moreover, how specific state of the art deep learning architectures are adapted for the case under consideration is discussed, and the main applications that introduce their own scientific and technological peculiarities are illustrated. Finally, the different datasets specifically designed to evaluate object detection from aerial views are reported and detailed. Summing up, the main contributions of this work are: \u2022 an update of dominant literature aiming at performing object detection from UAV and aerial views; \u2022 a taxonomy for the UAV based on the computer vision point of view, that considers how the same problem can drastically change when it is observed from a different perspective; \u2022 a critical discussion of the actual state of the art, with particular attention to the impact of deep learning. The manuscript is organized as follows: first of all, the role of object detection from UAVs in terms of higher-level operations is illustrated in Section 2, also introducing some important sensors employed in the state of the..",
              "url": "https://openalex.org/W3047386722",
              "openalex_id": "https://openalex.org/W3047386722",
              "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
              "publication_date": "2020-08-04"
            },
            {
              "id": "E8904502138",
              "text": ".., J.; Carlone, L. 3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans. arXiv 2020, arXiv:2002.06289. 38. Bavle, H.; De La Puente, P.; How, J.P.; Campoy, P. VPS-SLAM: Visual Planar Semantic SLAM for Aerial Robotic Systems. IEEE Access 2020, 8, 60704\u201360718. [CrossRef] 39. Zhang, L.; Wei, L.; Shen, P.; Wei, W.; Zhu, G.; Song, J. Semantic SLAM based on object detection and improved octomap. IEEE Access 2018, 6, 75545\u201375559. [CrossRef] 40. Manzoor, S.; Joo, S.H.; Rocha, Y.G.; Lee, H.U.; Kuc, T.Y. A Novel Semantic SLAM Framework for Humanlike High-Level Interaction and Planning in Global Environment. In Proceedings of the 1st International Workshop on the Semantic Descriptor, Semantic Modeling and Mapping for Humanlike Perception and Navigation of Mobile Robots toward Large Scale Long-Term Autonomy (SDMM1), Macau, China, 4\u20138 November 2019. 41. Sanchez-Lopez, J.L.; Sampedro, C.; Cazzato, D.; Voos, H. Deep learning based semantic situation awareness system for multirotor aerial robots using LIDAR. In Proceedings of the 2019 International Conference on Unmanned Aircraft Systems (ICUAS), Atlanta, GA, USA, 11\u201314 June 2019; pp. 899\u2013908. 42. Sanchez-Lopez, J.L.; Castillo-Lopez, M.; Voos, H. Semantic situation awareness of ellipse shapes via deep learning for multirotor aerial robots with a 2D LIDAR. In Proceedings of the 2020 International Conference on Unmanned Aircraft Systems (ICUAS), Athens, Greece, 9\u201312 June 2020; pp. 1\u201310. 43. Lin, Y.; Saripalli, S. Sampling-based path planning for UAV collision avoidance. IEEE Trans. Intell. Transp. Syst. 2017, 18, 3179\u20133192. [CrossRef] 44. Sanchez-Lopez, J.L.; Wang, M.; Olivares-Mendez, M.A.; Molina, M.; Voos, H. A real-time 3d path planning solution for collision-free navigation of multirotor aerial robots in dynamic environments. J. Intell. Robot. Syst. 2019, 93, 33\u201353. [CrossRef] 45. Castillo-Lopez, M.; Ludivig, P.; Sajadi-Alamdari, S.A.; Sanchez-Lopez, J.L.; Olivares-Mendez, M.A.; Voos, H. A Real-Time Approach for Chance-Constrained Motion Planning With Dynamic Obstacles. IEEE Robot. Autom. Lett. 2020, 5, 3620\u20133625. [CrossRef] 46. Pestana, J.; Sanchez-Lopez, J.L.; Saripalli, S.; Campoy, P. Computer vision based general object following for gps-denied multirotor unmanned vehicles. In Proceedings of the 2014 American Control Conference, Portland, OR, USA, 4\u20136 June 2014; pp. 1886\u20131891. 47. Pestana, J.; Sanchez-Lopez, J.L.; Campoy, P.; Saripalli, S. Vision based gps-denied object tracking and following for unmanned aerial vehicles. In Proceedings of the 2013 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR), Link\u00f6ping, Sweden, 21\u201326 October 2013; pp. 1\u20136. 48. Koga, Y.; Miyazaki, H.; Shibasaki, R. A Method for Vehicle Detection in High-Resolution Satellite Images that Uses a Region-Based Object Detector and Unsupervised Domain Adaptation. Remote Sens. 2020, 12, 575. [CrossRef] 49. Redding, J.D.; McLain, T.W.; Beard, R.W.; Taylor, C.N. Vision-based target localization from a fixed-wing miniature air vehicle. In Proceedings of the 2006 American Control Conference, Minneapolis, MN, USA, 14\u201316 June 2006; p. 6. 50. Semsch, E.; Jakob, M.; Pavlicek, D.; Pechoucek, M. Autonomous UAV surveillance in complex urban environments. In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, Milan, Italy, 15\u201318 September 2009; Volume 2, pp. 82\u201385. 51. Nikolic, J.; Burri, M.; Rehder, J.; Leutenegger, S.; Huerzeler, C.; Siegwart, R. A UAV system for inspection of industrial facilities. In Proceedings of the 2013 IEEE Aerospace Conference, Big Sky, MT, USA, 3\u201310 March 2013; pp. 1\u20138. 52. Sankey, T.; Donager, J.; McVay, J.; Sankey, J.B. UAV lidar and hyperspectral fusion for forest monitoring in the southwestern USA. Remote Sens. Environ. 2017, 195, 30\u201343. [CrossRef] J. Imaging 2020, 6, 78 31 of 38 53. Cazzato, D.; Olivares-Mendez, M.A.; Sanchez-Lopez, J.L.; Voos, H. Vision-Based Aircraft Pose Estimation for UAVs Autonomous Inspection without Fiducial Markers. In Proceedings of the IECON 2019\u201445th Annual Conference of the IEEE Industrial Electronics Society, Lisbon, Portugal, 14\u201317 October 2019; Volume 1, pp. 5642\u20135648. 54. Andreopoulos, A.; Tsotsos, J.K. 50 years of object recognition: Directions forward. Comput. Vis. Image Underst. 2013, 117, 827\u2013891. [CrossRef] 55. Liu, L.; Ouyang, W.; Wang, X.; Fieguth, P.; Chen, J.; Liu, X.; Pietik\u00e4inen, M. Deep learning for generic object detection: A survey. Int. J. Comput. Vis. 2020, 128, 261\u2013318. [CrossRef] 56. Ponce, J.; Hebert, M.; Schmid, C.; Zisserman, A. Toward Category-Level Object Recognition; Springer: Cham, Switzerland, 2007; Volume 4170. 57. Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; et al. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. 2015, 115, 211\u2013252. [CrossRef] 58. Wang, R.; Xu, J.; Han, T.X. Object instance detection with pruned Alexnet and extended training data. Signal Process. Image Commun. 2019, 70, 145\u2013156. [CrossRef] 59. Viola, P.; Jones, M. Rapid object detection using a boosted cascade of simple features. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Kauai, HI, USA, 8\u201314 December 2001; Volume 1, p. I-I. 60. Dalal, N.; Triggs, B. Histograms of oriented gradients for human detection. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), San Diego, CA, USA, 20\u201325 June 2005; Volume 1, pp. 886\u2013893. 61. Rublee, E.; Rabaud, V.; Konolige, K.; Bradski, G. ORB: An efficient alternative to SIFT or SURF. In Proceedings of the 2011 International Conference on Computer Vision, Barcelona, Spain, 6\u201313 November 2011; pp. 2564\u20132571. 62. Zou, Z.; Shi, Z.; Guo, Y.; Ye, J. Object detection in 20 years: A survey. arXiv 2019, arXiv:1905.05055. 63. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems; Neural Information Processing Systems Foundation, Inc.: San Diego, CA, USA, 2012; pp. 1097\u20131105. 64. Deng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20\u201325 June 2009; pp. 248\u2013255. 65. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA, 24\u201327 June 2014; pp. 580\u2013587. 66. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems; Neural Information Processing Systems Foundation, Inc.: San Diego, CA, USA, 2015; pp. 91\u201399. 67. He, K.; Gkioxari, G.; Doll\u00e1r, P.; Girshick,..",
              "url": "https://openalex.org/W3047386722",
              "openalex_id": "https://openalex.org/W3047386722",
              "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
              "publication_date": "2020-08-04"
            }
          ]
        },
        "S3140539497": {
          "id": "S3140539497",
          "text": "Challenges in high-speed navigation for autonomous drones include the need for rapid processing of high-resolution images to detect and track objects, which can be addressed by optimizing deep learning models for efficiency.",
          "children": [
            {
              "id": "E6058154070",
              "text": "Arman Asgharpoor Golroudbari\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nUniversity of Tehran\nTehranIran\nMohammad Hossein Sabour sabourmh@ut.ac.ir\nUniversity of Tehran\nTehranIran\nRECENT ADVANCEMENTS IN DEEP LEARNING APPLICATIONS AND METHODS FOR AUTONOMOUS NAVIGATION: A COMPREHENSIVE REVIEW\nDeep Learning \u00b7 Navigation \u00b7 Inertial Sensors, Intelligent Filter \u00b7 Sensor Fusion \u00b7 Long-Short Term Memory \u00b7 Convolutional Neural Network\nThis review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.\nIntroduction\nAutonomous navigation is a critical component of robotics that has transformed numerous application domains, such as medical, industrial, space, and agricultural. By equipping robots with the ability to navigate autonomously, they can efficiently and securely move through dynamic environments without human intervention, expanding their versatility and functionality. To enhance the performance of autonomous navigation systems, researchers have been pushing the technology to its limits, employing state-of-the-art techniques and methodologies.\nGiven the expansive and ever-evolving nature of the literature surrounding autonomous navigation, it is imperative to conduct regular literature surveys in order to remain abreast of the latest advancements. Therefore, the primary objective of this review is to provide a comprehensive and in-depth overview of the current state-of-the-art in autonomous navigation, with a focus on catering to both experienced researchers and novices in the field. Additionally, a terminology section is included to provide clarity and understanding of the technical vocabulary utilized throughout the article.\n\u2022 Mapping: Creating a map of an environment using sensor data and other inputs.\n\u2022 Simultaneous Localization and Mapping (SLAM): Creating a map of an unknown environment while simultaneously localizing the robot or autonomous system within that environment. \u2022 Control: Regulating the motion to follow a desired trajectory and achieve a specific task. \u2022 Obstacle Avoidance: Navigate around obstacles in the path. \u2022 Collision Avoidance: Using sensors and algorithms to detect potential collisions and take action to avoid them. \u2022 Path Planning: Determining a safe and efficient path for an autonomous system to follow. \u2022 Motion Planning: Determining the trajectory to reach a goal while avoiding obstacles and adhering to other constraints. \u2022 Sensor Fusion: Combining data from multiple sensors to obtain a more accurate and comprehensive understanding of the environment. \u2022 Odometry: Using sensory data to estimate the position and orientation by analyzing the movement over time.\n\u2022 Dead Reckoning: Estimates the current position by using its previous position and velocity. Navigation is a critical task for systems that operate in dynamic environments, such as robots, autonomous systems, and unmanned aerial vehicles [1]. It requires the ability to perceive the surroundings, plan a path, execute it, and adapt as needed, all while avoiding obstacles and collisions to ensure safe, efficient, and accurate travel. Recent developments in deep learning have made navigation more reliable, effective, and efficient, enabling its use in a wide range of applications, including transportation, search and rescue, and delivery. Autonomous systems are becoming increasingly prevalent and can determine their actions based on the current situation. These systems have numerous uses, such as self-driving cars [2], drones [3], and search-and-rescue robots [4].\nAutonomous systems fall into two broad categories [5]: reactive and deliberative. Reactive systems, also known as behavior-based systems, are designed to respond to the environment using predefined rules. These systems are typically used in robotics applications, where the environment is relatively stable, and the system has a specific task, such as pick and place. On the other hand, deliberative systems are designed to plan and execute a path to a destination. These systems are commonly used in transportation, where the environment is complex, and the system must navigate efficiently and safely.\nThe deliberative systems can be further classified into two categories [6]: (1) model-based and (2) model-free systems. Model-based systems use a mathematical model of the environment to plan a path, such as using dynamic programming or graph search algorithms [7]. Model-free systems, also known as model-agnostic systems, do not rely on a model of the environment to plan a path [8]. Instead, these systems use techniques such as Reinforcement Learning [9] or Apprenticeship Learning [10] to learn the optimal policy for navigation. The choice of autonomous system, whether reactive or deliberative and model-based or model-free, depends on the specific requirements and constraints of the task.\nAutonomous systems require effective and successful navigation to operate in dynamic environments without human intervention and guidance. This involves integrating various technologies, including sensors, actuators, and control systems. To improve the accuracy, efficiency, and robustness of navigation algorithms, deep learning has been applied to various navigation tasks such as perception and planning. Deep learning's ability to learn complex representations from vast amounts of data is well-suited for these tasks, such as image analysis and integrating multiple sources of information, such as speech and text. However, deep learning-based navigation systems must overcome challenges such as limited data, reliability, ethical concerns, such as privacy and bias. Techniques such as transfer learning, multi-modal fusion, model uncertainty estimation, and safety-critical architectures can address these challenges. Additionally, differential privacy and fairness-aware machine learning techniques can address privacy and bias concerns, respectively.\nThe application of deep learning in navigation has seen a rise in recent times, as evidenced by numerous studies and surveys (refer to Tables 1 and 2). Although deep learning holds great promise in enhancing navigation systems, it is crucial to tackle the challenges and ethical considerations that come with its usage. Furthermore, there is a need for further exploration on how deep learning techniques can be integrated with conventional navigation methods.\nNumerous surveys have been conducted on the applications of deep learning in various navigation domains, including urban navigation [11], visual navigation [12,13], reinforcement learning [14,9], obstacle detection [15], and spacecraft navigation [16,17]. However, there is a lack of comprehensive surveys that provide a general overview of the use of deep learning in navigation. This survey aims to fill this gap by presenting a comprehensive overview of the applications of deep learning in navigation. The paper is structured as follows: Section 2 provides an overview of deep learning and its methods, while Section 3 discusses the different activation functions used in deep learning. Section 4 presents an overview of navigation, autonomy, and autonomous navigation. Section 5 discusses the applications of deep learning in navigation, and Section 6 delves into the various components of deep learning in autonomous navigation, such as perception, localization, mapping, planning, and control. Finally, Section 7 concludes the paper and highlights future directions. Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization [18] Multi-robot Navigation\nHINNet: Inertial navigation with head-mounted sensors using a neural network [19] Inertial Navigation Multi-sensor integrated navigation/positioning systems using data fusion: From analytics-based to learning-based approaches [20] Integrated Navigation Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction [21] Visual Navigation Goal-guided Transformer-enabled Reinforcement Learning for Efficient Autonomous Navigation [22] Autonomous Navigation DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments [23] Visual Navigation Monocular vision with deep neural networks for autonomous mobile robots navigation [24] Visual Navigation URWalking: Indoor Navigation for Research and Daily Use [25] Indoor Navigation A Simple Self-Supervised IMU Denoising Method For Inertial Aided Navigation [26] Inertial Na",
              "url": "https://openalex.org/W4381333019",
              "openalex_id": "https://openalex.org/W4381333019",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-20"
            },
            {
              "id": "E7895996755",
              "text": "Journal of Imaging Review A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles Dario Cazzato 1,\u2217 , Claudio Cimarelli 1, Jose Luis Sanchez-Lopez 1, Holger Voos 1 and Marco Leo 2 1 Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, 1855 Luxembourg, Luxembourg; claudio.cimarelli@uni.lu (C.C.); joseluis.sanchezlopez@uni.lu (J.L.S.-L.); holger.voos@uni.lu (H.V.) 2 Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, 73100 Lecce, Italy; marco.leo@cnr.it * Correspondence: dario.cazzato@uni.lu Received: 29 June 2020; Accepted: 31 July 2020; Published: 4 August 2020 Abstract: The spread of Unmanned Aerial Vehicles (UAVs) in the last decade revolutionized many applications fields. Most investigated research topics focus on increasing autonomy during operational campaigns, environmental monitoring, surveillance, maps, and labeling. To achieve such complex goals, a high-level module is exploited to build semantic knowledge leveraging the outputs of the low-level module that takes data acquired from multiple sensors and extracts information concerning what is sensed. All in all, the detection of the objects is undoubtedly the most important low-level task, and the most employed sensors to accomplish it are by far RGB cameras due to costs, dimensions, and the wide literature on RGB-based object detection. This survey presents recent advancements in 2D object detection for the case of UAVs, focusing on the differences, strategies, and trade-offs between the generic problem of object detection, and the adaptation of such solutions for operations of the UAV. Moreover, a new taxonomy that considers different heights intervals and driven by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints is proposed. Keywords: computer vision; 2d object detection; unmanned aerial vehicles; deep learning 1. Introduction Unmanned Aerial Vehicles (UAVs), also called Unmanned Aircraft Systems (UASs), and commonly known as drones, are aircraft that fly without a pilot on-board. This places numerous advantages in terms of pilot safety, training, and aircraft costs and sizes, with a huge impact in the range of possible applications. Numbers behind the UAV industry are impressive: Value Market Research estimated that the market for VTOL (Vertical Take-Off and Landing) UAVs will touch around USD 10,163 M by 2024 [1]. Another report from PwC [2] estimates that, in 2030, there will be 76,000 drones operating in the UK skies, involving a total of 628,000 jobs. These forecasts will imply, still in the UK, an increase in GDP of 42 bn\u00a3 and net savings for the UK economy of 16 bn\u00a3. Finally, an EU report of 2016 [3] estimated an economic impact exceeding e10 bn per year within 20 years. As a consequence, both research and industry are investigating the challenges involved in the manufacturing as well as the design of hardware, software, sensors and algorithms to guarantee the UAV operability and to extend its range to unseen scenarios. In fact, UAVs already achieved an unprecedented seen level of growth in many civil and military application domains [4]. UAVs can be remotely controlled by a pilot or can fly autonomously. In the former scenario, the pilot is on land- or sea-based ground control station (GCS) for human control. J. Imaging 2020, 6, 78; doi:10.3390/jimaging6080078 www.mdpi.com/journal/jimaging J. Imaging 2020, 6, 78 2 of 38 The simplest GCS consists of a remote controller with an optional screen, even in the form of a tablet. In the latter scenario, instead, a pre-scheduled flight plan and a dynamic automation system are necessary. The Holy Grail for the involved actors in this revolution is the achievement of fully autonomous operational capabilities to flight over and understand real and complex scenarios. In such an ideal system, a high-level and possibly on-board module is exploited to build semantic knowledge used to reach the application goal. The semantic knowledge leverages low-level software components that extract information concerning what is sensed. Both exteroceptive and proprioceptive sensors are used to obtain situational and self-awareness [5]. From the beginning, UAVs were equipped with sensors such as Global Positioning System (GPS) and Inertial Navigation System (INS) to provide position and orientation in space, but they come with serious drawbacks. The precision of the GPS depends on the general number of available satellites; moreover, urban canyons and indoor navigation can seriously compromise the navigation. INS, instead, suffers from integration drift with acceleration and angular velocity error accumulation, requiring a correction scheme. Presently, the software and hardware advancements in embedded systems and the corresponding miniaturization have led to performing low-cost sensors and Inertial Measurement Units (IMUs) that can extract useful information on-board, such as force, angular rate, and orientation. Many approaches and configurations have been also proposed to get significant knowledge of the environment from data acquired by consumer RGB cameras, depth sensors, LiDAR (Light Detection and Ranging), and event-based cameras. Complex and complete sensor fusion suites that merge multiple data have been introduced too. For each sensor that can be potentially mounted on-board of the UAV, a plethora of works and applications have been proposed. Independently from the employed sensor, where evidently each one comes with own pro and cons, and/or each sensor is better performing in a specific scenario, all of the approaches share the goal of providing meaningful input for the high-level components. Undoubtedly, computer vision can provide a critical contribution to the autonomy of the UAVs and their operational capabilities [6]. Typical UAV operations are surveillance, person tracking, path planning, obstacle avoidance (see Section 2): all of these tasks strongly relies on the detection of one or more domain-related objects. Object detection has then been widely investigated since the beginning of computer vision. Historically, detecting objects in images taken from a camera has represented one of the first computer vision tasks ever: early works are dated to 1960s [7], and a kick-off work that is famous (and considered quite optimistic) in the computer vision community Summer Vision Project is dated 1966 [8]. Henceforth, the possibility of detecting and recognizing classes of objects has been considered to be the first component of any artificial intelligence system, and many proposed theoretical techniques in the computer vision community have been applied to such a task. If many impressive results have been achieved even outperforming human-level performance in object detection, there is still a gap to be filled when the problem is translated to the specific case of aerial robotics. Different challenges are involved in terms of performance, scene, object classes, point of view, perspective, data acquisition, and so on. Moreover, if many datasets are available for object recognition task, they cannot be directly employed in the case of UAVs since the scenes are different from the operational working conditions of the UAV (e.g., indoor, cluttered environments, cameras placed at ground level). Finally, computational constraints and/or communication schemes are an important issue to be addressed. Very precise surveys on the role of computer vision for the autonomous operation of UAV exist. They focus on high-level operations [6,9], they consider only a specific altitude and/or imaging type [10,11], a technology [12], or a precise use-case [5,13\u201316]. It can be observed how, in these very important surveys, the computer vision point of view has been only partially considered. This has been the first motivation behind this work. This work investigates the object detection problem for the specific case of the UAVs from RGB and 2D object detection, introducing also main concepts J. Imaging 2020, 6, 78 3 of 38 and references for mixed sensor suites and/or 3D object detection. The second motivation is in the need for a different categorization of the UAVs: there are many valid classification schemes based on characteristics as the mass, size, mission range, operation heights, level of autonomy, flying principle, operation condition [17]. Anyway, concerning the height, it is common to classify UAVs in intervals of 150\u2013300 m, 3000, 5000 and even 20,000 m, always starting from 50 m. From a computer vision point of view, we think that this scheme is not sufficient to understand the methodologies that are applied to address the object detection problem depending on the operational height. Thus, a conceptual approach that takes into account the operations carried out at very different heights has not been proposed yet. To the best of our knowledge, we present the first work that classifies object recognition methods for the case of UAVs that considers different heights intervals, whose definition is given by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints. At the same time, our proposal is well-integrated with active EU rules and procedures for the operation of unmanned aircraft. Moreover, how specific state of the art deep learning architectures are adapted for the case under consideration is discussed, and the main applications that introduce their own scientific and technological peculiarities are illustrated. Finally, the different datasets specifically designed to evaluate object detection from aerial views are reported and detailed. Summing up, the main contributions of this work are: \u2022 an update of dominant literature aiming at performing object detection from UAV and aerial views; \u2022 a taxonomy for the UAV based on the computer vision point of view, that considers how the same problem can drastically change when it is observed from a different perspective; \u2022 a critical discussion of the actual state of the art, with particular attention to the impact of deep learning. The manuscript is organized as follows: first of all, the role of object detection from UAVs in terms of higher-level operations is illustrated in Section 2, also introducing some important sensors employed in the state of the..",
              "url": "https://openalex.org/W3047386722",
              "openalex_id": "https://openalex.org/W3047386722",
              "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
              "publication_date": "2020-08-04"
            }
          ]
        },
        "S4678038695": {
          "id": "S4678038695",
          "text": "Adapting automotive safety standards like ISO 26262 for UAVs can improve safety in drone racing systems by establishing rigorous safety goals and integrity levels tailored to the unique challenges of aerial vehicles.",
          "children": [
            {
              "id": "E3768007399",
              "text": "..3) NHTSA Test Suite for Ground-Air Package Delivery Drones The National Highway Traffic Safety Administration (NHTSA) is the federal agency responsible for regulating the safety of motor vehicles and highway transportation in the United States. NHTSA also has a role in overseeing the integration of unmanned aircraft systems (UAS) or drones into This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 27 Volume xx, 2023 the national airspace system, especially for ground-air package delivery operations. Ground-air package delivery drones are UAS that can transport goods from a ground vehicle to a customer's location using autonomous flight capabilities. These drones have the potential to improve the efficiency, convenience, and environmental impact of e-commerce and other delivery services. However, ground-air package delivery drones also pose unique safety challenges that need to be addressed before they can be widely deployed. NHTSA has developed a test suite [81] for evaluating the performance and safety of these drones in various scenarios and environments. The test suite consists of a set of standardized procedures, metrics, and criteria that can be applied to different types of ground-air package delivery drones and operations. The test suite covers aspects such as: \u2022 Ensuring the drone design and specifications meet the safety requirements and standards for UAS operations \u2022 Testing and verification the drone flight control and navigation systems for accuracy, reliability, and robustness \u2022 Development of secure the drone communication and data link systems from interference, jamming, or hacking \u2022 Design and operations of the drone payload and delivery mechanisms to avoid damage, loss, or theft of goods \u2022 Integration of the drone launch and recovery systems with the ground vehicles and infrastructure without causing traffic disruptions or accidents \u2022 Compliance with traffic rules and regulations for both ground and air operations \u2022 Obstacle and hazard detection and avoidance in the air and on the ground \u2022 Respond to emergencies and contingencies such as weather, malfunctions, and collisions The purpose of this test suite is to provide a consistent and objective framework for assessing the safety and performance of ground-air package delivery drones, which are likely equipped with varying autonomy capabilities, under various conditions and scenarios. The test suite can be used by drone manufacturers, operators, regulators, researchers, and other stakeholders to validate, verify, certify, or evaluate ground-air package delivery drones and operations. The test suite can also support the development of standards, best practices, and regulations for this emerging sector of UAS applications. 4) Challenges in adopting ISO 26262 / ASIL-D for Airborne Systems ISO 26262 is an international standard that defines requirements and processes for ensuring functional safety of electrical and electronic systems in passenger vehicles [82]. Functional safety is the absence of unreasonable risk due to hazards caused by malfunctioning behavior of these systems. It defines guidelines to minimize the risk of accidents and ensure that automotive components perform their intended functions correctly and at the right time. ISO 26262 is based on the IEC 61508 standard for general industrial applications, but it is adapted to the specific needs and challenges of the automotive sector. ISO 26262 covers the entire lifecycle of safety-related automotive systems, from concept phase to development, production, operation, service, and decommissioning. One of the main challenges for applying ISO 26262 to UAVs is the definition and classification of safety goals and automotive safety integrity levels (ASILs). Safety goals are high-level requirements that specify the necessary risk reduction for avoiding or mitigating hazards. Safety standards assign integrity levels to systems or functions based on initial consequences analysis, with clear guidance for integrity level identification. ASILs are a measure of the severity, exposure, and controllability of hazards, ranging from A (lowest) to D (highest). For example, an eVTOL system that controls the flight stability would likely have a high ASIL level, while a system that provides entertainment functions would have a low ASIL level. The allocated integrity level dictates the rigor and stringency of development processes. However, it is important to note that ISO 26262 provides guidance and examples for defining safety goals and ASILs for passenger vehicles, but not for UAVs / eVTOLs specifically. Fig 12 below shows the various integrity levels for DO-178C AND ISO 26262. FIGURE 12 DO-178C and ISO 26262 comparison. UAVs have different types of hazards and risks than passenger vehicles, depending on their size, weight, speed, payload, operation mode, mission type, flight environment, and regulatory framework. For example, a small UAV flying over a rural area may have a lower risk of causing harm than a large UAV flying over an urban area. Therefore, it is necessary to adapt the ISO 26262 methodology for defining safety goals and ASILs for UAVs according to their specific characteristics and scenarios. Another challenge for applying ISO 26262 to UAVs is the verification and validation of autonomous functions. ISO 26262 proposes model-in-the-loop (MIL), software-in-the-loop (SIL), and hardware-in-the-loop (HIL) simulation for conducting software safety requirements verification. All of these simulation processes can be applied towards the common goal of generating autonomous vehicle requirements. However, simulation alone may not be sufficient to ensure the safety and This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 28 Volume xx, 2023 reliability of autonomous functions that involve complex interactions with dynamic and uncertain environments. Therefore, it is necessary to complement simulation with realworld testing and evaluation of autonomous functions in representative scenarios and conditions. Despite these challenges, applying ISO 26262 to UAVs also offers some opportunities for improving their safety and quality. One of the opportunities is the reuse and adaptation of existing standards and best practices from the automotive domain. ISO 26262 provides a comprehensive framework for managing functional safety throughout the lifecycle of safetyrelated systems. It also provides detailed guidance and recommendations for performing various activities and tasks related to functional safety. Therefore, applying ISO 26262 to UAVs can help to establish a common terminology, methodology, and documentation for ensuring functional safety of UAVs and their autonomous operations. It can also help to leverage existing knowledge and experience from the automotive domain and benefit from the lessons learned and good practices developed by other industries. Another opportunity for applying ISO 26262 to UAVs is the innovation and advancement of new technologies and solutions for autonomous operations. ISO 26262 encourages the use of state-of-the-art methods and tools for designing, developing, testing, and operating safety-related systems. It also supports the continuous improvement and optimization of functional safety processes and products. Therefore, applying ISO 26262 to UAVs can stimulate the research and development of new technologies and solutions that can enhance the capabilities, performance, and efficiency of autonomous functions. It can also foster the collaboration and integration of different disciplines and domains that are involved in the creation and operation of autonomous systems. VIII. EXPLORING ADJACENT TERRITORIES: A CLOSER LOOK AT TWO KEY ASPECTS A. SIM-TO-REAL ROBOTS AND SYSTEMS Sim-to-real robots and systems are challenging to develop and deploy due to the gap between simulation and reality [83]. In striving for a streamlined workflow to transition seamlessly from simulation to real-world applications, the key to success lies in: A) the meticulous optimization of the runtime and inference architecture, catering specifically to the target hardware and the intricacies of the application domain and B) comprehensively addressing all edge cases during the process of developing autonomy blocks, ensuring the robustness of algorithm performance to adhere to the standards of the safetycritical aviation industry. DO-178C and CoDANN help with the latter. A fundamental aspect of this optimization pertains to domain adaptation. Domain adaptation refers to the process of customizing the sensing and perception modules of the robot according to the specific application domain [84]. For example, different types of sensors may be required for indoor and outdoor environments, or for different weather conditions. Moreover, the sensor data may vary significantly from simulation to reality, requiring robust and adaptive models that can handle domain shifts. Various techniques such as data augmentation, domain randomization, and adversarial learning can be leveraged to train and test the AI models (powering various Autonomy Blocks) in diverse and realistic scenarios. Online fine-tuning refers to the ability of the robot to adapt its behavior and decision-making modules based on the feedback from the environment and the user [85]. Online finetuning enables the autonomous system to improve its performance based on continuous learning and real-world experiences, thereby promoting enhanced autonomy and reliability. For example, the aerial vehicle may need to adaptively adjust its speed, trajectory, or navigation strategy according to the dynamic and uncertain situations it encounters in the real world, which it has or hasn\u2019t necessarily encountered during the simulation-based training and testing. Methods such as reinforcement learning, imitation learning, and active learning can be employed to enable online learning and improvement of the AI models in an interactive and dataefficient manner. IoT edge device deployments refer to the implementation of autonomy solutions on power-efficient embedded AI computing devices that can be integrated with the aerial vehicle's hardware [86]. For example, the UAV may need to run its models on a low-power CPU or GPU that can fit within its SWaP-c constraints. Moreover, the V2X capability will require the vehicle to communicate with other devices or cloud services via wireless networks, requiring..",
              "url": "https://openalex.org/W4389352644",
              "openalex_id": "https://openalex.org/W4389352644",
              "title": "Autonomous Advanced Aerial Mobility \u2013An End-to-end Autonomy Framework for UAVs and Beyond",
              "publication_date": "2023-01-01"
            },
            {
              "id": "E8708102179",
              "text": "..autonomous systems. VIII. EXPLORING ADJACENT TERRITORIES: A CLOSER LOOK AT TWO KEY ASPECTS A. SIM-TO-REAL ROBOTS AND SYSTEMS Sim-to-real robots and systems are challenging to develop and deploy due to the gap between simulation and reality [83]. In striving for a streamlined workflow to transition seamlessly from simulation to real-world applications, the key to success lies in: A) the meticulous optimization of the runtime and inference architecture, catering specifically to the target hardware and the intricacies of the application domain and B) comprehensively addressing all edge cases during the process of developing autonomy blocks, ensuring the robustness of algorithm performance to adhere to the standards of the safetycritical aviation industry. DO-178C and CoDANN help with the latter. A fundamental aspect of this optimization pertains to domain adaptation. Domain adaptation refers to the process of customizing the sensing and perception modules of the robot according to the specific application domain [84]. For example, different types of sensors may be required for indoor and outdoor environments, or for different weather conditions. Moreover, the sensor data may vary significantly from simulation to reality, requiring robust and adaptive models that can handle domain shifts. Various techniques such as data augmentation, domain randomization, and adversarial learning can be leveraged to train and test the AI models (powering various Autonomy Blocks) in diverse and realistic scenarios. Online fine-tuning refers to the ability of the robot to adapt its behavior and decision-making modules based on the feedback from the environment and the user [85]. Online finetuning enables the autonomous system to improve its performance based on continuous learning and real-world experiences, thereby promoting enhanced autonomy and reliability. For example, the aerial vehicle may need to adaptively adjust its speed, trajectory, or navigation strategy according to the dynamic and uncertain situations it encounters in the real world, which it has or hasn\u2019t necessarily encountered during the simulation-based training and testing. Methods such as reinforcement learning, imitation learning, and active learning can be employed to enable online learning and improvement of the AI models in an interactive and dataefficient manner. IoT edge device deployments refer to the implementation of autonomy solutions on power-efficient embedded AI computing devices that can be integrated with the aerial vehicle's hardware [86]. For example, the UAV may need to run its models on a low-power CPU or GPU that can fit within its SWaP-c constraints. Moreover, the V2X capability will require the vehicle to communicate with other devices or cloud services via wireless networks, requiring reliable and secure data transmission protocols. The models need to be optimized for edge deployment using techniques such as model compression, quantization, pruning, and distillation, as well as leveraging edge computing platforms such as Azure IoT Edge. Leveraging the potency of embedded AI computing devices, this paradigm facilitates the efficient and seamless integration of autonomous capabilities into resource-constrained UAV environments. B. MONOLITHIC DEEP LEARNING FOR AUTONOMOUS AERIAL VEHICLES: CHALLENGES AND OPPORTUNITIES Monolithic deep learning algorithms typically refer to comprehensive, end-to-end machine learning models that handle multiple aspects of autonomous flight. These can include tasks such as obstacle detection and avoidance, path planning, and navigation. Monolithic deep learning models are tightly coupled systems where all the layers work in a highly synchronized manner. These models are often seen as a single, centralized unit, which can make them easier to develop, test, and debug. These algorithms are \"monolithic\" in the sense that they are designed to handle multiple tasks within the same framework, rather than relying on separate models or systems for each task. This can lead to more efficient and coordinated behavior in autonomous aerial robots. Monolithic models emphasize tight integration and synchronization of all components. However, this can also make them less flexible and adaptable compared to more modular or distributed systems. In recent years, the field of AI has witnessed a significant transformation, shifting from task-specific, narrow models to This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 29 Volume xx, 2023 larger, more versatile monolithic neural networks. For instance, within the domain of natural language processing (NLP), models like GPT-4 have demonstrated an impressive array of capabilities, encompassing tasks such as text summarization, translation, and sentiment analysis. Concurrently, visuallanguage models have been gaining proficiency in a multitude of tasks, including object detection, image captioning, and even generative tasks like creating artwork. This progression implies the potential for a unified, generalized model to potentially replace numerous task-specific models, offering enhanced efficiency and a simpler system architecture. However, when transitioning from NLP to the realm of robotics, a number of complexities come to the forefront. Firstly, there is a notable scarcity of data (see Section III.B for challenges with Synthetic Data creation process), as end-toend foundation models necessitate extensive training data, and there is a limited availability of curated datasets for pre-training robots. Consequently, the emphasis shifts towards enhancing the intelligence of existing foundation models for each of the proposed Autonomy Blocks (namely, Sense, Perceive, Plan, Actuate) even when their original application differs from that of aerial autonomy. Additionally, in the field of robotics, the wide variability in actuators and control systems introduces an additional layer of intricacy. Each type of aerial vehicle, whether it's a quadcopter, eVTOL, traditional aircraft, or helicopter, possesses a distinct set of actuators and corresponding control systems. This makes the concept of generalization more challenging. Employing a monolithic neural network that directly maps sensor inputs to actuator outputs is therefore not scalable and also risks overlooking the wealth of existing research in control theory. IX. CONCLUSION AND OUTLOOK The outlook of the Advanced Aerial Mobility (AAM) field is poised for transformative change, with an increasing recognition of the need for AAM solutions in both urban and rural contexts. Urban congestion and gridlock have become a ubiquitous problem, and AAM holds the potential to alleviate these issues by introducing unmanned aerial vehicles (UAVs) for passenger and cargo transportation. In parallel, the use of UAVs is already revolutionizing various industries, from agriculture and construction to healthcare and logistics. These aircraft provide cost-efficient and rapid solutions for tasks such as crop monitoring, site surveys, medical supply delivery, and last-mile logistics, offering a glimpse into the future where AAM will play a pivotal role in enhancing productivity and quality of life. The research and development of fully autonomous aerial vehicles is advancing at an impressive pace, propelling the AAM field toward its full potential. In this paper, we have presented a comprehensive study of the autonomous aerial mobility field, consisting of four main components: simulation, data, autonomy, and multi-agent fleets. We have described the functionalities and technical underpinnings of each component and how they interact with each other to enable safe and efficient operations of aerial vehicles, particularly AAMs based UAVs, in complex urban environments. We have outlined key innovations as well as existing systems. The focal point of our work is the autonomy blocks framework. This modular AIbased approach aims to address the full spectrum of autonomy for advanced aerial mobility, from sensing and perception to planning and control. We have proposed a customizable, modular, and extensible design paradigm that allows for building autonomy stack for different levels of autonomy and different types of aerial vehicles. We have also reviewed the state-of-the-art research and technologies in various domains and sectors that are relevant to our framework, including deep learning algorithms that cater to specific modules of the proposed autonomy stack. Furthermore, we have discussed the challenges and opportunities for benchmarking and validating our framework based on the up-and-coming standards, guidelines, and ConOps being established by regulatory bodies around the world. Autonomous aerial vehicles need to comply with the tight regulatory oversight that governs the aerial mobility industry as ensuring the safety of passengers, property, and infrastructure is of paramount importance. This requires high standards of safety, security, and reliability. Therefore, AAM requires a multidisciplinary effort that integrates cutting-edge research and development from various fields, such as aviation engineering, computer science, artificial intelligence, robotics, and human factors. We believe that our autonomy blocks framework offers a holistic and comprehensive approach to developing the underlying technology \u2013 rooted in the multidisciplinary foundations - to advancing the field of autonomous aerial mobility. We hope that our work will inspire further research and innovation in this exciting and important domain. REFERENCES [1] B. A. Adu-Gyamfi and C. Good, \"Electric aviation: A review of concepts and enabling technologies,\" Transportation Engineering, vol. 9, pp. 100 - 134, 2022. [2] MarketsAndMarkets, \"eVTOL Aircraft Market,\" 2023. [3] eHang, \" Air Mobility | Smart City Management | Aerial Media,\" [Online]. Available: https://ehang.com/. [Accessed 06 11 2023]. [4] A. Hussain and D. Silver, \"Advanced air mobility. Can the United States afford to lose the race?,\" 26 01 2021. [Online]. Available: https://www2.deloitte.com/us/en/insights/industry/aerospace -defense/advanced-air-mobility.html. [Accessed 06 11 2023]. [5] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil, N. S. Othman, A. Khreishah and M. Guizani, \"Unmanned Aerial Vehicles (UAVs): A Survey on This article has been accepted for publication in IEEE Access. This is the author'..",
              "url": "https://openalex.org/W4389352644",
              "openalex_id": "https://openalex.org/W4389352644",
              "title": "Autonomous Advanced Aerial Mobility \u2013An End-to-end Autonomy Framework for UAVs and Beyond",
              "publication_date": "2023-01-01"
            }
          ]
        },
        "S5538602873": {
          "id": "S5538602873",
          "text": "The integration of regulatory frameworks, such as the NHTSA test suite for evaluating drone safety, can significantly enhance safety protocols in drone racing systems by providing standardized procedures for performance assessment.",
          "children": [
            {
              "id": "E3768007399",
              "text": "..3) NHTSA Test Suite for Ground-Air Package Delivery Drones The National Highway Traffic Safety Administration (NHTSA) is the federal agency responsible for regulating the safety of motor vehicles and highway transportation in the United States. NHTSA also has a role in overseeing the integration of unmanned aircraft systems (UAS) or drones into This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 27 Volume xx, 2023 the national airspace system, especially for ground-air package delivery operations. Ground-air package delivery drones are UAS that can transport goods from a ground vehicle to a customer's location using autonomous flight capabilities. These drones have the potential to improve the efficiency, convenience, and environmental impact of e-commerce and other delivery services. However, ground-air package delivery drones also pose unique safety challenges that need to be addressed before they can be widely deployed. NHTSA has developed a test suite [81] for evaluating the performance and safety of these drones in various scenarios and environments. The test suite consists of a set of standardized procedures, metrics, and criteria that can be applied to different types of ground-air package delivery drones and operations. The test suite covers aspects such as: \u2022 Ensuring the drone design and specifications meet the safety requirements and standards for UAS operations \u2022 Testing and verification the drone flight control and navigation systems for accuracy, reliability, and robustness \u2022 Development of secure the drone communication and data link systems from interference, jamming, or hacking \u2022 Design and operations of the drone payload and delivery mechanisms to avoid damage, loss, or theft of goods \u2022 Integration of the drone launch and recovery systems with the ground vehicles and infrastructure without causing traffic disruptions or accidents \u2022 Compliance with traffic rules and regulations for both ground and air operations \u2022 Obstacle and hazard detection and avoidance in the air and on the ground \u2022 Respond to emergencies and contingencies such as weather, malfunctions, and collisions The purpose of this test suite is to provide a consistent and objective framework for assessing the safety and performance of ground-air package delivery drones, which are likely equipped with varying autonomy capabilities, under various conditions and scenarios. The test suite can be used by drone manufacturers, operators, regulators, researchers, and other stakeholders to validate, verify, certify, or evaluate ground-air package delivery drones and operations. The test suite can also support the development of standards, best practices, and regulations for this emerging sector of UAS applications. 4) Challenges in adopting ISO 26262 / ASIL-D for Airborne Systems ISO 26262 is an international standard that defines requirements and processes for ensuring functional safety of electrical and electronic systems in passenger vehicles [82]. Functional safety is the absence of unreasonable risk due to hazards caused by malfunctioning behavior of these systems. It defines guidelines to minimize the risk of accidents and ensure that automotive components perform their intended functions correctly and at the right time. ISO 26262 is based on the IEC 61508 standard for general industrial applications, but it is adapted to the specific needs and challenges of the automotive sector. ISO 26262 covers the entire lifecycle of safety-related automotive systems, from concept phase to development, production, operation, service, and decommissioning. One of the main challenges for applying ISO 26262 to UAVs is the definition and classification of safety goals and automotive safety integrity levels (ASILs). Safety goals are high-level requirements that specify the necessary risk reduction for avoiding or mitigating hazards. Safety standards assign integrity levels to systems or functions based on initial consequences analysis, with clear guidance for integrity level identification. ASILs are a measure of the severity, exposure, and controllability of hazards, ranging from A (lowest) to D (highest). For example, an eVTOL system that controls the flight stability would likely have a high ASIL level, while a system that provides entertainment functions would have a low ASIL level. The allocated integrity level dictates the rigor and stringency of development processes. However, it is important to note that ISO 26262 provides guidance and examples for defining safety goals and ASILs for passenger vehicles, but not for UAVs / eVTOLs specifically. Fig 12 below shows the various integrity levels for DO-178C AND ISO 26262. FIGURE 12 DO-178C and ISO 26262 comparison. UAVs have different types of hazards and risks than passenger vehicles, depending on their size, weight, speed, payload, operation mode, mission type, flight environment, and regulatory framework. For example, a small UAV flying over a rural area may have a lower risk of causing harm than a large UAV flying over an urban area. Therefore, it is necessary to adapt the ISO 26262 methodology for defining safety goals and ASILs for UAVs according to their specific characteristics and scenarios. Another challenge for applying ISO 26262 to UAVs is the verification and validation of autonomous functions. ISO 26262 proposes model-in-the-loop (MIL), software-in-the-loop (SIL), and hardware-in-the-loop (HIL) simulation for conducting software safety requirements verification. All of these simulation processes can be applied towards the common goal of generating autonomous vehicle requirements. However, simulation alone may not be sufficient to ensure the safety and This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 28 Volume xx, 2023 reliability of autonomous functions that involve complex interactions with dynamic and uncertain environments. Therefore, it is necessary to complement simulation with realworld testing and evaluation of autonomous functions in representative scenarios and conditions. Despite these challenges, applying ISO 26262 to UAVs also offers some opportunities for improving their safety and quality. One of the opportunities is the reuse and adaptation of existing standards and best practices from the automotive domain. ISO 26262 provides a comprehensive framework for managing functional safety throughout the lifecycle of safetyrelated systems. It also provides detailed guidance and recommendations for performing various activities and tasks related to functional safety. Therefore, applying ISO 26262 to UAVs can help to establish a common terminology, methodology, and documentation for ensuring functional safety of UAVs and their autonomous operations. It can also help to leverage existing knowledge and experience from the automotive domain and benefit from the lessons learned and good practices developed by other industries. Another opportunity for applying ISO 26262 to UAVs is the innovation and advancement of new technologies and solutions for autonomous operations. ISO 26262 encourages the use of state-of-the-art methods and tools for designing, developing, testing, and operating safety-related systems. It also supports the continuous improvement and optimization of functional safety processes and products. Therefore, applying ISO 26262 to UAVs can stimulate the research and development of new technologies and solutions that can enhance the capabilities, performance, and efficiency of autonomous functions. It can also foster the collaboration and integration of different disciplines and domains that are involved in the creation and operation of autonomous systems. VIII. EXPLORING ADJACENT TERRITORIES: A CLOSER LOOK AT TWO KEY ASPECTS A. SIM-TO-REAL ROBOTS AND SYSTEMS Sim-to-real robots and systems are challenging to develop and deploy due to the gap between simulation and reality [83]. In striving for a streamlined workflow to transition seamlessly from simulation to real-world applications, the key to success lies in: A) the meticulous optimization of the runtime and inference architecture, catering specifically to the target hardware and the intricacies of the application domain and B) comprehensively addressing all edge cases during the process of developing autonomy blocks, ensuring the robustness of algorithm performance to adhere to the standards of the safetycritical aviation industry. DO-178C and CoDANN help with the latter. A fundamental aspect of this optimization pertains to domain adaptation. Domain adaptation refers to the process of customizing the sensing and perception modules of the robot according to the specific application domain [84]. For example, different types of sensors may be required for indoor and outdoor environments, or for different weather conditions. Moreover, the sensor data may vary significantly from simulation to reality, requiring robust and adaptive models that can handle domain shifts. Various techniques such as data augmentation, domain randomization, and adversarial learning can be leveraged to train and test the AI models (powering various Autonomy Blocks) in diverse and realistic scenarios. Online fine-tuning refers to the ability of the robot to adapt its behavior and decision-making modules based on the feedback from the environment and the user [85]. Online finetuning enables the autonomous system to improve its performance based on continuous learning and real-world experiences, thereby promoting enhanced autonomy and reliability. For example, the aerial vehicle may need to adaptively adjust its speed, trajectory, or navigation strategy according to the dynamic and uncertain situations it encounters in the real world, which it has or hasn\u2019t necessarily encountered during the simulation-based training and testing. Methods such as reinforcement learning, imitation learning, and active learning can be employed to enable online learning and improvement of the AI models in an interactive and dataefficient manner. IoT edge device deployments refer to the implementation of autonomy solutions on power-efficient embedded AI computing devices that can be integrated with the aerial vehicle's hardware [86]. For example, the UAV may need to run its models on a low-power CPU or GPU that can fit within its SWaP-c constraints. Moreover, the V2X capability will require the vehicle to communicate with other devices or cloud services via wireless networks, requiring..",
              "url": "https://openalex.org/W4389352644",
              "openalex_id": "https://openalex.org/W4389352644",
              "title": "Autonomous Advanced Aerial Mobility \u2013An End-to-end Autonomy Framework for UAVs and Beyond",
              "publication_date": "2023-01-01"
            },
            {
              "id": "E8708102179",
              "text": "..autonomous systems. VIII. EXPLORING ADJACENT TERRITORIES: A CLOSER LOOK AT TWO KEY ASPECTS A. SIM-TO-REAL ROBOTS AND SYSTEMS Sim-to-real robots and systems are challenging to develop and deploy due to the gap between simulation and reality [83]. In striving for a streamlined workflow to transition seamlessly from simulation to real-world applications, the key to success lies in: A) the meticulous optimization of the runtime and inference architecture, catering specifically to the target hardware and the intricacies of the application domain and B) comprehensively addressing all edge cases during the process of developing autonomy blocks, ensuring the robustness of algorithm performance to adhere to the standards of the safetycritical aviation industry. DO-178C and CoDANN help with the latter. A fundamental aspect of this optimization pertains to domain adaptation. Domain adaptation refers to the process of customizing the sensing and perception modules of the robot according to the specific application domain [84]. For example, different types of sensors may be required for indoor and outdoor environments, or for different weather conditions. Moreover, the sensor data may vary significantly from simulation to reality, requiring robust and adaptive models that can handle domain shifts. Various techniques such as data augmentation, domain randomization, and adversarial learning can be leveraged to train and test the AI models (powering various Autonomy Blocks) in diverse and realistic scenarios. Online fine-tuning refers to the ability of the robot to adapt its behavior and decision-making modules based on the feedback from the environment and the user [85]. Online finetuning enables the autonomous system to improve its performance based on continuous learning and real-world experiences, thereby promoting enhanced autonomy and reliability. For example, the aerial vehicle may need to adaptively adjust its speed, trajectory, or navigation strategy according to the dynamic and uncertain situations it encounters in the real world, which it has or hasn\u2019t necessarily encountered during the simulation-based training and testing. Methods such as reinforcement learning, imitation learning, and active learning can be employed to enable online learning and improvement of the AI models in an interactive and dataefficient manner. IoT edge device deployments refer to the implementation of autonomy solutions on power-efficient embedded AI computing devices that can be integrated with the aerial vehicle's hardware [86]. For example, the UAV may need to run its models on a low-power CPU or GPU that can fit within its SWaP-c constraints. Moreover, the V2X capability will require the vehicle to communicate with other devices or cloud services via wireless networks, requiring reliable and secure data transmission protocols. The models need to be optimized for edge deployment using techniques such as model compression, quantization, pruning, and distillation, as well as leveraging edge computing platforms such as Azure IoT Edge. Leveraging the potency of embedded AI computing devices, this paradigm facilitates the efficient and seamless integration of autonomous capabilities into resource-constrained UAV environments. B. MONOLITHIC DEEP LEARNING FOR AUTONOMOUS AERIAL VEHICLES: CHALLENGES AND OPPORTUNITIES Monolithic deep learning algorithms typically refer to comprehensive, end-to-end machine learning models that handle multiple aspects of autonomous flight. These can include tasks such as obstacle detection and avoidance, path planning, and navigation. Monolithic deep learning models are tightly coupled systems where all the layers work in a highly synchronized manner. These models are often seen as a single, centralized unit, which can make them easier to develop, test, and debug. These algorithms are \"monolithic\" in the sense that they are designed to handle multiple tasks within the same framework, rather than relying on separate models or systems for each task. This can lead to more efficient and coordinated behavior in autonomous aerial robots. Monolithic models emphasize tight integration and synchronization of all components. However, this can also make them less flexible and adaptable compared to more modular or distributed systems. In recent years, the field of AI has witnessed a significant transformation, shifting from task-specific, narrow models to This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3339631 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Mishra et al.: Autonomous Advanced Aerial Mobility \u2013An End-to-End Autonomy Framework for UAVs and Beyond 29 Volume xx, 2023 larger, more versatile monolithic neural networks. For instance, within the domain of natural language processing (NLP), models like GPT-4 have demonstrated an impressive array of capabilities, encompassing tasks such as text summarization, translation, and sentiment analysis. Concurrently, visuallanguage models have been gaining proficiency in a multitude of tasks, including object detection, image captioning, and even generative tasks like creating artwork. This progression implies the potential for a unified, generalized model to potentially replace numerous task-specific models, offering enhanced efficiency and a simpler system architecture. However, when transitioning from NLP to the realm of robotics, a number of complexities come to the forefront. Firstly, there is a notable scarcity of data (see Section III.B for challenges with Synthetic Data creation process), as end-toend foundation models necessitate extensive training data, and there is a limited availability of curated datasets for pre-training robots. Consequently, the emphasis shifts towards enhancing the intelligence of existing foundation models for each of the proposed Autonomy Blocks (namely, Sense, Perceive, Plan, Actuate) even when their original application differs from that of aerial autonomy. Additionally, in the field of robotics, the wide variability in actuators and control systems introduces an additional layer of intricacy. Each type of aerial vehicle, whether it's a quadcopter, eVTOL, traditional aircraft, or helicopter, possesses a distinct set of actuators and corresponding control systems. This makes the concept of generalization more challenging. Employing a monolithic neural network that directly maps sensor inputs to actuator outputs is therefore not scalable and also risks overlooking the wealth of existing research in control theory. IX. CONCLUSION AND OUTLOOK The outlook of the Advanced Aerial Mobility (AAM) field is poised for transformative change, with an increasing recognition of the need for AAM solutions in both urban and rural contexts. Urban congestion and gridlock have become a ubiquitous problem, and AAM holds the potential to alleviate these issues by introducing unmanned aerial vehicles (UAVs) for passenger and cargo transportation. In parallel, the use of UAVs is already revolutionizing various industries, from agriculture and construction to healthcare and logistics. These aircraft provide cost-efficient and rapid solutions for tasks such as crop monitoring, site surveys, medical supply delivery, and last-mile logistics, offering a glimpse into the future where AAM will play a pivotal role in enhancing productivity and quality of life. The research and development of fully autonomous aerial vehicles is advancing at an impressive pace, propelling the AAM field toward its full potential. In this paper, we have presented a comprehensive study of the autonomous aerial mobility field, consisting of four main components: simulation, data, autonomy, and multi-agent fleets. We have described the functionalities and technical underpinnings of each component and how they interact with each other to enable safe and efficient operations of aerial vehicles, particularly AAMs based UAVs, in complex urban environments. We have outlined key innovations as well as existing systems. The focal point of our work is the autonomy blocks framework. This modular AIbased approach aims to address the full spectrum of autonomy for advanced aerial mobility, from sensing and perception to planning and control. We have proposed a customizable, modular, and extensible design paradigm that allows for building autonomy stack for different levels of autonomy and different types of aerial vehicles. We have also reviewed the state-of-the-art research and technologies in various domains and sectors that are relevant to our framework, including deep learning algorithms that cater to specific modules of the proposed autonomy stack. Furthermore, we have discussed the challenges and opportunities for benchmarking and validating our framework based on the up-and-coming standards, guidelines, and ConOps being established by regulatory bodies around the world. Autonomous aerial vehicles need to comply with the tight regulatory oversight that governs the aerial mobility industry as ensuring the safety of passengers, property, and infrastructure is of paramount importance. This requires high standards of safety, security, and reliability. Therefore, AAM requires a multidisciplinary effort that integrates cutting-edge research and development from various fields, such as aviation engineering, computer science, artificial intelligence, robotics, and human factors. We believe that our autonomy blocks framework offers a holistic and comprehensive approach to developing the underlying technology \u2013 rooted in the multidisciplinary foundations - to advancing the field of autonomous aerial mobility. We hope that our work will inspire further research and innovation in this exciting and important domain. REFERENCES [1] B. A. Adu-Gyamfi and C. Good, \"Electric aviation: A review of concepts and enabling technologies,\" Transportation Engineering, vol. 9, pp. 100 - 134, 2022. [2] MarketsAndMarkets, \"eVTOL Aircraft Market,\" 2023. [3] eHang, \" Air Mobility | Smart City Management | Aerial Media,\" [Online]. Available: https://ehang.com/. [Accessed 06 11 2023]. [4] A. Hussain and D. Silver, \"Advanced air mobility. Can the United States afford to lose the race?,\" 26 01 2021. [Online]. Available: https://www2.deloitte.com/us/en/insights/industry/aerospace -defense/advanced-air-mobility.html. [Accessed 06 11 2023]. [5] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil, N. S. Othman, A. Khreishah and M. Guizani, \"Unmanned Aerial Vehicles (UAVs): A Survey on This article has been accepted for publication in IEEE Access. This is the author'..",
              "url": "https://openalex.org/W4389352644",
              "openalex_id": "https://openalex.org/W4389352644",
              "title": "Autonomous Advanced Aerial Mobility \u2013An End-to-end Autonomy Framework for UAVs and Beyond",
              "publication_date": "2023-01-01"
            }
          ]
        },
        "S3345294563": {
          "id": "S3345294563",
          "text": "Future directions for enhancing the performance and safety of autonomous drone racing systems may involve the development of specialized datasets that simulate racing environments to improve training for object detection algorithms.",
          "children": [
            {
              "id": "E6058154070",
              "text": "Arman Asgharpoor Golroudbari\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nDepartment of Aerospace\nFaculty of New Sciences & Technologies\nUniversity of Tehran\nTehranIran\nMohammad Hossein Sabour sabourmh@ut.ac.ir\nUniversity of Tehran\nTehranIran\nRECENT ADVANCEMENTS IN DEEP LEARNING APPLICATIONS AND METHODS FOR AUTONOMOUS NAVIGATION: A COMPREHENSIVE REVIEW\nDeep Learning \u00b7 Navigation \u00b7 Inertial Sensors, Intelligent Filter \u00b7 Sensor Fusion \u00b7 Long-Short Term Memory \u00b7 Convolutional Neural Network\nThis review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.\nIntroduction\nAutonomous navigation is a critical component of robotics that has transformed numerous application domains, such as medical, industrial, space, and agricultural. By equipping robots with the ability to navigate autonomously, they can efficiently and securely move through dynamic environments without human intervention, expanding their versatility and functionality. To enhance the performance of autonomous navigation systems, researchers have been pushing the technology to its limits, employing state-of-the-art techniques and methodologies.\nGiven the expansive and ever-evolving nature of the literature surrounding autonomous navigation, it is imperative to conduct regular literature surveys in order to remain abreast of the latest advancements. Therefore, the primary objective of this review is to provide a comprehensive and in-depth overview of the current state-of-the-art in autonomous navigation, with a focus on catering to both experienced researchers and novices in the field. Additionally, a terminology section is included to provide clarity and understanding of the technical vocabulary utilized throughout the article.\n\u2022 Mapping: Creating a map of an environment using sensor data and other inputs.\n\u2022 Simultaneous Localization and Mapping (SLAM): Creating a map of an unknown environment while simultaneously localizing the robot or autonomous system within that environment. \u2022 Control: Regulating the motion to follow a desired trajectory and achieve a specific task. \u2022 Obstacle Avoidance: Navigate around obstacles in the path. \u2022 Collision Avoidance: Using sensors and algorithms to detect potential collisions and take action to avoid them. \u2022 Path Planning: Determining a safe and efficient path for an autonomous system to follow. \u2022 Motion Planning: Determining the trajectory to reach a goal while avoiding obstacles and adhering to other constraints. \u2022 Sensor Fusion: Combining data from multiple sensors to obtain a more accurate and comprehensive understanding of the environment. \u2022 Odometry: Using sensory data to estimate the position and orientation by analyzing the movement over time.\n\u2022 Dead Reckoning: Estimates the current position by using its previous position and velocity. Navigation is a critical task for systems that operate in dynamic environments, such as robots, autonomous systems, and unmanned aerial vehicles [1]. It requires the ability to perceive the surroundings, plan a path, execute it, and adapt as needed, all while avoiding obstacles and collisions to ensure safe, efficient, and accurate travel. Recent developments in deep learning have made navigation more reliable, effective, and efficient, enabling its use in a wide range of applications, including transportation, search and rescue, and delivery. Autonomous systems are becoming increasingly prevalent and can determine their actions based on the current situation. These systems have numerous uses, such as self-driving cars [2], drones [3], and search-and-rescue robots [4].\nAutonomous systems fall into two broad categories [5]: reactive and deliberative. Reactive systems, also known as behavior-based systems, are designed to respond to the environment using predefined rules. These systems are typically used in robotics applications, where the environment is relatively stable, and the system has a specific task, such as pick and place. On the other hand, deliberative systems are designed to plan and execute a path to a destination. These systems are commonly used in transportation, where the environment is complex, and the system must navigate efficiently and safely.\nThe deliberative systems can be further classified into two categories [6]: (1) model-based and (2) model-free systems. Model-based systems use a mathematical model of the environment to plan a path, such as using dynamic programming or graph search algorithms [7]. Model-free systems, also known as model-agnostic systems, do not rely on a model of the environment to plan a path [8]. Instead, these systems use techniques such as Reinforcement Learning [9] or Apprenticeship Learning [10] to learn the optimal policy for navigation. The choice of autonomous system, whether reactive or deliberative and model-based or model-free, depends on the specific requirements and constraints of the task.\nAutonomous systems require effective and successful navigation to operate in dynamic environments without human intervention and guidance. This involves integrating various technologies, including sensors, actuators, and control systems. To improve the accuracy, efficiency, and robustness of navigation algorithms, deep learning has been applied to various navigation tasks such as perception and planning. Deep learning's ability to learn complex representations from vast amounts of data is well-suited for these tasks, such as image analysis and integrating multiple sources of information, such as speech and text. However, deep learning-based navigation systems must overcome challenges such as limited data, reliability, ethical concerns, such as privacy and bias. Techniques such as transfer learning, multi-modal fusion, model uncertainty estimation, and safety-critical architectures can address these challenges. Additionally, differential privacy and fairness-aware machine learning techniques can address privacy and bias concerns, respectively.\nThe application of deep learning in navigation has seen a rise in recent times, as evidenced by numerous studies and surveys (refer to Tables 1 and 2). Although deep learning holds great promise in enhancing navigation systems, it is crucial to tackle the challenges and ethical considerations that come with its usage. Furthermore, there is a need for further exploration on how deep learning techniques can be integrated with conventional navigation methods.\nNumerous surveys have been conducted on the applications of deep learning in various navigation domains, including urban navigation [11], visual navigation [12,13], reinforcement learning [14,9], obstacle detection [15], and spacecraft navigation [16,17]. However, there is a lack of comprehensive surveys that provide a general overview of the use of deep learning in navigation. This survey aims to fill this gap by presenting a comprehensive overview of the applications of deep learning in navigation. The paper is structured as follows: Section 2 provides an overview of deep learning and its methods, while Section 3 discusses the different activation functions used in deep learning. Section 4 presents an overview of navigation, autonomy, and autonomous navigation. Section 5 discusses the applications of deep learning in navigation, and Section 6 delves into the various components of deep learning in autonomous navigation, such as perception, localization, mapping, planning, and control. Finally, Section 7 concludes the paper and highlights future directions. Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization [18] Multi-robot Navigation\nHINNet: Inertial navigation with head-mounted sensors using a neural network [19] Inertial Navigation Multi-sensor integrated navigation/positioning systems using data fusion: From analytics-based to learning-based approaches [20] Integrated Navigation Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction [21] Visual Navigation Goal-guided Transformer-enabled Reinforcement Learning for Efficient Autonomous Navigation [22] Autonomous Navigation DeepNAVI: A deep learning based smartphone navigation assistant for people with visual impairments [23] Visual Navigation Monocular vision with deep neural networks for autonomous mobile robots navigation [24] Visual Navigation URWalking: Indoor Navigation for Research and Daily Use [25] Indoor Navigation A Simple Self-Supervised IMU Denoising Method For Inertial Aided Navigation [26] Inertial Na",
              "url": "https://openalex.org/W4381333019",
              "openalex_id": "https://openalex.org/W4381333019",
              "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
              "publication_date": "2023-06-20"
            },
            {
              "id": "E7895996755",
              "text": "Journal of Imaging Review A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles Dario Cazzato 1,\u2217 , Claudio Cimarelli 1, Jose Luis Sanchez-Lopez 1, Holger Voos 1 and Marco Leo 2 1 Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, 1855 Luxembourg, Luxembourg; claudio.cimarelli@uni.lu (C.C.); joseluis.sanchezlopez@uni.lu (J.L.S.-L.); holger.voos@uni.lu (H.V.) 2 Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, 73100 Lecce, Italy; marco.leo@cnr.it * Correspondence: dario.cazzato@uni.lu Received: 29 June 2020; Accepted: 31 July 2020; Published: 4 August 2020 Abstract: The spread of Unmanned Aerial Vehicles (UAVs) in the last decade revolutionized many applications fields. Most investigated research topics focus on increasing autonomy during operational campaigns, environmental monitoring, surveillance, maps, and labeling. To achieve such complex goals, a high-level module is exploited to build semantic knowledge leveraging the outputs of the low-level module that takes data acquired from multiple sensors and extracts information concerning what is sensed. All in all, the detection of the objects is undoubtedly the most important low-level task, and the most employed sensors to accomplish it are by far RGB cameras due to costs, dimensions, and the wide literature on RGB-based object detection. This survey presents recent advancements in 2D object detection for the case of UAVs, focusing on the differences, strategies, and trade-offs between the generic problem of object detection, and the adaptation of such solutions for operations of the UAV. Moreover, a new taxonomy that considers different heights intervals and driven by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints is proposed. Keywords: computer vision; 2d object detection; unmanned aerial vehicles; deep learning 1. Introduction Unmanned Aerial Vehicles (UAVs), also called Unmanned Aircraft Systems (UASs), and commonly known as drones, are aircraft that fly without a pilot on-board. This places numerous advantages in terms of pilot safety, training, and aircraft costs and sizes, with a huge impact in the range of possible applications. Numbers behind the UAV industry are impressive: Value Market Research estimated that the market for VTOL (Vertical Take-Off and Landing) UAVs will touch around USD 10,163 M by 2024 [1]. Another report from PwC [2] estimates that, in 2030, there will be 76,000 drones operating in the UK skies, involving a total of 628,000 jobs. These forecasts will imply, still in the UK, an increase in GDP of 42 bn\u00a3 and net savings for the UK economy of 16 bn\u00a3. Finally, an EU report of 2016 [3] estimated an economic impact exceeding e10 bn per year within 20 years. As a consequence, both research and industry are investigating the challenges involved in the manufacturing as well as the design of hardware, software, sensors and algorithms to guarantee the UAV operability and to extend its range to unseen scenarios. In fact, UAVs already achieved an unprecedented seen level of growth in many civil and military application domains [4]. UAVs can be remotely controlled by a pilot or can fly autonomously. In the former scenario, the pilot is on land- or sea-based ground control station (GCS) for human control. J. Imaging 2020, 6, 78; doi:10.3390/jimaging6080078 www.mdpi.com/journal/jimaging J. Imaging 2020, 6, 78 2 of 38 The simplest GCS consists of a remote controller with an optional screen, even in the form of a tablet. In the latter scenario, instead, a pre-scheduled flight plan and a dynamic automation system are necessary. The Holy Grail for the involved actors in this revolution is the achievement of fully autonomous operational capabilities to flight over and understand real and complex scenarios. In such an ideal system, a high-level and possibly on-board module is exploited to build semantic knowledge used to reach the application goal. The semantic knowledge leverages low-level software components that extract information concerning what is sensed. Both exteroceptive and proprioceptive sensors are used to obtain situational and self-awareness [5]. From the beginning, UAVs were equipped with sensors such as Global Positioning System (GPS) and Inertial Navigation System (INS) to provide position and orientation in space, but they come with serious drawbacks. The precision of the GPS depends on the general number of available satellites; moreover, urban canyons and indoor navigation can seriously compromise the navigation. INS, instead, suffers from integration drift with acceleration and angular velocity error accumulation, requiring a correction scheme. Presently, the software and hardware advancements in embedded systems and the corresponding miniaturization have led to performing low-cost sensors and Inertial Measurement Units (IMUs) that can extract useful information on-board, such as force, angular rate, and orientation. Many approaches and configurations have been also proposed to get significant knowledge of the environment from data acquired by consumer RGB cameras, depth sensors, LiDAR (Light Detection and Ranging), and event-based cameras. Complex and complete sensor fusion suites that merge multiple data have been introduced too. For each sensor that can be potentially mounted on-board of the UAV, a plethora of works and applications have been proposed. Independently from the employed sensor, where evidently each one comes with own pro and cons, and/or each sensor is better performing in a specific scenario, all of the approaches share the goal of providing meaningful input for the high-level components. Undoubtedly, computer vision can provide a critical contribution to the autonomy of the UAVs and their operational capabilities [6]. Typical UAV operations are surveillance, person tracking, path planning, obstacle avoidance (see Section 2): all of these tasks strongly relies on the detection of one or more domain-related objects. Object detection has then been widely investigated since the beginning of computer vision. Historically, detecting objects in images taken from a camera has represented one of the first computer vision tasks ever: early works are dated to 1960s [7], and a kick-off work that is famous (and considered quite optimistic) in the computer vision community Summer Vision Project is dated 1966 [8]. Henceforth, the possibility of detecting and recognizing classes of objects has been considered to be the first component of any artificial intelligence system, and many proposed theoretical techniques in the computer vision community have been applied to such a task. If many impressive results have been achieved even outperforming human-level performance in object detection, there is still a gap to be filled when the problem is translated to the specific case of aerial robotics. Different challenges are involved in terms of performance, scene, object classes, point of view, perspective, data acquisition, and so on. Moreover, if many datasets are available for object recognition task, they cannot be directly employed in the case of UAVs since the scenes are different from the operational working conditions of the UAV (e.g., indoor, cluttered environments, cameras placed at ground level). Finally, computational constraints and/or communication schemes are an important issue to be addressed. Very precise surveys on the role of computer vision for the autonomous operation of UAV exist. They focus on high-level operations [6,9], they consider only a specific altitude and/or imaging type [10,11], a technology [12], or a precise use-case [5,13\u201316]. It can be observed how, in these very important surveys, the computer vision point of view has been only partially considered. This has been the first motivation behind this work. This work investigates the object detection problem for the specific case of the UAVs from RGB and 2D object detection, introducing also main concepts J. Imaging 2020, 6, 78 3 of 38 and references for mixed sensor suites and/or 3D object detection. The second motivation is in the need for a different categorization of the UAVs: there are many valid classification schemes based on characteristics as the mass, size, mission range, operation heights, level of autonomy, flying principle, operation condition [17]. Anyway, concerning the height, it is common to classify UAVs in intervals of 150\u2013300 m, 3000, 5000 and even 20,000 m, always starting from 50 m. From a computer vision point of view, we think that this scheme is not sufficient to understand the methodologies that are applied to address the object detection problem depending on the operational height. Thus, a conceptual approach that takes into account the operations carried out at very different heights has not been proposed yet. To the best of our knowledge, we present the first work that classifies object recognition methods for the case of UAVs that considers different heights intervals, whose definition is given by the methodological approaches introduced by the works in the state of the art instead of hardware, physical and/or technological constraints. At the same time, our proposal is well-integrated with active EU rules and procedures for the operation of unmanned aircraft. Moreover, how specific state of the art deep learning architectures are adapted for the case under consideration is discussed, and the main applications that introduce their own scientific and technological peculiarities are illustrated. Finally, the different datasets specifically designed to evaluate object detection from aerial views are reported and detailed. Summing up, the main contributions of this work are: \u2022 an update of dominant literature aiming at performing object detection from UAV and aerial views; \u2022 a taxonomy for the UAV based on the computer vision point of view, that considers how the same problem can drastically change when it is observed from a different perspective; \u2022 a critical discussion of the actual state of the art, with particular attention to the impact of deep learning. The manuscript is organized as follows: first of all, the role of object detection from UAVs in terms of higher-level operations is illustrated in Section 2, also introducing some important sensors employed in the state of the..",
              "url": "https://openalex.org/W3047386722",
              "openalex_id": "https://openalex.org/W3047386722",
              "title": "A Survey of Computer Vision Methods for 2D Object Detection from Unmanned Aerial Vehicles",
              "publication_date": "2020-08-04"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W3047386722",
        "https://openalex.org/W4389352644",
        "https://openalex.org/W4381333019"
      ],
      "evaluation": {
        "precision@10": 0.0,
        "recall@10": 0.0,
        "f1@10": 0,
        "rouge_1": 0.02084092399621074,
        "rouge_2": 0.008963707914298206,
        "rouge_l": 0.013408146906653061,
        "text_f1": 0.05521611224258882,
        "num_source_papers": 3
      }
    },
    {
      "id": "https://openalex.org/W4390494361",
      "limited_meta": {
        "title": "A Comprehensive Survey on 5G-and-Beyond Networks With UAVs: Applications, Emerging Technologies, Regulatory Aspects, Research Trends and Challenges",
        "publication_date": "2024-01-01",
        "cited_by_count": 15,
        "url": ""
      },
      "text": "1\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.Doi Number\nA Comprehensive Survey on 5G-and-Beyond\nNetworks with UAVs: Applications, Emerging\nTechnologies, Regulatory Aspects, Research Trends\nand Challenges\nMohammed Banafaa1, \u00d6mer Pepeo\u011flu2, Ibraheem Shayea3, Abdulraqeb Alhammadi4, Zaid Shamsan5, Muneef A.\nRazaz1, Majid Alsagabi5, Sulaiman Al-Sowayan5\n1Communication Engineering Department, Faculty of Electrical Engineering King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia\n2School of Computation, Information and Technology, Technical University of Munich, 80333 Munich, Germany\n3Electronics and Communication Engineering Department, Faculty of Electrical and Electronics Engineering, Istanbul Technical University (ITU), 34467 Istanbul, T\u00fcrkiye\n4Center for Artificial Intelligence and Robotics (CAIRO), Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, 54100 Kuala Lumpur, Malaysia\n5Department of Electrical Engineering, College of Engineering, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, 11432, Saudi Arabia\nCorresponding author: Mohammed Banafaa , Ibraheem Shayea, and Abdulraqeb Alhammadi (e-mail: eng.banafaa@gmail.com,\nshayea@itu.edu.tr, abdulraqeb.alhammadi@gmail.com ).\nThis work was supported and funded by the Deanship of Scientific Research at Imam Mohammad Ibn Saud Islamic University (IMSIU) (grant number IMSIU\u0002RG23042).\nABSTRACT The rapid advancement of fifth-generation (5G)-and-beyond networks coupled with unmanned aerial vehicles\n(UAVs) has opened up exciting possibilities for diverse applications and cutting-edge technologies, revolutionizing the way\nconnections, communications, and innovations unfold in the digital age. This paper presents a comprehensive survey of the\ndeployment scenarios, applications, emerging technologies, regulatory aspects, research trends, and challenges associated with\nthe use of UAVs in 5G-and-beyond networks. It begins with a succinct background and motivation, followed by a systematic\nUAV classification and a review of relevant works. The survey covers UAV deployment scenarios, including single and\nmultiple UAV configurations. The categorization of UAV applications in 5G is presented, along with investigations into\nemerging technologies for enhancing UAV communications. Regulatory considerations encompassing flight guidelines,\nspectrum allocation, privacy, and safety are discussed. Moreover, light is shed on the latest research trends and open challenges\nin the field, with promising directions for future investigations identified, concluding with a summary of key findings and\ncontributions. This survey serves as a valuable resource for researchers, practitioners, and policymakers in the UAV and\ncommunication domains. Additionally, it offers a comprehensive foundation for informed decision-making, fostering\ncollaboration, and driving advancements in UAV and communication technologies to address the evolving needs of our\ninterconnected world.\nINDEX TERMS Drone, 5G, unmanned aerial vehicles, wireless systems.\nI. INTRODUCTION\nDrones, alternatively referred to as UAVs or remotely piloted\naircraft systems, have garnered considerable interest in\nrecent times owing to their capacity to potentially transform\ndiverse industries and sectors [1]. UAVs that are equipped\nwith wireless communication capabilities possess the\ncapacity to gather and transmit data in real-time, execute\ntasks autonomously, and engage in collaborative operations\nwith other UAVs and ground control stations. The\navailability of connected UAVs has been facilitated by the\nprogress in wireless communication technologies and the\ndevelopment of next-generation wireless systems.\nTraditionally, UAV applications have predominantly\ncentered around their utilization for recreational purposes,\nphotography, and as a pastime for enthusiasts. Nevertheless,\nthe integration of wireless systems with UAVs has led to the\nfeasibility of various commercial, industrial, and public\nsector applications. Connected UAVs are currently being\nemployed in various sectors, including but not limited to\naerial surveillance for security and law enforcement,\nlogistics and delivery services, precision agriculture, disaster\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n2\nmanagement, environmental monitoring, and infrastructure\ninspection.\nThe growing incorporation of interconnected UAVs\nacross diverse industries has spurred significant scholarly\ninquiry and technological advancements aimed at tackling\nthe communication-related obstacles and prerequisites they\nentail. The forthcoming wireless systems, such as 5G and\nsubsequent generations, possess the capability to establish\nthe essential infrastructure and capacities required for\nfacilitating efficient and dependable communication\nbetween UAVs and ground control stations.\nIn particular, the utilization and management of UAVs can\noffer dependable and economically viable wireless\ncommunication alternatives for a diverse range of practical\nsituations. UAVs have the capability to function as aerial\nbase stations (ABSs), providing dependable, cost-efficient,\nand readily available wireless communication services to\ntargeted regions [2]. In addition, UAVs have the capability\nto operate as aerial user equipment (UE), commonly referred\nto as cellular-connected UAVs, alongside terrestrial users\nsuch as delivery or surveillance UAVs. The utilization of\nUAVs in this promising domain necessitates a\nreconsideration of the research obstacles, with an emphasis\non wireless communications and networking rather than\ncontrol and navigation.\nDespite the potential advantages of UAVs, several\nchallenging parameters exist, including bandwidth\nlimitations, high mobility, intermittent connectivity, a\nlimited transmission spectrum, and uncertain noisy channels.\nThe ad hoc multi-hop environment presents various\nchallenges, including collisions and latency issues. For\nexample, maintaining a communication range between two\nUAVs travelling at very high speeds in opposition to one\nanother might be challenging.\nAlthough there are many prospects for UAV utilization,\nthere are a number of technical issues that must be resolved\nbefore UAVs can be used effectively for any given\nnetworking application. When utilizing UAV-BS, important\nfactors to consider in its design encompass performance\ncharacterization, efficient three-dimensional deployment of\nUAVs, allocation of wireless and computational resources,\noptimization of flight time and trajectory, as well as network\nplanning. In the context of the UAV-UE scenario, several\nkey challenges arise, including handover management,\nchannel modeling, low-latency control, 3D localization, and\ninterference management [3].\nUAVs can be classified based on various factors,\nincluding their size, range, flight characteristics, purpose, and\nflight capabilities. UAV classification provides a framework\nfor categorizing UAVs into distinct groups, allowing for\nbetter understanding and analysis of their characteristics and\ncapabilities. The classifications of UAVs are based on\nseveral factors, including [4]:\nSize and Weight: UAVs are classified based on their\nphysical dimensions and weight. This classification helps in\nunderstanding the scale and capabilities of the UAVs, as\ndifferent sizes may have varying payload capacities, flight\ntimes, and operational capabilities.\nFlight Characteristics: The classification also takes into\naccount the flight characteristics of the UAVs, such as fixed\u0002wing or rotary-wing design. This classification helps to\ndifferentiate between UAVs that operate more like airplanes\n(fixed-wing) and those that operate with rotating blades for\nlift and maneuverability (rotary-wing).\nPayload Capacity: UAVs are categorized based on their\nability to carry and support different payload types. Payloads\ncan include cameras, sensors, communication equipment, or\nspecialized equipment for specific applications. The payload\ncapacity classification helps in selecting UAVs suitable for\nspecific mission requirements.\nFlight Range and Endurance: UAVs can be classified\nbased on their flight range and endurance capabilities. This\nclassification helps to differentiate between UAVs designed\nfor short-range or long-range missions and those with limited\nor extended flight times.\nPurpose and Application: The classification also considers\nthe intended purpose and application of UAVs. This\nclassification helps to group UAVs based on their specific\nuse cases, such as aerial surveillance, photography,\nagriculture, logistics, search and rescue, or scientific\nresearch.\nThis paper makes a significant contribution to the literature\non 5G-and-beyond networks with UAVs by offering a\ncomprehensive survey covering various dimensions of this\nevolving field. The major contributions of this study are:\nIn-depth coverage: A thorough examination of UAV\nnetworks in the context of 5G-and-beyond is presented,\nencompassing applications, technologies, regulations,\nresearch trends, and challenges. The paper serves as a one-stop\nreference for anyone interested in understanding the synergy\nbetween UAVs and advanced communication systems.\nClassification and deployment scenarios: A systematic\nclassification of UAVs based on their characteristics is\nprovided, enabling a better understanding of their diverse roles\nand functionalities. Additionally, an analysis of single and\nmultiple UAV deployment scenarios is conducted, shedding\nlight on their respective advantages and limitations.\nApplications and emerging technologies: Through the\ncategorization of UAV applications in 5G networks and the\ninvestigation of emerging communication technologies,\ninsights into potential use-cases and the state-of-the-art\nmethods for enhancing UAV communications are offered.\nRegulatory insights: Addressing the regulatory aspects of\nUAV deployment is crucial for real-world implementations.\nThe regulatory landscape, including pertinent guidelines and\nchallenges related to UAV operations, is discussed in the\npaper.\nResearch trends and open challenges: The latest research\ntrends and open challenges in the domain are identified and\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n3\nFIGURE 1. Diagram of survey organization.\ndiscussed, paving the way for future investigations and\ninnovations.\nThe structure of the paper is organized as follows. Section\nI, the introduction, lays the groundwork, emphasizing the\nsignificance of UAVs' impact on various industries and\noutlining the objectives of the survey. Section II covers related\nwork, providing context and building upon existing research\nin the field. In Section III, various UAV deployment scenarios\nare discussed alongside examples from the literature. Section\nIV categorizes UAV applications based on their functionalities\nand characteristics, providing a comprehensive overview of\nthe wide array of applications enabled by UAVs. Following\nthat, Section V explores emerging technologies, investigating\ncutting-edge advancements that augment UAV\ncommunications in 5G-and-beyond networks. In Section VI,\nthe critical legal and regulatory considerations related to UAV\ndeployment are addressed. This section explores airspace\nregulations, privacy concerns, and other policy frameworks\nshaping the integration of UAVs into existing communication\nnetworks. Section VII sheds light on the ongoing research\ntrends propelling the evolution of UAV networks. It also\noutlines the open challenges faced by researchers and industry\nprofessionals in achieving seamless and efficient UAV\ncommunications. Finally, Section VIII concludes by\nemphasizing the significance of continuous research and\ncollaboration to address existing challenges and unlock the full\npotential of UAV technology. Figure 1 demonstrates the\ndiagram of survey organization.\nII. RELATED WORK\nA. RECENT RELEVANT REVIEW ARTICLES\nCurrently, there is a proliferation of research and inquiries\npertaining to networks of UAVs. Several research studies on\nUAVs have been conducted, focusing primarily on offering a\ncomprehensive understanding of UAV communication\nmodels. These surveys have also explored various aspects\nsuch as applications, characteristics, challenges, and\nunresolved matters related to UAVs. Additionally, some\nsurveys have proposed solutions to address specific\nrequirements, including security concerns, medium access\ncontrol protocols, quality of service (QoS), and routing\nprotocols. Authors in [5] provided a comprehensive overview\nof pertinent research on machine learning (ML)-based\nstrategies for UAV communication. These strategies aim to\nenhance different aspects of UAV models and functionalities,\nincluding UAV channel modeling, managing resources,\npositioning, and security. The study referenced in [3]\npresented an extensive investigation into the utilization of\nUAVs within wireless networks. The study thoroughly\ninvestigates the fundamental tradeoffs and significant\nchallenges in UAV-enabled wireless networks. The objective\nof this study is to compile the most current and significant\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n4\nresearch findings from the limited and dispersed body of\nliterature on wireless communications using UAVs. This\npaper discusses the significant opportunities and challenges\nassociated with the deployment of UAVs as flying wireless\nbase stations (BSs). These UAVs serve as complementary\ncomponents to emerging wireless communication systems.\nAdditionally, the paper explores the utilization of UAVs as\ncellular-connected UAV-UEs, which rely on existing wireless\ninfrastructure. The emphasis is placed on various application\nscenarios, challenges, representative outcomes, open issues,\nand analytical techniques that are crucial for facilitating the\npractical implementation of UAVs as aerial communication\nplatforms.\nThe research reported in [6] provided a comprehensive\noverview of the use of UAVs in cellular communications. The\nauthors discussed the various practical aspects of UAV\ncellular communications, including standardization,\nregulation, challenges of integrating UAVs into existing\ncellular networks, the need for new protocols and standards,\nand the potential security risks associated with UAV cellular\ncommunications. In [7], the authors conducted an extensive\nsurvey of the current advancements in UAV-physical layer\nsecurity (PLS), covering fundamental concepts, static and\nmobile deployment scenarios, air-to-ground (A2G) channels,\nand various UAV roles. They reviewed secrecy performance\nanalysis and enhancement techniques for static UAV systems\nand scenarios involving UAV mobility.\nThe study conducted in [8] offered a thorough overview of\nthe research carried out for UAV deployment and trajectory to\nincrease the capacity of UAV wireless networks and to control\nthem effectively, in order to promote more research on UAV\nwireless networks. Additionally, this paper also discussed the\nchallenges and potential areas for future research.\nIn [9], the authors concentrated more on new UAV network\ntechnologies and their applications for next-generation\ncellular networks. The study comprehensively examined a\nrange of developing communication technologies for UAVs,\nincluding an analysis of their respective benefits, potential\napplications, technical obstacles, and future prospects. The\nresearch study encompassed an examination of\ncommunication and network technologies for UAVs, focusing\non the evaluation of appropriate task modules, antennas,\nresource managing platforms, and network structure. Further,\nit encompassed a comprehensive examination of emerging\ntechnologies, considering viewpoints from both academic and\nindustrial sectors, as supported by the latest scholarly\nliterature. Furthermore, the paper discussed the potential\nadvancements in UAV communication and their utilization in\ncontemporary technologies such as the Internet of Things\n(IoT), 5G networks, and wireless sensor networks. The\nresearch conducted in reference [10] presented a\ncomprehensive analysis and in-depth exploration of UAV\ncommunication protocols, networking systems, structures, and\nuse cases. Furthermore, the paper examined UAV solutions\nand emphasizes significant technical challenges and\nunresolved research issues that necessitate further\ninvestigation and development efforts.\nThe survey in [11] provided an overview of related works\nin UAV communications and technology integration. It\nexplored millimeter wave (mmWave) beamforming-enabled\nUAV communications, addressing both technical potential\nand challenges, as well as relevant mmWave antenna\nstructures and channel modelling. Additionally, technologies\nand solutions for UAV-connected mmWave cellular networks\nand mmWave-UAV ad hoc networks are reviewed. The status\nquo on UAV communications from an industrial standpoint is\noverviewed in [12]. Fresh updates from the 3GPP and details\non new 5G new radio (NR) features supporting aerial devices\nare provided. The potential and limitations of such features are\ndissected. The effectiveness of sub-6GHz massive multiple\ninput multiple output (MIMO) in addressing cell selection and\ninterference challenges is demonstrated, mmWave coverage is\nevaluated in different settings, and the specifics of direct\ndevice-to-device (D2D) communication in the aerial domain\nare examined.\nShifting the focus, in [13], the survey analyzed the impact\nof edge artificial intelligent (AI) on crucial UAV technical\naspects and applications, spanning diverse areas such as power\nmanagement, formation control, autonomous navigation,\ncomputer vision, privacy and security, and communication. It\nincluded applications like precision agriculture, delivery\nsystems, civil infrastructure inspection, search and rescue\noperations, acting as aerial wireless BSs, and UAV light\nshows. The work in [14] explored the contemporary landscape\nof UAV-assisted maritime communications, drawing from\nboth traditional optimization methods and ML techniques. It\ndiscussed various aspects, including UAV-based network\narchitectures, component roles, and categorized UAV-aided\nsolutions for maritime environments, addressing performance\ntargets such as physical-layer improvements, resource\nmanagement, and cloud/edge computing and caching.\nWith related to spectrum management for UAV operations\nis explored in [15], identifying suitable management schemes\naligned with UAV features and spectrum requirements.\nassumes coexistence with prevalent wireless technologies that\noccupy the spectrum. It also presented the rulings from\npolicymakers and regulators and discussed the operation\nbands and radio interfaces. In other survey [16], the\ndevelopment of existing regulation policies and critical\ntechnologies related to the safe and efficient operation of small\ncivil UAVs at low altitudes in urban areas is examined.\nThe integration of privacy and security in blockchain\u0002assisted UAV communication is discussed in [17], outlining\nfundamental analyses and critical requirements for\nconstructing privacy and security models and supporting\ndecentralized data storage systems. The review of [18] offered\na comprehensive examination of scenarios and essential\ntechnologies in UAV-assisted data collection. The system\nmodel, which includes the network framework and\nmathematical representation of UAV-assisted data collection\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n5\nTABLE 1\nSUMMARY OF EXISTING SURVEYS\nAuthors, year Brief Description\nBithas, et al. [5], 2019 This survey offers a comprehensive overview of ML-based strategies for UAV communication, targeting improvements in UAV\nchannel modelling, resource management, positioning, and security.\nFotouhi, et al. [6], 2019 This survey presents a comprehensive view of UAVs in cellular communications, covering practical aspects such as integration\nchallenges, protocol development, standards, and security concerns.\nWang, et al. [7], 2022 This survey discusses UAV-PLS advancements, covering fundamental concepts, deployment scenarios, A2G channels, and UAV\nroles, including secrecy performance analysis and enhancements for static and mobile UAV systems.\nHan [8], 2022 This survey provides an extensive overview of research on UAV deployment and trajectory for enhancing UAV wireless\nnetworks' capacity and control. It also highlights challenges and future research areas in this domain.\nSharma, et al. [9], 2020 This survey focuses on new UAV network technologies and their applications in next-generation cellular networks, covering a\nvariety of emerging communication technologies for UAVs, analysing their advantages, potential applications, technical\nchallenges, and future outlook.\nHentati, et al. [10], 2020 This survey thoroughly analyses UAV communication protocols, networking systems, structures, and use cases, while also\nhighlighting important technical challenges and unresolved research areas requiring further investigation and development.\nXiao, et al. [11], 2021 This survey offers an overview of related research in UAV communications and technology integration. It delves into mmWave\nbeamforming-enabled UAV communications, covering both technical possibilities and challenges, as well as discussing relevant\nmmWave antenna structures and channel modelling.\nGeraci, et al. [12], 2022 This survey discusses recent 3GPP updates and 5G NR features for aerial devices, analysing their potential and limitations. It\nalso demonstrates sub-6GHz massive MIMO's efficacy in addressing cell selection and interference, evaluates mmWave\ncoverage in various environments, and explores aerial direct D2D communication specifics.\nMcEnroe, et al. [13], 2022 This survey examines the influence of edge AI on vital UAV technical aspects and applications, encompassing areas like power\nmanagement, formation control, autonomous navigation, computer vision, privacy, security, and communication.\nJasim, et al. [15], 2021 This survey determines management schemes suitable for UAV features and spectrum needs, considering coexistence with\nexisting wireless technologies in the spectrum. It also outlines policymakers' and regulators' directives and explores operation\nbands and radio interfaces.\nXu, et al. [16], 2020 This survey examines the development of current regulatory policies and essential technologies concerning the safe and efficient\noperation of small civil UAVs at low altitudes in urban environments.\nHafeez, et al. [17], 2023 This survey addresses the incorporation of privacy and security in blockchain-assisted UAV communication, highlighting the\nneed for fundamental analyses and essential requirements to establish privacy and security models and facilitate decentralized\ndata storage systems.\nWei, et al. [18], 2022 This review provides a comprehensive analysis of scenarios and crucial technologies for UAV-assisted data collection in IoT. It\npresents the system model, covering network framework and mathematical representation, and conducts a thorough review of\nkey technologies.\nNomikos, et al. [14], 2022 This survey examines UAV-assisted maritime communications, combining traditional methods and ML techniques to enhance\nperformance in areas like the physical layer, resource management, and cloud/edge computing.\nDuong, et al. [19], 2022 This survey provides a comprehensive overview of UAV caching in 6G networks, encompassing caching model evolution from\nterrestrial to aerial domains, introducing a typical UAV caching system, and discussing recent advancements and performance\nmetrics.\nThis Survey It provides a comprehensive overview of deployment scenarios, applications, emerging technologies, regulatory aspects, research\ntrends, and challenges related to the integration of UAVs in 5G-and-beyond networks. It offers a holistic examination of the\nsubject matter.\nfor IoT, is presented. Subsequently, a thorough review of\ncritical technologies is conducted, encompassing sensor\nclustering, UAV data collection modes, and coordinated path\nplanning and resource allocation. The survey in [19] offered a\ncomprehensive survey of UAV caching models, techniques,\nand applications within sixth generation (6G) networks. It\ncovered the evolution of caching models from terrestrial to\naerial domains, introduces a typical UAV caching system, and\ndiscussed recent advancements and system performance\nmetrics in this context. Table 1 provides a concise overview of\nvarious related surveys, offering brief descriptions for each\none.\nB. OTHER RELEVANT WORKS ON UAV\nNumerous additional research areas exist that pertain to the\nuse of UAV communications and networking. In reference\n[20], the authors presented scenarios involving the\ndeployment of both single and multiple UAVs, along with a\nrange of use cases. These use cases involve the integration of\ndifferent wireless communication techniques. Furthermore, a\nthorough investigation is conducted to examine the\nramifications of the selected deployment techniques. The\nconcept of swarming UAVs and its intricacies have been\nintroduced in works [21] and [22] , which provides rigorous\nmathematical derivations elucidating the underlying theory of\nthe proposed approaches and algorithms. Additionally, the\nintroduction of other concerns was observed, including\ncollision avoidance and control latency. The utilization of\nchannel and antenna-based methodologies holds significant\nimportance in UAV communications. A comprehensive\nexplanation of channel modeling in UAV communication can\nbe found in [23]. In [24-27] various models for antenna\ndeployment are presented, encompassing both theoretical\nformulations and simulation outcomes. The articles [6] and\n[28] discussed various strategies and methodologies for\neffectively managing interference in a given system or\nnetwork. The concept of employing global positioning system\n(GPS) architecture for the purpose of system redundancy is\nelucidated in a notable study [29]. In [30] authors examined\nsecurity concerns pertaining to the communication systems of\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n6\nUAVs. The literature extensively covered considerations\npertaining to UAVs and structural design approaches, in\naddition to background and design considerations. The\nutilization of 5G technology in UAV applications is discussed\nin [31]. A comprehensive examination of UAVs with cellular\nconnectivity can be found in [32-34]. In addition, researchers\nproposed a network planning approach for UAV\ncommunication in their works in [35-37].\nIII. UAV DEPLOYMENT SCENARIOS\nIn a wireless communication network, which consists of one\nor multiple UAVs, there may be different deployment\nscenarios. Here, deployment means the position and mobility\nof the UAVs in the network. Two main criteria are\nconsidered when defining these deployment scenarios,\nwhich are the purpose of usage and the performance. The\npurpose of use may require a specific number of UAVs or\nmay cause any scenario which is unique for the relevant\napplication. Besides, the other criterion is performance\nconstraints, as in every wireless communication network. In\nthis section, some single and multiple UAV deployment\nscenarios are presented with their reasonings and\ncontributions, and the concept of swarming UAVs is\nexplained.\nA. SINGLE UAV DEPLOYMENT\nOnly one UAV is employed in a single UAV deployment\nscheme. This UAV behaves as a relay between the ground\nunits. The ground unit can be any member of the wireless\ncommunication networks, such as a BS or a mobile user. In\na single UAV deployment scenario, there are some issues\nhighlighted by [20] to provide optimal performance for the\nsystem. These issues can be summarized as the position of\nthe UAV in the air, the techniques that can be used for the\nrelay operation, and some advanced arrangements that can\nbe faced during practical applications. The possible\napproaches to these constraints are widely discussed in [20].\nFor example, when determining the optimal position of a\nsingle UAV in the air, parameters that identify the\noptimization problem are throughput, bit error rate, and\nsignal-to-noise ratio (SNR).\nAuthors in [38] proposed a single UAV deployment for\nindoor emergencies by minimizing power consumption. An\noptimization problem involving the relationship between the\ndimensions of the buildings and the location of the UAVs is\nderived using an exhaustive algorithm and an iterative\nalgorithm. A study conducted in [39] compared the single\nUAV deployment scenario to other scenarios in terms of cost.\nThe results proved that a single UAV deployment is more\ncost-effective than others. This study is useful for contrasting\nthe single UAV deployment with the following subsections in\nthis section.\nB. MULTIPLE UAV DEPLOYMENT\nContrary to a single UAV deployment scenario, there is more\nthan one UAV for multiple UAV deployment schemes. In\naddition to design considerations and optimization problems\nin a single UAV case, other critical problems that should be\nsolved: the relative positions and the reciprocal relations of the\nmultiple UAVs in the air. Since the medium of UAVs is three\u0002dimensional space and each UAV may have a different\nconnection scheme to transfer data with others, the problems\nrequire more complex approaches to find the optimal solution.\nA comprehensive study that includes many different\noptimization approaches such as mixed-integer optimization\nproblem, linear programming method, successive convex\noptimization, and penalty method is presented in [40]. In\nparticular, the authors focused on the downlink problem, but\nmany of the findings are relevant to different types of wireless\nnetworksin terms of optimization since they consider different\napproaches. Also, the effect of the increasing number of UAVs\nis well discussed. Authors in [41] discussed the optimization\nfor multiple UAVs from trajectory and power issues. They\nplaced more emphasis on theoretical formulations of the\nproblems than on [40], and proposed a deep neural network\nfor their models.\nIn multi-UAV systems, various topologies can be\nestablished, including the star, multi-star, mesh, and\nhierarchical mesh configurations. Each topology offers\ndifferent advantages and considerations for UAV\ncommunication. Table 2 shows the summary.\nTABLE 2\nSUMMARY OF UAV NETWORK TOPOLOGIES\nTopology Communication\nCharacteristics\nAdvantages\nStar Relies on UAV-to\u0002infrastructure\ncommunication\nSimple configuration,\ndirect communication\nwith ground node\nMulti-Star Multiple stars connected\nto a ground station\nCommunication within\nstars, easy inter-star\nconnection setup\nMesh UAVs interconnected;\npackets travel through\nintermediate nodes\nFlexibility, reliability,\nself-forming and\nreorganization features,\nbetter performance\ncharacteristics\nHierarchical\nMesh\nMultiple interconnected\nmesh networks with\nhierarchical connections\nHierarchical organization,\ninter-group\ncommunication\ncapabilities\n\u2022 Star Topology: In a star topology network, a ground node\nacts as the central point, and UAVs directly communicate\nwith this node. This configuration relies on UAV-to\u0002infrastructure communication, with all communication\npassing through the ground node. However, star topologies\nsuffer from high latency due to longer downlink lengths\ncompared to inter-UAV distances. The failure of the\nground node can result in mission failure, as there is no\ninter-UAV communication. Additionally, star\nconfigurations require expensive bandwidth downlinks.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n7\nMulti-UAVs Architecture\nGround WSN\nInternet of Things\nSwarming UAVs\nMulti-Layers UAV Networks\nCloud computing\nOpportunity-based networks of\nrelays\nDelay-tolerant UAVs networks\nTask-based UAV cooperation\nCooperative Multi-UAVs\nFIGURE 2. Multi-UAV architecture categorization\n\u2022 Multi-Star Topology: The multi-star topology network\nconsists of multiple stars formed by UAVs, with one node\nfrom each group connected to the ground station. Although\nthis topology enables communication among UAVs within\neach star, it still relies on the central ground station for\ninter-star communication. Similar to the star topology,\nmulti-star configurations face latency challenges and\ndepend on the availability of the ground station for\nsuccessful mission execution.\n\u2022 Mesh Topology: In the mesh topology network, UAVs\nare interconnected, and only one UAV may connect to the\ncontrol center. This configuration allows packets to travel\nthrough intermediate nodes, finding their way from any\nsource to any destination in multiple hops. Mesh networks\noffer flexibility, reliability, and better performance\ncharacteristics compared to star configurations. They are\nparticularly suitable for UAV networks as they support\nself-forming and reorganization features. When a node\nfails, the remaining nodes can reconfigure the network\namong themselves, ensuring continuous communication.\n\u2022 Hierarchical Mesh Topology: The hierarchical mesh\ntopology network involves multiple mesh networks\nformed by UAVs, with one node from each group\nconnected to other groups. Additionally, a small number\nof UAVs may directly connect to the control center. This\narchitecture provides hierarchical organization and inter\u0002group communication capabilities. Similar to the mesh\ntopology, it offers the advantages of flexibility, reliability,\nand self-healing capabilities.\nUAV architectures can be divided into two main categories:\ncooperative multi-UAVs and multi-layered UAV networks as\ndepicted in Figure 2.\nA) MULTI-LAYERED UAV NETWORKS\nMulti-layers UAV network architectures are based on UAVs\nnetwork that cooperates with other layers such as IoT or\nwireless sensor network (WSN) or with cloud computing\nsystems.\n1) SWARMING UAVS\nSwarming is another term used for multiple UAVs, especially\nin huge numbers of UAVs. The missions of a swarm of UAVs\nare not limited to wireless communications purposes, but also\nto many different scenarios as mentioned before. The\nswarming UAVs are presented in another subsection as it\nfocuses on the more coordinated and robotic behavior of\nmultiple UAVs. In addition to approaches that are similar to\nmultiple UAV deployment scenarios, some enhanced\ntechniques have been developed in the literature due to dealing\nwith relatively more data. To solve the computation\noffloading, an architecture named fog-computing-aided\nswarm of drones (FCSD) (see Figure 3) is introduced in [11].\nBoth the latency model and reliability model of FCSD are\nprovided to ensure powerful reliability by considering various\nscenarios. Another problem, which is discussed in the\nfollowing sections of this paper, is energy consumption of\nFCSD systems concerning reliability and latency\nperformance. For optimization problems, a Proximal Jacobi\nalternating direction method of multipliers is given to increase\nthe speed of the process, and simulation results are presented\nfor comparison in [11].\nAnother study [32] particularly focuses on the energy\nconsumption problem of swarming UAVs by analyzing\nvarious properties such as idle probabilities and collisions.\nThey have proposed a residual energy-aware online random\u0002access scheme and show how much outperforms conventional\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n8\nFIGURE 3. Swarming UAVs fog computing architecture\nFIGURE 4. Multiple UAVs\u2019 data dissemination\napproaches. In contrast, the study by [33] utilized\ncollaborative computation offloading by proposing a federated\nlearning-based method unlike [11] and stated that their\nmethods come up with improvements of 23% in energy\nconsumption and 15% in latency over other methods available\nin the literature.\nAdaptive data processing and dissemination for UAV\nswarms is another issue that takes a wide place in literature. A\nholistic solution named ADDSEN for this problem is\nintroduced in [22]. ADDSEN applies online learning\ntechnologies to achieve an adaptive balance between the rate\nof transmission and the rate of knowledge loss periodically\n(see Figure 4). This rate is used for some energy allocation and\nstorage problems. The mentioned technique also discusses the\nresults for both single and multiple UAV schemes. Since\nlarge-scale swarming UAV networks have numerous vehicles,\nclustering techniques are also applied for these scenarios. The\nstudy by [42] proposed a uniform clustering method to\nminimize communication latency by considering the number\nof cluster heads and the number of UAVs. Authors in [43]\nfocused on clustering in swarming UAVs for IoT applications\nby proposing a hybrid self-organized clustering scheme. Their\ncluster head determination approach is based on glowworm\nswarm optimization, unlike the method [42] utilizes.\nSwarming UAVs obviously presents many open\ninterdisciplinary problems.\n2) GROUND WIRELESS SENSOR NETWORK (WSN)\nThe aerial sensor network is a living ecosystem that consists\nof dispersed data sources and UAV nodes that gather and\ntransmit information to one another. UAVs make use of a\nvariety of sensors to acquire environmental variables such as\ntemperature and air pressure as they traverse their\nsurroundings. A wide variety of UAVs can be outfitted with\nspecialized sensors, such as infrared or high-resolution\ncameras, to fulfill a variety of sensing requirements.\nAdditionally, a comprehensive sensing network calls for both\naerial and ground sensors, which results in a two-layer\nstructure: UAV layer, which incorporates the aerial sensing\nlayer, and the ground WSN. This technique, which consists of\nseveral layers, guarantees thorough data collection and\nanalysis for a variety of activities and applications.\nAuthors in [44] presented a conceptual framework that\naddresses the utilization of heterogeneous UAVs for the\npurpose of fire detection. The framework incorporates infrared\nsensors and data fusion techniques by acquiring data from\ncomputer-aided modules. The framework being proposed\naims to achieve the localization of fire detection by utilizing\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n9\nthe positional data of UAVs and the localization information\nof cameras. In their study, the authors presented a WSN\nalgorithm for efficient data collection. The proposed algorithm\nutilizes a fleet of UAVs organized into groups. This approach\naims to enhance the overall performance of data collection in\nWSNs. The algorithm was discussed in detail in [45]. The\ncommunication architecture is exclusively dependent on ad\u0002hoc UAVs, sensors, and the ground station. A distributed\nalgorithm was proposed by researchers in order to effectively\nmanage the cyclic examination of a collection of points of\ninterest. The algorithm under consideration incorporates the\nvarious distributed data sources and potential reinforcements.\nThe authors conducted a study in [46] that focused on\nopportunistic routing (OR) in WSNs assisted by UAVs.\nAdditionally, two novel OR protocols were introduced. The\nfirst protocol is the adaptive neighbors opportunistic routing\nprotocol, which enables a source node to distribute its traffic\nto neighboring nodes within its communication range. The\nsecond protocol, known as Highest Velocity Opportunistic\nRouting (HVOR), involves the transmission of packets from\nthe source node to a single UAV that possesses the highest\nspeed. Hence, the HVOR algorithm dynamically determines\nan optimal route and identifies the sensor node that will serve\nas the forwarder.\n3) INTERNET OF THINGS (IoT)\nUAVs have the potential to fulfill a significant role within\nIoT framework and can provide numerous value-added\nservices in the realm of IoT. IoT provides the underlying\ninfrastructure and connectivity that enables Internet of\ndrones/UAVs UAVs (IoD) to function effectively. IoT\nnetworks, communication protocols, and cloud platforms\nfacilitate communication between UAVs and other devices\nor systems. This connectivity is essential for IoD\napplications. The key security, privacy, and communication\nrequirements are discussed, and a taxonomy of IoD is\npresented in [47] based on the most pertinent considerations.\nUAVs have the capability to offer wireless access to IoT\ndevices when terrestrial networks are unavailable, while also\nsupporting various IoT applications like cargo\ntransportation, video surveillance and pesticide spraying.\nNevertheless, the intricate, dynamic, and heterogeneous\nnature of UAV-assisted IoT networks has led to a heightened\ninterest in the utilization of AI-based techniques for the\noptimization, scheduling, and orchestration of such networks\n[48]. The structure of layers, in fact, offers services for\nvarious UAV applications, including but not limited to\nsurveillance, search and rescue, among others. The authors\ncarried out a study in [49] to examine the effectiveness of\ndeploying and mobilizing UAVs for the purpose of gathering\ndata from IoT devices located on the ground. The framework\noptimizes the placement and mobility of three-dimensional\n(3D) UAVs and facilitates dependable uplink transmissions\nfor IoT devices by minimizing the total transmit power.\n4) CLOUD COMPUTING\nUAVs have limited processing and storage capacity, which\nprecludes the performance of intense computation while in\nflight. The usage of cloud computing for data storage and\nprocessing can help mitigate this shortcoming. In [50], the\nauthors provided an extensive examination of optimization\ntechniques for radio resource management, mobile edge\ncomputing (MEC), fog, encompassing cloud and cloudlet\nsolutions tailored for UAVs. Additionally, it delves into the\nmathematical modeling of objectives and constraints and\noffers insights into the challenges associated with the\nutilization of these computing paradigms. In [51], the authors\nintroduced a cloud-based UAV management system called\nUAV map Planner that allows users to access UAVs via\nonline services, plan out missions, and guarantee cooperation\namong UAVs. To mediate communications between UAVs\nand humans, a cloud-based proxy server is built. The\nMAVLink protocol is the foundation for the connection\nbetween the UAVs, the users, and the cloud.\nA) COOPERATIVE MULTI-UAVS\nThe Cooperative Multi-UAVs architecture is employed in\nmission scenarios that entail the utilization of multiple UAVs\npossessing distinct communication characteristics. The\narchitecture of Cooperative Multi-UAVs encompasses the\nintegration of cooperative UAVs for task accomplishment, the\nutilization of opportunistic relaying networks, and the\nestablishment of delay-tolerant UAV networks.\n1) TASK-BASED UAVS COOPERATION\nA cooperative network can be defined as a specialized graph\nwherein each node operates in accordance with the role\nassigned to another node. The task accomplishment of\ncooperative UAVs involves the execution of intricate tasks\nthrough coordinated operability. A cooperative UAVs\nnetwork can exhibit either static or dynamic characteristics. In\nthe context of a static network, the functionalities are\npredetermined or pre-established. In contrast, within the\ndynamic network, the behavior of nodes is uncertain, and the\ntopology of the network may undergo changes, thereby\nimpacting its overall performance.\nThe research in [52] dedicated to the development of a\nmulti-UAV flight strategy aimed at enhancing cooperative\nsearch capabilities within a dynamic communication\nenvironment marked by uncertainty. Concretely, a novel\ncooperative framework tailored for a localized communication\nnetwork is formulated to govern the positioning of multiple\nUAVs during the search operation. Moreover, local\ncommunication networks are established by considering the\nspatial distribution of UAVs, effectively fulfilling the\ndemands of the search task.\n2) OPPORTUNITY-BASED RELAYING NETWORKS\nUAV networks exhibit a notable prevalence of link failures,\nnecessitating the implementation of opportunistic relaying\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n10\ntechniques. This enables an improved utilization of network\nparameters and resources. The impact of employing UAVs for\npurely opportunistic relay purposes in cooperative awareness\napplications within vehicular networks is examined in [53]. It\nis not required that UAVs alter their trajectory or speed for\nopportunistic relaying, ensuring minimal interference with the\nexecution of their primary missions. In [54], the authors\nintroduced a novel algorithm based on multi-agent DRL. It\nincludes the design of UAV trajectories to serve mobile\nground users while maintaining network connectivity, the\nproper allocation of frequency resources among UAVs to\nmitigate interference, and the selection of suitable next-hop\nUAVs for each data packet to minimize transmission time and\nreduce the probability of network congestion.\n3) DELAY-TOLERANT UAVS NETWORKS\nDelay-tolerant UAV networks exhibit a notable feature of\nlimited connectivity, necessitating the adoption of a store\u0002and-forward mechanism for the routing protocol due to the\nintermittent availability of links. These networks encounter\ncomparable challenges to conventional networks.\nThe authors in [55] employed UAVs as delay-tolerant\nnetwork relays in order to facilitate communication among the\nground nodes. Every UAV remains stationary above its\ndesignated home-ground node until it receives a signal\nprompting it to initiate the transmission of messages to other\nground nodes. The system employs a genetic algorithm to\nascertain the optimal route for maximizing the efficiency of\ndeliveries. The authors in [56] examined a network of UAVs\nthat are capable of tolerating delays. The researchers\nemployed a spray and wait methodology for the purpose of\npath selection and management. The presence of end-to-end\nconnectivity issues, continuous link fractures, and high latency\nresults in an increase in overheads during path selection. The\nsuggested methodology exhibits suboptimal performance.\nIV. UAV APPLICATIONS AND CATEGORIZATIONS\nUAVs and terrestrial BSs (TBSs) each have unique roles in\nwireless communication networks. UAVs are mobile and\nrapidly deployable, ideal for immediate coverage or areas\nlacking terrestrial infrastructure. However, they face\nlimitations like flight time and payload. In contrast, TBSs\noffer reliable, cost-effective, and long-term network support,\ncovering larger areas and handling higher capacity. The\nchoice depends on specific use cases and deployment needs.\nTable 3 highlights the key differences between UAV BSs\nand TBSs. UAVs have emerged as a groundbreaking\ntechnology with diverse applications across various\ndomains. This section delves into the extensive range of\napplications and categorizations of UAV networks. By\nexploring the main functionalities of UAVs and their specific\nuse cases, an in-depth understanding of the potential impact\nand opportunities they offer in different sectors is provided.\nThe section categorizes UAV networks based on their\napplications, including surveillance and monitoring,\ncommunication relay, IoT support, and swarm intelligence.\nTABLE 3\nUAVS VS. TBSS\nAspect UAV BS TBS\nDeployment\nFlexibility\nRapid and flexible\ndeployment\nFixed and time\u0002consuming\nFlight Time Limited by battery/fuel\ncapacity\nContinuous\noperation\nLine-of-Sight\nRequirements\nRequire clear line of\nsight\nLess affected by\nobstacles\nMobility High mobility and\ndynamic positioning\nStationary and fixed\nlocation\nPayload Capacity Limited due to weight\nconstraints\nLarger capacity for\nequipment\nReliability and\nStability\nProne to disruptions and\ninterference\nMore stable and\nreliable\nSafety Concerns Safety and airspace\nconflicts\nFewer safety\nconcerns\nCoverage Range Adaptable coverage\nrange\nFixed coverage area\nConnectivity in\nEmergencies\nVital for emergency\ncommunications\nSusceptible to\ninfrastructure\ndamage\nSignal\nInterference\nPotentially lower\ninterference Higher interference\nCost Potentially lower\ndeployment costs\nHigher initial\ninvestment and\nmaintenance\nCapacity and\nThroughput\nScalable capacity and\nthroughput\nLimited scalability\nand throughput\nRegulatory\nConsiderations\nAirspace regulations and\nsafety compliance\nLocal regulations\nand zoning permits\nEnergy\nEfficiency\nVariable energy\nefficiency\nStable energy\nconsumption\nA. UAV APPLICATIONS USING 5G\nA) 5G-AND-BEYOND UAV BS:\nThe incorporation of UAVs as flying BSs represents a notable\nprogression in the domain of 5G and subsequent generations\nof communication networks. The utilization of ABSs presents\ndistinct benefits, leading to transformative advancements in\ndiverse applications and augmenting connectivity in\ndemanding settings. This section examines the significant\nsignificance of UAVs as ABSs and investigates their various\napplications, highlighting their potential in influencing the\nfuture of communication.\n1) ENHANCING BEYOND 5G'S COVERAGE AND CAPACITY\nIn the realm of 5G-and-beyond wireless communication\nnetworks, the escalating demand for enhanced capacity and\nubiquitous coverage has propelled UAVs into the spotlight.\nThis heightened attention stems from their remarkable\nqualities, which surpass the capabilities of traditional BSs and\nrelays [57]. Consequently, the current cellular wireless\nnetworks have experienced significant strain in terms of their\ncapacity and coverage, resulting in the birth of numerous\nwireless technologies aimed at addressing this issue. The\nmentioned technologies, including D2D communications,\nultra dense tiny cell networks, and mmWave communications,\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n11\nhave been widely recognized as the focal point of future\nbeyond 5G wireless networks [58]. Nevertheless, although\ntheir immense advantages, these solutions possess inherent\nrestrictions. One example of a requirement for D2D\ncommunication in cellular networks is the need for improved\nfrequency planning and resource utilization. Figure 5\nillustrates UAVs assistance that utilizes for 5G cellular\nnetwork and beyond. In the realm of extremely dense small\ncell networks, numerous issues arise pertaining to backhaul,\ninterference, and the comprehensive modeling of the network\nas a whole. In a similar vein, mmWave communication\nencounters limitations due to blockage and a heavy need of\nline-of-sight (LoS) connection in order to successfully fulfill\nthe potential of providing high-speed, low latency\ncommunications. The aforementioned issues will be amplified\nin scenarios involving UAV and unmanned aerial systems\n(UASs). Also, UAVs equipped with high-definition cameras\nwere being used for surveillance and monitoring purposes in\nvarious industries, including public safety, agriculture, and\ninfrastructure inspection. These UAVs could capture and\ntransmit high-quality video feeds in real-time, providing\nvaluable data for decision-making. In [59], the authors\nconstructed a simulation-level model of a 6G system, and\nconducted a case study to examine the application of high\u0002definition video monitoring, utilizing the principles of UAV\u0002swarm-based surveillance.\nThe integration of UAVs as flying BSs is anticipated to be\nan essential addition to the diverse 5G landscape. This\nintegration holds the potential to address certain obstacles\nassociated with current technology. The utilization of low\naltitude platform UAVs presents a viable and economically\nefficient strategy for delivering wireless connection to regions\nthat possess inadequate cellular infrastructure.\nFIGURE 5. Cellular network for 5G-and-beyond\nFurthermore, the utilization of UAV BSs shows potential in\nsituations, where deploying tiny cells only to cater to brief\nevents, such as sports events and festivals, is not economically\nfeasible due to the limited duration of wireless connection\nrequired for these events [60]. In contrast, high altitude\nplatform UAVs have the potential to offer a viable and\nenduring resolution for addressing coverage concerns in rural\nsettings.\nFurthermore, the utilization of UAVs for mmWave\ncommunications is a promising application wherein UAVs\ncan establish LoS communication linkages with users.\nConsequently, this can serve as an appealing approach to offer\nwireless transmission with a large capacity, capitalizing on the\nbenefits of both UAVs and mmWave communications.\nAdditionally, the integration of UAVs with mmWave\ntechnology, together with the potential utilization of large\nMIMO techniques, has the potential to establish a novel and\ndynamic airborne cellular network. This network can\neffectively deliver high-capacity wireless services, provided\nthat it is meticulously planned and controlled.\nUAVs have the capability to provide support to several\nterrestrial networks, including D2D and vehicle networks. For\nexample, because of their ability to move easily and utilize\nLoS communication, UAVs have the capacity to expedite the\ndistribution of information among equipment on the ground.\nIn addition, the utilization of UAVs has the potential to\nenhance the dependability of wireless connections in D2D and\nvehicle-to-vehicle (V2V) communications by leveraging\ntransmit diversity. Flying UAVs have the potential to assist in\nthe dissemination of general information to ground devices,\nhence mitigating interference in ground networks through a\nreduction in the frequency of communications between\nequipment.\nIn addition, UAV BSs have the capability to employ air-to\u0002air (A2A) connections in order to provide service to other\ncellular-connected UAV- UEs, thereby reducing the burden on\nthe terrestrial network. Numerous investigations have been\nconducted pertaining to the utilization of UAVs in the context\nof 5G wireless communication technology. For instance,\nauthors in [61] addressed radio resource slicing in 5G uplink\nradio access networks. The radio resource slicing problem has\nbeen investigated by using UAV cells to minimize the\nconsumption of uplink resources. They claimed that their\nmethod improves performance in terms of network coverage,\ncosts, and resource utilization. The study undertaken by [62]\ndeals with the uplink issue in 5G networks from the\nperspective of mitigating UAVs\u2019 interference and inter-cell\ninterference. The resource allocation scheme, so-called\nreverse frequency allocation, has been utilized to mitigate the\naforementioned interferences. A study focusing on the\nplacement of UAVs in 5G networks by considering factors\nsuch as latency, throughput, and data rate is introduced by\n[63]. Several approaches have been investigated and\ncompared using methods such as simulated annealing and\ngenetic algorithms and have been aimed to provide efficient\ndata for service providers. Another study focusing on UAV\nplacements is presented by [64]. While [63] focuses more on\napplications during an emergency, this study focuses on\nlogistics applications. They propose a new algorithm that\nmaximizes network capacity by optimizing the placement of\nUAVs.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n12\n2) UAVS AS PLATFORMS FOR PUBLIC SAFETY\nDisasters caused by nature, including floods, hurricanes,\ntornadoes, and heavy snowstorms, frequently result in\ncatastrophic outcomes. During natural catastrophes, it is\ncommon for cellular BSs and ground communications\ninfrastructure to be susceptible to compromise. In situations of\nthis nature, there exists a crucial requirement for the\nestablishment of public safety communication channels\nbetween first responders and individuals in distress, with the\nprimary objective of facilitating search and rescue endeavors.\nTherefore, it is imperative to implement a resilient,\nexpeditious, and proficient emergency communication system\nin order to facilitate efficient communication amid public\nsafety endeavors. In the context of public safety scenarios, the\nimplementation of a dependable communication system is not\nonly vital in enhancing connection, but also in potentially\nmitigating loss of life. Efficiently assessing disaster situations\nposes a formidable challenge for public safety organizations.\nIn such scenarios, where extensive radio coverage of the\naffected area is paramount, UAVs emerge as the most fitting\nsolution [65].\nThe utilization of UAV-based aerial networks as depicted\nin Figure 6 holds significant potential in facilitating rapid,\nadaptable, and dependable wireless communication in public\nsafety contexts. UAVs, due to their lack of reliance on costly\nand restrictive infrastructure such as cables, possess the ability\nto effortlessly navigate and adapt their positions. This attribute\nenables them to promptly deliver communication services to\nindividuals on the ground during emergency scenarios. In fact,\nthe distinctive attributes of UAVs, including their mobility,\nflexible installation, and instant reconfiguration enable them\nto efficiently build on-demand public safety communication\nnetworks.\nFIGURE 6. UAV application in emergency situations\nFor example, UAVs can be utilized as mobile ABSs to\nprovide broadband connection to regions that have\nexperienced disruptions in their terrestrial wireless\ninfrastructure. In addition, UAVs have the capability to\nmaintain a continuous state of motion, enabling them to\neffectively traverse an entire designated region in the shortest\nfeasible amount of time. Hence, the utilization of UAV\u0002mounted BSs presents a viable approach to ensure rapid and\npervasive connection in public safety situations.\n3) MMWAVE COMMUNICATIONS WITH 3D MIMO\nUAV operating in the mmWave spectrum represent an\nexciting frontier in wireless communication technology.\nHowever, UAV mmWave communication presents several\nsignificant challenges that need to be addressed for successful\nimplementation and reliable operation [66, 67]. On the other\nhand, UAVs equipped with mmWave communication systems\noffer impressive data rates and low latency, making them well\u0002suited for various high-bandwidth applications. The mmWave\nsignals are highly directional and require a clear LoS path\nbetween the UAV and the ground station or another UAV for\neffective communication. Obstructions like buildings, trees,\nand even minor terrain variations can disrupt LoS connections,\nlimiting the coverage area and reliability. It is challenging to\nobtain LoS with UAVs is accurate and can be attributed to\nseveral factors: propagation characteristics, propagation loss,\nmobility challenges, weather, and environmental factors.\nDespite these challenges, mmWave frequencies are still\nattractive for UAV communication in certain scenarios. They\noffer high bandwidths and data rates, which are essential for\napplications like high-definition video streaming, real-time\nsurveillance, and UAV swarming.\nIn [68], the authors focused on optimizing the 3D placement\nand orientation of UAVs to ensure LoS coverage for users\nwhile maximizing the SNR between UAV-user pairs. In\nanother work presented in [69], the researchers studied how\nLoS blockage probability affects the connectivity of UAVs in\nterrestrial urban deployments of mmWave NR systems and its\nimplications for communication reliability and performance.\nUAVs also facilitate the utilization of mmWave\ncommunications, capitalizing on LoS links with terrestrial\nusers and mitigating signal attenuation at elevated frequencies.\nThe utilization of diminutive antennas on UAVs enables the\nimplementation of sophisticated MIMO techniques, including\nmassive MIMO, for mmWave communications. Moreover,\nthe utilization of several UAVs might facilitate the\nestablishment of adaptable antenna arrays in the atmosphere,\nhence augmenting the overall efficacy of communication\nsystems [70]. The densely congested sub-6 GHz frequency\nrange falls short of fulfilling the demands for ultra-high data\ntraffic. Exploring the mmWave frequency bands emerges as a\nprospective avenue for UAV communications, as they allow\nfor the deployment of sizable antenna arrays in a compact\nspace on the UAV, enabling three-dimensional (3D)\nbeamforming capabilities [11].\nIn recent times, there has been an increasing level of\nattention towards the concept of 3D MIMO technology, which\ninvolves the utilization of both vertical and horizontal\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n13\ndimensions inside terrestrial cellular networks [71]. The\nutilization of 3D beamforming facilitates the concurrent\ngeneration of distinct beams within 3D spatial domain, thereby\nmitigating inter-cell interference and facilitating the\naccommodation of a larger user population. UAV-based flying\nBSs possess the advantageous ability to effectively discern\nground users situated at varying heights due to their elevated\naltitude. Consequently, they are highly suitable for\naccommodating 3D MIMO scenarios characterized by a\nsubstantial density of users.\nIn addition, the utilization of UAV-based wireless antenna\narrays presents distinct possibilities for airborne beamforming.\nThe array comprises individual units, each representing a\nsingle-antenna UAV. This design enables the array to have the\nflexibility to modify the spacing between its elements and\nprovide effective mechanical beam-steering in any 3D\ndirection. The flexibility and mobility exhibited by UAVs\nfacilitate the provision of effective services to terrestrial\ncustomers in both downlink and uplink situations.\n4) ENHANCING IOT CONNECTIVITY WITH UAVS\nThe rapid progress of wireless networking technologies has\nled to the emergence of a significant IoT ecosystem,\nencompassing a diverse range of devices. To unlock the full\npotential of IoT applications, including the management of\nsmart city infrastructure, healthcare systems, transportation\nnetworks, and energy management, it becomes crucial to\nestablish efficient wireless communication for a vast multitude\nof IoT devices [72]. The immense scale of IoT presents\ndistinctive obstacles that necessitate a reconsideration of\ntraditional wireless networks, such as cellular systems. In the\ncontext of IoT environments, certain key criteria assume vital\nimportance, namely energy efficiency, ultra-low latency,\ndependability, and high-speed uplink communications.\nFurthermore, the deployment of IoT devices in regions with\ninsufficient terrestrial wireless infrastructure presents\nsignificant connectivity obstacles. In [73], the authors delve\ninto the concept of smart cities that harness innovative\ntechnologies such as the IoT and UAVs to improve the\nresidents' quality of life. Mobile UAVs offer a potential option\nto effectively tackle the issues faced by IoT networks. UAVs\nhave the capability to function as airborne BSs, effectively\ncatering to IoT focused situations by delivering dependable\nand energy-efficient uplink IoT communications. Through the\nutilization of their aerial characteristics and elevated\npositioning, UAVs possess the capability to alleviate the\nnegative impacts of shadowing and obstruction. This, in turn,\nenhances the communication channel between IoT devices\nand UAVs. Consequently, IoT devices that are constrained by\nbattery capacity necessitate reduced transmit power in order to\nestablish connections with UAVs, thereby prolonging their\nbattery longevity.\nFurthermore, UAVs have the capability to adaptively adjust\ntheir positions in response to the activation patterns of IoT\ndevices. This feature enables UAVs to effectively support\nlarge-scale IoT systems, eliminating the necessity for\nsignificant development of terrestrial small cell BSs. The\nutilization of UAVs can greatly boost the connection and\nenergy efficiency of IoT networks by leveraging their distinct\nattributes, including mobility, agility, and aerial deployment.\nUAVs present a viable solution for addressing the\ncommunication obstacles encountered in the IoT domain,\nhence enabling the successful implementation of a wide range\nof IoT applications.\nThe implementation of caching mechanisms at small base\nstations (SBSs) holds significant potential in improving user\nthroughput and minimizing transmission delay. Nevertheless,\nstatic ground BSs may have difficulties in catering to mobile\ncustomers who frequently undergo handovers, as the desired\ncontent may not be accessible at the next BS. To tackle this\nissue, the utilization of UAVs as airborne BSs equipped with\ndynamic caching functionalities is suggested. This approach\naims to effectively monitor user mobility patterns and\nfacilitate the delivery of necessary content.\n5) ENHANCING WIRELESS NETWORKS WITH CACHE\u0002ENABLED UAVS\nIntegrating caching mechanisms within SBSs represents a\npromising avenue for enhancing user throughput and reducing\ntransmission latency. Numerous caching models, initially\ndeployed at different types of BSs and mobile devices, have\nbeen thoroughly investigated and are now being extended to\naerial deployment through UAVs. This adaptation addresses\nthe unique challenges posed by 6G networks [19].\nStatic ground BSs may have difficulties in catering to\nmobile customers who frequently undergo handovers, as the\ndesired content may not be accessible at the next BS. In order\nto tackle this issue, the utilization of UAVs as airborne BSs\nequipped with dynamic caching functionalities is suggested.\nThis approach aims to effectively monitor user mobility\npatterns and facilitate the delivery of necessary content. In\n[74], the authors examined the coverage performance analysis\nof a cellular network assisted by cache-enabled UAV-BS by\ndeveloping analytical models to explore both the network's\noverall coverage probability and the average achievable rate\nof cellular users.\nCache-enabled UAVs present a viable approach to alleviate\ntraffic congestion in wireless networks. By utilizing user\u0002centric data such as the distribution of content requests and\npatterns of movement, UAVs can be strategically deployed in\norder to efficiently cater to the needs of users [75]. In contrast\nto static BSs, the utilization of UAVs in caching operations\nresults in a reduction in complexity. This is due to the UAVs'\nability to monitor and analyze user mobility patterns, hence\nobviating the necessity for supplementary caching at ground\u0002based stations. The utilization of a central cloud processor\ninvolves the integration of user-centric information, which is\nderived from past user data, in order to effectively oversee the\ndeployment of UAVs and ascertain the most advantageous\nplaces and mobility patterns. This minimizes the additional\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n14\ncomputational burden associated with updating the content\nstored in the cache. Cache-enabled UAVs employ predictive\nalgorithms to anticipate user mobility patterns and content\nrequest information, thereby decreasing the need for regular\nupdates of content requests across various locations. By using\nthe caching capabilities of mobile UAVs, efficient service\ndelivery to consumers on the ground is facilitated.\nB. RADIO-BASED SENSING\nTo fully integrate UAVs into 5G-and-beyond networks, it is\ncrucial to ensure high-performance wireless communications\nand effective sensing capabilities. Currently, commercial\nUAVs are fitted with a range of embedded sensors, including\nthe inertial measurement unit, accelerometers, tilt sensors,\nand current sensors. The sensors offer instantaneous data for\nensuring the secure operation of UAVs. This includes\nproviding estimations of the UAV's location and orientation,\nmaintaining the desired flight route, and managing power\nconsumption. Nevertheless, as UAVs are increasingly\nincluded into terrestrial communication networks in\nextensive implementations, depending simply on the sensors\nimplanted inside the UAVs will become insufficient. In order\nto get the highest level of sensing performance, it is\nimperative to employ a synergistic approach that combines\nboth sensing capabilities inherent inside UAVs and sensing\ninfrastructure. This integrated strategy offers better reaction\ntime, sensing range, coverage, dependability, precision, and\nefficiency.\nAs depicted in Figure 7, the use of radio-based sensing in\nUAVs may be categorized into two primary frameworks,\nspecifically sensing conducted by UAVs and UAVs employed\nfor sensing purposes. In the aforementioned scenario, sensing\ntechnologies are employed to facilitate the secure operation of\nUAVs and to monitor and manage air traffic in low-altitude\nairspace. In contrast, the paradigm of utilizing UAVs for\nsensing involves the deployment of specialized UAVs as\nairborne platforms to offer sensing assistance from an aerial\nperspective.\n1) SENSING FOR UAV\nSensing for UAVs involves two typical use case scenarios:\nsense-and-avoid (SAA) and UAV detection, tracking, and\nclassification. SAA is essential for safe UAV flying, especially\nfor autonomous or semi-autonomous UAVs [76]. UAVs use\nsensor data to prevent collisions and obstacles without pilots.\nDue to radio propagation delays and ground pilot response\ntimes, SAA is necessary for real-time remote-controlled\nUAVs. Ground-based SAA is an alternative to on-board\nsensors for fast reactions in dynamic settings. UAV-enabled\nspraying requires sensing for constant-altitude maintenance to\nensure consistent spraying even in difficult terrain [77].\nVision- or light-based sensing makes many commercial UAVs\nwith SAA capabilities vulnerable in adverse conditions.\nAnother notable application for UAV sensing involves the\nidentification, monitoring, and categorization of potentially\nunlawful and perilous UAVs. UAVs have the potential to be\nutilized in a manner that compromises public safety and\ninfringes upon individuals' privacy. The mitigation of non\u0002cooperative or deceptive UAVs necessitates the utilization of\npassive radar sensing techniques that rely on the analysis of\nechoed or dispersed signals [78, 79]. The domain under\nconsideration presents several challenges, one of which\npertains to the limited radar cross-section exhibited by UAVs.\nThis characteristic poses difficulties in their identification, as\nit becomes arduous to differentiate them from stationary\nclutter or other airborne entities like avian species.\nCurrent research endeavors have been primarily directed\ntowards the exploration of radar sensing techniques in the\ncontext of UAV networks. The exploration of methods for the\ndetection, tracking, and interception of non-professional\nUAVs has been conducted in [80]. An exhaustive survey on\nsecurity and privacy issues of UAV was presented in [81],\nwith a comprehensive examination of UAV security issues\nconducted at four distinct levels: the software-level, the\nhardware-level, the sensor, and the communication-level.\n2) UAV FOR SENSING\nOne potential and interesting approach in the field of UAV\nsensing is the utilization of UAVs as aerial nodes to offer\nwireless sensing capabilities from an aerial perspective. This\napproach, commonly referred to as UAV for sensing, holds\nsignificant potential. When comparing UAV-based sensing to\nconventional ground sensing, it becomes evident that the\nformer offers numerous advantages. Initially, it is important to\nacknowledge that UAV-based sensing exhibits some\nadvantages due to its increased height and less signal\nblockage. Consequently, UAV-based sensing generally offers\na broader field of vision in comparison to ground sensors.\nMoreover, the exceptional level of control over the three\u0002dimensional movement of UAVs enables the flexible\ndeployment of UAV sensors in challenging and inaccessible\nenvironments, including regions that are toxic or pose\nsignificant risks. In addition, the increased maneuverability of\nUAVs presents a novel opportunity for enhancing sensing\nperformance through the optimization of 3D sensor\ntrajectories. The aforementioned feature holds significant\nappeal in the context of target tracking, as it allows for the\ndynamic adjustment of UAV placements in order to optimize\ntarget tracking capabilities. Hence, the utilization of UAV for\nsensing purposes exhibits a diverse array of prospective\napplications, including but not limited to law enforcement,\nprecision agriculture, 3D environmental mapping, search and\nrescue operations, and military endeavors.\nUAV-based sensing has gained increasing attention due to\nits numerous advantages. Notable applications of UAV-based\nsensor platforms have been outlined in [77], including\nexperiments with an imaging radar. The use of UAVs for\nremote sensing in field-based crop phenotyping was surveyed\nin [82].\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n15\n(a) Sensing for UAV (b) UAV for Sensing\nAdversary UAV\nMonitoring UAV\nAdversary UAV\nSensing and Avoid\nFIGURE 7. Models of sensing in UAV networks: (a) sensing for UAV and (b) UAV for sensing\nIn [83], the authors explored UAV-aided air quality\nsensing. Furthermore,[84] introduced a dynamic and\nreconfigurable aerial radar network composed of UAVs to\ndetect and track unauthorized/malicious UAVs. The study\ndemonstrated that optimizing UAV trajectories offers\nadditional degrees of freedom, leading to improved tracking\nperformance compared to conventional terrestrial radar\nnetworks with fixed deployment.\nC. CELLULAR-CONNECTED UAVS\nThe utilization of cellular-connected UAVs has emerged as\na possible avenue for expanding cellular networks, as UAVs\ncan function as both aerial UE and ABSs. The utilization of\nUAVs in these specific functions offers distinct possibilities\nfor augmenting network coverage, capacity, and the\navailability of services. Nevertheless, additional research is\nnecessary to address the technological obstacles associated\nwith handover management, channel characterization, and\ninterference reduction in order to achieve seamless\nintegration of UAVs into the current cellular infrastructure.\nOngoing research and development efforts in this particular\ndomain have the potential to bring about a significant\ntransformation in communication networks and effectively\nhandle connectivity requirements across diverse situations\nthrough the utilization of cellular-connected UAVs.\nStudies on the use of UAVs in cellular networks have\nrecently gained momentum. Integration into current\napplications brings the problems related to the feasibility of\nUAVs to discussion and paves the way for new research areas.\nAuthors in [85] pointed out flying at different altitudes and\nhaving different operating bands as one of these new\nfeasibility problems. For examining the effects of these\nfactors, they used experimental measurements made in the\nfield and discussed the positive and negative aspects of UAV\nuse in 4G and future 5G applications. They emphasized that\ncarried out measurements show the need for further\nimprovements in cellular-connected UAVs. The same\nproblem was discussed for a carrier frequency of 1800 MHz\nand 2100 MHz, and various altitudes from 15 m to 100 m by\nthe same authors in [86]. As a result of these trials, they also\naddressed the importance of other factors such as antenna\ntilting and 3D coverage models. A study focusses on the\nperformance of different antenna deployments for cellular\u0002connected UAV scenarios can be found in [87]. Another study\nthat considers three-dimensional mobility and LoS channel\nproperties for cellular-connected UAVs is presented by [88].\nThey defined the essential challenges for them and introduced\nthe key performance indicators with analytical models by\nconsidering several performance outputs. Finally, ML solution\nwas proposed for the optimization problem that they\nconstituted. A network planning approach that comprises both\nUAV users and UAV BSs in a three-dimensional medium was\nintroduced by [36]. They claimed that their approach reduces\nthe average latency by 46% compared with the conventional\nsignal-to-interference-plus-noise ratio-based cell association.\nThe proposed approach also provides an improvement in\nspectral efficiency.\nThe present study investigates the potential utilization of\nUAVs as aerial UE and ABSs inside cellular networks.\nSpecifically, the focus is on exploring the integration of\ncellular connectivity in UAVs, thereby enabling them to\nfunction as mobile devices and serve as communication hubs\nin the sky.\n1) UAVS AS AERIAL USER EQUIPMENT\nUAVs have recently been developed as a distinct type of\naerial UE within the context of cellular networks. UAVs\nserve as airborne UE, functioning as communication devices\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n16\nwith the ability to establish cellular connectivity. The\nintroduction of this technology presents numerous benefits,\nincluding improved mobility and the capacity to utilize\ncellular services in geographically distant or temporarily\ndisconnected regions. UAVs serve as airborne UE,\nfacilitating real-time communication for diverse purposes\nsuch as aerial surveillance, disaster response, and\nenvironmental monitoring. Nonetheless, the incorporation of\nUAVs into cellular networks gives rise to various technical\nobstacles, including the need for effective management of\nhandovers, accurate characterization of aerial channels,\ncyber-physical threats, energy consumption, and\nauthentication purposes, and the potential influence of UAV\nmobility on the overall performance of the network. In order\nto tackle these complex issues, the academic community is\nactively engaged in the development of computational and\nalgorithmic solutions to these issues.\nThe influence of the implementation of aerial UEs on the\noverall network efficiency was analyzed by the authors in [89].\nThe simulation findings indicate that the substitution of a\nground user with UAV operating at an altitude of 100 meters\nresulted in a tenfold reduction in throughput. Additionally, the\ncoverage was reduced from 76% to 30%.\nHence, in order to facilitate a satisfactory integration of\nUAVs into cellular networks, the authors in [89] developed a\nmethod that aims to achieve optimal tilting of the directional\nantennas in UAV UEs, thereby enabling their operation at\nelevated altitudes. The simulation findings indicate a\nsignificant improvement in coverage, with an increase from\n23% to 89%. Additionally, the throughput has also shown a\nnotable enhancement, rising from 3.5 b/s/Hz to 5.8 b/s/Hz. The\nsuggested technique exhibits advantages in sparse and\nmoderately dense cellular networks, while presenting\ndisadvantages in highly dense cellular networks.\nThe ideal inter-cell interference coordination technique and\nair-ground performance trade-off were examined by the\nauthors in [90]. The researchers achieved the highest possible\nweighted sum-rate for both the ground users and the UAV by\ncollectively optimizing the uplink cell connections and power\nallocations across various resource blocks. The researchers put\nup a centralized strategy for coordinating inter-cell\ninterference in order to achieve an optimal solution at the local\nlevel. Additionally, they offered a decentralized scheme\nwherein the cellular BSs are divided into clusters, allowing for\ndata exchange solely between the cluster-head and the UAV.\n2) UAVS AS ABS\nThere has been an increasing interest in the utilization of\nUAVs as ABSs within cellular networks in recent times.\nUAVs function as ABSs, operating as mobile\ncommunication nodes that can be strategically positioned to\noptimize network coverage and capacity. ABS technology\npresents a novel approach to enhancing cellular connectivity\nin regions with inadequate infrastructure or on occasions\nnecessitating temporary network reinforcement. The\nutilization of UAVs in aerial deployment facilitates\nenhanced LoS circumstances, resulting in a reduction of\nsignal blockage and shadowing effects. This, in turn, can\ncontribute to an enhancement in communication\nperformance. Nevertheless, it is imperative to tackle certain\nobstacles in order to fully capitalize on the capabilities of\nUAVs as ABSs. These problems encompass the efficient\nplacement of UAVs, seamless coordination with pre-existing\nTBSs, power usage, and effective management of potential\ninterference.\nThe authors in [91] introduced a PLS technique designed to\nenhance the security of wireless access provided by numerous\nUAV-BSs to ground UEs. The researchers examined a cohort\nof individuals who engage in eavesdropping activities with the\nintention of disrupting the transmission of data from UAV\u0002BSs to the ground UEs. The developed technique aims to\nachieve maximum secrecy rate by simultaneously optimizing\nthe transmit beamforming and the power consumption of the\nUAV. The difficulty of enhancing network performance and\ncoverage in the deployment of UAV-BSs was addressed by\nthe authors in [92]. In order to address these concerns, the\nresearchers have developed a proposal for the deployment of\nUAV-BSs using a UAV-artificial bee colony method. The\nalgorithm that has been proposed has the capability to\nascertain the most suitable flying location for each UAV-BS\nin order to achieve the highest possible network throughput.\nThe simulation findings demonstrate that the suggested\nmechanism exhibits superior performance compared to some\nbio-inspired algorithms in terms of enhancing network\nthroughput and improving the coverage rate of UE.\nD. UAV-BASED FLYING AD-HOC NETWORKS\nThe idea of flying ad-hoc networks has become increasingly\nsignificant in the context of 5G and subsequent generations\nof wireless communication technology. Flying ad-hoc\nnetworks (FANETs) are a notable application of UAVs,\nwherein numerous UAVs establish communication linkages\nin an ad-hoc fashion [93, 94]. FANETs play vital roles in a\nwide range of applications, such as traffic monitoring,\nremote sensing, border surveillance, disaster management,\nagricultural management, wildfire management, and relay\nnetworks [95]. FANETs play a significant role in\nestablishing dependable communication connections\nbetween far transmitters and receivers that encounter barriers\nor considerable spatial separation, hence enabling\nuninterrupted connectivity in demanding settings.\nThe utilization of many small UAVs in FANETs offers\nseveral advantages. In contrast to single UAV operations, the\nutilization of FANETs comprising several tiny UAVs has\nsome noteworthy advantages [96].\n\u2022 Scalability: FANETs demonstrate notable scalability,\nfacilitating the seamless enlargement of their operating\ncoverage through the integration of additional UAVs and\nthe implementation of efficient dynamic routing strategies.\nThe scalability of the network enables it to effectively\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n17\nserve a wider geographical area and accommodate\nincreased communication requirements.\n\u2022 Cost Efficiency: The cost associated with deploying and\nmaintaining small UAVs is relatively lower as compared\nto larger UAVs that are equipped with intricate gear and\nsubstantial payloads. The cost-effectiveness of small\nUAVs enables the efficient extension and operation of\nnetworks at a reduced cost.\n\u2022 Enhanced Survivability: One of the advantages of\nFANETs is the improved ability to sustain operations in\nchallenging conditions or in the event of a UAV\nmalfunction. In such situations, the missions can go\nseamlessly by employing the remaining operating UAVs.\nThe improved ability to survive in challenging conditions\nguarantees the uninterrupted execution of missions and the\nresilience of the network, a quality that is lacking in\nindividual UAV systems.\nE. SATELLITE NETWORKS WITH UAVS\nSatellite networks have been utilized for several decades to\noffer a range of services, encompassing Earth observation,\nremote sensing, and satellite communication. The\nconventional architecture of satellite systems, though widely\nused, has certain limitations in terms of cost, reconfigurability,\nand real-time data provision. The incorporation of UAVs into\nsatellite networks offers a potential resolution to these\nobstacles. UAVs have the capability to serve as aerial\nplatforms for the deployment and maintenance of satellite\npayloads. This utilization of UAVs results in reduced costs\nassociated with satellite launches and improved\nreconfigurability of satellite constellations. UAVs have the\npotential to function as relays or data collection nodes for\nsatellites, thereby enhancing the data gathering and\ndistribution capabilities of satellite systems [97].\nIn this particular context, the authors in [98] developed an\nidea regarding the utilization of UAVs and satellites as a\nmeans to facilitate the integration of a substantial quantity of\nIoT solutions within the 5G framework. The proposed\nmethodology utilizes satellites as intermediary nodes, UAVs\nas 5G-UE, and the 5G-gNB is placed on the Earth's surface.\nThe suggested methodology enables a solution of challenges\nassociated with terrestrial infrastructure, such as the\nincreased concentration of IoT devices and limited coverage\nareas.\nThe study conducted by researchers [99] examined a\ncooperative network consisting of many antennas, UAVs,\nand satellites, with the aim of offering uninterrupted access\nto consumers. UAVs are employed as aerial relays to\nfacilitate the transmission of signals between satellites and\nmany ground users. This communication process is\nfacilitated through the implementation of an amplify-and\u0002forward protocol. The researchers derived the expression for\nthe maximum SNR output of the suggested architecture\nusing an opportunistic user scheduling methodology. The\nexpression for the outage probability of the proposed design\nwas derived based on the SNR expression. The equation for\noutage probability is utilized in the context of high SNR to\nascertain the coding gain and diversity order of the examined\ndesign.\nThe authors in reference [100] have presented a safe\ntechnique for relaying UAVs in a hybrid network consisting\nof both terrestrial and satellite components, while\nconsidering the potential threat of an eavesdropper. Three\nUAV Relay Selection (URS) approaches were investigated,\nwhich were based on the Closest (CURS), Maximum\n(MURS), or Uniform (UURS) SNR. Subsequently, an\nassessment was conducted to determine the effects of\ndifferent tactics on the hybrid network, specifically in\nrelation to the risk of secrecy outage. The simulation findings\nindicate that the secrecy performance of MURS is superior,\nwhereas the secrecy performance of UURS is comparatively\ninferior. Additionally, the CURS and UURS techniques do\nnot improve the secrecy diversity order.\nF. OTHER POTENTIAL UAV APPLICATIONS\n1) SURVEILLANCE AND MONITORING\nUAVs assume a paramount role in various surveillance and\nmonitoring applications, rendering valuable contributions\nacross diverse domains [101]. In border and coastal\nsurveillance, UAVs provide an indispensable tool for real\u0002time monitoring of vast and challenging terrains, bolstering\nborder security and maritime operations. Equipped with\nadvanced imaging and sensing technologies, UAVs offer an\nunmatched perspective, enabling researchers and\nconservationists to conduct wildlife studies, assess\nenvironmental changes, and monitor protected areas with\nheightened accuracy. This capability proves invaluable in\npreserving biodiversity and facilitating effective\nenvironmental conservation efforts.\nAdditionally, UAVs herald a transformative shift in\ninfrastructure inspection and maintenance practices. By\nreducing the need for labor-intensive manual inspections,\nUAVs optimize resource utilization and operational\nefficiency. With their ability to capture high-resolution\nimagery and conduct aerial surveys, UAVs empower\nengineers and infrastructure managers to perform detailed\nassessments of structures, utilities, and assets. This enhanced\ndata collection allows for proactive identification of potential\nissues, leading to timely maintenance and cost-effective asset\nmanagement. The integration of UAVs in surveillance and\nmonitoring operations epitomizes their potential to\nrevolutionize critical sectors, bolstering safety, efficiency, and\nprecision in safeguarding environments, structures, and\nresources [71].\n2) COMMUNICATION RELAY\nUAVs play an indispensable role as communication relays,\nparticularly in the face of natural disasters and emergencies.\nWhen terrestrial communication infrastructure is\ncompromised or rendered inaccessible, UAVs swiftly step in\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n18\nto restore critical connectivity [102]. In disaster-stricken areas,\nwhere communication lines may be disrupted, UAVs serve as\nlifelines, relaying essential information and facilitating timely\nrescue and relief efforts. These aerial relays bridge the\ncommunication gap, enabling affected communities to\nconnect with emergency responders and access vital\nresources.\nBeyond disaster scenarios, UAVs extend communication\nservices to remote and underserved regions, overcoming\ngeographical barriers that hinder conventional connectivity. In\nremote and challenging terrains, where establishing and\nmaintaining terrestrial communication networks is\nimpractical, UAVs serve as the much-needed link,\nempowering communities with access to information,\neducation, and essential services. This role is particularly\nsignificant in remote rural areas and isolated communities,\nwhere connectivity fosters socio-economic development and\nempowers individuals with knowledge and resources [103].\nMoreover, UAVs prove instrumental in industrial settings\nwhere communication infrastructure may be limited or non\u0002existent. In remote industrial operations such as mining, oil\nand gas exploration, or infrastructure development, UAVs act\nas communication relays, enabling real-time data transfer and\ncoordination among workers and management. This enhances\noperational efficiency, safety, and remote monitoring\ncapabilities, optimizing industrial processes and minimizing\ndowntime. In emergency response scenarios, such as search\nand rescue missions or medical aid delivery, UAVs serve as\nvital communication relays, facilitating seamless coordination\nbetween on-ground teams and command centers.\n3) INTERNET OF THINGS (IOT) SUPPORT\nUAVs play a crucial function in facilitating the IoT by offering\na wide array of applications. UAVs serve as data collection\nnodes, functioning as aerial sensors to acquire significant data\nfrom distant and demanding locations, catering to a range of\nIoT applications. In the field of precision agriculture, UAVs\nthat are equipped with specialized sensors play a crucial role\nin assisting farmers with the monitoring of crops, analysis of\nsoil conditions, and optimization of agricultural techniques\n[104]. These technological advancements aim to improve\nproductivity and optimize resource management in the\nagricultural sector. Moreover, UAVs that are equipped with\nenvironmental sensors play a significant role in the\nimplementation of smart city applications. These UAVs are\ncapable of monitoring various environmental parameters such\nas air quality and noise levels [105]. The data collected by\nthese sensors is crucial for urban planning and the efficient\nmanagement of cities. The capabilities of UAVs that support\nthe IoT demonstrate their considerable capacity to transform\ndata collection and improve decision-making in several fields.\n4) SWARM INTELLIGENCE\nThe concept of swarm intelligence has revolutionized the\ncapabilities of UAV networks, propelling them towards\nunprecedented levels of efficiency, adaptability, and\ncollaborative potential. Within the realm of UAV swarms,\nseveral key aspects stand out, showcasing their transformative\nimpact on various applications, thus including [106]:\n\u2022 Collaborative Sensing and Mapping: UAV swarms\ndemonstrate exceptional prowess in data collection and\nmapping tasks. By working together cohesively, these\nswarms enable collaborative sensing, efficiently covering\nvast areas and generating accurate 3D maps. This\ncapability finds valuable application in diverse fields,\nincluding land surveying, environmental monitoring, and\ndisaster assessment. Case studies exemplify the swarms'\nefficiency in mapping and monitoring large-scale\nenvironments, revealing their potential in expediting data\nacquisition and enhancing situational awareness in critical\nscenarios as in [107].\n\u2022 Distributed Task Execution: A defining strength of UAV\nswarms lies in their ability to efficiently distribute tasks\namong individual UAVs. In search and rescue operations,\ndisaster response, and emergency medical support, the\ncoordination and resource allocation of UAV swarms lead\nto heightened efficiency and rapid response times. By\ndividing complex tasks into smaller sub-tasks, UAV\nswarms leverage their collective intelligence,\naccomplishing missions that would be arduous or\nimpractical for a single UAV [108].\n\u2022 Adaptive and Resilient Networks: Swarm intelligence\nempowers UAV networks with a remarkable degree of\nadaptability and resilience. In dynamic and unpredictable\nenvironments, such as disaster-stricken areas or regions\nwith limited communication infrastructure, UAV swarms\ndemonstrate their capacity to swiftly reroute\ncommunication paths, ensuring the establishment of robust\nand reliable communication links. This adaptability\nenables UAV swarms to maintain seamless connectivity in\nthe face of challenges, further enhancing their suitability\nfor critical operations. Case studies exemplify the\neffectiveness of UAV swarm networks in maintaining\ncommunication under challenging conditions, solidifying\ntheir position as a reliable communication backbone in\nadverse scenarios as in [109].\nThe integration of swarm intelligence in UAV networks has\nunlocked new frontiers of efficiency and cooperation. UAV\nswarms exemplify a synergy of collective intelligence,\nenabling collaborative data sensing, agile task execution, and\nadaptive networking. Their capacity to tackle complex\nchallenges, provide comprehensive coverage, and respond\nrapidly to dynamic situations positions UAV swarms as a\nformidable force across a wide spectrum of applications. As\nresearch and development in swarm intelligence advance,\nUAV networks are poised to redefine the boundaries of aerial\ncapabilities, opening up new possibilities in fields ranging\nfrom disaster response and environmental monitoring to\ninfrastructure maintenance and beyond.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n19\nV. EXPLORING EMERGING TECHNOLOGIES\nUAVs have emerged as versatile platforms with vast\npotential in the field of wireless communication. Over the\nyears, significant advancements in communication\ntechnologies have been leveraged to enhance UAV\ncapabilities, enabling them to play critical roles in various\napplications. In this section, the recent technological\nbreakthroughs that have revolutionized UAV\ncommunications are explored. These advancements focus on\nsupporting enhanced mobile broadband (eMBB) services\nand enabling seamless communication for massive machine\u0002type communication (mMTC). Specific topics such as\nmassive MIMO (M-MIMO) and mmWave technologies,\nintelligent reflecting surfaces (IRS), non-orthogonal multiple\naccess (NOMA), energy harvesting, energy-efficient\ndesigns, and the integration of artificial intelligence (AI) will\nbe delved into. Through an in-depth analysis of these cutting\u0002edge technologies, the aim is to understand their\nimplications, benefits, and challenges for UAV\ncommunications. This knowledge will equip with the tools\nneeded to develop efficient and robust communication\nsystems for UAVs, leading toward a future where UAVs are\nseamlessly integrated into daily life, industries are\ntransformed, and wireless communication is revolutionized.\nA. M-MIMO AND mmWave\nM-MIMO is a crucial technology in the current 5G\nstandard, showing promise for supporting cellular-connected\nUAV communications [110, 111]. Large arrays at ground\u0002based BSs enable fine-grained 3D beamforming, reducing\ninterference between high-altitude UAVs and low-altitude\nterrestrial users, resulting in higher network throughput.\nHowever, accurate channel state information at ground BSs\nis essential for effective M-MIMO beamforming, presenting\nchallenges with cellular-connected UAVs. UAVs introduce\ncomplexities to the M-MIMO system. Their strong LoS\nchannels cause severe pilot contamination across ground\nBSs, not resolved by conventional decontamination\ntechniques for terrestrial users. Efficient beam tracking is\nchallenging due to UAVs' high mobility in 3D space, leading\nto excessive pilot overhead [112]. The application of hybrid\nbeamforming-based M-MIMO in practical scenarios can\nfacilitate the coordination of UAV groups or swarms.\nHowever, this approach introduces additional challenges\nrelated to pilot contamination and beam tracking [113].\nLeveraging LoS-dominant air-ground channels, it offers\nmore degrees of freedom for macro diversity. However,\nchallenges remain, including efficient power control, low\u0002complexity fronthaul/backhaul provisioning, and network\nscalability for UAV swarms [114].\nFurther, an alternative approach to support rate\u0002demanding eMBB services in 3D space involves utilizing the\nabundant spectrum available in the mmWave bands [92].\nAlthough mmWave communications have inherent\nlimitations like signal attenuation and vulnerability to\nblockage, UAV mobility can mitigate these challenges.\nUAVs, whether acting as aerial platforms or users, can adjust\ntheir trajectories intelligently to reduce propagation loss by\nmoving towards ground nodes and bypass obstacles to\nincrease LoS paths. However, like M-MIMO, mmWave\ncommunication requires a large number of antennas, and the\nhigh UAV mobility and shorter wavelength signals lead to\nfast channel variations. As a result, effective dynamic beam\ntraining and tracking techniques become crucial. Existing\nworks propose the use of movement prediction filters, like\nKalman filters, to track time-varying UAV-ground channels\n[115]. Future research should also address low-complexity\nspectrum management [116] and high-speed, reliable\nbackhaul design as important topics in this domain.\nB. INTELLIGENT REFLECTING SURFACE (IRS)\nAlthough M-MIMO and mmWave communications provide\npotential benefits, their actual application is hindered by\ndifficulties such as high complexity, hardware expense, and\nhigher energy consumption [117]. IRS have recently\nemerged as a viable and economically efficient approach to\nimprove received power and mitigate A2G interference in\nthree-dimensional space [118]. IRS is composed of passive\nreflecting elements, each possessing a configurable\nreflection coefficient. This characteristic allows for\nintelligent coordination of reflections, facilitating the\nreconfiguration of the wireless channel. The integration of\ndesired signals and interference cancellation in a coherent\nmanner leads to a substantial increase in communication\nthroughput, without the need for additional active BSs or.\nrelays. In addition, it should be noted that IRSs possess\npractical advantages such as their lightweight nature, which\nenables convenient deployment on walls or high-speed\nmoving vehicles, hence facilitating a wide range of\napplications [119]. IRS serves as a revolutionary technology\nthat effectively converts the radio environment into an\nintelligent one, hence yielding significant advantages for\nvarious industries such as transportation, manufacturing, and\nsmart cities. The use of IRS has garnered attention as a\nprospective technology for the development of 6G networks.\nExtensive research has been conducted on IRS in several\nsystem configurations, as seen by studies referenced in\nsources [120, 121]. The device has the capability to be\ninstalled on land in order to facilitate communications for\nUAVs (see Figure 8), or it can be affixed to UAVs for\ncommunications on the ground, as depicted in Figure 9\n[122]. Table 4 presents a comprehensive overview and\ncomparative analysis of prior research efforts pertaining to\nthe domains of IRS and UAV.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n20\nTABLE 4\nCOMPARISON OF CURRENT STUDIES ON IRS AND UAV\nFIGURE 8. IRS optimized UAV communications\nFIGURE 9. Improving terrestrial communications using UAV-IRS\nC. NON-ORTHOGONAL MULTIPLE ACCESS (NOMA)\nNOMA has garnered significant attention for its application\nin supporting machine-type devices (MTD). NOMA allows\nmultiple devices to share the same time-frequency resources\nsimultaneously using power domain multiplexing. This\ntechnology is particularly beneficial for the massive\nconnectivity requirements of the IoT and machine-type\ncommunications (mMTC), as it efficiently manages\nresources, increases capacity, and lowers latency. UAVs can\nleverage NOMA to connect and manage a large number of\nmachine-type devices efficiently, making it ideal for\napplications such as remote sensing, environmental\nmonitoring, and automated surveillance [131]. NOMA\nemploys the utilization of superposition coding at the\ntransmitters and successive interference cancellation at the\nreceivers in order to attain effective access and partially\nalleviate co-channel interference. Research has\ndemonstrated that NOMA has notable efficacy in scenarios\nwhen consumers encounter significantly disparate channel\ncircumstances [132].\n1) ADVANCING CELLULAR-CONNECTED UAVS WITH\nNOMA\nNOMA confers pragmatic merits within cellular networks\nhosting both coexisting UAV and ground users. It facilitates\nthe reutilization of resource blocks by UAVs, concurrently\naccommodating a greater number of aerial users, particularly\nin densely populated scenarios, in contrast to the non\u0002scalable orthogonal multiple access (OMA). The inclusion of\nLoS links engenders A2G communications of heightened\nreliability, surpassing that of non-LoS terrestrial channels.\nThe robust LoS air-ground pathways also empower UAVs to\nbe concurrently visible to multiple ground BSs, thereby\nattaining an elevated macro-diversity advantage for user\naffiliation. Nevertheless, the deployment of NOMA within\nsuch air-ground contexts presents its own set of challenges.\nUplink transmissions from UAVs have the potential to\nnotably impair signals from ground users across several\nground BSs, thereby constraining the performance gain of\nNOMA over OMA, particularly in scenarios marked by\nunfavorable channel conditions emanating from individual\nBSs. A resolution to this conundrum materialized in the form\nof a decode-and-forward (DF)-oriented collaborative\nNOMA strategy, which harnessed interference cancellation\nmechanisms among neighboring BSs interconnected by\nbackhaul links [133]. This particular strategy attains\namplified data rates compared to OMA and non\u0002collaborative methodologies, a distinction that becomes\nIRS Use Case No. of IRS Design Objective Approach Ref.\nTerrestrial IRS\nSingle Maximizing the rate successive convex approximation (SCA) [123]\nSingle Power reduction successive convex approximation, and Lagrange duality [124]\nMultiple Achieve maximum weighted rate Reinforcement Learning [125]\nMultiple Enhance received power successive convex approximation [126]\nMultiple Minimizing BER Penalty based algorithm [127]\nUAV-IRS\nSingle Maximum SNR with the lowest\npossible Two-step method [122]\nSingle Maximizing the rate Reinforcement learning [128]\nSingle Optimizing energy efficiency\nsecurity SCA [129]\nSingle Maximizing energy efficiency Fractional programming [130]\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n21\nmore pronounced during congested ground traffic\ncircumstances. Further refinements were introduced through\na quantize-and-forward-oriented cooperative interference\ncancellation schema [134], wherein proximate BSs quantize\nreceived UAV signals sans decoding them.\nIn the downlink phase, UAV receivers confront robust co\u0002channel interference stemming from multiple ground BSs.\nThe conventional methods of interference alleviation utilized\nin the uplink are inapplicable due to the transformation of\nUAVs' roles from sources of interference to recipients. An\napplicable strategy is cooperative beamforming, where non\u0002serving BSs collaborate in transmission to enhance the\nreceived power at UAVs and surmount co-channel\ninterference. Nevertheless, its efficacy wanes with the\nescalation of ground user density, thereby circumscribing the\npool of available BSs for cooperative beamforming\nendeavors. To surmount this predicament, a novel\ncooperative beamforming schema integrating interference\ntransmission and cancellation (ITC) was introduced in [135].\nThis schema involves the forwarding of signals from\nterrestrial users who share the same resource block as the\nUAV to the BSs catering to the UAV. Subsequently, these\nsignals, alongside the UAV's signals, are dispatched through\ncooperative beamforming. This results in the augmentation\nof desired signal potency at the UAV's receiver, concurrently\nquelling terrestrial interference sans exerting any influence\non existing transmissions. While the centralized\nimplementation of this approach involves extensive\nbackhaul transmissions among different BSs, a distributed\nalgorithm based on the concept of divide-and-conquer was\nconducted in [135]. The distributed design requires only\nlocal information exchange among BSs, reducing\nimplementation complexity and signaling overhead.\nResearch results show that the distributed design\nsignificantly improves UAV performance compared to\nconventional schemes without ITC, particularly in high\u0002density terrestrial user scenarios [135]. However, supporting\nmassive UAVs or UAV swarms remains a challenging\nproblem that requires further exploration [136].\n2) NOMA WITH UAV\nUAVs, operating as BSs, can efficiently exploit the varying\nchannel conditions of different ground devices in order to\nmaximize the potential performance advantages offered by\nNOMA. In a previous study, researchers in [137] examined\nthe utilization of UAVs for NOMA transmissions involving\ntwo stationary ground users. The capacity region was\ndetermined by simultaneously optimizing the trajectory of\nthe UAV and the allocation of transmit power/rate over time,\ntaking into account practical limitations such as the\nmaximum speed and transmit power of UAVs. The findings\nindicate that NOMA exhibits superior performance\ncompared to OMA, which includes time division multiple\naccess (TDMA) and frequency division multiple access\n(FDMA). Additionally, the capacity advantage of NOMA\nover OMA diminishes as the maximum speed and/or flight\nduration of UAVs increase. The comparison between two\u0002user NOMA and OMA for UAV-assisted communication\nwas expanded to include other design objectives, such as\nsum-rate [138] and outage probability. The study examined\na Rician air-ground channel conducted in [139] , where a\nUAV followed a circular trajectory at a constant speed. The\ngoal was to reduce the probability of outages, and criteria for\nthe superiority of NOMA over TDMA were established\nusing channel and UAV trajectory parameters.\nIn the context of a multi-user environment, it is imperative\nto establish appropriate user pairing and allocate bandwidth\neffectively in order to fully exploit the capabilities of\nNOMA. Ref. [140], the UAV paired with one user in close\nproximity (cell-centered) and one user at a greater distance\n(cell-edge). Subsequently, the problem of multi-user rate\nmax-min optimization was formulated by simultaneously\noptimizing the allocation of bandwidth, power, UAV height,\nand antenna beamwidth. In subsequent literature, the\napplication of NOMA was further expanded to encompass\nnetworks with multiple antennas and numerous UAVs. For\ninstance, the study conducted in [141] focused on the\ndownlink transmission from a multi-antenna UAV to many\nclusters of ground users. The researchers generated\nanalytical formulas for both the outage probability and the\nergodic rate. The exploration of several UAVs in a large\u0002scale cellular network was further expanded upon in [142].\nIn the study [143], the utilization of the user angle was\nemployed as a means of feedback information for mmWave\nNOMA communications. In circumstances involving multi\u0002antenna transmission, angle information has been\ndemonstrated to possess great potential in enhancing the\nseparation NOMA users in the power domain, as compared\nto the typical limited feedback system that relies solely on\ndistances of users. The study conducted in [144] focused on\nthe resource allocation problem inside a multi-UAV aided\nIoT NOMA uplink transmission system. The objective was\nto optimize the channel assignment, uplink transmit power,\nand flying heights of UAVs in order to maximize the system\ncapacity.\nD. ENERGY EFFICIENCY WITH INNOVATIVE DESIGNS\nEnergy harvesting and energy-efficient designs play a vital\nrole in supporting machine-type devices onboard UAVs.\nMachine-type devices, which are often resource-constrained\nand battery-powered, require efficient energy management\nfor extended operation. UAVs can integrate energy\nharvesting techniques, such as solar cells or kinetic energy\nharvesting, to recharge or supplement the power of\nconnected devices. Additionally, employing energy-efficient\ncommunication protocols and hardware design minimizes\npower consumption, prolonging the battery life of machine\u0002type devices. These advancements enable UAVs to\neffectively manage energy resources and ensure the\nsustained operation of connected devices for prolonged\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n22\nmissions.\nUAVs offer a promising solution to overcome these\nconstraints. In the initial study [145],\nfocused on examining a novel UAV-enabled wireless power\ntransfer system, in which a UAV-mounted energy\ntransmitter is utilized to wirelessly charge distributed energy\nreceivers situated on the ground. Specifically, we delve into\na fundamental scenario involving two users and explore how\nthe UAV can best utilize its mobility through trajectory\nplanning to maximize the energy transferred to both energy\nreceivers within a defined charging timeframe. In [146], the\nauthors introduced a novel wireless power transfer system,\nfacilitated by an UAV equipped with a mobile energy\ntransmitter. The system involves dispatching the UAV to\nprovide wireless energy to a group of energy receivers\nsituated at predetermined ground locations. The research\nexamines the optimal utilization of UAV mobility through\ntrajectory planning to maximize the energy delivered to all\nenergy receivers (ERs) within a finite charging duration.\nAn UAV-enabled wireless power transfer network is\ninvestigated in [147], in which a UAV operates at a\nconsistent altitude in the sky to deliver wireless energy to a\nlinearly arranged set of ground nodes. The primary aim is to\nenhance the minimum received energy for all ground nodes\nby optimizing the UAV's one-dimensional path while\nadhering to the maximum UAV flying speed limitation.\nTo illustrate, the pursuit of system energy amplification\nwas explored in [148], while [149] delved into the\nmaximization of max-min throughput. Broadening the\nscope, [150] extended the optimization endeavors to\nencompass a two-user interference channel housing two\nUAVs. Venturing into advanced methodologies, [151]\nintroduced multi-agent deep reinforcement learning as a\nmeans to grapple with the overarching max-min optimization\nquandary inherent in multi-UAV-enabled WPCNs. More\nrecently, the concept of UAV-empowered simultaneous\nwireless information and power transfer (SWIPT) has been\nintroduced, wherein the UAV undertakes the role of an aerial\nBS for the dual transmission of information and energy to\nterrestrial recipients [152, 153]. The sphere of optimization\ninquiries has embraced factors such as UAV trajectory,\ntransmission power, and the power splitting ratios of users.\nThese considerations are geared towards the maximization\nof achievable rates within the confines of energy harvesting\nconstraints. Furthermore, within the purview of IoT\napplications pertinent to emergency communications, [153]\nfurnishes an encompassing overview of UAV-facilitated\nSWIPT.\nE. ARTIFICIAL INTELLIGENCE INTEGRATION IN UAV\nThe integration of AI in UAV communication systems has\nshown remarkable potential for enhancing efficiency and\nintelligence. AI techniques, such as ML and deep\nreinforcement learning, can be leveraged to optimize UAV\ntrajectory planning, resource allocation, and signal\nprocessing. These approaches enable UAVs to dynamically\nadjust communication parameters for optimal network\nperformance and sensing services. Enormous recent surveys\npapers have been published that mainly focused on AI/ML.\nSeveral surveys discussed UAV communications with ML\ne.g., [154-156]. The survey in [157] explored the training of\nML models across a collection of geographically dispersed\nclusters of resource-limited devices using swarms of UAV.\nIn [158],the survey offered a current and extensive\noverview of ML techniques applied in UAV operations and\ncommunications while identifying areas of potential\nexpansion and research voids. A few general surveys\ndiscussed ML techniques for UAV trajectory in terms of\noptimization [159], planning [160].\nIn UAV-aided communication scenarios, AI-driven\nbeamforming and power control can optimize flight paths in\nreal-time based on wind patterns, detect and avoid obstacles,\nmanage battery usage, while intelligent spectrum\nmanagement can mitigate interference and improve overall\nsystem performance. Additionally, AI-based algorithms can\nenhance UAV sensing capabilities for applications such as\ntarget tracking, environmental monitoring, and disaster\nresponse. The synergy of AI and UAV technologies opens\nup new horizons for advanced communication and sensing\nservices.\nThe integration of AI is a pivotal element in the\ndevelopment of forthcoming cellular networks, playing a\nsignificant role in the achievement of network intelligence\n[161]. The integration of AI technology has given rise to two\nsignificant paradigms in the field of wireless\ncommunications: AI-empowered wireless communications\n[162] and edge intelligence [163]. AI and ML techniques are\nutilized in the former to optimize wireless systems and\nimprove communication performance. This departure from\nconventional model-driven approaches emphasized the use\nof data-driven mathematical techniques. In the\naforementioned framework, the integration of AI and MEC\ncapabilities occurs within BSs and access points located at\nthe network edge [164]. This integration facilitates the\ndeployment of intelligent applications that demand\nsignificant communication and computation resources,\nincluding autonomous driving and industrial automation\n[163]. Edge intelligence provides a reduction in end-to-end\nlatency and a decrease in traffic loads to the core network\nwhen compared to traditional cloud and on-device\nintelligence methods.\nIn addition to the aforementioned paradigms, AI is poised to\nplay a crucial role in the realm of 5G and future UAV\ncommunication networks, offering two pivotal features. AI\nand ML emerge as viable computational tools to address\nintricate challenges in highly dynamic UAV-enabled 3D\nnetworks. These methodologies present a promising\nalternative to conventional model-driven optimization\napproaches, particularly in scenarios where obtaining precise\nnetwork state information proves challenging. A notable\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n23\nproblem that benefits from these techniques is the joint\noptimization of UAV trajectory and communication design.\nThe integration of UAVs into edge intelligence not only\nenables the development of innovative applications, such as\nUAV virtual reality and UAV swarms, but also introduces\nnovel complexities in efficiently managing computationally\ndemanding and time-sensitive AI tasks from an aerial\nperspective. This situation necessitates the collaborative\ndevelopment of mobility and trajectory control for UAVs,\nalongside the allocation of communication and compute\nresources. However, this task presents significant challenges\ndue to the dual roles that UAVs can assume, acting as either\naerial users or aerial edge servers, or even adopting both\nroles simultaneously. The concurrent management of these\nfunctionalities requires careful consideration and effective\nresource allocation strategies to ensure seamless integration\nand optimal performance within UAV communication\nnetworks. In this subsection, the examination begins with\nML techniques utilized for the design of UAV trajectories\nand communications. Subsequently, the design of\ncomputation offloading for UAVs integrated with MEC is\ndelved into. Lastly, the concept of distributed edge ML\ninvolving UAVs is presented.\n1) OPTIMIZING UAV TRAJECTORY USING ML\nThe optimization of both UAV movements and\ncommunication utilization of resources is of utmost\nimportance in order to achieve optimal performance in 5G\nand future UAV communication networks. Conventional\noffline methodologies that rely on model-driven\noptimization presuppose complete or partial knowledge of\nnetwork state information. However, these approaches may\nnot be well-suited for dynamic environments characterized\nby fluctuating traffic demands, user mobility, and\ncomplicated channel propagation caused by UAVs. In order\nto solve this, academics are now adopting ML techniques and\ndata-driven methodologies.\nML can be categorized into three main types: supervised\nlearning, unsupervised learning, and reinforcement learning\n(RL). RL holds considerable promise in the realm of\ndesigning coordinated movement and communication\nstrategies for UAVs. RL facilitates the swift adaptation of\nUAV communication networks to dynamic environments by\nimproving various aspects such as UAV actions (e.g.,\ndeployment, trajectory, and resource allocations) and reward\nfunctions (e.g., communication rate). Two study areas have\nused RL to optimize UAV operations in the literature:\ncellular-connected UAVs [165] and UAV-assisted\ncommunication networks [166], where UAVs act as BSs and\nusers, respectively.\nIn [167], the authors proposed a deep reinforcement\nlearning (DRL) approach for optimizing the trajectories of\nUAVs in the context of ultra-dense small cell networks. The\npremise involves UAVs equipped with sensing radios to\nacquire distance data pertaining to the UE and other UAVs\nwithin the network, which is subsequently employed to\nadjust the trajectories of the UAVs. However, the complex\npath planning challenge for each UAV remains a persistent\nissue. An optimal operational strategy is introduced in [168]\nby utilizing multi-agent RL to address these challenges.\nMultiple parameters, including the quantity of deployed\nUAVs, initial charging capacity, and charging completion\ncapacity, define a multi-UAV system. In [169], the authors\npresented a multi-agent deep Q-network scheme, where the\nUAVs operate as agents, independently performing actions\nbased on their observations while sharing a common reward.\nAdditionally, a multi-agent meta-RL algorithm is introduced\nfor rapid adaptation to new tasks to address tasks with limited\nprior experience.\nThe authors in [170] introduced utilization of an UAV\u0002enabled relaying system in emergency communications\nbased on RL methods. The primary objective is to maximize\nthe aggregate data transmission from the users to the BS,\nachieved through the optimization of user communication\nscheduling, user association, power allocation, and UAV\ntrajectory.\nThe localization of ground users through the utilization of\nUAVs as aerial anchors is investigated in [171]. It introduced\nan innovative localization framework that incorporates FL\nand RL. This framework involves multiple UAVs learning\ntrajectories within diverse environmental settings, resulting\nin faster convergence of the RL model and reduced\nlocalization errors.\nThe researcher in [172] presented beamforming control\nand trajectory design algorithm based on as multi-pass deep\nQ-network. In this algorithm, the UAV serves as an agent\nresponsible for periodically observing the state of the UAV\nmulticast network and taking actions to adapt to the dynamic\nenvironment.\nA novel DRL technique named pointer network-A*\ndesigned in [173] to efficiently learn a UAV trajectory policy\nthat minimizes energy consumption. The parameters of\npointer network-A* are trained using small-scale cluster\nproblem instances, aiming for quicker training through an\nunsupervised approach with the actor-critic algorithm. In\n[174], the authors presented a new scheme for joint trajectory\nand communication scheduling in wireless caching\nnetworks, involving multiple UAVs. To simplify the solution\non each UAV, a model-specific deep neural network (DNN)\nis introduced to learn the optimal control solution in real\u0002time. The DNN is designed in accordance with the structural\nproperties of the value function and stationary distribution,\nbased on the analysis of the homotopy perturbation method.\n2) COMPUTATION OFFLOADING FOR UAV-ENABLED\nEDGE INTELLIGENCE\nEdge intelligence applications that involve the integration of\nUAVs and AI rely on the generation and processing of\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n24\nFIGURE 10. Computation Offloading Utilizing UAVs: (a) UAV Connected to Cellular Network; (b) UAV-Enhanced MEC System\nvast volumes of data at distributed edge devices. These\ndevices encompass not only UAVs but also conventional\nsmart sensors and smartphones. The seamless execution of\nsophisticated AI training and inference algorithms is\nessential to derive meaningful insights and enable intelligent\ndecision-making processes. However, the implementation of\nthese AI tasks is often computationally demanding and data\u0002intensive, surpassing the computational capacities of the\nlocal wireless devices themselves. To address this challenge,\ncomputation task offloading emerges as an appealing\nsolution. It allows UAV edge servers to offload their\nresource-intensive AI tasks to MEC servers equipped with\nhigh computation capabilities. Once offloaded, the MEC\nservers process the tasks and transmit the computation\nresults back to the UAV edge servers for further analysis or\naction. This approach optimizes the utilization of\ncomputational resources and ensures efficient AI execution\nwhile minimizing the computational burden on the UAVs\n[175].\nFigure 10(a) illustrates a typical scenario where a UAV\nserves as either an aerial user or an edge device within\ncellular networks. In such cases, the UAV may possess\ncomputation tasks that can be efficiently executed through\noffloading to ground-based BSs. This strategy enables\neffective collaboration between UAVs and BSs to handle\nsophisticated AI tasks, enhancing the overall network\nperformance. Moreover, UAVs have the potential to carry\nMEC servers themselves, extending their support to on\u0002ground devices' AI implementations. Figure 10(b)\ndemonstrates how UAVs can aid widely distributed devices\non the ground during computationally intensive AI tasks,\nparticularly in urgent situations, such as emergency response\nscenarios. The UAV's mobility and proximity to the devices\nenable rapid deployment of computational resources and\nseamless offloading of AI tasks, contributing to timely and\nintelligent decision-making processes. Under both scenarios,\nthe joint design of UAV trajectory, communication, and\ncomputation becomes paramount. The proper coordination\nof these aspects ensures efficient and effective task\noffloading, enabling edge intelligence applications to\nfunction seamlessly in real-world environments. In this line\nof research, AI tasks are typically modeled as general\ncomputation tasks with specific data and computation\nrequirements, facilitating the integration of AI capabilities\nwithin UAV-enabled communication networks and MEC\nenvironments. This integration promises to unlock novel\nopportunities for improving network intelligence, enhancing\ncommunication performance, and enabling intelligent\napplications with extensive communication and computation\nrequirements.\n3) UAV-DRIVEN FEDERATED EDGE LEARNING\nAside from the process of diverting computations to a central\nMEC server, edge devices such as UAVs have the potential\nto participate in cooperative AI tasks, capitalizing on their\nlocally dispersed data and computational proficiencies. This\nmethodology, recognized as distributed edge learning,\nencompasses the concept of federated edge learning, which\nconfers benefits in terms of data security and privacy [163].\nWithin the realm of federated edge learning, an assemblage\nof edge devices, encompassing UAVs, collaboratively\nemploy their dispersed data to facilitate the training of shared\nAI models without necessitating data exchange. The edge\nserver in this context can take the form of a terrestrial-based\nBS in instances involving cellular-connected UAVs, or\nanother UAV within UAV swarms, as portrayed in Figure\n11(a) and Figure 11(b). The iterative execution of federated\nedge learning encompasses UAVs revising their local AI\nmodels during each iteration and subsequently consolidating\nthese updates at the edge server to refine global AI models.\nThis undertaking demands recurrent interchange of AI model\nparameters between UAVs and the edge server, thereby\nnecessitating meticulous optimization of multiple UAV\ntrajectories, communication strategies, and computation\nscheduling over temporal intervals, attributing to the 3D\nmobility of the UAVs. Despite the escalating interest in\nfederated edge learning within the realms of wireless\ncommunications and ML communities, research pertaining\nto the integration of UAVs in edge learning is still in its\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n25\nFIGURE 11. Federated Edge Learning Utilizing UAVs: (a) Coordination with Ground BSs; (b) UAV Peers\nnascent stages. Several studies have investigated the\npotential of federated learning applications within UAV\nwireless communication networks [176]. The allocation of\nwireless resources to ensure the efficiency of UAV-enabled\nfederated edge learning was the subject of examination in\n[177]. Moreover, inquiries have delved into the realm of\nfederated learning for UAV swarms as documented in [178].\nFurthermore, proposals have been put forth wherein UAVs\nundertake the role of edge servers in the context of federated\nlearning, as presented in [179], including considerations for\nsupport within the Internet of vehicles framework as\nexplored in [180]. Nonetheless, a comprehensive exploration\ninto the fundamental performance thresholds of federated\nlearning when integrated with mobile UAV nodes remains\nlargely uncharted.\nIn [181], the authors introduced a federated learning\nframework for UAV swarms, incorporating MEC where\nmodel aggregation is shifted to edge servers. In this\nframework, the overall federated learning cost is\ncharacterized as a weighted combination of the total delay\nincurred by UAV swarms to complete the federated learning\ntask and the energy consumption of the system. To facilitate\ndynamic and intelligent UAV services, a centralized\ndynamic service algorithm called deep deterministic policy\ngradient based centralized has been introduced in [182],\nrelying on deep reinforcement learning. Nonetheless,\nconsidering the training complexities associated with the\ncentralized approach, a more favourable distributed learning\nalgorithm, federated learning-based federated, has been\nproposed, which integrates federated learning methods.\nA cognitive network of UAVs has been introduced in\n[183], with the primary objective of providing dependable\nedge computing services to IoT devices within a specified\narea. To minimize latency for IoT devices, a partial federated\nlearning model has been devised and implemented on UAVs.\nA critical challenge arises from handling the non\u0002independent and non-identically distributed nature of\nheterogeneous data while ensuring learning convergence. To\neffectively tackle this issue, a novel and high-performance\nfederated learning scheme, referred to as the hierarchical\nfederated learning algorithm, is introduced in [184] for the\nedge-assisted UAV network. This approach leverages edge\nservers positioned in BSs as intermediate aggregators,\nincorporating commonly shared data to address the challenge\neffectively.\nThe effectiveness of global federated learning models may\nbe hampered by the significant heterogeneity of local data,\npotentially hindering the training process, and undermining\nthe performance of local agents. To overcome these\nchallenges, a novel approach is introduced in [185], referred\nto as personalized federated DRL (PF-DRL), designed for\nmulti-UAV trajectory optimization. PF-DRL seeks to create\npersonalized models for each agent, effectively addressing\ndata scarcity concerns and alleviating the adverse effects of\ndata heterogeneity.\nF. SYMBIOTIC RADIO COMMUNICATION AND SENSING\nActive radio technology, depending on the design of\ntransmitters that consist of highly power-consuming\nelements such as oscillators, up-converters, and other\ncomponents, has a negative impact on the newly developed\nservices in the next wireless networks by shortening battery\nlife, especially for those devices that have a limited battery\nor are difficult to recharge and replace, such as IoT devices\nand UAVs that need to utilize the available battery efficiently\n[186].\nBackscattering radio technology is a recent approach to\ndesigning transmitters with zero active components, leading\nto the elimination of the consumed power by means of active\ntransmission. One more efficient technology is called\nambient backscatter communication (AmBC), which uses\nambient radio frequency (RF) signals such as cellular\nsignals, Wi-Fi signals, or any form of RF transmission that\ncoexists to serve a primary user. The transmitter in AmBC\nembeds its message in the ambient RF signal by varying the\nreflection coefficients through varying the load impedance at\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n26\nthe backscatter transmitter. Then an intelligent receiver can\ndetect changes in the reflection coefficients to decode the\nmessage transmitted by the AmBC transmitter.\nRecently, a new technology known as symbiotic radio\n(SR), which benefits from both CR and AmBC to overcome\ntheir drawbacks effectively. SR overcomes CR by having\ntwo spectrum sharing systems, primary and secondary,\nwhich helps in providing mutually beneficial spectrum\nsharing. On the other hand, SR overcomes AmBC by\nenhancing more reliable backscattering through joint\ndecoding [187]. Therefore, the utilization of SR technology\nhas considerable potential as a viable approach to facilitate\ncollaborative resource allocation across radio systems via\nsymbiotic associations [186].\nKnowing that, UAVs will play a major role in the next\ngeneration of wireless networks with a wide range of\napplications, as discussed in Section IV [188]. With limited\nbattery life, UAVs need efficient energy management to\nensure sustainability and long battery life [90]. To address\nthis challenge, UAVs can leverage SR to reduce the need for\nactive transmitters. Utilizing SR in 5G-and-beyond will\nsignificantly reduce the power consumption of UAVs,\nenabling long-term and sustainable applications in the future\n[187].\nFigure 12 shows that not only SR can be used for\ncommunication but also for sensing UAVs. In this figure, a\nscenario includes air-to-air and air-to-ground symbiotic\ncommunication and sensing (SCAS), in which a\ncommunication signal is transmitted from a satellite to an\nUAV and a ground station. The UAV utilizes backscatter\ntechnology to transmit its data via backscattering\ncommunication. Upon receiving the backscattered signal, the\nbase station performs SCAS to determine the location of the\nUAV as well as retrieve the accompanying data [186].\nFIGURE 12. Air-to-Air and Air-to-Ground SCAS\nVI. UAV REGULATIONS\nThe widespread deployment and operation of UAV-based\ncommunication systems face substantial challenges and\nlimitations due to regulatory issues. Governments and\nregulatory entities across the globe are currently engaged in\nthe formulation of regulations aimed at guaranteeing the\nsafe, secure, and responsible utilization of UAVs [189, 190].\nThe regulatory factors and considerations that influence\nUAV operations encompass the following:\n1) LICENSING AND REGISTRATION\nIn many regulatory frameworks, operators of UAVs are\nmandated to acquire licenses or certifications for both\ncommercial and recreational purposes. However, some\ncountries like Vietnam does not mandate the possession of a\npilot license for UAV operations, distinguishing its\nregulations from those of notable jurisdictions like the\nUnited Kingdom, the United States, Singapore, and Australia\n[191]. The acquisition of these licenses may entail the\ncompletion of knowledge assessments, the attainment of\npilot certifications, or the acquisition of specific\nauthorizations, which are contingent upon the weight and\nfunctionalities of the UAV. Furthermore, it may be necessary\nto register UAVs with the relevant governing bodies in order\nto establish a system of responsibility and the ability to track\ntheir movements.\n2) FLIGHT RESTRICTIONS AND AIRSPACE REGULATIONS\nUAVs are required to adhere to airspace regulations in order\nto uphold safety standards and mitigate the risk of potential\ndisruptions to manned aircraft operations. Restricted\nairspace zones, no-fly zones, and altitude limitations are\nestablished by authorities in order to mitigate the potential\nfor collisions and uphold the integrity of manned aviation\noperations [16, 190]. Adherence to these regulations is of\nutmost importance, in order to mitigate the risk of\nunauthorized access to restricted zones and ensure the\npreservation of a secure distance between aircraft.\n3) REMOTE IDENTIFICATION AND TRACKING\nRemote identification technology for UAVs is an emerging\ncapability enabling ground-based observers to identify UAVs\nin airspace while gathering relevant information about the\nUAV and its operator [192]. The overarching concept and\nstandardized framework for remote identification of UAS\nprimarily pertain to the electronic identification of airborne\nunmanned aircraft. These standards establish the baseline\nperformance criteria for direct remote identification [193].\nAdditionally, 3GPP has initiated various endeavors to cater to\nthe connectivity requirements of UAS using mobile networks,\nincluding the 5G system. The comprehensive requisites for\nremote identification of UAS are outlined in [194]. In order to\naugment accountability and streamline identification\nprocesses, it may be necessary for regulations to mandate the\ninclusion of remote identification and tracking capabilities in\nUAVs. This capability allows authorities to ascertain the\nidentity of the operator and monitor the UAV throughout its\nduration of flight. Remote identification systems commonly\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n27\nemploy the transmission of distinct identifiers, such as serial\nnumbers or digital signatures, to ground control stations or\nauthorized receivers.\n4) OPERATIONAL LIMITATIONS AND RESTRICTIONS\nRegulatory measures have the potential to impose operational\nconstraints on UAVs, encompassing limitations on factors\nsuch as flight altitude, proximity to airports, and traversing\nspecific regions such as densely populated areas, vulnerable\nfacilities, or essential infrastructure. The aforementioned\nlimitations are implemented with the objective of safeguarding\nthe well-being of the general public, ensuring the preservation\nof personal privacy, and maintaining the overall security of the\nsystem. Airspace restriction represents a prevalent regulatory\napproach applied across numerous countries and regions\nconcerning UAV operations [16]. Particularly in nations with\nstringent UAV regulations, such as the United States and\nChina, UAV operators must seek airspace authorization before\nconducting flights. In this process, assuming the UAV is\nregistered under the operator's real name and the pilot holds\nthe necessary certification, the pilot submits both the flight\nplan and airspace authorization request to the relevant airspace\nmanagement authority.\n5) PRIVACY AND DATA PROTECTION\nThe emergence of privacy issues pertaining to UAV\noperations has prompted the establishment of regulatory\nmeasures aimed at preserving individual privacy and ensuring\nthe security of personal data [195]. Recent developments in\nsecurity and privacy concerns impact the IoD network, along\nwith contemporary techniques for mitigating IoD attacks\n[196].\nRegulations have the potential to impose restrictions on the\ncapturing and processing of personal data without obtaining\nconsent, as well as to impose limitations on the utilization of\nsurveillance equipment in specific situations. Ensuring\nadherence to privacy regulations frequently entails the\nacquisition of explicit consent, the anonymization of collected\ndata, and the implementation of secure data handling\nprotocols.\n6) SPECTRUM ALLOCATION\nUAV communication systems heavily rely on the allocation of\nwireless spectrum to enable the seamless transmission of data\nand control signals. Regulatory authorities hold a pivotal role\nin this spectrum allocation process, ensuring the provision of\nsuitable frequency bands for UAV communication. Their\nprimary objective is to safeguard uninterrupted UAV\noperations by effectively managing potential interference\nissues. The overarching goal of these spectrum allocation\nregulations is to strike a delicate balance between the\nescalating demand for spectrum resources and the specific\nrequirements of UAV communication systems.\nThe operation of UAVs introduces numerous complexities\nin the realm of radio spectrum management, with a focus on\nensuring operational safety, efficient spectrum utilization, and\nharmonious coexistence with pre-existing wireless networks.\nConventional spectrum allocation methodologies prove\ninadequate when applied to UAV networks, primarily due to\nthe dynamic nature of UAV operations. This dynamicity\nnecessitates adaptive spectrum strategies and robust\nmechanisms to ensure the continuous and reliable delivery of\nservices [15]. For UAS operation, radio spectrum usage\nencompasses a range of critical functions, including\ncommunication, navigation and surveillance electronic\nconspicuity, command and control, detect and avoid, as well\nas the relay of payload data [197].\n7) OPERATIONAL PROCEDURES AND SAFETY\nSTANDARDS:\nRegulations have the potential to delineate operational\nprocedures, safety standards, and maintenance requirements\npertaining to UAVs. The primary objective of these\nregulations is to establish measures that guarantee the secure\nfunctioning of UAVs and minimize the potential hazards\narising from malfunctions, collisions, or incidents. The\nstandardization in [198] outlines specific prerequisites for\nensuring the quality and safety of UAS design and production,\nencompassing unmanned aircraft, remote pilot stations,\ndatalinks, payloads, and associated support equipment. The\nwork in [199] examined New Zealand's regulatory framework\nfor aviation safety concerning unmanned aircraft, considering\nthe viewpoint of unmanned aircraft operators. Generally, any\ndocuments provided by an organization may encompass a\nrange of essential components, such as pre-flight check\nprotocols, records of maintenance activities, emergency\nresponse protocols, and the necessary training prerequisites for\noperators.\n8) BEYOND VISUAL LINE OF SIGHT (BVLOS) OPERATIONS\nRegulations frequently impose limitations on BVLOS\noperations, referring to the operation of UAVs beyond the\noperator's direct visual range. BVLOS operations necessitate\nthe utilization of sophisticated technologies, seamless\nintegration into existing airspace systems, and the\nimplementation of comprehensive safety protocols. In\nBVLOS operations, the UAV is permitted to function beyond\nthe LoS, adhering to a predetermined flight path and relying\non instrumentation-based flight data, including onboard\ncameras and detect-and-avoid technologies [200].\nRegulatory bodies are currently engaged in active\nexploration and development of regulations aimed at\nfacilitating and overseeing BVLOS operations. These\nregulations are being formulated with the intention of\naddressing various concerns pertaining to safety, collision\navoidance, and the establishment of effective command and\ncontrol mechanisms.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n28\n9) INTEGRATION WITH AIR TRAFFIC MANAGEMENT\nSYSTEMS\nThe integration of UAVs into established air traffic\nmanagement systems assumes paramount importance as the\nfrequency of UAV operations continues to rise. A shared\ncomprehension of the essential functions and structural\naspects of UAS traffic management is presented in [201],\noffering a comprehensive explanation of the system layer\nwithin the UAS traffic management framework. Additionally,\nin [202], a framework for ensuring adherence to regulatory\nguidelines, encompassing safety, security, privacy, and\nvarious organizational prerequisites, is presented for entities\noffering UAS traffic management services. In order to ensure\nthe safe integration of UAVs and to prevent conflicts with\nmanned aircraft in controlled airspace, regulations may\nnecessitate the adherence of UAVs to designated protocols,\ncommunication standards, and coordination procedures. The\nfuture perspective of 6G-enabled UAV traffic management\necosystems in highly congested urban airspace, emphasizing\nnon-terrestrial aspects, encompassing aerial and satellite\ncommunication [203].\n10) SECURITY AND COUNTER-UAV MEASURES\nRegulations have the capacity to effectively mitigate security\nconcerns that arise from the operations of UAVs,\nencompassing issues such as unauthorized access, potential\nthreats, and the implementation of counter-UAV measures.\nThese regulations encompass a wide range of operational\nguidelines, safety measures, and legal frameworks aimed at\npromoting responsible UAV use while addressing potential\nsecurity threats posed by UAVs [204]. Government entities\nhave the ability to establish regulations regarding the\nutilization of specific UAV technologies, deploy counter\u0002UAV systems in areas of high sensitivity, and develop\nprocedures for reporting and addressing security incidents. A\nmethodology for assessing attack and countermeasure\ntechniques in of commercial UAVs is outlined in [205]. In\n[206] delineates prerequisites for the operational procedures\nof unmanned aircraft, which, when combined with existing\nand forthcoming standards on UAS, constitute a\ncomprehensive safety and quality standard for unmanned\naircrafts. Additionally, it applies universally to all commercial\nUAS, regardless of their size, classification, purpose, or\nlocation, and represents the global benchmark for the secure\noperation of commercial UAS.\nRegulations pertaining to UAVs exhibit variability across\ndifferent countries and geographical regions, with distinctions\nmade based on factors such as urban or rural settings.\nRegulations governing UAVs operations in the United States\nare promulgated by the federal aviation authority (FAA) and\nthe national aeronautics and space administration (NASA).\nNASA is currently engaged in a collaborative effort with the\nfederal communications commission (FCC) and the FAA to\nundertake the development of UAV control frameworks. FCC\nis presently engaged in an investigation to determine the\nnecessity of establishing a new spectrum policy specifically\ntailored to regulate UAV operations.\nVII. RESEARCH TRENDS AND OPEN CHALLENGES\nA. RESEARCH TRENDS\nThe integration of UAVs into 5G-and-beyond networks has\nemerged as a promising paradigm to revolutionize wireless\ncommunications, enabling a wide array of innovative\napplications and services. As the deployment of UAVs\nbecomes more prevalent, researchers and industry\nstakeholders are actively exploring novel approaches to\nharness the full potential of these aerial platforms in synergy\nwith advanced communication technologies. This section\ndelves into the latest trends shaping UAV-enabled\ncommunication systems, encompassing heterogeneous\nnetwork integration, UAV swarm communication, security\nand privacy, ML and AI for UAVs, green UAV\ncommunication, and spectrum management.\n1) HETEROGENEOUS NETWORK INTEGRATION\nThe integration of UAVs into terrestrial heterogeneous\nnetworks (HetNets) is currently a topic that has been\ncapturing significant attention in the research community.\nThis integration aims to enable UAVs to interact seamlessly\nwith various types of networks, such as 5G, 6G, Wi-Fi, and\nsatellite networks. By integrating UAVs into HetNets, the\nscope of applications for UAV communication and\nnetworking expands significantly.\nOne of the key benefits of heterogeneous network\nintegration is the enhanced coverage and connectivity that\nUAVs can provide. As UAVs can operate at varying altitudes\nand positions, they can extend the reach of terrestrial\nnetworks to remote or difficult-to-access areas. Additionally,\nUAVs can act as aerial relays to facilitate communication in\nareas with limited ground infrastructure. The flexibility of\nUAVs to switch between different networks based on factors\nlike network availability, demand, and application\nrequirements offers numerous advantages. For instance,\nduring emergencies or events with high data traffic, UAVs\ncan be deployed as temporary network boosters to offload\ntraffic from congested TBSs. Moreover, UAVs can be\nutilized to bridge communication gaps in disaster-stricken\nregions, enabling critical communication and coordination.\nResearch in this area is focused on addressing various\nchallenges, such as efficient handover mechanisms,\ndeveloping intelligent network selection algorithms [207],\ncross-layer design approaches, and seamless integration of\nUAVs into existing HetNets. By effectively addressing these\nchallenges, the integration of UAVs into HetNets promises\nto revolutionize the way wireless communication is\napproached in 5G-and-beyond networks [208].\n2) UAV SWARM COMMUNICATION\nThe concept of UAV swarms, where multiple UAVs operate\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n29\nin coordinated groups, is an exciting area of research with\npromising applications. UAV swarms have garnered\nconsiderable interest in fields like surveillance, delivery\nservices, and emergency response scenarios. These swarms\noffer significant advantages over individual UAVs, such as\nincreased reliability, enhanced coverage, and improved\nscalability.\nIn UAV swarm communication, research is focused on\ndeveloping efficient communication protocols and\nalgorithms to facilitate seamless and reliable inter-UAV\ncommunication. This entails addressing challenges related to\nswarm coordination, dynamic formation and dissolution of\nswarm members, and information exchange among UAVs.\nTo achieve effective swarm communication, researchers are\nexploring communication strategies that ensure\nsynchronized actions and cooperation among UAVs in\nswarm scenarios.\nOne of the key areas of interest is establishing robust\ncommunication links between UAVs within the swarm while\nminimizing interference and latency. By optimizing\ncommunication paths and dynamically adapting\ncommunication strategies based on swarm dynamics,\nresearchers aim to enhance the performance and resilience of\nUAV swarm communication [209].\n3) SECURITY AND PRIVACY\nAs UAVs become increasingly integrated into daily life,\nensuring secure and private communication is of paramount\nimportance. UAV communication networks handle sensitive\ndata and perform critical tasks, making them potential targets\nfor cyberattacks and privacy breaches.\nTo address these concerns, ongoing research focuses on\ncryptographic techniques, trust-based protocols, and\nprivacy-preserving mechanisms designed specifically for\nUAV communication networks. Encryption and secure key\nexchange protocols ensure that transmitted data remains\nconfidential and protected from unauthorized access. Trust\u0002based protocols establish a system of trust between UAVs\nand ground stations, enabling secure communication and\npreventing unauthorized access to the network. Additionally,\nprivacy-preserving mechanisms aim to protect user identities\nand sensitive information, ensuring that UAV operations\nremain anonymous and secure.\nMoreover, secure and robust communication protocols are\nessential to safeguard against potential jamming, spoofing,\nand eavesdropping attacks. By incorporating advanced\nsecurity measures into UAV communication networks,\nresearchers aim to establish reliable and resilient systems\nthat can withstand potential security threats. The challenge\nlies in striking a balance between security measures and the\nperformance and efficiency of UAV communication.\nResearchers are actively working to develop lightweight and\nscalable security solutions that do not compromise the real\u0002time communication demands of UAV applications.\nMeanwhile, they also focused on addressing the unique\nsecurity challenges posed by UAVs, such as aerial attacks\nand privacy invasion [6].\n4) AI TECHNIQUES FOR UAVS\nIn the upcoming decade, the utilization of AI methods in\nUAV communication systems is set to expand significantly.\nResearchers will exploit artificial neural networks, deep\nlearning (DL), and ML algorithms to enhance the\noptimization of UAV communication networks, as these\nmethodologies have demonstrated notable advantages across\nvarious applications. The research community still needs to\nfurther explore the development of an acceptable AI\ntechnique for the UAV communication system, which is a\nmulti-dimensional network that is more sophisticated than\ncurrent terrestrial communication networks. Meanwhile, AI\nhas shown great potential in addressing complex challenges\nfaced by UAVs, such as path planning, resource allocation,\nand interference management.\nOne of the primary applications of ML in UAV\ncommunication is predictive modeling for optimal path\nplanning [210]. By analyzing historical data, environmental\nconditions, and user requirements, ML algorithms can\npredict the most efficient and safe routes for UAVs. These\npredictive models enable UAVs to autonomously plan their\ntrajectories to optimize communication coverage and reduce\nflight time. In addition to path planning, ML techniques are\nalso utilized for dynamic resource allocation. By\ncontinuously monitoring network conditions and demand\npatterns, ML algorithms can optimize resource allocation,\nensuring efficient utilization of available bandwidth and\npower resources. Furthermore, AI-driven interference\nmanagement is a key focus area to enhance the reliability and\nperformance of UAV communication networks. AI\nalgorithms can intelligently analyze interference patterns and\nadapt communication parameters in real-time to mitigate\ninterference and improve overall network efficiency.\nThe successful implementation of AI in UAV\ncommunication and networking requires the availability of\nhigh-quality training data and the development of robust\nlearning models. Moreover, the integration of AI techniques\ninto UAV communication systems must be complemented\nby stringent security and privacy measures to safeguard\nsensitive data and prevent potential malicious attacks.\n5) BLOCKCHAIN IN UAV COMMUNICATIONS\nBlockchain is an important ongoing research trend driving\nthe evolution of 5G-and-beyond networks with UAVs. As a\ndistributed ledger technology, Blockchain offers secure and\ntransparent transactions, enabling trust and accountability in\nUAV communication and data management. It provides a\ndecentralized and tamper-resistant platform for sharing\nUAV-generated data among multiple stakeholders, ensuring\ndata integrity and authenticity in collaborative UAV\nmissions. Moreover, Blockchain enables the creation and\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n30\nmanagement of decentralized digital identities for UAVs,\nenhancing security through decentralized access\nmanagement. Smart contracts, programmable self-executing\ncontracts on the Blockchain, automate various aspects of\nUAV missions and enable autonomous decision-making\namong UAVs. Blockchain's support for cryptocurrency and\nmicrotransactions facilitates secure and real-time\ntransactions among UAVs, promoting resource exchange\nand service collaboration without traditional intermediaries\n[211]. Additionally, Blockchain-based consensus\nmechanisms can be applied to UAV traffic management,\nenabling decentralized coordination and conflict resolution\namong multiple UAVs in congested airspace, thereby\nimproving overall traffic efficiency and safety. Furthermore,\nBlockchain ensures data integrity and traceability, making\nrecorded data tamper-proof and auditable, vital for\napplications like UAV-based critical infrastructure\ninspections.\nWhile the integration of Blockchain in UAV\ncommunications offers numerous benefits, there are\nchallenges to address, including scalability, energy\nefficiency, and regulatory considerations. Researchers are\nactively exploring innovative approaches to optimize\nBlockchain solutions for UAV communications, unlocking\ntheir full potential in 5G-and-beyond UAV networks.\n6) SPECTRUM MANAGEMENT\nEfficient spectrum management stands as a paramount\nresearch trend in the context of 5G-and-beyond networks\nwith UAVs. With the increasing proliferation of UAVs in\ndiverse applications, ranging from aerial surveillance and\nmonitoring to disaster response and communications,\neffective spectrum allocation and utilization become\nimperative for ensuring seamless and reliable UAV\ncommunication [212]. Spectrum sharing techniques lie at the\ncore of spectrum management research. These techniques\nare designed to enable UAVs to efficiently share spectrum\nresources with existing communication systems, fostering\ncoexistence without causing harmful interference. The\ndevelopment of intelligent spectrum sharing mechanisms\nensures that UAVs can dynamically access available\nspectrum bands based on real-time requirements, optimizing\ncommunication performance while efficiently utilizing the\navailable resources. Dynamic spectrum access is a key focus\nwithin the spectrum management domain. By implementing\ndynamic access mechanisms, UAVs can opportunistically\nutilize underutilized spectrum bands, leveraging temporal\nand spatial variations in spectrum availability. Dynamic\nspectrum access empowers UAVs to adapt swiftly to\nchanging environmental conditions and communication\ndemands, resulting in enhanced communication reliability\nand throughput [213]. For instance, cognitive radio\ntechniques are being explored to enable UAVs to\ndynamically access and utilize underutilized spectrum bands.\nCognitive UAV communication enhances spectral efficiency\nby opportunistically accessing available frequency bands\nwhile avoiding interference with primary users [214].\nInterference management represents a critical aspect of\nspectrum management for UAVs. As UAVs often operate in\nhighly congested and dynamic environments, interference\nfrom neighboring communication systems can significantly\nimpact communication performance. Research efforts\nconcentrate on devising innovative interference mitigation\ntechniques that allow UAVs to coexist harmoniously with\nother wireless networks. Advanced interference avoidance\nalgorithms, intelligent beamforming strategies, and\ncooperative spectrum sensing mechanisms are among the\nkey solutions being explored to mitigate unwanted\ninterference. By effectively managing spectrum resources\nand addressing interference challenges, the spectrum\nmanagement research trend seeks to optimize spectral\nefficiency for UAV communication. This optimization not\nonly ensures the reliability and quality of UAV\ncommunication links but also enhances the overall\nperformance and capacity of UAV networks.\nB. OPEN CHALLENGES\nEvery system containing complex elements brings different\nchallenges to be solved. Since connected UAV schemes form\na complex system with elements of the transmission medium,\nthe complexity is greater than conventional networks. Also,\nthere are many different approaches to creating a wireless\nnetwork with UAVs as presented up to this point. In order to\nanalyze these approaches correctly, a correct relationship\nmust be established between UAVs and conventional\nwireless communication networks. In this way, difficulties\ncan be identified more easily. These difficulties will also\ndetermine future research trends. This section discusses the\nmain challenges in wireless communication with connected\nUAVs and research on these issues. These challenges are\npresented under the following subsections: collision\navoidance, control latency, limited energy, privacy problems,\nmobility management, channel modeling, UAV antenna\nconfigurations, interference management, GPS architecture\nfor system redundancy, and cyber-physical security.\n1) COLLISION AVOIDANCE\nIn multiple UAV scenarios, collision avoidance is also\nanother consideration for the system designs. This issue does\nnot cover only collision between different UAVs but also\ncollision between UAVs and other obstacles in the\nenvironment. A comprehensive review paper that focused on\ncomparison and analysis of different collision avoidance\nalgorithms available in the literature is provided by authors in\n[215]. Several research papers representing different\napproaches to this issue are described in this subsection. One\nof the interesting scenarios for collision avoidance algorithms\nis studied by [216]. They adapted an algorithm that considers\nsome channel restrictions, position data, and video\nparameters for first person view UAV applications. They\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n31\nclaimed that their algorithms made the probability of collision\n58.63% better than previous studies in this matter. A recent\nstudy that solves the avoidance problem by reinforcement\nlearning is presented in [217]. The study provided collision\navoidance without any previous data about other UAVs\u2019\ntrajectories in the network. They solved the optimization\nproblems for their given approach and proposed that the\nresults are promising compared to other studies. Authors in\n[218] provided a collision avoidance approach that considers\nthe quality of coverage for the UAVs in surveillance\nmissions. They developed a new coverage model by proving\nits convergence and providing its simulation results.\n2) CONTROL LATENCY\nControl latency is one of the crucial topics in connected\nUAVs. Control latency which is provided by varying cellular\nsystems using UAVs will be better thanks to technologies\nsuch as 5G. Advantages of 5G for improving control latency\nand additional benefits over 4G are discussed in [20]. Results\nare presented according to the 20 MHz carrier bandwidth and\n2.6 GHz carrier frequency using LTE-Advance Network.\nBesides; 50 m, 100 m and 300 m are selected as different\naltitudes for UAVs. Results related to these altitudes are\nillustrated for varying latency data samples. Details about\ncontrol latency and a more comprehensive literature review\ncan be found in [20].\n3) LIMITED ENERGY\nIn a network which consists of UAVs, energy is one of the\nmost significant challenges which limits performance. Since\nUAVs have limited energy, all scenarios related to UAVs\nmust be designed by considering energy issues. It requires\nmuch different research that must be done, such as battery\ndesign and charging optimization. In [219], a cloud-based\nUAV navigation system is explained to obtain a more\nefficient battery charging scenario for networks which use\nUAVs as relays. Since lack of coordination between UAVs\nduring battery charging may cause a blockage on the network,\nthey propose a solution by using globally coordinated routes\n[219]. In another research, an auction-based multiple UAV\ncharging plan is proposed to increase the performance of\nprocurement of energy on time [220]. Also, problem of how\nto place the charging station for UAVs is discussed in [221].\nThey propose a deployment solution for placement of UAVs\nand this system recursively changes the configuration of\ncharging stations with respect to usage data. All the provided\narticles show that charging optimization is quite popular in\nthe literature. It is not mentioned in this paper, but there are\nmany articles related to battery design, and it is another\ncrucial branch of this issue that requires much further\nresearch.\n4) PRIVACY PROBLEMS\nAs with every wireless communication system, privacy is one\nof the key aspects. UAVs were not designed to consider\nsecurity problems originally. Thus, when they are used in\ncommunication networks, privacy issues should be taken into\nconsideration. Security challenges in connected UAVs are\nsorted in detail in [222]. It highlights that UAV networks are\nquite different than classical wireless networks. Less power\nrequirement and carrying of less information are sorted as\nexamples for differences between classical networks than\nUAV networks [222]. Requirements of privacy and security\nfor wireless UAV networks and their architecture are studied\nin [223]. They also propose prospective approaches for\nproblems such as flexibility, protection, and leakage. Their\nsolutions cover location protection and privacy protection.\nMore detail related to these solutions can be found in [223].\n5) MOBILITY MANAGEMENT\nTo fully take advantage of UAV installation, further visual\nLoS processes is of critical significance as UAVs act as air\nusers, which maintain connection with the TBSs for direction\nand control objectives in the downlink [224]. Typically, BS\nantennas sidelobes may serve UAVs while flying in the sky\nwhich deliver lower antenna gains [26, 224]. This will create\nsignificant challenges to the mobility management of\ncellular-connected UAVs in terms of reference signal\nreceived power (RSRP). The highest RSRP will be\nmaintained when the distance is far between TBSs and UAV.\nFigure 13 illustrates the cellular-connected UAV when it is\nflying over a rural area. This scenario of irregular signal\ncoverage of TBSs will lead to reduced mobility performance\nsuch as radio link failure, handover failure, in addition to\nuseless handovers, named ping-pong events [225].\nRegardless of that, due to the loss of direction and control of\nsignal, the UAV may hit a civil aircraft or even collide into a\npopulated zone which might cause dangerous events.\nTherefore, functional mobility management to deliver\nauthoritative connections between UAVs and TBSs is of\ncritical significance. In [225, 226] provided more details on\nthe mobility management, particularly handover procedures,\nfor connected UAVs within forthcoming mobile networks,\nencompassing technologies such as 5G, 6G networks.\n6) CHANNEL MODELING\nOne of the main issues in UAV communication is channel\nmodelling. Similar to other wireless communication\napplications, determining aspects of wireless communication\nchannels in connected UAVs is critical to be able to design an\noptimal system in order to provide reliability. Unlike other\ncommonly known fading channels, there are more parameters\nthat characterize UAV communication channels. For example,\nsince a UAV may act as a component during connection, it\nshows some Rician fading channel characteristics. However,\nLoS components can disappear in certain time slots. Then, it\ncan be said that channel turns into a Rayleigh fading channel.\nThese examples can be increased. To model UAV\ncommunication, different approaches and formulations are\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n32\nFIGURE 13. HO scenario of a cellular-connected UAV moving towards horizontal direction. (a) UAV is associated with TBS2 due to its higher side-lobe\ngain than TBS1. (b) After moving forward, the UAV is now associated with TBS1 due to its higher sidelobe gain.\npresented in the literature. These approaches and formulations\ninclude parameters such as path loss exponents, shadowing\neffects, etc. In addition to these parameters, there are some\ndifferent channel definitions for A2G and A2A. A detailed\nreview of these differences is presented in [3]. There are other\npossible methods for characterizing UAV channels. One of\nthem is obtaining empirical results. Due to the complexity of\ntheoretical derivations in UAV communications, empirical\nresults are more supportive than other classical wireless\ncommunication studies for determining the future directions\nof channel modelling. In practical channel modelling studies,\nthere are many different setups and considerations. As an\nexample, a very effective method is proposed in [23].\nIn [227], the authors investigated continuous phase\nmodulated signal transmission in UAV communication\nnetworks with doubly-selective channels. It proposed a two\u0002stage receiver design that includes a linear time-varying\nequalizer and a recursive symbol recovery process from\npseudo-symbols in the Laurent representation.\nA2A channel modeling describes the wireless\ncommunication channels between UAVs when they are in\nflight. Several works have been done in A2A channel\nmodeling such as [228-230]. The researchers in [228] employed\nthe ray tracing method to conduct path gain analyses based on\nthe distance between two UAVs in distinct scenarios. The\nstudy explores various antenna types employed in A2A\ncommunication channels when one UAV functions as a\ntransmitter and the other as a receiver, assuming a LoS\nconnection between them. In [229], a 3D geometry-based\nstochastic channel model is introduced to account for unique\nA2A channel characteristics, including arbitrary 3D mobility\nand time-domain non-stationarity. They focused on deriving\nand examining critical channel attributes such as the root mean\nsquare delay spread, space-time correlation function,\nstationary interval and Doppler power spectrum density, as per\nthe theoretical framework. The authors in [230] presented the\nmeasurement of an A2A channel at a frequency of 1420 MHz\nwithin an urban environment. This study analysed the delay\nand power characteristics of multipath components using the\ngathered measurement data. Additionally, the study calibrated\na ray-tracing simulator to extend its applicability to various\nUAV scenarios.\nA2G channel modeling in UAV communication describes\nthe wireless communication channel characteristics between a\nUAV and a ground station or terrestrial device. In the\nresearchers in [231] measured A2G signals from UAVs and\ndeveloped a channel model incorporating a two-ray ground\u0002reflection effect for the UAV communication system. This\nchannel modelling approach offers a suitable path loss model\nfor assessing the quality of service in A2G wireless\ncommunication. Several channel measurement campaigns\nwere conducted within the airport vicinity, covering the UHF\nand L-band frequencies (approximately 433 MHz and 1.518\nGHz) [232]. In cases where there was a clear LoS, distance\u0002dependent path loss models were developed for both\nfrequency bands. In [233], the authors investigated A2G\nchannel modeling and transmission performance in a cellular\u0002connected M-MIMO UAV swarm system. They introduced a\ncorrelated A2G channel model that considers factors like non\u0002isotropic scattering, LoS propagation, and mobile scatterers.\nAdditionally, a novel analytical expression for uplink signal\u0002to-interference-and-noise ratio is derived, accounting for\nchannel aging and strong LoS effects. A 3-D elliptic-cylinder\nMIMO channel model for UAV communication in A2G\nscenarios is proposed in [234]. This study focused on the\nmobility and altitude of UAV transmitters in the elevation\nplane, utilizing the newly proposed UAV-MIMO channel\nmodel as a basis. The authors in [235] performed cluster-based\ncharacterization and modeling of A2G channels. This study\nrepresented the first of its kind to focus on the clustering and\ntracking of multipath components within dynamic A2G\nchannels\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n33\n7) UAV ANTENNA CONFIGURATIONS\nLike in all wireless communication applications, there are\nmany antennas configuration approaches for UAV\ncommunications. In addition to previous antenna knowledge\nfor wireless communications, some issues have to be taken\ninto consideration for UAV communication. For example,\nnoise which is caused by the UAV itself changes previous\napproaches. Moreover, the specific characteristics of aerial\ncommunications can be counted as another example.\nAccording to [24], optimal tilting of the UAV antenna\nenhanced the throughput from 3.5 to 5.8 b/s/Hz and the\ncoverage from 23% to 89%. And they also declare that their\nconfiguration increases performance of microcells limits and\nnetwork density performance. In [25], a new reconfigurable\nmicrostrip antenna including adjustable conical beams is\nproposed for UAV communications. That antenna covers a\nbandwidth from 2.39 to 2.49 GHz which is also so suitable for\n2.4 GHz applications. Beamforming is another hot topic in\nantenna designs and performance of beamforming with down\u0002tilted antennas for UAV communications applications is\npresented in [26].\n8) INTERFERENCE MANAGEMENT\nInterference is one of the inevitable problems with wireless\ncommunications systems, and it requires specific solutions to\nhandle it. Some possible solutions for interference problems\nmentioned by standardization institutions for UAV\ncommunications users are surveyed in [6]. As a novel\napproach, [28] it proposes an interference-aware path planning\nstructure for a UAV communication network. Their main\ncontribution is for solving the tradeoff problem between\nminimizing both the interference caused on the ground\nnetwork along its route and maximizing the energy efficiency.\nThey propose a deep reinforcement method based on the echo\nstate network (ESN). In their system, each UAV utilizes the\nESN to identify the optimal transmission power, path, and cell\nassociation vector at varying positions along its route. Also,\nthe proposed method is supported by simulation results. The\ninterference management problem in UAV communications\ncontains many possible approaches in itself thanks to ML\ntechniques for further studies.\n9) GPS ARCHITECTURE FOR SYSTEM REDUNDANCY\nThere are many applications for determining the position of\nUAVs exactly about Real Time Kinematic GPS (RTK-GPS).\nNevertheless, these applications are found to be instable for\nnetworks and cause some errors in services. Thus, GPS\narchitecture is another problem that has to be solved for better\nsystems. An effective architecture for improving system\nperformance is proposed in [29] by parallelizing or switching\nthe GPS data resources. They also discuss how to allocate\nmessages through the UAV network. Evaluation on empirical\ntestbeds and validation results are also available. Figure 14\nshows an example propagation system model.\n10) CYBER-PHYSICAL SECURITY\nIt is unarguable that security is so critical to any\ncommunication system. It can be considered as a more crucial\nproblem for UAV communication applications as UAVs\nrequire remote control by nature. When UAVs become an\ninevitable part of cellular systems, security becomes more\nimportant for providing coherence in networks. There is a\ncomprehensive background about security in UAV\ncommunication are introduced in [6]. Cyber-attacks on UAV\nsystems are increasing day by day. Thus, techniques should be\nFIGURE 14. Radio technical commission maritime services propagation system\ndesign.\ndeveloped continuously. Examples of these threats can be\ncounted as jamming, hijacking, eavesdropping, denial of\nservice, and spoofing. In addition to cyber-attacks, there may\nalso be physical attacks. These attacks are detailed in [6]. As a\nspecific example, a homomorphic cryptography system is\nproposed in [30] to provide the security of controllers. They\nalso proposed a linearly homomorphic authenticated\nencryption (LinHAE) architecture to make real-time\noperations safer for autonomous flight.\nA comprehensive overview of PLS within the context of\nUAV systems is provided in [228], with an examination of\nvarious communication channels, including ground-to\u0002ground, ground-to-air, A2G, and A2A [236]. PLS metrics,\nsuch as secrecy outage probability, average secrecy capacity,\nand the probability of strictly positive secrecy capacity, are\nstudied in UAV-to-ground communications, considering the\npresence of shadowing, as discussed in [237]. Also, an\noverview of the improvement of the PLS of UAV networks by\nthe IRS is presented in [238]. Various use cases of PLS for\nenhancing UAV communications by the IRS are examined,\nand recent advancements in this field are briefly summarized.\nNonetheless, a significant limitation of IRS lies in its\nconfinement of communication to the reflective dimension,\nthereby rendering it inaccessible to users positioned behind the\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n34\nIRS surface. In [239], the authors proposed the utilization of\nintelligent omni-surfaces as an alternative to IRS, alongside\nthe deployment of UAVs, to attain secure communication\nwithin an IoT communication system.\nAn emergent self-Awareness module is suggested to be\nincorporated into the physical layer of cognitive UAV radios\nfor the enhancement of PLS, particularly in the context of\ncountering jamming attacks. Self-Awareness is founded on the\nacquisition of a hierarchical representation of the radio\nenvironment through the utilization of a proposed hierarchical\ndynamic Bayesian network. DL-empowered punctured low\ndensity parity check codes are introduced in [240] for the\npurpose of ensuring secure and dependable data transmission\nfor UAVs over the additive white gaussian noise channel,\nindependent of the computational capabilities and channel\nstate information (CSI) of the Eavesdropper. Similarly, the\nauthors in [241] conducted the analysis of PLS for a dual-hop\nwireless network based on UAV, taking into account\nimperfect CSI and the influence of mobility effects.\nThe PLS of UAV-based communication is enhanced\nthrough the implementation of NOMA techniques. In [242],\nthe authors examined the secrecy performance of a full-duplex\nrelay NOMA system utilizing UAVs over the Nakagami-m\nfading channel. The insertion of an artificial noise component\ninto the transmit signal of the full-duplex aerial relay station,\nwith power allocation based on the NOMA protocol, is\nsuggested for the purpose of ensuring secure communication\nfor users. Besides, the secrecy outage probability in the\ncontext of in terrestrial networks aided by UAVs is\ninvestigated in [243], utilizing cooperative user selection. In\nthis system, the user is chosen from those directly connected\nto the UAV. With a particular emphasis on achieving optimal\nresource allocation, the research in [244] delved into\noptimizing PLS for UAV communication by employing\nNOMA. It studied the feasibility of pairing users with\ntrustworthiness disparities and the influence of optimal power\nallocation coefficients.\nVIII. CONCLUSION\nIn this paper, a comprehensive survey of the deployment\nscenarios, applications, emerging technologies, regulatory\naspects, research trends, and challenges associated with the\nuse of UAVs in 5G-and-beyond networks has been presented.\nThe paper begins with a brief background on UAVs and 5G\nnetworks, followed by a systematic classification of UAVs\nand a review of relevant works. Various UAV deployment\nscenarios, including single and multiple UAV configurations,\nare then discussed. UAV applications in 5G are categorized,\nand emerging technologies for enhancing UAV\ncommunications are investigated. Additionally, regulatory\nconsiderations, such as flight guidelines, spectrum allocation,\nprivacy, and safety, are addressed in the context of deploying\nUAVs in 5G networks. The latest research trends and open\nchallenges in the field are highlighted, and promising\ndirections for future investigations are identified. This survey\nis intended to serve as a valuable resource for researchers,\npractitioners, and policymakers in the UAV and\ncommunication domains, providing a comprehensive\noverview of the state-of-the-art in UAV-enabled 5G networks\nand outlining the key challenges and research directions\nessential for realizing the full potential of this technology. The\nhope is that this survey will inspire further research in this\nexciting area and accelerate the development of UAV-enabled\n5G networks.\nABBREVIATIONS LIST\nTerm Description\n3GPP 3rd generation partnership project\n5G fifth generation\n6G sixth generation\nA2A air-to-air\nA2G air-to-ground\nABS aerial base stations\nAI artificial intelligence\nAmBC ambient backscattering communications\nBS base stations\nBVLOS beyond visual line of sight\nCSI channel state information\nCURS closest UAV Relay Selection\nD2D device-to-device\nDL deep learning\nDNN deep neural network\nDRL deep reinforcement learning\neMBB enhanced mobile broadband\nESN echo state network\nFAA federal aviation authority\nFANETs flying ad-hoc networks\nFCC federal communications commission\nFCSD Fog computing-aided swarm of drones\nFDMA frequency division multiple access\nHetNets heterogeneous networks\nHVOR highest velocity opportunistic routing\nIoD Internet of drones\nIoT Internet of things\nIRS intelligent reflecting surfaces\nITC interference transmission and cancellation\nLoS line-of-sight\nLTE long term evolution\nMEC mobile edge computing\nMIMO multiple input multiple output\nML machine learning\nM-MIMO massive multiple input multiple output\nmMTC massive machine-type communication\nmmWave millimeter wave\nMURS maximum UAV relay selection\nNASA national aeronautics and space administration\nNOMA non-orthogonal multiple access\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n35\nNR new radio\nOMA orthogonal multiple access\nOR opportunistic routing\nPLS physical layer security\nQoS quality of service\nRL reinforcement learning\nRSRP reference signal received power\nRF radio frequency\nSAA sense-and-avoid\nSBSs small base stations\nSNR signal-to-noise ratio\nSWIPT simultaneous wireless information and power transfer\nSR symbiotic radio\nSCAS symbiotic communication and sensing\nTBSs terrestrial base stations\nTDMA time division multiple access\nUASs unmanned aerial systems\nUAV unmanned aerial vehicles\nUE user equipment\nURS UAV relay selection\nV2V vehicle-to-vehicle\nWPCNs wireless powered communication networks\nREFERENCES\n[1] M. Banafaa, M. \u00d6zg\u00fcm\u00fc\u015f, R. Ekin, I. Shayea, and A.\nAlhammadi, \"Connected Drone in Future Mobile Networks,\" in\n2022 Workshop on Microwave Theory and Techniques in\nWireless Communications (MTTW), 2022, pp. 183-188: IEEE.\n[2] I. Shayea et al., \"Handover Management for Drones in Future\nMobile Networks\u2014A Survey,\" vol. 22, no. 17, p. 6424, 2022.\n[3] M. Mozaffari, W. Saad, M. Bennis, Y. Nam, and M. Debbah, \"A\nTutorial on UAVs for Wireless Networks: Applications,\nChallenges, and Open Problems,\" IEEE Communications\nSurveys & Tutorials, vol. 21, no. 3, pp. 2334-2360, 2019.\n[4] S. A. H. Mohsan, M. A. Khan, F. Noor, I. Ullah, and M. H.\nAlsharif, \"Towards the unmanned aerial vehicles (UAVs): A\ncomprehensive review,\" Drones, vol. 6, no. 6, p. 147, 2022.\n[5] P. S. Bithas, E. T. Michailidis, N. Nomikos, D. Vouyioukas, and\nA. G. J. S. Kanatas, \"A survey on machine-learning techniques\nfor UAV-based communications,\" vol. 19, no. 23, p. 5170, 2019.\n[6] A. Fotouhi et al., \"Survey on UAV Cellular Communications:\nPractical Aspects, Standardization Advancements, Regulation,\nand Security Challenges,\" IEEE Communications Surveys &\nTutorials, vol. 21, no. 4, pp. 3417-3442, 2019.\n[7] J. Wang et al., \"Physical layer security for UAV\ncommunications: A comprehensive survey,\" China\nCommunications, vol. 19, no. 9, pp. 77-115, 2022.\n[8] S. I. J. I. Han, \"Survey on UAV deployment and trajectory in\nwireless communication networks: applications and challenges,\"\nvol. 13, no. 8, p. 389, 2022.\n[9] A. Sharma et al., \"Communication and networking technologies\nfor UAVs: A survey,\" vol. 168, p. 102739, 2020.\n[10] A. I. Hentati, L. C. J. C. S. Fourati, and Interfaces,\n\"Comprehensive survey of UAVs communication networks,\"\nvol. 72, p. 103451, 2020.\n[11] Z. Xiao et al., \"A survey on millimeter-wave beamforming\nenabled UAV communications and networking,\" IEEE\nCommunications Surveys & Tutorials, vol. 24, no. 1, pp. 557-\n610, 2021.\n[12] G. Geraci et al., \"What will the future of UAV cellular\ncommunications be? A flight from 5G to 6G,\" IEEE\ncommunications surveys & tutorials, vol. 24, no. 3, pp. 1304-\n1335, 2022.\n[13] P. McEnroe, S. Wang, and M. Liyanage, \"A survey on the\nconvergence of edge computing and AI for UAVs: Opportunities\nand challenges,\" IEEE Internet of Things Journal, vol. 9, no. 17,\npp. 15435-15459, 2022.\n[14] N. Nomikos, P. K. Gkonis, P. S. Bithas, and P. Trakadas, \"A\nsurvey on UAV-aided maritime communications: Deployment\nconsiderations, applications, and future challenges,\" IEEE Open\nJournal of the Communications Society, vol. 4, pp. 56-78, 2022.\n[15] M. A. Jasim, H. Shakhatreh, N. Siasi, A. H. Sawalmeh, A.\nAldalbahi, and A. Al-Fuqaha, \"A survey on spectrum\nmanagement for unmanned aerial vehicles (uavs),\" IEEE Access,\nvol. 10, pp. 11443-11499, 2021.\n[16] C. Xu, X. Liao, J. Tan, H. Ye, and H. Lu, \"Recent research\nprogress of unmanned aerial vehicle regulation policies and\ntechnologies in urban low altitude,\" IEEE Access, vol. 8, pp.\n74175-74194, 2020.\n[17] S. Hafeez et al., \"Blockchain-Assisted UAV Communication\nSystems: A Comprehensive Survey,\" IEEE Open Journal of\nVehicular Technology, 2023.\n[18] Z. Wei et al., \"UAV-assisted data collection for internet of\nthings: A survey,\" IEEE Internet of Things Journal, vol. 9, no.\n17, pp. 15460-15483, 2022.\n[19] T. Q. Duong, K. J. Kim, Z. Kaleem, M.-P. Bui, and N.-S. Vo,\n\"UAV caching in 6G networks: A Survey on models, techniques,\nand applications,\" Physical Communication, vol. 51, p. 101532,\n2022.\n[20] H. Ullah, N. G. Nair, A. Moore, C. Nugent, P. Muschamp, and\nM. Cuevas, \"5G Communication: An Overview of Vehicle-to\u0002Everything, Drones, and Healthcare Use-Cases,\" IEEE Access,\nvol. 7, pp. 37251-37268, 2019.\n[21] X. Hou, Z. Ren, J. Wang, S. Zheng, W. Cheng, and H. Zhang,\n\"Distributed Fog Computing for Latency and Reliability\nGuaranteed Swarm of Drones,\" IEEE Access, vol. 8, pp. 7117-\n7130, 2020.\n[22] D. Wu et al., \"ADDSEN: Adaptive Data Processing and\nDissemination for Drone Swarms in Urban Sensing,\" IEEE\nTransactions on Computers, vol. 66, no. 2, pp. 183-198, 2017.\n[23] P. A. Catherwood, B. Black, E. B. Mohamed, A. A. Cheema, J.\nRafferty, and J. A. D. Mclaughlin, \"Radio Channel\nCharacterization of Mid-Band 5G Service Delivery for Ultra\u0002Low Altitude Aerial Base Stations,\" IEEE Access, vol. 7, pp.\n8283-8299, 2019.\n[24] M. M. Azari, F. Rosas, and S. Pollin, \"Cellular Connectivity for\nUAVs: Network Modeling, Performance Analysis, and Design\nGuidelines,\" IEEE Transactions on Wireless Communications,\nvol. 18, no. 7, pp. 3366-3381, 2019.\n[25] Z. Liang, Y. Li, J. Liu, J. Qin, and Y. Long, \"Reconfigurable\nMicrostrip Magnetic Dipole Antenna With Switchable Conical\nBeams for Aerial Drone Applications,\" IEEE Access, vol. 7, pp.\n31043-31054, 2019.\n[26] R. Amer, W. Saad, and N. Marchetti, \"Toward a Connected Sky:\nPerformance of Beamforming With Down-Tilted Antennas for\nGround and UAV User Co-Existence,\" IEEE Communications\nLetters, vol. 23, no. 10, pp. 1840-1844, 2019.\n[27] M. K. Banafaa, M. H. Jamaluddin, S. H. Dahlan, and A. A.\nAlthuwayb, \"Miniature Dual Band Button Antenna Using\nCylindrical Dielectric Resonator Antenna for On/Off Body\nCommunication Devices,\" The Applied Computational\nElectromagnetics Society Journal (ACES), pp. 479-485, 2021.\n[28] U. Challita, W. Saad, and C. Bettstetter, \"Interference\nManagement for Cellular-Connected UAVs: A Deep\nReinforcement Learning Approach,\" IEEE Transactions on\nWireless Communications, vol. 18, no. 4, pp. 2125-2140, 2019.\n[29] I. Um, S. Park, H. T. Kim, and H. Kim, \"Configuring RTK-GPS\nArchitecture for System Redundancy in Multi-Drone\nOperations,\" IEEE Access, vol. 8, pp. 76228-76242, 2020.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n36\n[30] J. H. Cheon et al., \"Toward a Secure Drone System: Flying With\nReal-Time Homomorphic Authenticated Encryption,\" IEEE\nAccess, vol. 6, pp. 24325-24339, 2018.\n[31] A. Garcia-Rodriguez, G. Geraci, D. Lopez-Perez, L. G.\nGiordano, M. Ding, and E. Bjornson, \"The Essential Guide to\nRealizing 5G-Connected UAVs with Massive MIMO,\" IEEE\nCommunications Magazine, vol. 57, no. 12, pp. 84-90, 2019.\n[32] Y. Zeng, J. Lyu, and R. Zhang, \"Cellular-Connected UAV:\nPotential, Challenges, and Promising Technologies,\" IEEE\nWireless Communications, vol. 26, no. 1, pp. 120-127, 2019.\n[33] X. Lin et al., \"Mobile Network-Connected Drones: Field Trials,\nSimulations, and Design Insights,\" IEEE Vehicular Technology\nMagazine, vol. 14, no. 3, pp. 115-125, 2019.\n[34] R. Amer, W. Saad, and N. Marchetti, \"Mobility in the Sky:\nPerformance and Mobility Analysis for Cellular-Connected\nUAVs,\" IEEE Transactions on Communications, vol. 68, no. 5,\npp. 3229-3246, 2020.\n[35] W. Shi, H. Zhou, J. Li, W. Xu, N. Zhang, and X. Shen, \"Drone\nAssisted Vehicular Networks: Architecture, Challenges and\nOpportunities,\" IEEE Network, vol. 32, no. 3, pp. 130-137, 2018.\n[36] M. Mozaffari, A. T. Z. Kasgari, W. Saad, M. Bennis, and M.\nDebbah, \"Beyond 5G With UAVs: Foundations of a 3D Wireless\nCellular Network,\" IEEE Transactions on Wireless\nCommunications, vol. 18, no. 1, pp. 357-372, 2019.\n[37] Y. Zeng, S. Jin, Q. Wu, and F. Gao, \"Network-connected UAV\ncommunications,\" China Communications, vol. 15, no. 5, pp. iii\u0002v, 2018.\n[38] J. Cui, H. Shakhatreh, B. Hu, S. Chen, and C. Wang, \"Power\u0002Efficient Deployment of a UAV for Emergency Indoor Wireless\nCoverage,\" IEEE Access, vol. 6, pp. 73200-73209, 2018-01-01\n2018.\n[39] X. Wang and L. Duan, \"Economic Analysis of Unmanned Aerial\nVehicle (UAV) Provided Mobile Services,\" IEEE Transactions\non Mobile Computing, vol. 20, no. 5, pp. 1804-1816, 2021-05-\n01 2021.\n[40] S. Yin, L. Li, and F. R. Yu, \"Resource Allocation and\nBasestation Placement in Downlink Cellular Networks Assisted\nby Multiple Wireless Powered UAVs,\" IEEE Transactions on\nVehicular Technology, vol. 69, no. 2, pp. 2171-2184, 2020-02-\n01 2020.\n[41] S. Chai and V. K. N. Lau, \"Multi-UAV Trajectory and Power\nOptimization for Cached UAV Wireless Networks With Energy\nand Content Recharging-Demand Driven Deep Learning\nApproach,\" IEEE Journal on Selected Areas in\nCommunications, vol. 39, no. 10, pp. 3208-3224, 2021-10-01\n2021.\n[42] X. Zhu, C. Bian, Y. Chen, and S. Chen, \"A Low Latency\nClustering Method for Large-Scale Drone Swarms,\" IEEE\nAccess, vol. 7, pp. 186260-186267, 2019-01-01 2019.\n[43] F. Aftab, A. Khan, and Z. Zhang, \"Hybrid Self-Organized\nClustering Scheme for Drone Based Cognitive Internet of\nThings,\" IEEE Access, vol. 7, pp. 56217-56227, 2019-01-01\n2019.\n[44] L. Merino, F. Caballero, J. Martinez-de Dios, and A. Ollero,\n\"Cooperative fire detection using unmanned aerial vehicles,\" in\nProceedings of the 2005 IEEE international conference on\nrobotics and automation, 2005, pp. 1884-1889: IEEE.\n[45] B. Olivieri and M. Endler, \"An algorithm for aerial data\ncollection from wireless sensors networks by groups of UAVs,\"\nin 2017 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), 2017, pp. 967-972: IEEE.\n[46] X. Ma, S. Chisiu, R. Kacimi, and R. Dhaou, \"Opportunistic\ncommunications in WSN using UAV,\" in 2017 14th IEEE\nAnnual Consumer Communications & Networking Conference\n(CCNC), 2017, pp. 510-515: IEEE.\n[47] A. Abdelmaboud, \"The internet of drones: Requirements,\ntaxonomy, recent advances, and challenges of research trends,\"\nSensors, vol. 21, no. 17, p. 5718, 2021.\n[48] N. Cheng et al., \"AI for UAV-Assisted IoT Applications: A\nComprehensive Review,\" IEEE Internet of Things Journal,\n2023.\n[49] M. Mozaffari, W. Saad, M. Bennis, and M. J. I. T. o. W. C.\nDebbah, \"Mobile unmanned aerial vehicles (UAVs) for energy\u0002efficient Internet of Things communications,\" vol. 16, no. 11, pp.\n7574-7589, 2017.\n[50] Z. Shah, M. Naeem, U. Javed, W. Ejaz, and M. Altaf, \"A\ncompendium of radio resource management in UAV-assisted\nnext generation computing paradigms,\" Ad Hoc Networks, vol.\n131, p. 102844, 2022.\n[51] A. Koub\u00e2a, B. Qureshi, M.-F. Sriti, Y. Javed, and E. Tovar, \"A\nservice-oriented Cloud-based management system for the\nInternet-of-Drones,\" in 2017 IEEE International Conference on\nAutonomous Robot Systems and Competitions (ICARSC), 2017,\npp. 329-335: IEEE.\n[52] B. Fei, W. Bao, X. Zhu, D. Liu, T. Men, and Z. Xiao,\n\"Autonomous cooperative search model for multi-UAV with\nlimited communication network,\" IEEE Internet of Things\nJournal, vol. 9, no. 19, pp. 19346-19361, 2022.\n[53] T. Hardes and C. Sommer, \"Opportunistic UAV Relaying for\nUrban Vehicular Networks,\" in 2022 17th Wireless On-Demand\nNetwork Systems and Services Conference (WONS), 2022, pp. 1-\n8: IEEE.\n[54] R. Ding, J. Chen, W. Wu, J. Liu, F. Gao, and X. Shen, \"Packet\nrouting in dynamic multi-hop UAV relay network: A multi-agent\nlearning approach,\" IEEE Transactions on Vehicular\nTechnology, vol. 71, no. 9, pp. 10059-10072, 2022.\n[55] C. Barroca, A. Grilo, and P. R. Pereira, \"Improving message\ndelivery in UAV-based delay tolerant networks,\" in 2018 16th\nInternational Conference on Intelligent Transportation Systems\nTelecommunications (ITST), 2018, pp. 1-7: IEEE.\n[56] T. Spyropoulos, K. Psounis, and C. S. Raghavendra, \"Spray and\nwait: an efficient routing scheme for intermittently connected\nmobile networks,\" in Proceedings of the 2005 ACM SIGCOMM\nworkshop on Delay-tolerant networking, 2005, pp. 252-259.\n[57] I. A. Elnabty, Y. Fahmy, and M. Kafafy, \"A survey on UAV\nplacement optimization for UAV-assisted communication in 5G\nand beyond networks,\" Physical Communication, vol. 51, p.\n101564, 2022.\n[58] O. Semiari, W. Saad, and M. J. I. T. o. W. C. Bennis, \"Joint\nmillimeter wave and microwave resources allocation in cellular\nnetworks with dual-mode base stations,\" vol. 16, no. 7, pp. 4802-\n4816, 2017.\n[59] R. Kunst, E. Pignaton, T. Zhou, and H. Hu, \"Application of\nfuture 6G technology to support heavy data traffic in highly\nmobile networks,\" in 2020 First International Conference of\nSmart Systems and Emerging Technologies (SMARTTECH),\n2020, pp. 144-148: IEEE.\n[60] G. Castellanos, G. Vallero, M. Deruyck, L. Martens, M. Meo,\nand W. Joseph, \"Evaluation of flying caching servers in UAV\u0002BS based realistic environment,\" Vehicular Communications,\nvol. 32, p. 100390, 2021.\n[61] H. Shen, Q. Ye, W. Zhuang, W. Shi, G. Bai, and G. Yang,\n\"Drone-Small-Cell-Assisted Resource Slicing for 5G Uplink\nRadio Access Networks,\" IEEE Transactions on Vehicular\nTechnology, vol. 70, no. 7, pp. 7071-7086, 2021-07-01 2021.\n[62] M. S. Haroon et al., \"Interference Management in Ultra-Dense\n5G Networks With Excessive Drone Usage,\" IEEE Access, vol.\n8, pp. 102155-102164, 2020-01-01 2020.\n[63] F. Al-Turjman, J. P. Lemayian, S. Alturjman, and L. Mostarda,\n\"Enhanced Deployment Strategy for the 5G Drone-BS Using\nArtificial Intelligence,\" IEEE Access, vol. 7, pp. 75999-76008,\n2019-01-01 2019.\n[64] S. Iranmanesh, F. S. Abkenar, R. Raad, and A. Jamalipour,\n\"Improving Throughput of 5G Cellular Networks via 3D\nPlacement Optimization of Logistics Drones,\" IEEE\nTransactions on Vehicular Technology, vol. 70, no. 2, pp. 1448-\n1460, 2021-02-01 2021.\n[65] N. Mehallegue, M. Djellab, and K. Loukhaoukha, \"Efficient use\nof UAVs for public safety in disaster and crisis management,\"\nWireless Personal Communications, vol. 116, no. 1, pp. 369-\n380, 2021.\n[66] S. K. Khan, U. Naseem, H. Siraj, I. Razzak, and M. Imran, \"The\nrole of unmanned aerial vehicles and mmWave in 5G: Recent\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n37\nadvances and challenges,\" Transactions on Emerging\nTelecommunications Technologies, vol. 32, no. 7, p. e4241,\n2021.\n[67] C. Zhang, W. Zhang, W. Wang, L. Yang, and W. Zhang,\n\"Research challenges and opportunities of UAV millimeter\u0002wave communications,\" IEEE Wireless Communications, vol.\n26, no. 1, pp. 58-62, 2019.\n[68] J. Sabzehali, V. K. Shah, H. S. Dhillon, and J. H. Reed, \"3D\nplacement and orientation of mmWave-based UAVs for\nguaranteed LoS coverage,\" IEEE Wireless Communications\nLetters, vol. 10, no. 8, pp. 1662-1666, 2021.\n[69] V. Begishev, D. Moltchanov, A. Gaidamaka, and K. Samouylov,\n\"Closed-Form UAV LoS Blockage Probability in Mixed\nGround-and Rooftop-Mounted Urban mmWave NR\nDeployments,\" Sensors, vol. 22, no. 3, p. 977, 2022.\n[70] M. Mozaffari, W. Saad, M. Bennis, and M. J. I. T. o. C. Debbah,\n\"Communications and control for wireless drone-based antenna\narray,\" vol. 67, no. 1, pp. 820-834, 2018.\n[71] S. D. Muruganathan et al., \"An overview of 3GPP release-15\nstudy on enhanced LTE support for connected drones,\" vol. 5,\nno. 4, pp. 140-146, 2021.\n[72] M. Whaiduzzaman et al., \"A review of emerging technologies\nfor IoT-based smart cities,\" Sensors, vol. 22, no. 23, p. 9271,\n2022.\n[73] N. Abbas, Z. Abbas, X. Liu, S. S. Khan, E. D. Foster, and S.\nLarkin, \"A Survey: Future Smart Cities Based on Advance\nControl of Unmanned Aerial Vehicles (UAVs),\" Applied\nSciences, vol. 13, no. 17, p. 9881, 2023.\n[74] Q. Zhu, J. Zheng, and A. Jamalipour, \"Coverage Performance\nAnalysis of a Cache-Enabled UAV Base Station Assisted\nCellular Network,\" IEEE Transactions on Wireless\nCommunications, 2023.\n[75] R. Amer, W. Saad, H. ElSawy, M. M. Butt, and N. Marchetti,\n\"Caching to the sky: Performance analysis of cache-assisted\nCoMP for cellular-connected UAVs,\" in 2019 IEEE Wireless\nCommunications and Networking Conference (WCNC), 2019,\npp. 1-6: IEEE.\n[76] E. Balestrieri, P. Daponte, L. De Vito, F. Picariello, and I.\nTudosa, \"Sensors and measurements for UAV safety: An\noverview,\" Sensors, vol. 21, no. 24, p. 8253, 2021.\n[77] P. H\u00fcgler, F. Roos, M. Schartel, M. Geiger, and C. J. I. M. M.\nWaldschmidt, \"Radar taking off: New capabilities for UAVs,\"\nvol. 19, no. 7, pp. 43-53, 2018.\n[78] Z. Xiao and Y. J. S. C. I. S. Zeng, \"An overview on integrated\nlocalization and communication towards 6G,\" vol. 65, pp. 1-46,\n2022.\n[79] M. Banafaa et al., \"6G mobile communication technology:\nRequirements, targets, applications, challenges, advantages, and\nopportunities,\" 2022.\n[80] I. Guvenc, F. Koohifar, S. Singh, M. L. Sichitiu, and D. J. I. C.\nM. Matolak, \"Detection, tracking, and interdiction for amateur\ndrones,\" vol. 56, no. 4, pp. 75-81, 2018.\n[81] Y. Mekdad et al., \"A survey on security and privacy issues of\nUAVs,\" Computer Networks, vol. 224, p. 109626, 2023.\n[82] G. Yang et al., \"Unmanned aerial vehicle remote sensing for\nfield-based crop phenotyping: current status and perspectives,\"\nvol. 8, p. 1111, 2017.\n[83] Z. Hu, Z. Bai, Y. Yang, Z. Zheng, K. Bian, and L. J. I. N. Song,\n\"UAV aided aerial-ground IoT for air quality sensing in smart\ncity: Architecture, technologies, and implementation,\" vol. 33,\nno. 2, pp. 14-22, 2019.\n[84] A. Guerra, D. Dardari, and P. M. J. I. V. T. M. Djuric, \"Dynamic\nradar networks of UAVs: A tutorial overview and tracking\nperformance comparison with terrestrial radar networks,\" vol.\n15, no. 2, pp. 113-120, 2020.\n[85] S. Homayouni et al., \"On the Feasibility of Cellular-Connected\nDrones in Existing 4G/5G Networks: Field Trials,\" in 2021 IEEE\n4th 5G World Forum (5GWF), 2021: IEEE.\n[86] S. Homayouni, M. Paier, C. Benischek, G. Pernjak, M. Reichelt,\nand C. Fuchsjager, \"Field Trials and Design Insights of Cellular\u0002Connected Drones,\" in 2021 IEEE 94th Vehicular Technology\nConference (VTC2021-Fall), 2021: IEEE.\n[87] R. Amer, W. Saad, B. Galkin, and N. Marchetti, \"Performance\nAnalysis of Mobile Cellular-Connected Drones under Practical\nAntenna Configurations,\" in ICC 2020 - 2020 IEEE\nInternational Conference on Communications (ICC), 2020:\nIEEE.\n[88] A. Azari, F. Ghavimi, M. Ozger, R. Jantti, and C. Cavdar,\n\"Machine Learning assisted Handover and Resource\nManagement for Cellular Connected Drones,\" in 2020 IEEE 91st\nVehicular Technology Conference (VTC2020-Spring), 2020:\nIEEE.\n[89] M. M. Azari, F. Rosas, and S. J. I. T. o. W. C. Pollin, \"Cellular\nconnectivity for UAVs: Network modeling, performance\nanalysis, and design guidelines,\" vol. 18, no. 7, pp. 3366-3381,\n2019.\n[90] W. Mei, Q. Wu, and R. J. I. T. o. w. c. Zhang, \"Cellular\u0002connected UAV: Uplink association, power control and\ninterference coordination,\" vol. 18, no. 11, pp. 5380-5393, 2019.\n[91] B. Li, Z. Fei, Y. Zhang, and M. J. I. W. C. Guizani, \"Secure UAV\ncommunication networks over 5G,\" vol. 26, no. 5, pp. 114-120,\n2019.\n[92] J. Li, D. Lu, G. Zhang, J. Tian, and Y. J. I. A. Pang, \"Post-disaster\nunmanned aerial vehicle base station deployment method based\non artificial bee colony algorithm,\" vol. 7, pp. 168327-168336,\n2019.\n[93] A. S. Parihar and S. K. Chakraborty, \"Flying Ad Hoc Network\n(FANET): Opportunities, Trending Applications and\nSimulators,\" in 2022 IEEE Pune Section International\nConference (PuneCon), 2022, pp. 1-5: IEEE.\n[94] F. Noor, M. A. Khan, A. Al-Zahrani, I. Ullah, and K. A. Al\u0002Dhlan, \"A review on communications perspective of flying ad\u0002hoc networks: key enabling wireless technologies, applications,\nchallenges and open research topics,\" Drones, vol. 4, no. 4, p.\n65, 2020.\n[95] F. Pasandideh, J. P. J. da Costa, R. Kunst, N. Islam, W.\nHardjawana, and E. Pignaton de Freitas, \"A review of flying ad\nhoc networks: Key characteristics, applications, and wireless\ntechnologies,\" Remote Sensing, vol. 14, no. 18, p. 4459, 2022.\n[96] A. Srivastava and J. Prakash, \"Future FANET with application\nand enabling techniques: Anatomization and sustainability\nissues,\" Computer science review, vol. 39, p. 100359, 2021.\n[97] M. Giordani and M. J. I. N. Zorzi, \"Non-terrestrial networks in\nthe 6G era: Challenges and opportunities,\" vol. 35, no. 2, pp.\n244-251, 2020.\n[98] M. Marchese, A. Moheddine, and F. J. S. Patrone, \"IoT and UAV\nintegration in 5G hybrid terrestrial-satellite networks,\" vol. 19,\nno. 17, p. 3704, 2019.\n[99] X. Liu, M. Lin, Q. Huang, J. Wang, and J. J. P. C. Ouyang,\n\"Performance analysis for multi-user integrated satellite and\nUAV cooperative networks,\" vol. 36, p. 100762, 2019.\n[100] P. K. Sharma and D. I. J. I. T. o. W. C. Kim, \"Secure 3D mobile\nUAV relaying for hybrid satellite-terrestrial networks,\" vol. 19,\nno. 4, pp. 2770-2784, 2020.\n[101] D. Hein, T. Kraft, J. Brauchle, and R. J. I. I. J. o. G.-I. Berger,\n\"Integrated UAV-based real-time mapping for security\napplications,\" vol. 8, no. 5, p. 219, 2019.\n[102] B. Li, S. Zhao, R. Miao, and R. Zhang, \"A survey on unmanned\naerial vehicle relaying networks,\" IET Communications, vol. 15,\nno. 10, pp. 1262-1272, 2021.\n[103] D. Liu et al., \"Task-driven relay assignment in distributed UAV\ncommunication networks,\" vol. 68, no. 11, pp. 11003-11017,\n2019.\n[104] N. Cheng et al., \"AI for UAV-Assisted IoT Applications: A\nComprehensive Review,\" 2023.\n[105] H. Dai, H. Zhang, C. Li, and B. J. C. C. Wang, \"Efficient\ndeployment of multiple UAVs for IoT communication in\ndynamic environment,\" vol. 17, no. 1, pp. 89-103, 2020.\n[106] Y. Zhou, B. Rao, and W. J. I. A. Wang, \"UAV swarm\nintelligence: Recent advances and future trends,\" vol. 8, pp.\n183856-183878, 2020.\n[107] C. Qin, F. Candan, L. S. Mihaylova, and E. J. a. p. a. Pournaras,\n\"3, 2, 1, Drones Go! a testbed to take off UAV swarm\nintelligence for distributed sensing,\" 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n38\n[108] J. Schwarzrock, I. Zacarias, A. L. Bazzan, R. Q. de Araujo\nFernandes, L. H. Moreira, and E. P. J. E. A. o. A. I. de Freitas,\n\"Solving task allocation problem in multi unmanned aerial\nvehicles systems using swarm intelligence,\" vol. 72, pp. 10-20,\n2018.\n[109] M. Chen, H. Wang, C.-Y. Chang, and X. J. I. A. Wei, \"SIDR: A\nswarm intelligence-based damage-resilient mechanism for UAV\nswarm networks,\" vol. 8, pp. 77089-77105, 2020.\n[110] Y. Zeng, Q. Wu, and R. J. P. o. t. I. Zhang, \"Accessing from the\nsky: A tutorial on UAV communications for 5G and beyond,\"\nvol. 107, no. 12, pp. 2327-2375, 2019.\n[111] Y. Huang, Q. Wu, R. Lu, X. Peng, and R. J. I. C. M. Zhang,\n\"Massive MIMO for cellular-connected UAV: Challenges and\npromising solutions,\" vol. 59, no. 2, pp. 84-90, 2021.\n[112] Y. Huang, Q. Wu, T. Wang, G. Zhou, and R. J. I. W. C. L. Zhang,\n\"3D beam tracking for cellular-connected UAV,\" vol. 9, no. 5,\npp. 736-740, 2020.\n[113] H. Q. Ngo, A. Ashikhmin, H. Yang, E. G. Larsson, and T. L. J.\nI. T. o. W. C. Marzetta, \"Cell-free massive MIMO versus small\ncells,\" vol. 16, no. 3, pp. 1834-1850, 2017.\n[114] C. D\u2019Andrea, A. Garcia-Rodriguez, G. Geraci, L. G. Giordano,\nand S. J. I. O. J. o. t. C. S. Buzzi, \"Analysis of UAV\ncommunications in cell-free massive MIMO systems,\" vol. 1,\npp. 133-147, 2020.\n[115] W. Yuan, C. Liu, F. Liu, S. Li, and D. W. K. J. I. W. C. L. Ng,\n\"Learning-based predictive beamforming for UAV\ncommunications with jittering,\" vol. 9, no. 11, pp. 1970-1974,\n2020.\n[116] Z. Feng, L. Ji, Q. Zhang, and W. J. I. C. M. Li, \"Spectrum\nmanagement for mmWave enabled UAV swarm networks:\nChallenges and opportunities,\" vol. 57, no. 1, pp. 146-153, 2018.\n[117] R. Hussain and M. S. Sharawi, \"5G MIMO antenna designs for\nbase station and user equipment: Some recent developments and\ntrends,\" IEEE Antennas and Propagation Magazine, vol. 64, no.\n3, pp. 95-107, 2021.\n[118] Q. Wu, S. Zhang, B. Zheng, C. You, and R. J. I. T. o. C. Zhang,\n\"Intelligent reflecting surface-aided wireless communications: A\ntutorial,\" vol. 69, no. 5, pp. 3313-3351, 2021.\n[119] Y. Liu, J. Zhao, M. Li, and Q. J. I. T. o. C. Wu, \"Intelligent\nreflecting surface aided MISO uplink communication network:\nFeasibility and power minimization for perfect and imperfect\nCSI,\" vol. 69, no. 3, pp. 1975-1989, 2020.\n[120] Q. Wu, X. Zhou, and R. J. I. W. C. L. Schober, \"IRS-assisted\nwireless powered NOMA: Do we really need different phase\nshifts in DL and UL?,\" vol. 10, no. 7, pp. 1493-1497, 2021.\n[121] C. Chaccour, M. N. Soorki, W. Saad, M. Bennis, and P.\nPopovski, \"Risk-based optimization of virtual reality over\nterahertz reconfigurable intelligent surfaces,\" in ICC 2020-2020\nIEEE International Conference on Communications (ICC),\n2020, pp. 1-6: IEEE.\n[122] H. Lu, Y. Zeng, S. Jin, and R. J. I. T. o. W. C. Zhang, \"Aerial\nintelligent reflecting surface: Joint placement and passive\nbeamforming design with 3D beam flattening,\" vol. 20, no. 7,\npp. 4128-4143, 2021.\n[123] S. Li, B. Duo, X. Yuan, Y.-C. Liang, and M. J. I. W. C. L. Di\nRenzo, \"Reconfigurable intelligent surface assisted UAV\ncommunication: Joint trajectory design and passive\nbeamforming,\" vol. 9, no. 5, pp. 716-720, 2020.\n[124] Y. Cai, Z. Wei, S. Hu, D. W. K. Ng, and J. Yuan, \"Resource\nallocation for power-efficient IRS-assisted UAV\ncommunications,\" in 2020 IEEE International Conference on\nCommunications Workshops (ICC Workshops), 2020, pp. 1-7:\nIEEE.\n[125] L. Wang, K. Wang, C. Pan, and N. J. I. T. o. M. C. Aslam, \"Joint\ntrajectory and passive beamforming design for intelligent\nreflecting surface-aided UAV communications: A deep\nreinforcement learning approach,\" 2022.\n[126] L. Ge, P. Dong, H. Zhang, J.-B. Wang, and X. J. I. A. You, \"Joint\nbeamforming and trajectory optimization for intelligent\nreflecting surfaces-assisted UAV communications,\" vol. 8, pp.\n78702-78712, 2020.\n[127] M. Hua, L. Yang, Q. Wu, C. Pan, C. Li, and A. L. J. I. T. o. W.\nC. Swindlehurst, \"UAV-assisted intelligent reflecting surface\nsymbiotic radio system,\" vol. 20, no. 9, pp. 5769-5785, 2021.\n[128] Q. Zhang, W. Saad, and M. Bennis, \"Reflections in the sky:\nMillimeter wave communication with UAV-carried intelligent\nreflectors,\" in 2019 IEEE Global Communications Conference\n(GLOBECOM), 2019, pp. 1-6: IEEE.\n[129] H. Long et al., \"Reflections in the sky: Joint trajectory and\npassive beamforming design for secure UAV networks with\nreconfigurable intelligent surface,\" 2020.\n[130] T. Shafique, H. Tabassum, and E. J. I. T. o. C. Hossain,\n\"Optimization of wireless relaying with flexible UAV-borne\nreflecting surfaces,\" vol. 69, no. 1, pp. 309-325, 2020.\n[131] V. W. Wong, R. Schober, D. W. K. Ng, and L.-C. Wang, Key\ntechnologies for 5G wireless systems. Cambridge university\npress, 2017.\n[132] Z. Ding, X. Lei, G. K. Karagiannidis, R. Schober, J. Yuan, and\nV. K. J. I. J. o. S. A. i. C. Bhargava, \"A survey on non-orthogonal\nmultiple access for 5G networks: Research challenges and future\ntrends,\" vol. 35, no. 10, pp. 2181-2195, 2017.\n[133] W. Mei and R. J. I. J. o. S. T. i. S. P. Zhang, \"Uplink cooperative\nNOMA for cellular-connected UAV,\" vol. 13, no. 3, pp. 644-\n656, 2019.\n[134] W. Mei and R. J. I. W. C. L. Zhang, \"Uplink cooperative\ninterference cancellation for cellular-connected UAV: A\nquantize-and-forward approach,\" vol. 9, no. 9, pp. 1567-1571,\n2020.\n[135] W. Mei and R. J. I. T. o. C. Zhang, \"Cooperative downlink\ninterference transmission and cancellation for cellular-connected\nUAV: A divide-and-conquer approach,\" vol. 68, no. 2, pp. 1297-\n1311, 2019.\n[136] W. K. New, C. Y. Leow, K. Navaie, and Z. J. I. T. o. W. C. Ding,\n\"Robust non-orthogonal multiple access for aerial and ground\nusers,\" vol. 19, no. 7, pp. 4793-4805, 2020.\n[137] Q. Wu, J. Xu, and R. J. I. J. o. S. A. i. C. Zhang, \"Capacity\ncharacterization of UAV-enabled two-user broadcast channel,\"\nvol. 36, no. 9, pp. 1955-1971, 2018.\n[138] M. F. Sohail, C. Y. Leow, and S. J. I. A. Won, \"Non-orthogonal\nmultiple access for unmanned aerial vehicle assisted\ncommunication,\" vol. 6, pp. 22716-22727, 2018.\n[139] P. K. Sharma and D. I. Kim, \"UAV-enabled downlink wireless\nsystem with non-orthogonal multiple access,\" in 2017 IEEE\nGlobecom Workshops (GC Wkshps), 2017, pp. 1-6: IEEE.\n[140] A. A. Nasir, H. D. Tuan, T. Q. Duong, and H. V. J. I. T. o. C.\nPoor, \"UAV-enabled communication using NOMA,\" vol. 67, no.\n7, pp. 5126-5138, 2019.\n[141] T. Hou, Y. Liu, Z. Song, X. Sun, and Y. J. I. T. o. C. Chen,\n\"Multiple antenna aided NOMA in UAV networks: A stochastic\ngeometry approach,\" vol. 67, no. 2, pp. 1031-1044, 2018.\n[142] T. Hou, Y. Liu, Z. Song, X. Sun, and Y. J. I. T. o. C. Chen,\n\"Exploiting NOMA for UAV communications in large-scale\ncellular networks,\" vol. 67, no. 10, pp. 6897-6911, 2019.\n[143] R. Duan, J. Wang, C. Jiang, H. Yao, Y. Ren, and Y. J. I. I. o. T.\nJ. Qian, \"Resource allocation for multi-UAV aided IoT NOMA\nuplink transmission systems,\" vol. 6, no. 4, pp. 7025-7037, 2019.\n[144] N. Rupasinghe, Y. Yap\u0131c\u0131, I. G\u00fcven\u00e7, M. Ghosh, and Y. J. I. J.\no. S. T. i. S. P. Kakishima, \"Angle feedback for NOMA\ntransmission in mmWave drone networks,\" vol. 13, no. 3, pp.\n628-643, 2019.\n[145] J. Xu, Y. Zeng, and R. Zhang, \"UAV-enabled wireless power\ntransfer: Trajectory design and energy region characterization,\"\nin 2017 IEEE Globecom Workshops (GC Wkshps), 2017, pp. 1-\n7: IEEE.\n[146] J. Xu, Y. Zeng, and R. J. I. t. o. w. c. Zhang, \"UAV-enabled\nwireless power transfer: Trajectory design and energy\noptimization,\" vol. 17, no. 8, pp. 5092-5106, 2018.\n[147] Y. Hu, X. Yuan, J. Xu, and A. J. I. T. o. C. Schmeink, \"Optimal\n1D trajectory design for UAV-enabled multiuser wireless power\ntransfer,\" vol. 67, no. 8, pp. 5674-5688, 2019.\n[148] Z. Yang, W. Xu, and M. J. I. T. o. V. T. Shikh-Bahaei, \"Energy\nefficient UAV communication with energy harvesting,\" vol. 69,\nno. 2, pp. 1913-1927, 2019.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n39\n[149] L. Xie, J. Xu, and R. J. I. I. o. T. J. Zhang, \"Throughput\nmaximization for UAV-enabled wireless powered\ncommunication networks,\" vol. 6, no. 2, pp. 1690-1703, 2018.\n[150] L. Xie, J. Xu, and Y. J. I. T. o. C. Zeng, \"Common throughput\nmaximization for UAV-enabled interference channel with\nwireless powered communications,\" vol. 68, no. 5, pp. 3197-\n3212, 2020.\n[151] J. Tang, J. Song, J. Ou, J. Luo, X. Zhang, and K.-K. J. I. A.\nWong, \"Minimum throughput maximization for multi-UAV\nenabled WPCN: A deep reinforcement learning method,\" vol. 8,\npp. 9124-9132, 2020.\n[152] J.-M. Kang and C.-J. J. I. S. J. Chun, \"Joint trajectory design, Tx\npower allocation, and Rx power splitting for UAV-enabled\nmulticasting SWIPT systems,\" vol. 14, no. 3, pp. 3740-3743,\n2020.\n[153] W. Feng et al., \"UAV-enabled SWIPT in IoT networks for\nemergency communications,\" vol. 27, no. 5, pp. 140-147, 2020.\n[154] A. O. Hashesh, S. Hashima, R. M. Zaki, M. M. Fouda, K.\nHatano, and A. S. T. Eldien, \"AI-enabled UAV communications:\nChallenges and future directions,\" IEEE Access, vol. 10, pp.\n92048-92066, 2022.\n[155] S. Ben Aissa and A. Ben Letaifa, \"UAV communications with\nmachine learning: challenges, applications and open issues,\"\nArabian Journal for Science and Engineering, vol. 47, no. 2, pp.\n1559-1579, 2022.\n[156] B. Baig and A. Q. Shahzad, \"Machine learning and AI approach\nto improve UAV communication and networking,\" in\nComputational intelligence for unmanned aerial vehicles\ncommunication networks: Springer, 2022, pp. 1-15.\n[157] S. Wang, S. Hosseinalipour, M. Gorlatova, C. G. Brinton, and\nM. Chiang, \"UAV-assisted online machine learning over multi\u0002tiered networks: A hierarchical nested personalized federated\nlearning approach,\" IEEE Transactions on Network and Service\nManagement, 2022.\n[158] H. Kurunathan, H. Huang, K. Li, W. Ni, and E. Hossain,\n\"Machine learning-aided operations and communications of\nunmanned aerial vehicles: A contemporary survey,\" IEEE\nCommunications Surveys & Tutorials, 2023.\n[159] S. Rajendran, K. K. Samy, J. Chinnathevar, and D. P. Sethuraj,\n\"Machine Learning Techniques for UAV Trajectory\nOptimization\u2014A Survey,\" in Computational Intelligence for\nUnmanned Aerial Vehicles Communication Networks: Springer,\n2022, pp. 35-44.\n[160] G. Afifi and Y. Gadallah, \"Cellular Network-Supported Machine\nLearning Techniques for Autonomous UAV Trajectory\nPlanning,\" IEEE Access, vol. 10, pp. 131996-132011, 2022.\n[161] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. J. I. c. m.\nZhang, \"The roadmap to 6G: AI empowered wireless networks,\"\nvol. 57, no. 8, pp. 84-90, 2019.\n[162] D. G\u00fcnd\u00fcz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R.\nMurthy, and M. J. I. J. o. S. A. i. C. van der Schaar, \"Machine\nlearning in the air,\" vol. 37, no. 10, pp. 2184-2199, 2019.\n[163] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. J. I. c. m. Huang,\n\"Toward an intelligent edge: Wireless communication meets\nmachine learning,\" vol. 58, no. 1, pp. 19-25, 2020.\n[164] F. Wang, J. Xu, X. Wang, and S. J. I. T. o. W. C. Cui, \"Joint\noffloading and computing optimization in wireless powered\nmobile-edge computing systems,\" vol. 17, no. 3, pp. 1784-1797,\n2017.\n[165] Y. Zeng, X. Xu, S. Jin, and R. J. I. T. o. W. C. Zhang,\n\"Simultaneous navigation and radio mapping for cellular\u0002connected UAV with deep reinforcement learning,\" vol. 20, no.\n7, pp. 4205-4220, 2021.\n[166] X. Liu, M. Chen, Y. Liu, Y. Chen, S. Cui, and L. J. I. W. C.\nHanzo, \"Artificial intelligence aided next-generation networks\nrelying on UAVs,\" vol. 28, no. 1, pp. 120-127, 2020.\n[167] I. Orikumhi, J. Bae, H. Park, and S. Kim, \"DRL-based Multi\u0002UAV trajectory optimization for ultra-dense small cells,\" ICT\nExpress, 2023.\n[168] M. Seong, O. Jo, and K. Shin, \"Multi-UAV trajectory optimizer:\nA sustainable system for wireless data harvesting with deep\nreinforcement learning,\" Engineering Applications of Artificial\nIntelligence, vol. 120, p. 105891, 2023.\n[169] S. Zhou, Y. Cheng, X. Lei, and H. Duan, \"Multi-agent few-shot\nmeta reinforcement learning for trajectory design and channel\nselection in UAV-assisted networks,\" China Communications,\nvol. 19, no. 4, pp. 166-176, 2022.\n[170] C. Zhang, X. Li, C. He, X. Li, and D. Lin, \"Trajectory\noptimization for UAV-enabled relaying with reinforcement\nlearning,\" Digital Communications and Networks, 2023.\n[171] A. Shahbazi, I. Donevski, J. J. Nielsen, and M. Di Renzo,\n\"Federated reinforcement learning UAV trajectory design for\nfast localization of ground users,\" in 2022 30th European Signal\nProcessing Conference (EUSIPCO), 2022, pp. 663-666: IEEE.\n[172] P. Ji, J. Jia, J. Chen, L. Guo, A. Du, and X. Wang,\n\"Reinforcement learning based joint trajectory design and\nresource allocation for RIS-aided UAV multicast networks,\"\nComputer Networks, vol. 227, p. 109697, 2023.\n[173] B. Zhu, E. Bedeer, H. H. Nguyen, R. Barton, and J. Henry,\n\"UAV trajectory planning in wireless sensor networks for energy\nconsumption minimization by deep reinforcement learning,\"\nIEEE Transactions on Vehicular Technology, vol. 70, no. 9, pp.\n9540-9554, 2021.\n[174] S. Chai and V. K. Lau, \"Multi-UAV trajectory and power\noptimization for cached UAV wireless networks with energy and\ncontent recharging-demand driven deep learning approach,\"\nIEEE Journal on Selected Areas in Communications, vol. 39, no.\n10, pp. 3208-3224, 2021.\n[175] X. Cao, J. Xu, and R. Zhang, \"Mobile edge computing for\ncellular-connected UAV: Computation offloading and trajectory\noptimization,\" in 2018 IEEE 19th International Workshop on\nSignal Processing Advances in Wireless Communications\n(SPAWC), 2018, pp. 1-5: IEEE.\n[176] B. Brik, A. Ksentini, and M. J. I. A. Bouaziz, \"Federated learning\nfor UAVs-enabled wireless networks: Use cases, challenges, and\nopen problems,\" vol. 8, pp. 53841-53849, 2020.\n[177] T. Zeng, O. Semiari, M. Mozaffari, M. Chen, W. Saad, and M.\nBennis, \"Federated learning in the sky: Joint power allocation\nand scheduling with UAV swarms,\" in ICC 2020-2020 IEEE\nInternational Conference on Communications (ICC), 2020, pp.\n1-6: IEEE.\n[178] Y. Liu, J. Nie, X. Li, S. H. Ahmed, W. Y. B. Lim, and C. J. I. I.\no. T. J. Miao, \"Federated learning in the sky: Aerial-ground air\nquality sensing framework with UAV swarms,\" vol. 8, no. 12,\npp. 9827-9837, 2020.\n[179] C. Dong et al., \"UAVs as an intelligent service: Boosting edge\nintelligence for air-ground integrated networks,\" vol. 35, no. 4,\npp. 167-175, 2021.\n[180] J. S. Ng et al., \"Joint auction-coalition formation framework for\ncommunication-efficient federated learning in UAV-enabled\ninternet of vehicles,\" vol. 22, no. 4, pp. 2326-2344, 2020.\n[181] T. Liu, T. Zhang, J. Loo, and Y. Wang, \"Deep reinforcement\nlearning-based resource allocation for UAV-enabled federated\nedge learning,\" Journal of Communications and Information\nNetworks, vol. 8, no. 1, pp. 1-12, 2023.\n[182] P. Hou, X. Jiang, Z. Wang, S. Liu, and Z. Lu, \"Federated Deep\nReinforcement Learning-Based Intelligent Dynamic Services in\nUAV-Assisted MEC,\" IEEE Internet of Things Journal, 2023.\n[183] S. R. Sabuj, M. Elsharief, and H.-S. Jo, \"A Partial Federated\nLearning Model in Cognitive UAV-enabled Edge Computing\nNetworks,\" in 2022 13th International Conference on\nInformation and Communication Technology Convergence\n(ICTC), 2022, pp. 1437-1440: IEEE.\n[184] J. Tursunboev, Y.-S. Kang, S.-B. Huh, D.-W. Lim, J.-M. Kang,\nand H. Jung, \"Hierarchical federated learning for edge-aided\nunmanned aerial vehicle networks,\" Applied Sciences, vol. 12,\nno. 2, p. 670, 2022.\n[185] Z. Song, C. Ma, M. Ding, H. H. Yang, Y. Qian, and X. Zhou,\n\"Personalized Federated Deep Reinforcement Learning-based\nTrajectory Optimization for Multi-UAV Assisted Edge\nComputing,\" in 2023 IEEE/CIC International Conference on\nCommunications in China (ICCC), 2023, pp. 1-6: IEEE.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n40\n[186] M. B. Janjua and H. J. S. Arslan, \"A Survey of Symbiotic Radio:\nMethodologies, Applications, and Future Directions,\" vol. 23,\nno. 5, p. 2511, 2023.\n[187] Y.-C. Liang, Q. Zhang, E. G. Larsson, G. Y. J. I. T. o. C. C. Li,\nand Networking, \"Symbiotic radio: Cognitive backscattering\ncommunications for future wireless networks,\" vol. 6, no. 4, pp.\n1242-1255, 2020.\n[188] K. K. Vaigandla, S. Thatipamula, and R. K. J. I. J. o. S. W. S.\nKarne, \"Investigation on unmanned aerial vehicle (uav): An\noverview,\" vol. 4, no. 3, pp. 130-148, 2022.\n[189] H. Nakamura and Y. Kajikawa, \"Regulation and innovation:\nHow should small unmanned aerial vehicles be regulated?,\"\nTechnological forecasting and social change, vol. 128, pp. 262-\n274, 2018.\n[190] D. Lee, D. J. Hess, and M. A. Heldeweg, \"Safety and privacy\nregulations for unmanned aerial vehicles: A multiple\ncomparative analysis,\" Technology in Society, vol. 71, p.\n102079, 2022.\n[191] T.-H. Tran and D.-D. Nguyen, \"Management and Regulation of\nDrone Operation in Urban Environment: A Case Study,\" Social\nSciences, vol. 11, no. 10, p. 474, 2022.\n[192] K. Belwafi, R. Alkadi, S. A. Alameri, H. Al Hamadi, and A.\nShoufan, \"Unmanned aerial vehicles\u2019 remote identification: A\ntutorial and survey,\" IEEE Access, vol. 10, pp. 87577-87601,\n2022.\n[193] ISO, \"UAS traffic management (UTM) \u2014 Part 8: Remote\nidentification, ISO 23629-8,\" 2023.\n[194] 3GPP, \"Unmanned Aerial System (UAS) support in 3GPP, TS\n22.125,\" 2023.\n[195] Y. Zhi, Z. Fu, X. Sun, and J. Yu, \"Security and privacy issues of\nUAV: a survey,\" Mobile Networks and Applications, vol. 25, pp.\n95-101, 2020.\n[196] M. Yahuza et al., \"Internet of drones security and privacy issues:\nTaxonomy and open challenges,\" IEEE Access, vol. 9, pp.\n57243-57270, 2021.\n[197] OFCOM, \"Spectrum for Unmanned Aircraft Systems (UAS),\"\n2022.\n[198] ISO, \"Unmanned aircraft systems \u2014 Part 2: UAS components.\nISO 21384-2:2021,\" 2021.\n[199] I. L. Henderson, \"Aviation safety regulations for unmanned\naircraft operations: Perspectives from users,\" Transport Policy,\nvol. 125, pp. 192-206, 2022.\n[200] E. Politi, I. Varlamis, K. Tserpes, M. Larsen, and G.\nDimitrakopoulos, \"The future of safe BVLOS drone operations\nwith respect to system and service engineering,\" in 2022 IEEE\nInternational Conference on Service-Oriented System\nEngineering (SOSE), 2022, pp. 133-140: IEEE.\n[201] ISO, \"UAS traffic management (UTM) \u2014 Part 5: UTM\nfunctional structure, ISO 23629-5:2023,\" 2023.\n[202] ISO, \"UAS traffic management (UTM) \u2014 Part 12:\nRequirements for UTM service providers, ISO 23629-12:2022,\"\n2022.\n[203] R. Shrestha, R. Bajracharya, and S. Kim, \"6G enabled unmanned\naerial vehicle traffic management: A perspective,\" IEEE Access,\nvol. 9, pp. 91119-91136, 2021.\n[204] A. Shelley, \"Essays in the Regulation of Drones and Counter\u0002Drone Systems,\" 2020.\n[205] B. Nassi, R. Bitton, R. Masuoka, A. Shabtai, and Y. Elovici,\n\"SoK: Security and privacy in the age of commercial drones,\" in\n2021 IEEE Symposium on Security and Privacy (SP), 2021, pp.\n1434-1451: IEEE.\n[206] ISO, \"Unmanned aircraft systems \u2014 Part 3: Operational\nprocedures, ISO 21384-3:2019(en),\" 2019.\n[207] A. H. Arani, P. Hu, and Y. J. I. O. J. o. t. C. S. Zhu, \"HAPS\u0002UAV-Enabled Heterogeneous Networks: A Deep\nReinforcement Learning Approach,\" 2023.\n[208] D. Mishra, A. M. Vegni, V. Loscr\u00ed, and E. J. I. C. S. M. Natalizio,\n\"Drone networking in the 6G era: A technology overview,\" vol.\n5, no. 4, pp. 88-95, 2021.\n[209] M. A. Khan et al., \"Swarm of UAVs for network management in\n6G: A technical review,\" 2022.\n[210] J. Luo, Z. Wang, M. Xia, L. Wu, Y. Tian, and Y. J. A. C. S.\nChen, \"Path Planning for UAV Communication Networks:\nRelated Technologies, Solutions, and Opportunities,\" vol. 55,\nno. 9, pp. 1-37, 2023.\n[211] S. Hafeez et al., \"Blockchain-Assisted UAV Communication\nSystems: A Comprehensive Survey,\" 2023.\n[212] M. Basharat, M. Naeem, Z. Qadir, and A. J. T. o. E. T. T.\nAnpalagan, \"Resource optimization in UAV\u2010assisted wireless\nnetworks\u2014A comprehensive survey,\" vol. 33, no. 7, p. e4464,\n2022.\n[213] Z. Xu, I. Petrunin, A. J. J. o. I. Tsourdos, and R. Systems,\n\"Dynamic spectrum management with network function\nvirtualization for uav communication,\" vol. 101, pp. 1-18, 2021.\n[214] S. K. Nobar, M. H. Ahmed, Y. Morgan, S. A. J. I. T. o. C. C.\nMahmoud, and Networking, \"Resource allocation in cognitive\nradio-enabled UAV communication,\" vol. 8, no. 1, pp. 296-310,\n2021.\n[215] J. N. Yasin, S. A. S. Mohamed, M.-H. Haghbayan, J. Heikkonen,\nH. Tenhunen, and J. Plosila, \"Unmanned Aerial Vehicles\n(UAVs): Collision Avoidance Systems and Approaches,\" IEEE\nAccess, vol. 8, pp. 105139-105155, 2020-01-01 2020.\n[216] S. Singh et al., \"FPV Video Adaptation for UAV Collision\nAvoidance,\" IEEE Open Journal of the Communications\nSociety, vol. 2, pp. 2095-2110, 2021-01-01 2021.\n[217] Y.-H. Hsu and R.-H. Gau, \"Reinforcement Learning-Based\nCollision Avoidance and Optimal Trajectory Planning in UAV\nCommunication Networks,\" IEEE Transactions on Mobile\nComputing, vol. 21, no. 1, pp. 306-320, 2022-01-01 2022.\n[218] H. Huang and A. V. Savkin, \"An Algorithm of Reactive\nCollision Free 3-D Deployment of Networked Unmanned Aerial\nVehicles for Surveillance and Monitoring,\" IEEE Transactions\non Industrial Informatics, vol. 16, no. 1, pp. 132-140, 2020-01-\n01 2020.\n[219] J. Kim, S. Kim, J. Jeong, H. Kim, J. Park, and T. Kim, \"CBDN:\nCloud-Based Drone Navigation for Efficient Battery Charging\nin Drone Networks,\" IEEE Transactions on Intelligent\nTransportation Systems, vol. 20, no. 11, pp. 4174-4191, 2019.\n[220] M. Shin, J. Kim, and M. Levorato, \"Auction-Based Charging\nScheduling With Deep Learning Framework for Multi-Drone\nNetworks,\" IEEE Transactions on Vehicular Technology, vol.\n68, no. 5, pp. 4235-4248, 2019.\n[221] H. Huang and A. V. Savkin, \"A Method of Optimized\nDeployment of Charging Stations for Drone Delivery,\" IEEE\nTransactions on Transportation Electrification, vol. 6, no. 2, pp.\n510-518, 2020.\n[222] D. He, S. Chan, and M. Guizani, \"Drone-Assisted Public Safety\nNetworks: The Security Aspect,\" IEEE Communications\nMagazine, vol. 55, no. 8, pp. 218-223, 2017.\n[223] C. Lin, D. He, N. Kumar, K. R. Choo, A. Vinel, and X. Huang,\n\"Security and Privacy for the Internet of Drones: Challenges and\nSolutions,\" IEEE Communications Magazine, vol. 56, no. 1, pp.\n64-69, 2018.\n[224] G. Geraci, A. Garcia-Rodriguez, L. G. Giordano, D. L\u00f3pez\u0002P\u00e9rez, and E. Bj\u00f6rnson, \"Understanding UAV cellular\ncommunications: From existing networks to massive MIMO,\"\nIEEE Access, vol. 6, pp. 67853-67865, 2018.\n[225] J. Angjo, I. Shayea, M. Ergen, H. Mohamad, A. Alhammadi, and\nY. I. Daradkeh, \"Handover management of drones in future\nmobile networks: 6G technologies,\" IEEE access, vol. 9, pp.\n12803-12823, 2021.\n[226] W. Alshaibani, I. Shayea, R. Caglar, J. Din, and Y. I. Daradkeh,\n\"Mobility Management of Unmanned Aerial Vehicles in Ultra\u2013\nDense Heterogeneous Networks,\" Sensors, vol. 22, no. 16, p.\n6013, 2022.\n[227] D. Darsena, G. Gelli, I. Iudice, and F. Verde, \"Equalization\ntechniques of control and non-payload communication links for\nunmanned aerial vehicles,\" IEEE Access, vol. 6, pp. 4485-4496,\n2018.\n[228] A. Y\u0131lmaz and C. Toker, \"Air-to-Air Channel Model for UAV\nCommunications,\" in 2022 30th Signal Processing and\nCommunications Applications Conference (SIU), 2022, pp. 1-4:\nIEEE.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n41\n[229] X. Mao, C.-X. Wang, and H. Chang, \"A 3D non-stationary\ngeometry-based stochastic model for 6G UAV air-to-air\nchannels,\" in 2021 13th International Conference on Wireless\nCommunications and Signal Processing (WCSP), 2021, pp. 1-5:\nIEEE.\n[230] H. An et al., \"Measurement and Ray-tracing for UAV Air-to-air\nChannel Modeling,\" in 2022 IEEE 5th International Conference\non Electronic Information and Communication Technology\n(ICEICT), 2022, pp. 415-420: IEEE.\n[231] C.-C. Chiu, A.-H. Tsai, H.-P. Lin, C.-Y. Lee, and L.-C. Wang,\n\"Channel modeling of air-to-ground signal measurement with\ntwo-ray ground-reflection model for UAV communication\nsystems,\" in 2021 30th Wireless and Optical Communications\nConference (WOCC), 2021, pp. 251-256: IEEE.\n[232] C. Ge et al., \"Pathloss and Airframe Shadowing Loss of Air-to\u0002Ground UAV Channel in the Airport Area at UHF-and L-Band,\"\nIEEE Transactions on Vehicular Technology, 2023.\n[233] H. Li, L. Ding, Y. Wang, and Z. Wang, \"Air-to-Ground Channel\nModeling and Performance Analysis for Cellular-Connected\nUAV Swarm,\" IEEE Communications Letters, 2023.\n[234] H. Jiang, Z. Zhang, L. Wu, and J. Dang, \"Three-dimensional\ngeometry-based UAV-MIMO channel modeling for A2G\ncommunication environments,\" IEEE communications letters,\nvol. 22, no. 7, pp. 1438-1441, 2018.\n[235] Z. Cui, K. Guan, C. Oestges, C. Briso-Rodr\u00edguez, B. Ai, and Z.\nZhong, \"Cluster-based characterization and modeling for uav\nair-to-ground time-varying channels,\" IEEE Transactions on\nVehicular Technology, vol. 71, no. 7, pp. 6872-6883, 2022.\n[236] F. Xu et al., \"Beyond Encryption: Exploring the Potential of\nPhysical Layer Security in UAV Networks,\" Journal of King\nSaud University-Computer and Information Sciences, p. 101717,\n2023.\n[237] R. Polus, C. D\u2019Amours, and B. Kantarci, \"Physical Layer\nSecurity Over UAV-to-Ground Channels with Shadowing,\" in\n2023 IEEE 97th Vehicular Technology Conference (VTC2023-\nSpring), 2023, pp. 1-5: IEEE.\n[238] W. U. Khan et al., \"Opportunities for physical layer security in\nUAV communication enhanced with intelligent reflective\nsurfaces,\" IEEE Wireless Communications, vol. 29, no. 6, pp. 22-\n28, 2022.\n[239] A. Benaya, M. H. Ismail, A. S. Ibrahim, and A. A. Salem,\n\"Physical Layer Security Enhancement via Intelligent Omni\u0002Surfaces and UAV-Friendly Jamming,\" IEEE Access, vol. 11,\npp. 2531-2544, 2023.\n[240] H. Sharma, N. Kumar, R. K. Tekchandani, and N. Mohammad,\n\"Deep learning enabled channel secrecy codes for physical layer\nsecurity of UAVs in 5G and beyond networks,\" in ICC 2022-\nIEEE International Conference on Communications, 2022, pp.\n1-6: IEEE.\n[241] E. Illi, M. Qaraqe, F. El Bouanani, and S. Al-Kuwari, \"On the\nPhysical Layer Security of a Dual-Hop UAV-based Network in\nthe Presence of per-hop Eavesdropping and Imperfect CSI,\"\nIEEE Internet of Things Journal, 2022.\n[242] T. T. Nguyen, T. T. H. Le, and X. N. Tran, \"Physical Layer\nSecurity for UAV-Based Full-Duplex Relay NOMA System,\" in\n2022 International Conference on Advanced Technologies for\nCommunications (ATC), 2022, pp. 395-400: IEEE.\n[243] A. B. B\u00fcy\u00fck\u015far, M. Can, and \u0130. Altunba\u015f, \"Physical Layer\nSecurity Improvement of NOMA-Based UAV-Aided Terrestrial\nNetworks via Cooperative User Selection,\" in 2022 30th Signal\nProcessing and Communications Applications Conference\n(SIU), 2022, pp. 1-4: IEEE.\n[244] M. Prajapati and P. Tripathi, \"Physical Layer Security\nOptimisation for NOMA Based UAV Communication for\nOptimal Resource Allocation,\" in International Conference on\nComputing Science, Communication and Security, 2023, pp.\n133-147: Springer.\nBIOGRAPHY\nMOHAMMED BANAFAA hailing from\nYemen, holds a B.Eng. (First Class Hons.) from\nUniversity of Aden (2017) and an M.Eng.\n(Hons.) from Universiti Teknologi Malaysia\n(2019). He's pursuing a Ph.D. in Electronic and\nCommunication at King Fahd University of\nPetroleum and Minerals (KFUPM). An IEEE\nmember, his research interests span wireless\ncommunications, 5G/6G technologies, UAV\nmobility management, and Mobile Edge\nComputing. .[E-mail: eng.banafaa@gmail.com]\n\u00d6MER PEPEO\u011eLU received the B.S. degree in\nelectronics and communication engineering from\nIstanbul Technical University, T\u00fcrkiye, in 2020.\nHe is currently pursuing the M.S. degree in\ncommunications engineering at the Technical\nUniversity of Munich, Germany. His research\ninterests are mainly based on wireless\ncommunications, signal processing, and their\napplications.[E-mail: omerpepeoglu@gmail.com]\nIBRAHEEM SHAYEA is an Associate\nResearcher of Electrical Engineering in Istanbul\nTechnical University. Previously, he worked as a\npostdoctoral research fellow at Istanbul\nTechnical University (Turkey) and at Wireless\nCommunication Centre (WCC), Universiti\nTeknologi Malaysia (UTM) (Kuala Lumpur,\nMalaysia). He obtained his BSc. in Electronic\nEngineering from University of Diyala Iraq in\n2004, MSc in Computer and Communication\nEngineering from Universiti Kebangsaan\nMalaysia (UKM) Malaysia in 2010, and PhD in Mobile Communication\nEngineering from UKM, Malaysia in 2015. His research interests in\nMobility Management, Handover, LTE/LTE-A, Carrier Aggregation, radio\npropagation, indoors and outdoors wireless communication, IoT, 5G, and\n6G. He has published several papers related to wireless communication in\nnational/international journal and conference. [E-mail:\nibr.shayea@gmail.com].\nABDULRAQEB ALHAMMADI received\nhis B. Eng. in Electronic majoring in\ntelecommunications, M.S degree and PhD in\nwireless communication from Multimedia\nUniversity, Malaysia, in 2011, 2015 and\n2020, respectively. He served as a research\nassistant/research scholar at Multimedia\nUniversity from 2012 till 2019. He is\ncurrently a postdoctoral fellow at the\nMalaysia-Japan International Institute of\nTechnology, Universiti Teknologi Malaysia. He received several awards,\nincluding the Excellent Researcher Award 2019, Multimedia University and\nthe Outstanding Doctoral Dissertation Award 2020, IEEE Malaysia\n(Communication Society). He is also the author of more than 50 articles in\ninternational journals and conferences. His main research interests include\nheterogeneous networks, indoor localization, artificial intelligence, data\nanalytics, cognitive radio networks and IoT. He is a member of professional\ninstitutes and societies, such as IEEE, IEICE, IACSIT, and IAENG. He is\nalso a member of program committees at international conferences and\nworkshops.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n42\nZAID AHMED SHAMSAN is a Senior Member\nof IEEE with a B.Sc. (Hons.) degree in\nelectronics and communication engineering from\nSudan University of Sciences and Technology\n(SUST). He completed his master's and Ph.D.\ndegrees in telecommunication and electrical\nengineering from Universiti Teknologi Malaysia\n(UTM). Dr. Shamsan has worked as a\nPostdoctoral Research Fellow at UTM's Wireless\nCommunication Center (WCC) and has been\ninvolved in research projects on frequency spectrum management funded by\nthe Malaysian Communications and Multimedia Commission (MCMC). He\nreceived a Bronze Medal in Geneva Innovation 2021 for his RFID Lock\nSecurity Mechanism project. Currently a Professor at Imam Mohammad Ibn\nSaud Islamic University, he specializes in teaching various electrical\nengineering courses and has published papers on topics including wireless\nnetworks, wave propagation, and optical communications.\nMUNEEF A. RAZAZ, received his B.s. degree\nin Communications and Computer Engineering\nwith (First Class Hons.) from Taiz University in\n2020. Currently He's pursuing a M.S. in\nElectrical Engineering at King Fahd University\nof Petroleum and Minerals (KFUPM). His\nresearch focuses on cutting-edge areas such as\nHybrid RF/VLC, Symbiotic Radio, AI/DL for\nwireless communications, 5G/6G Network\nSlicing, and IoT. [E-mail:\nmneefbasha@gmail.com]\nMAJID ALSAGABI is an Assistant Professor of\nSignal processing and communication in\nElectrical Engineering Department, College of\nEngineering, Imam Mohammad Ibn Saud Islamic\nUniversity (IMSIU). Before joining the EE Dept\nat IMSIU he received his B.Sc. (Hons) in\nElectrical Engineering from king Saud university\nin Riyadh, Saudi Arabia in 2000, then he received\nhis MSc. (in the field of digital signal processing)\nand PhD (in the field of Bioinformatics) from\nUniversity of Minnesota, USA in 2008 and 2012, respectively. He was a\nlecturer at Almajmah University. From 2000 to 2006, Dr. Majid worked as\nan energy engineer at the Saudi Electricity Company.\nSULAIMAN AL-SOWAYAN is an Associate\nProfessor of Electronics and Communications\nin Electrical Engineering Department, College\nof Engineering, Imam Mohammad Ibn Saud\nIslamic University (IMSIU). Before joining the\nEE Dept at IMSIU he received his B.Sc. (Hons)\nfrom king Saud university in Riyadh, Saudi\nArabia in 1994, then he received his MSc. and\nPhD from Colorado State University, USA in\n2001 and 2005 respectively. He works on\nOptoelectronics, and Optical Communications. His focus is on optical fiber\ncommunications, optical sensors, Optical Coherence Tomography, and\nelectromagnetic applications. He is an author and coauthor of several\npublications in high impact journals and conferences. He worked as a part\ntime and full time consultant in the area of telecommunications for several\ninstitutions in the government and the private sector.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3349208\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4",
      "openalex_id": "https://openalex.org/W4390494361",
      "title": "A Comprehensive Survey on 5G-and-Beyond Networks With UAVs: Applications, Emerging Technologies, Regulatory Aspects, Research Trends and Challenges",
      "publication_date": "2024-01-01",
      "cited_by_count": 15.0,
      "topics": "Unmanned Aerial Vehicle Communications, Intelligent Reflecting Surfaces in Wireless Communications, Visual Object Tracking and Person Re-identification",
      "keywords": "5G and Beyond, Open research, UAV Networks, Drone Applications, Emerging technologies",
      "concepts": "Software deployment, Computer science, Open research, Emerging technologies, Resource (disambiguation), Field (mathematics), Data science, Key (lock), Telecommunications, Systems engineering, Computer security, Engineering, World Wide Web, Artificial intelligence, Computer network, Mathematics, Pure mathematics, Operating system",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10379625.pdf"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W2118548017",
        "https://openalex.org/W2125957038",
        "https://openalex.org/W2286275639",
        "https://openalex.org/W2461414758",
        "https://openalex.org/W2604830243",
        "https://openalex.org/W2604974201",
        "https://openalex.org/W2608727628",
        "https://openalex.org/W2613890548",
        "https://openalex.org/W2627863163",
        "https://openalex.org/W2728224506",
        "https://openalex.org/W2736565351",
        "https://openalex.org/W2772600596",
        "https://openalex.org/W2777180355",
        "https://openalex.org/W2782556342",
        "https://openalex.org/W2783189583",
        "https://openalex.org/W2783406823",
        "https://openalex.org/W2783764093",
        "https://openalex.org/W2785633818",
        "https://openalex.org/W2792798301",
        "https://openalex.org/W2795242006",
        "https://openalex.org/W2797821788",
        "https://openalex.org/W2800354041",
        "https://openalex.org/W2800793214",
        "https://openalex.org/W2809397343",
        "https://openalex.org/W2889655092",
        "https://openalex.org/W2898004290",
        "https://openalex.org/W2901594284",
        "https://openalex.org/W2903528516",
        "https://openalex.org/W2905025677",
        "https://openalex.org/W2905200871",
        "https://openalex.org/W2910734690",
        "https://openalex.org/W2910780793",
        "https://openalex.org/W2912159494",
        "https://openalex.org/W2919407654",
        "https://openalex.org/W2919779463",
        "https://openalex.org/W2921698889",
        "https://openalex.org/W2940785943",
        "https://openalex.org/W2942635578",
        "https://openalex.org/W2944533689",
        "https://openalex.org/W2945675984",
        "https://openalex.org/W2948019083",
        "https://openalex.org/W2952060029",
        "https://openalex.org/W2953091967",
        "https://openalex.org/W2957157287",
        "https://openalex.org/W2957382329",
        "https://openalex.org/W2962684895",
        "https://openalex.org/W2962691117",
        "https://openalex.org/W2962733038",
        "https://openalex.org/W2962748258",
        "https://openalex.org/W2963009535",
        "https://openalex.org/W2963035841",
        "https://openalex.org/W2963138728",
        "https://openalex.org/W2963288612",
        "https://openalex.org/W2963336322",
        "https://openalex.org/W2963427127",
        "https://openalex.org/W2963533436",
        "https://openalex.org/W2963539580",
        "https://openalex.org/W2963576178",
        "https://openalex.org/W2963896360",
        "https://openalex.org/W2963899602",
        "https://openalex.org/W2963988154",
        "https://openalex.org/W2964010262",
        "https://openalex.org/W2964135632",
        "https://openalex.org/W2969519626",
        "https://openalex.org/W2969731533",
        "https://openalex.org/W2969823908",
        "https://openalex.org/W2973289967",
        "https://openalex.org/W2974170798",
        "https://openalex.org/W2981125566",
        "https://openalex.org/W2982361136",
        "https://openalex.org/W2990088730",
        "https://openalex.org/W2990204812",
        "https://openalex.org/W2994008524",
        "https://openalex.org/W2994963916",
        "https://openalex.org/W2995268338",
        "https://openalex.org/W2998280308",
        "https://openalex.org/W2998842060",
        "https://openalex.org/W2999389395",
        "https://openalex.org/W3000577820",
        "https://openalex.org/W3002104503",
        "https://openalex.org/W3004018467",
        "https://openalex.org/W3004236608",
        "https://openalex.org/W3004277316",
        "https://openalex.org/W3004720322",
        "https://openalex.org/W3004946548",
        "https://openalex.org/W3006052720",
        "https://openalex.org/W3006653661",
        "https://openalex.org/W3010601625",
        "https://openalex.org/W3011766871",
        "https://openalex.org/W3016373553",
        "https://openalex.org/W3016657056",
        "https://openalex.org/W3016688072",
        "https://openalex.org/W3019497660",
        "https://openalex.org/W3019532323",
        "https://openalex.org/W3019585464",
        "https://openalex.org/W3022095849",
        "https://openalex.org/W3026439725",
        "https://openalex.org/W3030787901",
        "https://openalex.org/W3032973218",
        "https://openalex.org/W3033237219",
        "https://openalex.org/W3033271745",
        "https://openalex.org/W3033338032",
        "https://openalex.org/W3036252686",
        "https://openalex.org/W3037170119",
        "https://openalex.org/W3040137968",
        "https://openalex.org/W3040741665",
        "https://openalex.org/W3042391361",
        "https://openalex.org/W3042876727",
        "https://openalex.org/W3043932162",
        "https://openalex.org/W3045197138",
        "https://openalex.org/W3045898871",
        "https://openalex.org/W3046105742",
        "https://openalex.org/W3046150221",
        "https://openalex.org/W3048944357",
        "https://openalex.org/W3086892208",
        "https://openalex.org/W3090147969",
        "https://openalex.org/W3092163236",
        "https://openalex.org/W3093728622",
        "https://openalex.org/W3102483098",
        "https://openalex.org/W3102696584",
        "https://openalex.org/W3104458414",
        "https://openalex.org/W3106203633",
        "https://openalex.org/W3106530718",
        "https://openalex.org/W3106758637",
        "https://openalex.org/W3109400146",
        "https://openalex.org/W3112984084",
        "https://openalex.org/W3113018973",
        "https://openalex.org/W3118203807",
        "https://openalex.org/W3121070929",
        "https://openalex.org/W3125516798",
        "https://openalex.org/W3127504873",
        "https://openalex.org/W3127940641",
        "https://openalex.org/W3128146114",
        "https://openalex.org/W3129486236",
        "https://openalex.org/W3132105662",
        "https://openalex.org/W3134033224",
        "https://openalex.org/W3136760479",
        "https://openalex.org/W3152632745",
        "https://openalex.org/W3154816389",
        "https://openalex.org/W3155402977",
        "https://openalex.org/W3156154677",
        "https://openalex.org/W3159886835",
        "https://openalex.org/W3160082076",
        "https://openalex.org/W3162720533",
        "https://openalex.org/W3165716753",
        "https://openalex.org/W3169147363",
        "https://openalex.org/W3176508160",
        "https://openalex.org/W3182369559",
        "https://openalex.org/W3189230021",
        "https://openalex.org/W3191505721",
        "https://openalex.org/W3193427565",
        "https://openalex.org/W3194030134",
        "https://openalex.org/W3195306699",
        "https://openalex.org/W3195746043",
        "https://openalex.org/W3195948154",
        "https://openalex.org/W3198132193",
        "https://openalex.org/W3202150210",
        "https://openalex.org/W3211189031",
        "https://openalex.org/W3213912259",
        "https://openalex.org/W3215773138",
        "https://openalex.org/W4200037328",
        "https://openalex.org/W4200078133",
        "https://openalex.org/W4200490383",
        "https://openalex.org/W4200498111",
        "https://openalex.org/W4200618407",
        "https://openalex.org/W4206462583",
        "https://openalex.org/W4206770380",
        "https://openalex.org/W4210552697",
        "https://openalex.org/W4210555601",
        "https://openalex.org/W4210932344",
        "https://openalex.org/W4224290235",
        "https://openalex.org/W4224885329",
        "https://openalex.org/W4225097017",
        "https://openalex.org/W4225856095",
        "https://openalex.org/W4225925743",
        "https://openalex.org/W4226094091",
        "https://openalex.org/W4226199332",
        "https://openalex.org/W4226379933",
        "https://openalex.org/W4282970004",
        "https://openalex.org/W4283323538",
        "https://openalex.org/W4285105424",
        "https://openalex.org/W4285209996",
        "https://openalex.org/W4285211583",
        "https://openalex.org/W4289520778",
        "https://openalex.org/W4290996181",
        "https://openalex.org/W4291109519",
        "https://openalex.org/W4291238460",
        "https://openalex.org/W4292164052",
        "https://openalex.org/W4292186360",
        "https://openalex.org/W4292387473",
        "https://openalex.org/W4293499257",
        "https://openalex.org/W4293660867",
        "https://openalex.org/W4293863293",
        "https://openalex.org/W4293863302",
        "https://openalex.org/W4294599003",
        "https://openalex.org/W4294741352",
        "https://openalex.org/W4296105750",
        "https://openalex.org/W4296924472",
        "https://openalex.org/W4306160390",
        "https://openalex.org/W4309107718",
        "https://openalex.org/W4309532095",
        "https://openalex.org/W4309995310",
        "https://openalex.org/W4310364078",
        "https://openalex.org/W4312425871",
        "https://openalex.org/W4312460693",
        "https://openalex.org/W4312576847",
        "https://openalex.org/W4312663258",
        "https://openalex.org/W4312691328",
        "https://openalex.org/W4312932456",
        "https://openalex.org/W4313043980",
        "https://openalex.org/W4313123734",
        "https://openalex.org/W4313288712",
        "https://openalex.org/W4313591201",
        "https://openalex.org/W4317438544",
        "https://openalex.org/W4318217967",
        "https://openalex.org/W4319338698",
        "https://openalex.org/W4321789362",
        "https://openalex.org/W4360604201",
        "https://openalex.org/W4361282702",
        "https://openalex.org/W4362653871",
        "https://openalex.org/W4367665843",
        "https://openalex.org/W4378575495",
        "https://openalex.org/W4379984580",
        "https://openalex.org/W4381747944",
        "https://openalex.org/W4384161756",
        "https://openalex.org/W4384787787",
        "https://openalex.org/W4385623557",
        "https://openalex.org/W4385640809",
        "https://openalex.org/W4385801429",
        "https://openalex.org/W4386170213",
        "https://openalex.org/W4386380402",
        "https://openalex.org/W4386464315",
        "https://openalex.org/W4386590944"
      ],
      "openalex_rank": 10,
      "num_tokens": 49714,
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10379625.pdf",
      "successful_pdf_url": NaN,
      "question_generated": "What are the key applications, emerging technologies, regulatory challenges, and future research directions related to the integration of unmanned aerial vehicles in 5G-and-beyond networks?",
      "pipeline_answer": "### Key Applications of UAVs in 5G Networks\n- **Disaster Management**: UAVs enhance real-time data transmission and improve situational awareness during emergencies [S4711205946].  \n- **Surveillance**: They provide critical surveillance capabilities, leveraging the high-speed connectivity of 5G [S4711205946].  \n- **Logistics**: UAVs facilitate efficient logistics operations by enabling rapid delivery and monitoring [S4711205946].  \n\n### Emerging Technologies\n- **Multi-access Edge Computing (MEC)**: This technology optimizes UAV performance in 5G networks by enabling low-latency communication [S2141737830].  \n- **Network Slicing**: It allows for efficient resource allocation tailored to specific UAV applications, enhancing overall network efficiency [S2141737830].  \n- **Quantum Computing**: Anticipated to revolutionize UAV data processing, enabling more efficient route planning and real-time decision-making [S1979171577].  \n\n### Regulatory Challenges\n- **Standardization**: There is a critical need for standardized communication protocols to ensure safe UAV operations in 5G networks [S2585677171] [S6680170015].  \n- **Security Measures**: Protecting against cyber threats is essential for reliable UAV operations, necessitating robust security frameworks [S2585677171] [S8421165048].  \n- **Privacy Concerns**: Regulations such as EU 2019/945 and EU 2019/947 mandate remote identification and secure data links, addressing privacy issues in drone operations [S2736420461].  \n- **Operational Safety**: The evolving regulatory landscape must balance operational efficiency with safety, especially for Beyond Visual Line of Sight (BVLOS) flights [S8654834815] [S7763458077].  \n\n### Future Research Directions\n- **Security Frameworks**: Future studies should focus on developing comprehensive security frameworks tailored to UAV operations in dynamic environments [S2259813022] [S7642449272].  \n- **Real-time Data Management**: Research should address challenges related to real-time data processing and management for UAVs operating in complex scenarios [S2259813022].  \n- **Compliance Mechanisms**: Further investigation into effective compliance mechanisms for remote identification protocols is necessary to enhance operational efficiency while ensuring privacy [S7763458077].  \n\n### Conclusion\nThe integration of UAVs in 5G networks presents significant opportunities across various applications, driven by emerging technologies like MEC and network slicing. However, regulatory challenges regarding standardization, security, and privacy must be addressed to ensure safe and efficient operations. Future research should prioritize developing robust security frameworks and effective compliance mechanisms.",
      "pipeline_references": {
        "S2736420461": {
          "id": "S2736420461",
          "text": "The EU regulations EU 2019/945 and EU 2019/947, which mandate remote identification and secure data links for UAVs, are critical for ensuring compliance and privacy in 5G-enabled drone operations.",
          "children": [
            {
              "id": "E0679455280",
              "text": "..scheduled to be fully enforced on July 1 of 2020. These regulations cover the design, manufacture, and operation of drones, and has implications to manufacturers, importers, and distributors. In addition, the new regulations clarify on the technical requirements of drones in different classes. Another recent popular topic in the drone industry is the so-called Beyond Visual Line of Sight (BVLOS) flights, which can cover larger areas (including areas that are difficult or impossible for the pilots to keep an eye on). BVLOS flights can be deployed in adversarial and rough conditions, such as battlefields, inspection of key installations (e.g., oil and gas pipelines, power grids, and border control) and wild life, and search and rescue operations. However, there are also greater risky or ill-intentioned use in the operating of BVLOS flights, which may explain why they are generally not allowed in many countries. For example, the U.S. does not allow BVLOS flights, without a waiver from the relevant authority4. However, regulations on BVLOS flights are evolving at a very fast pace. An amendment5to EU2019/947 w.r.t. BVLOS flights is currently in progress, at the time of writing. We expect that the BVLOS flights will become better regulated across different countries in the near future, and it is an important aspect to consider when we study privacy preservation issues for drones. Standards bodies have also been very proactive in dronerelated activities. For example, the technical specification TS 22.125 of 3GPP6\u201cidentifies the requirements for operation of UAVs via the 3GPP system\u201d. The 3GPP Release 16 includes \u201crequirements for meeting the business, security, and public safety needs for the remote identification and tracking of Unmanned Aerial System (UAS) linked to a 3GPP subscription\u201d. In the 3GPP Release 17 (scheduled for delivery in 2021), it includes 5G enhancement for UAVs. As observed by Stocker et al. [13], an increase in drone \u0308 activities will also result in additional administrative processes, such as those relating to flight registration and approval. We believe that the decentralisation of blockchain systems is a viable approach to reducing administrative redtapes. ID of drones has been one of the main artefacts to ensure traceability and accountability. In the new EU rules, with the exception of class C0 (less than 250g), all classes must bear a unique physical serial number and more importantly a direct remote ID \u201callowing the upload of the operator registration 2Council of European Union, \u201cCommission Delegated Regulation (EU)2019/945 on unmanned aircraft systems and on third-country operators of unmanned aircraft systems,\u201d 2019, https://eur-lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:32019R0945. 3Council of European Union, \u201cCommission Delegated Regulation (EU) 2019/947 on the rules and procedures for the operation of unmanned aircraft,\u201d 2019, https://eur-lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:32019R0947. 4 It is reportedly very difficult to obtain such a waiver in the U.S.. For example, as of June 8, 2020, only 54 BVLOS (107.31) waivers have been issued, according to https://www.faa.gov/uas/commercial operators/part 107 waivers/ 5https://www.easa.europa.eu/sites/default/files/dfu/NPA%202020-07.pdf 6TS 22.125 Unmanned Aerial System (UAS) support in 3GPP, 2019 https: //www.3gpp.org/uas-uav 6 number and in real time during the whole duration of the flight, the direct periodic broadcast from the UA (unmanned aircraft) using an open and documented transmission protocol in a way that they can be received directly by existing mobile devices within the broadcasting range\u201d. On the other hand, TS 22.125 of 3GPP further elaborates that \u201cThe 3GPP system shall enable UAV to preserve the privacy of the owner of the UAV, UAV pilot, and the UAV operator in its broadcast of identity information\u201d. From these recent developments in regulations, it is clear that auditability and anonymity features due to the use of blockchain can facilitate traceability and accountability of drones. On data privacy protection, we can look at two aspects. First, the privacy of people, environment, and objects that may be intruded by drones, and second, the protection of legitimate data collected by drones and the communication privacy between the drone and the pilot. As mentioned earlier, the first aspect has been the focus of recent legislative changes. However, in practice it can be difficult to enforce. The sensory range of onboard sensors is constantly improving due to technology advancement. This compounds the challenge of tracking and identification, especially for smaller drones. The direct remote ID and the geo-awareness system required by EU2019/945 for drones in some classes are helpful in this aspect, so further exploration is necessary. The second aspect is also partially covered by EU2019/945, since drones in some classes are required to \u201cbe equipped with a data link protected against unauthorised access to the command and control functions\u201d and TS 22.125 of 3GPP states that \u201c3GPP system shall support the capability to provide different levels of integrity and privacy protection for the different connections between UAS and UTM (UAS Traffic Management) as well as the data being transferred via those connections\u201d. Table II presents a brief summary of regulations by selected representative countries7, in terms of privacy preservation. Recall that the new EU regulations EU2019/945 and EU2019/947 will be fully enforced on July 1 of 2020. Therefore, existing national regulations are in the process of being harmonised with or superseded by the new EU rules. The communication privacy on drones are largely not mentioned by the national regulations listed in Table II. In other words, the new EU regulations and the 3GPP standards are more advanced in this aspect. VI. RESEARCH CHALLENGES AND OPEN ISSUES Despite the potential benefits of blockchain in drone communication privacy preservation, there remain a number of open challenges which will be discussed next. \u2022 Resource constraints of drones: Most existing drones are resource-limited, in terms of energy, size and weight considerations. Encryption and/or consensus algorithms are generally required for blockchain systems, yet drones are generally incapable of computing-intensive tasks due to 7These countries are selected because they are representative of the most advanced development in drone regulations from different continents. Stocker \u0308 et al. [13] also studied these countries in their comparative analysis, with the exception of the new EU regulations EU2019/945 and EU2019/947. computationally constraints and battery life. In addition, a swarm of UAVs can generate and/or collect gigabytes of data per second, including both audio and video. Whether the storage capacity of blockchain can accommodate such high volume of data is still debatable, and whether and how to incorporate other storage resources (e.g., edge servers) with the UAV system remains an open challenge. Apart from these, drones are energy constrained devices, and thus they need energy efficient solutions. However, miners (i.e., drones) consume a disproportionate amount of electricity when generating blocks; thus, existing drones may not be capable of supporting sufficient energy required for mining of blocks. In the future, the orchestration of various computing facilities such as remote clouds, nearby edge servers and drones, and other technologies such as network coding, becomes a necessity to implement blockchain-based drone communications. \u2022 Full privacy preservation of drone data: In blockchainbased solutions for drone networks, each drone requires to store a copy of the data blocks (i.e., distributed ledger). This risks the dissemination of sensitive information to all participating drones. Although blockchain can guarantee certain level of privacy preservation of drone data, activities of both users of drone communications and drones can be inferred (or extracted) via statistical analysis or using other machine learning tools. For example, user private data relayed through drones may be leaked to malicious users who may compromise the drones with the aim of exfiltrating data. How to fully ensure data privacy of drone communications is still an open research question. Limiting the information sharing between drones is one potential solution, although this may not be practical in some applications. \u2022 Scalability of blockchain-based drone networks: Multiple drones can form a drone network for diverse tasks. As discussed earlier, the consensus of drone networks can help to mitigate the falsification of malicious drones and other security risks. However, it is challenging to achieve a scalable blockchain-based drone network due to the dynamics of drones (i.e., drones can join and leave at any time) as well as the scalability constraints of current blockchain systems (i.e., low throughput of transactions per second). For example, poor scalability may lead to the difficulty of forming a drone network and reaching a consensus when a new drone joins. Therefore, scalability of blockchain-based drone networks is an important issue to explore in the future. \u2022 Remote identification: As mentioned earlier, new regulations require drones to periodically broadcast their ID information that can be directly received by existing mobile devices within the broadcasting range. Such an activity needs to be conducted without violating the privacy of the owner, the pilot, and the operator. Designing an efficient solution for remote ID requires an in-depth understanding of the data transmission protocols and the various security and privacy risks (including emerging risks), and hence remains one of ongoing interest. \u2022 Regulation development and compliance enforcement: 7 TABLE II COMPARISON OF DRONE-RELATED REGULATIONS IN TERMS OF PRIVACY PRESERVATION Country Data Privacy on Drones Communication Privacy on Drones EU 2019/945 EU 2019/947 Legally regulated Drones in some classes required to be equipped with secure data link Australia Only advice to respect private privacy Privacy Act only applies on large organisations Authority plans to review privacy issues with recreational drones N/A Canada Privacy Act applies to commercial and government drones N/A China Not in national laws but covered by some provincial laws (e.g...",
              "url": "https://openalex.org/W3130357311",
              "openalex_id": "https://openalex.org/W3130357311",
              "title": "Blockchain-Based Privacy Preservation for 5G-Enabled Drone Communications",
              "publication_date": "2021-01-01"
            },
            {
              "id": "E7130157629",
              "text": "..blockchain-based drone communications. \u2022 Full privacy preservation of drone data: In blockchainbased solutions for drone networks, each drone requires to store a copy of the data blocks (i.e., distributed ledger). This risks the dissemination of sensitive information to all participating drones. Although blockchain can guarantee certain level of privacy preservation of drone data, activities of both users of drone communications and drones can be inferred (or extracted) via statistical analysis or using other machine learning tools. For example, user private data relayed through drones may be leaked to malicious users who may compromise the drones with the aim of exfiltrating data. How to fully ensure data privacy of drone communications is still an open research question. Limiting the information sharing between drones is one potential solution, although this may not be practical in some applications. \u2022 Scalability of blockchain-based drone networks: Multiple drones can form a drone network for diverse tasks. As discussed earlier, the consensus of drone networks can help to mitigate the falsification of malicious drones and other security risks. However, it is challenging to achieve a scalable blockchain-based drone network due to the dynamics of drones (i.e., drones can join and leave at any time) as well as the scalability constraints of current blockchain systems (i.e., low throughput of transactions per second). For example, poor scalability may lead to the difficulty of forming a drone network and reaching a consensus when a new drone joins. Therefore, scalability of blockchain-based drone networks is an important issue to explore in the future. \u2022 Remote identification: As mentioned earlier, new regulations require drones to periodically broadcast their ID information that can be directly received by existing mobile devices within the broadcasting range. Such an activity needs to be conducted without violating the privacy of the owner, the pilot, and the operator. Designing an efficient solution for remote ID requires an in-depth understanding of the data transmission protocols and the various security and privacy risks (including emerging risks), and hence remains one of ongoing interest. \u2022 Regulation development and compliance enforcement: 7 TABLE II COMPARISON OF DRONE-RELATED REGULATIONS IN TERMS OF PRIVACY PRESERVATION Country Data Privacy on Drones Communication Privacy on Drones EU 2019/945 EU 2019/947 Legally regulated Drones in some classes required to be equipped with secure data link Australia Only advice to respect private privacy Privacy Act only applies on large organisations Authority plans to review privacy issues with recreational drones N/A Canada Privacy Act applies to commercial and government drones N/A China Not in national laws but covered by some provincial laws (e.g., Sichuan) N/A Colombia Not allowed to violate the rights of privacy N/A France Operators obliged to respect privacy rights of individuals Germany Bundesdatenschutzgesetz (BDSG, federal data protection act) applies N/A Italy Italian Data Protection Code, enacting GDPR, applies N/A Japan Not linked to the Act on the Protection of Personal Information (APPI) but authority plans to cover privacy in next phase in the roadmap N/A Rwanda Operators oblighed to respect privacy rights of others surveillance of people and property without their consent is prohibited N/A The Netherlands Operators not allowed to violate other people\u2019s privacy N/A United Kingdom The Data Protection Act (DPA) applies N/A United States Covered differently by State- or City-level laws N/A Regulations and standards for drones are still evolving, and privacy preservation remains an prioritised agenda. Drone accidents may occur due to a range of reasons, such as technical malfunction, improper operations, unforeseen environmental events (e.g., sudden wind gusts), and hijacking. As more automation functionalities are being introduced into drones, clear definitions of liabilities and responsibilities for all participants involved across the entire life cycle of a drone will need to be explored. A closely related issue is how to enforce the compliance of regulations for drones. The collection and certification of digital evidence (enabled by blockchain) on drone accidents or privacy intrusions / breaches are also potential research topics. VII. CONCLUSION This article discussed blockchain-based privacy preservation solutions for 5G-enabled drone communications, as well as related data privacy legislation and regulations that need to be considered in the design of these solutions. We also identified potential challenges and open issues to inform future research agenda that will allow the community to leverage blockchain to facilitate privacy preservation in drone communications. REFERENCES [1] D. Tezza and M. Andujar, \u201cThe state-of-the-art of human\u2013drone interaction: A survey,\u201d IEEE Access, vol. 7, pp. 167 438\u2013167 454, 2019. [2] G. Yang, X. Shi, L. Feng, S. He, Z. Shi, and J. Chen, \u201cCedar: A costeffective crowdsensing system for detecting and localizing drones,\u201d IEEE Transactions on Mobile Computing, pp. 1\u20131, 2019. [3] Y. Zeng, Q. Wu, and R. Zhang, \u201cAccessing From the Sky: A Tutorial on UAV Communications for 5G and Beyond,\u201d Proceedings of the IEEE, vol. 107, no. 12, pp. 2327\u20132375, Dec 2019. [4] C. Lin, D. He, N. Kumar, K.-K. R. Choo, A. Vinel, and X. Huang, \u201csecurity and privacy for the internet of drones: Challenges and solutions,\u201d IEEE Communications Magazine. [5] G. D. L. T. Parra, P. Rad, and K. R. Choo, \u201cDriverless vehicle security: Challenges and future research opportunities,\u201d Future Gener. Comput. Syst., vol. 108, pp. 1092\u20131111, 2020. [6] L. Cheng, J. Liu, G. Xu, Z. Zhang, H. Wang, H. Dai, Y. Wu, and W. Wang, \u201cSCTSC: A Semicentralized Traffic Signal Control Mode With Attribute-Based Blockchain in IoVs,\u201d IEEE Transactions on Computational Social Systems, vol. 6, no. 6, pp. 1373\u20131385, Dec 2019. [7] W. Zou, D. Lo, P. S. Kochhar, X. D. Le, X. Xia, Y. Feng, Z. Chen, and B. Xu, \u201cSmart contract development: Challenges and opportunities,\u201d IEEE Transactions on Software Engineering, pp. 1\u20131, 2019. [8] B. Cao, Y. Li, L. Zhang, L. Zhang, S. Mumtaz, Z. Zhou, and M. Peng, \u201cWhen Internet of Things Meets Blockchain: Challenges in Distributed Consensus,\u201d IEEE Network, vol. 33, no. 6, pp. 133\u2013139, 2019. [9] Q. Wu, W. Mei, and R. Zhang, \u201cSafeguarding wireless network with uavs: A physical layer security perspective,\u201d IEEE Wireless Commun., vol. 26, no. 5, pp. 12\u201318, 2019. [10] Y. Qian, Y. Jiang, L. Hu, M. S. Hossain, M. Alrashoud, and M. AlHammadi, \u201cBlockchain-based privacy-aware content caching in cognitive internet of vehicles,\u201d IEEE Network, vol. 34, no. 2, pp. 46\u201351, 2020. [11] E. Yanmaz, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter, \u201cDrone networks: Communications, coordination, and sensing,\u201d Ad Hoc Networks, vol. 68, pp. 1 \u2013 15, 2018. [12] H.-N. Dai, Z. Zheng, and Y. Zhang, \u201cBlockchain for internet of things: A survey,\u201d IEEE Internet of Things Journal, vol. 6, no. 5, pp. 8076\u20138094, Oct 2019. [13] C. Stocker, R. Bennett, F. Nex, M. Gerke, and J. Zevenbergen, \u201cReview \u0308 of the current state of UAV regulations,\u201d Remote sensing, vol. 9, no. 5, p. 459, 2017. [14] A. Fotouhi, H. Qiang, M. Ding, M. Hassan, L. G. Giordano, A. GarciaRodriguez, and J. Yuan, \u201cSurvey on UAV cellular communications: Practical aspects, standardization advancements, regulation, and security challenges,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 4, pp. 3417\u20133442, 2019. [15] Z. Ullah, F. Al-Turjman, and L. Mostarda, \u201cCognition in UAV-Aided 5G and Beyond Communications: A Survey,\u201d IEEE Transactions on Cognitive Communications and Networking, 2020. Yulei Wu [SM\u201918] is a Senior Lecturer with the Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, United Kingdom. He received the B.Sc. degree (First Class Honours) in Computer Science and the Ph.D. degree in Computing and Mathematics from the University of Bradford, United Kingdom, in 2006 and 2010, respectively. His expertise is on networking and his main research interests include computer networks, networked systems, software defined networks and systems, network management, and network security and privacy. Dr. Wu contributes to major conferences on networking and networked systems as various roles, including the Steering Committee Chair, the General Chair and the Program Chair. His research has been supported by Engineering and Physical Sciences Research Council of United Kingdom, National Natural 8 Science Foundation of China, University\u2019s Innovation Platform and industry. He is an Editor of IEEE Transactions on Network and Service Management, Computer Networks (Elsevier) and IEEE Access. He is a Fellow of the HEA (Higher Education Academy). Hong-Ning Dai [SM\u201916] is currently with Faculty of Information Technology at Macau University of Science and Technology as an associate professor. He obtained the Ph.D. degree in Computer Science and Engineering from Department of Computer..",
              "url": "https://openalex.org/W3130357311",
              "openalex_id": "https://openalex.org/W3130357311",
              "title": "Blockchain-Based Privacy Preservation for 5G-Enabled Drone Communications",
              "publication_date": "2021-01-01"
            }
          ]
        },
        "S1979171577": {
          "id": "S1979171577",
          "text": "The application of quantum computing in UAV technology is anticipated to revolutionize data processing capabilities, enabling more efficient route planning and real-time decision-making in complex environments.",
          "children": [
            {
              "id": "E0052582723",
              "text": "..on AV within the core 5G SDOs. To resolve this, more AV stakeholders should contribute to 5G SDOs, and more AV-related subgroups would be formed within core 5G SDOs to develop dedicated AV-related standards. Second, how do we integrate and cooperate between different working groups and SDOs? The strong cooperation between AV SDOs and core 5G SDOs would be further encouraged. This will resolve some of the issues related to the first issue. More joint research and SDOs activities should be planned in the future. E. Emerging and Future Research Directions related to AV 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. 1) Quantum Computing: Quantum computing is the next future generation of automotive technology. Electric vehicles are significant part of quantum revolution. Automobile manufactures have started taking advantage of quantum computers to solve various automotive problems. AI in AV requires large amount of data for analysing and providing optimal response in dynamic situations. For detecting real time car locations and designing the optimal path requires high computing power and speed in AI. The former feature, high computing power can achieved by quantum computers. German automobile company Volkswagen collaborated with D-Wave systems to design and develop traffic routing in Beijing based on quantum computing systems. These systems can also solve optimization problems like waiting time, deployment of fleets etc., Volkswagen has also partnered with google to predict the state of traffic to avoid accident and to simulate the behaviour of electrical component and embed AI in driverless cars. As the AV are more 29 vulnerable to outside world, security breaches can be solved by quantum security. AV requires tremendous computing powers like optimized route planning and change the entire transport systems into smart systems. The cars will become smarter by communicating among themselves and outside world. More developments are expected in the field of AV integrated with quantum computers to achieve the benefits of computing and processing power. 2) Cognitive Cloud: Cognitive AI and algorithms would help us to simulate human-level performance specifically in level 5 AV. Satisfying the level 5 AV is a tedious task as it needs accurate decision making, object detection and localization under uncertain conditions like fog, rain and extreme darkness. Cognitive computing enhances the model accuracy to achieve closeness to humanlike performance in object detection and decision making. Integration of Cognitive computing in AV leads to improved safety and accuracy. Cognitive Internet of Vehicles allows the AV to focus on what, how and where to compute dynamically closer to human brain. VIII. CONCLUSION In this study, several aspects of AVs such as its features, levels of automation, architecture, key-enabling technologies and requirements for autonomus vehicular communication were discussed. Key requirements in terms of latency, security level, privacy, bandwidth, mobility, scalability, availability and reliability for potential AV applications (navigation and path planning, object detection, URLLC, mMTC, eMBB) were also identified. Several emerging technologies such as MEC, SDN and others were studied in detail and impact of 5G/B2G on these technologies was discussed. We also identified key security concerns in AVs with respect to 5G/B2G technology and highlighted recent standardization efforts by different organisation. Finally, several key research challenges and future research directions were also identified and discussed. REFERENCES [1] A. Manfreda, K. Ljubi, and A. Groznik, \u201cAutonomous vehicles in the smart city era: An empirical study of adoption factors important for millennials,\u201d International Journal of Information Management, p. 102050, 2019. [2] C. Ravi, A. Tigga, G. T. Reddy, S. Hakak, and M. Alazab, \u201cDriver identification using optimized deep learning model in smart transportation,\u201d ACM Transactions on Internet Technology, 2020. [3] K. Jadaan, S. Zeater, and Y. Abukhalil, \u201cConnected vehicles: an innovative transport technology,\u201d Procedia Engineering, vol. 187, pp. 641\u2013648, 2017. [4] Z. Wadud, D. MacKenzie, and P. Leiby, \u201cHelp or hindrance? the travel, energy and carbon impacts of highly automated vehicles,\u201d Transportation Research Part A: Policy and Practice, vol. 86, pp. 1\u201318, 2016. [5] J. Sachs, G. Wikstrom, T. Dudda, R. Baldemair, and K. Kittichokechai, \u201c5g radio network design for ultra-reliable low-latency communication,\u201d IEEE network, vol. 32, no. 2, pp. 24\u201331, 2018. [6] M. M. d. Silva and J. Guerreiro, \u201cOn the 5g and beyond,\u201d Applied Sciences, vol. 10, no. 20, p. 7091, 2020. [7] A. Rasouli and J. K. Tsotsos, \u201cAutonomous vehicles that interact with pedestrians: A survey of theory and practice,\u201d IEEE transactions on intelligent transportation systems, vol. 21, no. 3, pp. 900\u2013918, 2019. [8] Y. Ma, Z. Wang, H. Yang, and L. Yang, \u201cArtificial intelligence applications in the development of autonomous vehicles: a survey,\u201d IEEE/CAA Journal of Automatica Sinica, vol. 7, no. 2, pp. 315\u2013329, 2020. [9] M. N. Ahangar, Q. Z. Ahmed, F. A. Khan, and M. Hafeez, \u201cA survey of autonomous vehicles: Enabling communication technologies and challenges,\u201d Sensors, vol. 21, no. 3, p. 706, 2021. [10] C. R. Storck and F. Duarte-Figueiredo, \u201cA survey of 5g technology evolution, standards, and infrastructure associated with vehicle-toeverything communications by internet of vehicles,\u201d IEEE Access, vol. 8, pp. 117 593\u2013117 614, 2020. [11] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-Munoz, and J. M. Lopez-Soler, \u201cA survey on 5g usage scenarios and traffic models,\u201d IEEE Communications Surveys & Tutorials, vol. 22, no. 2, pp. 905\u2013929, 2020. [12] M. Pham and K. Xiong, \u201cA survey on security attacks and defense techniques for connected and autonomous vehicles,\u201d Computers & Security, p. 102269, 2021. [13] S. Khazraeian and M. Hadi, \u201cIntelligent transportation systems in future smart cities,\u201d in Sustainable Interdependent Networks II. Springer, 2019, pp. 109\u2013120. [14] S. E. Shladover, \u201cConnected and automated vehicle systems: Introduction and overview,\u201d Journal of Intelligent Transportation Systems, vol. 22, no. 3, pp. 190\u2013200, 2018. [15] R. Hussain and S. Zeadally, \u201cAutonomous cars: Research results, issues, and future challenges,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 2, pp. 1275\u20131313, 2018. [16] J. Contreras-Castillo, S. Zeadally, and J. A. Guerrero-Ibanez, \u201cInternet \u0303 of vehicles: architecture, protocols, and security,\u201d IEEE internet of things Journal, vol. 5, no. 5, pp. 3701\u20133709, 2018. [17] S. Zhang, J. Chen, F. Lyu, N. Cheng, W. Shi, and X. Shen, \u201cVehicular communication networks in the automated driving era,\u201d IEEE Communications Magazine, vol. 56, no. 9, pp. 26\u201332, 2018. [18] S. Taxonomy, \u201cDefinitions for terms related to driving automation systems for on-road motor vehicles (j3016),\u201d Technical report, Society for Automotive Engineering, Tech. Rep., 2016. [19] F. Jameel, Z. Chang, J. Huang, and T. Ristaniemi, \u201cInternet of autonomous vehicles: architecture, features, and socio-technological challenges,\u201d IEEE Wireless Communications, vol. 26, no. 4, pp. 21\u201329, 2019. [20] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, \u201cA survey of autonomous driving: Common practices and emerging technologies,\u201d IEEE access, vol. 8, pp. 58 443\u201358 469, 2020. [21] J. Van Brummelen, M. O\u2019Brien, D. Gruyer, and H. Najjaran, \u201cAutonomous vehicle perception: The technology of today and tomorrow,\u201d Transportation research part C: emerging technologies, vol. 89, pp. 384\u2013406, 2018. [22] D. Gonzalez, J. P \u0301 erez, V. Milan \u0301 es, and F. Nashashibi, \u201cA review of mo- \u0301 tion planning techniques for automated vehicles,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 17, no. 4, pp. 1135\u20131145, 2015. [23] N. H. Amer, H. Zamzuri, K. Hudha, and Z. A. Kadir, \u201cModelling and control strategies in path tracking control for autonomous ground vehicles: a review of state of the art and challenges,\u201d Journal of intelligent & robotic systems, vol. 86, no. 2, pp. 225\u2013254, 2017. [24] J. Guanetti, Y. Kim, and F. Borrelli, \u201cControl of connected and..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            },
            {
              "id": "E4933323753",
              "text": "... 5, pp. 4278\u20134291, 2019. [113] L. Xie, Y. Ding, H. Yang, and X. Wang, \u201cBlockchain-based secure and trustworthy internet of things in sdn-enabled 5g-vanets,\u201d IEEE Access, vol. 7, pp. 56 656\u201356 666, 2019. [114] B. Bera, S. Saha, A. K. Das, N. Kumar, P. Lorenz, and M. Alazab, \u201cBlockchain-envisioned secure data delivery and collection scheme for 5g-based iot-enabled internet of drones environment,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9097\u20139111, 2020. [115] M. Aloqaily, O. Bouachir, A. Boukerche, and I. Al Ridhawi, \u201cDesign guidelines for blockchain-assisted 5g-uav networks,\u201d IEEE Network, vol. 35, no. 1, pp. 64\u201371, 2021. [116] R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain and 5g integrated softwarized uav network management: Architecture, solutions, and challenges,\u201d Physical Communication, vol. 47, p. 101355, 2021. [117] X. Jian, P. Leng, Y. Wang, M. Alrashoud, and M. S. Hossain, \u201cBlockchain-empowered trusted networking for unmanned aerial vehicles in the b5g era,\u201d IEEE Network, vol. 35, no. 1, pp. 72\u201377, 2021. [118] C. Feng, K. Yu, A. K. Bashir, Y. D. Al-Otaibi, Y. Lu, S. Chen, and D. Zhang, \u201cEfficient and secure data sharing for 5g flying drones: a blockchain-enabled approach,\u201d IEEE Network, vol. 35, no. 1, pp. 130\u2013 137, 2021. [119] B. Ghimire, D. B. Rawat, C. Liu, and J. Li, \u201cSharding-enabled blockchain for software-defined internet of unmanned vehicles in the battlefield,\u201d IEEE Network, vol. 35, no. 1, pp. 101\u2013107, 2021. 32 [120] A. Gumaei, M. Al-Rakhami, M. M. Hassan, P. Pace, G. Alai, K. Lin, and G. Fortino, \u201cDeep learning and blockchain with edge computing for 5g-enabled drone identification and flight mode detection,\u201d IEEE Network, vol. 35, no. 1, pp. 94\u2013100, 2021. [121] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, \u201cFederated learning meets blockchain in edge computing: Opportunities and challenges,\u201d IEEE Internet of Things Journal, vol. 8, no. 16, pp. 12 806\u201312 825, 2021. [122] P. Kumar, R. Kumar, G. Srivastava, G. P. Gupta, R. Tripathi, T. R. Gadekallu, and N. Xiong, \u201cPpsf: A privacy-preserving and secure framework using blockchain-based machine-learning for iot-driven smart cities,\u201d IEEE Transactions on Network Science and Engineering, 2021. [123] S. Bouraga, \u201cA taxonomy of blockchain consensus protocols: A survey and classification framework,\u201d Expert Systems with Applications, vol. 168, p. 114384, 2021. [124] V. Ortega, F. Bouchmal, and J. F. Monserrat, \u201cTrusted 5g vehicular networks: Blockchains and content-centric networking,\u201d IEEE Vehicular Technology Magazine, vol. 13, no. 2, pp. 121\u2013127, 2018. [125] R. Shrestha, S. Y. Nam, R. Bajracharya, and S. Kim, \u201cEvolution of v2x communication and integration of blockchain for security enhancements,\u201d Electronics, vol. 9, no. 9, p. 1338, 2020. [126] S. Rahmadika, K. Lee, and K.-H. Rhee, \u201cBlockchain-enabled 5g autonomous vehicular networks,\u201d in 2019 International Conference on Sustainable Engineering and Creative Computing (ICSECC). IEEE, 2019, pp. 275\u2013280. [127] D. Reebadiya, T. Rathod, R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain-based secure and intelligent sensing scheme for autonomous vehicles activity tracking beyond 5g networks,\u201d Peer-to-Peer Networking and Applications, pp. 1\u201318, 2021. [128] L. Nkenyereye, B. Adhi Tama, M. K. Shahzad, and Y.-H. Choi, \u201cSecure and blockchain-based emergency driven message protocol for 5g enabled vehicular edge computing,\u201d Sensors, vol. 20, no. 1, p. 154, 2020. [129] J. Chen, W. Wang, Y. Zhou, S. H. Ahmed, and W. Wei, \u201cExploiting 5g and blockchain for medical applications of drones,\u201d IEEE Network, vol. 35, no. 1, pp. 30\u201336, 2021. [130] G. Kakkavas, A. Stamou, V. Karyotis, and S. Papavassiliou, \u201cNetwork tomography for efficient monitoring in sdn-enabled 5g networks and beyond: Challenges and opportunities,\u201d IEEE Communications Magazine, vol. 59, no. 3, pp. 70\u201376, 2021. [131] C. Benzaid and T. Taleb, \u201cAi-driven zero touch network and service management in 5g and beyond: Challenges and research directions,\u201d IEEE Network, vol. 34, no. 2, pp. 186\u2013194, 2020. [132] M. Liyanage, Q.-V. Pham, K. Dev, S. Bhattacharya, P. K. R. Maddikunta, T. R. Gadekallu, and G. Yenduri, \u201cA survey on zero touch network and service (zsm) management for 5g and beyond networks,\u201d Journal of Network and Computer Applications, p. 103362, 2022. [133] J. Cui, L. S. Liew, G. Sabaliauskaite, and F. Zhou, \u201cA review on safety failures, security attacks, and available countermeasures for autonomous vehicles,\u201d Ad Hoc Networks, vol. 90, p. 101823, 2019. [134] A. M. Malla and R. K. Sahu, \u201cSecurity attacks with an effective solution for dos attacks in vanet,\u201d International Journal of Computer Applications, vol. 66, no. 22, 2013. [135] S. S. Manvi and S. Tangade, \u201cA survey on authentication schemes in vanets for secured communication,\u201d Vehicular Communications, vol. 9, pp. 19\u201330, 2017. [136] L. B. Othmane, H. Weffers, M. M. Mohamad, and M. Wolf, \u201cA survey of security and privacy in connected vehicles,\u201d in Wireless sensor and mobile ad-hoc networks. Springer, 2015, pp. 217\u2013247. [137] S. Zhang, Y. Lin, Q. Liu, J. Jiang, B. Yin, and K.-K. R. Choo, \u201cSecure hitch in location based social networks,\u201d Computer Communications, vol. 100, pp. 65\u201377, 2017. [138] M. Raya and J.-P. Hubaux, \u201cSecuring vehicular ad hoc networks,\u201d Journal of computer security, vol. 15, no. 1, pp. 39\u201368, 2007. [139] J. Petit, B. Stottelaar, M. Feiri, and F. Kargl, \u201cRemote attacks on automated vehicles sensors: Experiments on camera and lidar,\u201d Black Hat Europe, vol. 11, no. 2015, p. 995, 2015. [140] B. G. Stottelaar, \u201cPractical cyber-attacks on autonomous vehicles,\u201d Master\u2019s thesis, University of Twente, 2015. [141] R. Chauhan, A platform for false data injection in frequency modulated continuous wave radar. Utah State University, 2014. [142] W. E. Buehler, R. M. Whitson, and M. J. Lewis, \u201cAirborne radar jamming system,\u201d Sep. 9 2014, uS Patent 8,830,112. [143] N. Fairfield and C. Urmson, \u201cTraffic light mapping and detection,\u201d in 2011 IEEE International Conference on Robotics and Automation. IEEE, 2011, pp. 5421\u20135426. [144] A. B. Hillel, R. Lerner, D. Levi, and G. Raz, \u201cRecent progress in road and lane detection: a survey,\u201d Machine vision and applications, vol. 25, no. 3, pp. 727\u2013745, 2014. [145] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, \u201cPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8445\u20138453. [146] T. B. Brown, D. Mane, A. Roy, M. Abadi, and J. Gilmer, \u201cAdversarial \u0301 patch,\u201d arXiv preprint arXiv:1712.09665,..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            }
          ]
        },
        "S4711205946": {
          "id": "S4711205946",
          "text": "The integration of unmanned aerial vehicles (UAVs) in 5G networks is expected to enhance applications such as disaster management, surveillance, and logistics by providing real-time data transmission and improved situational awareness.",
          "children": [
            {
              "id": "E0052582723",
              "text": "..on AV within the core 5G SDOs. To resolve this, more AV stakeholders should contribute to 5G SDOs, and more AV-related subgroups would be formed within core 5G SDOs to develop dedicated AV-related standards. Second, how do we integrate and cooperate between different working groups and SDOs? The strong cooperation between AV SDOs and core 5G SDOs would be further encouraged. This will resolve some of the issues related to the first issue. More joint research and SDOs activities should be planned in the future. E. Emerging and Future Research Directions related to AV 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. 1) Quantum Computing: Quantum computing is the next future generation of automotive technology. Electric vehicles are significant part of quantum revolution. Automobile manufactures have started taking advantage of quantum computers to solve various automotive problems. AI in AV requires large amount of data for analysing and providing optimal response in dynamic situations. For detecting real time car locations and designing the optimal path requires high computing power and speed in AI. The former feature, high computing power can achieved by quantum computers. German automobile company Volkswagen collaborated with D-Wave systems to design and develop traffic routing in Beijing based on quantum computing systems. These systems can also solve optimization problems like waiting time, deployment of fleets etc., Volkswagen has also partnered with google to predict the state of traffic to avoid accident and to simulate the behaviour of electrical component and embed AI in driverless cars. As the AV are more 29 vulnerable to outside world, security breaches can be solved by quantum security. AV requires tremendous computing powers like optimized route planning and change the entire transport systems into smart systems. The cars will become smarter by communicating among themselves and outside world. More developments are expected in the field of AV integrated with quantum computers to achieve the benefits of computing and processing power. 2) Cognitive Cloud: Cognitive AI and algorithms would help us to simulate human-level performance specifically in level 5 AV. Satisfying the level 5 AV is a tedious task as it needs accurate decision making, object detection and localization under uncertain conditions like fog, rain and extreme darkness. Cognitive computing enhances the model accuracy to achieve closeness to humanlike performance in object detection and decision making. Integration of Cognitive computing in AV leads to improved safety and accuracy. Cognitive Internet of Vehicles allows the AV to focus on what, how and where to compute dynamically closer to human brain. VIII. CONCLUSION In this study, several aspects of AVs such as its features, levels of automation, architecture, key-enabling technologies and requirements for autonomus vehicular communication were discussed. Key requirements in terms of latency, security level, privacy, bandwidth, mobility, scalability, availability and reliability for potential AV applications (navigation and path planning, object detection, URLLC, mMTC, eMBB) were also identified. Several emerging technologies such as MEC, SDN and others were studied in detail and impact of 5G/B2G on these technologies was discussed. We also identified key security concerns in AVs with respect to 5G/B2G technology and highlighted recent standardization efforts by different organisation. Finally, several key research challenges and future research directions were also identified and discussed. REFERENCES [1] A. Manfreda, K. Ljubi, and A. Groznik, \u201cAutonomous vehicles in the smart city era: An empirical study of adoption factors important for millennials,\u201d International Journal of Information Management, p. 102050, 2019. [2] C. Ravi, A. Tigga, G. T. Reddy, S. Hakak, and M. Alazab, \u201cDriver identification using optimized deep learning model in smart transportation,\u201d ACM Transactions on Internet Technology, 2020. [3] K. Jadaan, S. Zeater, and Y. Abukhalil, \u201cConnected vehicles: an innovative transport technology,\u201d Procedia Engineering, vol. 187, pp. 641\u2013648, 2017. [4] Z. Wadud, D. MacKenzie, and P. Leiby, \u201cHelp or hindrance? the travel, energy and carbon impacts of highly automated vehicles,\u201d Transportation Research Part A: Policy and Practice, vol. 86, pp. 1\u201318, 2016. [5] J. Sachs, G. Wikstrom, T. Dudda, R. Baldemair, and K. Kittichokechai, \u201c5g radio network design for ultra-reliable low-latency communication,\u201d IEEE network, vol. 32, no. 2, pp. 24\u201331, 2018. [6] M. M. d. Silva and J. Guerreiro, \u201cOn the 5g and beyond,\u201d Applied Sciences, vol. 10, no. 20, p. 7091, 2020. [7] A. Rasouli and J. K. Tsotsos, \u201cAutonomous vehicles that interact with pedestrians: A survey of theory and practice,\u201d IEEE transactions on intelligent transportation systems, vol. 21, no. 3, pp. 900\u2013918, 2019. [8] Y. Ma, Z. Wang, H. Yang, and L. Yang, \u201cArtificial intelligence applications in the development of autonomous vehicles: a survey,\u201d IEEE/CAA Journal of Automatica Sinica, vol. 7, no. 2, pp. 315\u2013329, 2020. [9] M. N. Ahangar, Q. Z. Ahmed, F. A. Khan, and M. Hafeez, \u201cA survey of autonomous vehicles: Enabling communication technologies and challenges,\u201d Sensors, vol. 21, no. 3, p. 706, 2021. [10] C. R. Storck and F. Duarte-Figueiredo, \u201cA survey of 5g technology evolution, standards, and infrastructure associated with vehicle-toeverything communications by internet of vehicles,\u201d IEEE Access, vol. 8, pp. 117 593\u2013117 614, 2020. [11] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-Munoz, and J. M. Lopez-Soler, \u201cA survey on 5g usage scenarios and traffic models,\u201d IEEE Communications Surveys & Tutorials, vol. 22, no. 2, pp. 905\u2013929, 2020. [12] M. Pham and K. Xiong, \u201cA survey on security attacks and defense techniques for connected and autonomous vehicles,\u201d Computers & Security, p. 102269, 2021. [13] S. Khazraeian and M. Hadi, \u201cIntelligent transportation systems in future smart cities,\u201d in Sustainable Interdependent Networks II. Springer, 2019, pp. 109\u2013120. [14] S. E. Shladover, \u201cConnected and automated vehicle systems: Introduction and overview,\u201d Journal of Intelligent Transportation Systems, vol. 22, no. 3, pp. 190\u2013200, 2018. [15] R. Hussain and S. Zeadally, \u201cAutonomous cars: Research results, issues, and future challenges,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 2, pp. 1275\u20131313, 2018. [16] J. Contreras-Castillo, S. Zeadally, and J. A. Guerrero-Ibanez, \u201cInternet \u0303 of vehicles: architecture, protocols, and security,\u201d IEEE internet of things Journal, vol. 5, no. 5, pp. 3701\u20133709, 2018. [17] S. Zhang, J. Chen, F. Lyu, N. Cheng, W. Shi, and X. Shen, \u201cVehicular communication networks in the automated driving era,\u201d IEEE Communications Magazine, vol. 56, no. 9, pp. 26\u201332, 2018. [18] S. Taxonomy, \u201cDefinitions for terms related to driving automation systems for on-road motor vehicles (j3016),\u201d Technical report, Society for Automotive Engineering, Tech. Rep., 2016. [19] F. Jameel, Z. Chang, J. Huang, and T. Ristaniemi, \u201cInternet of autonomous vehicles: architecture, features, and socio-technological challenges,\u201d IEEE Wireless Communications, vol. 26, no. 4, pp. 21\u201329, 2019. [20] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, \u201cA survey of autonomous driving: Common practices and emerging technologies,\u201d IEEE access, vol. 8, pp. 58 443\u201358 469, 2020. [21] J. Van Brummelen, M. O\u2019Brien, D. Gruyer, and H. Najjaran, \u201cAutonomous vehicle perception: The technology of today and tomorrow,\u201d Transportation research part C: emerging technologies, vol. 89, pp. 384\u2013406, 2018. [22] D. Gonzalez, J. P \u0301 erez, V. Milan \u0301 es, and F. Nashashibi, \u201cA review of mo- \u0301 tion planning techniques for automated vehicles,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 17, no. 4, pp. 1135\u20131145, 2015. [23] N. H. Amer, H. Zamzuri, K. Hudha, and Z. A. Kadir, \u201cModelling and control strategies in path tracking control for autonomous ground vehicles: a review of state of the art and challenges,\u201d Journal of intelligent & robotic systems, vol. 86, no. 2, pp. 225\u2013254, 2017. [24] J. Guanetti, Y. Kim, and F. Borrelli, \u201cControl of connected and..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            },
            {
              "id": "E9577549159",
              "text": "..M. Ding, M. Hassan, L. G. Giordano, A. Garcia-Rodriguez, J. Yuan, Survey on UAV Cellular Communications: Practical Aspects, Standardization Advancements, Regulation, and Security Challenges, IEEE Communications Surveys & Tutorials 21 (4) (2019) 3417\u20133442. [14] Z. Ullah, F. Al-Turjman, L. Mostarda, Cognition in UAV-Aided 5G and Beyond Communications: A Survey, IEEE Transactions on Cognitive Communications and Networking (2020). [15] P. Marsch, I. Da Silva, O. Bulakci, M. Tesanovic, S. E. El Ayoubi, T. Rosowski, A. Kaloxylos, M. Boldi, 5G Radio Access Network Architecture: Design Guidelines and Key Considerations, IEEE Communications Magazine 54 (11) (2016) 24\u201332. [16] P. Marsch, I. Da Silva, \u00d6. Bulakci, M. Tesanovic, S. E. El Ayoubi, M. S\u00e4ily, Emerging network architecture and functional design considerations for 5G radio access, Transactions on Emerging Telecommunications Technologies 27 (9) (2016) 1168\u20131177. [17] S. D. Muruganathan, X. Lin, H.-L. Maattanen, Z. Zou, W. A. Hapsari, S. Yasukawa, An Overview of 3GPP Release-15 Study on Enhanced LTE Support for Connected Drones, arXiv preprint arXiv:1805.00826 (2018). [18] B. Li, Z. Fei, Y. Zhang, UAV Communications for 5G and Beyond: Recent Advances and Future Trends, IEEE Internet of Things Journal 6 (2) (2018) 2241\u20132263. [19] I. Bor-Yaliniz, M. Salem, G. Senerath, H. Yanikomeroglu, Is 5G Ready for Drones: A Look into Contemporary and Prospective Wireless Networks from a Standardization Perspective, IEEE Wireless Communications 26 (1) (2019) 18\u201327. [20] S. A. R. Naqvi, S. A. Hassan, H. Pervaiz, Q. Ni, Drone-Aided Communication as a Key Enabler for 5G and Resilient Public Safety Networks, IEEE Communications Magazine 56 (1) (2018) 36\u201342. [21] R. J. Kerczewski, J. D. Wilson, W. D. Bishop, Frequency spectrum for integration of unmanned aircraft, 2013 IEEE/AIAA 32nd Digital Avionics Systems Conference (DASC) (2013) 6D5\u20131. [22] 3GPP TR 36.777, Technical specification group radio access network:study on enhanced LTE support for aerial vehicles (V15.0.0, Dec, 2017). [23] Y. Zeng, Q. Wu, R. Zhang, Accessing From the Sky: A Tutorial on UAV Communications for 5G and Beyond, Proceedings of the IEEE 107 (12) (2019) 2327\u20132375. [24] E. Vinogradov, H. Sallouha, S. De Bast, M. M. Azari, S. Pollin, Tutorial on UAV: A Blue Sky View on Wireless Communication, arXiv preprint arXiv:1901.02306 (2019). [25] A. Chakraborty, E. Chai, K. Sundaresan, A. Khojastepour, S. Rangarajan, SkyRAN: a self-organizing LTE RAN in the sky, Proceedings of the 14th International Conference on emerging Networking EXperiments and Technologies (2018) 280\u2013292. [26] K. Sundaresan, E. Chai, A. Chakraborty, S. Rangarajan, SkyLiTE: End-to-End Design of Low-Altitude UAV Networks for Providing LTE Connectivity, arXiv preprint arXiv:1802.06042 (2018). [27] F. Al-Turjman, M. Abujubbeh, A. Malekloo, L. Mostarda, UAVs assessment in software-defined IoT networks: An overview, Computer Communications 150 (2020) 519\u2013536. [28] Y. Zeng, J. Lyu, R. Zhang, Cellular-Connected UAV: Potential, Challenges, and Promising Technologies, IEEE Wireless Communications 26 (1) (2018) 120\u2013127. [29] M. M. Azari, G. Geraci, A. Garcia-Rodriguez, S. Pollin, Cellular UAV-to-UAV Communications, IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) (2019) 1\u20137. [30] S. Zhang, H. Zhang, B. Di, L. Song, Cellular UAV-to-X Communications: Design and optimization for multi-UAV networks, IEEE Transactions on Wireless Communications 18 (2) (2019) 1346\u2013 1359. [31] W. Shi, J. Li, N. Cheng, F. Lyu, S. Zhang, H. Zhou, X. Shen, MultiDrone 3-D Trajectory Planning and Scheduling in Drone-Assisted Radio Access Networks, IEEE Transactions on Vehicular Technology 68 (8) (2019) 8145\u20138158. [32] L. Zhang, H. Zhao, S. Hou, Z. Zhao, H. Xu, X. Wu, Q. Wu, R. Zhang, A Survey on 5G Millimeter Wave Communications for UAV-Assisted Wireless Networks, IEEE Access 7 (2019) 117460\u2013 117504. [33] M. E. Mkiramweni, C. Yang, J. Li, W. Zhang, A Survey of Game Theory in Unmanned Aerial Vehicles Communications, IEEE Communications Surveys & Tutorials 21 (4) (2019) 3386\u20133416. [34] A. I. Hentati, L. C. Fourati, Comprehensive Survey of UAVs Communication Networks, Computer Standards & Interfaces (2020) 103451. [35] A. A. Khuwaja, Y. Chen, N. Zhao, M.-S. Alouini, P. Dobbins, A Survey of Channel Modeling for UAV Communications, IEEE Communications Surveys & Tutorials 20 (4) (2018) 2804\u20132821. [36] L. Gupta, R. Jain, G. Vaszkun, Survey of Important Issues in UAV Communication Networks, IEEE Communications Surveys & Tutorials 18 (2) (2015) 1123\u20131152. [37] C. Yan, L. Fu, J. Zhang, J. Wang, A Comprehensive Survey on UAV Communication Channel Modeling, IEEE Access 7 (2019) 107769\u2013 107792. [38] M. M. Azari, F. Rosas, S. Pollin, Cellular Connectivity for UAVs: Network Modeling, Performance Analysis, and Design Guidelines, IEEE Transactions on Wireless Communications 18 (7) (2019) 3366\u20133381. [39] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil, N. S. Othman, A. Khreishah, M. Guizani, Unmanned Aerial Vehicles (UAVs): A Survey on Civil Applications and Key Research Challenges, IEEE Access 7 (2019) 48572\u201348634. [40] W. Khawaja, I. Guvenc, D. W. Matolak, U.-C. Fiebig, N. Schneckenberger, A Survey of Air-to-Ground Propagation Channel Modeling for Unmanned Aerial Vehicles, IEEE Communications Surveys & Tutorials (2019). [41] N. H. Motlagh, T. Taleb, O. Arouk, Low-Altitude Unmanned Aerial Vehicles-Based Internet of Things Services: Comprehensive Survey and Future Perspectives, IEEE Internet of Things Journal 3 (6) (2016) 899\u2013922. [42] M. M. Azari, F. Rosas, S. Pollin, Reshaping Cellular Networks for the Sky: Major Factors and Feasibility, IEEE International Conference on Communications (ICC) (2018) 1\u20137. [43] G. Geraci, A. Garcia-Rodriguez, M. Hassan, M. Ding, UAV Cellular Communications: Practical Insights and Future Vision (2018). [44] H. Wang, J. Wang, J. Chen, Y. Gong, G. Ding, Network-connected UAV communications: Potentials and challenges, China Communications 15 (12) (2018) 111\u2013121. [45] O. Alvear, N. R. Zema, E. Natalizio, C. T. Calafate, Using UAVBased Systems to Monitor Air Pollution in Areas with Poor Accessibility, Journal of Advanced Transportation 2017 (2017). [46] O. Alvear, C. Calafate, N. Zema, et al., A Discretized Approach to Air Pollution Monitoring Using UAV-based Sensing, Mobile Network Applications 23 (2018) 1693\u20131702. [47] H. Hildmann, E. Kovacs, Using Unmanned Aerial Vehicles (UAVs) as Mobile Sensing Platforms (MSPs) for Disaster Response, Civil Security and Public Safety, Drones 3 (3) (2019) 59. [48] M. Erdelj, O. Saif, E. Natalizio, I. Fantoni, UAVs that fly forever: Uninterrupted structural inspection through automatic UAV replaceD. Mishra et al.: Preprint submitted to Elsevier Page 29 of 32 A Survey on Cellular-connected UAVs: Design Challenges, Enabling 5G/B5G Innovations, and Experimental Advancements ment, Ad Hoc Networks 94 (2019) 101612. [49] A. Trotta, F. D. Andreagiovanni, M. Di Felice, E. Natalizio, K. R. Chowdhury, When UAVs Ride A Bus: Towards Energy-..",
              "url": "https://openalex.org/W3047778227",
              "openalex_id": "https://openalex.org/W3047778227",
              "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
              "publication_date": "2020-08-07"
            }
          ]
        },
        "S8654834815": {
          "id": "S8654834815",
          "text": "The regulatory landscape for UAVs is rapidly evolving, with countries like the U.S. and EU actively developing frameworks to address privacy concerns and operational safety, particularly for Beyond Visual Line of Sight (BVLOS) flights.",
          "children": [
            {
              "id": "E0679455280",
              "text": "..scheduled to be fully enforced on July 1 of 2020. These regulations cover the design, manufacture, and operation of drones, and has implications to manufacturers, importers, and distributors. In addition, the new regulations clarify on the technical requirements of drones in different classes. Another recent popular topic in the drone industry is the so-called Beyond Visual Line of Sight (BVLOS) flights, which can cover larger areas (including areas that are difficult or impossible for the pilots to keep an eye on). BVLOS flights can be deployed in adversarial and rough conditions, such as battlefields, inspection of key installations (e.g., oil and gas pipelines, power grids, and border control) and wild life, and search and rescue operations. However, there are also greater risky or ill-intentioned use in the operating of BVLOS flights, which may explain why they are generally not allowed in many countries. For example, the U.S. does not allow BVLOS flights, without a waiver from the relevant authority4. However, regulations on BVLOS flights are evolving at a very fast pace. An amendment5to EU2019/947 w.r.t. BVLOS flights is currently in progress, at the time of writing. We expect that the BVLOS flights will become better regulated across different countries in the near future, and it is an important aspect to consider when we study privacy preservation issues for drones. Standards bodies have also been very proactive in dronerelated activities. For example, the technical specification TS 22.125 of 3GPP6\u201cidentifies the requirements for operation of UAVs via the 3GPP system\u201d. The 3GPP Release 16 includes \u201crequirements for meeting the business, security, and public safety needs for the remote identification and tracking of Unmanned Aerial System (UAS) linked to a 3GPP subscription\u201d. In the 3GPP Release 17 (scheduled for delivery in 2021), it includes 5G enhancement for UAVs. As observed by Stocker et al. [13], an increase in drone \u0308 activities will also result in additional administrative processes, such as those relating to flight registration and approval. We believe that the decentralisation of blockchain systems is a viable approach to reducing administrative redtapes. ID of drones has been one of the main artefacts to ensure traceability and accountability. In the new EU rules, with the exception of class C0 (less than 250g), all classes must bear a unique physical serial number and more importantly a direct remote ID \u201callowing the upload of the operator registration 2Council of European Union, \u201cCommission Delegated Regulation (EU)2019/945 on unmanned aircraft systems and on third-country operators of unmanned aircraft systems,\u201d 2019, https://eur-lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:32019R0945. 3Council of European Union, \u201cCommission Delegated Regulation (EU) 2019/947 on the rules and procedures for the operation of unmanned aircraft,\u201d 2019, https://eur-lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:32019R0947. 4 It is reportedly very difficult to obtain such a waiver in the U.S.. For example, as of June 8, 2020, only 54 BVLOS (107.31) waivers have been issued, according to https://www.faa.gov/uas/commercial operators/part 107 waivers/ 5https://www.easa.europa.eu/sites/default/files/dfu/NPA%202020-07.pdf 6TS 22.125 Unmanned Aerial System (UAS) support in 3GPP, 2019 https: //www.3gpp.org/uas-uav 6 number and in real time during the whole duration of the flight, the direct periodic broadcast from the UA (unmanned aircraft) using an open and documented transmission protocol in a way that they can be received directly by existing mobile devices within the broadcasting range\u201d. On the other hand, TS 22.125 of 3GPP further elaborates that \u201cThe 3GPP system shall enable UAV to preserve the privacy of the owner of the UAV, UAV pilot, and the UAV operator in its broadcast of identity information\u201d. From these recent developments in regulations, it is clear that auditability and anonymity features due to the use of blockchain can facilitate traceability and accountability of drones. On data privacy protection, we can look at two aspects. First, the privacy of people, environment, and objects that may be intruded by drones, and second, the protection of legitimate data collected by drones and the communication privacy between the drone and the pilot. As mentioned earlier, the first aspect has been the focus of recent legislative changes. However, in practice it can be difficult to enforce. The sensory range of onboard sensors is constantly improving due to technology advancement. This compounds the challenge of tracking and identification, especially for smaller drones. The direct remote ID and the geo-awareness system required by EU2019/945 for drones in some classes are helpful in this aspect, so further exploration is necessary. The second aspect is also partially covered by EU2019/945, since drones in some classes are required to \u201cbe equipped with a data link protected against unauthorised access to the command and control functions\u201d and TS 22.125 of 3GPP states that \u201c3GPP system shall support the capability to provide different levels of integrity and privacy protection for the different connections between UAS and UTM (UAS Traffic Management) as well as the data being transferred via those connections\u201d. Table II presents a brief summary of regulations by selected representative countries7, in terms of privacy preservation. Recall that the new EU regulations EU2019/945 and EU2019/947 will be fully enforced on July 1 of 2020. Therefore, existing national regulations are in the process of being harmonised with or superseded by the new EU rules. The communication privacy on drones are largely not mentioned by the national regulations listed in Table II. In other words, the new EU regulations and the 3GPP standards are more advanced in this aspect. VI. RESEARCH CHALLENGES AND OPEN ISSUES Despite the potential benefits of blockchain in drone communication privacy preservation, there remain a number of open challenges which will be discussed next. \u2022 Resource constraints of drones: Most existing drones are resource-limited, in terms of energy, size and weight considerations. Encryption and/or consensus algorithms are generally required for blockchain systems, yet drones are generally incapable of computing-intensive tasks due to 7These countries are selected because they are representative of the most advanced development in drone regulations from different continents. Stocker \u0308 et al. [13] also studied these countries in their comparative analysis, with the exception of the new EU regulations EU2019/945 and EU2019/947. computationally constraints and battery life. In addition, a swarm of UAVs can generate and/or collect gigabytes of data per second, including both audio and video. Whether the storage capacity of blockchain can accommodate such high volume of data is still debatable, and whether and how to incorporate other storage resources (e.g., edge servers) with the UAV system remains an open challenge. Apart from these, drones are energy constrained devices, and thus they need energy efficient solutions. However, miners (i.e., drones) consume a disproportionate amount of electricity when generating blocks; thus, existing drones may not be capable of supporting sufficient energy required for mining of blocks. In the future, the orchestration of various computing facilities such as remote clouds, nearby edge servers and drones, and other technologies such as network coding, becomes a necessity to implement blockchain-based drone communications. \u2022 Full privacy preservation of drone data: In blockchainbased solutions for drone networks, each drone requires to store a copy of the data blocks (i.e., distributed ledger). This risks the dissemination of sensitive information to all participating drones. Although blockchain can guarantee certain level of privacy preservation of drone data, activities of both users of drone communications and drones can be inferred (or extracted) via statistical analysis or using other machine learning tools. For example, user private data relayed through drones may be leaked to malicious users who may compromise the drones with the aim of exfiltrating data. How to fully ensure data privacy of drone communications is still an open research question. Limiting the information sharing between drones is one potential solution, although this may not be practical in some applications. \u2022 Scalability of blockchain-based drone networks: Multiple drones can form a drone network for diverse tasks. As discussed earlier, the consensus of drone networks can help to mitigate the falsification of malicious drones and other security risks. However, it is challenging to achieve a scalable blockchain-based drone network due to the dynamics of drones (i.e., drones can join and leave at any time) as well as the scalability constraints of current blockchain systems (i.e., low throughput of transactions per second). For example, poor scalability may lead to the difficulty of forming a drone network and reaching a consensus when a new drone joins. Therefore, scalability of blockchain-based drone networks is an important issue to explore in the future. \u2022 Remote identification: As mentioned earlier, new regulations require drones to periodically broadcast their ID information that can be directly received by existing mobile devices within the broadcasting range. Such an activity needs to be conducted without violating the privacy of the owner, the pilot, and the operator. Designing an efficient solution for remote ID requires an in-depth understanding of the data transmission protocols and the various security and privacy risks (including emerging risks), and hence remains one of ongoing interest. \u2022 Regulation development and compliance enforcement: 7 TABLE II COMPARISON OF DRONE-RELATED REGULATIONS IN TERMS OF PRIVACY PRESERVATION Country Data Privacy on Drones Communication Privacy on Drones EU 2019/945 EU 2019/947 Legally regulated Drones in some classes required to be equipped with secure data link Australia Only advice to respect private privacy Privacy Act only applies on large organisations Authority plans to review privacy issues with recreational drones N/A Canada Privacy Act applies to commercial and government drones N/A China Not in national laws but covered by some provincial laws (e.g...",
              "url": "https://openalex.org/W3130357311",
              "openalex_id": "https://openalex.org/W3130357311",
              "title": "Blockchain-Based Privacy Preservation for 5G-Enabled Drone Communications",
              "publication_date": "2021-01-01"
            },
            {
              "id": "E7130157629",
              "text": "..blockchain-based drone communications. \u2022 Full privacy preservation of drone data: In blockchainbased solutions for drone networks, each drone requires to store a copy of the data blocks (i.e., distributed ledger). This risks the dissemination of sensitive information to all participating drones. Although blockchain can guarantee certain level of privacy preservation of drone data, activities of both users of drone communications and drones can be inferred (or extracted) via statistical analysis or using other machine learning tools. For example, user private data relayed through drones may be leaked to malicious users who may compromise the drones with the aim of exfiltrating data. How to fully ensure data privacy of drone communications is still an open research question. Limiting the information sharing between drones is one potential solution, although this may not be practical in some applications. \u2022 Scalability of blockchain-based drone networks: Multiple drones can form a drone network for diverse tasks. As discussed earlier, the consensus of drone networks can help to mitigate the falsification of malicious drones and other security risks. However, it is challenging to achieve a scalable blockchain-based drone network due to the dynamics of drones (i.e., drones can join and leave at any time) as well as the scalability constraints of current blockchain systems (i.e., low throughput of transactions per second). For example, poor scalability may lead to the difficulty of forming a drone network and reaching a consensus when a new drone joins. Therefore, scalability of blockchain-based drone networks is an important issue to explore in the future. \u2022 Remote identification: As mentioned earlier, new regulations require drones to periodically broadcast their ID information that can be directly received by existing mobile devices within the broadcasting range. Such an activity needs to be conducted without violating the privacy of the owner, the pilot, and the operator. Designing an efficient solution for remote ID requires an in-depth understanding of the data transmission protocols and the various security and privacy risks (including emerging risks), and hence remains one of ongoing interest. \u2022 Regulation development and compliance enforcement: 7 TABLE II COMPARISON OF DRONE-RELATED REGULATIONS IN TERMS OF PRIVACY PRESERVATION Country Data Privacy on Drones Communication Privacy on Drones EU 2019/945 EU 2019/947 Legally regulated Drones in some classes required to be equipped with secure data link Australia Only advice to respect private privacy Privacy Act only applies on large organisations Authority plans to review privacy issues with recreational drones N/A Canada Privacy Act applies to commercial and government drones N/A China Not in national laws but covered by some provincial laws (e.g., Sichuan) N/A Colombia Not allowed to violate the rights of privacy N/A France Operators obliged to respect privacy rights of individuals Germany Bundesdatenschutzgesetz (BDSG, federal data protection act) applies N/A Italy Italian Data Protection Code, enacting GDPR, applies N/A Japan Not linked to the Act on the Protection of Personal Information (APPI) but authority plans to cover privacy in next phase in the roadmap N/A Rwanda Operators oblighed to respect privacy rights of others surveillance of people and property without their consent is prohibited N/A The Netherlands Operators not allowed to violate other people\u2019s privacy N/A United Kingdom The Data Protection Act (DPA) applies N/A United States Covered differently by State- or City-level laws N/A Regulations and standards for drones are still evolving, and privacy preservation remains an prioritised agenda. Drone accidents may occur due to a range of reasons, such as technical malfunction, improper operations, unforeseen environmental events (e.g., sudden wind gusts), and hijacking. As more automation functionalities are being introduced into drones, clear definitions of liabilities and responsibilities for all participants involved across the entire life cycle of a drone will need to be explored. A closely related issue is how to enforce the compliance of regulations for drones. The collection and certification of digital evidence (enabled by blockchain) on drone accidents or privacy intrusions / breaches are also potential research topics. VII. CONCLUSION This article discussed blockchain-based privacy preservation solutions for 5G-enabled drone communications, as well as related data privacy legislation and regulations that need to be considered in the design of these solutions. We also identified potential challenges and open issues to inform future research agenda that will allow the community to leverage blockchain to facilitate privacy preservation in drone communications. REFERENCES [1] D. Tezza and M. Andujar, \u201cThe state-of-the-art of human\u2013drone interaction: A survey,\u201d IEEE Access, vol. 7, pp. 167 438\u2013167 454, 2019. [2] G. Yang, X. Shi, L. Feng, S. He, Z. Shi, and J. Chen, \u201cCedar: A costeffective crowdsensing system for detecting and localizing drones,\u201d IEEE Transactions on Mobile Computing, pp. 1\u20131, 2019. [3] Y. Zeng, Q. Wu, and R. Zhang, \u201cAccessing From the Sky: A Tutorial on UAV Communications for 5G and Beyond,\u201d Proceedings of the IEEE, vol. 107, no. 12, pp. 2327\u20132375, Dec 2019. [4] C. Lin, D. He, N. Kumar, K.-K. R. Choo, A. Vinel, and X. Huang, \u201csecurity and privacy for the internet of drones: Challenges and solutions,\u201d IEEE Communications Magazine. [5] G. D. L. T. Parra, P. Rad, and K. R. Choo, \u201cDriverless vehicle security: Challenges and future research opportunities,\u201d Future Gener. Comput. Syst., vol. 108, pp. 1092\u20131111, 2020. [6] L. Cheng, J. Liu, G. Xu, Z. Zhang, H. Wang, H. Dai, Y. Wu, and W. Wang, \u201cSCTSC: A Semicentralized Traffic Signal Control Mode With Attribute-Based Blockchain in IoVs,\u201d IEEE Transactions on Computational Social Systems, vol. 6, no. 6, pp. 1373\u20131385, Dec 2019. [7] W. Zou, D. Lo, P. S. Kochhar, X. D. Le, X. Xia, Y. Feng, Z. Chen, and B. Xu, \u201cSmart contract development: Challenges and opportunities,\u201d IEEE Transactions on Software Engineering, pp. 1\u20131, 2019. [8] B. Cao, Y. Li, L. Zhang, L. Zhang, S. Mumtaz, Z. Zhou, and M. Peng, \u201cWhen Internet of Things Meets Blockchain: Challenges in Distributed Consensus,\u201d IEEE Network, vol. 33, no. 6, pp. 133\u2013139, 2019. [9] Q. Wu, W. Mei, and R. Zhang, \u201cSafeguarding wireless network with uavs: A physical layer security perspective,\u201d IEEE Wireless Commun., vol. 26, no. 5, pp. 12\u201318, 2019. [10] Y. Qian, Y. Jiang, L. Hu, M. S. Hossain, M. Alrashoud, and M. AlHammadi, \u201cBlockchain-based privacy-aware content caching in cognitive internet of vehicles,\u201d IEEE Network, vol. 34, no. 2, pp. 46\u201351, 2020. [11] E. Yanmaz, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter, \u201cDrone networks: Communications, coordination, and sensing,\u201d Ad Hoc Networks, vol. 68, pp. 1 \u2013 15, 2018. [12] H.-N. Dai, Z. Zheng, and Y. Zhang, \u201cBlockchain for internet of things: A survey,\u201d IEEE Internet of Things Journal, vol. 6, no. 5, pp. 8076\u20138094, Oct 2019. [13] C. Stocker, R. Bennett, F. Nex, M. Gerke, and J. Zevenbergen, \u201cReview \u0308 of the current state of UAV regulations,\u201d Remote sensing, vol. 9, no. 5, p. 459, 2017. [14] A. Fotouhi, H. Qiang, M. Ding, M. Hassan, L. G. Giordano, A. GarciaRodriguez, and J. Yuan, \u201cSurvey on UAV cellular communications: Practical aspects, standardization advancements, regulation, and security challenges,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 4, pp. 3417\u20133442, 2019. [15] Z. Ullah, F. Al-Turjman, and L. Mostarda, \u201cCognition in UAV-Aided 5G and Beyond Communications: A Survey,\u201d IEEE Transactions on Cognitive Communications and Networking, 2020. Yulei Wu [SM\u201918] is a Senior Lecturer with the Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, United Kingdom. He received the B.Sc. degree (First Class Honours) in Computer Science and the Ph.D. degree in Computing and Mathematics from the University of Bradford, United Kingdom, in 2006 and 2010, respectively. His expertise is on networking and his main research interests include computer networks, networked systems, software defined networks and systems, network management, and network security and privacy. Dr. Wu contributes to major conferences on networking and networked systems as various roles, including the Steering Committee Chair, the General Chair and the Program Chair. His research has been supported by Engineering and Physical Sciences Research Council of United Kingdom, National Natural 8 Science Foundation of China, University\u2019s Innovation Platform and industry. He is an Editor of IEEE Transactions on Network and Service Management, Computer Networks (Elsevier) and IEEE Access. He is a Fellow of the HEA (Higher Education Academy). Hong-Ning Dai [SM\u201916] is currently with Faculty of Information Technology at Macau University of Science and Technology as an associate professor. He obtained the Ph.D. degree in Computer Science and Engineering from Department of Computer..",
              "url": "https://openalex.org/W3130357311",
              "openalex_id": "https://openalex.org/W3130357311",
              "title": "Blockchain-Based Privacy Preservation for 5G-Enabled Drone Communications",
              "publication_date": "2021-01-01"
            }
          ]
        },
        "S7763458077": {
          "id": "S7763458077",
          "text": "Case studies on the implementation of remote identification protocols for UAVs in various jurisdictions reveal significant challenges in balancing operational efficiency with privacy concerns, necessitating further research into effective compliance mechanisms.",
          "children": [
            {
              "id": "E0679455280",
              "text": "..scheduled to be fully enforced on July 1 of 2020. These regulations cover the design, manufacture, and operation of drones, and has implications to manufacturers, importers, and distributors. In addition, the new regulations clarify on the technical requirements of drones in different classes. Another recent popular topic in the drone industry is the so-called Beyond Visual Line of Sight (BVLOS) flights, which can cover larger areas (including areas that are difficult or impossible for the pilots to keep an eye on). BVLOS flights can be deployed in adversarial and rough conditions, such as battlefields, inspection of key installations (e.g., oil and gas pipelines, power grids, and border control) and wild life, and search and rescue operations. However, there are also greater risky or ill-intentioned use in the operating of BVLOS flights, which may explain why they are generally not allowed in many countries. For example, the U.S. does not allow BVLOS flights, without a waiver from the relevant authority4. However, regulations on BVLOS flights are evolving at a very fast pace. An amendment5to EU2019/947 w.r.t. BVLOS flights is currently in progress, at the time of writing. We expect that the BVLOS flights will become better regulated across different countries in the near future, and it is an important aspect to consider when we study privacy preservation issues for drones. Standards bodies have also been very proactive in dronerelated activities. For example, the technical specification TS 22.125 of 3GPP6\u201cidentifies the requirements for operation of UAVs via the 3GPP system\u201d. The 3GPP Release 16 includes \u201crequirements for meeting the business, security, and public safety needs for the remote identification and tracking of Unmanned Aerial System (UAS) linked to a 3GPP subscription\u201d. In the 3GPP Release 17 (scheduled for delivery in 2021), it includes 5G enhancement for UAVs. As observed by Stocker et al. [13], an increase in drone \u0308 activities will also result in additional administrative processes, such as those relating to flight registration and approval. We believe that the decentralisation of blockchain systems is a viable approach to reducing administrative redtapes. ID of drones has been one of the main artefacts to ensure traceability and accountability. In the new EU rules, with the exception of class C0 (less than 250g), all classes must bear a unique physical serial number and more importantly a direct remote ID \u201callowing the upload of the operator registration 2Council of European Union, \u201cCommission Delegated Regulation (EU)2019/945 on unmanned aircraft systems and on third-country operators of unmanned aircraft systems,\u201d 2019, https://eur-lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:32019R0945. 3Council of European Union, \u201cCommission Delegated Regulation (EU) 2019/947 on the rules and procedures for the operation of unmanned aircraft,\u201d 2019, https://eur-lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:32019R0947. 4 It is reportedly very difficult to obtain such a waiver in the U.S.. For example, as of June 8, 2020, only 54 BVLOS (107.31) waivers have been issued, according to https://www.faa.gov/uas/commercial operators/part 107 waivers/ 5https://www.easa.europa.eu/sites/default/files/dfu/NPA%202020-07.pdf 6TS 22.125 Unmanned Aerial System (UAS) support in 3GPP, 2019 https: //www.3gpp.org/uas-uav 6 number and in real time during the whole duration of the flight, the direct periodic broadcast from the UA (unmanned aircraft) using an open and documented transmission protocol in a way that they can be received directly by existing mobile devices within the broadcasting range\u201d. On the other hand, TS 22.125 of 3GPP further elaborates that \u201cThe 3GPP system shall enable UAV to preserve the privacy of the owner of the UAV, UAV pilot, and the UAV operator in its broadcast of identity information\u201d. From these recent developments in regulations, it is clear that auditability and anonymity features due to the use of blockchain can facilitate traceability and accountability of drones. On data privacy protection, we can look at two aspects. First, the privacy of people, environment, and objects that may be intruded by drones, and second, the protection of legitimate data collected by drones and the communication privacy between the drone and the pilot. As mentioned earlier, the first aspect has been the focus of recent legislative changes. However, in practice it can be difficult to enforce. The sensory range of onboard sensors is constantly improving due to technology advancement. This compounds the challenge of tracking and identification, especially for smaller drones. The direct remote ID and the geo-awareness system required by EU2019/945 for drones in some classes are helpful in this aspect, so further exploration is necessary. The second aspect is also partially covered by EU2019/945, since drones in some classes are required to \u201cbe equipped with a data link protected against unauthorised access to the command and control functions\u201d and TS 22.125 of 3GPP states that \u201c3GPP system shall support the capability to provide different levels of integrity and privacy protection for the different connections between UAS and UTM (UAS Traffic Management) as well as the data being transferred via those connections\u201d. Table II presents a brief summary of regulations by selected representative countries7, in terms of privacy preservation. Recall that the new EU regulations EU2019/945 and EU2019/947 will be fully enforced on July 1 of 2020. Therefore, existing national regulations are in the process of being harmonised with or superseded by the new EU rules. The communication privacy on drones are largely not mentioned by the national regulations listed in Table II. In other words, the new EU regulations and the 3GPP standards are more advanced in this aspect. VI. RESEARCH CHALLENGES AND OPEN ISSUES Despite the potential benefits of blockchain in drone communication privacy preservation, there remain a number of open challenges which will be discussed next. \u2022 Resource constraints of drones: Most existing drones are resource-limited, in terms of energy, size and weight considerations. Encryption and/or consensus algorithms are generally required for blockchain systems, yet drones are generally incapable of computing-intensive tasks due to 7These countries are selected because they are representative of the most advanced development in drone regulations from different continents. Stocker \u0308 et al. [13] also studied these countries in their comparative analysis, with the exception of the new EU regulations EU2019/945 and EU2019/947. computationally constraints and battery life. In addition, a swarm of UAVs can generate and/or collect gigabytes of data per second, including both audio and video. Whether the storage capacity of blockchain can accommodate such high volume of data is still debatable, and whether and how to incorporate other storage resources (e.g., edge servers) with the UAV system remains an open challenge. Apart from these, drones are energy constrained devices, and thus they need energy efficient solutions. However, miners (i.e., drones) consume a disproportionate amount of electricity when generating blocks; thus, existing drones may not be capable of supporting sufficient energy required for mining of blocks. In the future, the orchestration of various computing facilities such as remote clouds, nearby edge servers and drones, and other technologies such as network coding, becomes a necessity to implement blockchain-based drone communications. \u2022 Full privacy preservation of drone data: In blockchainbased solutions for drone networks, each drone requires to store a copy of the data blocks (i.e., distributed ledger). This risks the dissemination of sensitive information to all participating drones. Although blockchain can guarantee certain level of privacy preservation of drone data, activities of both users of drone communications and drones can be inferred (or extracted) via statistical analysis or using other machine learning tools. For example, user private data relayed through drones may be leaked to malicious users who may compromise the drones with the aim of exfiltrating data. How to fully ensure data privacy of drone communications is still an open research question. Limiting the information sharing between drones is one potential solution, although this may not be practical in some applications. \u2022 Scalability of blockchain-based drone networks: Multiple drones can form a drone network for diverse tasks. As discussed earlier, the consensus of drone networks can help to mitigate the falsification of malicious drones and other security risks. However, it is challenging to achieve a scalable blockchain-based drone network due to the dynamics of drones (i.e., drones can join and leave at any time) as well as the scalability constraints of current blockchain systems (i.e., low throughput of transactions per second). For example, poor scalability may lead to the difficulty of forming a drone network and reaching a consensus when a new drone joins. Therefore, scalability of blockchain-based drone networks is an important issue to explore in the future. \u2022 Remote identification: As mentioned earlier, new regulations require drones to periodically broadcast their ID information that can be directly received by existing mobile devices within the broadcasting range. Such an activity needs to be conducted without violating the privacy of the owner, the pilot, and the operator. Designing an efficient solution for remote ID requires an in-depth understanding of the data transmission protocols and the various security and privacy risks (including emerging risks), and hence remains one of ongoing interest. \u2022 Regulation development and compliance enforcement: 7 TABLE II COMPARISON OF DRONE-RELATED REGULATIONS IN TERMS OF PRIVACY PRESERVATION Country Data Privacy on Drones Communication Privacy on Drones EU 2019/945 EU 2019/947 Legally regulated Drones in some classes required to be equipped with secure data link Australia Only advice to respect private privacy Privacy Act only applies on large organisations Authority plans to review privacy issues with recreational drones N/A Canada Privacy Act applies to commercial and government drones N/A China Not in national laws but covered by some provincial laws (e.g...",
              "url": "https://openalex.org/W3130357311",
              "openalex_id": "https://openalex.org/W3130357311",
              "title": "Blockchain-Based Privacy Preservation for 5G-Enabled Drone Communications",
              "publication_date": "2021-01-01"
            },
            {
              "id": "E7130157629",
              "text": "..blockchain-based drone communications. \u2022 Full privacy preservation of drone data: In blockchainbased solutions for drone networks, each drone requires to store a copy of the data blocks (i.e., distributed ledger). This risks the dissemination of sensitive information to all participating drones. Although blockchain can guarantee certain level of privacy preservation of drone data, activities of both users of drone communications and drones can be inferred (or extracted) via statistical analysis or using other machine learning tools. For example, user private data relayed through drones may be leaked to malicious users who may compromise the drones with the aim of exfiltrating data. How to fully ensure data privacy of drone communications is still an open research question. Limiting the information sharing between drones is one potential solution, although this may not be practical in some applications. \u2022 Scalability of blockchain-based drone networks: Multiple drones can form a drone network for diverse tasks. As discussed earlier, the consensus of drone networks can help to mitigate the falsification of malicious drones and other security risks. However, it is challenging to achieve a scalable blockchain-based drone network due to the dynamics of drones (i.e., drones can join and leave at any time) as well as the scalability constraints of current blockchain systems (i.e., low throughput of transactions per second). For example, poor scalability may lead to the difficulty of forming a drone network and reaching a consensus when a new drone joins. Therefore, scalability of blockchain-based drone networks is an important issue to explore in the future. \u2022 Remote identification: As mentioned earlier, new regulations require drones to periodically broadcast their ID information that can be directly received by existing mobile devices within the broadcasting range. Such an activity needs to be conducted without violating the privacy of the owner, the pilot, and the operator. Designing an efficient solution for remote ID requires an in-depth understanding of the data transmission protocols and the various security and privacy risks (including emerging risks), and hence remains one of ongoing interest. \u2022 Regulation development and compliance enforcement: 7 TABLE II COMPARISON OF DRONE-RELATED REGULATIONS IN TERMS OF PRIVACY PRESERVATION Country Data Privacy on Drones Communication Privacy on Drones EU 2019/945 EU 2019/947 Legally regulated Drones in some classes required to be equipped with secure data link Australia Only advice to respect private privacy Privacy Act only applies on large organisations Authority plans to review privacy issues with recreational drones N/A Canada Privacy Act applies to commercial and government drones N/A China Not in national laws but covered by some provincial laws (e.g., Sichuan) N/A Colombia Not allowed to violate the rights of privacy N/A France Operators obliged to respect privacy rights of individuals Germany Bundesdatenschutzgesetz (BDSG, federal data protection act) applies N/A Italy Italian Data Protection Code, enacting GDPR, applies N/A Japan Not linked to the Act on the Protection of Personal Information (APPI) but authority plans to cover privacy in next phase in the roadmap N/A Rwanda Operators oblighed to respect privacy rights of others surveillance of people and property without their consent is prohibited N/A The Netherlands Operators not allowed to violate other people\u2019s privacy N/A United Kingdom The Data Protection Act (DPA) applies N/A United States Covered differently by State- or City-level laws N/A Regulations and standards for drones are still evolving, and privacy preservation remains an prioritised agenda. Drone accidents may occur due to a range of reasons, such as technical malfunction, improper operations, unforeseen environmental events (e.g., sudden wind gusts), and hijacking. As more automation functionalities are being introduced into drones, clear definitions of liabilities and responsibilities for all participants involved across the entire life cycle of a drone will need to be explored. A closely related issue is how to enforce the compliance of regulations for drones. The collection and certification of digital evidence (enabled by blockchain) on drone accidents or privacy intrusions / breaches are also potential research topics. VII. CONCLUSION This article discussed blockchain-based privacy preservation solutions for 5G-enabled drone communications, as well as related data privacy legislation and regulations that need to be considered in the design of these solutions. We also identified potential challenges and open issues to inform future research agenda that will allow the community to leverage blockchain to facilitate privacy preservation in drone communications. REFERENCES [1] D. Tezza and M. Andujar, \u201cThe state-of-the-art of human\u2013drone interaction: A survey,\u201d IEEE Access, vol. 7, pp. 167 438\u2013167 454, 2019. [2] G. Yang, X. Shi, L. Feng, S. He, Z. Shi, and J. Chen, \u201cCedar: A costeffective crowdsensing system for detecting and localizing drones,\u201d IEEE Transactions on Mobile Computing, pp. 1\u20131, 2019. [3] Y. Zeng, Q. Wu, and R. Zhang, \u201cAccessing From the Sky: A Tutorial on UAV Communications for 5G and Beyond,\u201d Proceedings of the IEEE, vol. 107, no. 12, pp. 2327\u20132375, Dec 2019. [4] C. Lin, D. He, N. Kumar, K.-K. R. Choo, A. Vinel, and X. Huang, \u201csecurity and privacy for the internet of drones: Challenges and solutions,\u201d IEEE Communications Magazine. [5] G. D. L. T. Parra, P. Rad, and K. R. Choo, \u201cDriverless vehicle security: Challenges and future research opportunities,\u201d Future Gener. Comput. Syst., vol. 108, pp. 1092\u20131111, 2020. [6] L. Cheng, J. Liu, G. Xu, Z. Zhang, H. Wang, H. Dai, Y. Wu, and W. Wang, \u201cSCTSC: A Semicentralized Traffic Signal Control Mode With Attribute-Based Blockchain in IoVs,\u201d IEEE Transactions on Computational Social Systems, vol. 6, no. 6, pp. 1373\u20131385, Dec 2019. [7] W. Zou, D. Lo, P. S. Kochhar, X. D. Le, X. Xia, Y. Feng, Z. Chen, and B. Xu, \u201cSmart contract development: Challenges and opportunities,\u201d IEEE Transactions on Software Engineering, pp. 1\u20131, 2019. [8] B. Cao, Y. Li, L. Zhang, L. Zhang, S. Mumtaz, Z. Zhou, and M. Peng, \u201cWhen Internet of Things Meets Blockchain: Challenges in Distributed Consensus,\u201d IEEE Network, vol. 33, no. 6, pp. 133\u2013139, 2019. [9] Q. Wu, W. Mei, and R. Zhang, \u201cSafeguarding wireless network with uavs: A physical layer security perspective,\u201d IEEE Wireless Commun., vol. 26, no. 5, pp. 12\u201318, 2019. [10] Y. Qian, Y. Jiang, L. Hu, M. S. Hossain, M. Alrashoud, and M. AlHammadi, \u201cBlockchain-based privacy-aware content caching in cognitive internet of vehicles,\u201d IEEE Network, vol. 34, no. 2, pp. 46\u201351, 2020. [11] E. Yanmaz, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter, \u201cDrone networks: Communications, coordination, and sensing,\u201d Ad Hoc Networks, vol. 68, pp. 1 \u2013 15, 2018. [12] H.-N. Dai, Z. Zheng, and Y. Zhang, \u201cBlockchain for internet of things: A survey,\u201d IEEE Internet of Things Journal, vol. 6, no. 5, pp. 8076\u20138094, Oct 2019. [13] C. Stocker, R. Bennett, F. Nex, M. Gerke, and J. Zevenbergen, \u201cReview \u0308 of the current state of UAV regulations,\u201d Remote sensing, vol. 9, no. 5, p. 459, 2017. [14] A. Fotouhi, H. Qiang, M. Ding, M. Hassan, L. G. Giordano, A. GarciaRodriguez, and J. Yuan, \u201cSurvey on UAV cellular communications: Practical aspects, standardization advancements, regulation, and security challenges,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 4, pp. 3417\u20133442, 2019. [15] Z. Ullah, F. Al-Turjman, and L. Mostarda, \u201cCognition in UAV-Aided 5G and Beyond Communications: A Survey,\u201d IEEE Transactions on Cognitive Communications and Networking, 2020. Yulei Wu [SM\u201918] is a Senior Lecturer with the Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, United Kingdom. He received the B.Sc. degree (First Class Honours) in Computer Science and the Ph.D. degree in Computing and Mathematics from the University of Bradford, United Kingdom, in 2006 and 2010, respectively. His expertise is on networking and his main research interests include computer networks, networked systems, software defined networks and systems, network management, and network security and privacy. Dr. Wu contributes to major conferences on networking and networked systems as various roles, including the Steering Committee Chair, the General Chair and the Program Chair. His research has been supported by Engineering and Physical Sciences Research Council of United Kingdom, National Natural 8 Science Foundation of China, University\u2019s Innovation Platform and industry. He is an Editor of IEEE Transactions on Network and Service Management, Computer Networks (Elsevier) and IEEE Access. He is a Fellow of the HEA (Higher Education Academy). Hong-Ning Dai [SM\u201916] is currently with Faculty of Information Technology at Macau University of Science and Technology as an associate professor. He obtained the Ph.D. degree in Computer Science and Engineering from Department of Computer..",
              "url": "https://openalex.org/W3130357311",
              "openalex_id": "https://openalex.org/W3130357311",
              "title": "Blockchain-Based Privacy Preservation for 5G-Enabled Drone Communications",
              "publication_date": "2021-01-01"
            }
          ]
        },
        "S2585677171": {
          "id": "S2585677171",
          "text": "Regulatory challenges for UAV integration in 5G networks include the need for standardized communication protocols and security measures to protect against cyber threats, which are critical for ensuring safe and reliable UAV operations.",
          "children": [
            {
              "id": "E4933323753",
              "text": "... 5, pp. 4278\u20134291, 2019. [113] L. Xie, Y. Ding, H. Yang, and X. Wang, \u201cBlockchain-based secure and trustworthy internet of things in sdn-enabled 5g-vanets,\u201d IEEE Access, vol. 7, pp. 56 656\u201356 666, 2019. [114] B. Bera, S. Saha, A. K. Das, N. Kumar, P. Lorenz, and M. Alazab, \u201cBlockchain-envisioned secure data delivery and collection scheme for 5g-based iot-enabled internet of drones environment,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9097\u20139111, 2020. [115] M. Aloqaily, O. Bouachir, A. Boukerche, and I. Al Ridhawi, \u201cDesign guidelines for blockchain-assisted 5g-uav networks,\u201d IEEE Network, vol. 35, no. 1, pp. 64\u201371, 2021. [116] R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain and 5g integrated softwarized uav network management: Architecture, solutions, and challenges,\u201d Physical Communication, vol. 47, p. 101355, 2021. [117] X. Jian, P. Leng, Y. Wang, M. Alrashoud, and M. S. Hossain, \u201cBlockchain-empowered trusted networking for unmanned aerial vehicles in the b5g era,\u201d IEEE Network, vol. 35, no. 1, pp. 72\u201377, 2021. [118] C. Feng, K. Yu, A. K. Bashir, Y. D. Al-Otaibi, Y. Lu, S. Chen, and D. Zhang, \u201cEfficient and secure data sharing for 5g flying drones: a blockchain-enabled approach,\u201d IEEE Network, vol. 35, no. 1, pp. 130\u2013 137, 2021. [119] B. Ghimire, D. B. Rawat, C. Liu, and J. Li, \u201cSharding-enabled blockchain for software-defined internet of unmanned vehicles in the battlefield,\u201d IEEE Network, vol. 35, no. 1, pp. 101\u2013107, 2021. 32 [120] A. Gumaei, M. Al-Rakhami, M. M. Hassan, P. Pace, G. Alai, K. Lin, and G. Fortino, \u201cDeep learning and blockchain with edge computing for 5g-enabled drone identification and flight mode detection,\u201d IEEE Network, vol. 35, no. 1, pp. 94\u2013100, 2021. [121] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, \u201cFederated learning meets blockchain in edge computing: Opportunities and challenges,\u201d IEEE Internet of Things Journal, vol. 8, no. 16, pp. 12 806\u201312 825, 2021. [122] P. Kumar, R. Kumar, G. Srivastava, G. P. Gupta, R. Tripathi, T. R. Gadekallu, and N. Xiong, \u201cPpsf: A privacy-preserving and secure framework using blockchain-based machine-learning for iot-driven smart cities,\u201d IEEE Transactions on Network Science and Engineering, 2021. [123] S. Bouraga, \u201cA taxonomy of blockchain consensus protocols: A survey and classification framework,\u201d Expert Systems with Applications, vol. 168, p. 114384, 2021. [124] V. Ortega, F. Bouchmal, and J. F. Monserrat, \u201cTrusted 5g vehicular networks: Blockchains and content-centric networking,\u201d IEEE Vehicular Technology Magazine, vol. 13, no. 2, pp. 121\u2013127, 2018. [125] R. Shrestha, S. Y. Nam, R. Bajracharya, and S. Kim, \u201cEvolution of v2x communication and integration of blockchain for security enhancements,\u201d Electronics, vol. 9, no. 9, p. 1338, 2020. [126] S. Rahmadika, K. Lee, and K.-H. Rhee, \u201cBlockchain-enabled 5g autonomous vehicular networks,\u201d in 2019 International Conference on Sustainable Engineering and Creative Computing (ICSECC). IEEE, 2019, pp. 275\u2013280. [127] D. Reebadiya, T. Rathod, R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain-based secure and intelligent sensing scheme for autonomous vehicles activity tracking beyond 5g networks,\u201d Peer-to-Peer Networking and Applications, pp. 1\u201318, 2021. [128] L. Nkenyereye, B. Adhi Tama, M. K. Shahzad, and Y.-H. Choi, \u201cSecure and blockchain-based emergency driven message protocol for 5g enabled vehicular edge computing,\u201d Sensors, vol. 20, no. 1, p. 154, 2020. [129] J. Chen, W. Wang, Y. Zhou, S. H. Ahmed, and W. Wei, \u201cExploiting 5g and blockchain for medical applications of drones,\u201d IEEE Network, vol. 35, no. 1, pp. 30\u201336, 2021. [130] G. Kakkavas, A. Stamou, V. Karyotis, and S. Papavassiliou, \u201cNetwork tomography for efficient monitoring in sdn-enabled 5g networks and beyond: Challenges and opportunities,\u201d IEEE Communications Magazine, vol. 59, no. 3, pp. 70\u201376, 2021. [131] C. Benzaid and T. Taleb, \u201cAi-driven zero touch network and service management in 5g and beyond: Challenges and research directions,\u201d IEEE Network, vol. 34, no. 2, pp. 186\u2013194, 2020. [132] M. Liyanage, Q.-V. Pham, K. Dev, S. Bhattacharya, P. K. R. Maddikunta, T. R. Gadekallu, and G. Yenduri, \u201cA survey on zero touch network and service (zsm) management for 5g and beyond networks,\u201d Journal of Network and Computer Applications, p. 103362, 2022. [133] J. Cui, L. S. Liew, G. Sabaliauskaite, and F. Zhou, \u201cA review on safety failures, security attacks, and available countermeasures for autonomous vehicles,\u201d Ad Hoc Networks, vol. 90, p. 101823, 2019. [134] A. M. Malla and R. K. Sahu, \u201cSecurity attacks with an effective solution for dos attacks in vanet,\u201d International Journal of Computer Applications, vol. 66, no. 22, 2013. [135] S. S. Manvi and S. Tangade, \u201cA survey on authentication schemes in vanets for secured communication,\u201d Vehicular Communications, vol. 9, pp. 19\u201330, 2017. [136] L. B. Othmane, H. Weffers, M. M. Mohamad, and M. Wolf, \u201cA survey of security and privacy in connected vehicles,\u201d in Wireless sensor and mobile ad-hoc networks. Springer, 2015, pp. 217\u2013247. [137] S. Zhang, Y. Lin, Q. Liu, J. Jiang, B. Yin, and K.-K. R. Choo, \u201cSecure hitch in location based social networks,\u201d Computer Communications, vol. 100, pp. 65\u201377, 2017. [138] M. Raya and J.-P. Hubaux, \u201cSecuring vehicular ad hoc networks,\u201d Journal of computer security, vol. 15, no. 1, pp. 39\u201368, 2007. [139] J. Petit, B. Stottelaar, M. Feiri, and F. Kargl, \u201cRemote attacks on automated vehicles sensors: Experiments on camera and lidar,\u201d Black Hat Europe, vol. 11, no. 2015, p. 995, 2015. [140] B. G. Stottelaar, \u201cPractical cyber-attacks on autonomous vehicles,\u201d Master\u2019s thesis, University of Twente, 2015. [141] R. Chauhan, A platform for false data injection in frequency modulated continuous wave radar. Utah State University, 2014. [142] W. E. Buehler, R. M. Whitson, and M. J. Lewis, \u201cAirborne radar jamming system,\u201d Sep. 9 2014, uS Patent 8,830,112. [143] N. Fairfield and C. Urmson, \u201cTraffic light mapping and detection,\u201d in 2011 IEEE International Conference on Robotics and Automation. IEEE, 2011, pp. 5421\u20135426. [144] A. B. Hillel, R. Lerner, D. Levi, and G. Raz, \u201cRecent progress in road and lane detection: a survey,\u201d Machine vision and applications, vol. 25, no. 3, pp. 727\u2013745, 2014. [145] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, \u201cPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8445\u20138453. [146] T. B. Brown, D. Mane, A. Roy, M. Abadi, and J. Gilmer, \u201cAdversarial \u0301 patch,\u201d arXiv preprint arXiv:1712.09665,..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            },
            {
              "id": "E2138630072",
              "text": "... Furthermore, energy consumption and throughput of UAV is optimized. Based on the priorities given, efficiency of data and computing services are also optimized. Bockelmann et al. [44] have designed a FP7 METIS project using 5G system concept in which well-suitable chase for high data rates are summarized by the term \u201dExtreme Mobile Broadband\u201d (xMBB). The overhead seen in exchange of messages required before the transmission of data payload affects the energy efficiency of MTC devices. Hence, less frequent and shorter transmissions preserves energy. Therefore MAC protocols coupled with Physical layer approaches enable devices with long battery lives. In [51], orthogonal frequency division multiplexing with index modulation is proposed to improve inter carrier interference caused by asynchronous transmission for uncoordinated mMTC networks. Data transmission is performed by the indices of active subcarriers. Subcarrier Mapping Scheme called inner subcarrier activation scheme is proposed to further improve the interference of adjacent user in asynchronous systems. In [52], proposed a novel 5G based network architecture called Non-Orthogonal Multiple Access (NOMA) to reduce the latency and reliability requirements in V2X communication. One of the critical challenge for next-generation wireless communication is to satisfy the high demand for mMTC systems which performs a random transmission between base station and machine users. Therefore, the lack of coordination between the base station causes inter-carrier interference. Researchers are facing a big challenge in providing the services for asynchronous massive machine users. Activation ration and optimization of subblock size can be evaluated for clustering users and conflicting users with respect to their requirements. Autonomous vehicle in reality faces various challenges like real-time data analytics, software heterogeneity, validation, verification and latency issues. Aforementioned challenges, researchers across the globe are trying to provide solutions using 5G-based testbeds. Summary: V2X and 5G connections helps AV to visualize the objects and obstacles around corners and beyond. Connectivity between the cars and infrastructure provides awareness ahead among the vehicles like reduce the speed automatically in slow-moving traffic areas. By the time when it reaches the signal, traffic would have been cleared thereby reducing the waiting time on the traffic signal. \u2019Traffic Light Information\u2019 is a classic case study based on V2I initiated by Audi in Europe. All these scenarios requires good connectivity at the speed of light. This can be achieved by providing 5G network that enables fast processing and prior decision making.To summarize, AV would become reality with the application of 5G technologies. E. eMBB 1) Introduction: eMBB service is used particularly to enhance the Quality of Experience(QoE) in bandwidth for invehicle applications. 2) Existing Challenges/Limitations: One of the significant way in 5G to deliver wireless broadband to previously unreached areas is through the technology Fixed Wireless Access (FWA) network. Globally, FWA has gained its importance in developed and developing countries and is expected to expand exponentially from 2018-2025. This FWA creates a platform for eMBB for wide coverage using higher-spectrum bands. Some of the use cases of eMBB are as follows: Broadband everywhere: FWA technology can provide wide coverage globally with minimum speed of 50 Mbps. Public transportation: Broadband access used in high-speed trains and public transport systems Hot spots: Enhancing broadband coverage in densely populated areas and in high rise buildings Large-scale events: Enabling high speed of broadband data where thousands of people are gathered in one place for any kind of big event. Smart offices: Delivering high-bandwidth of data connections to thousands of users even in the environment with heavy data traffic. Enhanced Multimedia: Provides high quality video streaming and real-time content over wide coverage areas. 3) How B5G help (with Related work): In [42], authors have addressed the problem of resource scheduling problem in eMBB traffics. Initially the resource blocks are assigned to the beginning of each time slot based on the channel state and the previous average data rate upto the current time slot of each eMBB users. Two dimensional Hopfield Neural Network and the energy function is used to solve the resource allocation problem. Then, chance constraint problem is applied to maximize the eMBB data rates. The authors in [53] targets more number of users in data transmission in AV. Non-Orthogonal Multiple Access (NOMA) is proposed to optimize the distribution in unicast/multicast scenarios. The complexity of the algorithm is measured and compared using Time Division Multiplexing (TDM). In order to reduce the complexity of the algorithm, two solutions are proposed. First, the numbers of injection levels are reduced and second one is choosing the smart algorithm that selects the optimum injection levels. In [43], authors have considered eMBB and URLLC to be the important prerequisites of smart intelligent transportation systems. The eMBB traffic is scheduled at the boundary of each time slot for data transmissions. During the transmission interval, random arrivals of URLLC traffics are allowed. Summary: URLLC and mMTC works together with eMBB to fulfill the needs in new wireless networks to provide the facilities in applications like healthcare, manufacturing, military and emergency response. These three features in 5G provides the solutions for the issues with respect to bandwidth, density and latency in applications with restricted LTE\u2019s capabilities like autonomous vehicles, smart city and augmented reality. IV. IMPACT OF 5G AND B5G TECHNOLOGIES ON AV In this section we discuss the impact of some of the prominent technologies in 5G and B5G on AV. 13 TABLE VII BENEFITS AND CHALLENGES OF TECHNICAL ASPECTS OF AUTONOMOUS VEHICLES. Application Existing Challenges Solutions Limitations Navigation and Path Planning [27]\u2013[35] 1) Autonomous planning and navigation easily plunge into the local optimum in the complex scenarios and thus fall into traps, resulting in a low probability of finding a reasonable route to the target 2) conventional algorithms cannot plan the environment completely in advance 3) path planning for unstructured roads are very complex. 1) Deep learning and Deep reinforcement learning is used to achieve endto-end learning for complex tasks 2) Gradually transformation of low-level feature representations into highlevel feature representations through a multilayer process is implemented 3) Free-form navigation like graph based search algorithm can be used to plan for navigation in unstructured roads 4) V2X shares intent, accurate and fast information to perform higher level of prediction 5) V2X wireless sensor visualize 360 degree and sense non-line sight and wide range of area 6) Precise positioning of location and dynamic decision making are faster and accurate using 5G technologies enabled with AI 1) Detecting and localizing the object at specific time and adjusting the path dynamically has to be still optimal and faster 2) Constructing the maps from the inputs received from the sensors and devices is a time consuming task 3) Fails to provide accurate predictions in poor climatic conditions 4) Unable to predict the agent behaviour on roads Object Detection/ Collision Avoidance [36]\u2013[39] 1) Massive volumes of data are generated by autonomous vehicles. In terms of latency and security, delivering cloud services to autonomous vehicles is seen as a significant challenge 2) Localization is one of the most major aspects of autonomous vehicles for avoiding collisions and ensuring safe navigation 3) The reliability of high-end sensors is limited over longer distances or when a vehicle enters a low-visibility region 1) To address the latency issue in 5G-enabled vehicular networks, a software-defined networking architecture at the network edge is being constructed. 2) Gaussian filter techniques are used to reduce granularity in order to estimate the correct lane with greater accuracy 3) 5G communication technology for CL provides an accurate vehicle position 4) Laplacian Graph Processing provides faster response time and a higher GPS accuracy rate 5) Roadside equipment is used to improve data sharing between automobiles 1) The lane detection algorithm performs tasks such as vehicle location and path detection with a moderate accuracy rate 2) AV share information with signals and act adaptively according to the situation, whereas humans in manual vehicles act more appropriately based on the scenario 3) Certain risks are inherent in data sharing, such as when a malicious vehicle sends fake data in order to manipulate the receivers, or when faulty sensors communicate incorrect data 4) If the AV trust the data provided by the malicious vehicle, they will be trapped, causing them to switch lanes or accelerate faster. As a result, there may be serious threats to human life URLLC [40]\u2013 [46], [48]\u2013[52] 1) Onboard processing is not sufficient to store massive amount of data generated from sensors and other devices. 2) Connecting the vehicles based on cloud computing provides high latency which is not suitable AV 3) Providing communication with low latency and high reliability is a complex task in dynamic real time process which is connected to different devices. 1) Resource scheduling problem is solved using puncturing approach 2) To maintain the latency constraint, URLLC are considered inside the minislot of each eMBB time slot 3) To ensure the reliability, guard zones are deployed around the vehicle. 4) 5G can make instantaneous communication with URLLC 5) The time delay using 5G is 1 to 5 millisecond when compared to 20 milliseconds in 4G 6) Reduced time delay in AV provides users with safety information 7) One slice in network slicing can provide high reliability and security for URLLC 1) Resource scheduling should be considered for both uplink and downlink scenario for URLLC 2) Guard zone based URLLC scheduling policy is compared with the baseline policy where the guard zone receiver is not applied mMTC/ V2V communication/ V2X communication [44], [44]\u2013[52] 1) A common framework for Machine type communication must be available 2) Current link adaption mechanisms are not suitable for MTC 3) Resource allocation and channel coding schemes are unsuitable for small packets 1) optimization algorithms are used for trajectory planning which in turn minimizes the power consumption of MTC 2) Extreme mobile broadband is used for high data rates 3) less frequent and shorter transmissions preserves energy. So MAC protocols with physical layer approaches ensure device with long battery life 4) NOMA architecture is used to reduce the latency and reliability requirements in V2X 1)..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            }
          ]
        },
        "S7642449272": {
          "id": "S7642449272",
          "text": "Future research directions in the regulatory landscape for UAV deployment in advanced communication networks should focus on developing comprehensive security frameworks that address the unique challenges posed by UAV operations in urban and rural settings.",
          "children": [
            {
              "id": "E5490018945",
              "text": "..cellular communication. \u2022 Security and privacy problems such as eavesdropping, jamming and spoofing in aerial networks is another challenge in UAV communication. Lightweight techniques and AI solutions can be used to address these issues. \u2022 Mobility support and collision avoidance in terms of Internet of UAVs are some of the other challenges that need to be researched in the future. \u2022 Finally, in terms of healthcare, online consultation, robotic surgery and telemedicine requires a network that can support high-definition video streaming. In such cases ultra-low latency of less than 10ms and ultrahigh reliable connection are required. Latency of less than 10ms is a critical challenge and an open research problem in future cellular communication. \u2022 Reliable and real-time remote monitoring requires a high speed network and a reliable connection, otherwise practitioners will be unable to get the real-time data that can be used for quick and on-time healthcare decisions. Providing higher bandwidth and reliable connections are still an open problem that need to be addressed in future cellular networks. \u2022 Integration of AI techniques for potential diagnosis (in the healthcare sector) that can lead to the best treatment plan is also a research area that needs to be explored. VII. CONCLUSIONS In this paper, we have explored 5G use-cases emphasising on three main case studies: V2X communication, Drones, and Healthcare in order to identify the most challenging use-case for our future work. Starting with V2X communication, we explored V2V, V2P, V2I, and IV Communication along with their application areas. We also addressed the limitations and challenges of the existing communication technologies and proposed solutions based on 5G technologies. We further studied drone communication for mission critical infrastructures and provide a comparative analysis of 4G (LTE/LTE-A) and 5G with respect to control latency. Moreover, we examined healthcare with respect to 4G and 5G technologies and provide an overview of online consultation, online health monitoring, remote diagnosis, and mobile robotic surgery. Based on the literature review and critical analysis, we have concluded that drone communication for the provision and maintenance of critical infrastructures is the most challenging scenario that can be carried to the next phase of our work. In the future, we are planning to extend this work and propose to identify tools in which we can simulate our initial models in order to validate them and check their applicability for real-time applications. We also look forward to implementing the model in a real-time testbed. The potential impact on standard bodies along with community benefits is also part of our future plans. ACKNOWLEDGMENT This research work is supported by BTIIC (BT Ireland Innovation Centre), funded by BT and Invest Northern Ireland. REFERENCES [1] Ericsson, Huawei, and Qualcomm. The road to 5g: Drivers, applications, requirements and technical development. Technical report, Global mobile Suppliers Association (GSA), 2015. [2] Alvaro Neira. 5g: a network of opportunities and chal- \u0301 lenges. Technical report, Axon Partners Group, 2017. [3] S. A. A. Shah, E. Ahmed, M. Imran, and S. Zeadally. 5g for vehicular communications. IEEE Communications Magazine, 56(1):111\u2013117, Jan 2018. ISSN 0163-6804. [4] Sana Ullah, Ki-Il Kim, Kyong-Hoon Kim, Muhammad Imran, Pervez Khan, Eduardo Tovar, and Farman Ali. Uav-enabled healthcare architecture: Issues and challenges. Future Generation Computer Systems, 2019. ISSN 0167-739X. doi: https://doi.org/10.1016/j.future. 2019. 01. 028. URL http://www. sciencedirect. com/ science/article/pii/S0167739X18318247. [5] Nokia. 5g use cases and requirements. Technical report, Nokia, 2016. 2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2019.2905347, IEEE Access 16 [6] Barbara Masini, Alessandro Bazzi, and Alberto Zanella. A survey on the roadmap to mandate on board connectivity and enable v2v-based vehicular sensor networks. Sensors, 18(7):2207, 2018. [7] M. Boban, A. Kousaridas, K. Manolakis, J. Eichinger, and W. Xu. Connected roads of the future: Use cases, requirements, and design considerations for vehicle-toeverything communications. IEEE Vehicular Technology Magazine, 13(3):110\u2013123, 2018. [8] Hazim Shakhatreh, Ahmad Sawalmeh, Ala Al-Fuqaha, Zuochao Dou, Eyad Almaita, Issa Khalil, Noor Shamsiah Othman, Abdallah Khreishah, and Mohsen Guizani. Unmanned aerial vehicles: A survey on civil applications and key research challenges. arXiv preprint arXiv:1805.00881, 2018. [9] Bin Li, Zesong Fei, and Yan Zhang. Uav communications for 5g and beyond: Recent advances and future trends. IEEE Internet of Things Journal, 2018. [10] Darrell M. West. How 5G technology enables the health internet of things. Technical Report July 2016, 2016. URL https://www.brookings.edu/research/how-5gtechnology-enables-the-health-internet-of-things/. [11] Siddique Latif, Junaid Qadir, Shahzad Farooq, and Muhammad Ali Imran. How 5G wireless (and Concomitant Technologies) will revolutionize healthcare? Future Internet, 9(4):1\u201310, 2017. ISSN 19995903. [12] National Center for Statistics Analysis. Summary of motor vehicle crashes: 2016 data. (traffic safety facts. report no. dot hs 812 580). Technical report, National Highway Traffic Safety Administration, Washington, DC, August, 2018. [13] Department for Transport. Reported road casualties great britain, provisional estimates: April to june 2017, 2017. URL https://assets.publishing.service.gov.uk/government/ uploads / system / uploads / attachment data / file / 648081 / rrcgb2016-01.pdf. [14] Texas A&M. Transportation Institute INRIX. 2015 urban mobility scorecard. Technical report, August, 2015. [15] Mark Patrick. V2x communications \u2013 lte versus dsrc. accessed online:. URL http://www.mwee.com/designcenter/v2x-communications-lte-versus-dsrc. [16] Iftikhar Ahmad, Rafidah Md Noor, Ismail Ahmedy, Syed Adeel Ali Shah, Ibrar Yaqoob, Ejaz Ahmed, and Muhammad Imran. Vanet\u2013lte based heterogeneous vehicular clustering for driving assistance and route planning applications. Computer Networks, 145:128\u2013140, 2018. [17] Ioannis Mavromatis, Andrea Tassi, Robert J. Piechocki, and Andrew Nix. Efficient v2v communication scheme for 5g mmwave hyper-connected cavs. IEEE International Conference on Communications, 2018. [18] I. Chatzigeorgiou and A. Tassi. Decoding delay performance of random linear network coding for broadcast. IEEE Transactions on Vehicular Technology, 66(8):7050\u2013 7060, 2017. [19] Ibrar Yaqoob, Iftikhar Ahmad, Ejaz Ahmed, Abdullah Gani, Muhammad Imran, and Nadra Guizani. Overcoming the key challenges to establishing vehicular communication: Is sdn the answer? IEEE Communications Magazine, 55(7):128\u2013134, 2017. [20] N. Lu, N. Cheng, N. Zhang, X. Shen, and J. W. Mark. Connected vehicles: Solutions and challenges. IEEE Internet of Things Journal, 1(4):289\u2013299, 2014. [21] Robert W. Irving and Sandy Scott. The stable fixtures problem\u2014a many-to-many extension of stable roommates. Discrete Applied Mathematics, 155(16):2118\u2013 2129, 2007. URL http://www.sciencedirect.com/science/ article/pii/S0166218X07001552. [22] X. Chen, Y. Miao, M. Jin, and Q. Zhang. Driving decision-making analysis of lane-changing for autonomous vehicle under complex urban environment. In 2017 29th Chinese Control And Decision Conference (CCDC), pages 6878\u20136883, 2017. [23] T. A. S. S. International. Simulation of adas and active safety, 2018. Accessed Online:. URL https://tass.plm. automation.siemens.com/prescan. [24] Gasp \u0301 ar P \u0301 eter, Szalay Zsolt, and Aradi Szil \u0301 ard. Highly \u0301 automated vehicle systems, 2014. URL http : / / www.mogi.bme.hu/TAMOP/jarmurendszerek iranyitasa angol/book.html. [25] Yasir Mehmood, Noman Haider, Muhammad Imran, Andreas Timm-Giel, and Mohsen Guizani. M2m communications in 5g: State-of-the-art architecture, recent advances, and research challenges. IEEE Communications Magazine, 55(9):194\u2013201, 2017. [26] Kyungmin Park and Esmael Dinan. Traffic control and everything-to-vehicle..",
              "url": "https://openalex.org/W2921698889",
              "openalex_id": "https://openalex.org/W2921698889",
              "title": "5G Communication: An Overview of Vehicle-to-Everything, Drones, and Healthcare Use-Cases",
              "publication_date": "2019-01-01"
            },
            {
              "id": "S2259813022",
              "text": "Future research directions in the integration of UAVs with 5G networks should focus on developing robust security frameworks and addressing the challenges of real-time data processing and management in dynamic environments.",
              "children": [
                {
                  "id": "E9557096701",
                  "text": "..vol. 5, no. 1, pp. 48\u201354, 2021. [90] H. D. R. Albonda and J. Perez-Romero, \u201cAn efficient ran slicing \u0301 strategy for a heterogeneous network with embb and v2x services,\u201d IEEE access, vol. 7, pp. 44 771\u201344 782, 2019. [91] S. R. Pokhrel, J. Ding, J. Park, O.-S. Park, and J. Choi, \u201cTowards enabling critical mmtc: A review of urllc within mmtc,\u201d IEEE Access, vol. 8, pp. 131 796\u2013131 813, 2020. [92] M. I. Ashraf, C.-F. Liu, M. Bennis, W. Saad, and C. S. Hong, \u201cDynamic resource allocation for optimized latency and reliability in vehicular networks,\u201d IEEE Access, vol. 6, pp. 63 843\u201363 858, 2018. [93] W. Anwar, N. Franchi, and G. Fettweis, \u201cPhysical layer evaluation of v2x communications technologies: 5g nr-v2x, lte-v2x, ieee 802.11 bd, and ieee 802.11 p,\u201d in 2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall). IEEE, 2019, pp. 1\u20137. [94] J. Mei, X. Wang, and K. Zheng, \u201cIntelligent network slicing for v2x services toward 5g,\u201d IEEE Network, vol. 33, no. 6, pp. 196\u2013204, 2019. [95] S. S. Husain, A. Kunz, A. Prasad, E. Pateromichelakis, and K. Samdanis, \u201cUltra-high reliable 5g v2x communications,\u201d IEEE Communications Standards Magazine, vol. 3, no. 2, pp. 46\u201352, 2019. [96] S. A. Ashraf, R. Blasco, H. Do, G. Fodor, C. Zhang, and W. Sun, \u201cSupporting vehicle-to-everything services by 5g new radio release-16 systems,\u201d IEEE Communications Standards Magazine, vol. 4, no. 1, pp. 26\u201332, 2020. [97] K. Ganesan, J. Lohr, P. B. Mallick, A. Kunz, and R. Kuchibhotla, \u201cNr sidelink design overview for advanced v2x service,\u201d IEEE Internet of Things Magazine, vol. 3, no. 1, pp. 26\u201330, 2020. [98] S.-Y. Lien, D.-J. Deng, C.-C. Lin, H.-L. Tsai, T. Chen, C. Guo, and S.-M. Cheng, \u201c3gpp nr sidelink transmissions toward 5g v2x,\u201d IEEE Access, vol. 8, pp. 35 368\u201335 382, 2020. [99] S. Gyawali, S. Xu, Y. Qian, and R. Q. Hu, \u201cChallenges and solutions for cellular based v2x communications,\u201d IEEE Communications Surveys & Tutorials, 2020. [100] S. Chen, J. Hu, Y. Shi, Y. Peng, J. Fang, R. Zhao, and L. Zhao, \u201cVehicle-to-everything (v2x) services supported by lte-based systems and 5g,\u201d IEEE Communications Standards Magazine, vol. 1, no. 2, pp. 70\u201376, 2017. [101] Y. Lu, M. Gerasimenko, R. Kovalchukov, M. Stusek, J. Urama, J. Hosek, M. Valkama, and E. S. Lohan, \u201cFeasibility of locationaware handover for autonomous vehicles in industrial multi-radio environments,\u201d Sensors, vol. 20, no. 21, p. 6290, 2020. [102] I. Rasheed and F. Hu, \u201cIntelligent super-fast vehicle-to-everything 5g communications with predictive switching between mmwave and thz links,\u201d Vehicular Communications, vol. 27, p. 100303, 2021. [103] E. Kampert, C. Schettler, R. Woodman, P. A. Jennings, and M. D. Higgins, \u201cMillimeter-wave communication for a last-mile autonomous transport vehicle,\u201d IEEE Access, vol. 8, pp. 8386\u20138392, 2020. [104] D. M. Manias and A. Shami, \u201cMaking a case for federated learning in the internet of vehicles and intelligent transportation systems,\u201d IEEE Network, vol. 35, no. 3, pp. 88\u201394, 2021. [105] S. Savazzi, M. Nicoli, M. Bennis, S. Kianoush, and L. Barbieri, \u201cOpportunities of federated learning in connected, cooperative, and automated industrial systems,\u201d IEEE Communications Magazine, vol. 59, no. 2, pp. 16\u201321, 2021. [106] Z. Du, C. Wu, T. Yoshinaga, K.-L. A. Yau, Y. Ji, and J. Li, \u201cFederated learning for vehicular internet of things: Recent advances and open issues,\u201d IEEE Open Journal of the Computer Society, vol. 1, pp. 45\u2013 61, 2020. [107] S. R. Pokhrel and J. Choi, \u201cImproving tcp performance over wifi for internet of vehicles: A federated learning approach,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 6, pp. 6798\u20136802, 2020. [108] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, \u201cBlockchain empowered asynchronous federated learning for secure data sharing in internet of vehicles,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 4, pp. 4298\u20134311, 2020. [109] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, \u201cFederated learning for data privacy preservation in vehicular cyber-physical systems,\u201d IEEE Network, vol. 34, no. 3, pp. 50\u201356, 2020. [110] X. Kong, H. Gao, G. Shen, G. Duan, and S. K. Das, \u201cFedvcp: A federated-learning-based cooperative positioning scheme for social internet of vehicles,\u201d IEEE Transactions on Computational Social Systems, 2021. [111] T. R. Gadekallu, Q.-V. Pham, T. Huynh-The, S. Bhattacharya, P. K. R. Maddikunta, and M. Liyanage, \u201cFederated learning for big data: A survey on opportunities, applications, and future directions,\u201d arXiv preprint arXiv:2110.04160, 2021. [112] J. Gao, K. O.-B. O. Agyekum, E. B. Sifah, K. N. Acheampong, Q. Xia, X. Du, M. Guizani, and H. Xia, \u201cA blockchain-sdn-enabled internet of vehicles environment for fog computing and 5g networks,\u201d IEEE Internet of Things Journal, vol. 7, no. 5, pp. 4278\u20134291, 2019. [113] L. Xie, Y. Ding, H. Yang, and X. Wang, \u201cBlockchain-based secure and trustworthy internet of things in sdn-enabled 5g-vanets,\u201d IEEE Access, vol. 7, pp. 56 656\u201356 666, 2019. [114] B. Bera, S. Saha, A. K. Das, N. Kumar, P. Lorenz, and M. Alazab, \u201cBlockchain-envisioned secure data delivery and collection scheme for 5g-based iot-enabled internet of drones environment,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9097\u20139111, 2020. [115] M. Aloqaily, O. Bouachir, A. Boukerche, and I. Al Ridhawi, \u201cDesign guidelines for blockchain-assisted 5g-uav networks,\u201d IEEE Network, vol. 35, no. 1, pp. 64\u201371, 2021. [116] R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain and 5g integrated softwarized uav network management: Architecture, solutions, and challenges,\u201d Physical Communication, vol. 47, p. 101355, 2021. [117] X. Jian, P. Leng, Y. Wang, M. Alrashoud, and M. S. Hossain, \u201cBlockchain-empowered trusted networking for unmanned aerial vehicles in the b5g era,\u201d IEEE Network, vol. 35, no. 1, pp. 72\u201377, 2021. [118] C. Feng, K. Yu, A. K. Bashir, Y. D. Al-Otaibi, Y. Lu, S. Chen, and D. Zhang, \u201cEfficient and secure data sharing for 5g flying drones: a blockchain-enabled approach,\u201d IEEE Network, vol. 35, no. 1, pp. 130\u2013 137, 2021. [119] B. Ghimire, D. B. Rawat, C. Liu, and J. Li, \u201cSharding-enabled blockchain for software-defined internet of unmanned vehicles in the battlefield,\u201d IEEE Network, vol. 35, no. 1, pp. 101\u2013107, 2021. 32 [120] A. Gumaei, M. Al-Rakhami, M. M. Hassan, P. Pace, G. Alai..",
                  "url": "https://openalex.org/W4310130661",
                  "openalex_id": "https://openalex.org/W4310130661",
                  "title": "Autonomous vehicles in 5G and beyond: A survey",
                  "publication_date": "2023-02-01"
                },
                {
                  "id": "E2631264487",
                  "text": "..order to provide fast processing and prior decision making, good connectivity is required which is achieved by 5G in AV. emBB provides high quality in bandwidth for vehicular communication. URLLC, mMTC and eMBB in 5G works together to provide faster connection speeds, higher device capacity and lower latency 125G Alliance for Connected Industries and Automation endorses testbeds for evaluation of industrial 5G use cases https://5g-acia.org/press-releases/ 5g-alliance-for-connected-industries-and-automation-endorses-testbeds-forevaluation-of-industrial-5g-use-cases/ in AV applications.Some of the benefits of using 5G in AV are driving the vehicles at high speed with high bandwidth, information delivery in minimal time limits due to low latency, notification alerts ahead about any hazardous events and provides self-decision using AV integrated with AI 2) Possible Future Directions: Some of the challenges in using 5G in AV are basically 5G spectrum are too expensive and can be purchased by spectrum auctions.The cost incurred in extending the existing network to 5G is too high and mapping network with reference to geo locations is a tedious task. AV using 5G and beyond can be further extended to perform the following activities \u2022 Predictive maintenance:Drivers can proactively maintain the vehicle to avoid failure during driving. Invehicle sensors monitors the conditions of the components present inside the vehicle like battery life, fuel pump and starter motor. Data from cloud combined with AI can predict potential maintenance issues before the occurrence of failure. \u2022 Advanced Infotainment Customers can be provided with additional information by integrating AI with AR, VR and sensors to have realistic experience. \u2022 Traffic safety service: 5G communication devices embedded inside the vehicles collects the information from the environment and pedestrians are stored in the cloud. Analytics on these data warns the drivers about the hazardous roads and traffic congestion which in turn recommends the solution for alternate path. 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become an important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. B. B5G technologies 1) Lessons Learned: Apart from large bandwidth requirements, several functionalities of AVs like object detection, lane detection, collision avoidance, navigation, V2V/V2X/V2I communications require several features like reduced latency, improved physical layer infrastructure, privacy & security for seamless AD. The supporting technologies of B5G such as MEC, network slicing, SDN/NFV, 5GNR, blockchain, FL, and ZSM can help in providing these requirements to AVs. However, several challenges such as generation of labels in real time for training the ML algorithms, lack of justification of the predictions/recommendations of ML algorithms, huge dimensionality of the data generated, etc. have to be addressed to realize the full potential of B5G for AV. Also, the autonomous vehicles may be connected to multiple networks on the road. Due to mobility of vehicles, it is likely that an AV may move out of coverage area of the access network and may have to join another network. Availability of availability of MEC for multiple access technologies is a significant challenge. Another significant challenge is the management of huge data generated from the AVs that are interconnected in real time [216]. 28 2) Possible Future Directions: Explainable Artificial Intelligence (XAI) can be adopted in B5G to address for trustability/justification/explainability of the decisions/predictions from ML algorithms in applications such as object detection, lane detection, collision avoidance, etc [217]\u2013[219]. For instance, consider that the AV is taking a human to the destination. Suddenly, the AI/ML model gives suggestion to the AV to take another route, which is different than the intended route. In these situations, the AV should be in a position to explain/justify the actions it has taken to avoid a particular route or choosing a particular route. XAI can help the humans to understand the actions of AVs in these kinds of situations. Unsupervised ML models can work on unlabelled data, that can address the challenge of big data generated by AV in 5GB era [220]. Several dimensionality reduction techniques can be used to extract the most important features from the large volumes of data generated in real time [221]. Handover problems associated with the AVs registered to one network and requiring to join another network during the mobility can be addressed by open radio access network (Open RAN) platform [222]. C. Security Concerns in AVs 1) Lessons Learned: From the security point of view, as highlighted in Table XII, it can observed that compared to In-Vehicle communication, the vulnerabilities brought by V2X technologies are more. V2X technologies will widen the attack surfaces thereby increasing the level of threats for the AVs. All the basic security goals i.e. confidentiality, integrity, availability and authentication needs to be addressed while incorporating V2X technologies with AVs. There is need of effective countermeasures to mitigate the threats arising from three main attack categories i.e. spoofing, denial of service, injection and physical attacks. 2) Possible Future Directions: The potential future directions can be categorised based on basic security goals as mentioned above especially with the main focus on V2X technologies. A strong and robust authentication mechanisms will be required for AVs to communicate with trusted entities while moving from one place to the other. An authentication mechanism with high latency and computing power requirement might provide attackers ample time to carry out spoofing or man-in-the-middle attack. Similarly, most of the protocols within AVs do not use encryption, how to secure communication within vehicle and while interacting with other entities outside the vehicles also seems interesting future research direction. Also, there will be huge amount of data generated by AVs, storing that data while preserving privacy of users and making that data available to legitimate users is yet another interesting research direction. D. Projects 1) Lessons Learned: Today, many research projects and SDO activities are focusing on the development of 5G and B5G networks to support the deployment of autonomous vehicles. These activities primarily focus on technical developments of 5G/b5G related technologies such as MEC, NS, 5G NR, ZSM, and AI. Significantly, NS and MEC got particular focus as these technologies can play a critical role in deploying autonomous vehicle-related applications and services. In addition, there are a significant number of research projects and standardization activities contributing to relevant 5G and B5G technical aspects such as path planning, mobility, service migration, security, and privacy. However, it might take a few more years for these standardizations to be fully deployed by different stakeholders such as mobile operators, governments, regulators, manufacturers, and third-party service providers. Thus, the continuous cooperation between different working groups (i.e., academia and industry) and SDOs are highly required in the coming years. Especially, funding frameworks such as H2020 by European Commission (EC) fuel these activities. 2) Possible Future Directions: In the future, there are two primary questions to be addressed. First, how to address the lack of AV-specific standardization in core 5G SDOs? Currently, AV is considered just one use case of 5G/B5G networks and still lacks a dedicated focus on AV within the core 5G SDOs. To resolve this, more AV stakeholders should contribute to 5G SDOs, and more AV-related subgroups would be formed within core 5G SDOs to develop dedicated AV-related standards. Second, how do we integrate and cooperate between different working groups and SDOs? The strong cooperation between AV SDOs and core 5G SDOs would be further encouraged. This will resolve some of the issues related to the first issue. More joint research and SDOs activities should be planned in the future. E. Emerging and Future Research Directions related to AV 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. 1) Quantum Computing: Quantum computing is the next future generation of automotive technology. Electric vehicles are significant part of quantum revolution. Automobile manufactures have started taking advantage of quantum computers to solve various automotive problems. AI in AV requires large amount of data for analysing and providing optimal response in dynamic situations. For detecting real time car locations and designing the optimal path requires high computing power and speed in AI. The former feature, high computing power can achieved by quantum computers. German automobile company Volkswagen collaborated with D-Wave systems to design and develop traffic routing in Beijing based on quantum computing systems. These systems can also solve optimization problems like waiting time, deployment of fleets etc., Volkswagen has also partnered with google to predict the state of traffic to avoid accident and to simulate the behaviour of electrical component and embed AI in driverless cars. As the AV are more 29 vulnerable to outside world, security breaches can be solved by quantum security. AV requires tremendous computing powers like optimized route planning and change the entire transport systems into smart systems. The cars will become smarter by communicating among themselves and outside world. More developments are expected in the field of AV integrated with quantum computers to achieve the benefits of computing and processing power. 2) Cognitive Cloud: Cognitive AI and algorithms would help us to simulate human-level performance specifically in level 5 AV. Satisfying the level 5 AV is a tedious task as it needs accurate decision making, object detection and localization under uncertain conditions like fog, rain and extreme darkness. Cognitive computing enhances the model accuracy to achieve closeness to humanlike performance in object detection and..",
                  "url": "https://openalex.org/W4310130661",
                  "openalex_id": "https://openalex.org/W4310130661",
                  "title": "Autonomous vehicles in 5G and beyond: A survey",
                  "publication_date": "2023-02-01"
                }
              ]
            }
          ]
        },
        "S6680170015": {
          "id": "S6680170015",
          "text": "Regulatory challenges for the deployment of drones in advanced communication networks include the need for standardized communication protocols and security measures to protect against cyber threats, which are essential for ensuring safe and reliable UAV operations.",
          "children": [
            {
              "id": "E5490018945",
              "text": "..cellular communication. \u2022 Security and privacy problems such as eavesdropping, jamming and spoofing in aerial networks is another challenge in UAV communication. Lightweight techniques and AI solutions can be used to address these issues. \u2022 Mobility support and collision avoidance in terms of Internet of UAVs are some of the other challenges that need to be researched in the future. \u2022 Finally, in terms of healthcare, online consultation, robotic surgery and telemedicine requires a network that can support high-definition video streaming. In such cases ultra-low latency of less than 10ms and ultrahigh reliable connection are required. Latency of less than 10ms is a critical challenge and an open research problem in future cellular communication. \u2022 Reliable and real-time remote monitoring requires a high speed network and a reliable connection, otherwise practitioners will be unable to get the real-time data that can be used for quick and on-time healthcare decisions. Providing higher bandwidth and reliable connections are still an open problem that need to be addressed in future cellular networks. \u2022 Integration of AI techniques for potential diagnosis (in the healthcare sector) that can lead to the best treatment plan is also a research area that needs to be explored. VII. CONCLUSIONS In this paper, we have explored 5G use-cases emphasising on three main case studies: V2X communication, Drones, and Healthcare in order to identify the most challenging use-case for our future work. Starting with V2X communication, we explored V2V, V2P, V2I, and IV Communication along with their application areas. We also addressed the limitations and challenges of the existing communication technologies and proposed solutions based on 5G technologies. We further studied drone communication for mission critical infrastructures and provide a comparative analysis of 4G (LTE/LTE-A) and 5G with respect to control latency. Moreover, we examined healthcare with respect to 4G and 5G technologies and provide an overview of online consultation, online health monitoring, remote diagnosis, and mobile robotic surgery. Based on the literature review and critical analysis, we have concluded that drone communication for the provision and maintenance of critical infrastructures is the most challenging scenario that can be carried to the next phase of our work. In the future, we are planning to extend this work and propose to identify tools in which we can simulate our initial models in order to validate them and check their applicability for real-time applications. We also look forward to implementing the model in a real-time testbed. The potential impact on standard bodies along with community benefits is also part of our future plans. ACKNOWLEDGMENT This research work is supported by BTIIC (BT Ireland Innovation Centre), funded by BT and Invest Northern Ireland. REFERENCES [1] Ericsson, Huawei, and Qualcomm. The road to 5g: Drivers, applications, requirements and technical development. Technical report, Global mobile Suppliers Association (GSA), 2015. [2] Alvaro Neira. 5g: a network of opportunities and chal- \u0301 lenges. Technical report, Axon Partners Group, 2017. [3] S. A. A. Shah, E. Ahmed, M. Imran, and S. Zeadally. 5g for vehicular communications. IEEE Communications Magazine, 56(1):111\u2013117, Jan 2018. ISSN 0163-6804. [4] Sana Ullah, Ki-Il Kim, Kyong-Hoon Kim, Muhammad Imran, Pervez Khan, Eduardo Tovar, and Farman Ali. Uav-enabled healthcare architecture: Issues and challenges. Future Generation Computer Systems, 2019. ISSN 0167-739X. doi: https://doi.org/10.1016/j.future. 2019. 01. 028. URL http://www. sciencedirect. com/ science/article/pii/S0167739X18318247. [5] Nokia. 5g use cases and requirements. Technical report, Nokia, 2016. 2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2019.2905347, IEEE Access 16 [6] Barbara Masini, Alessandro Bazzi, and Alberto Zanella. A survey on the roadmap to mandate on board connectivity and enable v2v-based vehicular sensor networks. Sensors, 18(7):2207, 2018. [7] M. Boban, A. Kousaridas, K. Manolakis, J. Eichinger, and W. Xu. Connected roads of the future: Use cases, requirements, and design considerations for vehicle-toeverything communications. IEEE Vehicular Technology Magazine, 13(3):110\u2013123, 2018. [8] Hazim Shakhatreh, Ahmad Sawalmeh, Ala Al-Fuqaha, Zuochao Dou, Eyad Almaita, Issa Khalil, Noor Shamsiah Othman, Abdallah Khreishah, and Mohsen Guizani. Unmanned aerial vehicles: A survey on civil applications and key research challenges. arXiv preprint arXiv:1805.00881, 2018. [9] Bin Li, Zesong Fei, and Yan Zhang. Uav communications for 5g and beyond: Recent advances and future trends. IEEE Internet of Things Journal, 2018. [10] Darrell M. West. How 5G technology enables the health internet of things. Technical Report July 2016, 2016. URL https://www.brookings.edu/research/how-5gtechnology-enables-the-health-internet-of-things/. [11] Siddique Latif, Junaid Qadir, Shahzad Farooq, and Muhammad Ali Imran. How 5G wireless (and Concomitant Technologies) will revolutionize healthcare? Future Internet, 9(4):1\u201310, 2017. ISSN 19995903. [12] National Center for Statistics Analysis. Summary of motor vehicle crashes: 2016 data. (traffic safety facts. report no. dot hs 812 580). Technical report, National Highway Traffic Safety Administration, Washington, DC, August, 2018. [13] Department for Transport. Reported road casualties great britain, provisional estimates: April to june 2017, 2017. URL https://assets.publishing.service.gov.uk/government/ uploads / system / uploads / attachment data / file / 648081 / rrcgb2016-01.pdf. [14] Texas A&M. Transportation Institute INRIX. 2015 urban mobility scorecard. Technical report, August, 2015. [15] Mark Patrick. V2x communications \u2013 lte versus dsrc. accessed online:. URL http://www.mwee.com/designcenter/v2x-communications-lte-versus-dsrc. [16] Iftikhar Ahmad, Rafidah Md Noor, Ismail Ahmedy, Syed Adeel Ali Shah, Ibrar Yaqoob, Ejaz Ahmed, and Muhammad Imran. Vanet\u2013lte based heterogeneous vehicular clustering for driving assistance and route planning applications. Computer Networks, 145:128\u2013140, 2018. [17] Ioannis Mavromatis, Andrea Tassi, Robert J. Piechocki, and Andrew Nix. Efficient v2v communication scheme for 5g mmwave hyper-connected cavs. IEEE International Conference on Communications, 2018. [18] I. Chatzigeorgiou and A. Tassi. Decoding delay performance of random linear network coding for broadcast. IEEE Transactions on Vehicular Technology, 66(8):7050\u2013 7060, 2017. [19] Ibrar Yaqoob, Iftikhar Ahmad, Ejaz Ahmed, Abdullah Gani, Muhammad Imran, and Nadra Guizani. Overcoming the key challenges to establishing vehicular communication: Is sdn the answer? IEEE Communications Magazine, 55(7):128\u2013134, 2017. [20] N. Lu, N. Cheng, N. Zhang, X. Shen, and J. W. Mark. Connected vehicles: Solutions and challenges. IEEE Internet of Things Journal, 1(4):289\u2013299, 2014. [21] Robert W. Irving and Sandy Scott. The stable fixtures problem\u2014a many-to-many extension of stable roommates. Discrete Applied Mathematics, 155(16):2118\u2013 2129, 2007. URL http://www.sciencedirect.com/science/ article/pii/S0166218X07001552. [22] X. Chen, Y. Miao, M. Jin, and Q. Zhang. Driving decision-making analysis of lane-changing for autonomous vehicle under complex urban environment. In 2017 29th Chinese Control And Decision Conference (CCDC), pages 6878\u20136883, 2017. [23] T. A. S. S. International. Simulation of adas and active safety, 2018. Accessed Online:. URL https://tass.plm. automation.siemens.com/prescan. [24] Gasp \u0301 ar P \u0301 eter, Szalay Zsolt, and Aradi Szil \u0301 ard. Highly \u0301 automated vehicle systems, 2014. URL http : / / www.mogi.bme.hu/TAMOP/jarmurendszerek iranyitasa angol/book.html. [25] Yasir Mehmood, Noman Haider, Muhammad Imran, Andreas Timm-Giel, and Mohsen Guizani. M2m communications in 5g: State-of-the-art architecture, recent advances, and research challenges. IEEE Communications Magazine, 55(9):194\u2013201, 2017. [26] Kyungmin Park and Esmael Dinan. Traffic control and everything-to-vehicle..",
              "url": "https://openalex.org/W2921698889",
              "openalex_id": "https://openalex.org/W2921698889",
              "title": "5G Communication: An Overview of Vehicle-to-Everything, Drones, and Healthcare Use-Cases",
              "publication_date": "2019-01-01"
            },
            {
              "id": "S2585677171",
              "text": "Regulatory challenges for UAV integration in 5G networks include the need for standardized communication protocols and security measures to protect against cyber threats, which are critical for ensuring safe and reliable UAV operations.",
              "children": [
                {
                  "id": "E4933323753",
                  "text": "... 5, pp. 4278\u20134291, 2019. [113] L. Xie, Y. Ding, H. Yang, and X. Wang, \u201cBlockchain-based secure and trustworthy internet of things in sdn-enabled 5g-vanets,\u201d IEEE Access, vol. 7, pp. 56 656\u201356 666, 2019. [114] B. Bera, S. Saha, A. K. Das, N. Kumar, P. Lorenz, and M. Alazab, \u201cBlockchain-envisioned secure data delivery and collection scheme for 5g-based iot-enabled internet of drones environment,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9097\u20139111, 2020. [115] M. Aloqaily, O. Bouachir, A. Boukerche, and I. Al Ridhawi, \u201cDesign guidelines for blockchain-assisted 5g-uav networks,\u201d IEEE Network, vol. 35, no. 1, pp. 64\u201371, 2021. [116] R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain and 5g integrated softwarized uav network management: Architecture, solutions, and challenges,\u201d Physical Communication, vol. 47, p. 101355, 2021. [117] X. Jian, P. Leng, Y. Wang, M. Alrashoud, and M. S. Hossain, \u201cBlockchain-empowered trusted networking for unmanned aerial vehicles in the b5g era,\u201d IEEE Network, vol. 35, no. 1, pp. 72\u201377, 2021. [118] C. Feng, K. Yu, A. K. Bashir, Y. D. Al-Otaibi, Y. Lu, S. Chen, and D. Zhang, \u201cEfficient and secure data sharing for 5g flying drones: a blockchain-enabled approach,\u201d IEEE Network, vol. 35, no. 1, pp. 130\u2013 137, 2021. [119] B. Ghimire, D. B. Rawat, C. Liu, and J. Li, \u201cSharding-enabled blockchain for software-defined internet of unmanned vehicles in the battlefield,\u201d IEEE Network, vol. 35, no. 1, pp. 101\u2013107, 2021. 32 [120] A. Gumaei, M. Al-Rakhami, M. M. Hassan, P. Pace, G. Alai, K. Lin, and G. Fortino, \u201cDeep learning and blockchain with edge computing for 5g-enabled drone identification and flight mode detection,\u201d IEEE Network, vol. 35, no. 1, pp. 94\u2013100, 2021. [121] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, \u201cFederated learning meets blockchain in edge computing: Opportunities and challenges,\u201d IEEE Internet of Things Journal, vol. 8, no. 16, pp. 12 806\u201312 825, 2021. [122] P. Kumar, R. Kumar, G. Srivastava, G. P. Gupta, R. Tripathi, T. R. Gadekallu, and N. Xiong, \u201cPpsf: A privacy-preserving and secure framework using blockchain-based machine-learning for iot-driven smart cities,\u201d IEEE Transactions on Network Science and Engineering, 2021. [123] S. Bouraga, \u201cA taxonomy of blockchain consensus protocols: A survey and classification framework,\u201d Expert Systems with Applications, vol. 168, p. 114384, 2021. [124] V. Ortega, F. Bouchmal, and J. F. Monserrat, \u201cTrusted 5g vehicular networks: Blockchains and content-centric networking,\u201d IEEE Vehicular Technology Magazine, vol. 13, no. 2, pp. 121\u2013127, 2018. [125] R. Shrestha, S. Y. Nam, R. Bajracharya, and S. Kim, \u201cEvolution of v2x communication and integration of blockchain for security enhancements,\u201d Electronics, vol. 9, no. 9, p. 1338, 2020. [126] S. Rahmadika, K. Lee, and K.-H. Rhee, \u201cBlockchain-enabled 5g autonomous vehicular networks,\u201d in 2019 International Conference on Sustainable Engineering and Creative Computing (ICSECC). IEEE, 2019, pp. 275\u2013280. [127] D. Reebadiya, T. Rathod, R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain-based secure and intelligent sensing scheme for autonomous vehicles activity tracking beyond 5g networks,\u201d Peer-to-Peer Networking and Applications, pp. 1\u201318, 2021. [128] L. Nkenyereye, B. Adhi Tama, M. K. Shahzad, and Y.-H. Choi, \u201cSecure and blockchain-based emergency driven message protocol for 5g enabled vehicular edge computing,\u201d Sensors, vol. 20, no. 1, p. 154, 2020. [129] J. Chen, W. Wang, Y. Zhou, S. H. Ahmed, and W. Wei, \u201cExploiting 5g and blockchain for medical applications of drones,\u201d IEEE Network, vol. 35, no. 1, pp. 30\u201336, 2021. [130] G. Kakkavas, A. Stamou, V. Karyotis, and S. Papavassiliou, \u201cNetwork tomography for efficient monitoring in sdn-enabled 5g networks and beyond: Challenges and opportunities,\u201d IEEE Communications Magazine, vol. 59, no. 3, pp. 70\u201376, 2021. [131] C. Benzaid and T. Taleb, \u201cAi-driven zero touch network and service management in 5g and beyond: Challenges and research directions,\u201d IEEE Network, vol. 34, no. 2, pp. 186\u2013194, 2020. [132] M. Liyanage, Q.-V. Pham, K. Dev, S. Bhattacharya, P. K. R. Maddikunta, T. R. Gadekallu, and G. Yenduri, \u201cA survey on zero touch network and service (zsm) management for 5g and beyond networks,\u201d Journal of Network and Computer Applications, p. 103362, 2022. [133] J. Cui, L. S. Liew, G. Sabaliauskaite, and F. Zhou, \u201cA review on safety failures, security attacks, and available countermeasures for autonomous vehicles,\u201d Ad Hoc Networks, vol. 90, p. 101823, 2019. [134] A. M. Malla and R. K. Sahu, \u201cSecurity attacks with an effective solution for dos attacks in vanet,\u201d International Journal of Computer Applications, vol. 66, no. 22, 2013. [135] S. S. Manvi and S. Tangade, \u201cA survey on authentication schemes in vanets for secured communication,\u201d Vehicular Communications, vol. 9, pp. 19\u201330, 2017. [136] L. B. Othmane, H. Weffers, M. M. Mohamad, and M. Wolf, \u201cA survey of security and privacy in connected vehicles,\u201d in Wireless sensor and mobile ad-hoc networks. Springer, 2015, pp. 217\u2013247. [137] S. Zhang, Y. Lin, Q. Liu, J. Jiang, B. Yin, and K.-K. R. Choo, \u201cSecure hitch in location based social networks,\u201d Computer Communications, vol. 100, pp. 65\u201377, 2017. [138] M. Raya and J.-P. Hubaux, \u201cSecuring vehicular ad hoc networks,\u201d Journal of computer security, vol. 15, no. 1, pp. 39\u201368, 2007. [139] J. Petit, B. Stottelaar, M. Feiri, and F. Kargl, \u201cRemote attacks on automated vehicles sensors: Experiments on camera and lidar,\u201d Black Hat Europe, vol. 11, no. 2015, p. 995, 2015. [140] B. G. Stottelaar, \u201cPractical cyber-attacks on autonomous vehicles,\u201d Master\u2019s thesis, University of Twente, 2015. [141] R. Chauhan, A platform for false data injection in frequency modulated continuous wave radar. Utah State University, 2014. [142] W. E. Buehler, R. M. Whitson, and M. J. Lewis, \u201cAirborne radar jamming system,\u201d Sep. 9 2014, uS Patent 8,830,112. [143] N. Fairfield and C. Urmson, \u201cTraffic light mapping and detection,\u201d in 2011 IEEE International Conference on Robotics and Automation. IEEE, 2011, pp. 5421\u20135426. [144] A. B. Hillel, R. Lerner, D. Levi, and G. Raz, \u201cRecent progress in road and lane detection: a survey,\u201d Machine vision and applications, vol. 25, no. 3, pp. 727\u2013745, 2014. [145] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, \u201cPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8445\u20138453. [146] T. B. Brown, D. Mane, A. Roy, M. Abadi, and J. Gilmer, \u201cAdversarial \u0301 patch,\u201d arXiv preprint arXiv:1712.09665,..",
                  "url": "https://openalex.org/W4310130661",
                  "openalex_id": "https://openalex.org/W4310130661",
                  "title": "Autonomous vehicles in 5G and beyond: A survey",
                  "publication_date": "2023-02-01"
                },
                {
                  "id": "E2138630072",
                  "text": "... Furthermore, energy consumption and throughput of UAV is optimized. Based on the priorities given, efficiency of data and computing services are also optimized. Bockelmann et al. [44] have designed a FP7 METIS project using 5G system concept in which well-suitable chase for high data rates are summarized by the term \u201dExtreme Mobile Broadband\u201d (xMBB). The overhead seen in exchange of messages required before the transmission of data payload affects the energy efficiency of MTC devices. Hence, less frequent and shorter transmissions preserves energy. Therefore MAC protocols coupled with Physical layer approaches enable devices with long battery lives. In [51], orthogonal frequency division multiplexing with index modulation is proposed to improve inter carrier interference caused by asynchronous transmission for uncoordinated mMTC networks. Data transmission is performed by the indices of active subcarriers. Subcarrier Mapping Scheme called inner subcarrier activation scheme is proposed to further improve the interference of adjacent user in asynchronous systems. In [52], proposed a novel 5G based network architecture called Non-Orthogonal Multiple Access (NOMA) to reduce the latency and reliability requirements in V2X communication. One of the critical challenge for next-generation wireless communication is to satisfy the high demand for mMTC systems which performs a random transmission between base station and machine users. Therefore, the lack of coordination between the base station causes inter-carrier interference. Researchers are facing a big challenge in providing the services for asynchronous massive machine users. Activation ration and optimization of subblock size can be evaluated for clustering users and conflicting users with respect to their requirements. Autonomous vehicle in reality faces various challenges like real-time data analytics, software heterogeneity, validation, verification and latency issues. Aforementioned challenges, researchers across the globe are trying to provide solutions using 5G-based testbeds. Summary: V2X and 5G connections helps AV to visualize the objects and obstacles around corners and beyond. Connectivity between the cars and infrastructure provides awareness ahead among the vehicles like reduce the speed automatically in slow-moving traffic areas. By the time when it reaches the signal, traffic would have been cleared thereby reducing the waiting time on the traffic signal. \u2019Traffic Light Information\u2019 is a classic case study based on V2I initiated by Audi in Europe. All these scenarios requires good connectivity at the speed of light. This can be achieved by providing 5G network that enables fast processing and prior decision making.To summarize, AV would become reality with the application of 5G technologies. E. eMBB 1) Introduction: eMBB service is used particularly to enhance the Quality of Experience(QoE) in bandwidth for invehicle applications. 2) Existing Challenges/Limitations: One of the significant way in 5G to deliver wireless broadband to previously unreached areas is through the technology Fixed Wireless Access (FWA) network. Globally, FWA has gained its importance in developed and developing countries and is expected to expand exponentially from 2018-2025. This FWA creates a platform for eMBB for wide coverage using higher-spectrum bands. Some of the use cases of eMBB are as follows: Broadband everywhere: FWA technology can provide wide coverage globally with minimum speed of 50 Mbps. Public transportation: Broadband access used in high-speed trains and public transport systems Hot spots: Enhancing broadband coverage in densely populated areas and in high rise buildings Large-scale events: Enabling high speed of broadband data where thousands of people are gathered in one place for any kind of big event. Smart offices: Delivering high-bandwidth of data connections to thousands of users even in the environment with heavy data traffic. Enhanced Multimedia: Provides high quality video streaming and real-time content over wide coverage areas. 3) How B5G help (with Related work): In [42], authors have addressed the problem of resource scheduling problem in eMBB traffics. Initially the resource blocks are assigned to the beginning of each time slot based on the channel state and the previous average data rate upto the current time slot of each eMBB users. Two dimensional Hopfield Neural Network and the energy function is used to solve the resource allocation problem. Then, chance constraint problem is applied to maximize the eMBB data rates. The authors in [53] targets more number of users in data transmission in AV. Non-Orthogonal Multiple Access (NOMA) is proposed to optimize the distribution in unicast/multicast scenarios. The complexity of the algorithm is measured and compared using Time Division Multiplexing (TDM). In order to reduce the complexity of the algorithm, two solutions are proposed. First, the numbers of injection levels are reduced and second one is choosing the smart algorithm that selects the optimum injection levels. In [43], authors have considered eMBB and URLLC to be the important prerequisites of smart intelligent transportation systems. The eMBB traffic is scheduled at the boundary of each time slot for data transmissions. During the transmission interval, random arrivals of URLLC traffics are allowed. Summary: URLLC and mMTC works together with eMBB to fulfill the needs in new wireless networks to provide the facilities in applications like healthcare, manufacturing, military and emergency response. These three features in 5G provides the solutions for the issues with respect to bandwidth, density and latency in applications with restricted LTE\u2019s capabilities like autonomous vehicles, smart city and augmented reality. IV. IMPACT OF 5G AND B5G TECHNOLOGIES ON AV In this section we discuss the impact of some of the prominent technologies in 5G and B5G on AV. 13 TABLE VII BENEFITS AND CHALLENGES OF TECHNICAL ASPECTS OF AUTONOMOUS VEHICLES. Application Existing Challenges Solutions Limitations Navigation and Path Planning [27]\u2013[35] 1) Autonomous planning and navigation easily plunge into the local optimum in the complex scenarios and thus fall into traps, resulting in a low probability of finding a reasonable route to the target 2) conventional algorithms cannot plan the environment completely in advance 3) path planning for unstructured roads are very complex. 1) Deep learning and Deep reinforcement learning is used to achieve endto-end learning for complex tasks 2) Gradually transformation of low-level feature representations into highlevel feature representations through a multilayer process is implemented 3) Free-form navigation like graph based search algorithm can be used to plan for navigation in unstructured roads 4) V2X shares intent, accurate and fast information to perform higher level of prediction 5) V2X wireless sensor visualize 360 degree and sense non-line sight and wide range of area 6) Precise positioning of location and dynamic decision making are faster and accurate using 5G technologies enabled with AI 1) Detecting and localizing the object at specific time and adjusting the path dynamically has to be still optimal and faster 2) Constructing the maps from the inputs received from the sensors and devices is a time consuming task 3) Fails to provide accurate predictions in poor climatic conditions 4) Unable to predict the agent behaviour on roads Object Detection/ Collision Avoidance [36]\u2013[39] 1) Massive volumes of data are generated by autonomous vehicles. In terms of latency and security, delivering cloud services to autonomous vehicles is seen as a significant challenge 2) Localization is one of the most major aspects of autonomous vehicles for avoiding collisions and ensuring safe navigation 3) The reliability of high-end sensors is limited over longer distances or when a vehicle enters a low-visibility region 1) To address the latency issue in 5G-enabled vehicular networks, a software-defined networking architecture at the network edge is being constructed. 2) Gaussian filter techniques are used to reduce granularity in order to estimate the correct lane with greater accuracy 3) 5G communication technology for CL provides an accurate vehicle position 4) Laplacian Graph Processing provides faster response time and a higher GPS accuracy rate 5) Roadside equipment is used to improve data sharing between automobiles 1) The lane detection algorithm performs tasks such as vehicle location and path detection with a moderate accuracy rate 2) AV share information with signals and act adaptively according to the situation, whereas humans in manual vehicles act more appropriately based on the scenario 3) Certain risks are inherent in data sharing, such as when a malicious vehicle sends fake data in order to manipulate the receivers, or when faulty sensors communicate incorrect data 4) If the AV trust the data provided by the malicious vehicle, they will be trapped, causing them to switch lanes or accelerate faster. As a result, there may be serious threats to human life URLLC [40]\u2013 [46], [48]\u2013[52] 1) Onboard processing is not sufficient to store massive amount of data generated from sensors and other devices. 2) Connecting the vehicles based on cloud computing provides high latency which is not suitable AV 3) Providing communication with low latency and high reliability is a complex task in dynamic real time process which is connected to different devices. 1) Resource scheduling problem is solved using puncturing approach 2) To maintain the latency constraint, URLLC are considered inside the minislot of each eMBB time slot 3) To ensure the reliability, guard zones are deployed around the vehicle. 4) 5G can make instantaneous communication with URLLC 5) The time delay using 5G is 1 to 5 millisecond when compared to 20 milliseconds in 4G 6) Reduced time delay in AV provides users with safety information 7) One slice in network slicing can provide high reliability and security for URLLC 1) Resource scheduling should be considered for both uplink and downlink scenario for URLLC 2) Guard zone based URLLC scheduling policy is compared with the baseline policy where the guard zone receiver is not applied mMTC/ V2V communication/ V2X communication [44], [44]\u2013[52] 1) A common framework for Machine type communication must be available 2) Current link adaption mechanisms are not suitable for MTC 3) Resource allocation and channel coding schemes are unsuitable for small packets 1) optimization algorithms are used for trajectory planning which in turn minimizes the power consumption of MTC 2) Extreme mobile broadband is used for high data rates 3) less frequent and shorter transmissions preserves energy. So MAC protocols with physical layer approaches ensure device with long battery life 4) NOMA architecture is used to reduce the latency and reliability requirements in V2X 1)..",
                  "url": "https://openalex.org/W4310130661",
                  "openalex_id": "https://openalex.org/W4310130661",
                  "title": "Autonomous vehicles in 5G and beyond: A survey",
                  "publication_date": "2023-02-01"
                }
              ]
            }
          ]
        },
        "S8421165048": {
          "id": "S8421165048",
          "text": "The integration of unmanned aerial vehicles (UAVs) in advanced communication networks, such as 5G, necessitates robust regulatory frameworks to address issues of security, privacy, and operational safety, particularly in dynamic environments.",
          "children": [
            {
              "id": "E5490018945",
              "text": "..cellular communication. \u2022 Security and privacy problems such as eavesdropping, jamming and spoofing in aerial networks is another challenge in UAV communication. Lightweight techniques and AI solutions can be used to address these issues. \u2022 Mobility support and collision avoidance in terms of Internet of UAVs are some of the other challenges that need to be researched in the future. \u2022 Finally, in terms of healthcare, online consultation, robotic surgery and telemedicine requires a network that can support high-definition video streaming. In such cases ultra-low latency of less than 10ms and ultrahigh reliable connection are required. Latency of less than 10ms is a critical challenge and an open research problem in future cellular communication. \u2022 Reliable and real-time remote monitoring requires a high speed network and a reliable connection, otherwise practitioners will be unable to get the real-time data that can be used for quick and on-time healthcare decisions. Providing higher bandwidth and reliable connections are still an open problem that need to be addressed in future cellular networks. \u2022 Integration of AI techniques for potential diagnosis (in the healthcare sector) that can lead to the best treatment plan is also a research area that needs to be explored. VII. CONCLUSIONS In this paper, we have explored 5G use-cases emphasising on three main case studies: V2X communication, Drones, and Healthcare in order to identify the most challenging use-case for our future work. Starting with V2X communication, we explored V2V, V2P, V2I, and IV Communication along with their application areas. We also addressed the limitations and challenges of the existing communication technologies and proposed solutions based on 5G technologies. We further studied drone communication for mission critical infrastructures and provide a comparative analysis of 4G (LTE/LTE-A) and 5G with respect to control latency. Moreover, we examined healthcare with respect to 4G and 5G technologies and provide an overview of online consultation, online health monitoring, remote diagnosis, and mobile robotic surgery. Based on the literature review and critical analysis, we have concluded that drone communication for the provision and maintenance of critical infrastructures is the most challenging scenario that can be carried to the next phase of our work. In the future, we are planning to extend this work and propose to identify tools in which we can simulate our initial models in order to validate them and check their applicability for real-time applications. We also look forward to implementing the model in a real-time testbed. The potential impact on standard bodies along with community benefits is also part of our future plans. ACKNOWLEDGMENT This research work is supported by BTIIC (BT Ireland Innovation Centre), funded by BT and Invest Northern Ireland. REFERENCES [1] Ericsson, Huawei, and Qualcomm. The road to 5g: Drivers, applications, requirements and technical development. Technical report, Global mobile Suppliers Association (GSA), 2015. [2] Alvaro Neira. 5g: a network of opportunities and chal- \u0301 lenges. Technical report, Axon Partners Group, 2017. [3] S. A. A. Shah, E. Ahmed, M. Imran, and S. Zeadally. 5g for vehicular communications. IEEE Communications Magazine, 56(1):111\u2013117, Jan 2018. ISSN 0163-6804. [4] Sana Ullah, Ki-Il Kim, Kyong-Hoon Kim, Muhammad Imran, Pervez Khan, Eduardo Tovar, and Farman Ali. Uav-enabled healthcare architecture: Issues and challenges. Future Generation Computer Systems, 2019. ISSN 0167-739X. doi: https://doi.org/10.1016/j.future. 2019. 01. 028. URL http://www. sciencedirect. com/ science/article/pii/S0167739X18318247. [5] Nokia. 5g use cases and requirements. Technical report, Nokia, 2016. 2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2019.2905347, IEEE Access 16 [6] Barbara Masini, Alessandro Bazzi, and Alberto Zanella. A survey on the roadmap to mandate on board connectivity and enable v2v-based vehicular sensor networks. Sensors, 18(7):2207, 2018. [7] M. Boban, A. Kousaridas, K. Manolakis, J. Eichinger, and W. Xu. Connected roads of the future: Use cases, requirements, and design considerations for vehicle-toeverything communications. IEEE Vehicular Technology Magazine, 13(3):110\u2013123, 2018. [8] Hazim Shakhatreh, Ahmad Sawalmeh, Ala Al-Fuqaha, Zuochao Dou, Eyad Almaita, Issa Khalil, Noor Shamsiah Othman, Abdallah Khreishah, and Mohsen Guizani. Unmanned aerial vehicles: A survey on civil applications and key research challenges. arXiv preprint arXiv:1805.00881, 2018. [9] Bin Li, Zesong Fei, and Yan Zhang. Uav communications for 5g and beyond: Recent advances and future trends. IEEE Internet of Things Journal, 2018. [10] Darrell M. West. How 5G technology enables the health internet of things. Technical Report July 2016, 2016. URL https://www.brookings.edu/research/how-5gtechnology-enables-the-health-internet-of-things/. [11] Siddique Latif, Junaid Qadir, Shahzad Farooq, and Muhammad Ali Imran. How 5G wireless (and Concomitant Technologies) will revolutionize healthcare? Future Internet, 9(4):1\u201310, 2017. ISSN 19995903. [12] National Center for Statistics Analysis. Summary of motor vehicle crashes: 2016 data. (traffic safety facts. report no. dot hs 812 580). Technical report, National Highway Traffic Safety Administration, Washington, DC, August, 2018. [13] Department for Transport. Reported road casualties great britain, provisional estimates: April to june 2017, 2017. URL https://assets.publishing.service.gov.uk/government/ uploads / system / uploads / attachment data / file / 648081 / rrcgb2016-01.pdf. [14] Texas A&M. Transportation Institute INRIX. 2015 urban mobility scorecard. Technical report, August, 2015. [15] Mark Patrick. V2x communications \u2013 lte versus dsrc. accessed online:. URL http://www.mwee.com/designcenter/v2x-communications-lte-versus-dsrc. [16] Iftikhar Ahmad, Rafidah Md Noor, Ismail Ahmedy, Syed Adeel Ali Shah, Ibrar Yaqoob, Ejaz Ahmed, and Muhammad Imran. Vanet\u2013lte based heterogeneous vehicular clustering for driving assistance and route planning applications. Computer Networks, 145:128\u2013140, 2018. [17] Ioannis Mavromatis, Andrea Tassi, Robert J. Piechocki, and Andrew Nix. Efficient v2v communication scheme for 5g mmwave hyper-connected cavs. IEEE International Conference on Communications, 2018. [18] I. Chatzigeorgiou and A. Tassi. Decoding delay performance of random linear network coding for broadcast. IEEE Transactions on Vehicular Technology, 66(8):7050\u2013 7060, 2017. [19] Ibrar Yaqoob, Iftikhar Ahmad, Ejaz Ahmed, Abdullah Gani, Muhammad Imran, and Nadra Guizani. Overcoming the key challenges to establishing vehicular communication: Is sdn the answer? IEEE Communications Magazine, 55(7):128\u2013134, 2017. [20] N. Lu, N. Cheng, N. Zhang, X. Shen, and J. W. Mark. Connected vehicles: Solutions and challenges. IEEE Internet of Things Journal, 1(4):289\u2013299, 2014. [21] Robert W. Irving and Sandy Scott. The stable fixtures problem\u2014a many-to-many extension of stable roommates. Discrete Applied Mathematics, 155(16):2118\u2013 2129, 2007. URL http://www.sciencedirect.com/science/ article/pii/S0166218X07001552. [22] X. Chen, Y. Miao, M. Jin, and Q. Zhang. Driving decision-making analysis of lane-changing for autonomous vehicle under complex urban environment. In 2017 29th Chinese Control And Decision Conference (CCDC), pages 6878\u20136883, 2017. [23] T. A. S. S. International. Simulation of adas and active safety, 2018. Accessed Online:. URL https://tass.plm. automation.siemens.com/prescan. [24] Gasp \u0301 ar P \u0301 eter, Szalay Zsolt, and Aradi Szil \u0301 ard. Highly \u0301 automated vehicle systems, 2014. URL http : / / www.mogi.bme.hu/TAMOP/jarmurendszerek iranyitasa angol/book.html. [25] Yasir Mehmood, Noman Haider, Muhammad Imran, Andreas Timm-Giel, and Mohsen Guizani. M2m communications in 5g: State-of-the-art architecture, recent advances, and research challenges. IEEE Communications Magazine, 55(9):194\u2013201, 2017. [26] Kyungmin Park and Esmael Dinan. Traffic control and everything-to-vehicle..",
              "url": "https://openalex.org/W2921698889",
              "openalex_id": "https://openalex.org/W2921698889",
              "title": "5G Communication: An Overview of Vehicle-to-Everything, Drones, and Healthcare Use-Cases",
              "publication_date": "2019-01-01"
            },
            {
              "id": "S2585677171",
              "text": "Regulatory challenges for UAV integration in 5G networks include the need for standardized communication protocols and security measures to protect against cyber threats, which are critical for ensuring safe and reliable UAV operations.",
              "children": [
                {
                  "id": "E4933323753",
                  "text": "... 5, pp. 4278\u20134291, 2019. [113] L. Xie, Y. Ding, H. Yang, and X. Wang, \u201cBlockchain-based secure and trustworthy internet of things in sdn-enabled 5g-vanets,\u201d IEEE Access, vol. 7, pp. 56 656\u201356 666, 2019. [114] B. Bera, S. Saha, A. K. Das, N. Kumar, P. Lorenz, and M. Alazab, \u201cBlockchain-envisioned secure data delivery and collection scheme for 5g-based iot-enabled internet of drones environment,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9097\u20139111, 2020. [115] M. Aloqaily, O. Bouachir, A. Boukerche, and I. Al Ridhawi, \u201cDesign guidelines for blockchain-assisted 5g-uav networks,\u201d IEEE Network, vol. 35, no. 1, pp. 64\u201371, 2021. [116] R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain and 5g integrated softwarized uav network management: Architecture, solutions, and challenges,\u201d Physical Communication, vol. 47, p. 101355, 2021. [117] X. Jian, P. Leng, Y. Wang, M. Alrashoud, and M. S. Hossain, \u201cBlockchain-empowered trusted networking for unmanned aerial vehicles in the b5g era,\u201d IEEE Network, vol. 35, no. 1, pp. 72\u201377, 2021. [118] C. Feng, K. Yu, A. K. Bashir, Y. D. Al-Otaibi, Y. Lu, S. Chen, and D. Zhang, \u201cEfficient and secure data sharing for 5g flying drones: a blockchain-enabled approach,\u201d IEEE Network, vol. 35, no. 1, pp. 130\u2013 137, 2021. [119] B. Ghimire, D. B. Rawat, C. Liu, and J. Li, \u201cSharding-enabled blockchain for software-defined internet of unmanned vehicles in the battlefield,\u201d IEEE Network, vol. 35, no. 1, pp. 101\u2013107, 2021. 32 [120] A. Gumaei, M. Al-Rakhami, M. M. Hassan, P. Pace, G. Alai, K. Lin, and G. Fortino, \u201cDeep learning and blockchain with edge computing for 5g-enabled drone identification and flight mode detection,\u201d IEEE Network, vol. 35, no. 1, pp. 94\u2013100, 2021. [121] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, \u201cFederated learning meets blockchain in edge computing: Opportunities and challenges,\u201d IEEE Internet of Things Journal, vol. 8, no. 16, pp. 12 806\u201312 825, 2021. [122] P. Kumar, R. Kumar, G. Srivastava, G. P. Gupta, R. Tripathi, T. R. Gadekallu, and N. Xiong, \u201cPpsf: A privacy-preserving and secure framework using blockchain-based machine-learning for iot-driven smart cities,\u201d IEEE Transactions on Network Science and Engineering, 2021. [123] S. Bouraga, \u201cA taxonomy of blockchain consensus protocols: A survey and classification framework,\u201d Expert Systems with Applications, vol. 168, p. 114384, 2021. [124] V. Ortega, F. Bouchmal, and J. F. Monserrat, \u201cTrusted 5g vehicular networks: Blockchains and content-centric networking,\u201d IEEE Vehicular Technology Magazine, vol. 13, no. 2, pp. 121\u2013127, 2018. [125] R. Shrestha, S. Y. Nam, R. Bajracharya, and S. Kim, \u201cEvolution of v2x communication and integration of blockchain for security enhancements,\u201d Electronics, vol. 9, no. 9, p. 1338, 2020. [126] S. Rahmadika, K. Lee, and K.-H. Rhee, \u201cBlockchain-enabled 5g autonomous vehicular networks,\u201d in 2019 International Conference on Sustainable Engineering and Creative Computing (ICSECC). IEEE, 2019, pp. 275\u2013280. [127] D. Reebadiya, T. Rathod, R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain-based secure and intelligent sensing scheme for autonomous vehicles activity tracking beyond 5g networks,\u201d Peer-to-Peer Networking and Applications, pp. 1\u201318, 2021. [128] L. Nkenyereye, B. Adhi Tama, M. K. Shahzad, and Y.-H. Choi, \u201cSecure and blockchain-based emergency driven message protocol for 5g enabled vehicular edge computing,\u201d Sensors, vol. 20, no. 1, p. 154, 2020. [129] J. Chen, W. Wang, Y. Zhou, S. H. Ahmed, and W. Wei, \u201cExploiting 5g and blockchain for medical applications of drones,\u201d IEEE Network, vol. 35, no. 1, pp. 30\u201336, 2021. [130] G. Kakkavas, A. Stamou, V. Karyotis, and S. Papavassiliou, \u201cNetwork tomography for efficient monitoring in sdn-enabled 5g networks and beyond: Challenges and opportunities,\u201d IEEE Communications Magazine, vol. 59, no. 3, pp. 70\u201376, 2021. [131] C. Benzaid and T. Taleb, \u201cAi-driven zero touch network and service management in 5g and beyond: Challenges and research directions,\u201d IEEE Network, vol. 34, no. 2, pp. 186\u2013194, 2020. [132] M. Liyanage, Q.-V. Pham, K. Dev, S. Bhattacharya, P. K. R. Maddikunta, T. R. Gadekallu, and G. Yenduri, \u201cA survey on zero touch network and service (zsm) management for 5g and beyond networks,\u201d Journal of Network and Computer Applications, p. 103362, 2022. [133] J. Cui, L. S. Liew, G. Sabaliauskaite, and F. Zhou, \u201cA review on safety failures, security attacks, and available countermeasures for autonomous vehicles,\u201d Ad Hoc Networks, vol. 90, p. 101823, 2019. [134] A. M. Malla and R. K. Sahu, \u201cSecurity attacks with an effective solution for dos attacks in vanet,\u201d International Journal of Computer Applications, vol. 66, no. 22, 2013. [135] S. S. Manvi and S. Tangade, \u201cA survey on authentication schemes in vanets for secured communication,\u201d Vehicular Communications, vol. 9, pp. 19\u201330, 2017. [136] L. B. Othmane, H. Weffers, M. M. Mohamad, and M. Wolf, \u201cA survey of security and privacy in connected vehicles,\u201d in Wireless sensor and mobile ad-hoc networks. Springer, 2015, pp. 217\u2013247. [137] S. Zhang, Y. Lin, Q. Liu, J. Jiang, B. Yin, and K.-K. R. Choo, \u201cSecure hitch in location based social networks,\u201d Computer Communications, vol. 100, pp. 65\u201377, 2017. [138] M. Raya and J.-P. Hubaux, \u201cSecuring vehicular ad hoc networks,\u201d Journal of computer security, vol. 15, no. 1, pp. 39\u201368, 2007. [139] J. Petit, B. Stottelaar, M. Feiri, and F. Kargl, \u201cRemote attacks on automated vehicles sensors: Experiments on camera and lidar,\u201d Black Hat Europe, vol. 11, no. 2015, p. 995, 2015. [140] B. G. Stottelaar, \u201cPractical cyber-attacks on autonomous vehicles,\u201d Master\u2019s thesis, University of Twente, 2015. [141] R. Chauhan, A platform for false data injection in frequency modulated continuous wave radar. Utah State University, 2014. [142] W. E. Buehler, R. M. Whitson, and M. J. Lewis, \u201cAirborne radar jamming system,\u201d Sep. 9 2014, uS Patent 8,830,112. [143] N. Fairfield and C. Urmson, \u201cTraffic light mapping and detection,\u201d in 2011 IEEE International Conference on Robotics and Automation. IEEE, 2011, pp. 5421\u20135426. [144] A. B. Hillel, R. Lerner, D. Levi, and G. Raz, \u201cRecent progress in road and lane detection: a survey,\u201d Machine vision and applications, vol. 25, no. 3, pp. 727\u2013745, 2014. [145] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Q. Weinberger, \u201cPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8445\u20138453. [146] T. B. Brown, D. Mane, A. Roy, M. Abadi, and J. Gilmer, \u201cAdversarial \u0301 patch,\u201d arXiv preprint arXiv:1712.09665,..",
                  "url": "https://openalex.org/W4310130661",
                  "openalex_id": "https://openalex.org/W4310130661",
                  "title": "Autonomous vehicles in 5G and beyond: A survey",
                  "publication_date": "2023-02-01"
                },
                {
                  "id": "E2138630072",
                  "text": "... Furthermore, energy consumption and throughput of UAV is optimized. Based on the priorities given, efficiency of data and computing services are also optimized. Bockelmann et al. [44] have designed a FP7 METIS project using 5G system concept in which well-suitable chase for high data rates are summarized by the term \u201dExtreme Mobile Broadband\u201d (xMBB). The overhead seen in exchange of messages required before the transmission of data payload affects the energy efficiency of MTC devices. Hence, less frequent and shorter transmissions preserves energy. Therefore MAC protocols coupled with Physical layer approaches enable devices with long battery lives. In [51], orthogonal frequency division multiplexing with index modulation is proposed to improve inter carrier interference caused by asynchronous transmission for uncoordinated mMTC networks. Data transmission is performed by the indices of active subcarriers. Subcarrier Mapping Scheme called inner subcarrier activation scheme is proposed to further improve the interference of adjacent user in asynchronous systems. In [52], proposed a novel 5G based network architecture called Non-Orthogonal Multiple Access (NOMA) to reduce the latency and reliability requirements in V2X communication. One of the critical challenge for next-generation wireless communication is to satisfy the high demand for mMTC systems which performs a random transmission between base station and machine users. Therefore, the lack of coordination between the base station causes inter-carrier interference. Researchers are facing a big challenge in providing the services for asynchronous massive machine users. Activation ration and optimization of subblock size can be evaluated for clustering users and conflicting users with respect to their requirements. Autonomous vehicle in reality faces various challenges like real-time data analytics, software heterogeneity, validation, verification and latency issues. Aforementioned challenges, researchers across the globe are trying to provide solutions using 5G-based testbeds. Summary: V2X and 5G connections helps AV to visualize the objects and obstacles around corners and beyond. Connectivity between the cars and infrastructure provides awareness ahead among the vehicles like reduce the speed automatically in slow-moving traffic areas. By the time when it reaches the signal, traffic would have been cleared thereby reducing the waiting time on the traffic signal. \u2019Traffic Light Information\u2019 is a classic case study based on V2I initiated by Audi in Europe. All these scenarios requires good connectivity at the speed of light. This can be achieved by providing 5G network that enables fast processing and prior decision making.To summarize, AV would become reality with the application of 5G technologies. E. eMBB 1) Introduction: eMBB service is used particularly to enhance the Quality of Experience(QoE) in bandwidth for invehicle applications. 2) Existing Challenges/Limitations: One of the significant way in 5G to deliver wireless broadband to previously unreached areas is through the technology Fixed Wireless Access (FWA) network. Globally, FWA has gained its importance in developed and developing countries and is expected to expand exponentially from 2018-2025. This FWA creates a platform for eMBB for wide coverage using higher-spectrum bands. Some of the use cases of eMBB are as follows: Broadband everywhere: FWA technology can provide wide coverage globally with minimum speed of 50 Mbps. Public transportation: Broadband access used in high-speed trains and public transport systems Hot spots: Enhancing broadband coverage in densely populated areas and in high rise buildings Large-scale events: Enabling high speed of broadband data where thousands of people are gathered in one place for any kind of big event. Smart offices: Delivering high-bandwidth of data connections to thousands of users even in the environment with heavy data traffic. Enhanced Multimedia: Provides high quality video streaming and real-time content over wide coverage areas. 3) How B5G help (with Related work): In [42], authors have addressed the problem of resource scheduling problem in eMBB traffics. Initially the resource blocks are assigned to the beginning of each time slot based on the channel state and the previous average data rate upto the current time slot of each eMBB users. Two dimensional Hopfield Neural Network and the energy function is used to solve the resource allocation problem. Then, chance constraint problem is applied to maximize the eMBB data rates. The authors in [53] targets more number of users in data transmission in AV. Non-Orthogonal Multiple Access (NOMA) is proposed to optimize the distribution in unicast/multicast scenarios. The complexity of the algorithm is measured and compared using Time Division Multiplexing (TDM). In order to reduce the complexity of the algorithm, two solutions are proposed. First, the numbers of injection levels are reduced and second one is choosing the smart algorithm that selects the optimum injection levels. In [43], authors have considered eMBB and URLLC to be the important prerequisites of smart intelligent transportation systems. The eMBB traffic is scheduled at the boundary of each time slot for data transmissions. During the transmission interval, random arrivals of URLLC traffics are allowed. Summary: URLLC and mMTC works together with eMBB to fulfill the needs in new wireless networks to provide the facilities in applications like healthcare, manufacturing, military and emergency response. These three features in 5G provides the solutions for the issues with respect to bandwidth, density and latency in applications with restricted LTE\u2019s capabilities like autonomous vehicles, smart city and augmented reality. IV. IMPACT OF 5G AND B5G TECHNOLOGIES ON AV In this section we discuss the impact of some of the prominent technologies in 5G and B5G on AV. 13 TABLE VII BENEFITS AND CHALLENGES OF TECHNICAL ASPECTS OF AUTONOMOUS VEHICLES. Application Existing Challenges Solutions Limitations Navigation and Path Planning [27]\u2013[35] 1) Autonomous planning and navigation easily plunge into the local optimum in the complex scenarios and thus fall into traps, resulting in a low probability of finding a reasonable route to the target 2) conventional algorithms cannot plan the environment completely in advance 3) path planning for unstructured roads are very complex. 1) Deep learning and Deep reinforcement learning is used to achieve endto-end learning for complex tasks 2) Gradually transformation of low-level feature representations into highlevel feature representations through a multilayer process is implemented 3) Free-form navigation like graph based search algorithm can be used to plan for navigation in unstructured roads 4) V2X shares intent, accurate and fast information to perform higher level of prediction 5) V2X wireless sensor visualize 360 degree and sense non-line sight and wide range of area 6) Precise positioning of location and dynamic decision making are faster and accurate using 5G technologies enabled with AI 1) Detecting and localizing the object at specific time and adjusting the path dynamically has to be still optimal and faster 2) Constructing the maps from the inputs received from the sensors and devices is a time consuming task 3) Fails to provide accurate predictions in poor climatic conditions 4) Unable to predict the agent behaviour on roads Object Detection/ Collision Avoidance [36]\u2013[39] 1) Massive volumes of data are generated by autonomous vehicles. In terms of latency and security, delivering cloud services to autonomous vehicles is seen as a significant challenge 2) Localization is one of the most major aspects of autonomous vehicles for avoiding collisions and ensuring safe navigation 3) The reliability of high-end sensors is limited over longer distances or when a vehicle enters a low-visibility region 1) To address the latency issue in 5G-enabled vehicular networks, a software-defined networking architecture at the network edge is being constructed. 2) Gaussian filter techniques are used to reduce granularity in order to estimate the correct lane with greater accuracy 3) 5G communication technology for CL provides an accurate vehicle position 4) Laplacian Graph Processing provides faster response time and a higher GPS accuracy rate 5) Roadside equipment is used to improve data sharing between automobiles 1) The lane detection algorithm performs tasks such as vehicle location and path detection with a moderate accuracy rate 2) AV share information with signals and act adaptively according to the situation, whereas humans in manual vehicles act more appropriately based on the scenario 3) Certain risks are inherent in data sharing, such as when a malicious vehicle sends fake data in order to manipulate the receivers, or when faulty sensors communicate incorrect data 4) If the AV trust the data provided by the malicious vehicle, they will be trapped, causing them to switch lanes or accelerate faster. As a result, there may be serious threats to human life URLLC [40]\u2013 [46], [48]\u2013[52] 1) Onboard processing is not sufficient to store massive amount of data generated from sensors and other devices. 2) Connecting the vehicles based on cloud computing provides high latency which is not suitable AV 3) Providing communication with low latency and high reliability is a complex task in dynamic real time process which is connected to different devices. 1) Resource scheduling problem is solved using puncturing approach 2) To maintain the latency constraint, URLLC are considered inside the minislot of each eMBB time slot 3) To ensure the reliability, guard zones are deployed around the vehicle. 4) 5G can make instantaneous communication with URLLC 5) The time delay using 5G is 1 to 5 millisecond when compared to 20 milliseconds in 4G 6) Reduced time delay in AV provides users with safety information 7) One slice in network slicing can provide high reliability and security for URLLC 1) Resource scheduling should be considered for both uplink and downlink scenario for URLLC 2) Guard zone based URLLC scheduling policy is compared with the baseline policy where the guard zone receiver is not applied mMTC/ V2V communication/ V2X communication [44], [44]\u2013[52] 1) A common framework for Machine type communication must be available 2) Current link adaption mechanisms are not suitable for MTC 3) Resource allocation and channel coding schemes are unsuitable for small packets 1) optimization algorithms are used for trajectory planning which in turn minimizes the power consumption of MTC 2) Extreme mobile broadband is used for high data rates 3) less frequent and shorter transmissions preserves energy. So MAC protocols with physical layer approaches ensure device with long battery life 4) NOMA architecture is used to reduce the latency and reliability requirements in V2X 1)..",
                  "url": "https://openalex.org/W4310130661",
                  "openalex_id": "https://openalex.org/W4310130661",
                  "title": "Autonomous vehicles in 5G and beyond: A survey",
                  "publication_date": "2023-02-01"
                }
              ]
            }
          ]
        },
        "S2259813022": {
          "id": "S2259813022",
          "text": "Future research directions in the integration of UAVs with 5G networks should focus on developing robust security frameworks and addressing the challenges of real-time data processing and management in dynamic environments.",
          "children": [
            {
              "id": "E9557096701",
              "text": "..vol. 5, no. 1, pp. 48\u201354, 2021. [90] H. D. R. Albonda and J. Perez-Romero, \u201cAn efficient ran slicing \u0301 strategy for a heterogeneous network with embb and v2x services,\u201d IEEE access, vol. 7, pp. 44 771\u201344 782, 2019. [91] S. R. Pokhrel, J. Ding, J. Park, O.-S. Park, and J. Choi, \u201cTowards enabling critical mmtc: A review of urllc within mmtc,\u201d IEEE Access, vol. 8, pp. 131 796\u2013131 813, 2020. [92] M. I. Ashraf, C.-F. Liu, M. Bennis, W. Saad, and C. S. Hong, \u201cDynamic resource allocation for optimized latency and reliability in vehicular networks,\u201d IEEE Access, vol. 6, pp. 63 843\u201363 858, 2018. [93] W. Anwar, N. Franchi, and G. Fettweis, \u201cPhysical layer evaluation of v2x communications technologies: 5g nr-v2x, lte-v2x, ieee 802.11 bd, and ieee 802.11 p,\u201d in 2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall). IEEE, 2019, pp. 1\u20137. [94] J. Mei, X. Wang, and K. Zheng, \u201cIntelligent network slicing for v2x services toward 5g,\u201d IEEE Network, vol. 33, no. 6, pp. 196\u2013204, 2019. [95] S. S. Husain, A. Kunz, A. Prasad, E. Pateromichelakis, and K. Samdanis, \u201cUltra-high reliable 5g v2x communications,\u201d IEEE Communications Standards Magazine, vol. 3, no. 2, pp. 46\u201352, 2019. [96] S. A. Ashraf, R. Blasco, H. Do, G. Fodor, C. Zhang, and W. Sun, \u201cSupporting vehicle-to-everything services by 5g new radio release-16 systems,\u201d IEEE Communications Standards Magazine, vol. 4, no. 1, pp. 26\u201332, 2020. [97] K. Ganesan, J. Lohr, P. B. Mallick, A. Kunz, and R. Kuchibhotla, \u201cNr sidelink design overview for advanced v2x service,\u201d IEEE Internet of Things Magazine, vol. 3, no. 1, pp. 26\u201330, 2020. [98] S.-Y. Lien, D.-J. Deng, C.-C. Lin, H.-L. Tsai, T. Chen, C. Guo, and S.-M. Cheng, \u201c3gpp nr sidelink transmissions toward 5g v2x,\u201d IEEE Access, vol. 8, pp. 35 368\u201335 382, 2020. [99] S. Gyawali, S. Xu, Y. Qian, and R. Q. Hu, \u201cChallenges and solutions for cellular based v2x communications,\u201d IEEE Communications Surveys & Tutorials, 2020. [100] S. Chen, J. Hu, Y. Shi, Y. Peng, J. Fang, R. Zhao, and L. Zhao, \u201cVehicle-to-everything (v2x) services supported by lte-based systems and 5g,\u201d IEEE Communications Standards Magazine, vol. 1, no. 2, pp. 70\u201376, 2017. [101] Y. Lu, M. Gerasimenko, R. Kovalchukov, M. Stusek, J. Urama, J. Hosek, M. Valkama, and E. S. Lohan, \u201cFeasibility of locationaware handover for autonomous vehicles in industrial multi-radio environments,\u201d Sensors, vol. 20, no. 21, p. 6290, 2020. [102] I. Rasheed and F. Hu, \u201cIntelligent super-fast vehicle-to-everything 5g communications with predictive switching between mmwave and thz links,\u201d Vehicular Communications, vol. 27, p. 100303, 2021. [103] E. Kampert, C. Schettler, R. Woodman, P. A. Jennings, and M. D. Higgins, \u201cMillimeter-wave communication for a last-mile autonomous transport vehicle,\u201d IEEE Access, vol. 8, pp. 8386\u20138392, 2020. [104] D. M. Manias and A. Shami, \u201cMaking a case for federated learning in the internet of vehicles and intelligent transportation systems,\u201d IEEE Network, vol. 35, no. 3, pp. 88\u201394, 2021. [105] S. Savazzi, M. Nicoli, M. Bennis, S. Kianoush, and L. Barbieri, \u201cOpportunities of federated learning in connected, cooperative, and automated industrial systems,\u201d IEEE Communications Magazine, vol. 59, no. 2, pp. 16\u201321, 2021. [106] Z. Du, C. Wu, T. Yoshinaga, K.-L. A. Yau, Y. Ji, and J. Li, \u201cFederated learning for vehicular internet of things: Recent advances and open issues,\u201d IEEE Open Journal of the Computer Society, vol. 1, pp. 45\u2013 61, 2020. [107] S. R. Pokhrel and J. Choi, \u201cImproving tcp performance over wifi for internet of vehicles: A federated learning approach,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 6, pp. 6798\u20136802, 2020. [108] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, \u201cBlockchain empowered asynchronous federated learning for secure data sharing in internet of vehicles,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 4, pp. 4298\u20134311, 2020. [109] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, \u201cFederated learning for data privacy preservation in vehicular cyber-physical systems,\u201d IEEE Network, vol. 34, no. 3, pp. 50\u201356, 2020. [110] X. Kong, H. Gao, G. Shen, G. Duan, and S. K. Das, \u201cFedvcp: A federated-learning-based cooperative positioning scheme for social internet of vehicles,\u201d IEEE Transactions on Computational Social Systems, 2021. [111] T. R. Gadekallu, Q.-V. Pham, T. Huynh-The, S. Bhattacharya, P. K. R. Maddikunta, and M. Liyanage, \u201cFederated learning for big data: A survey on opportunities, applications, and future directions,\u201d arXiv preprint arXiv:2110.04160, 2021. [112] J. Gao, K. O.-B. O. Agyekum, E. B. Sifah, K. N. Acheampong, Q. Xia, X. Du, M. Guizani, and H. Xia, \u201cA blockchain-sdn-enabled internet of vehicles environment for fog computing and 5g networks,\u201d IEEE Internet of Things Journal, vol. 7, no. 5, pp. 4278\u20134291, 2019. [113] L. Xie, Y. Ding, H. Yang, and X. Wang, \u201cBlockchain-based secure and trustworthy internet of things in sdn-enabled 5g-vanets,\u201d IEEE Access, vol. 7, pp. 56 656\u201356 666, 2019. [114] B. Bera, S. Saha, A. K. Das, N. Kumar, P. Lorenz, and M. Alazab, \u201cBlockchain-envisioned secure data delivery and collection scheme for 5g-based iot-enabled internet of drones environment,\u201d IEEE Transactions on Vehicular Technology, vol. 69, no. 8, pp. 9097\u20139111, 2020. [115] M. Aloqaily, O. Bouachir, A. Boukerche, and I. Al Ridhawi, \u201cDesign guidelines for blockchain-assisted 5g-uav networks,\u201d IEEE Network, vol. 35, no. 1, pp. 64\u201371, 2021. [116] R. Gupta, S. Tanwar, and N. Kumar, \u201cBlockchain and 5g integrated softwarized uav network management: Architecture, solutions, and challenges,\u201d Physical Communication, vol. 47, p. 101355, 2021. [117] X. Jian, P. Leng, Y. Wang, M. Alrashoud, and M. S. Hossain, \u201cBlockchain-empowered trusted networking for unmanned aerial vehicles in the b5g era,\u201d IEEE Network, vol. 35, no. 1, pp. 72\u201377, 2021. [118] C. Feng, K. Yu, A. K. Bashir, Y. D. Al-Otaibi, Y. Lu, S. Chen, and D. Zhang, \u201cEfficient and secure data sharing for 5g flying drones: a blockchain-enabled approach,\u201d IEEE Network, vol. 35, no. 1, pp. 130\u2013 137, 2021. [119] B. Ghimire, D. B. Rawat, C. Liu, and J. Li, \u201cSharding-enabled blockchain for software-defined internet of unmanned vehicles in the battlefield,\u201d IEEE Network, vol. 35, no. 1, pp. 101\u2013107, 2021. 32 [120] A. Gumaei, M. Al-Rakhami, M. M. Hassan, P. Pace, G. Alai..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            },
            {
              "id": "E2631264487",
              "text": "..order to provide fast processing and prior decision making, good connectivity is required which is achieved by 5G in AV. emBB provides high quality in bandwidth for vehicular communication. URLLC, mMTC and eMBB in 5G works together to provide faster connection speeds, higher device capacity and lower latency 125G Alliance for Connected Industries and Automation endorses testbeds for evaluation of industrial 5G use cases https://5g-acia.org/press-releases/ 5g-alliance-for-connected-industries-and-automation-endorses-testbeds-forevaluation-of-industrial-5g-use-cases/ in AV applications.Some of the benefits of using 5G in AV are driving the vehicles at high speed with high bandwidth, information delivery in minimal time limits due to low latency, notification alerts ahead about any hazardous events and provides self-decision using AV integrated with AI 2) Possible Future Directions: Some of the challenges in using 5G in AV are basically 5G spectrum are too expensive and can be purchased by spectrum auctions.The cost incurred in extending the existing network to 5G is too high and mapping network with reference to geo locations is a tedious task. AV using 5G and beyond can be further extended to perform the following activities \u2022 Predictive maintenance:Drivers can proactively maintain the vehicle to avoid failure during driving. Invehicle sensors monitors the conditions of the components present inside the vehicle like battery life, fuel pump and starter motor. Data from cloud combined with AI can predict potential maintenance issues before the occurrence of failure. \u2022 Advanced Infotainment Customers can be provided with additional information by integrating AI with AR, VR and sensors to have realistic experience. \u2022 Traffic safety service: 5G communication devices embedded inside the vehicles collects the information from the environment and pedestrians are stored in the cloud. Analytics on these data warns the drivers about the hazardous roads and traffic congestion which in turn recommends the solution for alternate path. 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become an important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. B. B5G technologies 1) Lessons Learned: Apart from large bandwidth requirements, several functionalities of AVs like object detection, lane detection, collision avoidance, navigation, V2V/V2X/V2I communications require several features like reduced latency, improved physical layer infrastructure, privacy & security for seamless AD. The supporting technologies of B5G such as MEC, network slicing, SDN/NFV, 5GNR, blockchain, FL, and ZSM can help in providing these requirements to AVs. However, several challenges such as generation of labels in real time for training the ML algorithms, lack of justification of the predictions/recommendations of ML algorithms, huge dimensionality of the data generated, etc. have to be addressed to realize the full potential of B5G for AV. Also, the autonomous vehicles may be connected to multiple networks on the road. Due to mobility of vehicles, it is likely that an AV may move out of coverage area of the access network and may have to join another network. Availability of availability of MEC for multiple access technologies is a significant challenge. Another significant challenge is the management of huge data generated from the AVs that are interconnected in real time [216]. 28 2) Possible Future Directions: Explainable Artificial Intelligence (XAI) can be adopted in B5G to address for trustability/justification/explainability of the decisions/predictions from ML algorithms in applications such as object detection, lane detection, collision avoidance, etc [217]\u2013[219]. For instance, consider that the AV is taking a human to the destination. Suddenly, the AI/ML model gives suggestion to the AV to take another route, which is different than the intended route. In these situations, the AV should be in a position to explain/justify the actions it has taken to avoid a particular route or choosing a particular route. XAI can help the humans to understand the actions of AVs in these kinds of situations. Unsupervised ML models can work on unlabelled data, that can address the challenge of big data generated by AV in 5GB era [220]. Several dimensionality reduction techniques can be used to extract the most important features from the large volumes of data generated in real time [221]. Handover problems associated with the AVs registered to one network and requiring to join another network during the mobility can be addressed by open radio access network (Open RAN) platform [222]. C. Security Concerns in AVs 1) Lessons Learned: From the security point of view, as highlighted in Table XII, it can observed that compared to In-Vehicle communication, the vulnerabilities brought by V2X technologies are more. V2X technologies will widen the attack surfaces thereby increasing the level of threats for the AVs. All the basic security goals i.e. confidentiality, integrity, availability and authentication needs to be addressed while incorporating V2X technologies with AVs. There is need of effective countermeasures to mitigate the threats arising from three main attack categories i.e. spoofing, denial of service, injection and physical attacks. 2) Possible Future Directions: The potential future directions can be categorised based on basic security goals as mentioned above especially with the main focus on V2X technologies. A strong and robust authentication mechanisms will be required for AVs to communicate with trusted entities while moving from one place to the other. An authentication mechanism with high latency and computing power requirement might provide attackers ample time to carry out spoofing or man-in-the-middle attack. Similarly, most of the protocols within AVs do not use encryption, how to secure communication within vehicle and while interacting with other entities outside the vehicles also seems interesting future research direction. Also, there will be huge amount of data generated by AVs, storing that data while preserving privacy of users and making that data available to legitimate users is yet another interesting research direction. D. Projects 1) Lessons Learned: Today, many research projects and SDO activities are focusing on the development of 5G and B5G networks to support the deployment of autonomous vehicles. These activities primarily focus on technical developments of 5G/b5G related technologies such as MEC, NS, 5G NR, ZSM, and AI. Significantly, NS and MEC got particular focus as these technologies can play a critical role in deploying autonomous vehicle-related applications and services. In addition, there are a significant number of research projects and standardization activities contributing to relevant 5G and B5G technical aspects such as path planning, mobility, service migration, security, and privacy. However, it might take a few more years for these standardizations to be fully deployed by different stakeholders such as mobile operators, governments, regulators, manufacturers, and third-party service providers. Thus, the continuous cooperation between different working groups (i.e., academia and industry) and SDOs are highly required in the coming years. Especially, funding frameworks such as H2020 by European Commission (EC) fuel these activities. 2) Possible Future Directions: In the future, there are two primary questions to be addressed. First, how to address the lack of AV-specific standardization in core 5G SDOs? Currently, AV is considered just one use case of 5G/B5G networks and still lacks a dedicated focus on AV within the core 5G SDOs. To resolve this, more AV stakeholders should contribute to 5G SDOs, and more AV-related subgroups would be formed within core 5G SDOs to develop dedicated AV-related standards. Second, how do we integrate and cooperate between different working groups and SDOs? The strong cooperation between AV SDOs and core 5G SDOs would be further encouraged. This will resolve some of the issues related to the first issue. More joint research and SDOs activities should be planned in the future. E. Emerging and Future Research Directions related to AV 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. 1) Quantum Computing: Quantum computing is the next future generation of automotive technology. Electric vehicles are significant part of quantum revolution. Automobile manufactures have started taking advantage of quantum computers to solve various automotive problems. AI in AV requires large amount of data for analysing and providing optimal response in dynamic situations. For detecting real time car locations and designing the optimal path requires high computing power and speed in AI. The former feature, high computing power can achieved by quantum computers. German automobile company Volkswagen collaborated with D-Wave systems to design and develop traffic routing in Beijing based on quantum computing systems. These systems can also solve optimization problems like waiting time, deployment of fleets etc., Volkswagen has also partnered with google to predict the state of traffic to avoid accident and to simulate the behaviour of electrical component and embed AI in driverless cars. As the AV are more 29 vulnerable to outside world, security breaches can be solved by quantum security. AV requires tremendous computing powers like optimized route planning and change the entire transport systems into smart systems. The cars will become smarter by communicating among themselves and outside world. More developments are expected in the field of AV integrated with quantum computers to achieve the benefits of computing and processing power. 2) Cognitive Cloud: Cognitive AI and algorithms would help us to simulate human-level performance specifically in level 5 AV. Satisfying the level 5 AV is a tedious task as it needs accurate decision making, object detection and localization under uncertain conditions like fog, rain and extreme darkness. Cognitive computing enhances the model accuracy to achieve closeness to humanlike performance in object detection and..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            }
          ]
        },
        "S2141737830": {
          "id": "S2141737830",
          "text": "Emerging technologies such as multi-access edge computing (MEC) and network slicing are crucial for optimizing the performance of UAVs in 5G networks, enabling low-latency communication and efficient resource allocation.",
          "children": [
            {
              "id": "E0052582723",
              "text": "..on AV within the core 5G SDOs. To resolve this, more AV stakeholders should contribute to 5G SDOs, and more AV-related subgroups would be formed within core 5G SDOs to develop dedicated AV-related standards. Second, how do we integrate and cooperate between different working groups and SDOs? The strong cooperation between AV SDOs and core 5G SDOs would be further encouraged. This will resolve some of the issues related to the first issue. More joint research and SDOs activities should be planned in the future. E. Emerging and Future Research Directions related to AV 5G network is considered as a key technology to design a driverless vehicles which is one of the most exciting domain in near future. It has become important technology in automobile industry integrated with the telecom industry to provide a best experience to the customer. With the advancement in the technologies like V2X and wireless communication, new generation of driverless vehicles is going to drive the automobile industry globally. 1) Quantum Computing: Quantum computing is the next future generation of automotive technology. Electric vehicles are significant part of quantum revolution. Automobile manufactures have started taking advantage of quantum computers to solve various automotive problems. AI in AV requires large amount of data for analysing and providing optimal response in dynamic situations. For detecting real time car locations and designing the optimal path requires high computing power and speed in AI. The former feature, high computing power can achieved by quantum computers. German automobile company Volkswagen collaborated with D-Wave systems to design and develop traffic routing in Beijing based on quantum computing systems. These systems can also solve optimization problems like waiting time, deployment of fleets etc., Volkswagen has also partnered with google to predict the state of traffic to avoid accident and to simulate the behaviour of electrical component and embed AI in driverless cars. As the AV are more 29 vulnerable to outside world, security breaches can be solved by quantum security. AV requires tremendous computing powers like optimized route planning and change the entire transport systems into smart systems. The cars will become smarter by communicating among themselves and outside world. More developments are expected in the field of AV integrated with quantum computers to achieve the benefits of computing and processing power. 2) Cognitive Cloud: Cognitive AI and algorithms would help us to simulate human-level performance specifically in level 5 AV. Satisfying the level 5 AV is a tedious task as it needs accurate decision making, object detection and localization under uncertain conditions like fog, rain and extreme darkness. Cognitive computing enhances the model accuracy to achieve closeness to humanlike performance in object detection and decision making. Integration of Cognitive computing in AV leads to improved safety and accuracy. Cognitive Internet of Vehicles allows the AV to focus on what, how and where to compute dynamically closer to human brain. VIII. CONCLUSION In this study, several aspects of AVs such as its features, levels of automation, architecture, key-enabling technologies and requirements for autonomus vehicular communication were discussed. Key requirements in terms of latency, security level, privacy, bandwidth, mobility, scalability, availability and reliability for potential AV applications (navigation and path planning, object detection, URLLC, mMTC, eMBB) were also identified. Several emerging technologies such as MEC, SDN and others were studied in detail and impact of 5G/B2G on these technologies was discussed. We also identified key security concerns in AVs with respect to 5G/B2G technology and highlighted recent standardization efforts by different organisation. Finally, several key research challenges and future research directions were also identified and discussed. REFERENCES [1] A. Manfreda, K. Ljubi, and A. Groznik, \u201cAutonomous vehicles in the smart city era: An empirical study of adoption factors important for millennials,\u201d International Journal of Information Management, p. 102050, 2019. [2] C. Ravi, A. Tigga, G. T. Reddy, S. Hakak, and M. Alazab, \u201cDriver identification using optimized deep learning model in smart transportation,\u201d ACM Transactions on Internet Technology, 2020. [3] K. Jadaan, S. Zeater, and Y. Abukhalil, \u201cConnected vehicles: an innovative transport technology,\u201d Procedia Engineering, vol. 187, pp. 641\u2013648, 2017. [4] Z. Wadud, D. MacKenzie, and P. Leiby, \u201cHelp or hindrance? the travel, energy and carbon impacts of highly automated vehicles,\u201d Transportation Research Part A: Policy and Practice, vol. 86, pp. 1\u201318, 2016. [5] J. Sachs, G. Wikstrom, T. Dudda, R. Baldemair, and K. Kittichokechai, \u201c5g radio network design for ultra-reliable low-latency communication,\u201d IEEE network, vol. 32, no. 2, pp. 24\u201331, 2018. [6] M. M. d. Silva and J. Guerreiro, \u201cOn the 5g and beyond,\u201d Applied Sciences, vol. 10, no. 20, p. 7091, 2020. [7] A. Rasouli and J. K. Tsotsos, \u201cAutonomous vehicles that interact with pedestrians: A survey of theory and practice,\u201d IEEE transactions on intelligent transportation systems, vol. 21, no. 3, pp. 900\u2013918, 2019. [8] Y. Ma, Z. Wang, H. Yang, and L. Yang, \u201cArtificial intelligence applications in the development of autonomous vehicles: a survey,\u201d IEEE/CAA Journal of Automatica Sinica, vol. 7, no. 2, pp. 315\u2013329, 2020. [9] M. N. Ahangar, Q. Z. Ahmed, F. A. Khan, and M. Hafeez, \u201cA survey of autonomous vehicles: Enabling communication technologies and challenges,\u201d Sensors, vol. 21, no. 3, p. 706, 2021. [10] C. R. Storck and F. Duarte-Figueiredo, \u201cA survey of 5g technology evolution, standards, and infrastructure associated with vehicle-toeverything communications by internet of vehicles,\u201d IEEE Access, vol. 8, pp. 117 593\u2013117 614, 2020. [11] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-Munoz, and J. M. Lopez-Soler, \u201cA survey on 5g usage scenarios and traffic models,\u201d IEEE Communications Surveys & Tutorials, vol. 22, no. 2, pp. 905\u2013929, 2020. [12] M. Pham and K. Xiong, \u201cA survey on security attacks and defense techniques for connected and autonomous vehicles,\u201d Computers & Security, p. 102269, 2021. [13] S. Khazraeian and M. Hadi, \u201cIntelligent transportation systems in future smart cities,\u201d in Sustainable Interdependent Networks II. Springer, 2019, pp. 109\u2013120. [14] S. E. Shladover, \u201cConnected and automated vehicle systems: Introduction and overview,\u201d Journal of Intelligent Transportation Systems, vol. 22, no. 3, pp. 190\u2013200, 2018. [15] R. Hussain and S. Zeadally, \u201cAutonomous cars: Research results, issues, and future challenges,\u201d IEEE Communications Surveys & Tutorials, vol. 21, no. 2, pp. 1275\u20131313, 2018. [16] J. Contreras-Castillo, S. Zeadally, and J. A. Guerrero-Ibanez, \u201cInternet \u0303 of vehicles: architecture, protocols, and security,\u201d IEEE internet of things Journal, vol. 5, no. 5, pp. 3701\u20133709, 2018. [17] S. Zhang, J. Chen, F. Lyu, N. Cheng, W. Shi, and X. Shen, \u201cVehicular communication networks in the automated driving era,\u201d IEEE Communications Magazine, vol. 56, no. 9, pp. 26\u201332, 2018. [18] S. Taxonomy, \u201cDefinitions for terms related to driving automation systems for on-road motor vehicles (j3016),\u201d Technical report, Society for Automotive Engineering, Tech. Rep., 2016. [19] F. Jameel, Z. Chang, J. Huang, and T. Ristaniemi, \u201cInternet of autonomous vehicles: architecture, features, and socio-technological challenges,\u201d IEEE Wireless Communications, vol. 26, no. 4, pp. 21\u201329, 2019. [20] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, \u201cA survey of autonomous driving: Common practices and emerging technologies,\u201d IEEE access, vol. 8, pp. 58 443\u201358 469, 2020. [21] J. Van Brummelen, M. O\u2019Brien, D. Gruyer, and H. Najjaran, \u201cAutonomous vehicle perception: The technology of today and tomorrow,\u201d Transportation research part C: emerging technologies, vol. 89, pp. 384\u2013406, 2018. [22] D. Gonzalez, J. P \u0301 erez, V. Milan \u0301 es, and F. Nashashibi, \u201cA review of mo- \u0301 tion planning techniques for automated vehicles,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 17, no. 4, pp. 1135\u20131145, 2015. [23] N. H. Amer, H. Zamzuri, K. Hudha, and Z. A. Kadir, \u201cModelling and control strategies in path tracking control for autonomous ground vehicles: a review of state of the art and challenges,\u201d Journal of intelligent & robotic systems, vol. 86, no. 2, pp. 225\u2013254, 2017. [24] J. Guanetti, Y. Kim, and F. Borrelli, \u201cControl of connected and..",
              "url": "https://openalex.org/W4310130661",
              "openalex_id": "https://openalex.org/W4310130661",
              "title": "Autonomous vehicles in 5G and beyond: A survey",
              "publication_date": "2023-02-01"
            },
            {
              "id": "E6896023186",
              "text": "..paradigm of cellular-assisted UAV communications. In order to emphasize the relevance and uniqueness of our current survey work compared to existing surveys, first, we plan to summarize the existing surveys and tutorials along with works pertaining to cellular-connected UAVs in Table 2. As summarized in Table 2, majority of existing surveys are based on UAV-assisted cellular communication and discusses them in-depth. There are few surveys that focuses on cellular-connected UAV paradigm, but these existing works are largely fragmented and do not provide a holistic view of this paradigm. In other words, only few selected aspects of cellular-connected UAV like UAV-ground channel modelling or trajectory optimization or MIMO are studied in depth so far. These works do not present an extensive study including all kinds of research highlights dedicated to cellular-connected UAVs, rather present a singular topic in depth. Thus, a unified work providing the broad picture of all kinds of research developments is still missing. With this survey, we aim at addressing this gap and focus solely on cellular-connected UAVs. The research highlights pertaining to state-of-the-art advancements, synergistic integration challenges of UAVs as aerial users in 5G/B5G cellular networks, underlying network architectures, physical layer enhancements of 5G, field trials, simulations and testbed developments are some of unique contributions made in this survey. The cloudification and softwarization of network resources portrayed as the interplay of Network Function Virtualization (NFV) and cloud computing technologies for the cellular-connected UAVs is also presented from the architectural context of enabling 5G/B5G innovations supporting them. 2.1. Key Contributions The key contributions of this work are the following. The final column of Table 2, bearing the heading \u201cThis Work\", also summarizes the contributions made in this work: \u2022 To present an overview of emerging applications and taxonomy of use cases for cellular-connected UAVs; \u2022 To highlight the state-of-the-art trends of communication requirements of UAVs and detailed discussion of design challenges, which must be accounted for successful integration of this technology within 5G/B5G cellular systems; \u2022 To showcase the emerging 5G technology innovations in network architectures such as virtualization & softwarization of network resources, slicing & physical layer improvements in the interest of cellularconnected UAVs; \u2022 To present the detailed efforts for design and development of experimental testbeds, trials and prototyping carried out by academia, industries and standardization bodies to understand the gap of theoretical analysis and realistic deployments; \u2022 To identify a fairly exhaustive outline of features for realization of an ideal experimental prototype for cellular-connected UAV & existing works to achieve them; \u2022 To discuss about the ongoing standardization activities, regulatory frameworks, market and socioeconomic issues that must be thoroughly investigated before successful and widespread adoption of cellularconnected UAVs; \u2022 To present insights to future research opportunities. The high level organization of this work is summarized in Fig. 3. Related Surveys and Tutorials Taxonomy of UAV Applications & Use cases Integration Challenges of UAVs over 5G Synergies of 5G/B5G innovations for Cellularconnected UAVs Design Trials and Prototyping Standardization & Socio-economic Concerns Conclusions Existing surveys & tutorials, key contributions 3GPP standardization, regulatory framework, market & social challenges Experimental test-beds, field trials 3D Coverage, channel model, system ops & mobility, trajectory optimization, security concerns Exploration of various emerging cellular connected UAV applications & diverse use cases Key 5G technology innovations in network architectures, virtualization & softwarization & PHY layer improvements Concluding remarks Introduction Overview of UAV Cellular Communication Fundamentals Future Outlooks Future research opportunities Figure 3: High level organization of this work D. Mishra et al.: Preprint submitted to Elsevier Page 4 of 32 A Survey on Cellular-connected UAVs: Design Challenges, Enabling 5G/B5G Innovations, and Experimental Advancements Table 2 Existing surveys and tutorials for UAV cellular communication Broad Category References \u2192 Contributions \u2193 [8] [13] [23] [39] [35] [40] [36] [10] [33] [37] [24] [18] [41] [5] [42] [43] [38] [44] [28] This Work Nature ofIntegration UAV-AssistedCellular Comm \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Cellular-AssistedUAV Comm \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Applications& Use cases Applications& Use cases \u2713 \u2713 \u2713 \u2713 \u2713 Design &Challenges TechnicalChallenges \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 PropagationChannel Models \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Mobility &Handovers \u2713 \u2713 TrajectoryOptimization \u2713 \u2713 \u2713 Technology &Experiment NetworkArchitecture \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 5G/B5GInnovations \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 ExperimentalPrototyping \u2713 \u2713 Ideal Featuresof Prototype \u2713 Harmonization& Compliance Standardization \u2713 \u2713 \u2713 Regulations \u2713 \u2713 \u2713 CommunicationRequirement \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 SocioeconomicConcerns Security Aspects \u2713 \u2713 \u2713 Social Concerns \u2713 \u2713 Market Concerns \u2713 D. Mishra et al.: Preprint submitted to Elsevier Page 5 of 32 A Survey on Cellular-connected UAVs: Design Challenges, Enabling 5G/B5G Innovations, and Experimental Advancements First, we begin with the detailed taxonomy of application domains and corresponding use cases for cellularconnected UAVs, and then highlight the key integration challenges of UAVs being supported from cellular 5G/B5G systems. For seamless integration, the recent technical innovations of 5G/B5G mobile network architectures and physical layer improvements are also presented. Then, we highlight the testbeds, field trials and measurement campaigns that showcase some early efforts to develop working prototypes of cellular-connected UAV. Furthermore, the ongoing standardization works, regulatory and socio-economic concerns are also discussed that must be accounted before successful adoption of this new technology. 3. Taxonomy of UAV Applications and Use cases Cellular-connected UAVs find their applicability in a wide range of emerging applications with varying demands and goals. In this work, we showcase some of the attractive researched domains as a starting basis for the following discussion. A bird\u2019s-eye view of this section is presented in Fig 4. 3.1. Earth and Atmospheric Observations As an innovative and efficient platform for gathering data, UAVs have become a preferred choice over traditional geomatics mechanisms of data acquisition. UAVs could autonomously fly in a defined trajectory and could precisely capture real-time measurements of the ongoing geophysical processes for abnormal hazards, such as volcanoes, landslides, sea dynamics, earthquakes, etc. Furthermore, the UAVs are equipped with various sensors to capture atmospheric temperature, pollutant levels in the air, carbon emissions, terrestrial biomass characterization, precipitation distribution in industrial zones, etc. As an efficient mechanism, the deployment of a fleet of UAVs, equipped with onboard sensors can perform the sensing for the presence of pollutant levels or any hazardous chemicals in the target areas [45, 46]. In a disaster situation, first 48 to 72 hours are very crucial to perform any kind of mitigation to the damage or outage and to restore the normal state of the environment. The response time is the key in saving lives in the affected regions. The major problems in these initial hours are: lack of proper communication infrastructure, massive or often unpredictable losses of lives and property. Thus, the situation forces the first responder teams to implement and improvise the search and rescue (SAR) mission to be conducted quickly and efficiently. Latest advancements of UAVs and sensor networks are capable to meet this need in terms of disaster prediction, assessment and fast recovery. UAVs can gather the information (e.g., situational awareness, early warnings, persons movement) during disaster phase and these information are helpful for first responder teams to react efficiently [47]. UAVs can re-establish the communication infrastructure (i.e., UAV-assisted paradigm) destroyed at the time of disaster. 3.2. Civil and Commercial Services Government constructions and public infrastructures such as highways and railways are greatly benefited by these flying platforms for efficient surveillance, land surveying, tracking workers and employees, on-site construction and demolition [39, 48, 49, 50]. Furthermore, UAV-based delivery systems are gaining wide popularity in logistics domain to achieve faster and cost effective good delivery service [51]. Such a system handles consumer orders, manages autonomous flight and status tracking using real time control. Google\u2019s Wing project [52] and Amazon Prime Air [53] are the some of the efforts to realize such a use case of UAVs. In precision agriculture, UAVs are capable of observing the agricultural fields for health monitoring, spraying pesticides and perform hyper spectral imagery [54]. Such activities by humans are time consuming and prone to risks. Unmanned aircrafts are well suited for such use cases enhancing productivity and cost efficacy. The cellular operators have started envisioning UAVs as backup wireless infrastructure (flying base stations or relays) in the absence of terrestrial communication infrastructure to boost network capacity [55, 56]. Google\u2019s Loon project [57] aims to provide ubiquitous Internet services & wireless connectivity to both remote and rural areas by employing high altitude platform (HAP) UAVs as balloons. 3.3. Disaster management & Security UAVs are an effective means of surveillance and monitoring of areas stricken by a natural disaster [58]. For instance, autonomous UAVs are sent to landslide, fire, earthquake and flooding areas to help with assessing the risks, the damages and support first responders teams as well as providing connectivity to isolated people [59, 60, 61]. Similarly, low cost UAVs revolutionize the conservation and management of forest and wildlife ecosystem by assisting in counting animal populations, tracking illegal activities, etc. UAVs are also an effective means of surveillance and control for the homeland security and public safety [62, 63, 64]. In case of anti-terroristic operations, UAVs are used to develop and prepare for situational awareness of threat, carrying out pre-emptive strikes or reconnaissance mission. UAVs assist in speeding up the rescue and recovery (search and rescue) missions in certain disastrous and crime control situations in a target area. 3.4. Industrial IoT platforms (IIoT) Industry 4.0 is an emerging paradigm embracing..",
              "url": "https://openalex.org/W3047778227",
              "openalex_id": "https://openalex.org/W3047778227",
              "title": "A survey on cellular-connected UAVs: Design challenges, enabling 5G/B5G innovations, and experimental advancements",
              "publication_date": "2020-08-07"
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W4310130661",
        "https://openalex.org/W3130357311",
        "https://openalex.org/W3047778227",
        "https://openalex.org/W2921698889"
      ],
      "evaluation": {
        "precision@10": 0.25,
        "recall@10": 0.1,
        "f1@10": 0.14285714285714288,
        "rouge_1": 0.015856582388840454,
        "rouge_2": 0.008555392076726065,
        "rouge_l": 0.010625544899738448,
        "text_f1": 0.07453954496208018,
        "num_source_papers": 4
      }
    },
    {
      "id": "https://openalex.org/W4391065733",
      "limited_meta": {
        "title": "Computational Pathology: A Survey Review and The Way Forward",
        "publication_date": "2024-01-21",
        "cited_by_count": 13,
        "url": ""
      },
      "text": "Computational Pathology: A Survey Review and The Way Forward\nMahdi S. Hosseinia,\u2217, Babak Ehteshami Bejnordib,1, Vincent Quoc-Huy Trinhc, Lyndon Chand, Danial Hasand, Xingwen Lid,\nStephen Yangd, Taehyo Kimd, Haochen Zhangd, Theodore Wud, Kajanan Chinniahd, Sina Maghsoudloua, Ryan Zhangd,\nJiadai Zhud, Samir Khakid, Andrei Buine, Fatemeh Chajia, Ala Salehif, Bich Ngoc Nguyeng,\nDimitris Samarash, Konstantinos N. Plataniotisd\naDepartment of Computer Science and Software Engineering (CSSE), Concordia Univeristy, Montreal, QC H3H 2R9, Canada\nbQualcomm AI Research, Qualcomm Technologies Netherlands B.V., Amsterdam, The Netherlands\nc\nInstitute for Research in Immunology and Cancer of the University of Montreal, Montreal, QC H3T 1J4, Canada\ndThe Edward S. Rogers Sr. Department of Electrical & Computer Engineering (ECE), University of Toronto, Toronto, ON M5S 3G4, Canada\neHuron Digitial Pathology, St. Jacobs, ON N0B 2N0, Canada\nfDepartment of Electrical and Computer Engineering, University of New Brunswick, Fredericton, NB E3B 5A3, Canada\ngUniversity of Montreal Hospital Center, Montreal, QC H2X 0C2, Canada\nhDepartment of Computer Science, Stony Brook University, Stony Brook, NY 11794, United States\nThis work is dedicated to the beloved memories of Kuanhou Fang, Shahnaz Habibpanah, Zakiyeh Khaliji-Oskoui, James Liang,\nMahsa MohammadiMoghaddam, Vily Panoutsakopoulou, Athanasia Samara, Huoyuan Yu, Dexi Zhang\nand all people around the world who have lost their lives because of cancer\nAbstract. Computational Pathology (CPath) is an interdisciplinary science that augments developments of computational\napproaches to analyze and model medical histopathology images. The main objective for CPath is to develop infrastructure and\nworkflows of digital diagnostics as an assistive CAD system for clinical pathology, facilitating transformational changes in the\ndiagnosis and treatment of cancer that are mainly address by CPath tools. With evergrowing developments in deep learning\nand computer vision algorithms, and the ease of the data flow from digital pathology, currently CPath is witnessing a paradigm\nshift. Despite the sheer volume of engineering and scientific works being introduced for cancer image analysis, there is still a\nconsiderable gap of adopting and integrating these algorithms in clinical practice. This raises a significant question regarding the\ndirection and trends that are undertaken in CPath. In this article we provide a comprehensive review of more than 700 papers to\naddress the challenges faced in problem design all-the-way to the application and implementation viewpoints. We have catalogued\neach paper into a model-card by examining the key works and challenges faced to layout the current landscape in CPath. We hope\nthis helps the community to locate relevant works and facilitate understanding of the field\u2019s future directions. In a nutshell, we\noversee the CPath developments in cycle of stages which are required to be cohesively linked together to address the challenges\nassociated with such multidisciplinary science. We overview this cycle from different perspectives of data-centric, model-centric,\nand application-centric problems. We finally sketch remaining challenges and provide directions for future technical developments\nand clinical integration of CPath. For updated information on this survey review paper and accessing to the original model cards\nrepository, please refer to GitHub.\nKeywords: digital pathology, whole slide image (WSI), deep learning, computer aided diagnosis (CAD), clinical pathology, survey\nContents\n1 Introduction 2\n2 Clinical Applications for CPath 4\n2.1 Clinical Pathology Workflow . . . . . . . . . . . . . . 4\n2.2 Diagnostic Tasks . . . . . . . . . . . . . . . . . . . . 5\n2.3 Prognosis . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.4 Prediction of treatment response . . . . . . . . . . . . 6\n2.5 Organs and Diseases . . . . . . . . . . . . . . . . . . 6\n3 Data Collection for CPath 9\n3.1 Tissue Slide Preparation . . . . . . . . . . . . . . . . 10\n\u2217Corresponding author.\nE-mail address: mahdi.hosseini@concordia.ca\n1Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n3.2 Whole Slide Imaging (WSI) . . . . . . . . . . . . . . 10\n3.3 Cohort Selection, Scale, and Challenges . . . . . . . . 11\n4 Domain Expert Knowledge Annotation 13\n4.1 Supervised Annotation . . . . . . . . . . . . . . . . . 13\n4.2 Optimum Labeling Workflow Design . . . . . . . . . . 15\n5 Model Learning for CPath 16\n5.1 Classification Architectures . . . . . . . . . . . . . . . 17\n5.2 Segmentation Architectures . . . . . . . . . . . . . . . 17\n5.3 Object Detection Architectures . . . . . . . . . . . . . 17\n5.4 Multi-Task Learning . . . . . . . . . . . . . . . . . . 18\n5.5 Multi-Modal Learning . . . . . . . . . . . . . . . . . 18\n5.6 Vision-Language Models . . . . . . . . . . . . . . . . 18\n5.7 Sequential Models . . . . . . . . . . . . . . . . . . . 19\n5.8 Synthetic Data and Generative Models . . . . . . . . . 19\n5.9 Multi-Instance Learning (MIL) Models . . . . . . . . 19\narXiv:2304.05482v3 [eess.IV] 27 Jan 2024\n2\n5.10 Contrastive Self-Supervised Learning for Few-Shot\nGeneralization . . . . . . . . . . . . . . . . . . . . . . 20\n5.11 Novel CPath Architectures . . . . . . . . . . . . . . . 20\n5.12 Model Comparison . . . . . . . . . . . . . . . . . . . 20\n6 Evaluation and Regulations 21\n6.1 Clinical Validation . . . . . . . . . . . . . . . . . . . 21\n6.2 FDA Regulations . . . . . . . . . . . . . . . . . . . . 22\n7 Emerging Trends in CPath Research 22\n7.1 Contrastive Self-Supervised Learning becomes Main\u0002stream . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n7.2 Prediction becoming increasingly High-Level . . . . . 22\n7.3 Spatial and Hierarchical Relationships receiving At\u0002tention . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n7.4 Vision-Language Models for Explainable Predictions . 22\n7.5 Synthetic Data now Realistic Enough . . . . . . . . . 23\n8 Existing Challenges and Future Opportunities 23\n8.1 CPath as Anomaly Detection . . . . . . . . . . . . . . 23\n8.2 Leveraging Existing Datasets . . . . . . . . . . . . . . 23\n8.3 Creating New Datasets . . . . . . . . . . . . . . . . . 24\n8.4 Pre- and Post-Analytical CAD Tools . . . . . . . . . . 24\n8.5 Multi Domain Learning . . . . . . . . . . . . . . . . . 24\n8.6 Federated Learning for Multi-Central CPath . . . . . . 24\n8.7 CPath-specific Architecture Designs . . . . . . . . . . 25\n8.8 Digital and Computational Pathology Adoption . . . . 25\n8.9 Institutional Challenges . . . . . . . . . . . . . . . . . 26\n8.10 Clinical Alignment of CPath Tasks . . . . . . . . . . . 26\n8.11 Concluding Remarks . . . . . . . . . . . . . . . . . . 26\n9 Appendix 51\n9.1 Clinical Pathology Workflow . . . . . . . . . . . . . . 51\n9.2 Diagnostic Tasks . . . . . . . . . . . . . . . . . . . . 52\n9.3 Prognosis . . . . . . . . . . . . . . . . . . . . . . . . 53\n9.4 Prediction of Treatment Response . . . . . . . . . . . 54\n9.5 Cancer Statistics . . . . . . . . . . . . . . . . . . . . . 54\n9.6 Whole Slide Imaging . . . . . . . . . . . . . . . . . . 55\n9.7 Organs and Diseases . . . . . . . . . . . . . . . . . . 56\n9.8 Ground Truth Labelling and Annotation . . . . . . . . 58\n9.9 Surveyed Datasets . . . . . . . . . . . . . . . . . . . . 59\n9.9.1 Table Creation Details . . . . . . . . . . . . . 59\n9.10 Organ Overview . . . . . . . . . . . . . . . . . . . . . 59\n9.11 Technicalities by Task . . . . . . . . . . . . . . . . . . 59\n9.12 Model Card Categorization . . . . . . . . . . . . . . . 81\n9.12.1 Template . . . . . . . . . . . . . . . . . . . . 81\n9.12.2 Samples . . . . . . . . . . . . . . . . . . . . . 82\n1. Introduction\nApril 2017 marked a turning point for digital pathology when\nthe Philips IntelliSite digital scanner received the US Food &\nDrugs Administration (FDA) approval (with limited use case)\nfor diagnostic applications in clinical pathology [1, 2]. A sub\u0002sequent validation guideline was created to help ensure the pro\u0002duced Whole Slide Image (WSI) scans could be used in clin\u0002ical settings without compromising patient care, while main\u0002taining similar results to the current gold standard of optical\nmicroscopy [3, 4, 5, 6]. The use of WSIs offers significant ad\u0002vantages to the pathologist\u2019s workflow: digitally captured im\u0002ages, compared to tissue slides, are immune from accidental\nphysical damage and maintain their quality over time [7, 8].\nClinics and practices can share and store these high-resolution\nimages digitally enabling asynchronous viewing/collaboration\nworldwide [9, 10]. The development of digital pathology shows\ngreat promise as a framework to improve work efficiency in the\npractice of pathology [11, 10]. Adopting a digital workflow also\nopens immense opportunities for using computational methods\nto augment and expedite their workflow\u2013the field of Computa\u0002tional Pathology (CPath) is dedicated to researching and devel\u0002oping these methods [12, 13, 14, 15, 16, 17].\nHowever, despite the aforementioned advantages, the adop\u0002tion of digital pathology, and hence computational pathology,\nhas been slow. Some pathologists consider the analysis of WSIs\nas opposed to glass slides as an unnecessary change in their\nworkflow [18, 19, 9, 20] and recent surveys indicate that the\nswitch to digital pathology does not provide enough financial\nincentive [21, 22, 23, 24, 25, 8]. This is where advances from\nCPath can address or overpower many of the concerns in adopt\u0002ing a digital workflow. For example, CPath models to identify\nmorphological features that correlate with breast cancer [26]\nprovide substantial benefits to clinical accuracy. Further, CPath\nmodels that identify lymph node metastases with better sensi\u0002tivity while reducing diagnostic time [27] can streamline work\u0002flows to increase pathologist throughput and generate more rev\u0002enue [28, 29].\nSimilar to digital pathology, the adoption of CPath methods\nhas also lagged despite the many benefits it offers to improve\nefficiency and accuracy in pathology [30, 31, 2, 32]. This lack\nof adoption and integration into clinical practice raises a sig\u0002nificant question regarding the direction and trends of current\nwork in CPath. This survey looks to review the field of CPath\nin a systematic fashion by breaking down the various steps in\u0002volved in a CPath workflow and categorizing CPath works to\nboth determine trends in the field and provide a resource for the\ncommunity to reference when creating new works.\nExisting survey papers in the field of CPath can be clus\u0002tered into a few groups. The first focuses on the design and\napplications of smart diagnosis tools [33, 34, 35, 36, 37, 38,\n17, 16, 15, 39, 40, 41, 42, 43]. These works focus on de\u0002signing novel architectures for artificial intelligence (AI) mod\u0002els with regards to specific clinical tasks, although they may\nbriefly discuss clinical challenges and limitations. A sec\u0002ond group of works focus on clinical barriers for AI integra\u0002tion discussing specific certifications and regulations required\nfor the development of medical devices under clinical settings\n[44, 45, 46, 47, 48, 49]. Lastly, the final group focuses on both\nthe design and the integration of AI tools with clinical applica\u0002tions [50, 51, 12, 13, 52, 29, 53, 54, 14, 55, 56]. These works\nspeak to both the computer vision and pathology communities\nin developing machine learning (ML) models that can satisfy\nclinical use cases.\nOur work is situated in this final group as we breakdown the\nend-to-end CPath workflow into stages and systematically re\u0002view works related to and addressing those stages. We oversee\nthis as a workflow for CPath research that breaks down the pro\u0002cess of problem definition, data collection, model creation, and\nclinical validation into a cycle of stages. A visual representation\nof this cycle is provided in Figure 1. We review over 700 papers\n3\nFig. 1: We divide the data science workflow for pathology into multiple stages,\nwherein each brings a different level of experience. For example, the anno\u0002tation/ground truth labelling stage (c) is where domain expert knowledge is\nconsulted as to augment images with associated metadata. Meanwhile, in the\nevaluation phase (e), we have computer vision scientists, software developers,\nand pathologists working in concert to extract meaningful results and implica\u0002tions from the representation learning.\nfrom all areas of the CPath field to examine key works and chal\u0002lenges faced. By reviewing the field so comprehensively, our\ngoal is to layout the current landscape of key developments to\nallow computer scientists and pathologists alike to situate their\nwork in the overall CPath workflow, locate relevant works, and\nfacilitate an understanding of the field\u2019s future directions. We\nalso adopt the idea of generating model cards from [57] and de\u0002signed a card format specifically tailored for CPath. Each paper\nwe reviewed was catalogued as a model card that concisely de\u0002scribes (1) the organ of application, (2) the compiled dataset, (3)\nthe machine learning model, and (4) the target task. The com\u0002plete model card categorization of the reviewed publications is\nprovided in Appendix 9.12 for the reader\u2019s use.\nIn our review of the CPath field, we find that two main ap\u0002proaches emerge in works: 1) a data-centric approach and 2) a\nmodel-centric approach. Considering a given application area,\nsuch as specific cancers, e.g. breast ductal carcinoma in-situ\n(DCIS), or a specific task, e.g. segmentation of benign and\nmalignant regions of tissue, researchers in the CPath field fo\u0002cus generally on either improving the data or innovating on the\nmodel used.\nWorks with data-centric approaches focus on collecting\npathology data and compiling datasets to train models on cer\u0002tain tasks based on the premise that the transfer of domain ex\u0002pert knowledge to models is captured by the process of collect\u0002ing and labeling high-quality data [58, 51, 59]. The motivation\nbehind this approach in CPath is driven by the need to 1) ad\u0002dress the lack of labeled WSI data representing both histology\nand histopathology cases due to the laborious annotation pro\u0002cess [24] and 2) capture a predefined pathology ontology pro\u0002vided by domain expert pathologists for the class definitions\nand relations in tissue samples. Regarding the lack of labeled\nWSI data our analysis reveals that there are a larger number of\ndatasets with granular labels, but there is a larger total amount\nof data available for a given organ and disease application that\nhave weakly supervised labels at the Slide or Patient-level. Al\u0002though some tasks, such as segmentation and detection, require\nWSI data to have more granular labels at the region-of-interest\n(ROI) or image mosaic/tiles (known as patch) levels, to capture\nmore precise information for training models, there is a poten\u0002tial gap to leverage the large amount of weakly-supervised data\nto train models that can be later used downstream on smaller\nstrongly-supervised datasets for those tasks. When considering\nthe ontology of pathology as compared to the field of computer\nvision, we note that pathology datasets have far fewer classes\nthan computer vision (e.g. ImageNet-20K contains 20,000 class\ncategories for natural images [60] whereas CAMELYON17 has\nfour annotated classes for breast cancer metastases [61]), but\nhas much more variation within each of these classes in terms\nof representations and fuzzy boundaries around the grade of\ncancers which subdivides each class into many more in reality.\nThere are also very rare classes in the form of rare diseases and\ncancers, as presented in Figure 12 and discussed in Section 2,\nwhich present a class imbalance challenge when compiling data\nor training models. If one considers the complexities involved\nin representational learning of related tissues and diseases, it\nraises the question of whether there is a clear understanding\nand consensus in the field of how an efficient dataset should\nbe compiled for model development. Our survey analyzes the\navailability of CPath datasets along with what area of applica\u0002tion they address and their annotation level in detail in Section\n3.3, and the complete table of datasets we have covered is avail\u0002able in Appendix 9.9. Section 4 goes into more depth about the\nvarious levels of annotation, the annotation process, and select\u0002ing the appropriate annotation level for a task.\nThe model-centric approach, by contrast, is favoured by\ncomputer scientists and engineers, who design algorithmic ap\u0002proaches based on the available pathology data. Selection\nof a modelling approach, such as self-supervised, weakly\u0002supervised, or strongly-supervised learning, is dictated directly\nby the amount of data available for a given annotation level and\ntask. Currently, many models are developed on datasets with\nstrongly-supervised labels at the ROI, Patch, or Pixel-levels to\naddress tasks such as tissue type classification or disease de\u0002tection. However, a recent trend is developing to apply self\u0002supervised and weakly-supervised learning methods to leverage\nthe large amount of data with Slide and Patient-level annota\u0002tions [62]. Models are trained in a self or weakly supervised\nmanner to learn representations on a wider range of pathol\u0002ogy data across organs and diseases, which can be leveraged\nfor other tasks requiring more supervision but without the need\nfor massive labeled datasets [63, 64, 65]. This trend points to\nthe future direction of CPath models following a similar trend\nto that in computer vision, where large-scale models are being\n4\npre-trained using self-supervised techniques to achieve state-of\u0002the-art performance in downstream tasks [66, 67].\nAlthough data and model centric approaches are both im\u0002portant in advancing the performance of models and tools in\nCPath, we note a need for much more application centric work\nin CPath. We define a study to be application centric if the\nprimary focus is on addressing a particularly impactful task or\nneed in the clinical workflow, ideally including clinical valida\u0002tion of the method or tool. To this end, Section 2 details the\nclinical pathology workflow from specimen collection to report\ngeneration, major task categories in CPath, and specific applica\u0002tions per organ. Particularly, we find that very few works focus\non the pre or post-analytical phases of the pathology workflow\nwhere many errors can occur, instead focusing on the analyt\u0002ical phase where interpretation tasks take place. Additionally,\ncertain types of cancer with deadly survival rates are underrep\u0002resented in CPath datasets and works. Very few CPath models\nand tools have been validated in a clinical setting by pathol\u0002ogists, suggesting that there may still be massive barriers to\nactually using CPath tools in practice. All of this points to a\nsevere oversight by the CPath community towards considering\nthe actual application and implementation of tools in a clinical\nsetting. We suspect this to be a major reason as to why there is\na slow uptake in adopting CPath tools by pathology labs.\nThe contributions of this survey include the provision of an\nend-to-end workflow for developing CPath work which outlines\nthe various stages involved and is reflected within the survey\nsections. Further, we propose and provide a comprehensive\nconceptual model card framework for CPath that clearly cat\u0002egorizes works by their application of interest, dataset usage,\nand model, enabling consistent and easy comparison and re\u0002trieval of papers in relevant areas. Based on our analysis of\nthe field, we highlight several challenges and trends, including\nthe availability of datasets, focus on models leveraging exist\u0002ing data, disregard of impactful application areas, and lack of\nclinical validation. Finally, we give suggestions for address\u0002ing these aforementioned challenges and provide directions for\nfuture work in the hopes of aiding the adoption and implemen\u0002tation of CPath tools in clinical settings.\nThe structure of this survey closely follows the CPath data\nworkflow illustrated in Figure 1. Section 2 begins by outlining\nthe clinical pathology workflow and covers the various task do\u0002mains in CPath, along with organ specific tasks and diseases.\nThe next step of the workflow involves the processes and meth\u0002ods of histopathology data collection, which is outlined in Sec\u0002tion 3. Following data collection, Section 4 details the corre\u0002sponding annotation and labeling methodology and considera\u0002tions. Section 5 covers deep learning designs and methodolo\u0002gies for CPath applications. Section 6 focuses on regulatory\nmeasures and clinical validation of CPath tools. Section 7 ex\u0002plores emerging trends in recent CPath research. Finally, we\nprovide our perceived challenges and future outlook of CPath\nin Section 8.\n2. Clinical Applications for CPath\nThe field of CPath is dedicated to the creation of tools\nthat address and aid steps in the clinical pathology workflow.\nThus, a grounded understanding of the clinical workflow is\nof paramount importance before development of any CPath\ntool. The outcomes of clinical pathology are diagnostics, prog\u0002nostics, and predictions of therapy response. Computational\npathology systems that focus on diagnostic tasks aim to as\u0002sist the pathologists in tasks such as tumour detection, tumour\ngrading, quantification of cell numbers, etc. Prognostic systems\naim to predict survival for individual patients while therapy re\u0002sponse predictive models aid personalized treatment decisions\nbased on histopathology images. Figure 3 visualizes the goals\npertaining to these tasks. In this section, we provide detail on\nthe clinical pathology workflow, the major application areas in\ndiagnostics, prognostics, and therapy response, and finally de\u0002tail the cancers and CPath applications in specific organs. The\ngoal is to outline the tasks and areas of application in pathology\nwhere CPath tools and systems can be developed and imple\u0002mented.\n2.1. Clinical Pathology Workflow\nThis subsection provides a general overview of the clini\u0002cal workflow in pathology covering the collection of a tissue\nsample, its subsequent processing into a slide, inspection by\na pathologist, and compilation of the analysis and diagnosis\ninto a pathology. Figure 2 summarizes these steps at a high\nlevel and provides suggestions for corresponding CPath appli\u0002cations. The steps are organized under the conventional pathol\u0002ogy phases for samples: pre-analytical, analytical, and post\u0002analytical. These phases were developed to categorize qual\u0002ity control measures, as each phase has its own set of potential\nsources of errors [68], and thus potential sources of corrections\nduring which CPath and healthcare artificial intelligence tools\ncould prove useful. For details about each step of the workflow,\nplease refer to the Appendix 9.1.\nPre Analytical Phase The first step of the pre-analytical\nphase is a biopsy performed to collect a tissue sample, where\nthe biopsy method is dependent on the type of sample required\nand the tissue characteristics. Sample collection is followed by\naccessioning of the sample which involves entering of the pa\u0002tient and specimen information into a Laboratory Information\nSystem (LIS) and linking to the Electronic Medical Records\n(EMR) and potentially a Slide Tracking System (STS). Af\u0002ter accessioning, smaller specimens that have not already been\npreserved by fixation in formalin are fixated. Once the basic\nspecimen preparation has occurred, the tissue is analyzed by\nthe pathology team without the use of a microscope; a step\ncalled grossing. Grossing involves cross-referencing the clin\u0002ical findings and the EMR reports, with the operator localiz\u0002ing the disease, locating the pathological landmarks, describing\nthese landmarks, and measuring disease extent. Specific sam\u0002pling of these landmarks is performed, and these samples are\nthen put into cassettes for the final fixation. Subsequently, the\nsamples are then sliced using a microtome, stained using the\nrelevant stains for diagnosis, and covered with a glass slide.\nAnalytical Phase After a slide is processed and prepared, a\npathologist views the slide to analyze and interpret the sample.\nThe approach to interpretation varies depending on the spec\u0002imen type. Interpretation of smaller specimens is focused on\n5\nFig. 2: Quality assurance and control phases developed by pathologists to over\u0002see the clinical pathology workflow into three main phases of pre-analytical,\nanalytical, and post-analytica phases. We further show how each of these pro\u0002cesses can be augmented under the potential CPath applications in an end-to\u0002end pipeline.\ndiagnosis of any disease. Analysis is performed in a decision\u0002tree style approach to add diagnosis-specific parameters, e.g.\nesophagus biopsy \u2192 type of sampled mucosa \u2192 presence of\nfolveolar-type mucosa \u2192 identify Barrett\u2019s metaplasia \u2192 iden\u0002tify degree of dysplasia. Once the main diagnosis has been\nidentified and characterized, the pathologist sweeps the remain\u0002ing tissue for secondary diagnoses which can also be charac\u0002terized depending on their nature. Larger specimens are more\ncomplex and usually focus on characterizing the tissue and\nidentifying unexpected diagnoses beyond the prior diagnosis\nfrom a small specimen biopsy. Microscopic interpretation of\nlarge specimens is highly dependent on the quality of the gross\u0002ing and the appropriate detection and sampling of landmarks.\nEach landmark (e.g., tumor surface, tumor at deepest point,\nsurgical margins, lymph node in mesenteric fat) is character\u0002ized either according to guidelines, if available, or according\nto the pathologist\u2019s judgment. After the initial microscopic\ninterpretation additional deeper cuts (\u201clevels\u201d), special stains,\nimmunohistochemistry (IHC), and/or molecular testing may be\nperformed to hone the diagnosis by generating new material or\nslides from the original tissue block.\nPost-Analytical Phase The pathologist synthesizes a diagno\u0002sis by aggregating their findings from grossing and microscopic\nexamination in combination with the patient\u2019s clinical informa\u0002tion, all of which are included in a final pathology report. The\nclassic sections of a pathology report are patient information,\na list of specimens included, clinical findings, grossing report,\nmicroscopic description, final diagnosis, and comment. The\nlength and degree of complexity of the report again depends\non the specimen type. Small specimen reports are often suc\u0002cinct, clearly and unambiguously listing relevant findings which\nguide treatment and follow-up. Large specimen reports depend\non the disease, for example, in cancer resection specimens the\ngrossing landmarks are specifically targeted at elements that\nwill guide subsequent treatment.\nIn the past, pathology reports had no standardized format,\nusually taking a narrative-free text form. Free text reports can\nomit necessary data, include irrelevant information, and con\u0002tain inconsistent descriptions [69]. To combat this, synoptic re\u0002porting was introduced to provide a structured and standardized\nreporting format specific to each organ and cancer of interest\n[69, 70]. Over the last 15 years, synoptic reporting has enabled\npathologists to communicate information to surgeons, oncolo\u0002gists, patients, and researchers in a consistent manner across in\u0002stitutions and even countries. The College of American Pathol\u0002ogists (CAP) and the International Collaboration on Cancer Re\u0002porting (ICCR) are the two major institutions publishing synop\u0002tic reporting protocols. The parameters included in these pro\u0002tocols are determined and updated by CAP and ICCR respec\u0002tively to remain up-to-date and relevant for diagnosis of each\ncancer type. For the field of computational pathology, synoptic\nreporting provides a significant advantage in dataset and model\ncreation, as a pre-normalized set of labels exist across a vari\u0002ety of cases and slides in the form of the synoptic parameters\nfilled out in each report. Additionally, suggestion or prediction\nof synoptic report values are a possible CPath application area.\n2.2. Diagnostic Tasks\nComputational pathology systems that focus on diagnostic\ntasks can broadly be categorized as: (1) disease detection, (2)\ntissue subtype classification, (3) disease diagnosis, and (4) seg\u0002mentation. These tasks are visually depicted in Figure 3. Note\nhow the detection tasks all involve visual analysis of the tissue\nin WSI format. Thus computer vision approach is primarily\nadopted towards tackling diagnostic tasks in computer aided di\u0002agnosis (CAD). For additional detail on some previous works\non these diagnostic tasks, we refer the reader to Appendix 9.2\nDetection We define the detection task as a binary classifica\u0002tion problem where inputs are labeled as positive or negative,\nindicating the presence or absence of a certain feature. There\nmay be variations in the level of annotation required, e.g. slide\u0002level, patch-level, pixel-level detection depending on the fea\u0002ture in question. Although detection tasks may not provide\nan immediate disease diagnosis, it is a highly relevant task in\nmany pathology workflows as pathologists incorporate the pres\u0002ence or absence of various histological features into synoptic\nreports that lead to diagnosis. Broadly, detection tasks fall into\ntwo main categories: (1) screening the presence of cancers and\n(2) detecting histopathological features specific to certain diag\u0002noses.\nCancer detection algorithms can assist the pathologists by fil\u0002tering obviously normal WSIs and directing pathologist\u2019s focus\n6\nFig. 3: The categorization of diagnostic tasks in computational pathology along\nwith examples A) Detection: common detection task such as differentiating\npositive from negative classes like malignant from benign, B) Tissue Subtype\nClassification: classification task for tumorous tissue, Stroma, and adipose tis\u0002sue, C) Disease Diagnosis: common disease diagnosis task like cancer stag\u0002ing, D) Segmentation: tumor segmentation in WSIs, and E) Prognosis tasks:\nshows a graph comparing survival rate and months after surgery.\nto metastatic regions [71]. Although pathologists have to re\u0002view all the slides to check for multiple conditions regardless of\nthe clinical diagnosis, an accurate cancer detection CAD would\nexpedite the workflow by pinpointing the ROIs and summariz\u0002ing results into synoptic reports, ultimately leading to a reduces\ntime per slide. Due to this potential impact, cancer detection\ntasks have been explored in a broad set of organs. Additionally,\nthe simple labeling in binary detection tasks allows for deep\nlearning methods to generalize across different organs where\nsimilar cancers form [72, 73, 74].\nTissue Subtype Classification Treatment and patient progno\u0002sis can vary widely depending on the stage of cancer, and finely\nclassifying specific tissue structures associated with a specific\ndisease type provides essential diagnostic and prognostic in\u0002formation [75]. Accordingly, accurately classifying tissue sub\u0002types is a crucial component of the disease diagnosis process.\nAs an example, discriminating between two forms of glioma (a\ntype of brain cancer), glioblastoma multiforme and lower grade\nglioma, is critical as they differ by over 45% in patient survival\nrates [76]. Additionally, accurate classification is key in col\u0002orectal cancer (CRC) diagnosis, as high morphological varia\u0002tion in tumor cells [77] makes certain forms of CRC difficult to\ndiagnose by pathologists [78]. We define this classification of\nhistological features as tissue subtype classification.\nDisease Diagnosis The most frequently explored design of\ndeep learning in digital pathology involves emulating pathol\u0002ogist diagnosis. We define this multi-class diagnosis problem\nas a disease diagnosis task. Note the similarity with detection\u2013\ndisease diagnosis can be considered a fine-grained classification\nproblem which subdivides the general positive disease class\ninto finer disease-specific labels based on the organ and patient\ncontext.\nSegmentation The segmentation task moves one step beyond\nclassification by adding an element of spatial localization to the\npredicted label(s). In semantic segmentation, objects of inter\u0002est are delineated in an image by assigning class labels to every\npixel. These class labels can be discrete or non-discrete, the lat\u0002ter being a more difficult task [79]. Another variant of the seg\u0002mentation task is instance segmentation, which aims to achieve\nboth pixel-level segmentation accuracy as well as clearly de\u0002fined object (instance) boundaries. Segmentation approaches\ncan accurately capture many morphological statistics [80] and\ntextural features [81], both of which are relevant for cancer di\u0002agnosis and prognosis. Most frequently, segmentation is used\nto capture characteristics of individual glands, nuclei, and tu\u0002mor regions in WSIs. For instance, glandular structure is a\ncritical indicator of the severity of colorectal carcinoma [82],\nthus accurate segmentation could highlight particularly abnor\u0002mal glands to the pathologist as demonstrated in [82, 83, 84].\nOverall, segmentation provides localization and classification\nof cancer-specific tumors and of specific histological features\nthat can be meaningful for the pathologist\u2019s clinical interpreta\u0002tion.\n2.3. Prognosis\nPrognosis involves predicting the likely development of a\ndisease based on given patient features. For accurate survival\nprediction, models must learn to both identify and infer the\neffects of histological features on patient risk. Prognosis rep\u0002resents a merging of the diagnosis classification task and the\ndisease-survivability regression task.\nTraining a model for prognosis requires a comprehensive set\nof both histopathology slides and patient survival data (i.e. a\nvariant of multi-modal representation learning). Despite the\ncomplexity of the input data, ML models are still capable of\nextracting novel histological patterns for disease-specific sur\u0002vivability [85, 86, 87]. Furthermore, strong models can dis\u0002cover novel prognostically-relevant histological features from\nWSI analysis [88, 89]. As the quality and comprehensiveness\nof data improves, additional clinical factors could be incorpo\u0002rated into deep learning analysis to improve prognosis.\n2.4. Prediction of treatment response\nWith the recent advances in targeted therapy for cancer treat\u0002ment, clinicians are able to use treatment options that precisely\nidentify and attack certain types of cancer cells. While the num\u0002ber of options for targeted therapy are constantly increasing,\nit becomes increasingly important to identify patients who are\npotential therapy responders to a specific therapy option and\navoid treating non-responding patients who may experience se\u0002vere side effects. Deep learning can be used to detect structures\nand transformations in tumour tissue that could be used as pre\u0002dictive markers of a positive treatment response. Training such\ndeep learning models usually requires large cohorts of patient\ndata for whom the specific type of treatment option and the cor\u0002responding response is known.\n2.5. Organs and Diseases\nThis section presents an overview of the various anatomical\napplication areas for computational pathology grouped by the\ntargeted organ. Each organ section gives a brief overview of the\ntypes of cancers typically found and the content of the pathol\u0002ogy report as noted from the corresponding CAP synoptic re\u0002porting outline (discussed at 2.1). Figure 4 highlights the inter\u0002section between the major diagnostic tasks and the anatomical\n7\nFig. 4: Distribution of diagnostic tasks in CPath for different organs from Table\n9.11. This distribution includes more than 400 cited works from 2018 to 2022\ninclusive. The x-axis covers different organs, the y-axis displays different di\u0002agnostic tasks, and the height of the bars along the vertical axis measures the\nnumber of works that have examined the specific task and organ. Please refer\nto Table 9.11 in the supplementary section for more information.\nfocuses in state-of-the-art research. The majority of papers are\ndedicated to the four most common cancer sites: breast, colon,\nprostate, and lung [90]. Additionally, a significant amount of re\u0002search is also done on cancer types with highest mortality, brain\nand liver. [90]. Note that details of some additional works that\nmay be of interest for each organ type can be found in Appendix\n9.7\nBreast Breast cancers can start from different parts of the\nbreast and majorly consist of 1) Lobular cancers that start from\nlobular glands, 2) Ductal cancers, 3) Paget cancer which in\u0002volves the nipple, 4) Phyllodes tumor that stems from fat and\nconnective tissue surrounding the ducts and lobules, and 5) An\u0002giosarcoma which starts in the lining of the blood and lymph\nvessels. In addition, based on whether the cancer has spread\nor not, breast cancers can be categorized into in situ or inva\u0002sive/infiltrating forms. DCIS is a precancerous state and is still\nconfined to the ducts. Once the cancerous cells grow out of the\nducts, the carcinoma is now considered invasive or infiltrative\nand can metastasize [91].\nSynoptic reports for breast cancer diagnosis are divided\nbased on the type of cancers mentioned above. For DCIS and\ninvasive breast cancers, synoptic reports focus on the histo\u0002logic type and grade, along with the nuclear grade, evidence\nof necrosis, margin, involvement of regional lymph nodes, and\nbiomarker status. Notably, architectural patterns are no longer\na valuable predictive tool compared to nuclear grade and necro\u0002sis to determine a relative ordering of diagnostic importance for\nDCIS [92]. In contrast to DCIS and invasive cancers, Phyl\u0002lodes tumours vary due to their differing origin in the fat and\nconnective tissue, focusing on analyzing the stroma character\u0002istics, existence of heterologous elements, mitotic rate, along\nwith the involvement of lymph nodes. Finally, to determine\ntherapy response and treatments, biomarker tests for Estrogen,\nProgesterone [93] and HER-2 [94] receptors are recommended,\nalong with occasional tests for Ki67 antigens [95, 96].\nMost breast cancer-focused works in CPath propose vari\u0002ous solutions for carcinoma detection and metastasis detection,\nan important step for assessing cancer stage and morbidity.\nMetastasis detection using deep learning methods was shown\nto outperform pathologists\u2019 exhaustive diagnosis by 9% free\u0002response receiver operating characteristic (FROC) in [97].\nProstate Prostate cancer is the second most prevalent cancer\namong the total population and the most common cancer among\nmen (both excluding non-melanoma skin cancers). However,\nmost prostate cancers are not lethal. Prostate cancer can oc\u0002cur in any of the three prostate zones: Central (CZ), Periph\u0002eral (PZ), and Transition (TZ), in increasing order of aggres\u0002siveness. Prostate cancers are almost always adenocarcino\u0002mas, which develop from the gland cells that make prostate\nfluid. The other types of prostate cancers are small cell car\u0002cinomas, neuroendocrine tumors, transitional cell carcinomas,\nisolated intraductal carcinoma, and sarcomas (which are very\nrare). Other than cancers, there are multiple conditions that\nare important to identify or diagnose as precursors to cancer\nor not. Prostatic intraepithelial neoplasia (PIN) is diagnosed as\neither low-grade PIN or high-grade PIN. Men with high-grade\nPIN need closely monitored follow-up sessions to screen for\nprostate cancer. Similarly, atypical small acinar proliferation\n(ASAP) is another precancerous condition requiring follow-up\nbiopsies. [98]\nTo grade and score tumours, pathologists use a Tumor,\nNodes, Metastasis (TNM) framework. In the synoptic report,\npathologists identify and report the histologic type and grades,\nand involvement of regional lymph nodes to help grade and\nprovide prognosis for any tumours. Specifically for prostate\nanalysis, tumour size and volume are both important factors\nin prognosis according to multiple studies [99, 100, 101, 102].\nSimilarly, location is important to note for both prognosis and\ntherapy response [103]. Invasion to nearby (except perineural\ninvasion) tissues is noted and can correlate to TMN classifi\u0002cation [104]. Additionally, margin analysis is especially im\u0002portant in prostate cancers as the presence of a positive margin\nincreases the risk of cancer recurrence and metastasis [105]. Fi\u0002nally, intraductal carcinoma (IDC) must be identified and dis\u0002tinguished from PIN and PIA; as it is strongly associated with\na high Gleason score, a high-volume tumor, and metastatic dis\u0002ease [106, 107, 108, 109, 110].\nAfter a prostate cancer diagnosis is established, pathologists\nassign a Gleason Score to determine the cancer\u2019s grade: a grade\nfrom 1 to 5 is assigned to the two most common areas and\nthose two grades are summed to make a final Gleason Score\n[111]. For Gleason scores of 7, where survival and clinical out\u0002comes demonstrate large variance, the identification of Crib\u0002riform glands is key in helping to narrow possible outcomes\n[112, 113].\nOvary Ovarian cancer is the deadliest gynecologic malig\u0002nancy and accounts for more than 14, 000 deaths each year\n[114]. Ovarian cancer manifests in three types: 1) epithelial cell\ntumors that start from the epithelial cells covering the outer sur\u0002face of the ovary, 2) germ cell tumors which start from the cells\nthat produce eggs, and 3) stromal tumors which start from cells\nthat hold the ovary together and produce the hormones estrogen\nand progesterone. Each of these cancer types can be classified\ninto benign, intermediate and malignant categories. Overall,\nepithelial cell tumors are the most common ovarian cancer and\nhave the worst prognosis [115].\nWhen compiling a synoptic report for ovarian cancer diag-\n8\nnosis, pathologists focus on histologic type and grade, extra\u0002organ involvement, regional lymph nodes, T53 gene mutations,\nand serous tubal intraeptithelial carconma (STIC). Varying his\u0002tologic tissue types are vital to determine the pathology char\u0002acteristics and determining eventual prognosis. For example,\ngenerally endometrioid, mucinous, and clear cell carcinomas\nhave better outcomes than serous carcinomas [116]. Addition\u0002ally, lymph node involvement and metastasis in both regional\nand distant nodes has a direct correlation to patient survival,\ngrading, and treatment. Determining the presence of STICs\ncorrelates directly to the presence of ovarian cancer, as 60% of\novarian cancer patients will also have an associated STIC [114].\nFinally, T53 gene mutations are the most common in epithelial\novarian cancer; which has the worst prognosis among ovarian\ncancers, so determining their presence is critical to patient can\u0002cer risk and therapy response [117, 118]. There are not a large\nnumber of works dedicated to the ovary specifically, but most\nworks on ovary focus on classification of its five most common\ncancer subtypes: high-grade serous (HGSC), low-grade serous\n(LGSC), endometriod (ENC), clear cell (CCC), and mucinous\n(MUC) [119, 120].\nLung Lung cancer is the third most common cancer type,\nnext to breast and prostate cancer [121]. Lung cancers mostly\nstart in the bronchi, bronchioles, or alveoli and are divided\ninto two major types, non-small cell lung carcinomas (NSCLC)\n(80\u221285%) and small cell lung carcinomas (SCLC) (10\u221215%).\nAlthough NSCLS cancers are different in terms of origin, they\nare grouped because they have similar outcomes and treatment\nplans. Common NSCLS cancers are 1) adenocarcinoma, 2)\nsquamous cell carcinoma 3) large cell carcinoma, and some\nother uncommon subtypes [122].\nFor reporting, histologic type helps determine NSCLC vs\nSCLC and the subtype of NSCLC. Although NSCLC gener\u0002ally has favourable survival rates and prognosis as compared\nto SCLC, certain subtypes of NSCLC can have lower survival\nrates due to co-factors [123]. Histologic patterns are applicable\nin adenocarcinomas, consisting of favourable types: lepidic, in\u0002termediate: acinar and papillary, and unfavourable: micropap\u0002illary and solid [124]. Grading each histologic type aids in cat\u0002egorization but is differentiated based on each type, and thus is\nout of scope for this paper. Importantly for lung cancers, tu\u0002mour size is an independent prognostic factor for early cancer\nstages, lymph node positivity, and locally invasive disease. Ad\u0002ditionally, the size of the invasive portion is an important fac\u0002tor for prognosis of nonmucinous adenocarcinoma with lepidic\npattern [125, 126, 127, 128, 129, 123]. Other important lung\nspecific features are visceral pleural invasion, which is associ\u0002ated with worse prognosis in early-stage lung cancer even with\ntumors",
      "openalex_id": "https://openalex.org/W4391065733",
      "title": "Computational Pathology: A Survey Review and The Way Forward",
      "publication_date": "2024-01-21",
      "cited_by_count": 13.0,
      "topics": "Deep Learning in Medical Image Analysis, Advanced Techniques in Bioimage Analysis and Microscopy, Automated Analysis of Blood Cell Images",
      "keywords": "Viewpoints, Digital Pathology, Cyberinfrastructure, Sketch",
      "concepts": "Computer science, Digital pathology, Viewpoints, Data science, Workflow, Multidisciplinary approach, Field (mathematics), Cyberinfrastructure, Sketch, Interoperability, Management science, Artificial intelligence, World Wide Web, Art, Social science, Mathematics, Algorithm, Database, Sociology, Pure mathematics, Economics, Visual arts",
      "pdf_urls_by_priority": [
        "https://arxiv.org/pdf/2304.05482"
      ],
      "text_type": "full_text",
      "referenced_works": [
        "https://openalex.org/W1147193425",
        "https://openalex.org/W1173721425",
        "https://openalex.org/W1213336605",
        "https://openalex.org/W1536377526",
        "https://openalex.org/W1548827537",
        "https://openalex.org/W1576445103",
        "https://openalex.org/W1603593764",
        "https://openalex.org/W1812458631",
        "https://openalex.org/W1847493486",
        "https://openalex.org/W1888786797",
        "https://openalex.org/W1901164038",
        "https://openalex.org/W1932469787",
        "https://openalex.org/W1932924519",
        "https://openalex.org/W1950315773",
        "https://openalex.org/W1968629884",
        "https://openalex.org/W1970962264",
        "https://openalex.org/W1972719937",
        "https://openalex.org/W1974467617",
        "https://openalex.org/W1974728334",
        "https://openalex.org/W1978874224",
        "https://openalex.org/W1983346554",
        "https://openalex.org/W1985098533",
        "https://openalex.org/W1987944428",
        "https://openalex.org/W1989514509",
        "https://openalex.org/W1990362208",
        "https://openalex.org/W1994052710",
        "https://openalex.org/W1994829455",
        "https://openalex.org/W1995531130",
        "https://openalex.org/W1997864537",
        "https://openalex.org/W1997882893",
        "https://openalex.org/W2003103742",
        "https://openalex.org/W2004558777",
        "https://openalex.org/W2005393216",
        "https://openalex.org/W2011831342",
        "https://openalex.org/W2013413079",
        "https://openalex.org/W2021481801",
        "https://openalex.org/W2025464786",
        "https://openalex.org/W2026526178",
        "https://openalex.org/W2027925344",
        "https://openalex.org/W2038264173",
        "https://openalex.org/W2039544142",
        "https://openalex.org/W2047176521",
        "https://openalex.org/W2055145522",
        "https://openalex.org/W2055541283",
        "https://openalex.org/W2055965251",
        "https://openalex.org/W2057114171",
        "https://openalex.org/W2061872473",
        "https://openalex.org/W2065360903",
        "https://openalex.org/W2073501891",
        "https://openalex.org/W2075923670",
        "https://openalex.org/W2081139503",
        "https://openalex.org/W2083542943",
        "https://openalex.org/W2083927153",
        "https://openalex.org/W2085678918",
        "https://openalex.org/W2092123708",
        "https://openalex.org/W2093795918",
        "https://openalex.org/W2095114940",
        "https://openalex.org/W2103061399",
        "https://openalex.org/W2110243528",
        "https://openalex.org/W2110972788",
        "https://openalex.org/W2111574404",
        "https://openalex.org/W2115521901",
        "https://openalex.org/W2119460059",
        "https://openalex.org/W2122394460",
        "https://openalex.org/W2126179037",
        "https://openalex.org/W2128252595",
        "https://openalex.org/W2129439022",
        "https://openalex.org/W2130805253",
        "https://openalex.org/W2131673797",
        "https://openalex.org/W2132031490",
        "https://openalex.org/W2141292206",
        "https://openalex.org/W2141486320",
        "https://openalex.org/W2141967861",
        "https://openalex.org/W2150134401",
        "https://openalex.org/W2151554678",
        "https://openalex.org/W2152397906",
        "https://openalex.org/W2153702861",
        "https://openalex.org/W2158826452",
        "https://openalex.org/W2162548780",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W2164582889",
        "https://openalex.org/W2164589152",
        "https://openalex.org/W2166581609",
        "https://openalex.org/W2170566684",
        "https://openalex.org/W2185094885",
        "https://openalex.org/W2200290088",
        "https://openalex.org/W2221930818",
        "https://openalex.org/W2253429366",
        "https://openalex.org/W2264887978",
        "https://openalex.org/W2267490210",
        "https://openalex.org/W2279098554",
        "https://openalex.org/W2281825886",
        "https://openalex.org/W2282915343",
        "https://openalex.org/W2288892845",
        "https://openalex.org/W2302302587",
        "https://openalex.org/W2312404985",
        "https://openalex.org/W2314178310",
        "https://openalex.org/W2335197832",
        "https://openalex.org/W2338965302",
        "https://openalex.org/W2342014610",
        "https://openalex.org/W2342985385",
        "https://openalex.org/W2344480160",
        "https://openalex.org/W2401520370",
        "https://openalex.org/W2419518659",
        "https://openalex.org/W2435090885",
        "https://openalex.org/W2470130773",
        "https://openalex.org/W2470317160",
        "https://openalex.org/W2482581235",
        "https://openalex.org/W2491123226",
        "https://openalex.org/W2494544274",
        "https://openalex.org/W2497979631",
        "https://openalex.org/W2502134830",
        "https://openalex.org/W2504150216",
        "https://openalex.org/W2514628397",
        "https://openalex.org/W2521492299",
        "https://openalex.org/W2526468814",
        "https://openalex.org/W2529923829",
        "https://openalex.org/W2531813393",
        "https://openalex.org/W2564463480",
        "https://openalex.org/W2573152477",
        "https://openalex.org/W2581082771",
        "https://openalex.org/W2581851997",
        "https://openalex.org/W2592905743",
        "https://openalex.org/W2593345132",
        "https://openalex.org/W2594760301",
        "https://openalex.org/W2597507805",
        "https://openalex.org/W2601530120",
        "https://openalex.org/W2604440528",
        "https://openalex.org/W2605850958",
        "https://openalex.org/W2606429533",
        "https://openalex.org/W2607075141",
        "https://openalex.org/W2611254939",
        "https://openalex.org/W2613181504",
        "https://openalex.org/W2614808277",
        "https://openalex.org/W2616747498",
        "https://openalex.org/W2618999197",
        "https://openalex.org/W2620578070",
        "https://openalex.org/W2622416784",
        "https://openalex.org/W2623902889",
        "https://openalex.org/W2693096534",
        "https://openalex.org/W2716665989",
        "https://openalex.org/W2730845691",
        "https://openalex.org/W2740784636",
        "https://openalex.org/W2741128077",
        "https://openalex.org/W2741544066",
        "https://openalex.org/W2743136221",
        "https://openalex.org/W2749613778",
        "https://openalex.org/W2751723768",
        "https://openalex.org/W2752060020",
        "https://openalex.org/W2755347608",
        "https://openalex.org/W2756270667",
        "https://openalex.org/W2759577099",
        "https://openalex.org/W2759912964",
        "https://openalex.org/W2760354783",
        "https://openalex.org/W2760946358",
        "https://openalex.org/W2761290139",
        "https://openalex.org/W2761668583",
        "https://openalex.org/W2762672048",
        "https://openalex.org/W2764072425",
        "https://openalex.org/W2765375430",
        "https://openalex.org/W2769999077",
        "https://openalex.org/W2772723798",
        "https://openalex.org/W2774064151",
        "https://openalex.org/W2786974903",
        "https://openalex.org/W2788072220",
        "https://openalex.org/W2789755511",
        "https://openalex.org/W2791081619",
        "https://openalex.org/W2791915981",
        "https://openalex.org/W2793249006",
        "https://openalex.org/W2794803511",
        "https://openalex.org/W2795587190",
        "https://openalex.org/W2796409016",
        "https://openalex.org/W2801370692",
        "https://openalex.org/W2801540580",
        "https://openalex.org/W2801876365",
        "https://openalex.org/W2803416021",
        "https://openalex.org/W2803432927",
        "https://openalex.org/W2803469628",
        "https://openalex.org/W2805735218",
        "https://openalex.org/W2805886241",
        "https://openalex.org/W2806242585",
        "https://openalex.org/W2806562834",
        "https://openalex.org/W2808210572",
        "https://openalex.org/W2808324059",
        "https://openalex.org/W2809209815",
        "https://openalex.org/W2809275911",
        "https://openalex.org/W2809293192",
        "https://openalex.org/W2810818658",
        "https://openalex.org/W2811017491",
        "https://openalex.org/W2811123232",
        "https://openalex.org/W2883198641",
        "https://openalex.org/W2884083106",
        "https://openalex.org/W2884988214",
        "https://openalex.org/W2885343725",
        "https://openalex.org/W2885628138",
        "https://openalex.org/W2885650974",
        "https://openalex.org/W2885824038",
        "https://openalex.org/W2886093703",
        "https://openalex.org/W2886175403",
        "https://openalex.org/W2886631828",
        "https://openalex.org/W2889232360",
        "https://openalex.org/W2894917609",
        "https://openalex.org/W2896494478",
        "https://openalex.org/W2897068067",
        "https://openalex.org/W2897434820",
        "https://openalex.org/W2898020899",
        "https://openalex.org/W2899537006",
        "https://openalex.org/W2900118953",
        "https://openalex.org/W2900257566",
        "https://openalex.org/W2900654702",
        "https://openalex.org/W2901612843",
        "https://openalex.org/W2903119292",
        "https://openalex.org/W2903759724",
        "https://openalex.org/W2903829201",
        "https://openalex.org/W2905969903",
        "https://openalex.org/W2906233029",
        "https://openalex.org/W2906774465",
        "https://openalex.org/W2909488080",
        "https://openalex.org/W2910592851",
        "https://openalex.org/W2912194425",
        "https://openalex.org/W2912814480",
        "https://openalex.org/W2913326971",
        "https://openalex.org/W2913510405",
        "https://openalex.org/W2914038321",
        "https://openalex.org/W2914568698",
        "https://openalex.org/W2915853139",
        "https://openalex.org/W2915860004",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2922239620",
        "https://openalex.org/W2922268597",
        "https://openalex.org/W2924158535",
        "https://openalex.org/W2928842276",
        "https://openalex.org/W2929968583",
        "https://openalex.org/W2933939325",
        "https://openalex.org/W2938004941",
        "https://openalex.org/W2938704257",
        "https://openalex.org/W2941548848",
        "https://openalex.org/W2943370629",
        "https://openalex.org/W2943857514",
        "https://openalex.org/W2945123041",
        "https://openalex.org/W2945334889",
        "https://openalex.org/W2945500496",
        "https://openalex.org/W2945807221",
        "https://openalex.org/W2946027615",
        "https://openalex.org/W2946099713",
        "https://openalex.org/W2947078801",
        "https://openalex.org/W2947825023",
        "https://openalex.org/W2948141910",
        "https://openalex.org/W2948930564",
        "https://openalex.org/W2949226441",
        "https://openalex.org/W2949306187",
        "https://openalex.org/W2952003460",
        "https://openalex.org/W2952367870",
        "https://openalex.org/W2952481429",
        "https://openalex.org/W2952527443",
        "https://openalex.org/W2952800276",
        "https://openalex.org/W2952833648",
        "https://openalex.org/W2952846726",
        "https://openalex.org/W2955375681",
        "https://openalex.org/W2956019261",
        "https://openalex.org/W2956228567",
        "https://openalex.org/W2956362951",
        "https://openalex.org/W2956998909",
        "https://openalex.org/W2958836289",
        "https://openalex.org/W2960960151",
        "https://openalex.org/W2962804068",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W2963129226",
        "https://openalex.org/W2963258365",
        "https://openalex.org/W2963432486",
        "https://openalex.org/W2963519862",
        "https://openalex.org/W2964282006",
        "https://openalex.org/W2964345665",
        "https://openalex.org/W2964358045",
        "https://openalex.org/W2964756323",
        "https://openalex.org/W2965481926",
        "https://openalex.org/W2967444033",
        "https://openalex.org/W2968027356",
        "https://openalex.org/W2969278648",
        "https://openalex.org/W2969657290",
        "https://openalex.org/W2970152602",
        "https://openalex.org/W2971045153",
        "https://openalex.org/W2972214324",
        "https://openalex.org/W2972535786",
        "https://openalex.org/W2973237362",
        "https://openalex.org/W2973431617",
        "https://openalex.org/W2974825848",
        "https://openalex.org/W2975107127",
        "https://openalex.org/W2978575375",
        "https://openalex.org/W2978639826",
        "https://openalex.org/W2978882452",
        "https://openalex.org/W2979417040",
        "https://openalex.org/W2980998394",
        "https://openalex.org/W2981358604",
        "https://openalex.org/W2981466131",
        "https://openalex.org/W2982406227",
        "https://openalex.org/W2985301765",
        "https://openalex.org/W2986235590",
        "https://openalex.org/W2986668407",
        "https://openalex.org/W2988037626",
        "https://openalex.org/W2989695963",
        "https://openalex.org/W2989743390",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W2991151034",
        "https://openalex.org/W2994908874",
        "https://openalex.org/W2994910508",
        "https://openalex.org/W2995325438",
        "https://openalex.org/W2995682783",
        "https://openalex.org/W2996092187",
        "https://openalex.org/W2996780833",
        "https://openalex.org/W2998010746",
        "https://openalex.org/W2998141119",
        "https://openalex.org/W2998472909",
        "https://openalex.org/W2998794254",
        "https://openalex.org/W2999400187",
        "https://openalex.org/W2999417355",
        "https://openalex.org/W3003199121",
        "https://openalex.org/W3004016611",
        "https://openalex.org/W3004713990",
        "https://openalex.org/W3005130116",
        "https://openalex.org/W3005226777",
        "https://openalex.org/W3005999761",
        "https://openalex.org/W3006954975",
        "https://openalex.org/W3007110023",
        "https://openalex.org/W3007464329",
        "https://openalex.org/W3007935259",
        "https://openalex.org/W3008355217",
        "https://openalex.org/W3008693315",
        "https://openalex.org/W3008724309",
        "https://openalex.org/W3008782862",
        "https://openalex.org/W3009210879",
        "https://openalex.org/W3009332900",
        "https://openalex.org/W3009334705",
        "https://openalex.org/W3009926465",
        "https://openalex.org/W3009928129",
        "https://openalex.org/W3010168554",
        "https://openalex.org/W3010834500",
        "https://openalex.org/W3011037969",
        "https://openalex.org/W3011592483",
        "https://openalex.org/W3011599283",
        "https://openalex.org/W3011941780",
        "https://openalex.org/W3013627541",
        "https://openalex.org/W3014402422",
        "https://openalex.org/W3015101370",
        "https://openalex.org/W3015357052",
        "https://openalex.org/W3015450039",
        "https://openalex.org/W3016265934",
        "https://openalex.org/W3016293126",
        "https://openalex.org/W3016315468",
        "https://openalex.org/W3016553397",
        "https://openalex.org/W3017354830",
        "https://openalex.org/W3018429932",
        "https://openalex.org/W3018647685",
        "https://openalex.org/W3020560751",
        "https://openalex.org/W3020916919",
        "https://openalex.org/W3021413349",
        "https://openalex.org/W3021500318",
        "https://openalex.org/W3022952790",
        "https://openalex.org/W3024962477",
        "https://openalex.org/W3027616379",
        "https://openalex.org/W3027869849",
        "https://openalex.org/W3030555654",
        "https://openalex.org/W3030889987",
        "https://openalex.org/W3032131871",
        "https://openalex.org/W3035834839",
        "https://openalex.org/W3036686253",
        "https://openalex.org/W3037083567",
        "https://openalex.org/W3037683614",
        "https://openalex.org/W3037771725",
        "https://openalex.org/W3038704997",
        "https://openalex.org/W3040472729",
        "https://openalex.org/W3040734937",
        "https://openalex.org/W3040784645",
        "https://openalex.org/W3043535018",
        "https://openalex.org/W3044364303",
        "https://openalex.org/W3044753181",
        "https://openalex.org/W3045295565",
        "https://openalex.org/W3046305306",
        "https://openalex.org/W3046400384",
        "https://openalex.org/W3046446892",
        "https://openalex.org/W3049608122",
        "https://openalex.org/W3081006013",
        "https://openalex.org/W3081081614",
        "https://openalex.org/W3083699157",
        "https://openalex.org/W3086458417",
        "https://openalex.org/W3086667591",
        "https://openalex.org/W3089090082",
        "https://openalex.org/W3090112028",
        "https://openalex.org/W3092571470",
        "https://openalex.org/W3092698347",
        "https://openalex.org/W3093002967",
        "https://openalex.org/W3093046161",
        "https://openalex.org/W3094111018",
        "https://openalex.org/W3096540902",
        "https://openalex.org/W3098027819",
        "https://openalex.org/W3098525219",
        "https://openalex.org/W3099287508",
        "https://openalex.org/W3099661382",
        "https://openalex.org/W3100003598",
        "https://openalex.org/W3100084586",
        "https://openalex.org/W3100279624",
        "https://openalex.org/W3100887207",
        "https://openalex.org/W3101216323",
        "https://openalex.org/W3104041126",
        "https://openalex.org/W3104135675",
        "https://openalex.org/W3104211245",
        "https://openalex.org/W3104370314",
        "https://openalex.org/W3105141008",
        "https://openalex.org/W3105980361",
        "https://openalex.org/W3106231035",
        "https://openalex.org/W3107410755",
        "https://openalex.org/W3108329040",
        "https://openalex.org/W3109000640",
        "https://openalex.org/W3109246949",
        "https://openalex.org/W3112301260",
        "https://openalex.org/W3113328935",
        "https://openalex.org/W3113891464",
        "https://openalex.org/W3114754248",
        "https://openalex.org/W3115970425",
        "https://openalex.org/W3119036516",
        "https://openalex.org/W3119330513",
        "https://openalex.org/W3119441468",
        "https://openalex.org/W3121859091",
        "https://openalex.org/W3121943121",
        "https://openalex.org/W3126783352",
        "https://openalex.org/W3127935213",
        "https://openalex.org/W3128158496",
        "https://openalex.org/W3128202743",
        "https://openalex.org/W3128210037",
        "https://openalex.org/W3129033913",
        "https://openalex.org/W3130735597",
        "https://openalex.org/W3131525335",
        "https://openalex.org/W3132095500",
        "https://openalex.org/W3132964805",
        "https://openalex.org/W3133782060",
        "https://openalex.org/W3135068374",
        "https://openalex.org/W3135547872",
        "https://openalex.org/W3135550350",
        "https://openalex.org/W3137675974",
        "https://openalex.org/W3138548427",
        "https://openalex.org/W3138848412",
        "https://openalex.org/W3138973186",
        "https://openalex.org/W3141797743",
        "https://openalex.org/W3144167624",
        "https://openalex.org/W3148150040",
        "https://openalex.org/W3152579480",
        "https://openalex.org/W3154566661",
        "https://openalex.org/W3159302505",
        "https://openalex.org/W3160261825",
        "https://openalex.org/W3160817203",
        "https://openalex.org/W3161570989",
        "https://openalex.org/W3163363865",
        "https://openalex.org/W3164754318",
        "https://openalex.org/W3165730810",
        "https://openalex.org/W3166254754",
        "https://openalex.org/W3170284304",
        "https://openalex.org/W3172675935",
        "https://openalex.org/W3174246647",
        "https://openalex.org/W3174477388",
        "https://openalex.org/W3176446229",
        "https://openalex.org/W3176719058",
        "https://openalex.org/W3178519255",
        "https://openalex.org/W3180779372",
        "https://openalex.org/W3181044391",
        "https://openalex.org/W3181082054",
        "https://openalex.org/W3181414820",
        "https://openalex.org/W3181481201",
        "https://openalex.org/W3181599570",
        "https://openalex.org/W3183075879",
        "https://openalex.org/W3183916696",
        "https://openalex.org/W3185094524",
        "https://openalex.org/W3185935705",
        "https://openalex.org/W3186317940",
        "https://openalex.org/W3186679119",
        "https://openalex.org/W3186966887",
        "https://openalex.org/W3187624058",
        "https://openalex.org/W3189996993",
        "https://openalex.org/W3190070581",
        "https://openalex.org/W3191409400",
        "https://openalex.org/W3191831606",
        "https://openalex.org/W3192682950",
        "https://openalex.org/W3195127593",
        "https://openalex.org/W3196270674",
        "https://openalex.org/W3196396697",
        "https://openalex.org/W3197713479",
        "https://openalex.org/W3199764674",
        "https://openalex.org/W3201077454",
        "https://openalex.org/W3201366746",
        "https://openalex.org/W3202910984",
        "https://openalex.org/W3203957008",
        "https://openalex.org/W3204013916",
        "https://openalex.org/W3205594709",
        "https://openalex.org/W3205605830",
        "https://openalex.org/W3206263253",
        "https://openalex.org/W3206795303",
        "https://openalex.org/W3207188103",
        "https://openalex.org/W3207878045",
        "https://openalex.org/W3208672800",
        "https://openalex.org/W3208934141",
        "https://openalex.org/W3209331124",
        "https://openalex.org/W3210191307",
        "https://openalex.org/W3210579795",
        "https://openalex.org/W3211109300",
        "https://openalex.org/W3211190672",
        "https://openalex.org/W3211324396",
        "https://openalex.org/W3211647829",
        "https://openalex.org/W3212108649",
        "https://openalex.org/W3212889265",
        "https://openalex.org/W3213974477",
        "https://openalex.org/W3214733388",
        "https://openalex.org/W3214930412",
        "https://openalex.org/W3215159356",
        "https://openalex.org/W3215375644",
        "https://openalex.org/W3215409948",
        "https://openalex.org/W3216717225",
        "https://openalex.org/W3217644914",
        "https://openalex.org/W4200008359",
        "https://openalex.org/W4200040427",
        "https://openalex.org/W4200124884",
        "https://openalex.org/W4200219316",
        "https://openalex.org/W4200268060",
        "https://openalex.org/W4205382356",
        "https://openalex.org/W4205876996",
        "https://openalex.org/W4205900565",
        "https://openalex.org/W4206174637",
        "https://openalex.org/W4206759694",
        "https://openalex.org/W4207066074",
        "https://openalex.org/W4210426457",
        "https://openalex.org/W4210957076",
        "https://openalex.org/W4211119275",
        "https://openalex.org/W4214547251",
        "https://openalex.org/W4220675364",
        "https://openalex.org/W4220677231",
        "https://openalex.org/W4220712379",
        "https://openalex.org/W4220795582",
        "https://openalex.org/W4220896502",
        "https://openalex.org/W4220911728",
        "https://openalex.org/W4221022458",
        "https://openalex.org/W4221030665",
        "https://openalex.org/W4221165931",
        "https://openalex.org/W4224294628",
        "https://openalex.org/W4225142522",
        "https://openalex.org/W4225480823",
        "https://openalex.org/W4225746601",
        "https://openalex.org/W4225919190",
        "https://openalex.org/W4225981821",
        "https://openalex.org/W4226089600",
        "https://openalex.org/W4226190792",
        "https://openalex.org/W4226236462",
        "https://openalex.org/W4226369938",
        "https://openalex.org/W4226409235",
        "https://openalex.org/W4226426271",
        "https://openalex.org/W4226512818",
        "https://openalex.org/W4229049871",
        "https://openalex.org/W4229080888",
        "https://openalex.org/W4229958410",
        "https://openalex.org/W4229980196",
        "https://openalex.org/W4232611972",
        "https://openalex.org/W4234173207",
        "https://openalex.org/W4235809187",
        "https://openalex.org/W4236965008",
        "https://openalex.org/W4238455477",
        "https://openalex.org/W4239252183",
        "https://openalex.org/W4241929126",
        "https://openalex.org/W4242628528",
        "https://openalex.org/W4242705165",
        "https://openalex.org/W4243985867",
        "https://openalex.org/W4246673929",
        "https://openalex.org/W4247943011",
        "https://openalex.org/W4247960443",
        "https://openalex.org/W4248810809",
        "https://openalex.org/W4249502209",
        "https://openalex.org/W4249751009",
        "https://openalex.org/W4249876349",
        "https://openalex.org/W4249914127",
        "https://openalex.org/W4251573514",
        "https://openalex.org/W4253944268",
        "https://openalex.org/W4255542875",
        "https://openalex.org/W4255556797",
        "https://openalex.org/W4256012820",
        "https://openalex.org/W4280531683",
        "https://openalex.org/W4280555205",
        "https://openalex.org/W4280595708",
        "https://openalex.org/W4281386768",
        "https://openalex.org/W4283266161",
        "https://openalex.org/W4283520680",
        "https://openalex.org/W4283806266",
        "https://openalex.org/W4283813454",
        "https://openalex.org/W4285105280",
        "https://openalex.org/W4285236225",
        "https://openalex.org/W4285651992",
        "https://openalex.org/W4285721697",
        "https://openalex.org/W4287774581",
        "https://openalex.org/W4288257105",
        "https://openalex.org/W4288860568",
        "https://openalex.org/W4289286782",
        "https://openalex.org/W4289454179",
        "https://openalex.org/W4289704839",
        "https://openalex.org/W4290973455",
        "https://openalex.org/W4291019968",
        "https://openalex.org/W4292318690",
        "https://openalex.org/W4292369513",
        "https://openalex.org/W4292907187",
        "https://openalex.org/W4293163051",
        "https://openalex.org/W4293213992",
        "https://openalex.org/W4293583832",
        "https://openalex.org/W4294612686",
        "https://openalex.org/W4295093036",
        "https://openalex.org/W4296083357",
        "https://openalex.org/W4296693015",
        "https://openalex.org/W4297249536",
        "https://openalex.org/W4297371220",
        "https://openalex.org/W4298204340",
        "https://openalex.org/W4302192999",
        "https://openalex.org/W4309214336",
        "https://openalex.org/W4310051789",
        "https://openalex.org/W431215260",
        "https://openalex.org/W4312747027",
        "https://openalex.org/W4312906155",
        "https://openalex.org/W4313028954",
        "https://openalex.org/W4313397115",
        "https://openalex.org/W4313830579",
        "https://openalex.org/W4318220659",
        "https://openalex.org/W4320350422",
        "https://openalex.org/W4322721845",
        "https://openalex.org/W4323926462",
        "https://openalex.org/W4377010966",
        "https://openalex.org/W4381573401",
        "https://openalex.org/W4382142072",
        "https://openalex.org/W4383101683",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385948838",
        "https://openalex.org/W4386076316",
        "https://openalex.org/W4386171305",
        "https://openalex.org/W4387211376",
        "https://openalex.org/W4387211411",
        "https://openalex.org/W4393053949",
        "https://openalex.org/W611401390",
        "https://openalex.org/W92498844"
      ],
      "openalex_rank": 15,
      "num_tokens": 8965,
      "best_oa_location_pdf_url": NaN,
      "successful_pdf_url": NaN,
      "question_generated": "What are the current methodologies, challenges, and future directions in the field of computational pathology, particularly regarding the integration of digital diagnostics into clinical practice?",
      "pipeline_answer": "### Current Methodologies in Computational Pathology\n- **Computer-Aided Diagnosis (CAD)**: CAD systems are essential for enhancing diagnostic accuracy and efficiency in histopathology. They assist pathologists by identifying benign areas, allowing them to concentrate on more complex cases [S8880063867] [S0407710670].\n- **Quantitative Image Analysis**: This approach is increasingly recognized for disease grading, particularly in cancers like prostate cancer, where traditional grading systems have shown variability [S2342171777].\n- **Deep Learning Techniques**: The advent of deep learning has revolutionized histopathological image analysis, enabling automated detection and classification of cancerous tissues with improved accuracy compared to traditional methods [S1852133643] [S4628432253].\n\n### Challenges in Computational Pathology\n- **Data Requirements**: A significant challenge is the need for large, annotated datasets to train machine learning models effectively [S2587377261].\n- **Complexity of Images**: The intricate nature of histopathological images necessitates advanced segmentation and analysis techniques, complicating the development of reliable computational tools [S2587377261].\n\n### Future Directions\n- **Integration of Multimodal Data**: Future advancements may involve combining histopathological images with genomic data to enhance diagnostic capabilities and support personalized treatment strategies [S5781812900] [S1925973567].\n\n### Perspectives of Pathologists\n- **Need for Robust Frameworks**: Pathologists emphasize the necessity for evidence-based frameworks to develop and implement digital diagnostic tools that can achieve regulatory approval, reflecting a cautious yet optimistic view on AI integration [S7579489834].\n- **Positive Outlook on Digital Solutions**: The increasing complexity of histopathological images and the demand for precise biomarker assessments are driving pathologists to seek advanced digital solutions, indicating a favorable perspective on digital diagnostics [S5314181021].\n- **Efficiency and Error Reduction**: There is a recognition among pathologists that integrating digital pathology workflows can enhance efficiency and reduce human error, further supporting the adoption of digital diagnostics in clinical practice [S0013014263].",
      "pipeline_references": {
        "S7579489834": {
          "id": "S7579489834",
          "text": "Pathologists express a need for robust, evidence-based frameworks to develop and implement digital diagnostic tools that meet regulatory approval, indicating a cautious but optimistic perspective on the integration of AI in clinical practice.",
          "children": [
            {
              "id": "E1088818471",
              "text": "Perspective  \n   ,    \nCorresponding Author\nRichard Colling\n  richard.colling@nds.ox.ac.uk  \n  orcid.org/0000-0001-6344-9081  \nNuffield Department of Surgical Sciences, University of Oxford, John Radcliffe Hospital, Oxford UK Correspondence to: R Colling, Nuffield Department of Surgical Sciences, University of Oxford, John Radcliffe Hospital, Oxford OX3 9DU, UK. E-mail:  richard.colling@nds.ox.ac.uk  Search for more papers by this author     Helen Pitman  ,  \nHelen Pitman\nNational Cancer Research Institute, London, UK  Search for more papers by this author      Karin Oien  ,  \nKarin Oien\nInstitute of Cancer Sciences, University of Glasgow, Glasgow, UK  Search for more papers by this author      Nasir Rajpoot  ,  \nNasir Rajpoot\nDepartment of Computer Science, University of Warwick, Coventry, UK  Search for more papers by this author      Philip Macklin  ,  \nPhilip Macklin\n  orcid.org/0000-0002-7750-3242  \nNuffield Department of Medicine, University of Oxford, Oxford, UK  Search for more papers by this author      CM-Path AI in Histopathology Working Group  ,  \nCM-Path AI in Histopathology Working Group CM-Path AI in Histopathology Working Group: Velicia Bachtiar, Richard Booth, Alyson Bryant, Joshua Bull, Jonathan Bury, Fiona Carragher, Richard Colling, Graeme Collins, Clare Craig, Maria Freitas da Silva, Daniel Gosling, Jaco Jacobs, Lena Kajland-Wil\u00e9n, Johanna Karling, Darragh Lawler, Stephen Lee, Philip Macklin, Keith Miller, Guy Mozolowski, Richard Nicholson, Daniel O'Connor, Mikkel Rahbek, Nasir Rajpoot, Alan Sumner, Dirk Vossen, Kieron White, Charlotte Wing, Corrina Wright.Search for more papers by this author      David Snead  ,  \nDavid Snead\nPathLAKE (Director) and Histopathology, University Hospitals Coventry and Warwickshire NHS Trust, University Hospital, Coventry, UK Joint senior authors contributed equally.Search for more papers by this author      Tony Sackville  ,  \nTony Sackville\nBritish In Vitro Diagnostics Association, London, UK Joint senior authors contributed equally.Search for more papers by this author      Clare Verrill  ,  \nClare Verrill\nPathLAKE (Principal Investigator), Nuffield Department of Surgical Sciences and Oxford NIHR Biomedical Research Centre, University of Oxford, John Radcliffe Hospital, Oxford, UK Joint senior authors contributed equally.Search for more papers by this author      \n  First published:  29 May 2019 \n Citations: 163   \nNo conflicts of interest were declared.\n  \nAbstract\nThe use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence-based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM-Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. \u00a9 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley &amp; Sons, Ltd.\n \n References \n    Citing Literature       \n   \n    References     Related     Information    \nRecommended              Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association       Esther Abels  ,       Liron Pantanowitz  ,       Famke Aeffner  ,       Mark D Zarella  ,       Jeroen van der Laak  ,       Marilyn M Bui  ,       Venkata NP Vemuri  ,       Anil V Parwani  ,       Jeff Gibbs  ,       Emmanuel Agosto-Arroyo  ,       Andrew H Beck  ,       Cleopatra Kozlowski  ,",
              "url": "https://openalex.org/W2945500496",
              "openalex_id": "https://openalex.org/W2945500496",
              "title": "Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",
              "publication_date": "2019-05-30"
            },
            {
              "id": "E4893251842",
              "text": "Wiley Online Library\nEsther Abels\nRegulatory and Clinical Affairs\nPathAI, BostonMAUSA\nLiron Pantanowitz\nDepartment of Pathology\nUniversity of Pittsburgh Medical Center\nPittsburghPAUSA\nFamke Aeffner\nComparative Biology and Safety Sciences\nAmgen Research\nAmgen Inc\nSouth San FranciscoCAUSA\nMark D Zarella\nDepartment of Pathology and Laboratory Medicine\nDrexel University College of Medicine\nPhiladelphiaPAUSA\nJeroen Van Der Laak\nDepartment of Pathology\nRadboud University Medical Center\nNijmegenThe Netherlands\nCenter for Medical Image Science and Visualization\nLink\u00f6ping University\nLink\u00f6pingSweden\nMarilyn M Bui\nDepartment of Pathology\nMoffitt Cancer Center\nTampaFLUSA\nVenkata Np Vemuri\nData Science Department\nChan Zuckerberg Biohub\nSan FranciscoCAUSA\nAnil V Parwani\nDepartment of Pathology\nThe Ohio State University\nColumbusOHUSA\nJeff Gibbs\nHyman, Phelps & McNamara, P.C\nWashingtonDCUSA\nEmmanuel Agosto-Arroyo\nDepartment of Pathology\nMoffitt Cancer Center\nTampaFLUSA\nAndrew H Beck\nPathAI\nBostonMAUSA\nCleopatra Kozlowski cleopatk@gene.com\nDepartment of Development Sciences\nGenentech Inc\nSouth San FranciscoCAUSA\nCorrespondence to: C Kozlowski\nGenentech Inc\n1 DNA Way, South San Francisco94080CAUSA\nChan Zuckerberg Biohub\nEsther Abels\nPathAI)Andrew Beck\nCleopatra Kozlowski\nComputational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association\nLaak ; is a member of the advisory boards of Philips, The Netherlands and ContextVision, Sweden, and receives research funding from Philips, The Netherlands and Sectra, Sweden. Liron Pantanowitz is on the medical advisory board of Leica and Ibex; consults for Hamamatsu; and receives research funding from Ibex, Lunit, and Huron.\nJournal of Pathology J Pathol\n249201910.1002/path.5331Received 16 April 2019; Revised 18 July 2019; Accepted 26 July 2019REVIEW (Genentech is a member of the Roche Group, which also owns Ventana Medical Systems), and Jeffrey Gibbs (Hyman, Phelps & McNamara, P.C.). Jeroen van derartificial intelligencecomputational pathologyconvolutional neural networksdigital pathologydeep learningimage analysismachine learning\nIn this white paper, experts from the Digital Pathology Association (DPA) define terminology and concepts in the emerging field of computational pathology, with a focus on its application to histology images analyzed together with their associated patient data to extract information. This review offers a historical perspective and describes the potential clinical benefits from research and applications in this field, as well as significant obstacles to adoption. Best practices for implementing computational pathology workflows are presented. These include infrastructure considerations, acquisition of training data, quality assessments, as well as regulatory, ethical, and cyber-security concerns. Recommendations are provided for regulators, vendors, and computational pathology practitioners in order to facilitate progress in the field.Conflict of interest statement: All authors are members of the Digital Pathology Association (DPA), a non-profit organization comprising pathologists, scientists, technologists, and representatives from industry that focuses on developing education, awareness, and implementation of digital pathology applications and artificial intelligence in healthcare and life sciences. The DPA's committees and task forces are dedicated to enhancing the field.The association collaborates with the FDA on equipment approvals and addressing technology regulations. The members are encouraged to share best practices and promote the use of the technology among colleagues to demonstrate efficiencies and share knowledge and its ultimate benefits to patient care. For more information, visit https://digitalpathologyassociation.org. The following authors either are employed by, and/or own stock in, companies that offer computational pathology related or provide legal services to such companies: Venkata NP Vemuri (\nIntroduction: goals of this paper\nThe term computational pathology (CPATH) has become a buzz-word among the digital pathology community, yet it often leads to confusion due to its use in different contexts [1][2][3]. The expert authors of the Digital Pathology Association (DPA) define CPATH as the 'omics' or 'big-data' approach to pathology, where multiple sources of patient information including pathology image data and meta-data are combined to Table 1\n. Definitions of CPATH terms\nAnnotation\nIndication of the position and/or outline of structures or objects within digital images, usually produced by humans using a computer mouse or drawing tablet. Annotations may have associated labels and possible other meta-data. Annotations can be manually generated or can be established by algorithm tools\nArtificial intelligence (AI)\nA branch of computer science dealing with the simulation of intelligent behavior in computers Black box/glass box A neural network can be perceived as a black box that lacks a clear depiction of the image features used for a decision. However, methods can be employed to transform it into a glass box in an effort to understand the relationship between the input parameters and the output of the network Cloud computing\nThe practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer Computational pathology (CPATH)\nA branch of pathology that involves computational analysis of a broad array of methods to analyze patient specimens for the study of disease. In this paper, we focus on the extraction of information from digitized pathology images in combination with their associated meta-data, typically using AI methods such as deep learning\nConvolutional neural network (CNN)\nA type of deep neural network particularly designed for images. It uses a kernel or filter to convolve an image, which results in features useful for differentiating images Data augmentation\nMethod commonly used in deep learning to increase the training data using operations such as rotating, cropping, zooming, and image histogram-based modifications. This provides a number of advantages such as promoting positional and rotational invariance, robustness to staining variability, and improves the generalizability of the classifier Deep learning\nThe subset of machine learning composed of algorithms that permit software to train itself to perform tasks by exposing multilayered artificial neural networks to vast amounts of data. Data are fed into the input layers and are sequentially processed in a hierarchical manner with increasing complexity at each layer, modeled loosely after the hierarchical organization in the brain. Optimization functions are iteratively trained to shape the processing functions of the layers and the connections between them Digital pathology A blanket term that encompasses tools and systems to digitize pathology slides and associated meta-data, their storage, review, analysis, and enabling infrastructure Gold standard\nThe practical standard that is used to capture the 'ground truth'. The gold standard may not always be perfectly correct, but in general is viewed as the best approximation Ground truth (as considered within AI) A category, quantity, or label assigned to a dataset that provides guidance to an algorithm during training.\nDepending on the task, the ground truth can be a patient-or slide-level characterization or can be applied to objects or regions within the image. The ground truth is an abstract concept of the 'truth'\nImage analysis\nA method to extract typically quantifiable information from images. In this paper, we only discuss image analysis as applied to images of histology slides, but the term itself is broader, and applies to the extraction of information from any image, biomedical or not\nMachine learning (ML)\nA branch of AI in which computer software learns to perform a task by being exposed to representative data Meta-data\nIn the context of digital pathology, the term meta-data describes descriptive data associated with the individual, sample, or slide. They may include image acquisition information, patient demographic data, pathologist annotation or classification, or outcome data from treatment. Typically, meta-data are entries that allow searches in databases, for example. Highly complex, large, multiple-time-point associated data, such as longitudinal image data (such as radiology) or genomic data, are not usually called 'meta-data' Supervised machine learning Supervised learning is used to train a model to predict an outcome or to classify a dataset based on a label associated with a data point (i.e. ground truth). An example of supervised machine learning includes the design of classifiers to distinguish benign from malignant regions based on manual annotations Unsupervised machine learning Unsupervised learning seeks to identify natural divisions in a dataset without the need for a ground truth, often using methods such as cluster analysis or pattern matching. Examples of unsupervised machine learning include the identification of images with similar attributes or the clustering of tumors into subtypes Whole slide image Digital representation of an entire histopathological glass slide, digitized at microscope resolution. These whole slide scans are typically produced using slide scanners. Slide scan viewing software enables inspection of the image in a way that mimics the use of a traditional microscope; the image can be viewed at different magnifications extract patterns and analyze features. In this white paper, we will focus on a subset of this field, encompassing CPATH applications related to whole slide imaging (WSI) and analysis. CPATH is only one of a large number of fashionable terms that are confusingly used apparently interchangeably, yet mean somewhat different things. To assi",
              "url": "https://openalex.org/W2964756323",
              "openalex_id": "https://openalex.org/W2964756323",
              "title": "Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association",
              "publication_date": "2019-07-29"
            }
          ]
        },
        "S5314181021": {
          "id": "S5314181021",
          "text": "The increasing complexity of histopathological images and the demand for precise biomarker assessment are driving pathologists to seek advanced digital solutions, reflecting a positive perspective on the potential of digital diagnostics to enhance diagnostic accuracy.",
          "children": [
            {
              "id": "E7573133703",
              "text": "..multigene testing, to obtain surrogate definition of subtypes.68 The ER-expressing luminal A and B subtypes account for 70% of all breast tumors. The hormone receptor positive, low proliferating luminal A tumors generally have good prognosis and endocrine response,69 whereas the high proliferating luminal B subtype show poorer prognosis and reduced sensitivity to endocrine therapy.47 The HER2- enriched subgroup, which can be effectively targeted by anti-HER2-therapy, corresponds to ER-negative and HER2-positive tumors.70 Triple-negative tumors correspond more or less to the basal-like subtype.69 WHY DIGITAL PATHOLOGY? Around the world, pathology laboratories are faced with fixed health-care budgets in the face of growing numbers of patients from an aging population. At the same time, there is a shortage of trained pathologists, and patient demand for precision diagnostics and treatment is increasing. Efficient and cost-effective methods are highly desired to address these needs and to modernize routine pathology. The digitization of pathology data has opened the door to faster, more reproducible, and more precise diagnoses through computerized image analysis. Digital pathology is a dynamic, rapidly evolving, image-based discipline that incorporates acquisition, management, and interpretation of pathology information obtained from a digital scan of a glass slide. In several validation studies, the concordance between digital image diagnosis and conventional glass slide diagnosis has shown to be good to superior.71-77 Digitalized images make consultation between expert pathologists easier, and facilitate automated image analysis. The goal of digital pathology is not to take over the pathologist\u2019s work, but to improve accuracy, reduce human error, and provide tools for a more efficient workflow and increased reproducibility. Other benefits of digitization include a reduction in the cost of handling glass slides and the ability to share data for education or long-distance consultations.78 In addition, the image quality of a digital slide scan is preserved, whereas the staining in the physical sample fades over time. Laboratories with integrated digital pathology workflows and interpretation are still sparse today. This stands in contrast to radiology, which in most countries underwent digitalization over the past decades. Picture archiving and communication systems have been in place for radiology departments for more than 20 years. Consequently, medical image analysis development has largely focused on digital radiology data, producing important advances with clinical implementations.79,80 Technical limitations have slowed progress in digital histopathology\u2014 primarily because the images can be up to 10 times the size of radiology images. Histologic glass slides are converted to digital images using high-resolution wholeslide imaging (WSI) scanners. The entire process involves several steps: image acquisition, storage and management, annotation, and viewing or sharing. Several pathology laboratories are integrating WSI scanners into the routine workflow in an effort to digitalize the diagnostic workflow. However, challenges to find solutions that provide sufficient storage capacity with reasonable archiving costs remain. The digitalization of pathology data opens the door to quantitative computer-based image analysis, and could prove to be clinically important as a tool to accurately identify high-risk patients. Novel digital image analysis algorithms to improve therapy prediction and survival prognostication are of utmost value in the clinicopathologic setting. Current classification of biomarkers based on manual measurement is arbitrary: furthermore, manual measurement and counting of cell numbers hampers reproducibility. Depending on the downstream analyses that will be performed, manual annotation of the digitalized slides by a trained pathologist can be required, for example, to delineate cancer or Translational Research 22 Robertson et al April 2018 metastasis and exclude artifacts. That can be a timeconsuming step. Computerized image analysis promises to improve reproducibility. However, there is controversy about how image analysis should be implemented.28 EXISTING DIGITAL IMAGE ANALYSIS METHODS Pathology is an image-based discipline, traditionally with the light field microscope as the major working tool for image interpretation. Computerized image analysis software has been developed to aid the pathologist\u2019s evaluation of whole-slide images. Such software enables quantitative image analysis, in an effort to improve accuracy, reliability, reproducibility, and productivity. Computerized image analysis is not a novel research area in histopathology. Over the years, methods have been developed to reduce variations in image quality, for example, through color standardization, spatial filtering, denoising, or enhancement.81 A significant amount of work has been devoted to the automatic segmentation of nuclei, for example, applying active contour models.82 This is a general segmentation technique, which fits a deformable shape model to a given image.83-85 Research has also concentrated on mitosis detection.56 For a detailed review of object detection and segmentation, see Veta et al. and Gurcan et al.86,87 In this review, we do not intend to cover all the previous techniques for image analysis in histopathology, but to give the current state of DIA focusing on breast pathology. ImageJ, the free and accessible Java-based, userfriendly image analysis tool developed by the National Institutes of Health (Bethesda, MD), is arguably the most popular open tool for biomedical image analysis. It can readily be applied to quantify IHC or other specific tasks through different analysis and processing plugins. ImmunoRatio (University of Tampere, Finland) is a tool developed for automated analysis of IHC biomarker assessment (ER, PR, and Ki67). It provides a ratio of positively stained tumor nuclei area (Fig 2). It is available as a web application or as a plugin for ImageJ. ImmunoRatio has been used by various research groups, and studies computing the Ki67 labeling index have shown excellent concordance to manual assessment in breast cancer.88,89 Similarly, the publicly available application ImmunoMembrane has been evaluated for HER2 IHC.90 Today, in industry, there is increasing competition for digital pathology image analysis solutions. This section gives a brief overview of the most common commercially available software for breast pathology. Roche VENTANA image analysis algorithm for IHC assays in breast pathology is a solution used to quantify breast panel biomarkers and to provide an integrated solution including antibody assays. In 2014, AstraZeneca acquired the imaging and data analysis technology company Definiens, and is incorporating their Tissue Phenomics software for clinical programs in immune-oncology and predictive biomarker discovery. Visiopharm (Hoersholm, Denmark) Virtual Double Staining techniques is using a pancytokeratin-stained tissue section, which is aligned to the IHC-stained biomarker of interest and enables automated detection of tumor regions (Fig 2).91-93 The abovementioned commercial platforms operate on input from a WSI scanner, whereas the Aperio Digital Pathology (Leica Biosystems, Nussloch, Germany) platform operates by integrating a digital microscope with the image software. The Aperio Digital Pathology usersupervised platform has been compared with the automated Definiens Tissue Studio platform for classification of ER and PR IHC positivity.94 TissueGnostics analysis software (Vienna, Austria) offers image analysis applications for clinical and research assessment of biomarkers in breast cancer. The rapid development of image analysis software and integrated solutions for histopathologic diagnostics will most certainly continue for the coming years, with close competition in the industry. If trends continue, these or similar software packages will become an integrated part of routine digitalized diagnostics. Up to now, most research on digital image analysis has focused on quantifying biomarkers by IHC. DIA has shown excellent reproducibility, although limited to subsets with individual biomarkers or smaller cohorts.93,95-97 Hartman et al. have recently shown the advantages in congruence to gene expression assays and the prognostic power of digital image analysis compared with current manual methods of biomarker assessments.91 Automated image analysis has been applied to analyze several biomarkers, including HER2 expression, holding promise to reduce the need for additional ISH analysis in HER2 equivocal cases95,98 and significantly reducing inter- and intraobserver variability.41,42 Applying automated image analysis on IHC cytokeratin-stained sentinel node biopsies, negative (metastasis-free) samples can be eliminated with 100% sensitivity and used as a screening tool.99 Furthermore, significant compression and scaling of large whole-slide images can be performed without comprising automated IHC biomarker assessment.100 An important next step will be to focus on the assessment of the basic H&E-stained tissue sections and the development of computer-aided diagnosis (CAD) algorithms for WSI. MACHINE LEARNING IN DIGITAL PATHOLOGY IMAGE ANALYSIS Recent breakthroughs in AI promise to fundamentally change the way we detect and treat breast cancer in Translational Research Volume 194 Robertson et al 23 the near future. The difference between AI, machine learning, and deep learning is not always obvious to nonexperts. AI is an umbrella term encompassing the techniques for a machine to mimic or go beyond human intelligence, mainly in cognitive capabilities. AI includes a variety of subfields such as rule-based systems,101 a classical approach to AI, in which the programmer explicitly encodes the knowledge provided by the task experts. In contrast, machine learning is a subfield of AI that applies statistical methods to learn to recognize patterns from a set of provided data without explicit human instruction. Deep learning is a recent machine learning approach that uses biologically inspired networks to represent data through multiple levels of simple but nonlinear modules that transform the previous representation into a higher, slightly more abstract representation. The compositional nature of the architecture allows deep neural networks to form highly complex and nonlinear representations that provide unprecedented discriminatory power. Deep networks have produced groundbreaking results in many important problems including image classification1 and speech recognition.2 Computerized diagnostic systems in medicine102 and technology in general,103,104 have traditionally been rulebased. However, in the past years, we have witnessed pivotal progress in several aspects; the advent of powerful machine learning techniques, the advancement of graphics computational resources, and the ever-increasing digitization of medical data. These developments resulted in an explosion of interest in machine learning as these systems gradually replace classic image analysis techniques for automatic medical diagnosis. The purpose of a machine learning algorithm is to use the provided task-related training data to learn a task, Fig 2. Digital image analysis for breast pathology. (A-B) Digital automated scoring of immunohistochemical (IHC) Ki67 using ImmunoRatio application for ImageJ. (A) Original image of IHC staining for Ki67 (brown nuclei). (B) Image showing staining components. Positive nuclei = orange. Negative nuclei = blue. (C-D) Visiopharm..",
              "url": "https://openalex.org/W2767410506",
              "openalex_id": "https://openalex.org/W2767410506",
              "title": "Digital image analysis in breast pathology\u2014from image processing techniques to artificial intelligence",
              "publication_date": "2017-11-08"
            },
            {
              "id": "E4893251842",
              "text": "Wiley Online Library\nEsther Abels\nRegulatory and Clinical Affairs\nPathAI, BostonMAUSA\nLiron Pantanowitz\nDepartment of Pathology\nUniversity of Pittsburgh Medical Center\nPittsburghPAUSA\nFamke Aeffner\nComparative Biology and Safety Sciences\nAmgen Research\nAmgen Inc\nSouth San FranciscoCAUSA\nMark D Zarella\nDepartment of Pathology and Laboratory Medicine\nDrexel University College of Medicine\nPhiladelphiaPAUSA\nJeroen Van Der Laak\nDepartment of Pathology\nRadboud University Medical Center\nNijmegenThe Netherlands\nCenter for Medical Image Science and Visualization\nLink\u00f6ping University\nLink\u00f6pingSweden\nMarilyn M Bui\nDepartment of Pathology\nMoffitt Cancer Center\nTampaFLUSA\nVenkata Np Vemuri\nData Science Department\nChan Zuckerberg Biohub\nSan FranciscoCAUSA\nAnil V Parwani\nDepartment of Pathology\nThe Ohio State University\nColumbusOHUSA\nJeff Gibbs\nHyman, Phelps & McNamara, P.C\nWashingtonDCUSA\nEmmanuel Agosto-Arroyo\nDepartment of Pathology\nMoffitt Cancer Center\nTampaFLUSA\nAndrew H Beck\nPathAI\nBostonMAUSA\nCleopatra Kozlowski cleopatk@gene.com\nDepartment of Development Sciences\nGenentech Inc\nSouth San FranciscoCAUSA\nCorrespondence to: C Kozlowski\nGenentech Inc\n1 DNA Way, South San Francisco94080CAUSA\nChan Zuckerberg Biohub\nEsther Abels\nPathAI)Andrew Beck\nCleopatra Kozlowski\nComputational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association\nLaak ; is a member of the advisory boards of Philips, The Netherlands and ContextVision, Sweden, and receives research funding from Philips, The Netherlands and Sectra, Sweden. Liron Pantanowitz is on the medical advisory board of Leica and Ibex; consults for Hamamatsu; and receives research funding from Ibex, Lunit, and Huron.\nJournal of Pathology J Pathol\n249201910.1002/path.5331Received 16 April 2019; Revised 18 July 2019; Accepted 26 July 2019REVIEW (Genentech is a member of the Roche Group, which also owns Ventana Medical Systems), and Jeffrey Gibbs (Hyman, Phelps & McNamara, P.C.). Jeroen van derartificial intelligencecomputational pathologyconvolutional neural networksdigital pathologydeep learningimage analysismachine learning\nIn this white paper, experts from the Digital Pathology Association (DPA) define terminology and concepts in the emerging field of computational pathology, with a focus on its application to histology images analyzed together with their associated patient data to extract information. This review offers a historical perspective and describes the potential clinical benefits from research and applications in this field, as well as significant obstacles to adoption. Best practices for implementing computational pathology workflows are presented. These include infrastructure considerations, acquisition of training data, quality assessments, as well as regulatory, ethical, and cyber-security concerns. Recommendations are provided for regulators, vendors, and computational pathology practitioners in order to facilitate progress in the field.Conflict of interest statement: All authors are members of the Digital Pathology Association (DPA), a non-profit organization comprising pathologists, scientists, technologists, and representatives from industry that focuses on developing education, awareness, and implementation of digital pathology applications and artificial intelligence in healthcare and life sciences. The DPA's committees and task forces are dedicated to enhancing the field.The association collaborates with the FDA on equipment approvals and addressing technology regulations. The members are encouraged to share best practices and promote the use of the technology among colleagues to demonstrate efficiencies and share knowledge and its ultimate benefits to patient care. For more information, visit https://digitalpathologyassociation.org. The following authors either are employed by, and/or own stock in, companies that offer computational pathology related or provide legal services to such companies: Venkata NP Vemuri (\nIntroduction: goals of this paper\nThe term computational pathology (CPATH) has become a buzz-word among the digital pathology community, yet it often leads to confusion due to its use in different contexts [1][2][3]. The expert authors of the Digital Pathology Association (DPA) define CPATH as the 'omics' or 'big-data' approach to pathology, where multiple sources of patient information including pathology image data and meta-data are combined to Table 1\n. Definitions of CPATH terms\nAnnotation\nIndication of the position and/or outline of structures or objects within digital images, usually produced by humans using a computer mouse or drawing tablet. Annotations may have associated labels and possible other meta-data. Annotations can be manually generated or can be established by algorithm tools\nArtificial intelligence (AI)\nA branch of computer science dealing with the simulation of intelligent behavior in computers Black box/glass box A neural network can be perceived as a black box that lacks a clear depiction of the image features used for a decision. However, methods can be employed to transform it into a glass box in an effort to understand the relationship between the input parameters and the output of the network Cloud computing\nThe practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer Computational pathology (CPATH)\nA branch of pathology that involves computational analysis of a broad array of methods to analyze patient specimens for the study of disease. In this paper, we focus on the extraction of information from digitized pathology images in combination with their associated meta-data, typically using AI methods such as deep learning\nConvolutional neural network (CNN)\nA type of deep neural network particularly designed for images. It uses a kernel or filter to convolve an image, which results in features useful for differentiating images Data augmentation\nMethod commonly used in deep learning to increase the training data using operations such as rotating, cropping, zooming, and image histogram-based modifications. This provides a number of advantages such as promoting positional and rotational invariance, robustness to staining variability, and improves the generalizability of the classifier Deep learning\nThe subset of machine learning composed of algorithms that permit software to train itself to perform tasks by exposing multilayered artificial neural networks to vast amounts of data. Data are fed into the input layers and are sequentially processed in a hierarchical manner with increasing complexity at each layer, modeled loosely after the hierarchical organization in the brain. Optimization functions are iteratively trained to shape the processing functions of the layers and the connections between them Digital pathology A blanket term that encompasses tools and systems to digitize pathology slides and associated meta-data, their storage, review, analysis, and enabling infrastructure Gold standard\nThe practical standard that is used to capture the 'ground truth'. The gold standard may not always be perfectly correct, but in general is viewed as the best approximation Ground truth (as considered within AI) A category, quantity, or label assigned to a dataset that provides guidance to an algorithm during training.\nDepending on the task, the ground truth can be a patient-or slide-level characterization or can be applied to objects or regions within the image. The ground truth is an abstract concept of the 'truth'\nImage analysis\nA method to extract typically quantifiable information from images. In this paper, we only discuss image analysis as applied to images of histology slides, but the term itself is broader, and applies to the extraction of information from any image, biomedical or not\nMachine learning (ML)\nA branch of AI in which computer software learns to perform a task by being exposed to representative data Meta-data\nIn the context of digital pathology, the term meta-data describes descriptive data associated with the individual, sample, or slide. They may include image acquisition information, patient demographic data, pathologist annotation or classification, or outcome data from treatment. Typically, meta-data are entries that allow searches in databases, for example. Highly complex, large, multiple-time-point associated data, such as longitudinal image data (such as radiology) or genomic data, are not usually called 'meta-data' Supervised machine learning Supervised learning is used to train a model to predict an outcome or to classify a dataset based on a label associated with a data point (i.e. ground truth). An example of supervised machine learning includes the design of classifiers to distinguish benign from malignant regions based on manual annotations Unsupervised machine learning Unsupervised learning seeks to identify natural divisions in a dataset without the need for a ground truth, often using methods such as cluster analysis or pattern matching. Examples of unsupervised machine learning include the identification of images with similar attributes or the clustering of tumors into subtypes Whole slide image Digital representation of an entire histopathological glass slide, digitized at microscope resolution. These whole slide scans are typically produced using slide scanners. Slide scan viewing software enables inspection of the image in a way that mimics the use of a traditional microscope; the image can be viewed at different magnifications extract patterns and analyze features. In this white paper, we will focus on a subset of this field, encompassing CPATH applications related to whole slide imaging (WSI) and analysis. CPATH is only one of a large number of fashionable terms that are confusingly used apparently interchangeably, yet mean somewhat different things. To assi",
              "url": "https://openalex.org/W2964756323",
              "openalex_id": "https://openalex.org/W2964756323",
              "title": "Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association",
              "publication_date": "2019-07-29"
            }
          ]
        },
        "S4628432253": {
          "id": "S4628432253",
          "text": "Deep learning techniques have significantly improved the integration of computational pathology into clinical workflows by enabling automated detection and classification of cancerous tissues, which enhances diagnostic efficiency and accuracy.",
          "children": [
            {
              "id": "E2158333086",
              "text": "The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65-75% of slides while retaining 100% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.",
              "url": "https://openalex.org/W2956228567",
              "openalex_id": "https://openalex.org/W2956228567",
              "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
              "publication_date": "2019-07-15"
            },
            {
              "id": "S1852133643",
              "text": "The development of deep learning techniques has revolutionized the analysis of histopathological images, enabling automated detection and classification of cancerous tissues with improved accuracy compared to traditional methods.",
              "children": [
                {
                  "id": "E4427529636",
                  "text": "Chetan L Srinidhi\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nOzan Ciga\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nAnne L Martel\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nDeep neural network models for computational histopathology: A survey\n28 Dec 2019Deep LearningConvolutional Neural NetworksComputational HistopathologyDigital PathologyHistology Image AnalysisSurveyReview\nHistopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field's progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.\nIntroduction\nThe examination and interpretation of tissue sections stained with haematoxylin and eosin (H&E) by anatomic pathologists is an essential component in the assessment of disease. In addition to providing diagnostic information, the phenotypic information contained in histology slides can be used for prognosis. Features such as nuclear atypia, degree of gland formation, presence of mitosis and inflammation can all be indicative of how aggressive a tumour is, and may also allow predictions to be made about the likelihood of recurrence after surgery. Over the last 50 years, several scoring systems have been proposed that allow pathologists to grade tumours based on their appearance, for example, the Gleason score for prostate cancer (Epstein et al., 2005) and the Nottingham score for breast cancer (Rakha et al., 2008). These systems provide important information to guide decisions about treatment and are valuable in assessing heterogeneous disease. There is, however, considerable inter-pathologist variability, and some systems that require quantitative analysis, for example the residual cancer burden index (Symmans et al., 2007), are too time-consuming to use in a routine clinical setting.\nThe first efforts to extract quantitative measures from microscopy images were in cytology. Prewitt and Mendelsohn (1966) laid out the steps required for the effective and efficient discrimination and interpretation of images which described the basic paradigm of object detection, feature extraction and finally the training of a classification function that is still in use more than 50 years later. Early work in cytology and histopathology was usually limited to the analysis of the small fields of view that could be captured using conventional microscopy, and image acquisition was a time-consuming process (Mukhopadhyay et al., 2018). The introduction of whole slide scanners in the 1990s made it much easier to produce digitized images of whole tissue slides at microscopic resolution, and this led to renewed interest in the application of image analysis and machine learning techniques to histopathology.\nIn 2012, Krizhevsky et al. (2012) showed that convolutional neural networks (CNNs) could outperform previous machine learning approaches by classifying 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into 1000 different classes. At the same time, Cirecsan et al. (2012) showed that CNNs could outperform competing methods in segmenting nerves in electron microscopy images and detecting mitotic cells in histopathology images (Cirecsan et al., 2013). Since then, methods based on CNNs have consistently outperformed other handcrafted methods in a variety of deep learning (DL) tasks in digital pathology. The ability of CNNs to learn features directly from the raw data without the need for specialist input from pathologists and the availability of annotated histopathology datasets has also fueled the explosion of interest in deep learning applied to histopathology.\nThe analysis of whole-slide digital pathology images (WSIs) poses some unique challenges. The images are very large and have to be broken down into hundreds or thousands of smaller tiles before they can be processed. Both the context at low magnification, and the detail at high magnification, may be important for a task, therefore information from multiple scales needs to be integrated. In the case of survival prediction, salient regions of the image are not known a priori and we may only have weak slide level labels. The variability within each disease subtype can be high and it usually requires a highly trained pathologist to make annotations. For cell based methods, many thousands of objects need to be detected and characterized. These challenges have made it necessary to adapt existing deep learning architectures and to design novel approaches specific to the digital pathology domain. In this work, we surveyed more than 130 papers, where deep learning has been applied to a wide variety of detection, diagnosis, prediction and prognosis tasks. We carried out this extensive review by searching Google Scholar, PubMed and arXiv for papers containing keywords such as (\"convolutional\" or \"deep learning\") and (\"digital pathology\" or \"histopathology\" or \"compu-tational pathology\"). Additionally, we also included conference proceedings from MICCAI, ISBI, MIDL, SPIE and EMBC based on title/abstract of the papers. We also iterated over the selected papers to include any additional cross-referenced works that were missing from our initial search criteria. The body of research in this area is growing rapidly and this survey covers the period up to and including December 2019. A descriptive statistics of published papers according to their category and year is illustrated in Fig. 1. The remainder of this paper is organised as follows. Section 2 presents an overview of various learning schemes in DL literature in the context of computational histopathology. Section 3 discusses in detail different categories of DL schemes commonly used in this field. We categorize these learning mechanisms into supervised (Section 3.1), weakly supervised (Section 3.2), unsupervised (Section 3.3), transfer learning (Section 3.4). Section 4 discusses survival models related to disease prognosis task. In Section 5, we discuss various open challenges including prospective applications and future trends in computational pathology, and finally, conclusions are presented in Section 6.\nOverview of learning schemas\nIn this section, we provide a formal introduction to various learning schemes in the context of DL applied to computational pathology. These learning schemes are illustrated with an example of classifying a histology WSI has cancerous or normal. Based on these formulations, various DL models have been proposed in the literature, which are traditionally based on convolutional neural network (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), auto-encoders (AEs) and various other variants. For a detailed and thorough background of DL fundamentals and its existing architectures, we refer readers to LeCun et al. (2015); Goodfellow et al. (2016), and with specific application of DL in medical image analysis to Litjens et al. (2017); Shen et al. (2017); Yi et al. (2019).\nIn supervised learning, we have a set of N training examples {(x i , y i )} N i=1 , where, each sample x i \u2208 R C h \u00d7H\u00d7W is an input image (a WSI of dimension H\u00d7W pixels, with C h channels. For example, C h = 3 channels for an RGB image) associated with a class label y i = R C , with C possible classes. For example, in binary classification, C takes the scalar form {0, 1}, and the set R for a regression task. The goal is to train a model f \u03b8 : x \u2192 y that best predicts the label for an unknown test image based on a loss function L. For instance, x's are the patches in WSIs and y's are the labels annotated by the pathologist either as cancerous or normal. During the inference time, the model predicts the label of a patch from a previously unseen test set. This scheme is detailed in Section 3.1, with an example illustrated in Fig. 3.\nIn weakly supervised learning (WSL), the goal is to train a model f \u03b8 using the readily available coarse-grained (image-level) annotations C i , to automatically infer the fine-grained (pixel/patch)-level labels c i . In histopathology, a pathologist labels a WSI as cancer, as long as a small part of this image contains cancerous region, without indicating its exact location. Such image-level annotations (often called \"weak labels\") are relatively easier to obtain in practice compared to expensive pixel-wise labels for supervised methods. An illustrative example for WSL scheme is shown in Fig. 4, and this scheme is covered in-depth in Section 3.2.\nThe unsupervised learning aims at identifying patterns on the image, without mapping an input image sample into a predefined set of output (i.e. label). This type of models includes fully unsupervised methods, where the raw data comes in the form of images without any expert-annotated labels. A common technique in unsupervised learning is to transform the input data into a lower-dimensional subspace, and then group these lowerdimension representations (i.e. the latent vector) into mutually exclusive or hierarchical",
                  "url": "https://openalex.org/W3089090082",
                  "openalex_id": "https://openalex.org/W3089090082",
                  "title": "Deep neural network models for computational histopathology: A survey",
                  "publication_date": "2020-09-25"
                },
                {
                  "id": "E1874694509",
                  "text": "Andreas S Panayides\nAmir Amini\nNenad D Filipovic\nAshish Sharma\nSotirios A Tsaftaris\nAlan Turing The\nU K Alistair Institute\nYoung\nDavid Foran\nNhan Do\nSpyretta Golemati\nTahsin Kurc\nKun Huang\nBen P Veasey\nMichalis Zervakis\nJoel H Saltz\nConstantinos S Pattichis\nDepartment of Computer Science\nElectrical and Computer Engineering Department\nUniversity of Cyprus\n1678NicosiaCyprus\nUniversity of Louisville\n40292LouisvilleKYUSA\nUniversity of Kragujevac\n2W94+H5KragujevacSerbia\nSchool of Engineering\nEmory University Atlanta\n30322GAUSA\nDepartment of Anatomy and Medical Imaging\nThe University of Edinburgh\nEH9 3FGU.K\nDepartment of Pathology and Laboratory Medicine\nUniversity of Auckland\nRobert Wood Johnson Medical School1142Auckland, RutgersNew Zealand\nMedical School, National and Kapodistrian\nU.S. Department of Veterans Affairs Boston Healthcare System\nThe State University of New Jersey\n08854, 02130Piscataway, BostonNJ, MAUSA, USA\nUniversity of Athens\n10675AthensGreece\nSchool of Medicine, Regenstrief Institute\nStony Brook University\n11794Stony BrookNYUSA\nSchool of Electrical and Computer Engineering\nS. Nikita [Fellow, IEEE], Biomedical Simulations and Imaging Lab\nIndiana University\n46202INUSA Konstantina\nElectrical and Computer Engineering Department\nNational Technical University of Athens\n157 80AthensGreece\nUniversity of Louisville\n40292LouisvilleKYUSA\nTechnical University of Crete\n73100Chania, CreteGreece\nDepartment of Computer Science\nStony Brook University\n11794Stony BrookNYUSA\nUniversity of Cyprus\n1678, 1066Nicosia, NicosiaCyprus, Cyprus\nAI in Medical Imaging Informatics: Current Challenges and Future Directions HHS Public Access\nIEEE J Biomed Health Inform\n2472020 July10.1109/JBHI.2020.2991043This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/ licenses/by/4.0/ Corresponding author: Andreas S. Panayides.: panayides@cs.ucy.ac.cy . Author manuscript IEEE J Biomed Health Inform. Author manuscript; available in PMC 2021 November 10. Published in final edited form as: Author Manuscript Author Manuscript Author Manuscript Author ManuscriptMedical ImagingImage AnalysisImage ClassificationImage ProcessingImage SegmentationImage VisualizationIntegrative AnalyticsMachine LearningDeep LearningBig Data\nThis paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.\nstorage and retrieval, to image processing, analysis and understanding, to image visualization and data navigation; to image interpretation, reporting, and communications. The field serves as the integrative catalyst for these processes and forms a bridge with imaging and other medical disciplines.\"\nThe objective of medical imaging informatics is thus, according to SIIM, to improve efficiency, accuracy, and reliability of services within the medical enterprise [3], concerning medical image usage and exchange throughout complex healthcare systems [4]. In that context, linked with the associate technological advances in big-data imaging, -omics and electronic health records (EHR) analytics, dynamic workflow optimization, context awareness, and visualization, a new era is emerging for medical imaging informatics, prescribing the way towards precision medicine [5]- [7]. This paper provides an overview of prevailing concepts, highlights challenges and opportunities, and discusses future trends.\nFollowing the key areas of medical imaging informatics in the definition given above, the rest of the paper is organized as follows: Section II covers advances in medical image acquisition highlighting primary imaging modalities used in clinical practice. Section III discusses emerging trends pertaining to the data management and sharing in the medical imaging big data era. Then, Section IV introduces emerging data processing paradigms in radiology, providing a snapshot of the timeline that has today led to increasingly adopting AI and deep learning analytics approaches. Likewise, Section V reviews the state-of-the-art in digital pathology. Section VI describes the challenges pertaining to 3D reconstruction and visualization in view of different application scenarios. Digital pathology visualization challenges are further documented in this section, while in-silico modelling advances are presented next, debating the need of introducing new integrative, multi-compartment modelling approaches. Section VII discusses the need of integrative analytics and discusses emerging radiogenomics paradigm for both radiology and digital pathology approaches. Finally, Section VIII provides the concluding remarks along with a summary of future directions.\nII. Image Formation and Acquisition\nBiomedical imaging has revolutionized the practice of medicine with unprecedented ability to diagnose disease through imaging the human body and high-resolution viewing of cells and pathological specimens. Broadly speaking, images are formed through interaction of electromagnetic waves at various wavelengths (energies) with biological tissues for modalities other than Ultrasound, which involves use of mechanical sound waves. Images formed with high-energy radiation at shorter wavelength such as X-ray and Gamma-rays at one end of the spectrum are ionizing whereas at longer wavelength -optical and still longer wavelength -MRI and Ultrasound are nonionizing. The imaging modalities covered in this section are X-ray, ultrasound, magnetic resonance (MR), X-ray computed tomography (CT), nuclear medicine, and high-resolution microscopy [8], [9] (see Table I). Fig. 1 shows some examples of images produced by these modalities.\nby an X-ray source through the body and detecting the attenuated X-rays on the other side via a detector array; the resulting image is a 2D projection with resolutions down to 100 microns and where the intensities are indicative of the degree of X-ray attenuation [9]. To improve visibility, iodinated contrast agents that attenuate X-rays are often injected into a region of interest (e.g., imaging arterial disease through fluoroscopy). Phase-contrast X-ray imaging can also improve soft-tissue image contrast by using the phase-shifts of the X-rays as they traverse through the tissue [10]. X-ray projection imaging has been pervasive in cardiovascular, mammography, musculoskeletal, and abdominal imaging applications among others [11].\nUltrasound imaging (US) employs pulses in the range of 1-10 MHz to image tissue in a noninvasive and relatively inexpensive way. The backscattering effect of the acoustic pulse interacting with internal structures is used to measure the echo to produce the image. Ultrasound imaging is fast, enabling, for example, real-time imaging of blood flow in arteries through the Doppler shift. A major benefit of ultrasonic imaging is that no ionizing radiation is used, hence less harmful to the patient. However, bone and air hinder the propagation of sound waves and can cause artifacts. Still, ultrasound remains one of the most used imaging techniques employed extensively for real-time cardiac and fetal imaging [11]. Contrast-enhanced ultrasound has allowed for greater contrast and imaging accuracy with the use of injected microbubbles to increase reflection in specific areas in some applications [12]. Ultrasound elasticity imaging has also been used for measuring the stiffness of tissue for virtual palpation [13]. Importantly, ultrasound is not limited to 2D imaging and use of 3D and 4D imaging is expanding, though with reduced temporal resolution [14].\nMR imaging [15] produces high spatial resolution volumetric images primarily of Hydrogen nuclei, using an externally applied magnetic field in conjunction with radio-frequency (RF) pulses which are non-ionizing [1]. MRI is commonly used in numerous applications including musculoskeletal, cardiovascular, and neurological imaging with superb soft-tissue contrast [16], [17]. Additionally, functional MRI has evolved into a large sub-field of study with applications in areas such as mapping the functional connectivity in the brain [18]. Similarly, diffusion-weighted MRI images the diffusion of water molecules in the body and has found much use in neuroimaging and oncology applications [19]. Moreover, Magnetic Resonance Elastography (MRE) allows virtual palpation with significant applications in liver fibrosis [20], while 4D flow methods permit exquisite visualization of flow in 3D + t [17], [21]. Techniques that accelerate the acquisition time of scans, e.g. compressed sensing, non-Cartesian acquisitions [22], and parallel imaging [23], have led to increased growth and utilization of MR imaging. In 2017, 36 million MRI scans were performed in the US alone [24].\nX-ray CT imaging [25] also offers volumetric scans like MRI. However, CT CT produces a 3D image via th",
                  "url": "https://openalex.org/W3030790048",
                  "openalex_id": "https://openalex.org/W3030790048",
                  "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
                  "publication_date": "2020-05-29"
                }
              ]
            }
          ]
        },
        "S0013014263": {
          "id": "S0013014263",
          "text": "Pathologists recognize that the integration of digital pathology workflows can improve efficiency and reduce human error, suggesting a favorable view towards adopting digital diagnostics in their practice.",
          "children": [
            {
              "id": "E4893251842",
              "text": "Wiley Online Library\nEsther Abels\nRegulatory and Clinical Affairs\nPathAI, BostonMAUSA\nLiron Pantanowitz\nDepartment of Pathology\nUniversity of Pittsburgh Medical Center\nPittsburghPAUSA\nFamke Aeffner\nComparative Biology and Safety Sciences\nAmgen Research\nAmgen Inc\nSouth San FranciscoCAUSA\nMark D Zarella\nDepartment of Pathology and Laboratory Medicine\nDrexel University College of Medicine\nPhiladelphiaPAUSA\nJeroen Van Der Laak\nDepartment of Pathology\nRadboud University Medical Center\nNijmegenThe Netherlands\nCenter for Medical Image Science and Visualization\nLink\u00f6ping University\nLink\u00f6pingSweden\nMarilyn M Bui\nDepartment of Pathology\nMoffitt Cancer Center\nTampaFLUSA\nVenkata Np Vemuri\nData Science Department\nChan Zuckerberg Biohub\nSan FranciscoCAUSA\nAnil V Parwani\nDepartment of Pathology\nThe Ohio State University\nColumbusOHUSA\nJeff Gibbs\nHyman, Phelps & McNamara, P.C\nWashingtonDCUSA\nEmmanuel Agosto-Arroyo\nDepartment of Pathology\nMoffitt Cancer Center\nTampaFLUSA\nAndrew H Beck\nPathAI\nBostonMAUSA\nCleopatra Kozlowski cleopatk@gene.com\nDepartment of Development Sciences\nGenentech Inc\nSouth San FranciscoCAUSA\nCorrespondence to: C Kozlowski\nGenentech Inc\n1 DNA Way, South San Francisco94080CAUSA\nChan Zuckerberg Biohub\nEsther Abels\nPathAI)Andrew Beck\nCleopatra Kozlowski\nComputational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association\nLaak ; is a member of the advisory boards of Philips, The Netherlands and ContextVision, Sweden, and receives research funding from Philips, The Netherlands and Sectra, Sweden. Liron Pantanowitz is on the medical advisory board of Leica and Ibex; consults for Hamamatsu; and receives research funding from Ibex, Lunit, and Huron.\nJournal of Pathology J Pathol\n249201910.1002/path.5331Received 16 April 2019; Revised 18 July 2019; Accepted 26 July 2019REVIEW (Genentech is a member of the Roche Group, which also owns Ventana Medical Systems), and Jeffrey Gibbs (Hyman, Phelps & McNamara, P.C.). Jeroen van derartificial intelligencecomputational pathologyconvolutional neural networksdigital pathologydeep learningimage analysismachine learning\nIn this white paper, experts from the Digital Pathology Association (DPA) define terminology and concepts in the emerging field of computational pathology, with a focus on its application to histology images analyzed together with their associated patient data to extract information. This review offers a historical perspective and describes the potential clinical benefits from research and applications in this field, as well as significant obstacles to adoption. Best practices for implementing computational pathology workflows are presented. These include infrastructure considerations, acquisition of training data, quality assessments, as well as regulatory, ethical, and cyber-security concerns. Recommendations are provided for regulators, vendors, and computational pathology practitioners in order to facilitate progress in the field.Conflict of interest statement: All authors are members of the Digital Pathology Association (DPA), a non-profit organization comprising pathologists, scientists, technologists, and representatives from industry that focuses on developing education, awareness, and implementation of digital pathology applications and artificial intelligence in healthcare and life sciences. The DPA's committees and task forces are dedicated to enhancing the field.The association collaborates with the FDA on equipment approvals and addressing technology regulations. The members are encouraged to share best practices and promote the use of the technology among colleagues to demonstrate efficiencies and share knowledge and its ultimate benefits to patient care. For more information, visit https://digitalpathologyassociation.org. The following authors either are employed by, and/or own stock in, companies that offer computational pathology related or provide legal services to such companies: Venkata NP Vemuri (\nIntroduction: goals of this paper\nThe term computational pathology (CPATH) has become a buzz-word among the digital pathology community, yet it often leads to confusion due to its use in different contexts [1][2][3]. The expert authors of the Digital Pathology Association (DPA) define CPATH as the 'omics' or 'big-data' approach to pathology, where multiple sources of patient information including pathology image data and meta-data are combined to Table 1\n. Definitions of CPATH terms\nAnnotation\nIndication of the position and/or outline of structures or objects within digital images, usually produced by humans using a computer mouse or drawing tablet. Annotations may have associated labels and possible other meta-data. Annotations can be manually generated or can be established by algorithm tools\nArtificial intelligence (AI)\nA branch of computer science dealing with the simulation of intelligent behavior in computers Black box/glass box A neural network can be perceived as a black box that lacks a clear depiction of the image features used for a decision. However, methods can be employed to transform it into a glass box in an effort to understand the relationship between the input parameters and the output of the network Cloud computing\nThe practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer Computational pathology (CPATH)\nA branch of pathology that involves computational analysis of a broad array of methods to analyze patient specimens for the study of disease. In this paper, we focus on the extraction of information from digitized pathology images in combination with their associated meta-data, typically using AI methods such as deep learning\nConvolutional neural network (CNN)\nA type of deep neural network particularly designed for images. It uses a kernel or filter to convolve an image, which results in features useful for differentiating images Data augmentation\nMethod commonly used in deep learning to increase the training data using operations such as rotating, cropping, zooming, and image histogram-based modifications. This provides a number of advantages such as promoting positional and rotational invariance, robustness to staining variability, and improves the generalizability of the classifier Deep learning\nThe subset of machine learning composed of algorithms that permit software to train itself to perform tasks by exposing multilayered artificial neural networks to vast amounts of data. Data are fed into the input layers and are sequentially processed in a hierarchical manner with increasing complexity at each layer, modeled loosely after the hierarchical organization in the brain. Optimization functions are iteratively trained to shape the processing functions of the layers and the connections between them Digital pathology A blanket term that encompasses tools and systems to digitize pathology slides and associated meta-data, their storage, review, analysis, and enabling infrastructure Gold standard\nThe practical standard that is used to capture the 'ground truth'. The gold standard may not always be perfectly correct, but in general is viewed as the best approximation Ground truth (as considered within AI) A category, quantity, or label assigned to a dataset that provides guidance to an algorithm during training.\nDepending on the task, the ground truth can be a patient-or slide-level characterization or can be applied to objects or regions within the image. The ground truth is an abstract concept of the 'truth'\nImage analysis\nA method to extract typically quantifiable information from images. In this paper, we only discuss image analysis as applied to images of histology slides, but the term itself is broader, and applies to the extraction of information from any image, biomedical or not\nMachine learning (ML)\nA branch of AI in which computer software learns to perform a task by being exposed to representative data Meta-data\nIn the context of digital pathology, the term meta-data describes descriptive data associated with the individual, sample, or slide. They may include image acquisition information, patient demographic data, pathologist annotation or classification, or outcome data from treatment. Typically, meta-data are entries that allow searches in databases, for example. Highly complex, large, multiple-time-point associated data, such as longitudinal image data (such as radiology) or genomic data, are not usually called 'meta-data' Supervised machine learning Supervised learning is used to train a model to predict an outcome or to classify a dataset based on a label associated with a data point (i.e. ground truth). An example of supervised machine learning includes the design of classifiers to distinguish benign from malignant regions based on manual annotations Unsupervised machine learning Unsupervised learning seeks to identify natural divisions in a dataset without the need for a ground truth, often using methods such as cluster analysis or pattern matching. Examples of unsupervised machine learning include the identification of images with similar attributes or the clustering of tumors into subtypes Whole slide image Digital representation of an entire histopathological glass slide, digitized at microscope resolution. These whole slide scans are typically produced using slide scanners. Slide scan viewing software enables inspection of the image in a way that mimics the use of a traditional microscope; the image can be viewed at different magnifications extract patterns and analyze features. In this white paper, we will focus on a subset of this field, encompassing CPATH applications related to whole slide imaging (WSI) and analysis. CPATH is only one of a large number of fashionable terms that are confusingly used apparently interchangeably, yet mean somewhat different things. To assi",
              "url": "https://openalex.org/W2964756323",
              "openalex_id": "https://openalex.org/W2964756323",
              "title": "Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association",
              "publication_date": "2019-07-29"
            },
            {
              "id": "E1088818471",
              "text": "Perspective  \n   ,    \nCorresponding Author\nRichard Colling\n  richard.colling@nds.ox.ac.uk  \n  orcid.org/0000-0001-6344-9081  \nNuffield Department of Surgical Sciences, University of Oxford, John Radcliffe Hospital, Oxford UK Correspondence to: R Colling, Nuffield Department of Surgical Sciences, University of Oxford, John Radcliffe Hospital, Oxford OX3 9DU, UK. E-mail:  richard.colling@nds.ox.ac.uk  Search for more papers by this author     Helen Pitman  ,  \nHelen Pitman\nNational Cancer Research Institute, London, UK  Search for more papers by this author      Karin Oien  ,  \nKarin Oien\nInstitute of Cancer Sciences, University of Glasgow, Glasgow, UK  Search for more papers by this author      Nasir Rajpoot  ,  \nNasir Rajpoot\nDepartment of Computer Science, University of Warwick, Coventry, UK  Search for more papers by this author      Philip Macklin  ,  \nPhilip Macklin\n  orcid.org/0000-0002-7750-3242  \nNuffield Department of Medicine, University of Oxford, Oxford, UK  Search for more papers by this author      CM-Path AI in Histopathology Working Group  ,  \nCM-Path AI in Histopathology Working Group CM-Path AI in Histopathology Working Group: Velicia Bachtiar, Richard Booth, Alyson Bryant, Joshua Bull, Jonathan Bury, Fiona Carragher, Richard Colling, Graeme Collins, Clare Craig, Maria Freitas da Silva, Daniel Gosling, Jaco Jacobs, Lena Kajland-Wil\u00e9n, Johanna Karling, Darragh Lawler, Stephen Lee, Philip Macklin, Keith Miller, Guy Mozolowski, Richard Nicholson, Daniel O'Connor, Mikkel Rahbek, Nasir Rajpoot, Alan Sumner, Dirk Vossen, Kieron White, Charlotte Wing, Corrina Wright.Search for more papers by this author      David Snead  ,  \nDavid Snead\nPathLAKE (Director) and Histopathology, University Hospitals Coventry and Warwickshire NHS Trust, University Hospital, Coventry, UK Joint senior authors contributed equally.Search for more papers by this author      Tony Sackville  ,  \nTony Sackville\nBritish In Vitro Diagnostics Association, London, UK Joint senior authors contributed equally.Search for more papers by this author      Clare Verrill  ,  \nClare Verrill\nPathLAKE (Principal Investigator), Nuffield Department of Surgical Sciences and Oxford NIHR Biomedical Research Centre, University of Oxford, John Radcliffe Hospital, Oxford, UK Joint senior authors contributed equally.Search for more papers by this author      \n  First published:  29 May 2019 \n Citations: 163   \nNo conflicts of interest were declared.\n  \nAbstract\nThe use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence-based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM-Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. \u00a9 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley &amp; Sons, Ltd.\n \n References \n    Citing Literature       \n   \n    References     Related     Information    \nRecommended              Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association       Esther Abels  ,       Liron Pantanowitz  ,       Famke Aeffner  ,       Mark D Zarella  ,       Jeroen van der Laak  ,       Marilyn M Bui  ,       Venkata NP Vemuri  ,       Anil V Parwani  ,       Jeff Gibbs  ,       Emmanuel Agosto-Arroyo  ,       Andrew H Beck  ,       Cleopatra Kozlowski  ,",
              "url": "https://openalex.org/W2945500496",
              "openalex_id": "https://openalex.org/W2945500496",
              "title": "Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",
              "publication_date": "2019-05-30"
            }
          ]
        },
        "S2342171777": {
          "id": "S2342171777",
          "text": "Quantitative image analysis is increasingly recognized as necessary for disease grading in histopathology, particularly for cancers like prostate cancer, where traditional grading systems have shown variability and inaccuracies.",
          "children": [
            {
              "id": "E1163273716",
              "text": "University of Warwick institutional repository: http://go.warwick.ac.uk/wrap This paper is made available online in accordance with publisher policies. Please scroll down to view the document itself. Please refer to the repository record for this item and our policy information available from the repository home page for further information. To see the final version of this paper please visit the publisher\u2019s website. Access to the published version may require a subscription. Author(s): Gurcan, M.N. Boucheron, L.E. Can, A. Madabhushi, A. Rajpoot, N.M. Yener, B. Article Title: Histopathological Image Analysis: A Review Year of publication: 2009 Link to published article: http://dx.doi.org/ 10.1109/RBME.2009.2034865 Publisher statement: (c) 2009 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 147 Histopathological Image Analysis: A Review Metin N. Gurcan, Senior Member, IEEE, Laura E. Boucheron, Member, IEEE, Ali Can, Anant Madabhushi, Senior Member, IEEE, Nasir M. Rajpoot, Member, IEEE, and Bulent Yener, Senior Member, IEEE Methodological Review Abstract\u2014Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe. Index Terms\u2014Computer-aided diagnosis, computer-assisted interpretation, digital pathology, histopathology, image analysis, microscopy analysis. I. INTRODUCTION AND MOTIVATION T HE widespread use of computer-assisted diagnosis (CAD) can be traced back to the emergence of digital mammography in the early 1990s [1]. Recently, CAD has become a part of routine clinical detection of breast cancer on mammograms at many screening sites and hospitals [2] in Manuscript received July 20, 2009; revised October 07, 2009. First published October 30, 2009; current version published December 09, 2009. This work was supported in part by the National Cancer Institute under Grants R01 CA134451, R01CA136535-01, ARRA-NCl-3 21CA127186\u201302S1, R21CA127186\u201301, R03CA128081-01, and R03CA143991-01, National Library of Medicine R01 LM010119, American Cancer Society, The Children\u2019s Neuroblastoma Cancer Foundation, Wallace H. Coulter Foundation, New Jersey Commission on Cancer Research, The Cancer Institute of New Jersey, and the Life Science Commercialization Award from Rutgers University, The Ohio State University Center for Clinical and Translational Science, Department of Defense under Grant W81XWH-07-1-0402. M. N. Gurcan is with the Department of Biomedical Informatics, The Ohio State University, Columbus, OH 43210 USA (e-mail: metin.gurcan@osumc. edu). L. E. Boucheron is with the Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM 88003 USA (e-mail: lboucher@nmsu.edu). A. Can is with the Global Research Center, General Electric Corporation, Niskayuna, NY 12309 USA (e-mail: can@research.ge.com). A. Madabhushi is with the Biomedical Engineering Department, Rutgers University, Piscataway, NJ 08854 USA (e-mail: anantm@rci.rutgers.edu). N. M. Rajpoot is with the Department of Computer Science, University of Warwick, Coventry, CV4 7AL, U.K. (e-mail: N.M.Rajpoot@warwick.ac.uk) B. Yener is with the Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180 USA (e-mail: yener@cs.rpi.edu) Digital Object Identifier 10.1109/RBME.2009.2034865 the United States. In fact, CAD has become one of the major research subjects in medical imaging and diagnostic radiology. Given recent advances in high-throughput tissue bank and archiving of digitized histological studies, it is now possible to use histological tissue patterns with computer-aided image analysis to facilitate disease classification. There is also a pressing need for CAD to relieve the workload on pathologists by sieving out obviously benign areas, so that the pathologist can focus on the more difficult-to-diagnose suspicious cases. For example, approximately 80% of the 1 million prostate biopsies performed in the U.S. every year are benign; this suggests that prostate pathologists are spending 80% of their time sieving through benign tissue. Researchers in both the image analysis and pathology fields have recognized the importance of quantitative analysis of pathology images. Since most current pathology diagnosis is based on the subjective (but educated) opinion of pathologists, there is clearly a need for quantitative image-based assessment of digital pathology slides. This quantitative analysis of digital pathology is important not only from a diagnostic perspective, but also in order to understand the underlying reasons for a specific diagnosis being rendered (e.g., specific chromatin texture in the cancerous nuclei which may indicate certain genetic abnormalities). In addition, quantitative characterization of pathology imagery is important not only for clinical applications (e.g., to reduce/eliminate inter- and intra-observer variations in diagnosis) but also for research applications (e.g., to understand the biological mechanisms of the disease process). A large focus of pathological image analysis has been on the automated analysis of cytology imagery. Since cytology imagery often results from the least invasive biopsies (e.g., the cervical Pap smear), they are some of the most commonly encountered imagery for both disease screening and biopsy purposes. Additionally, the characteristics of cytology imagery, namely the presence of isolated cells and cell clusters in the images and the absence of more complicated structures such as glands make it easier to analyze these specimens compared to histopathology. For example, the segmentation of individual cells or nuclei is a relatively easier process in such imagery since most of the cells are inherently separated from each other. Histopathology slides, on the other hand, provide a more comprehensive view of disease and its effect on tissues, since the preparation process preserves the underlying tissue architecture. As such, some disease characteristics, e.g., lymphocytic infiltration of cancer, may be deduced only from 1937-3333/$26.00 \u00a9 2009 IEEE Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. 148 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 a histopathology image. Additionally, the diagnosis from a histopathology image remains the \u201cgold standard\u201d in diagnosing considerable number of diseases including almost all types of cancer [3]. The additional structure in these images, while providing a wealth of information, also presents a new set of challenges from an automated image analysis perspective. It is expected that the proper leverage of this spatial information will allow for more specific characterizations of the imagery from a diagnostic perspective. The analysis of histopathology imagery has generally followed directly from techniques used to analyze cytology imagery. In particular, certain characteristics of nuclei are hallmarks of cancerous conditions. Thus, quantitative metrics for cancerous nuclei were developed to appropriately encompass the general observations of the experienced pathologist, and were tested on cytology imagery. These same metrics can also be applied to histopathological imagery, provided histological structures such as cell nuclei, glands, and lymphocytes have been adequately segmented (a complication due to the complex structure of histopathological imagery). The analysis of the spatial structure of histopathology imagery can be traced back to the works of Wiend et al. [4], Bartels [5] and Hamilton [6] but has largely been overlooked perhaps due to the lack of computational resources and the relatively high cost of digital imaging equipment for pathology. However, spatial analysis of histopathology imagery has recently become the backbone of most automated histopathology image analysis techniques. Despite the progress made in this area thus far, this is still a large area of open research due to the variety of imaging methods and disease-specific characteristics. A. Need for Quantitative Image Analysis for Disease Grading Currently, histopathological tissue analysis by a pathologist represents the only definitive method (a) for confirmation of presence or absence of disease and (b) disease grading, or the measurement of disease progression. The need for quantitative image analysis in the context of one specific disease (prostate cancer) is described below. Similar conclusions hold for quantitative analysis of other disease imagery. Higher Gleason scores are given to prostate cancers, which are more aggressive, and the grading scheme is used to predict cancer prognosis and help guide therapy. The Gleason grading system is based solely on architectural patterns; cytological features are not evaluated. The standard schematic diagram created by Gleason and his group (see Fig. 1) separated architectural features into 1 of 5 histological patterns of decreasing differentiation, pattern 1 being most differentiated and pattern 5 being least differentiated. The second unique feature of Gleason grading is that grade is not based on the highest (least differentiated..",
              "url": "https://openalex.org/W2103243046",
              "openalex_id": "https://openalex.org/W2103243046",
              "title": "Histopathological Image Analysis: A Review",
              "publication_date": "2009-01-01"
            },
            {
              "id": "E1522486102",
              "text": "..variety of conditions studied in histopathology image analysis is greater, it is still important that standard datasets be compiled as well as a standard metric of performance. This will allow for direct comparison of the variety of analysis methods being reported in the literature. An additional complication is the variety of analyses performed on the histopathology imagery. Thus, there is a need for a dataset with ground truth pertaining to all the analyses described in this paper. Going forward, clinical annotation of histopathology data will be a large bottleneck in the evaluation of histopathology related CAD algorithms. Apart from the time constraints on the pathologist to generate this data, the process should be streamlined with active communication between the image analysis scientists and the clinicians with regard to the sort of annotation required, the format and scale at which the annotation is generated, and the ease with which the data can be shared (since histopathology files typically tend to be very large). For instance, the sophistication of annotation required to train a CAD system to distinguish cancerous versus noncancerous regions on pathology images may be very different than the annotation detail required to train a classifier to distinguish grades of cancer. While for the former problem the annotation could be done on a coarser scale (lower resolution), the latter annotation may require explicit segmentation of glands and nuclei, a far more laborious and time consuming process. Due to the large size of pathological images, usually it is not possible to process the whole image on a single-core processor. Therefore, the whole image may be divided into tiles and each tile is processed independently. As a consequence, automatic load balancing in the distribution of the cases to different processors need to be handled carefully [120]. Additionally, the processing can be accelerated even further by the use of graphical processing units (GPUs), cell blades, or any other emerging high-performance architecture [121]. Histopathological image analysis system evaluation needs to be carried out in a statistical framework. Depending on whether it is a problem of detection (e.g., nuclei detection) or characterization (e.g., grading), some commonly accepted evaluation methodologies need to be followed. Some of these methods, e.g., receiver operating characteristics (ROC) and free response operating characteristics (FROC), have been successfully used for many years in radiology [122]. These techniques could be adopted or adapted accordingly. The level and detailed of quantitative evaluation will vary as a function of the specific problem being addressed. For instance, in order to evaluate a nuclear segmentation algorithm on a digitized histological section containing several tens of thousands of nuclei, it is unreasonable to 1http://marathon.csee.usf.edu/Mammography/Database.html Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. 166 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 Fig. 17. (a) Histology section of prostate gland with CaP extent stained in purple (upper right) and corresponding mapping of CaP extent via COFEMI onto (b) MRI (CaP extent shown in green). (c) Overlay of histological and MRI prostate sections following registration. expect that a human reader will be able to manually annotate all nuclei. Evaluation of the scheme may have to be performed on randomly chosen segments of the image. Similarly, if the ultimate objective of the CAD algorithm is, for instance, cancer grading, perfect segmentation of histological structures may not guarantee perfect grade-based classification. Evaluation should hence be tailored towards the ultimate objective that the CAD algorithm is being employed for. Additionally, special attention needs to be paid to clearly separate training and testing datasets and explain the evaluation methodology. A. Multimodal Data Fusion/Registration While digital pathology offers very interesting, highly dense data, one of the exciting challenges will be in the area of multimodal data fusion. One of the big open questions, especially as it pertains to personalized medicine, will be the use of multimodal data classifiers to be able to make therapy recommendations. This will require solving questions both in terms of data alignment and in terms of knowledge representation for fusion of heterogeneous sources of data, in order to answer questions that go beyond just diagnosis, such as theragnosis (therapy prediction) and prognosis. H&E staining is traditionally used for histopathology imaging. Several other modalities exist for imaging of the tissue, each offering its own advantages and limitations. Combining images from different modalities, therefore, may seem to be an attractive proposition, although it does not come without its own challenges, most importantly registration, not to mention the extra cost associated with imaging, storage, and computational time. Registration of image data across the different modalities and fusion of the information contained therein result in a powerful resource of information for diagnosis and prognosis purposes. Fusion methods have been developed for images from different microscopy imaging methods [26] and micro-scale histopathology and large-scale MR images [123]\u2013[125]. Madabhushi et al. [126] have been developing computerized detection methods for prostate cancer from high-resolution multimodal MRI . A prerequisite to training a supervised classifier to identify prostate cancer (CaP) on MRI is the ability to precisely determine spatial extent of CaP on the radiological imaging modality. CaP can be precisely determined on whole mount histopathology specimens [Fig. 17(a)] which can then be mapped onto MRI [Fig. 17(b)]. Fig. 17(c) shows the result of registering [Fig. 17(b)] the 2-D MRI slice to the histological section [Fig. 17(a)]. This requires the use of sophisticated and robust multimodal deformable registration methods to account for (a) deformations and tissue loss in the whole mount histological specimens during acquisition, and (b) ability to overcome intensity and feature differences between the two modalities (histopathology and MRI). In [123], [124] a rigid registration scheme called combined feature ensemble based mutual information (COFEMI) was presented that used alternate feature representations of the target and source images to be registered to facilitate the alignment. B. Correlating Histological Signatures With Protein and Gene Expression Multiplexing, imaging of a tissue sample with several antibodies simultaneously, allows correlation of characteristic patterns in histopathology images to expression of proteins. Teverovskiy et al. [127] recently proposed a novel scheme for automated localization and quantification of the expression of protein biomarkers using a DAPI counter-stain and three other biomarkers. They showed it to be useful for predicting recurrence of prostate cancer in patients undergoing prostatectomy. Recently, it has become clear that information regarding expression of certain proteins related to the onset of cancer is not sufficient. Analyzing multiple-stained histopathology images can help identify oncogenesis-induced changes in sub-cellular location patterns of proteins. Glory et al. [128] proposed a novel approach to compare the sub-cellular location of proteins between normal and cancerous tissues. Such a method can also be used for identification of proteins to be used as potential biomarkers. C. Exploratory Histopathology Image Analysis Exploratory analysis of histopathology images can help in finding salient diagnostic features used by humans, associating them with the computed features, and visualizing relationships between different features in high-dimensional spaces. Lessmann et al. [129] have proposed the use of self-organizing maps (SOMs) for exploratory analysis of their wavelet-based feature space. The SOM-based visualization of the feature space allowed the authors of [129] to establish a correlation between single features and histologically relevant image structures, making the selection of a subset of clinically important features possible. Iglesias-Rozas and Hopf [130] showed that SOMs can be effectively employed to correctly classify different subtypes of human Glioblastomas (GB) and also to select significant histological and clinical or genetic variables. Alternatively, dimensionality reduction methods may offer a way of looking at trends and patterns in the data in a reduced dimensional space [131]\u2013[133]. D. Computer-Aided Prognosis The use of computer-aided diagnosis for digitized histopathology could begin to be employed for disease prognostics, allowing physicians to predict which patients may be susceptible to a particular disease and also predicting disease outcome and survival. For instance, since grade is known to be correlated to outcome (high grade correlates to worse outcome), image-based predictors could be used to predict disease recurrence and survival based on analysis of biopsy specimens alone. This would have significant translational implications in Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. GURCAN et al.: HISTOPATHOLOGICAL IMAGE ANALYSIS: A REVIEW 167 that; more expensive molecular assays may not be required for predicting disease. While there may be a small minority of researchers who are experts in both computer vision and pathology, the vast majority of histopathology image analysis researchers are computer vision researchers. As such, it is important to maintain a constant collaboration with clinical and research pathologists throughout the research process. There are unique challenges to analysis of medical imagery, particularly in the performances required for eventual use of the technique in a clinical setting. It is the pathologist who can best provide the feedback on the performance of the system, as well as suggesting new avenues of research that would provide beneficial information to the pathologist community. Additionally, it is the pathologist that is best equipped to interpret the analysis results in light of underlying biological mechanisms which, in turn, may lead to new research ideas. Similarly, where appropriate it might be pertinent to include the oncologist and radiologist within the algorithmic development and evaluation loop as well. ACKNOWLEDGMENT M. N. Gurcan would like to thank O. Sertel and K. Boussaid for carefully reviewing the manuscript and for useful discussions. REFERENCES [1] A. J. Mendez, P. G. Tahoces, M. J. Lado, M. Souto, and J. J. Vidal, \u201cComputer-aided diagnosis: Automatic detection of malignant masses in digitized mammograms,\u201d Med Phys., vol. 25, pp. 957\u201364, Jun. 1998. [2] J. Tang, R. Rangay..",
              "url": "https://openalex.org/W2103243046",
              "openalex_id": "https://openalex.org/W2103243046",
              "title": "Histopathological Image Analysis: A Review",
              "publication_date": "2009-01-01"
            }
          ]
        },
        "S8880063867": {
          "id": "S8880063867",
          "text": "The integration of computer-aided diagnosis (CAD) systems in histopathology is essential for improving diagnostic accuracy and efficiency, as these systems can assist pathologists by identifying benign areas and allowing them to focus on more complex cases.",
          "children": [
            {
              "id": "E1163273716",
              "text": "University of Warwick institutional repository: http://go.warwick.ac.uk/wrap This paper is made available online in accordance with publisher policies. Please scroll down to view the document itself. Please refer to the repository record for this item and our policy information available from the repository home page for further information. To see the final version of this paper please visit the publisher\u2019s website. Access to the published version may require a subscription. Author(s): Gurcan, M.N. Boucheron, L.E. Can, A. Madabhushi, A. Rajpoot, N.M. Yener, B. Article Title: Histopathological Image Analysis: A Review Year of publication: 2009 Link to published article: http://dx.doi.org/ 10.1109/RBME.2009.2034865 Publisher statement: (c) 2009 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 147 Histopathological Image Analysis: A Review Metin N. Gurcan, Senior Member, IEEE, Laura E. Boucheron, Member, IEEE, Ali Can, Anant Madabhushi, Senior Member, IEEE, Nasir M. Rajpoot, Member, IEEE, and Bulent Yener, Senior Member, IEEE Methodological Review Abstract\u2014Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe. Index Terms\u2014Computer-aided diagnosis, computer-assisted interpretation, digital pathology, histopathology, image analysis, microscopy analysis. I. INTRODUCTION AND MOTIVATION T HE widespread use of computer-assisted diagnosis (CAD) can be traced back to the emergence of digital mammography in the early 1990s [1]. Recently, CAD has become a part of routine clinical detection of breast cancer on mammograms at many screening sites and hospitals [2] in Manuscript received July 20, 2009; revised October 07, 2009. First published October 30, 2009; current version published December 09, 2009. This work was supported in part by the National Cancer Institute under Grants R01 CA134451, R01CA136535-01, ARRA-NCl-3 21CA127186\u201302S1, R21CA127186\u201301, R03CA128081-01, and R03CA143991-01, National Library of Medicine R01 LM010119, American Cancer Society, The Children\u2019s Neuroblastoma Cancer Foundation, Wallace H. Coulter Foundation, New Jersey Commission on Cancer Research, The Cancer Institute of New Jersey, and the Life Science Commercialization Award from Rutgers University, The Ohio State University Center for Clinical and Translational Science, Department of Defense under Grant W81XWH-07-1-0402. M. N. Gurcan is with the Department of Biomedical Informatics, The Ohio State University, Columbus, OH 43210 USA (e-mail: metin.gurcan@osumc. edu). L. E. Boucheron is with the Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM 88003 USA (e-mail: lboucher@nmsu.edu). A. Can is with the Global Research Center, General Electric Corporation, Niskayuna, NY 12309 USA (e-mail: can@research.ge.com). A. Madabhushi is with the Biomedical Engineering Department, Rutgers University, Piscataway, NJ 08854 USA (e-mail: anantm@rci.rutgers.edu). N. M. Rajpoot is with the Department of Computer Science, University of Warwick, Coventry, CV4 7AL, U.K. (e-mail: N.M.Rajpoot@warwick.ac.uk) B. Yener is with the Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180 USA (e-mail: yener@cs.rpi.edu) Digital Object Identifier 10.1109/RBME.2009.2034865 the United States. In fact, CAD has become one of the major research subjects in medical imaging and diagnostic radiology. Given recent advances in high-throughput tissue bank and archiving of digitized histological studies, it is now possible to use histological tissue patterns with computer-aided image analysis to facilitate disease classification. There is also a pressing need for CAD to relieve the workload on pathologists by sieving out obviously benign areas, so that the pathologist can focus on the more difficult-to-diagnose suspicious cases. For example, approximately 80% of the 1 million prostate biopsies performed in the U.S. every year are benign; this suggests that prostate pathologists are spending 80% of their time sieving through benign tissue. Researchers in both the image analysis and pathology fields have recognized the importance of quantitative analysis of pathology images. Since most current pathology diagnosis is based on the subjective (but educated) opinion of pathologists, there is clearly a need for quantitative image-based assessment of digital pathology slides. This quantitative analysis of digital pathology is important not only from a diagnostic perspective, but also in order to understand the underlying reasons for a specific diagnosis being rendered (e.g., specific chromatin texture in the cancerous nuclei which may indicate certain genetic abnormalities). In addition, quantitative characterization of pathology imagery is important not only for clinical applications (e.g., to reduce/eliminate inter- and intra-observer variations in diagnosis) but also for research applications (e.g., to understand the biological mechanisms of the disease process). A large focus of pathological image analysis has been on the automated analysis of cytology imagery. Since cytology imagery often results from the least invasive biopsies (e.g., the cervical Pap smear), they are some of the most commonly encountered imagery for both disease screening and biopsy purposes. Additionally, the characteristics of cytology imagery, namely the presence of isolated cells and cell clusters in the images and the absence of more complicated structures such as glands make it easier to analyze these specimens compared to histopathology. For example, the segmentation of individual cells or nuclei is a relatively easier process in such imagery since most of the cells are inherently separated from each other. Histopathology slides, on the other hand, provide a more comprehensive view of disease and its effect on tissues, since the preparation process preserves the underlying tissue architecture. As such, some disease characteristics, e.g., lymphocytic infiltration of cancer, may be deduced only from 1937-3333/$26.00 \u00a9 2009 IEEE Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. 148 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 a histopathology image. Additionally, the diagnosis from a histopathology image remains the \u201cgold standard\u201d in diagnosing considerable number of diseases including almost all types of cancer [3]. The additional structure in these images, while providing a wealth of information, also presents a new set of challenges from an automated image analysis perspective. It is expected that the proper leverage of this spatial information will allow for more specific characterizations of the imagery from a diagnostic perspective. The analysis of histopathology imagery has generally followed directly from techniques used to analyze cytology imagery. In particular, certain characteristics of nuclei are hallmarks of cancerous conditions. Thus, quantitative metrics for cancerous nuclei were developed to appropriately encompass the general observations of the experienced pathologist, and were tested on cytology imagery. These same metrics can also be applied to histopathological imagery, provided histological structures such as cell nuclei, glands, and lymphocytes have been adequately segmented (a complication due to the complex structure of histopathological imagery). The analysis of the spatial structure of histopathology imagery can be traced back to the works of Wiend et al. [4], Bartels [5] and Hamilton [6] but has largely been overlooked perhaps due to the lack of computational resources and the relatively high cost of digital imaging equipment for pathology. However, spatial analysis of histopathology imagery has recently become the backbone of most automated histopathology image analysis techniques. Despite the progress made in this area thus far, this is still a large area of open research due to the variety of imaging methods and disease-specific characteristics. A. Need for Quantitative Image Analysis for Disease Grading Currently, histopathological tissue analysis by a pathologist represents the only definitive method (a) for confirmation of presence or absence of disease and (b) disease grading, or the measurement of disease progression. The need for quantitative image analysis in the context of one specific disease (prostate cancer) is described below. Similar conclusions hold for quantitative analysis of other disease imagery. Higher Gleason scores are given to prostate cancers, which are more aggressive, and the grading scheme is used to predict cancer prognosis and help guide therapy. The Gleason grading system is based solely on architectural patterns; cytological features are not evaluated. The standard schematic diagram created by Gleason and his group (see Fig. 1) separated architectural features into 1 of 5 histological patterns of decreasing differentiation, pattern 1 being most differentiated and pattern 5 being least differentiated. The second unique feature of Gleason grading is that grade is not based on the highest (least differentiated..",
              "url": "https://openalex.org/W2103243046",
              "openalex_id": "https://openalex.org/W2103243046",
              "title": "Histopathological Image Analysis: A Review",
              "publication_date": "2009-01-01"
            },
            {
              "id": "E1522486102",
              "text": "..variety of conditions studied in histopathology image analysis is greater, it is still important that standard datasets be compiled as well as a standard metric of performance. This will allow for direct comparison of the variety of analysis methods being reported in the literature. An additional complication is the variety of analyses performed on the histopathology imagery. Thus, there is a need for a dataset with ground truth pertaining to all the analyses described in this paper. Going forward, clinical annotation of histopathology data will be a large bottleneck in the evaluation of histopathology related CAD algorithms. Apart from the time constraints on the pathologist to generate this data, the process should be streamlined with active communication between the image analysis scientists and the clinicians with regard to the sort of annotation required, the format and scale at which the annotation is generated, and the ease with which the data can be shared (since histopathology files typically tend to be very large). For instance, the sophistication of annotation required to train a CAD system to distinguish cancerous versus noncancerous regions on pathology images may be very different than the annotation detail required to train a classifier to distinguish grades of cancer. While for the former problem the annotation could be done on a coarser scale (lower resolution), the latter annotation may require explicit segmentation of glands and nuclei, a far more laborious and time consuming process. Due to the large size of pathological images, usually it is not possible to process the whole image on a single-core processor. Therefore, the whole image may be divided into tiles and each tile is processed independently. As a consequence, automatic load balancing in the distribution of the cases to different processors need to be handled carefully [120]. Additionally, the processing can be accelerated even further by the use of graphical processing units (GPUs), cell blades, or any other emerging high-performance architecture [121]. Histopathological image analysis system evaluation needs to be carried out in a statistical framework. Depending on whether it is a problem of detection (e.g., nuclei detection) or characterization (e.g., grading), some commonly accepted evaluation methodologies need to be followed. Some of these methods, e.g., receiver operating characteristics (ROC) and free response operating characteristics (FROC), have been successfully used for many years in radiology [122]. These techniques could be adopted or adapted accordingly. The level and detailed of quantitative evaluation will vary as a function of the specific problem being addressed. For instance, in order to evaluate a nuclear segmentation algorithm on a digitized histological section containing several tens of thousands of nuclei, it is unreasonable to 1http://marathon.csee.usf.edu/Mammography/Database.html Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. 166 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 Fig. 17. (a) Histology section of prostate gland with CaP extent stained in purple (upper right) and corresponding mapping of CaP extent via COFEMI onto (b) MRI (CaP extent shown in green). (c) Overlay of histological and MRI prostate sections following registration. expect that a human reader will be able to manually annotate all nuclei. Evaluation of the scheme may have to be performed on randomly chosen segments of the image. Similarly, if the ultimate objective of the CAD algorithm is, for instance, cancer grading, perfect segmentation of histological structures may not guarantee perfect grade-based classification. Evaluation should hence be tailored towards the ultimate objective that the CAD algorithm is being employed for. Additionally, special attention needs to be paid to clearly separate training and testing datasets and explain the evaluation methodology. A. Multimodal Data Fusion/Registration While digital pathology offers very interesting, highly dense data, one of the exciting challenges will be in the area of multimodal data fusion. One of the big open questions, especially as it pertains to personalized medicine, will be the use of multimodal data classifiers to be able to make therapy recommendations. This will require solving questions both in terms of data alignment and in terms of knowledge representation for fusion of heterogeneous sources of data, in order to answer questions that go beyond just diagnosis, such as theragnosis (therapy prediction) and prognosis. H&E staining is traditionally used for histopathology imaging. Several other modalities exist for imaging of the tissue, each offering its own advantages and limitations. Combining images from different modalities, therefore, may seem to be an attractive proposition, although it does not come without its own challenges, most importantly registration, not to mention the extra cost associated with imaging, storage, and computational time. Registration of image data across the different modalities and fusion of the information contained therein result in a powerful resource of information for diagnosis and prognosis purposes. Fusion methods have been developed for images from different microscopy imaging methods [26] and micro-scale histopathology and large-scale MR images [123]\u2013[125]. Madabhushi et al. [126] have been developing computerized detection methods for prostate cancer from high-resolution multimodal MRI . A prerequisite to training a supervised classifier to identify prostate cancer (CaP) on MRI is the ability to precisely determine spatial extent of CaP on the radiological imaging modality. CaP can be precisely determined on whole mount histopathology specimens [Fig. 17(a)] which can then be mapped onto MRI [Fig. 17(b)]. Fig. 17(c) shows the result of registering [Fig. 17(b)] the 2-D MRI slice to the histological section [Fig. 17(a)]. This requires the use of sophisticated and robust multimodal deformable registration methods to account for (a) deformations and tissue loss in the whole mount histological specimens during acquisition, and (b) ability to overcome intensity and feature differences between the two modalities (histopathology and MRI). In [123], [124] a rigid registration scheme called combined feature ensemble based mutual information (COFEMI) was presented that used alternate feature representations of the target and source images to be registered to facilitate the alignment. B. Correlating Histological Signatures With Protein and Gene Expression Multiplexing, imaging of a tissue sample with several antibodies simultaneously, allows correlation of characteristic patterns in histopathology images to expression of proteins. Teverovskiy et al. [127] recently proposed a novel scheme for automated localization and quantification of the expression of protein biomarkers using a DAPI counter-stain and three other biomarkers. They showed it to be useful for predicting recurrence of prostate cancer in patients undergoing prostatectomy. Recently, it has become clear that information regarding expression of certain proteins related to the onset of cancer is not sufficient. Analyzing multiple-stained histopathology images can help identify oncogenesis-induced changes in sub-cellular location patterns of proteins. Glory et al. [128] proposed a novel approach to compare the sub-cellular location of proteins between normal and cancerous tissues. Such a method can also be used for identification of proteins to be used as potential biomarkers. C. Exploratory Histopathology Image Analysis Exploratory analysis of histopathology images can help in finding salient diagnostic features used by humans, associating them with the computed features, and visualizing relationships between different features in high-dimensional spaces. Lessmann et al. [129] have proposed the use of self-organizing maps (SOMs) for exploratory analysis of their wavelet-based feature space. The SOM-based visualization of the feature space allowed the authors of [129] to establish a correlation between single features and histologically relevant image structures, making the selection of a subset of clinically important features possible. Iglesias-Rozas and Hopf [130] showed that SOMs can be effectively employed to correctly classify different subtypes of human Glioblastomas (GB) and also to select significant histological and clinical or genetic variables. Alternatively, dimensionality reduction methods may offer a way of looking at trends and patterns in the data in a reduced dimensional space [131]\u2013[133]. D. Computer-Aided Prognosis The use of computer-aided diagnosis for digitized histopathology could begin to be employed for disease prognostics, allowing physicians to predict which patients may be susceptible to a particular disease and also predicting disease outcome and survival. For instance, since grade is known to be correlated to outcome (high grade correlates to worse outcome), image-based predictors could be used to predict disease recurrence and survival based on analysis of biopsy specimens alone. This would have significant translational implications in Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. GURCAN et al.: HISTOPATHOLOGICAL IMAGE ANALYSIS: A REVIEW 167 that; more expensive molecular assays may not be required for predicting disease. While there may be a small minority of researchers who are experts in both computer vision and pathology, the vast majority of histopathology image analysis researchers are computer vision researchers. As such, it is important to maintain a constant collaboration with clinical and research pathologists throughout the research process. There are unique challenges to analysis of medical imagery, particularly in the performances required for eventual use of the technique in a clinical setting. It is the pathologist who can best provide the feedback on the performance of the system, as well as suggesting new avenues of research that would provide beneficial information to the pathologist community. Additionally, it is the pathologist that is best equipped to interpret the analysis results in light of underlying biological mechanisms which, in turn, may lead to new research ideas. Similarly, where appropriate it might be pertinent to include the oncologist and radiologist within the algorithmic development and evaluation loop as well. ACKNOWLEDGMENT M. N. Gurcan would like to thank O. Sertel and K. Boussaid for carefully reviewing the manuscript and for useful discussions. REFERENCES [1] A. J. Mendez, P. G. Tahoces, M. J. Lado, M. Souto, and J. J. Vidal, \u201cComputer-aided diagnosis: Automatic detection of malignant masses in digitized mammograms,\u201d Med Phys., vol. 25, pp. 957\u201364, Jun. 1998. [2] J. Tang, R. Rangay..",
              "url": "https://openalex.org/W2103243046",
              "openalex_id": "https://openalex.org/W2103243046",
              "title": "Histopathological Image Analysis: A Review",
              "publication_date": "2009-01-01"
            }
          ]
        },
        "S2587377261": {
          "id": "S2587377261",
          "text": "Challenges in computational pathology include the need for large, annotated datasets for training machine learning models, as well as the complexity of histopathological images which require advanced segmentation and analysis techniques.",
          "children": [
            {
              "id": "E4427529636",
              "text": "Chetan L Srinidhi\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nOzan Ciga\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nAnne L Martel\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nDeep neural network models for computational histopathology: A survey\n28 Dec 2019Deep LearningConvolutional Neural NetworksComputational HistopathologyDigital PathologyHistology Image AnalysisSurveyReview\nHistopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field's progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.\nIntroduction\nThe examination and interpretation of tissue sections stained with haematoxylin and eosin (H&E) by anatomic pathologists is an essential component in the assessment of disease. In addition to providing diagnostic information, the phenotypic information contained in histology slides can be used for prognosis. Features such as nuclear atypia, degree of gland formation, presence of mitosis and inflammation can all be indicative of how aggressive a tumour is, and may also allow predictions to be made about the likelihood of recurrence after surgery. Over the last 50 years, several scoring systems have been proposed that allow pathologists to grade tumours based on their appearance, for example, the Gleason score for prostate cancer (Epstein et al., 2005) and the Nottingham score for breast cancer (Rakha et al., 2008). These systems provide important information to guide decisions about treatment and are valuable in assessing heterogeneous disease. There is, however, considerable inter-pathologist variability, and some systems that require quantitative analysis, for example the residual cancer burden index (Symmans et al., 2007), are too time-consuming to use in a routine clinical setting.\nThe first efforts to extract quantitative measures from microscopy images were in cytology. Prewitt and Mendelsohn (1966) laid out the steps required for the effective and efficient discrimination and interpretation of images which described the basic paradigm of object detection, feature extraction and finally the training of a classification function that is still in use more than 50 years later. Early work in cytology and histopathology was usually limited to the analysis of the small fields of view that could be captured using conventional microscopy, and image acquisition was a time-consuming process (Mukhopadhyay et al., 2018). The introduction of whole slide scanners in the 1990s made it much easier to produce digitized images of whole tissue slides at microscopic resolution, and this led to renewed interest in the application of image analysis and machine learning techniques to histopathology.\nIn 2012, Krizhevsky et al. (2012) showed that convolutional neural networks (CNNs) could outperform previous machine learning approaches by classifying 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into 1000 different classes. At the same time, Cirecsan et al. (2012) showed that CNNs could outperform competing methods in segmenting nerves in electron microscopy images and detecting mitotic cells in histopathology images (Cirecsan et al., 2013). Since then, methods based on CNNs have consistently outperformed other handcrafted methods in a variety of deep learning (DL) tasks in digital pathology. The ability of CNNs to learn features directly from the raw data without the need for specialist input from pathologists and the availability of annotated histopathology datasets has also fueled the explosion of interest in deep learning applied to histopathology.\nThe analysis of whole-slide digital pathology images (WSIs) poses some unique challenges. The images are very large and have to be broken down into hundreds or thousands of smaller tiles before they can be processed. Both the context at low magnification, and the detail at high magnification, may be important for a task, therefore information from multiple scales needs to be integrated. In the case of survival prediction, salient regions of the image are not known a priori and we may only have weak slide level labels. The variability within each disease subtype can be high and it usually requires a highly trained pathologist to make annotations. For cell based methods, many thousands of objects need to be detected and characterized. These challenges have made it necessary to adapt existing deep learning architectures and to design novel approaches specific to the digital pathology domain. In this work, we surveyed more than 130 papers, where deep learning has been applied to a wide variety of detection, diagnosis, prediction and prognosis tasks. We carried out this extensive review by searching Google Scholar, PubMed and arXiv for papers containing keywords such as (\"convolutional\" or \"deep learning\") and (\"digital pathology\" or \"histopathology\" or \"compu-tational pathology\"). Additionally, we also included conference proceedings from MICCAI, ISBI, MIDL, SPIE and EMBC based on title/abstract of the papers. We also iterated over the selected papers to include any additional cross-referenced works that were missing from our initial search criteria. The body of research in this area is growing rapidly and this survey covers the period up to and including December 2019. A descriptive statistics of published papers according to their category and year is illustrated in Fig. 1. The remainder of this paper is organised as follows. Section 2 presents an overview of various learning schemes in DL literature in the context of computational histopathology. Section 3 discusses in detail different categories of DL schemes commonly used in this field. We categorize these learning mechanisms into supervised (Section 3.1), weakly supervised (Section 3.2), unsupervised (Section 3.3), transfer learning (Section 3.4). Section 4 discusses survival models related to disease prognosis task. In Section 5, we discuss various open challenges including prospective applications and future trends in computational pathology, and finally, conclusions are presented in Section 6.\nOverview of learning schemas\nIn this section, we provide a formal introduction to various learning schemes in the context of DL applied to computational pathology. These learning schemes are illustrated with an example of classifying a histology WSI has cancerous or normal. Based on these formulations, various DL models have been proposed in the literature, which are traditionally based on convolutional neural network (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), auto-encoders (AEs) and various other variants. For a detailed and thorough background of DL fundamentals and its existing architectures, we refer readers to LeCun et al. (2015); Goodfellow et al. (2016), and with specific application of DL in medical image analysis to Litjens et al. (2017); Shen et al. (2017); Yi et al. (2019).\nIn supervised learning, we have a set of N training examples {(x i , y i )} N i=1 , where, each sample x i \u2208 R C h \u00d7H\u00d7W is an input image (a WSI of dimension H\u00d7W pixels, with C h channels. For example, C h = 3 channels for an RGB image) associated with a class label y i = R C , with C possible classes. For example, in binary classification, C takes the scalar form {0, 1}, and the set R for a regression task. The goal is to train a model f \u03b8 : x \u2192 y that best predicts the label for an unknown test image based on a loss function L. For instance, x's are the patches in WSIs and y's are the labels annotated by the pathologist either as cancerous or normal. During the inference time, the model predicts the label of a patch from a previously unseen test set. This scheme is detailed in Section 3.1, with an example illustrated in Fig. 3.\nIn weakly supervised learning (WSL), the goal is to train a model f \u03b8 using the readily available coarse-grained (image-level) annotations C i , to automatically infer the fine-grained (pixel/patch)-level labels c i . In histopathology, a pathologist labels a WSI as cancer, as long as a small part of this image contains cancerous region, without indicating its exact location. Such image-level annotations (often called \"weak labels\") are relatively easier to obtain in practice compared to expensive pixel-wise labels for supervised methods. An illustrative example for WSL scheme is shown in Fig. 4, and this scheme is covered in-depth in Section 3.2.\nThe unsupervised learning aims at identifying patterns on the image, without mapping an input image sample into a predefined set of output (i.e. label). This type of models includes fully unsupervised methods, where the raw data comes in the form of images without any expert-annotated labels. A common technique in unsupervised learning is to transform the input data into a lower-dimensional subspace, and then group these lowerdimension representations (i.e. the latent vector) into mutually exclusive or hierarchical",
              "url": "https://openalex.org/W3089090082",
              "openalex_id": "https://openalex.org/W3089090082",
              "title": "Deep neural network models for computational histopathology: A survey",
              "publication_date": "2020-09-25"
            },
            {
              "id": "E1874694509",
              "text": "Andreas S Panayides\nAmir Amini\nNenad D Filipovic\nAshish Sharma\nSotirios A Tsaftaris\nAlan Turing The\nU K Alistair Institute\nYoung\nDavid Foran\nNhan Do\nSpyretta Golemati\nTahsin Kurc\nKun Huang\nBen P Veasey\nMichalis Zervakis\nJoel H Saltz\nConstantinos S Pattichis\nDepartment of Computer Science\nElectrical and Computer Engineering Department\nUniversity of Cyprus\n1678NicosiaCyprus\nUniversity of Louisville\n40292LouisvilleKYUSA\nUniversity of Kragujevac\n2W94+H5KragujevacSerbia\nSchool of Engineering\nEmory University Atlanta\n30322GAUSA\nDepartment of Anatomy and Medical Imaging\nThe University of Edinburgh\nEH9 3FGU.K\nDepartment of Pathology and Laboratory Medicine\nUniversity of Auckland\nRobert Wood Johnson Medical School1142Auckland, RutgersNew Zealand\nMedical School, National and Kapodistrian\nU.S. Department of Veterans Affairs Boston Healthcare System\nThe State University of New Jersey\n08854, 02130Piscataway, BostonNJ, MAUSA, USA\nUniversity of Athens\n10675AthensGreece\nSchool of Medicine, Regenstrief Institute\nStony Brook University\n11794Stony BrookNYUSA\nSchool of Electrical and Computer Engineering\nS. Nikita [Fellow, IEEE], Biomedical Simulations and Imaging Lab\nIndiana University\n46202INUSA Konstantina\nElectrical and Computer Engineering Department\nNational Technical University of Athens\n157 80AthensGreece\nUniversity of Louisville\n40292LouisvilleKYUSA\nTechnical University of Crete\n73100Chania, CreteGreece\nDepartment of Computer Science\nStony Brook University\n11794Stony BrookNYUSA\nUniversity of Cyprus\n1678, 1066Nicosia, NicosiaCyprus, Cyprus\nAI in Medical Imaging Informatics: Current Challenges and Future Directions HHS Public Access\nIEEE J Biomed Health Inform\n2472020 July10.1109/JBHI.2020.2991043This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/ licenses/by/4.0/ Corresponding author: Andreas S. Panayides.: panayides@cs.ucy.ac.cy . Author manuscript IEEE J Biomed Health Inform. Author manuscript; available in PMC 2021 November 10. Published in final edited form as: Author Manuscript Author Manuscript Author Manuscript Author ManuscriptMedical ImagingImage AnalysisImage ClassificationImage ProcessingImage SegmentationImage VisualizationIntegrative AnalyticsMachine LearningDeep LearningBig Data\nThis paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.\nstorage and retrieval, to image processing, analysis and understanding, to image visualization and data navigation; to image interpretation, reporting, and communications. The field serves as the integrative catalyst for these processes and forms a bridge with imaging and other medical disciplines.\"\nThe objective of medical imaging informatics is thus, according to SIIM, to improve efficiency, accuracy, and reliability of services within the medical enterprise [3], concerning medical image usage and exchange throughout complex healthcare systems [4]. In that context, linked with the associate technological advances in big-data imaging, -omics and electronic health records (EHR) analytics, dynamic workflow optimization, context awareness, and visualization, a new era is emerging for medical imaging informatics, prescribing the way towards precision medicine [5]- [7]. This paper provides an overview of prevailing concepts, highlights challenges and opportunities, and discusses future trends.\nFollowing the key areas of medical imaging informatics in the definition given above, the rest of the paper is organized as follows: Section II covers advances in medical image acquisition highlighting primary imaging modalities used in clinical practice. Section III discusses emerging trends pertaining to the data management and sharing in the medical imaging big data era. Then, Section IV introduces emerging data processing paradigms in radiology, providing a snapshot of the timeline that has today led to increasingly adopting AI and deep learning analytics approaches. Likewise, Section V reviews the state-of-the-art in digital pathology. Section VI describes the challenges pertaining to 3D reconstruction and visualization in view of different application scenarios. Digital pathology visualization challenges are further documented in this section, while in-silico modelling advances are presented next, debating the need of introducing new integrative, multi-compartment modelling approaches. Section VII discusses the need of integrative analytics and discusses emerging radiogenomics paradigm for both radiology and digital pathology approaches. Finally, Section VIII provides the concluding remarks along with a summary of future directions.\nII. Image Formation and Acquisition\nBiomedical imaging has revolutionized the practice of medicine with unprecedented ability to diagnose disease through imaging the human body and high-resolution viewing of cells and pathological specimens. Broadly speaking, images are formed through interaction of electromagnetic waves at various wavelengths (energies) with biological tissues for modalities other than Ultrasound, which involves use of mechanical sound waves. Images formed with high-energy radiation at shorter wavelength such as X-ray and Gamma-rays at one end of the spectrum are ionizing whereas at longer wavelength -optical and still longer wavelength -MRI and Ultrasound are nonionizing. The imaging modalities covered in this section are X-ray, ultrasound, magnetic resonance (MR), X-ray computed tomography (CT), nuclear medicine, and high-resolution microscopy [8], [9] (see Table I). Fig. 1 shows some examples of images produced by these modalities.\nby an X-ray source through the body and detecting the attenuated X-rays on the other side via a detector array; the resulting image is a 2D projection with resolutions down to 100 microns and where the intensities are indicative of the degree of X-ray attenuation [9]. To improve visibility, iodinated contrast agents that attenuate X-rays are often injected into a region of interest (e.g., imaging arterial disease through fluoroscopy). Phase-contrast X-ray imaging can also improve soft-tissue image contrast by using the phase-shifts of the X-rays as they traverse through the tissue [10]. X-ray projection imaging has been pervasive in cardiovascular, mammography, musculoskeletal, and abdominal imaging applications among others [11].\nUltrasound imaging (US) employs pulses in the range of 1-10 MHz to image tissue in a noninvasive and relatively inexpensive way. The backscattering effect of the acoustic pulse interacting with internal structures is used to measure the echo to produce the image. Ultrasound imaging is fast, enabling, for example, real-time imaging of blood flow in arteries through the Doppler shift. A major benefit of ultrasonic imaging is that no ionizing radiation is used, hence less harmful to the patient. However, bone and air hinder the propagation of sound waves and can cause artifacts. Still, ultrasound remains one of the most used imaging techniques employed extensively for real-time cardiac and fetal imaging [11]. Contrast-enhanced ultrasound has allowed for greater contrast and imaging accuracy with the use of injected microbubbles to increase reflection in specific areas in some applications [12]. Ultrasound elasticity imaging has also been used for measuring the stiffness of tissue for virtual palpation [13]. Importantly, ultrasound is not limited to 2D imaging and use of 3D and 4D imaging is expanding, though with reduced temporal resolution [14].\nMR imaging [15] produces high spatial resolution volumetric images primarily of Hydrogen nuclei, using an externally applied magnetic field in conjunction with radio-frequency (RF) pulses which are non-ionizing [1]. MRI is commonly used in numerous applications including musculoskeletal, cardiovascular, and neurological imaging with superb soft-tissue contrast [16], [17]. Additionally, functional MRI has evolved into a large sub-field of study with applications in areas such as mapping the functional connectivity in the brain [18]. Similarly, diffusion-weighted MRI images the diffusion of water molecules in the body and has found much use in neuroimaging and oncology applications [19]. Moreover, Magnetic Resonance Elastography (MRE) allows virtual palpation with significant applications in liver fibrosis [20], while 4D flow methods permit exquisite visualization of flow in 3D + t [17], [21]. Techniques that accelerate the acquisition time of scans, e.g. compressed sensing, non-Cartesian acquisitions [22], and parallel imaging [23], have led to increased growth and utilization of MR imaging. In 2017, 36 million MRI scans were performed in the US alone [24].\nX-ray CT imaging [25] also offers volumetric scans like MRI. However, CT CT produces a 3D image via th",
              "url": "https://openalex.org/W3030790048",
              "openalex_id": "https://openalex.org/W3030790048",
              "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
              "publication_date": "2020-05-29"
            }
          ]
        },
        "S1925973567": {
          "id": "S1925973567",
          "text": "The integration of multimodal data sources, such as combining histopathological images with genomic data, is a future direction in computational pathology that could enhance diagnostic capabilities and personalized treatment strategies.",
          "children": [
            {
              "id": "E4036904696",
              "text": "HomeRadiologyVol. 285, No. 1 PreviousNext Reviews and CommentaryFree AccessOpinionIntegrated Diagnostics: The Computational Revolution Catalyzing Cross-disciplinary Practices in Radiology, Pathology, and GenomicsClaes F. Lundstr\u00f6m , Hannah L. Gilmore, Pablo R. RosClaes F. Lundstr\u00f6m , Hannah L. Gilmore, Pablo R. RosAuthor AffiliationsFrom the Center for Medical Image Science and Visualization, Link\u00f6ping University Hospital, 581 85 Link\u00f6ping, Sweden (C.F.L.); Sectra, Link\u00f6ping, Sweden (C.F.L.); and Departments of Pathology (H.L.G.) and Radiology (P.R.R.), University Hospitals Cleveland Medical Center, Case Western Reserve University, Cleveland, Ohio.Address correspondence to C.F.L. (e-mail: [email protected]).Claes F. Lundstr\u00f6m Hannah L. GilmorePablo R. RosPublished Online:Sep 19 2017https://doi.org/10.1148/radiol.2017170062MoreSectionsPDF ToolsImage ViewerAdd to favoritesCiteTrack CitationsPermissionsReprints ShareShare onFacebookTwitterLinked In AbstractIntroductionThe silo metaphor is an adequate description of current practices in the diagnostic specialties. Radiology, pathology, and genomics constitute three diagnostic disciplines with similar characteristics in terms of complex exploratory pathways. Despite their closeness and mutual interdependence, touching points are today remarkably few, and the support for close collaboration is meager. In recent years, however, voices have been raised calling for tighter collaboration creating deeply integrated workflows between radiology, pathology, and genomics. This multidisciplinary convergence is captured by the term integrated diagnostics (ID).We fully agree with the potential benefits of a development into ID practices that are characterized by profound teamwork on a daily basis, and in this article, we will describe the rationale for our standpoint. We focus primarily on the interplay of radiology and pathology and map out a viable and desirable scope for integrated practices across these two areas. We explore as well how genetics docks into the ID concepts.The concept of imaging-pathologic correlation is already close at heart for radiologists globally, who by training know the value of detailed correlation with pathologic findings. However, for many radiologists, this insight has no corollary in practical work, in the form of a rich and frequent exchange in clinical routine.This is the right time for a major move toward ID, because current technology can remove some hindrances and add new possibilities. One major change is that pathology diagnostics is transforming from an analog-slide-and-microscope approach to a digital workflow at a rapid pace thanks to whole-slide imaging scanners and pathology picture archiving and communication systems (PACS) for large-scale, clinical use. In Europe, several hospitals are already running digital workflows for routine primary review (1,2), and there are a vast amount of clinical digitization efforts ongoing across the globe. In the United States, many laboratories have begun to integrate digital pathology into practice, and there is a major effort underway to seek U.S. Food and Drug Administration approval for digital pathology as a primary diagnostic tool, as is done in Europe.With the use of digital pathology, the playing field for cross-disciplinary information technology (IT) tools greatly expands. Moreover, there is the strong trend for quantification of image contents to enable large-scale computational analysis. This is equally applicable for pathology and radiology, and in the latter case, it is known as radiomics (3). The disciplinary border actually becomes blurred and irrelevant when computational approaches such as deep learning are applied to quantified imaging features\u2014the computational methods are the same regardless of the data source. In addition, the possibility of combining radiologic and pathologic imaging in machine learning approaches is a particularly promising aspect of ID.Three pillars for an effective ID practice are described in this article: steering of diagnostic pathways, concerted diagnostic decisions, and a unified interface to referring clinicians. The pillar descriptions are followed by a wish list for the development of supporting IT tools. The emphasis is not on blue-sky visions for the future, but rather on a practical, clinical agenda for change in disease diagnosis than can be materialized with existing technologies and organizations.Steering Diagnostic PathwaysThe pathway through diagnostics can be quite complex. Whereas standardized routes are desirable when possible, they fail to well represent many patient journeys. One aspect is that the patient may be referred to diagnostics several times in the course of finding the correct conclusion, in a fragmented process based on a referring specialist receiving partial results before ordering further examinations.The first pillar is to boost the role of the diagnostician in terms of pathway steering. In our design of an ID practice, the diagnosticians would take on a larger responsibility and would directly decide on follow-up studies when the next step in the pathway is clear. Whenever no mindset or organization barrier exists between disciplines, the most appropriate examination can be selected, regardless of whether the diagnostic examination is offered by radiology, pathology, or a laboratory-based test, including genomics. In this way, costs can potentially be reduced by shortening pathways and avoiding unnecessary studies. Overutilization is a well-known concern, and the need to guide referring clinicians is manifested within radiology by, for example, the appropriateness criteria of the American College of Radiology. An extended diagnostician role could be a complementary and very effective way to let the true experts implement desired diagnostic pathways. An ID approach to guidance would also avoid separating imaging from other diagnostic tests, which is a criticism voiced regarding the radiology criteria (4). Such a new paradigm poses a challenge, however, because referring physicians may fear that unnecessarily costly examination sets are chosen beyond their control. Perhaps this issue can be addressed by investing in close, trusted relationships, or perhaps payment models would need to be reformed, but nevertheless we argue that the empowered diagnostician role is worth pursuing.Krestin et al (5) argue that this role is suitable for the radiologist, initially for the benefit of the patient and ultimately for the benefit of radiology as a specialty. To that, we would add that teamwork among diagnosticians would be essential, because of the ever-increasing complexity of the diagnostic toolbox as it expands. The need is further accentuated as genomic testing continues to make its way into everyday diagnostic practice for an ever-increasing number of diseases. It has become clear that genomics does not mean a one-size-fits-all test but instead means carefully selected diagnostic panels guided by detailed findings in radiologic and pathologic imaging that must be put into the appropriate clinical context.The ID mindset also supports the coming flood of computational methods expected to be giant leaps forward for precision medicine. There is no reason to limit, for instance, predictive artificial intelligence methods, such as the very promising deep learning technology, to data from just one discipline. In our view, to run effectively computational examinations, a separate diagnostic specialty may be required. The role of this specialist would be very similar to that of other diagnosticians in general terms; the skill set would include selecting and designing appropriate studies and interpreting the results in the context of all other patient data. Special expertise is needed, however, to assess the benefit of computational methods in relation to the available data for the patient and comparable populations, and, above all, to understand the potential errors that can be made\u2014in the same way as a radiologist knows about the potential for artifact creation in a computed tomography examination. In an ID department, such a role is natural, although it would be hampered if it were limited to just one of the silos. Jha and Topol (6) underline the importance of this future role of \"information specialist.\" But while they envision a full merger of the radiology and pathology disciplines, including genomics, our view is that the need for medical specialization will not decrease even though the toolbox of computational diagnostic methods is expanded. Artificial intelligence will be a great new cross-disciplinary examination modality, but it will be a complement to, rather than a replacement for, the existing specialized diagnostic skill sets that require deep understanding of conventional examination methods.Finally, an important pathway challenge in everyday pathology is to determine whether a tissue sample is representative. When there is uncertainty about sampling error, the choice between potentially unnecessary additional biopsies or potentially missing disease is problematic. Sorace et al (7) highlight breast cancer and lung disease as particularly pertinent areas for joint radiology-pathology decision making. One example is interstitial lung disease: Tissue sampling error can be particularly misleading because different disease patterns can coexist in sampling sites, and some cases are indeterminate at histopathologic examination alone, even for specialized lung pathologists. Focusing on ID as a specific domain would pave the way for the development of novel IT solutions to highlight potential discordance where single-discipline systems fall short, which we discuss further in the next section.Concerted Diagnostic DecisionsThe second pillar of ID is facilitated joint radiology-pathology-genomics decision making. Today this is primarily taking place in multidisciplinary case conferences. We believe that ID can greatly enrich these conferences. When pathologic images are digitized, a basic demand is to have the images from both disciplines side by side, with clear and direct anatomic links between specimens and their correlations in the radiologic images. Likewise, with the increasing focus in oncology on genomic testing for optimal targeted therapy, showing the anatomic and morphologic context of these genomic changes will be highly beneficial for personalized medicine. New and improved IT tools can have a strong positive impact for the case conferences, boosting meeting efficiency at the same time as documenting vital components of the decision process. Computational methods can also assist with input on questions arising in the course of the discussion, such as eliciting actual outcomes for the institution's previous comparable patients for different treatment alternatives. In this way, multidisciplinary conferences such as tumor boards would become sessions where new insights are jointly developed, rather than sessions where predefined static presentations are shown.Another crucial aspect is discordance handling. Today, the communication between radiologist and pathologist is limited, and a discordance between their conclusions is not systematically handled. This is potentially hazardous for the quality of care. In addition, it is a missed opportunity for continuous learning on both sides that could reduce the risk of future misjudgments and..",
              "url": "https://openalex.org/W2756350390",
              "openalex_id": "https://openalex.org/W2756350390",
              "title": "Integrated Diagnostics: The Computational Revolution Catalyzing Cross-disciplinary Practices in Radiology, Pathology, and Genomics",
              "publication_date": "2017-09-19"
            },
            {
              "id": "S5781812900",
              "text": "Future directions in computational pathology involve the integration of multimodal data sources, such as combining histopathological images with genomic data, to enhance diagnostic capabilities and personalized treatment strategies.",
              "children": [
                {
                  "id": "E4427529636",
                  "text": "Chetan L Srinidhi\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nOzan Ciga\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nAnne L Martel\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nDeep neural network models for computational histopathology: A survey\n28 Dec 2019Deep LearningConvolutional Neural NetworksComputational HistopathologyDigital PathologyHistology Image AnalysisSurveyReview\nHistopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field's progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.\nIntroduction\nThe examination and interpretation of tissue sections stained with haematoxylin and eosin (H&E) by anatomic pathologists is an essential component in the assessment of disease. In addition to providing diagnostic information, the phenotypic information contained in histology slides can be used for prognosis. Features such as nuclear atypia, degree of gland formation, presence of mitosis and inflammation can all be indicative of how aggressive a tumour is, and may also allow predictions to be made about the likelihood of recurrence after surgery. Over the last 50 years, several scoring systems have been proposed that allow pathologists to grade tumours based on their appearance, for example, the Gleason score for prostate cancer (Epstein et al., 2005) and the Nottingham score for breast cancer (Rakha et al., 2008). These systems provide important information to guide decisions about treatment and are valuable in assessing heterogeneous disease. There is, however, considerable inter-pathologist variability, and some systems that require quantitative analysis, for example the residual cancer burden index (Symmans et al., 2007), are too time-consuming to use in a routine clinical setting.\nThe first efforts to extract quantitative measures from microscopy images were in cytology. Prewitt and Mendelsohn (1966) laid out the steps required for the effective and efficient discrimination and interpretation of images which described the basic paradigm of object detection, feature extraction and finally the training of a classification function that is still in use more than 50 years later. Early work in cytology and histopathology was usually limited to the analysis of the small fields of view that could be captured using conventional microscopy, and image acquisition was a time-consuming process (Mukhopadhyay et al., 2018). The introduction of whole slide scanners in the 1990s made it much easier to produce digitized images of whole tissue slides at microscopic resolution, and this led to renewed interest in the application of image analysis and machine learning techniques to histopathology.\nIn 2012, Krizhevsky et al. (2012) showed that convolutional neural networks (CNNs) could outperform previous machine learning approaches by classifying 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into 1000 different classes. At the same time, Cirecsan et al. (2012) showed that CNNs could outperform competing methods in segmenting nerves in electron microscopy images and detecting mitotic cells in histopathology images (Cirecsan et al., 2013). Since then, methods based on CNNs have consistently outperformed other handcrafted methods in a variety of deep learning (DL) tasks in digital pathology. The ability of CNNs to learn features directly from the raw data without the need for specialist input from pathologists and the availability of annotated histopathology datasets has also fueled the explosion of interest in deep learning applied to histopathology.\nThe analysis of whole-slide digital pathology images (WSIs) poses some unique challenges. The images are very large and have to be broken down into hundreds or thousands of smaller tiles before they can be processed. Both the context at low magnification, and the detail at high magnification, may be important for a task, therefore information from multiple scales needs to be integrated. In the case of survival prediction, salient regions of the image are not known a priori and we may only have weak slide level labels. The variability within each disease subtype can be high and it usually requires a highly trained pathologist to make annotations. For cell based methods, many thousands of objects need to be detected and characterized. These challenges have made it necessary to adapt existing deep learning architectures and to design novel approaches specific to the digital pathology domain. In this work, we surveyed more than 130 papers, where deep learning has been applied to a wide variety of detection, diagnosis, prediction and prognosis tasks. We carried out this extensive review by searching Google Scholar, PubMed and arXiv for papers containing keywords such as (\"convolutional\" or \"deep learning\") and (\"digital pathology\" or \"histopathology\" or \"compu-tational pathology\"). Additionally, we also included conference proceedings from MICCAI, ISBI, MIDL, SPIE and EMBC based on title/abstract of the papers. We also iterated over the selected papers to include any additional cross-referenced works that were missing from our initial search criteria. The body of research in this area is growing rapidly and this survey covers the period up to and including December 2019. A descriptive statistics of published papers according to their category and year is illustrated in Fig. 1. The remainder of this paper is organised as follows. Section 2 presents an overview of various learning schemes in DL literature in the context of computational histopathology. Section 3 discusses in detail different categories of DL schemes commonly used in this field. We categorize these learning mechanisms into supervised (Section 3.1), weakly supervised (Section 3.2), unsupervised (Section 3.3), transfer learning (Section 3.4). Section 4 discusses survival models related to disease prognosis task. In Section 5, we discuss various open challenges including prospective applications and future trends in computational pathology, and finally, conclusions are presented in Section 6.\nOverview of learning schemas\nIn this section, we provide a formal introduction to various learning schemes in the context of DL applied to computational pathology. These learning schemes are illustrated with an example of classifying a histology WSI has cancerous or normal. Based on these formulations, various DL models have been proposed in the literature, which are traditionally based on convolutional neural network (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), auto-encoders (AEs) and various other variants. For a detailed and thorough background of DL fundamentals and its existing architectures, we refer readers to LeCun et al. (2015); Goodfellow et al. (2016), and with specific application of DL in medical image analysis to Litjens et al. (2017); Shen et al. (2017); Yi et al. (2019).\nIn supervised learning, we have a set of N training examples {(x i , y i )} N i=1 , where, each sample x i \u2208 R C h \u00d7H\u00d7W is an input image (a WSI of dimension H\u00d7W pixels, with C h channels. For example, C h = 3 channels for an RGB image) associated with a class label y i = R C , with C possible classes. For example, in binary classification, C takes the scalar form {0, 1}, and the set R for a regression task. The goal is to train a model f \u03b8 : x \u2192 y that best predicts the label for an unknown test image based on a loss function L. For instance, x's are the patches in WSIs and y's are the labels annotated by the pathologist either as cancerous or normal. During the inference time, the model predicts the label of a patch from a previously unseen test set. This scheme is detailed in Section 3.1, with an example illustrated in Fig. 3.\nIn weakly supervised learning (WSL), the goal is to train a model f \u03b8 using the readily available coarse-grained (image-level) annotations C i , to automatically infer the fine-grained (pixel/patch)-level labels c i . In histopathology, a pathologist labels a WSI as cancer, as long as a small part of this image contains cancerous region, without indicating its exact location. Such image-level annotations (often called \"weak labels\") are relatively easier to obtain in practice compared to expensive pixel-wise labels for supervised methods. An illustrative example for WSL scheme is shown in Fig. 4, and this scheme is covered in-depth in Section 3.2.\nThe unsupervised learning aims at identifying patterns on the image, without mapping an input image sample into a predefined set of output (i.e. label). This type of models includes fully unsupervised methods, where the raw data comes in the form of images without any expert-annotated labels. A common technique in unsupervised learning is to transform the input data into a lower-dimensional subspace, and then group these lowerdimension representations (i.e. the latent vector) into mutually exclusive or hierarchical",
                  "url": "https://openalex.org/W3089090082",
                  "openalex_id": "https://openalex.org/W3089090082",
                  "title": "Deep neural network models for computational histopathology: A survey",
                  "publication_date": "2020-09-25"
                },
                {
                  "id": "E1874694509",
                  "text": "Andreas S Panayides\nAmir Amini\nNenad D Filipovic\nAshish Sharma\nSotirios A Tsaftaris\nAlan Turing The\nU K Alistair Institute\nYoung\nDavid Foran\nNhan Do\nSpyretta Golemati\nTahsin Kurc\nKun Huang\nBen P Veasey\nMichalis Zervakis\nJoel H Saltz\nConstantinos S Pattichis\nDepartment of Computer Science\nElectrical and Computer Engineering Department\nUniversity of Cyprus\n1678NicosiaCyprus\nUniversity of Louisville\n40292LouisvilleKYUSA\nUniversity of Kragujevac\n2W94+H5KragujevacSerbia\nSchool of Engineering\nEmory University Atlanta\n30322GAUSA\nDepartment of Anatomy and Medical Imaging\nThe University of Edinburgh\nEH9 3FGU.K\nDepartment of Pathology and Laboratory Medicine\nUniversity of Auckland\nRobert Wood Johnson Medical School1142Auckland, RutgersNew Zealand\nMedical School, National and Kapodistrian\nU.S. Department of Veterans Affairs Boston Healthcare System\nThe State University of New Jersey\n08854, 02130Piscataway, BostonNJ, MAUSA, USA\nUniversity of Athens\n10675AthensGreece\nSchool of Medicine, Regenstrief Institute\nStony Brook University\n11794Stony BrookNYUSA\nSchool of Electrical and Computer Engineering\nS. Nikita [Fellow, IEEE], Biomedical Simulations and Imaging Lab\nIndiana University\n46202INUSA Konstantina\nElectrical and Computer Engineering Department\nNational Technical University of Athens\n157 80AthensGreece\nUniversity of Louisville\n40292LouisvilleKYUSA\nTechnical University of Crete\n73100Chania, CreteGreece\nDepartment of Computer Science\nStony Brook University\n11794Stony BrookNYUSA\nUniversity of Cyprus\n1678, 1066Nicosia, NicosiaCyprus, Cyprus\nAI in Medical Imaging Informatics: Current Challenges and Future Directions HHS Public Access\nIEEE J Biomed Health Inform\n2472020 July10.1109/JBHI.2020.2991043This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/ licenses/by/4.0/ Corresponding author: Andreas S. Panayides.: panayides@cs.ucy.ac.cy . Author manuscript IEEE J Biomed Health Inform. Author manuscript; available in PMC 2021 November 10. Published in final edited form as: Author Manuscript Author Manuscript Author Manuscript Author ManuscriptMedical ImagingImage AnalysisImage ClassificationImage ProcessingImage SegmentationImage VisualizationIntegrative AnalyticsMachine LearningDeep LearningBig Data\nThis paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.\nstorage and retrieval, to image processing, analysis and understanding, to image visualization and data navigation; to image interpretation, reporting, and communications. The field serves as the integrative catalyst for these processes and forms a bridge with imaging and other medical disciplines.\"\nThe objective of medical imaging informatics is thus, according to SIIM, to improve efficiency, accuracy, and reliability of services within the medical enterprise [3], concerning medical image usage and exchange throughout complex healthcare systems [4]. In that context, linked with the associate technological advances in big-data imaging, -omics and electronic health records (EHR) analytics, dynamic workflow optimization, context awareness, and visualization, a new era is emerging for medical imaging informatics, prescribing the way towards precision medicine [5]- [7]. This paper provides an overview of prevailing concepts, highlights challenges and opportunities, and discusses future trends.\nFollowing the key areas of medical imaging informatics in the definition given above, the rest of the paper is organized as follows: Section II covers advances in medical image acquisition highlighting primary imaging modalities used in clinical practice. Section III discusses emerging trends pertaining to the data management and sharing in the medical imaging big data era. Then, Section IV introduces emerging data processing paradigms in radiology, providing a snapshot of the timeline that has today led to increasingly adopting AI and deep learning analytics approaches. Likewise, Section V reviews the state-of-the-art in digital pathology. Section VI describes the challenges pertaining to 3D reconstruction and visualization in view of different application scenarios. Digital pathology visualization challenges are further documented in this section, while in-silico modelling advances are presented next, debating the need of introducing new integrative, multi-compartment modelling approaches. Section VII discusses the need of integrative analytics and discusses emerging radiogenomics paradigm for both radiology and digital pathology approaches. Finally, Section VIII provides the concluding remarks along with a summary of future directions.\nII. Image Formation and Acquisition\nBiomedical imaging has revolutionized the practice of medicine with unprecedented ability to diagnose disease through imaging the human body and high-resolution viewing of cells and pathological specimens. Broadly speaking, images are formed through interaction of electromagnetic waves at various wavelengths (energies) with biological tissues for modalities other than Ultrasound, which involves use of mechanical sound waves. Images formed with high-energy radiation at shorter wavelength such as X-ray and Gamma-rays at one end of the spectrum are ionizing whereas at longer wavelength -optical and still longer wavelength -MRI and Ultrasound are nonionizing. The imaging modalities covered in this section are X-ray, ultrasound, magnetic resonance (MR), X-ray computed tomography (CT), nuclear medicine, and high-resolution microscopy [8], [9] (see Table I). Fig. 1 shows some examples of images produced by these modalities.\nby an X-ray source through the body and detecting the attenuated X-rays on the other side via a detector array; the resulting image is a 2D projection with resolutions down to 100 microns and where the intensities are indicative of the degree of X-ray attenuation [9]. To improve visibility, iodinated contrast agents that attenuate X-rays are often injected into a region of interest (e.g., imaging arterial disease through fluoroscopy). Phase-contrast X-ray imaging can also improve soft-tissue image contrast by using the phase-shifts of the X-rays as they traverse through the tissue [10]. X-ray projection imaging has been pervasive in cardiovascular, mammography, musculoskeletal, and abdominal imaging applications among others [11].\nUltrasound imaging (US) employs pulses in the range of 1-10 MHz to image tissue in a noninvasive and relatively inexpensive way. The backscattering effect of the acoustic pulse interacting with internal structures is used to measure the echo to produce the image. Ultrasound imaging is fast, enabling, for example, real-time imaging of blood flow in arteries through the Doppler shift. A major benefit of ultrasonic imaging is that no ionizing radiation is used, hence less harmful to the patient. However, bone and air hinder the propagation of sound waves and can cause artifacts. Still, ultrasound remains one of the most used imaging techniques employed extensively for real-time cardiac and fetal imaging [11]. Contrast-enhanced ultrasound has allowed for greater contrast and imaging accuracy with the use of injected microbubbles to increase reflection in specific areas in some applications [12]. Ultrasound elasticity imaging has also been used for measuring the stiffness of tissue for virtual palpation [13]. Importantly, ultrasound is not limited to 2D imaging and use of 3D and 4D imaging is expanding, though with reduced temporal resolution [14].\nMR imaging [15] produces high spatial resolution volumetric images primarily of Hydrogen nuclei, using an externally applied magnetic field in conjunction with radio-frequency (RF) pulses which are non-ionizing [1]. MRI is commonly used in numerous applications including musculoskeletal, cardiovascular, and neurological imaging with superb soft-tissue contrast [16], [17]. Additionally, functional MRI has evolved into a large sub-field of study with applications in areas such as mapping the functional connectivity in the brain [18]. Similarly, diffusion-weighted MRI images the diffusion of water molecules in the body and has found much use in neuroimaging and oncology applications [19]. Moreover, Magnetic Resonance Elastography (MRE) allows virtual palpation with significant applications in liver fibrosis [20], while 4D flow methods permit exquisite visualization of flow in 3D + t [17], [21]. Techniques that accelerate the acquisition time of scans, e.g. compressed sensing, non-Cartesian acquisitions [22], and parallel imaging [23], have led to increased growth and utilization of MR imaging. In 2017, 36 million MRI scans were performed in the US alone [24].\nX-ray CT imaging [25] also offers volumetric scans like MRI. However, CT CT produces a 3D image via th",
                  "url": "https://openalex.org/W3030790048",
                  "openalex_id": "https://openalex.org/W3030790048",
                  "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
                  "publication_date": "2020-05-29"
                }
              ]
            }
          ]
        },
        "S5781812900": {
          "id": "S5781812900",
          "text": "Future directions in computational pathology involve the integration of multimodal data sources, such as combining histopathological images with genomic data, to enhance diagnostic capabilities and personalized treatment strategies.",
          "children": [
            {
              "id": "E4427529636",
              "text": "Chetan L Srinidhi\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nOzan Ciga\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nAnne L Martel\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nDeep neural network models for computational histopathology: A survey\n28 Dec 2019Deep LearningConvolutional Neural NetworksComputational HistopathologyDigital PathologyHistology Image AnalysisSurveyReview\nHistopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field's progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.\nIntroduction\nThe examination and interpretation of tissue sections stained with haematoxylin and eosin (H&E) by anatomic pathologists is an essential component in the assessment of disease. In addition to providing diagnostic information, the phenotypic information contained in histology slides can be used for prognosis. Features such as nuclear atypia, degree of gland formation, presence of mitosis and inflammation can all be indicative of how aggressive a tumour is, and may also allow predictions to be made about the likelihood of recurrence after surgery. Over the last 50 years, several scoring systems have been proposed that allow pathologists to grade tumours based on their appearance, for example, the Gleason score for prostate cancer (Epstein et al., 2005) and the Nottingham score for breast cancer (Rakha et al., 2008). These systems provide important information to guide decisions about treatment and are valuable in assessing heterogeneous disease. There is, however, considerable inter-pathologist variability, and some systems that require quantitative analysis, for example the residual cancer burden index (Symmans et al., 2007), are too time-consuming to use in a routine clinical setting.\nThe first efforts to extract quantitative measures from microscopy images were in cytology. Prewitt and Mendelsohn (1966) laid out the steps required for the effective and efficient discrimination and interpretation of images which described the basic paradigm of object detection, feature extraction and finally the training of a classification function that is still in use more than 50 years later. Early work in cytology and histopathology was usually limited to the analysis of the small fields of view that could be captured using conventional microscopy, and image acquisition was a time-consuming process (Mukhopadhyay et al., 2018). The introduction of whole slide scanners in the 1990s made it much easier to produce digitized images of whole tissue slides at microscopic resolution, and this led to renewed interest in the application of image analysis and machine learning techniques to histopathology.\nIn 2012, Krizhevsky et al. (2012) showed that convolutional neural networks (CNNs) could outperform previous machine learning approaches by classifying 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into 1000 different classes. At the same time, Cirecsan et al. (2012) showed that CNNs could outperform competing methods in segmenting nerves in electron microscopy images and detecting mitotic cells in histopathology images (Cirecsan et al., 2013). Since then, methods based on CNNs have consistently outperformed other handcrafted methods in a variety of deep learning (DL) tasks in digital pathology. The ability of CNNs to learn features directly from the raw data without the need for specialist input from pathologists and the availability of annotated histopathology datasets has also fueled the explosion of interest in deep learning applied to histopathology.\nThe analysis of whole-slide digital pathology images (WSIs) poses some unique challenges. The images are very large and have to be broken down into hundreds or thousands of smaller tiles before they can be processed. Both the context at low magnification, and the detail at high magnification, may be important for a task, therefore information from multiple scales needs to be integrated. In the case of survival prediction, salient regions of the image are not known a priori and we may only have weak slide level labels. The variability within each disease subtype can be high and it usually requires a highly trained pathologist to make annotations. For cell based methods, many thousands of objects need to be detected and characterized. These challenges have made it necessary to adapt existing deep learning architectures and to design novel approaches specific to the digital pathology domain. In this work, we surveyed more than 130 papers, where deep learning has been applied to a wide variety of detection, diagnosis, prediction and prognosis tasks. We carried out this extensive review by searching Google Scholar, PubMed and arXiv for papers containing keywords such as (\"convolutional\" or \"deep learning\") and (\"digital pathology\" or \"histopathology\" or \"compu-tational pathology\"). Additionally, we also included conference proceedings from MICCAI, ISBI, MIDL, SPIE and EMBC based on title/abstract of the papers. We also iterated over the selected papers to include any additional cross-referenced works that were missing from our initial search criteria. The body of research in this area is growing rapidly and this survey covers the period up to and including December 2019. A descriptive statistics of published papers according to their category and year is illustrated in Fig. 1. The remainder of this paper is organised as follows. Section 2 presents an overview of various learning schemes in DL literature in the context of computational histopathology. Section 3 discusses in detail different categories of DL schemes commonly used in this field. We categorize these learning mechanisms into supervised (Section 3.1), weakly supervised (Section 3.2), unsupervised (Section 3.3), transfer learning (Section 3.4). Section 4 discusses survival models related to disease prognosis task. In Section 5, we discuss various open challenges including prospective applications and future trends in computational pathology, and finally, conclusions are presented in Section 6.\nOverview of learning schemas\nIn this section, we provide a formal introduction to various learning schemes in the context of DL applied to computational pathology. These learning schemes are illustrated with an example of classifying a histology WSI has cancerous or normal. Based on these formulations, various DL models have been proposed in the literature, which are traditionally based on convolutional neural network (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), auto-encoders (AEs) and various other variants. For a detailed and thorough background of DL fundamentals and its existing architectures, we refer readers to LeCun et al. (2015); Goodfellow et al. (2016), and with specific application of DL in medical image analysis to Litjens et al. (2017); Shen et al. (2017); Yi et al. (2019).\nIn supervised learning, we have a set of N training examples {(x i , y i )} N i=1 , where, each sample x i \u2208 R C h \u00d7H\u00d7W is an input image (a WSI of dimension H\u00d7W pixels, with C h channels. For example, C h = 3 channels for an RGB image) associated with a class label y i = R C , with C possible classes. For example, in binary classification, C takes the scalar form {0, 1}, and the set R for a regression task. The goal is to train a model f \u03b8 : x \u2192 y that best predicts the label for an unknown test image based on a loss function L. For instance, x's are the patches in WSIs and y's are the labels annotated by the pathologist either as cancerous or normal. During the inference time, the model predicts the label of a patch from a previously unseen test set. This scheme is detailed in Section 3.1, with an example illustrated in Fig. 3.\nIn weakly supervised learning (WSL), the goal is to train a model f \u03b8 using the readily available coarse-grained (image-level) annotations C i , to automatically infer the fine-grained (pixel/patch)-level labels c i . In histopathology, a pathologist labels a WSI as cancer, as long as a small part of this image contains cancerous region, without indicating its exact location. Such image-level annotations (often called \"weak labels\") are relatively easier to obtain in practice compared to expensive pixel-wise labels for supervised methods. An illustrative example for WSL scheme is shown in Fig. 4, and this scheme is covered in-depth in Section 3.2.\nThe unsupervised learning aims at identifying patterns on the image, without mapping an input image sample into a predefined set of output (i.e. label). This type of models includes fully unsupervised methods, where the raw data comes in the form of images without any expert-annotated labels. A common technique in unsupervised learning is to transform the input data into a lower-dimensional subspace, and then group these lowerdimension representations (i.e. the latent vector) into mutually exclusive or hierarchical",
              "url": "https://openalex.org/W3089090082",
              "openalex_id": "https://openalex.org/W3089090082",
              "title": "Deep neural network models for computational histopathology: A survey",
              "publication_date": "2020-09-25"
            },
            {
              "id": "E1874694509",
              "text": "Andreas S Panayides\nAmir Amini\nNenad D Filipovic\nAshish Sharma\nSotirios A Tsaftaris\nAlan Turing The\nU K Alistair Institute\nYoung\nDavid Foran\nNhan Do\nSpyretta Golemati\nTahsin Kurc\nKun Huang\nBen P Veasey\nMichalis Zervakis\nJoel H Saltz\nConstantinos S Pattichis\nDepartment of Computer Science\nElectrical and Computer Engineering Department\nUniversity of Cyprus\n1678NicosiaCyprus\nUniversity of Louisville\n40292LouisvilleKYUSA\nUniversity of Kragujevac\n2W94+H5KragujevacSerbia\nSchool of Engineering\nEmory University Atlanta\n30322GAUSA\nDepartment of Anatomy and Medical Imaging\nThe University of Edinburgh\nEH9 3FGU.K\nDepartment of Pathology and Laboratory Medicine\nUniversity of Auckland\nRobert Wood Johnson Medical School1142Auckland, RutgersNew Zealand\nMedical School, National and Kapodistrian\nU.S. Department of Veterans Affairs Boston Healthcare System\nThe State University of New Jersey\n08854, 02130Piscataway, BostonNJ, MAUSA, USA\nUniversity of Athens\n10675AthensGreece\nSchool of Medicine, Regenstrief Institute\nStony Brook University\n11794Stony BrookNYUSA\nSchool of Electrical and Computer Engineering\nS. Nikita [Fellow, IEEE], Biomedical Simulations and Imaging Lab\nIndiana University\n46202INUSA Konstantina\nElectrical and Computer Engineering Department\nNational Technical University of Athens\n157 80AthensGreece\nUniversity of Louisville\n40292LouisvilleKYUSA\nTechnical University of Crete\n73100Chania, CreteGreece\nDepartment of Computer Science\nStony Brook University\n11794Stony BrookNYUSA\nUniversity of Cyprus\n1678, 1066Nicosia, NicosiaCyprus, Cyprus\nAI in Medical Imaging Informatics: Current Challenges and Future Directions HHS Public Access\nIEEE J Biomed Health Inform\n2472020 July10.1109/JBHI.2020.2991043This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/ licenses/by/4.0/ Corresponding author: Andreas S. Panayides.: panayides@cs.ucy.ac.cy . Author manuscript IEEE J Biomed Health Inform. Author manuscript; available in PMC 2021 November 10. Published in final edited form as: Author Manuscript Author Manuscript Author Manuscript Author ManuscriptMedical ImagingImage AnalysisImage ClassificationImage ProcessingImage SegmentationImage VisualizationIntegrative AnalyticsMachine LearningDeep LearningBig Data\nThis paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.\nstorage and retrieval, to image processing, analysis and understanding, to image visualization and data navigation; to image interpretation, reporting, and communications. The field serves as the integrative catalyst for these processes and forms a bridge with imaging and other medical disciplines.\"\nThe objective of medical imaging informatics is thus, according to SIIM, to improve efficiency, accuracy, and reliability of services within the medical enterprise [3], concerning medical image usage and exchange throughout complex healthcare systems [4]. In that context, linked with the associate technological advances in big-data imaging, -omics and electronic health records (EHR) analytics, dynamic workflow optimization, context awareness, and visualization, a new era is emerging for medical imaging informatics, prescribing the way towards precision medicine [5]- [7]. This paper provides an overview of prevailing concepts, highlights challenges and opportunities, and discusses future trends.\nFollowing the key areas of medical imaging informatics in the definition given above, the rest of the paper is organized as follows: Section II covers advances in medical image acquisition highlighting primary imaging modalities used in clinical practice. Section III discusses emerging trends pertaining to the data management and sharing in the medical imaging big data era. Then, Section IV introduces emerging data processing paradigms in radiology, providing a snapshot of the timeline that has today led to increasingly adopting AI and deep learning analytics approaches. Likewise, Section V reviews the state-of-the-art in digital pathology. Section VI describes the challenges pertaining to 3D reconstruction and visualization in view of different application scenarios. Digital pathology visualization challenges are further documented in this section, while in-silico modelling advances are presented next, debating the need of introducing new integrative, multi-compartment modelling approaches. Section VII discusses the need of integrative analytics and discusses emerging radiogenomics paradigm for both radiology and digital pathology approaches. Finally, Section VIII provides the concluding remarks along with a summary of future directions.\nII. Image Formation and Acquisition\nBiomedical imaging has revolutionized the practice of medicine with unprecedented ability to diagnose disease through imaging the human body and high-resolution viewing of cells and pathological specimens. Broadly speaking, images are formed through interaction of electromagnetic waves at various wavelengths (energies) with biological tissues for modalities other than Ultrasound, which involves use of mechanical sound waves. Images formed with high-energy radiation at shorter wavelength such as X-ray and Gamma-rays at one end of the spectrum are ionizing whereas at longer wavelength -optical and still longer wavelength -MRI and Ultrasound are nonionizing. The imaging modalities covered in this section are X-ray, ultrasound, magnetic resonance (MR), X-ray computed tomography (CT), nuclear medicine, and high-resolution microscopy [8], [9] (see Table I). Fig. 1 shows some examples of images produced by these modalities.\nby an X-ray source through the body and detecting the attenuated X-rays on the other side via a detector array; the resulting image is a 2D projection with resolutions down to 100 microns and where the intensities are indicative of the degree of X-ray attenuation [9]. To improve visibility, iodinated contrast agents that attenuate X-rays are often injected into a region of interest (e.g., imaging arterial disease through fluoroscopy). Phase-contrast X-ray imaging can also improve soft-tissue image contrast by using the phase-shifts of the X-rays as they traverse through the tissue [10]. X-ray projection imaging has been pervasive in cardiovascular, mammography, musculoskeletal, and abdominal imaging applications among others [11].\nUltrasound imaging (US) employs pulses in the range of 1-10 MHz to image tissue in a noninvasive and relatively inexpensive way. The backscattering effect of the acoustic pulse interacting with internal structures is used to measure the echo to produce the image. Ultrasound imaging is fast, enabling, for example, real-time imaging of blood flow in arteries through the Doppler shift. A major benefit of ultrasonic imaging is that no ionizing radiation is used, hence less harmful to the patient. However, bone and air hinder the propagation of sound waves and can cause artifacts. Still, ultrasound remains one of the most used imaging techniques employed extensively for real-time cardiac and fetal imaging [11]. Contrast-enhanced ultrasound has allowed for greater contrast and imaging accuracy with the use of injected microbubbles to increase reflection in specific areas in some applications [12]. Ultrasound elasticity imaging has also been used for measuring the stiffness of tissue for virtual palpation [13]. Importantly, ultrasound is not limited to 2D imaging and use of 3D and 4D imaging is expanding, though with reduced temporal resolution [14].\nMR imaging [15] produces high spatial resolution volumetric images primarily of Hydrogen nuclei, using an externally applied magnetic field in conjunction with radio-frequency (RF) pulses which are non-ionizing [1]. MRI is commonly used in numerous applications including musculoskeletal, cardiovascular, and neurological imaging with superb soft-tissue contrast [16], [17]. Additionally, functional MRI has evolved into a large sub-field of study with applications in areas such as mapping the functional connectivity in the brain [18]. Similarly, diffusion-weighted MRI images the diffusion of water molecules in the body and has found much use in neuroimaging and oncology applications [19]. Moreover, Magnetic Resonance Elastography (MRE) allows virtual palpation with significant applications in liver fibrosis [20], while 4D flow methods permit exquisite visualization of flow in 3D + t [17], [21]. Techniques that accelerate the acquisition time of scans, e.g. compressed sensing, non-Cartesian acquisitions [22], and parallel imaging [23], have led to increased growth and utilization of MR imaging. In 2017, 36 million MRI scans were performed in the US alone [24].\nX-ray CT imaging [25] also offers volumetric scans like MRI. However, CT CT produces a 3D image via th",
              "url": "https://openalex.org/W3030790048",
              "openalex_id": "https://openalex.org/W3030790048",
              "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
              "publication_date": "2020-05-29"
            }
          ]
        },
        "S1852133643": {
          "id": "S1852133643",
          "text": "The development of deep learning techniques has revolutionized the analysis of histopathological images, enabling automated detection and classification of cancerous tissues with improved accuracy compared to traditional methods.",
          "children": [
            {
              "id": "E4427529636",
              "text": "Chetan L Srinidhi\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nOzan Ciga\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nAnne L Martel\nPhysical Sciences\nSunnybrook Research Institute\nTorontoCanada\nDepartment of Medical Biophysics\nUniversity of Toronto\nCanada\nDeep neural network models for computational histopathology: A survey\n28 Dec 2019Deep LearningConvolutional Neural NetworksComputational HistopathologyDigital PathologyHistology Image AnalysisSurveyReview\nHistopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field's progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.\nIntroduction\nThe examination and interpretation of tissue sections stained with haematoxylin and eosin (H&E) by anatomic pathologists is an essential component in the assessment of disease. In addition to providing diagnostic information, the phenotypic information contained in histology slides can be used for prognosis. Features such as nuclear atypia, degree of gland formation, presence of mitosis and inflammation can all be indicative of how aggressive a tumour is, and may also allow predictions to be made about the likelihood of recurrence after surgery. Over the last 50 years, several scoring systems have been proposed that allow pathologists to grade tumours based on their appearance, for example, the Gleason score for prostate cancer (Epstein et al., 2005) and the Nottingham score for breast cancer (Rakha et al., 2008). These systems provide important information to guide decisions about treatment and are valuable in assessing heterogeneous disease. There is, however, considerable inter-pathologist variability, and some systems that require quantitative analysis, for example the residual cancer burden index (Symmans et al., 2007), are too time-consuming to use in a routine clinical setting.\nThe first efforts to extract quantitative measures from microscopy images were in cytology. Prewitt and Mendelsohn (1966) laid out the steps required for the effective and efficient discrimination and interpretation of images which described the basic paradigm of object detection, feature extraction and finally the training of a classification function that is still in use more than 50 years later. Early work in cytology and histopathology was usually limited to the analysis of the small fields of view that could be captured using conventional microscopy, and image acquisition was a time-consuming process (Mukhopadhyay et al., 2018). The introduction of whole slide scanners in the 1990s made it much easier to produce digitized images of whole tissue slides at microscopic resolution, and this led to renewed interest in the application of image analysis and machine learning techniques to histopathology.\nIn 2012, Krizhevsky et al. (2012) showed that convolutional neural networks (CNNs) could outperform previous machine learning approaches by classifying 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into 1000 different classes. At the same time, Cirecsan et al. (2012) showed that CNNs could outperform competing methods in segmenting nerves in electron microscopy images and detecting mitotic cells in histopathology images (Cirecsan et al., 2013). Since then, methods based on CNNs have consistently outperformed other handcrafted methods in a variety of deep learning (DL) tasks in digital pathology. The ability of CNNs to learn features directly from the raw data without the need for specialist input from pathologists and the availability of annotated histopathology datasets has also fueled the explosion of interest in deep learning applied to histopathology.\nThe analysis of whole-slide digital pathology images (WSIs) poses some unique challenges. The images are very large and have to be broken down into hundreds or thousands of smaller tiles before they can be processed. Both the context at low magnification, and the detail at high magnification, may be important for a task, therefore information from multiple scales needs to be integrated. In the case of survival prediction, salient regions of the image are not known a priori and we may only have weak slide level labels. The variability within each disease subtype can be high and it usually requires a highly trained pathologist to make annotations. For cell based methods, many thousands of objects need to be detected and characterized. These challenges have made it necessary to adapt existing deep learning architectures and to design novel approaches specific to the digital pathology domain. In this work, we surveyed more than 130 papers, where deep learning has been applied to a wide variety of detection, diagnosis, prediction and prognosis tasks. We carried out this extensive review by searching Google Scholar, PubMed and arXiv for papers containing keywords such as (\"convolutional\" or \"deep learning\") and (\"digital pathology\" or \"histopathology\" or \"compu-tational pathology\"). Additionally, we also included conference proceedings from MICCAI, ISBI, MIDL, SPIE and EMBC based on title/abstract of the papers. We also iterated over the selected papers to include any additional cross-referenced works that were missing from our initial search criteria. The body of research in this area is growing rapidly and this survey covers the period up to and including December 2019. A descriptive statistics of published papers according to their category and year is illustrated in Fig. 1. The remainder of this paper is organised as follows. Section 2 presents an overview of various learning schemes in DL literature in the context of computational histopathology. Section 3 discusses in detail different categories of DL schemes commonly used in this field. We categorize these learning mechanisms into supervised (Section 3.1), weakly supervised (Section 3.2), unsupervised (Section 3.3), transfer learning (Section 3.4). Section 4 discusses survival models related to disease prognosis task. In Section 5, we discuss various open challenges including prospective applications and future trends in computational pathology, and finally, conclusions are presented in Section 6.\nOverview of learning schemas\nIn this section, we provide a formal introduction to various learning schemes in the context of DL applied to computational pathology. These learning schemes are illustrated with an example of classifying a histology WSI has cancerous or normal. Based on these formulations, various DL models have been proposed in the literature, which are traditionally based on convolutional neural network (CNNs), recurrent neural networks (RNNs), generative adversarial networks (GANs), auto-encoders (AEs) and various other variants. For a detailed and thorough background of DL fundamentals and its existing architectures, we refer readers to LeCun et al. (2015); Goodfellow et al. (2016), and with specific application of DL in medical image analysis to Litjens et al. (2017); Shen et al. (2017); Yi et al. (2019).\nIn supervised learning, we have a set of N training examples {(x i , y i )} N i=1 , where, each sample x i \u2208 R C h \u00d7H\u00d7W is an input image (a WSI of dimension H\u00d7W pixels, with C h channels. For example, C h = 3 channels for an RGB image) associated with a class label y i = R C , with C possible classes. For example, in binary classification, C takes the scalar form {0, 1}, and the set R for a regression task. The goal is to train a model f \u03b8 : x \u2192 y that best predicts the label for an unknown test image based on a loss function L. For instance, x's are the patches in WSIs and y's are the labels annotated by the pathologist either as cancerous or normal. During the inference time, the model predicts the label of a patch from a previously unseen test set. This scheme is detailed in Section 3.1, with an example illustrated in Fig. 3.\nIn weakly supervised learning (WSL), the goal is to train a model f \u03b8 using the readily available coarse-grained (image-level) annotations C i , to automatically infer the fine-grained (pixel/patch)-level labels c i . In histopathology, a pathologist labels a WSI as cancer, as long as a small part of this image contains cancerous region, without indicating its exact location. Such image-level annotations (often called \"weak labels\") are relatively easier to obtain in practice compared to expensive pixel-wise labels for supervised methods. An illustrative example for WSL scheme is shown in Fig. 4, and this scheme is covered in-depth in Section 3.2.\nThe unsupervised learning aims at identifying patterns on the image, without mapping an input image sample into a predefined set of output (i.e. label). This type of models includes fully unsupervised methods, where the raw data comes in the form of images without any expert-annotated labels. A common technique in unsupervised learning is to transform the input data into a lower-dimensional subspace, and then group these lowerdimension representations (i.e. the latent vector) into mutually exclusive or hierarchical",
              "url": "https://openalex.org/W3089090082",
              "openalex_id": "https://openalex.org/W3089090082",
              "title": "Deep neural network models for computational histopathology: A survey",
              "publication_date": "2020-09-25"
            },
            {
              "id": "E1874694509",
              "text": "Andreas S Panayides\nAmir Amini\nNenad D Filipovic\nAshish Sharma\nSotirios A Tsaftaris\nAlan Turing The\nU K Alistair Institute\nYoung\nDavid Foran\nNhan Do\nSpyretta Golemati\nTahsin Kurc\nKun Huang\nBen P Veasey\nMichalis Zervakis\nJoel H Saltz\nConstantinos S Pattichis\nDepartment of Computer Science\nElectrical and Computer Engineering Department\nUniversity of Cyprus\n1678NicosiaCyprus\nUniversity of Louisville\n40292LouisvilleKYUSA\nUniversity of Kragujevac\n2W94+H5KragujevacSerbia\nSchool of Engineering\nEmory University Atlanta\n30322GAUSA\nDepartment of Anatomy and Medical Imaging\nThe University of Edinburgh\nEH9 3FGU.K\nDepartment of Pathology and Laboratory Medicine\nUniversity of Auckland\nRobert Wood Johnson Medical School1142Auckland, RutgersNew Zealand\nMedical School, National and Kapodistrian\nU.S. Department of Veterans Affairs Boston Healthcare System\nThe State University of New Jersey\n08854, 02130Piscataway, BostonNJ, MAUSA, USA\nUniversity of Athens\n10675AthensGreece\nSchool of Medicine, Regenstrief Institute\nStony Brook University\n11794Stony BrookNYUSA\nSchool of Electrical and Computer Engineering\nS. Nikita [Fellow, IEEE], Biomedical Simulations and Imaging Lab\nIndiana University\n46202INUSA Konstantina\nElectrical and Computer Engineering Department\nNational Technical University of Athens\n157 80AthensGreece\nUniversity of Louisville\n40292LouisvilleKYUSA\nTechnical University of Crete\n73100Chania, CreteGreece\nDepartment of Computer Science\nStony Brook University\n11794Stony BrookNYUSA\nUniversity of Cyprus\n1678, 1066Nicosia, NicosiaCyprus, Cyprus\nAI in Medical Imaging Informatics: Current Challenges and Future Directions HHS Public Access\nIEEE J Biomed Health Inform\n2472020 July10.1109/JBHI.2020.2991043This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/ licenses/by/4.0/ Corresponding author: Andreas S. Panayides.: panayides@cs.ucy.ac.cy . Author manuscript IEEE J Biomed Health Inform. Author manuscript; available in PMC 2021 November 10. Published in final edited form as: Author Manuscript Author Manuscript Author Manuscript Author ManuscriptMedical ImagingImage AnalysisImage ClassificationImage ProcessingImage SegmentationImage VisualizationIntegrative AnalyticsMachine LearningDeep LearningBig Data\nThis paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.\nstorage and retrieval, to image processing, analysis and understanding, to image visualization and data navigation; to image interpretation, reporting, and communications. The field serves as the integrative catalyst for these processes and forms a bridge with imaging and other medical disciplines.\"\nThe objective of medical imaging informatics is thus, according to SIIM, to improve efficiency, accuracy, and reliability of services within the medical enterprise [3], concerning medical image usage and exchange throughout complex healthcare systems [4]. In that context, linked with the associate technological advances in big-data imaging, -omics and electronic health records (EHR) analytics, dynamic workflow optimization, context awareness, and visualization, a new era is emerging for medical imaging informatics, prescribing the way towards precision medicine [5]- [7]. This paper provides an overview of prevailing concepts, highlights challenges and opportunities, and discusses future trends.\nFollowing the key areas of medical imaging informatics in the definition given above, the rest of the paper is organized as follows: Section II covers advances in medical image acquisition highlighting primary imaging modalities used in clinical practice. Section III discusses emerging trends pertaining to the data management and sharing in the medical imaging big data era. Then, Section IV introduces emerging data processing paradigms in radiology, providing a snapshot of the timeline that has today led to increasingly adopting AI and deep learning analytics approaches. Likewise, Section V reviews the state-of-the-art in digital pathology. Section VI describes the challenges pertaining to 3D reconstruction and visualization in view of different application scenarios. Digital pathology visualization challenges are further documented in this section, while in-silico modelling advances are presented next, debating the need of introducing new integrative, multi-compartment modelling approaches. Section VII discusses the need of integrative analytics and discusses emerging radiogenomics paradigm for both radiology and digital pathology approaches. Finally, Section VIII provides the concluding remarks along with a summary of future directions.\nII. Image Formation and Acquisition\nBiomedical imaging has revolutionized the practice of medicine with unprecedented ability to diagnose disease through imaging the human body and high-resolution viewing of cells and pathological specimens. Broadly speaking, images are formed through interaction of electromagnetic waves at various wavelengths (energies) with biological tissues for modalities other than Ultrasound, which involves use of mechanical sound waves. Images formed with high-energy radiation at shorter wavelength such as X-ray and Gamma-rays at one end of the spectrum are ionizing whereas at longer wavelength -optical and still longer wavelength -MRI and Ultrasound are nonionizing. The imaging modalities covered in this section are X-ray, ultrasound, magnetic resonance (MR), X-ray computed tomography (CT), nuclear medicine, and high-resolution microscopy [8], [9] (see Table I). Fig. 1 shows some examples of images produced by these modalities.\nby an X-ray source through the body and detecting the attenuated X-rays on the other side via a detector array; the resulting image is a 2D projection with resolutions down to 100 microns and where the intensities are indicative of the degree of X-ray attenuation [9]. To improve visibility, iodinated contrast agents that attenuate X-rays are often injected into a region of interest (e.g., imaging arterial disease through fluoroscopy). Phase-contrast X-ray imaging can also improve soft-tissue image contrast by using the phase-shifts of the X-rays as they traverse through the tissue [10]. X-ray projection imaging has been pervasive in cardiovascular, mammography, musculoskeletal, and abdominal imaging applications among others [11].\nUltrasound imaging (US) employs pulses in the range of 1-10 MHz to image tissue in a noninvasive and relatively inexpensive way. The backscattering effect of the acoustic pulse interacting with internal structures is used to measure the echo to produce the image. Ultrasound imaging is fast, enabling, for example, real-time imaging of blood flow in arteries through the Doppler shift. A major benefit of ultrasonic imaging is that no ionizing radiation is used, hence less harmful to the patient. However, bone and air hinder the propagation of sound waves and can cause artifacts. Still, ultrasound remains one of the most used imaging techniques employed extensively for real-time cardiac and fetal imaging [11]. Contrast-enhanced ultrasound has allowed for greater contrast and imaging accuracy with the use of injected microbubbles to increase reflection in specific areas in some applications [12]. Ultrasound elasticity imaging has also been used for measuring the stiffness of tissue for virtual palpation [13]. Importantly, ultrasound is not limited to 2D imaging and use of 3D and 4D imaging is expanding, though with reduced temporal resolution [14].\nMR imaging [15] produces high spatial resolution volumetric images primarily of Hydrogen nuclei, using an externally applied magnetic field in conjunction with radio-frequency (RF) pulses which are non-ionizing [1]. MRI is commonly used in numerous applications including musculoskeletal, cardiovascular, and neurological imaging with superb soft-tissue contrast [16], [17]. Additionally, functional MRI has evolved into a large sub-field of study with applications in areas such as mapping the functional connectivity in the brain [18]. Similarly, diffusion-weighted MRI images the diffusion of water molecules in the body and has found much use in neuroimaging and oncology applications [19]. Moreover, Magnetic Resonance Elastography (MRE) allows virtual palpation with significant applications in liver fibrosis [20], while 4D flow methods permit exquisite visualization of flow in 3D + t [17], [21]. Techniques that accelerate the acquisition time of scans, e.g. compressed sensing, non-Cartesian acquisitions [22], and parallel imaging [23], have led to increased growth and utilization of MR imaging. In 2017, 36 million MRI scans were performed in the US alone [24].\nX-ray CT imaging [25] also offers volumetric scans like MRI. However, CT CT produces a 3D image via th",
              "url": "https://openalex.org/W3030790048",
              "openalex_id": "https://openalex.org/W3030790048",
              "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
              "publication_date": "2020-05-29"
            }
          ]
        },
        "S0407710670": {
          "id": "S0407710670",
          "text": "The integration of computer-aided diagnosis (CAD) systems in histopathology is essential for improving diagnostic workflows, as these systems assist pathologists by identifying benign areas and allowing them to focus on more complex cases.",
          "children": [
            {
              "id": "E5075243097",
              "text": "..2016; 33: 170-175Crossref PubMed Scopus (370) Google Scholar, 8Niazi M.K.K. Parwani A.V. Gurcan M.N. Digital pathology and artificial intelligence.Lancet Oncol. 2019; 20: e253-e261Abstract Full Text Full Text PDF PubMed Scopus (204) Google Scholar, 9Shen D. Wu G. Suk H.-I. Deep learning in medical image analysis.Annu Rev Biomed Eng. 2017; 19: 221-248Crossref PubMed Scopus (1580) Google Scholar, 10Stenzinger A. Alber M. Allg\u00e4uer M. Jurmeister P. Bockmayr M. Budczies J. Lennerz J. Eschrich J. Kazdal D. Schirmacher P. Wagner A.H. Tacke F. Capper D. M\u00fcller K.-R. Klauschen F. Artificial intelligence and pathology: from principles to practice and future applications in histomorphology and molecular profiling.Semin Cancer Biol. 2021; ([Epub ahead of print] doi:10.1016/j.semcancer.2021.02.011)Crossref Scopus (7) Google Scholar Radiologists are adept in the use of technology (eg, computed tomography and magnetic resonance imaging), using it as a key driver of the practice of radiology.8Niazi M.K.K. Parwani A.V. Gurcan M.N. Digital pathology and artificial intelligence.Lancet Oncol. 2019; 20: e253-e261Abstract Full Text Full Text PDF PubMed Scopus (204) Google Scholar, 9Shen D. Wu G. Suk H.-I. Deep learning in medical image analysis.Annu Rev Biomed Eng. 2017; 19: 221-248Crossref PubMed Scopus (1580) Google Scholar, 10Stenzinger A. Alber M. Allg\u00e4uer M. Jurmeister P. Bockmayr M. Budczies J. Lennerz J. Eschrich J. Kazdal D. Schirmacher P. Wagner A.H. Tacke F. Capper D. M\u00fcller K.-R. Klauschen F. Artificial intelligence and pathology: from principles to practice and future applications in histomorphology and molecular profiling.Semin Cancer Biol. 2021; ([Epub ahead of print] doi:10.1016/j.semcancer.2021.02.011)Crossref Scopus (7) Google Scholar, 11Miotto R. Wang F. Wang S. Jiang X. Dudley J.T. Deep learning for healthcare: review, opportunities and challenges.Brief Bioinform. 2018; 19: 1236-1246Crossref PubMed Scopus (740) Google Scholar Technological innovation is playing an increasingly dominant role in the practice of pathology as well. Both radiology and pathology are image-intensive specialties that make extensive use of image data for patient care via specialist-generated interpretations. Although radiology is further along the path of digitization and management of medical images, pathology is increasingly moving along the same path.8Niazi M.K.K. Parwani A.V. Gurcan M.N. Digital pathology and artificial intelligence.Lancet Oncol. 2019; 20: e253-e261Abstract Full Text Full Text PDF PubMed Scopus (204) Google Scholar,10Stenzinger A. Alber M. Allg\u00e4uer M. Jurmeister P. Bockmayr M. Budczies J. Lennerz J. Eschrich J. Kazdal D. Schirmacher P. Wagner A.H. Tacke F. Capper D. M\u00fcller K.-R. Klauschen F. Artificial intelligence and pathology: from principles to practice and future applications in histomorphology and molecular profiling.Semin Cancer Biol. 2021; ([Epub ahead of print] doi:10.1016/j.semcancer.2021.02.011)Crossref Scopus (7) Google Scholar, 11Miotto R. Wang F. Wang S. Jiang X. Dudley J.T. Deep learning for healthcare: review, opportunities and challenges.Brief Bioinform. 2018; 19: 1236-1246Crossref PubMed Scopus (740) Google Scholar, 12Abels E. Pantanowitz L. Aeffner F. Zarella M.D. van der Laak J. Bui M.M. Vemuri V.N. Parwani A.V. Gibbs J. Agosto-Arroyo E. Beck A.H. Kozlowski C. Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association.J Pathol. 2019; 249: 286-294Crossref PubMed Scopus (76) Google Scholar, 13Komura D. Ishikawa S. Machine learning methods for histopathological image analysis.Comput Struct Biotechnol J. 2018; 16: 34-42Crossref PubMed Scopus (309) Google Scholar The heavy reliance on images and digitization makes these two health care specialties most attractive to AI researchers for testing emerging ideas of imaging AI research. Imaging AI algorithms have seen the greatest amount of research and advances over the past decade. The confluence of abundant imaging data, ever-increasing cheap and powerful computational capacity, and advancing algorithmic AI research make radiology and pathology prime targets for disruptive innovation of health care AI applications over the next decade.7Madabhushi A. Lee G. Image analysis and machine learning in digital pathology: challenges and opportunities.Med Image Anal. 2016; 33: 170-175Crossref PubMed Scopus (370) Google Scholar,9Shen D. Wu G. Suk H.-I. Deep learning in medical image analysis.Annu Rev Biomed Eng. 2017; 19: 221-248Crossref PubMed Scopus (1580) Google Scholar,14Langlotz C.P. Allen B. Erickson B.J. Kalpathy-Cramer J. Bigelow K. Cook T.S. Flanders A.E. Lungren M.P. Mendelson D.S. Rudie J.D. Wang G. Kandarpa K. A roadmap for foundational research on artificial intelligence in medical imaging: from the 2018 NIH/RSNA/ACR/The Academy Workshop.Radiology. 2019; 291: 781-791Crossref PubMed Scopus (114) Google Scholar,15Louis D.N. Feldman M. Carter A.B. Dighe A.S. Pfeifer J.D. Bry L. Almeida J.S. Saltz J. Braun J. Tomaszewski J.E. Gilbertson J.R. Sinard J.H. Gerber G.K. Galli S.J. Golden J.A. Becich M.J. Computational pathology: a path ahead.Arch Pathol Lab Med. 2016; 140: 41-50Crossref PubMed Scopus (61) Google Scholar The second arm of the practice of pathology (in addition to image-intensive anatomic pathology) is the area of clinical laboratory medicine. Automation in clinical laboratory medicine has been well underway for many decades, resulting in vastly improved efficiency in delivering patient test results. In emerging fields such as precision medicine, there is great interest in the use of genomics and other forms of -omics data for both diagnosis and prognosis, with the use of information at a molecular level.16Min S. Lee B. Yoon S. Deep learning in bioinformatics.Brief Bioinform. 2017; 18: 851-869PubMed Google Scholar The new frontier of omics technologies is a true big data specialty with vast amounts of omics patient data generated in each encounter.17Diao J.A. Wang J.K. Chui W.F. Mountain V. Gullapally S.C. Srinivasan R. Mitchell R.N. Glass B. Hoffman S. Rao S.K. Maheshwari C. Lahiri A. Prakash A. McLoughlin R. Kerner J.K. Resnick M.B. Montalto M.C. Khosla A. Wapinski I.N. Beck A.H. Elliott H.L. Taylor-Weiner A. Human-interpretable image features derived from densely mapped cancer pathology slides predict diverse molecular phenotypes.Nat Commun. 2021; 12: 1613Crossref PubMed Scopus (13) Google Scholar,18Djuric U. Zadeh G. Aldape K. Diamandis P. Precision histology: how deep learning is poised to revitalize histomorphology for personalized cancer care.NPJ Precis Oncol. 2017; 1: 22Crossref PubMed Google Scholar The field of bioinformatics focuses on algorithmic computational methods to manage and interpret such omics data in various clinical settings. AI researchers are highly interested in using AI-based methods to understand omics data in the context of patient health care.16Min S. Lee B. Yoon S. Deep learning in bioinformatics.Brief Bioinform. 2017; 18: 851-869PubMed Google Scholar,19Castaneda C. Nalley K. Mannion C. Bhattacharyya P. Blake P. Pecora A. Goy A. Suh K.S. Clinical decision support systems for improving diagnostic accuracy and achieving precision medicine.J Clin Bioinforma. 2015; 5: 4Crossref PubMed Google Scholar Perhaps the ultimate challenge in the use of AI-enabled health care is to synthesize both imaging and genomics data from a patient to provide novel insights into clinical outcomes and management.17Diao J.A. Wang J.K. Chui W.F. Mountain V. Gullapally S.C. Srinivasan R. Mitchell R.N. Glass B. Hoffman S. Rao S.K. Maheshwari C. Lahiri A. Prakash A. McLoughlin R. Kerner J.K. Resnick M.B. Montalto M.C. Khosla A. Wapinski I.N. Beck A.H. Elliott H.L. Taylor-Weiner A. Human-interpretable image features derived from densely mapped cancer pathology slides predict diverse molecular phenotypes.Nat Commun. 2021; 12: 16..",
              "url": "https://openalex.org/W3181481201",
              "openalex_id": "https://openalex.org/W3181481201",
              "title": "Ethics of AI in Pathology",
              "publication_date": "2021-07-10"
            },
            {
              "id": "S8880063867",
              "text": "The integration of computer-aided diagnosis (CAD) systems in histopathology is essential for improving diagnostic accuracy and efficiency, as these systems can assist pathologists by identifying benign areas and allowing them to focus on more complex cases.",
              "children": [
                {
                  "id": "E1163273716",
                  "text": "University of Warwick institutional repository: http://go.warwick.ac.uk/wrap This paper is made available online in accordance with publisher policies. Please scroll down to view the document itself. Please refer to the repository record for this item and our policy information available from the repository home page for further information. To see the final version of this paper please visit the publisher\u2019s website. Access to the published version may require a subscription. Author(s): Gurcan, M.N. Boucheron, L.E. Can, A. Madabhushi, A. Rajpoot, N.M. Yener, B. Article Title: Histopathological Image Analysis: A Review Year of publication: 2009 Link to published article: http://dx.doi.org/ 10.1109/RBME.2009.2034865 Publisher statement: (c) 2009 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 147 Histopathological Image Analysis: A Review Metin N. Gurcan, Senior Member, IEEE, Laura E. Boucheron, Member, IEEE, Ali Can, Anant Madabhushi, Senior Member, IEEE, Nasir M. Rajpoot, Member, IEEE, and Bulent Yener, Senior Member, IEEE Methodological Review Abstract\u2014Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe. Index Terms\u2014Computer-aided diagnosis, computer-assisted interpretation, digital pathology, histopathology, image analysis, microscopy analysis. I. INTRODUCTION AND MOTIVATION T HE widespread use of computer-assisted diagnosis (CAD) can be traced back to the emergence of digital mammography in the early 1990s [1]. Recently, CAD has become a part of routine clinical detection of breast cancer on mammograms at many screening sites and hospitals [2] in Manuscript received July 20, 2009; revised October 07, 2009. First published October 30, 2009; current version published December 09, 2009. This work was supported in part by the National Cancer Institute under Grants R01 CA134451, R01CA136535-01, ARRA-NCl-3 21CA127186\u201302S1, R21CA127186\u201301, R03CA128081-01, and R03CA143991-01, National Library of Medicine R01 LM010119, American Cancer Society, The Children\u2019s Neuroblastoma Cancer Foundation, Wallace H. Coulter Foundation, New Jersey Commission on Cancer Research, The Cancer Institute of New Jersey, and the Life Science Commercialization Award from Rutgers University, The Ohio State University Center for Clinical and Translational Science, Department of Defense under Grant W81XWH-07-1-0402. M. N. Gurcan is with the Department of Biomedical Informatics, The Ohio State University, Columbus, OH 43210 USA (e-mail: metin.gurcan@osumc. edu). L. E. Boucheron is with the Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM 88003 USA (e-mail: lboucher@nmsu.edu). A. Can is with the Global Research Center, General Electric Corporation, Niskayuna, NY 12309 USA (e-mail: can@research.ge.com). A. Madabhushi is with the Biomedical Engineering Department, Rutgers University, Piscataway, NJ 08854 USA (e-mail: anantm@rci.rutgers.edu). N. M. Rajpoot is with the Department of Computer Science, University of Warwick, Coventry, CV4 7AL, U.K. (e-mail: N.M.Rajpoot@warwick.ac.uk) B. Yener is with the Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180 USA (e-mail: yener@cs.rpi.edu) Digital Object Identifier 10.1109/RBME.2009.2034865 the United States. In fact, CAD has become one of the major research subjects in medical imaging and diagnostic radiology. Given recent advances in high-throughput tissue bank and archiving of digitized histological studies, it is now possible to use histological tissue patterns with computer-aided image analysis to facilitate disease classification. There is also a pressing need for CAD to relieve the workload on pathologists by sieving out obviously benign areas, so that the pathologist can focus on the more difficult-to-diagnose suspicious cases. For example, approximately 80% of the 1 million prostate biopsies performed in the U.S. every year are benign; this suggests that prostate pathologists are spending 80% of their time sieving through benign tissue. Researchers in both the image analysis and pathology fields have recognized the importance of quantitative analysis of pathology images. Since most current pathology diagnosis is based on the subjective (but educated) opinion of pathologists, there is clearly a need for quantitative image-based assessment of digital pathology slides. This quantitative analysis of digital pathology is important not only from a diagnostic perspective, but also in order to understand the underlying reasons for a specific diagnosis being rendered (e.g., specific chromatin texture in the cancerous nuclei which may indicate certain genetic abnormalities). In addition, quantitative characterization of pathology imagery is important not only for clinical applications (e.g., to reduce/eliminate inter- and intra-observer variations in diagnosis) but also for research applications (e.g., to understand the biological mechanisms of the disease process). A large focus of pathological image analysis has been on the automated analysis of cytology imagery. Since cytology imagery often results from the least invasive biopsies (e.g., the cervical Pap smear), they are some of the most commonly encountered imagery for both disease screening and biopsy purposes. Additionally, the characteristics of cytology imagery, namely the presence of isolated cells and cell clusters in the images and the absence of more complicated structures such as glands make it easier to analyze these specimens compared to histopathology. For example, the segmentation of individual cells or nuclei is a relatively easier process in such imagery since most of the cells are inherently separated from each other. Histopathology slides, on the other hand, provide a more comprehensive view of disease and its effect on tissues, since the preparation process preserves the underlying tissue architecture. As such, some disease characteristics, e.g., lymphocytic infiltration of cancer, may be deduced only from 1937-3333/$26.00 \u00a9 2009 IEEE Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. 148 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 a histopathology image. Additionally, the diagnosis from a histopathology image remains the \u201cgold standard\u201d in diagnosing considerable number of diseases including almost all types of cancer [3]. The additional structure in these images, while providing a wealth of information, also presents a new set of challenges from an automated image analysis perspective. It is expected that the proper leverage of this spatial information will allow for more specific characterizations of the imagery from a diagnostic perspective. The analysis of histopathology imagery has generally followed directly from techniques used to analyze cytology imagery. In particular, certain characteristics of nuclei are hallmarks of cancerous conditions. Thus, quantitative metrics for cancerous nuclei were developed to appropriately encompass the general observations of the experienced pathologist, and were tested on cytology imagery. These same metrics can also be applied to histopathological imagery, provided histological structures such as cell nuclei, glands, and lymphocytes have been adequately segmented (a complication due to the complex structure of histopathological imagery). The analysis of the spatial structure of histopathology imagery can be traced back to the works of Wiend et al. [4], Bartels [5] and Hamilton [6] but has largely been overlooked perhaps due to the lack of computational resources and the relatively high cost of digital imaging equipment for pathology. However, spatial analysis of histopathology imagery has recently become the backbone of most automated histopathology image analysis techniques. Despite the progress made in this area thus far, this is still a large area of open research due to the variety of imaging methods and disease-specific characteristics. A. Need for Quantitative Image Analysis for Disease Grading Currently, histopathological tissue analysis by a pathologist represents the only definitive method (a) for confirmation of presence or absence of disease and (b) disease grading, or the measurement of disease progression. The need for quantitative image analysis in the context of one specific disease (prostate cancer) is described below. Similar conclusions hold for quantitative analysis of other disease imagery. Higher Gleason scores are given to prostate cancers, which are more aggressive, and the grading scheme is used to predict cancer prognosis and help guide therapy. The Gleason grading system is based solely on architectural patterns; cytological features are not evaluated. The standard schematic diagram created by Gleason and his group (see Fig. 1) separated architectural features into 1 of 5 histological patterns of decreasing differentiation, pattern 1 being most differentiated and pattern 5 being least differentiated. The second unique feature of Gleason grading is that grade is not based on the highest (least differentiated..",
                  "url": "https://openalex.org/W2103243046",
                  "openalex_id": "https://openalex.org/W2103243046",
                  "title": "Histopathological Image Analysis: A Review",
                  "publication_date": "2009-01-01"
                },
                {
                  "id": "E1522486102",
                  "text": "..variety of conditions studied in histopathology image analysis is greater, it is still important that standard datasets be compiled as well as a standard metric of performance. This will allow for direct comparison of the variety of analysis methods being reported in the literature. An additional complication is the variety of analyses performed on the histopathology imagery. Thus, there is a need for a dataset with ground truth pertaining to all the analyses described in this paper. Going forward, clinical annotation of histopathology data will be a large bottleneck in the evaluation of histopathology related CAD algorithms. Apart from the time constraints on the pathologist to generate this data, the process should be streamlined with active communication between the image analysis scientists and the clinicians with regard to the sort of annotation required, the format and scale at which the annotation is generated, and the ease with which the data can be shared (since histopathology files typically tend to be very large). For instance, the sophistication of annotation required to train a CAD system to distinguish cancerous versus noncancerous regions on pathology images may be very different than the annotation detail required to train a classifier to distinguish grades of cancer. While for the former problem the annotation could be done on a coarser scale (lower resolution), the latter annotation may require explicit segmentation of glands and nuclei, a far more laborious and time consuming process. Due to the large size of pathological images, usually it is not possible to process the whole image on a single-core processor. Therefore, the whole image may be divided into tiles and each tile is processed independently. As a consequence, automatic load balancing in the distribution of the cases to different processors need to be handled carefully [120]. Additionally, the processing can be accelerated even further by the use of graphical processing units (GPUs), cell blades, or any other emerging high-performance architecture [121]. Histopathological image analysis system evaluation needs to be carried out in a statistical framework. Depending on whether it is a problem of detection (e.g., nuclei detection) or characterization (e.g., grading), some commonly accepted evaluation methodologies need to be followed. Some of these methods, e.g., receiver operating characteristics (ROC) and free response operating characteristics (FROC), have been successfully used for many years in radiology [122]. These techniques could be adopted or adapted accordingly. The level and detailed of quantitative evaluation will vary as a function of the specific problem being addressed. For instance, in order to evaluate a nuclear segmentation algorithm on a digitized histological section containing several tens of thousands of nuclei, it is unreasonable to 1http://marathon.csee.usf.edu/Mammography/Database.html Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. 166 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 2, 2009 Fig. 17. (a) Histology section of prostate gland with CaP extent stained in purple (upper right) and corresponding mapping of CaP extent via COFEMI onto (b) MRI (CaP extent shown in green). (c) Overlay of histological and MRI prostate sections following registration. expect that a human reader will be able to manually annotate all nuclei. Evaluation of the scheme may have to be performed on randomly chosen segments of the image. Similarly, if the ultimate objective of the CAD algorithm is, for instance, cancer grading, perfect segmentation of histological structures may not guarantee perfect grade-based classification. Evaluation should hence be tailored towards the ultimate objective that the CAD algorithm is being employed for. Additionally, special attention needs to be paid to clearly separate training and testing datasets and explain the evaluation methodology. A. Multimodal Data Fusion/Registration While digital pathology offers very interesting, highly dense data, one of the exciting challenges will be in the area of multimodal data fusion. One of the big open questions, especially as it pertains to personalized medicine, will be the use of multimodal data classifiers to be able to make therapy recommendations. This will require solving questions both in terms of data alignment and in terms of knowledge representation for fusion of heterogeneous sources of data, in order to answer questions that go beyond just diagnosis, such as theragnosis (therapy prediction) and prognosis. H&E staining is traditionally used for histopathology imaging. Several other modalities exist for imaging of the tissue, each offering its own advantages and limitations. Combining images from different modalities, therefore, may seem to be an attractive proposition, although it does not come without its own challenges, most importantly registration, not to mention the extra cost associated with imaging, storage, and computational time. Registration of image data across the different modalities and fusion of the information contained therein result in a powerful resource of information for diagnosis and prognosis purposes. Fusion methods have been developed for images from different microscopy imaging methods [26] and micro-scale histopathology and large-scale MR images [123]\u2013[125]. Madabhushi et al. [126] have been developing computerized detection methods for prostate cancer from high-resolution multimodal MRI . A prerequisite to training a supervised classifier to identify prostate cancer (CaP) on MRI is the ability to precisely determine spatial extent of CaP on the radiological imaging modality. CaP can be precisely determined on whole mount histopathology specimens [Fig. 17(a)] which can then be mapped onto MRI [Fig. 17(b)]. Fig. 17(c) shows the result of registering [Fig. 17(b)] the 2-D MRI slice to the histological section [Fig. 17(a)]. This requires the use of sophisticated and robust multimodal deformable registration methods to account for (a) deformations and tissue loss in the whole mount histological specimens during acquisition, and (b) ability to overcome intensity and feature differences between the two modalities (histopathology and MRI). In [123], [124] a rigid registration scheme called combined feature ensemble based mutual information (COFEMI) was presented that used alternate feature representations of the target and source images to be registered to facilitate the alignment. B. Correlating Histological Signatures With Protein and Gene Expression Multiplexing, imaging of a tissue sample with several antibodies simultaneously, allows correlation of characteristic patterns in histopathology images to expression of proteins. Teverovskiy et al. [127] recently proposed a novel scheme for automated localization and quantification of the expression of protein biomarkers using a DAPI counter-stain and three other biomarkers. They showed it to be useful for predicting recurrence of prostate cancer in patients undergoing prostatectomy. Recently, it has become clear that information regarding expression of certain proteins related to the onset of cancer is not sufficient. Analyzing multiple-stained histopathology images can help identify oncogenesis-induced changes in sub-cellular location patterns of proteins. Glory et al. [128] proposed a novel approach to compare the sub-cellular location of proteins between normal and cancerous tissues. Such a method can also be used for identification of proteins to be used as potential biomarkers. C. Exploratory Histopathology Image Analysis Exploratory analysis of histopathology images can help in finding salient diagnostic features used by humans, associating them with the computed features, and visualizing relationships between different features in high-dimensional spaces. Lessmann et al. [129] have proposed the use of self-organizing maps (SOMs) for exploratory analysis of their wavelet-based feature space. The SOM-based visualization of the feature space allowed the authors of [129] to establish a correlation between single features and histologically relevant image structures, making the selection of a subset of clinically important features possible. Iglesias-Rozas and Hopf [130] showed that SOMs can be effectively employed to correctly classify different subtypes of human Glioblastomas (GB) and also to select significant histological and clinical or genetic variables. Alternatively, dimensionality reduction methods may offer a way of looking at trends and patterns in the data in a reduced dimensional space [131]\u2013[133]. D. Computer-Aided Prognosis The use of computer-aided diagnosis for digitized histopathology could begin to be employed for disease prognostics, allowing physicians to predict which patients may be susceptible to a particular disease and also predicting disease outcome and survival. For instance, since grade is known to be correlated to outcome (high grade correlates to worse outcome), image-based predictors could be used to predict disease recurrence and survival based on analysis of biopsy specimens alone. This would have significant translational implications in Authorized licensed use limited to: WARWICK UNIVERSITY. Downloaded on August 12,2010 at 10:37:35 UTC from IEEE Xplore. Restrictions apply. GURCAN et al.: HISTOPATHOLOGICAL IMAGE ANALYSIS: A REVIEW 167 that; more expensive molecular assays may not be required for predicting disease. While there may be a small minority of researchers who are experts in both computer vision and pathology, the vast majority of histopathology image analysis researchers are computer vision researchers. As such, it is important to maintain a constant collaboration with clinical and research pathologists throughout the research process. There are unique challenges to analysis of medical imagery, particularly in the performances required for eventual use of the technique in a clinical setting. It is the pathologist who can best provide the feedback on the performance of the system, as well as suggesting new avenues of research that would provide beneficial information to the pathologist community. Additionally, it is the pathologist that is best equipped to interpret the analysis results in light of underlying biological mechanisms which, in turn, may lead to new research ideas. Similarly, where appropriate it might be pertinent to include the oncologist and radiologist within the algorithmic development and evaluation loop as well. ACKNOWLEDGMENT M. N. Gurcan would like to thank O. Sertel and K. Boussaid for carefully reviewing the manuscript and for useful discussions. REFERENCES [1] A. J. Mendez, P. G. Tahoces, M. J. Lado, M. Souto, and J. J. Vidal, \u201cComputer-aided diagnosis: Automatic detection of malignant masses in digitized mammograms,\u201d Med Phys., vol. 25, pp. 957\u201364, Jun. 1998. [2] J. Tang, R. Rangay..",
                  "url": "https://openalex.org/W2103243046",
                  "openalex_id": "https://openalex.org/W2103243046",
                  "title": "Histopathological Image Analysis: A Review",
                  "publication_date": "2009-01-01"
                }
              ]
            }
          ]
        }
      },
      "pipeline_source_papers": [
        "https://openalex.org/W2956228567",
        "https://openalex.org/W2103243046",
        "https://openalex.org/W2945500496",
        "https://openalex.org/W3030790048",
        "https://openalex.org/W2767410506",
        "https://openalex.org/W3089090082",
        "https://openalex.org/W2756350390",
        "https://openalex.org/W3181481201",
        "https://openalex.org/W2964756323"
      ],
      "evaluation": {
        "precision@10": 0.5555555555555556,
        "recall@10": 0.5,
        "f1@10": 0.5263157894736842,
        "rouge_1": 0.06318207926479034,
        "rouge_2": 0.020109164033323756,
        "rouge_l": 0.03130384836300976,
        "text_f1": 0.11447492904446546,
        "num_source_papers": 9
      }
    }
  ]
}