[
  {
    "id": "https://openalex.org/W4391109864",
    "meta": {
      "id": "https://openalex.org/W4391109864",
      "title": "Segment anything in medical images",
      "publication_date": "2024-01-22",
      "cited_by_count": 206,
      "topics": "Radiomics in Medical Imaging Analysis, Deep Learning in Medical Image Analysis, Applications of Deep Learning in Medical Imaging",
      "keywords": "Medical Imaging, Medical Image Analysis, Modalities, Robustness (evolution), Modality (human\u2013computer interaction), Cancer Imaging, Whole Slide Imaging, Image-Based Diagnosis",
      "concepts": "Generalizability theory, Computer science, Segmentation, Modalities, Robustness (evolution), Artificial intelligence, Modality (human\u2013computer interaction), Image segmentation, Medical imaging, Personalization, Computer vision, Bridging (networking), Machine learning, Computer network, Mathematics, Social science, Biochemistry, Gene, Statistics, World Wide Web, Sociology, Chemistry",
      "best_oa_location_pdf_url": "https://www.nature.com/articles/s41467-024-44824-z.pdf",
      "pdf_urls_by_priority": [
        "https://www.nature.com/articles/s41467-024-44824-z.pdf"
      ],
      "text_type": "full_text",
      "successful_pdf_url": "https://www.nature.com/articles/s41467-024-44824-z.pdf",
      "openalex_rank": 7,
      "num_tokens": 10761,
      "url": "https://www.nature.com/articles/s41467-024-44824-z.pdf"
    },
    "text": "Article https://doi.org/10.1038/s41467-024-44824-z\nSegment anything in medical images\nJun Ma1,2,3, Yuting He4, Feifei Li 1, Lin Han5, Chenyu You 6 &\nBo Wang 1,2,3,7,8\nMedical image segmentation is a critical component in clinical practice, facil\u0002itating accurate diagnosis, treatment planning, and disease monitoring.\nHowever, existing methods, often tailored to specific modalities or disease\ntypes, lack generalizability across the diverse spectrum of medical image\nsegmentation tasks. Here we present MedSAM, a foundation model designed\nfor bridging this gap by enabling universal medical image segmentation. The\nmodel is developed on a large-scale medical image dataset with 1,570,263\nimage-mask pairs, covering 10 imaging modalities and over 30 cancer types.\nWe conduct a comprehensive evaluation on 86 internal validation tasks and 60\nexternal validation tasks, demonstrating better accuracy and robustness than\nmodality-wise specialist models. By delivering accurate and efficient seg\u0002mentation across a wide spectrum of tasks, MedSAM holds significant\npotential to expedite the evolution of diagnostic tools and the personalization\nof treatment plans.\nSegmentation is a fundamental task in medical imaging analysis, which\ninvolves identifying and delineating regions of interest (ROI) in various\nmedical images, such as organs, lesions, and tissues1\n. Accurate seg\u0002mentation is essential for many clinical applications, including disease\ndiagnosis, treatment planning, and monitoring of disease\nprogression2,3\n. Manual segmentation has long been the gold standard\nfor delineating anatomical structures and pathological regions, but\nthis process is time-consuming, labor-intensive, and often requires a\nhigh degree of expertise. Semi- or fully automatic segmentation\nmethods can significantly reduce the time and labor required, increase\nconsistency, and enable the analysis of large-scale datasets4\n.\nDeep learning-based models have shown great promise in medical\nimage segmentation due to their ability to learn intricate image fea\u0002tures and deliver accurate segmentation results across a diverse range\nof tasks, from segmenting specific anatomical structures to identifying\npathological regions5\n. However, a significant limitation of many cur\u0002rent medical image segmentation models is their task-specific nature.\nThese models are typically designed and trained for a specific seg\u0002mentation task, and their performance can degrade significantly when\napplied to new tasks or different types of imaging data6\n. This lack of\ngenerality poses a substantial obstacle to the wider application of\nthese models in clinical practice. In contrast, recent advances in the\nfield of natural image segmentation have witnessed the emergence of\nsegmentation foundation models, such as segment anything model\n(SAM)7 and Segment Everything Everywhere with Multi-modal\nprompts all at once8\n, showcasing remarkable versatility and perfor\u0002mance across various segmentation tasks.\nThere is a growing demand for universal models in medical image\nsegmentation: models that can be trained once and then applied to a\nwide range of segmentation tasks. Such models would not only exhibit\nheightened versatility in terms of model capacity but also potentially\nlead to more consistent results across different tasks. However, the\napplicability of the segmentation foundation models (e.g., SAM7\n) to\nmedical image segmentation remains limited due to the significant\ndifferences between natural images and medical images. Essentially,\nSAM is a promptable segmentation method that requires points or\nbounding boxes to specify the segmentation targets. This resembles\nconventional interactive segmentation methods4,9\u201311 but SAM has bet\u0002ter generalization ability, while existing deep learning-based inter\u0002active segmentation methods focus mainly on limited tasks and image\nmodalities.\nMany studies have applied the out-of-the-box SAM models to\ntypical medical image segmentation tasks12\u201317 and other challenging\nscenarios18\u201321. For example, the concurrent studies22,23 conducted a\nReceived: 24 October 2023\nAccepted: 5 January 2024\nCheck for updates\n1\nPeter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada. 2Department of Laboratory Medicine and Pathobiology, University of Toronto,\nToronto, ON, Canada. 3\nVector Institute, Toronto, ON, Canada. 4Department of Computer Science, Western University, London, ON, Canada. 5Tandon School\nof Engineering, New York University, New York, NY, USA. 6\nDepartment of Electrical Engineering, Yale University, New Haven, CT, USA. 7Department of\nComputer Science, University of Toronto, Toronto, ON, Canada. 8\nUHN AI Hub, Toronto, ON, Canada. e-mail: bowang@vectorinstitute.ai\nNature Communications | (2024) 15:654 1\n1234567890():,;1234567890():,;\ncomprehensive assessment of SAM across a diverse array of medical\nimages, underscoring that SAM achieved satisfactory segmentation\noutcomes primarily on targets characterized by distinct boundaries.\nHowever, the model exhibited substantial limitations in segmenting\ntypical medical targets with weak boundaries or low contrast. In con\u0002gruence with these observations, we further introduce MedSAM, a\nrefined foundation model that significantly enhances the segmenta\u0002tion performance of SAM on medical images. MedSAM accomplishes\nthis by fine-tuning SAM on an unprecedented dataset with more than\none million medical image-mask pairs.\nWe thoroughly evaluate MedSAM through comprehensive\nexperiments on 86 internal validation tasks and 60 external validation\ntasks, spanning a variety of anatomical structures, pathological con\u0002ditions, and medical imaging modalities. Experimental results\ndemonstrate that MedSAM consistently outperforms the state-of-the\u0002art (SOTA) segmentation foundation model7\n, while achieving perfor\u0002mance on par with, or even surpassing specialist models1,24 that were\ntrained on the images from the same modality. These results highlight\nthe potential of MedSAM as a new paradigm for versatile medical\nimage segmentation.\nResults\nMedSAM: a foundation model for promptable medical image\nsegmentation\nMedSAM aims to fulfill the role of a foundation model for universal\nmedical image segmentation. A crucial aspect of constructing such a\nmodel is the capacity to accommodate a wide range of variations in\nimaging conditions, anatomical structures, and pathological condi\u0002tions. To address this challenge, we curated a diverse and large-scale\nmedical image segmentation dataset with 1,570,263 medical image\u0002mask pairs, covering 10 imaging modalities, over 30 cancer types, and\na multitude of imaging protocols (Fig. 1 and Supplementary\nTables 1\u20134). This large-scale dataset allows MedSAM to learn a rich\nrepresentation of medical images, capturing a broad spectrum of\nanatomies and lesions across different modalities. Figure 2a provides\nan overview of the distribution of images across different medical\nimaging modalities in the dataset, ranked by their total numbers. It is\nevident that computed tomography (CT), magnetic resonance ima\u0002ging (MRI), and endoscopy are the dominant modalities, reflecting\ntheir ubiquity in clinical practice. CT and MRI images provide detailed\ncross-sectional views of 3D body structures, making them indis\u0002pensable for non-invasive diagnostic imaging. Endoscopy, albeit more\ninvasive, enables direct visual inspection of organ interiors, proving\ninvaluable for diagnosing gastrointestinal and urological conditions.\nDespite the prevalence of these modalities, others such as ultrasound,\npathology, fundus, dermoscopy, mammography, and optical coher\u0002ence tomography (OCT) also hold significant roles in clinical practice.\nThe diversity of these modalities and their corresponding segmenta\u0002tion targets underscores the necessity for universal and effective\nsegmentation models capable of handling the unique characteristics\nassociated with each modality.\nAnother critical consideration is the selection of the appropriate\nsegmentation prompt and network architecture. While the concept of\nfully automatic segmentation foundation models is enticing, it is\nfraught with challenges that make it impractical. One of the primary\nchallenges is the variability inherent in segmentation tasks. For\nexample, given a liver cancer CT image, the segmentation task can vary\ndepending on the specific clinical scenario. One clinician might be\nFig. 1 | MedSAM is trained on a large-scale dataset that can handle diverse segmentation tasks. The dataset covers a variety of anatomical structures, pathological\nconditions, and medical imaging modalities. The magenta contours and mask overlays denote the expert annotations and MedSAM segmentation results, respectively.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 2\ninterested in segmenting the liver tumor, while another might need to\nsegment the entire liver and surrounding organs. Additionally, the\nvariability in imaging modalities presents another challenge. Mod\u0002alities such as CT and MR generate 3D images, whereas others like\nX-ray and ultrasound yield 2D images. These variabilities in task defi\u0002nition and imaging modalities complicate the design of a fully auto\u0002matic model capable of accurately anticipating and addressing the\ndiverse requirements of different users.\nConsidering these challenges, we argue that a more practical\napproach is to develop a promptable 2D segmentation model. The\nmodel can be easily adapted to specific tasks based on user-provided\nprompts, offering enhanced flexibility and adaptability. It is also able\nto handle both 2D and 3D images by processing 3D images as a series\nof 2D slices. Typical user prompts include points and bounding boxes\nand we show some segmentation examples with the different prompts\nin Supplementary Fig. 1. It can be found that bounding boxes provide a\nmore unambiguous spatial context for the region of interest, enabling\nthe algorithm to more precisely discern the target area. This stands in\ncontrast to point-based prompts, which can introduce ambiguity,\nparticularly when proximate structures resemble each other. More\u0002over, drawing a bounding box is efficient, especially in scenarios\ninvolving multi-object segmentation. We follow the network archi\u0002tecture in SAM7\n, including an image encoder, a prompt encoder, and a\nmask decoder (Fig. 2b). The image encoder25 maps the input image\ninto a high-dimensional image embedding space. The prompt encoder\ntransforms the user-drawn bounding boxes into feature representa\u0002tions via positional encoding26. Finally, the mask decoder fuses the\nimage embedding and prompt features using cross-attention27\n(Methods).\nQuantitative and qualitative analysis\nWe evaluated MedSAM through both internal validation and external\nvalidation. Specifically, we compared it to the SOTA segmentation\nfoundation model SAM7 as well as modality-wise specialist U-Net1 and\nDeepLabV3+24 models. Each specialized model was trained on images\nfrom the corresponding modality, resulting in 10 dedicated specialist\nmodels for each method. During inference, these specialist models\nwere used to segment the images from corresponding modalities,\nwhile SAM and MedSAM were employed for segmenting images across\nall modalities (Methods). The internal validation contained 86 seg\u0002mentation tasks (Supplementary Tables 5\u20138 and Fig. 2), and Fig. 3a\nshows the median dice similarity coefficient (DSC) score of these tasks\nfor the four methods. Overall, SAM obtained the lowest performance\non most segmentation tasks although it performed promisingly on\nsome RGB image segmentation tasks, such as polyp (DSC: 91.3%,\ninterquartile range (IQR): 81.2\u201395.1%) segmentation in endoscopy\nimages. This could be attributed to SAM\u2019s training on a variety of RGB\nimages, and the fact that many targets in these images are relatively\nstraightforward to segment due to their distinct appearances. The\nother three models outperformed SAM by a large margin and MedSAM\nhas a narrower distribution of DSC scores of the 86 interval validation\ntasks than the two groups of specialist models, reflecting the robust\u0002ness of MedSAM across different tasks. We further connected the DSC\nscores corresponding to the same task of the four models with the\npodium plot Fig. 3b, which is complementary to the box plot. In the\nupper part, each colored dot denotes the median DSC achieved with\nthe respective method on one task. Dots corresponding to identical\ntest cases are connected by a line. In the lower part, the frequency of\nachieved ranks for each method is presented with bar charts. It can be\nfound that MedSAM ranked in first place on most tasks, surpassing the\nperformance of the U-Net and DeepLabV3+ specialist models that have\na high frequency of ranks with second and third places, respectively, In\ncontrast, SAM ranked last place in almost all tasks. Figure 3c (and\nSupplementary Fig. 9) visualizes some randomly selected segmenta\u0002tion examples where MedSAM obtained a median DSC score, including\nliver tumor in CT images, brain tumor in MR images, breast tumor in\nultrasound images, and polyp in endoscopy images. SAM struggles\nwith targets of weak boundaries, which is prone to under or over\u0002segmentation errors. In contrast, MedSAM can accurately segment a\nwide range of targets across various imaging conditions, which\nachieves comparable of even better than the specialist U-Net and\nDeepLabV3+ models.\nThe external validation included 60 segmentation tasks, all of\nwhich either were from new datasets or involved unseen segmen\u0002tation targets (Supplementary Tables 9\u201311 and Figs. 10\u201312). Fig\u0002ure 4a, b show the task-wise median DSC score distribution and their\ncorrespondence of the 60 tasks, respectively. Although SAM con\u0002tinued exhibiting lower performance on most CT and MR segmen\u0002tation tasks, the specialist models no longer consistently\noutperformed SAM (e.g., right kidney segmentation in MR T1-\nweighted images: 90.1%, 85.3%, 86.4% for SAM, U-Net, and Dee\u0002pLabV3+, respectively). This indicates the limited generalization\nability of such specialist models on unseen targets. In contrast,\nMedSAM consistently delivers superior performance. For example,\nMedSAM obtained median DSC scores of 87.8% (IQR: 85.0-91.4%) on\nthe nasopharynx cancer segmentation task, demonstrating 52.3%,\n15.5%, and 22.7 improvements over SAM, the specialist U-Net, and\nDeepLabV3+, respectively. Significantly, MedSAM also achieved\nbetter performance in some unseen modalities (e.g., abdomen T1\nInphase and Outphase), surpassing SAM and the specialist models\nwith improvements by up to 10%. Figure 4c presents four randomly\nselected segmentation examples for qualitative evaluation, reveal\u0002ing that while all the methods have the ability to handle simple\nsegmentation targets, MedSAM performs better at segmenting\nchallenging targets with indistinguishable boundaries, such as cer\u0002vical cancer in MR images (more examples are presented in Sup\u0002plementary Fig. 13). Furthermore, we evaluated MedSAM on the\nmultiple myeloma plasma cell dataset, which represents a distinct\nmodality and task in contrast to all previously leveraged validation\ntasks. Although this task had never been seen during training,\na b\nImage\nencoder\nBounding box prompts\nMask decoder\nPrompt encoder\nInput Image Segmentation\nImage\nembedding\nFig. 2 | Overview of the modality distribution in the dataset and the network architecture. a The number of medical image-mask pairs in each modality. b MedSAM is a\npromptable segmentation method where users can use bounding boxes to specify the segmentation targets. Source data are provided as a Source Data file.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 3\nc\na b\nSAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM\nFig. 4 | Quantitative and qualitative evaluation results on the external\nvalidation set. a Performance distribution of 60 external validation tasks in terms\nof median dice similarity coefficient (DSC) score. The center line within the box\nrepresents the median value, with the bottom and top bounds of the box deli\u0002neating the 25th and 75th percentiles, respectively. Whiskers are chosen to show\nthe 1.5 of the interquartile range. Up-triangles denote the minima and down\u0002triangles denote the maxima. b Podium plots for visualizing the performance\ncorrespondence of 60 external validation tasks. Upper part: each colored dot\ndenotes the median DSC achieved with the respective method on one task. Dots\ncorresponding to identical tasks are connected by a line. Lower part: bar charts\nrepresent the frequency of achieved ranks for each method. MedSAM ranks in the\nfirst place on most tasks. c Visualized segmentation examples on the external\nvalidation set. The four examples are the lymph node, cervical cancer, fetal head,\nand polyp in CT, MR, ultrasound, and endoscopy images, respectively. Source data\nare provided as a Source Data file.\nSAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM\na\nc\nb\nFig. 3 | Quantitative and qualitative evaluation results on the internal\nvalidation set. a Performance distribution of 86 internal validation tasks in terms\nof median dice similarity coefficient (DSC) score. The center line within the box\nrepresents the median value, with the bottom and top bounds of the box deli\u0002neating the 25th and 75th percentiles, respectively. Whiskers are chosen to show\nthe 1.5 of the interquartile range. Up-triangles denote the minima and down\u0002triangles denote the maxima. b Podium plots for visualizing the performance\ncorrespondence of 86 internal validation tasks. Upper part: each colored dot\ndenotes the median DSC achieved with the respective method on one task. Dots\ncorresponding to identical tasks are connected by a line. Lower part: bar charts\nrepresent the frequency of achieved ranks for each method. MedSAM ranks in the\nfirst place on most tasks. c Visualized segmentation examples on the internal\nvalidation set. The four examples are liver cancer, brain cancer, breast cancer, and\npolyp in computed tomography (CT), (Magnetic Resonance Imaging) MRI, ultra\u0002sound, and endoscopy images, respectively. Blue: bounding box prompts; Yellow:\nsegmentation results. Magenta: expert annotations. Source data are provided as a\nSource Data file.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 4\nMedSAM still exhibited superior performance compared to the SAM\n(Supplementary Fig. 14), highlighting its remarkable generalization\nability.\nThe effect of training dataset size\nWe also investigated the effect of varying dataset sizes on MedSAM\u2019s\nperformance because the training dataset size has been proven to be\npivotal in model performance28. We additionally trained MedSAM on\ntwo different dataset sizes: 10,000 (10K) and 100,000 (100K) images\nand their performances were compared with the default MedSAM\nmodel. The 10K and 100K training images were uniformly sampled\nfrom the whole training set, to maintain data diversity. As shown in\n(Fig. 5a) (Supplementary Tables 12\u201314), the performance adhered to\nthe scaling rule, where increasing the number of training images sig\u0002nificantly improved the performance in both internal and external\nvalidation sets.\nMedSAM can improve the annotation efficiency\nFurthermore, we conducted a human annotation study to assess the\ntime cost of two pipelines (Methods). For the first pipeline, two human\nexperts manually annotate 3D adrenal tumors in a slice-by-slice way. For\nthe second pipeline, the experts first drew the long and short tumor axes\nwith the linear marker (initial marker) every 3-10 slices, which is a com\u0002mon practice in tumor response evaluation. Then, MedSAM was used to\nsegment the tumors based on these sparse linear annotations. Finally,\nthe expert manually revised the segmentation results until they were\nsatisfied. We quantitatively compared the annotation time cost between\nthe two pipelines (Fig. 5b). The results demonstrate that with the assis\u0002tance of MedSAM, the annotation time is substantially reduced by\n82.37% and 82.95% for the two experts, respectively.\nDiscussion\nWe introduce MedSAM, a deep learning-powered foundation model\ndesigned for the segmentation of a wide array of anatomical structures\nand lesions across diverse medical imaging modalities. MedSAM is\ntrained on a meticulously assembled large-scale dataset comprised of\nover one million medical image-mask pairs. Its promptable config\u0002uration strikes an optimal balance between automation and customi\u0002zation, rendering MedSAM a versatile tool for universal medical image\nsegmentation.\nThrough comprehensive evaluations encompassing both internal\nand external validation, MedSAM has demonstrated substantial cap\u0002abilities in segmenting a diverse array of targets and robust general\u0002ization abilities to manage new data and tasks. Its performance not\nonly significantly exceeds that of existing the state-of-the-art seg\u0002mentation foundation model, but also rivals or even surpasses spe\u0002cialist models. By providing precise delineation of anatomical\nstructures and pathological regions, MedSAM facilitates the compu\u0002tation of various quantitative measures that serve as biomarkers. For\ninstance, in the field of oncology, MedSAM could play a crucial role in\naccelerating the 3D tumor annotation process, enabling subsequent\ncalculations of tumor volume, which is a critical biomarker29 for\nassessing disease progression and response to treatment. Additionally,\nMedSAM provides a successful paradigm for adapting natural image\nfoundation models to new domains, which can be further extended to\nbiological image segmentation30, such as cell segmentation in light\nmicroscopy images31 and organelle segmentation in electron micro\u0002scopy images32.\nWhile MedSAM boasts strong capabilities, it does present certain\nlimitations. One such limitation is the modality imbalance in the\ntraining set, with CT, MRI, and endoscopy images dominating the\ndataset. This could potentially impact the model\u2019s performance on\nless-represented modalities, such as mammography. Another limita\u0002tion is its difficulty in the segmentation of vessel-like branching\nstructures because the bounding box prompt can be ambiguous in this\nsetting. For example, arteries and veins share the same bounding box\nin eye fundus images. However, these limitations do not diminish\nMedSAM\u2019s utility. Since MedSAM has learned rich and representative\nmedical image features from the large-scale training set, it can be fine\u0002tuned to effectively segment new tasks from less-represented mod\u0002alities or intricate structures like vessels.\nIn conclusion, this study highlights the feasibility of constructing a\nsingle foundation model capable of managing a multitude of seg\u0002mentation tasks, thereby eliminating the need for task-specific models.\nMedSAM, as the inaugural foundation model in medical image seg\u0002mentation, holds great potential to accelerate the advancement of new\ndiagnostic and therapeutic tools, and ultimately contribute to\nimproved patient care33.\nMethods\nDataset curation and pre-processing\nWe curated a comprehensive dataset by collating images from publicly\navailable medical image segmentation datasets, which were obtained\nfrom various sources across the internet, including the Cancer Imaging\nArchive (TCIA)34, Kaggle, Grand-Challenge, Scientific Data, CodaLab,\nand segmentation challenges in the Medical Image Computing and\nComputer Assisted Intervention Society (MICCAI). All the datasets\nprovided segmentation annotations by human experts, which have\nbeen widely used in existing literature (Supplementary Table 1\u20134). We\nincorporated these annotations directly for both model development\nand validation.\nThe original 3D datasets consisted of computed tomography (CT)\nand magnetic resonance (MR) images in DICOM, nrrd, or mhd formats.\nTo ensure uniformity and compatibility with developing medical\nimage deep learning models, we converted the images to the widely\nused NifTI format. Additionally, grayscale images (such as X-Ray and\nUltrasound) as well as RGB images (including endoscopy, dermoscopy,\nfundus, and pathology images), were converted to the png format.\nFig. 5 | The effect of training dataset size and a user study of tumor annotation\nefficiency. a Scaling up the training image size to one million can significantly\nimprove the model performance on both internal and external validation sets.\nb MedSAM can be used to substantially reduce the annotation time cost. Source\ndata are provided as a Source Data file.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 5\nSeveral exclusive criteria are applied to improve the dataset quality\nand consistency, including incomplete images and segmentation tar\u0002gets with branching structures, inaccurate annotations, and tiny\nvolumes. Notably, image intensities varied significantly across differ\u0002ent modalities. For instance, CT images had intensity values ranging\nfrom -2000 to 2000, while MR images exhibited a range of 0 to 3000.\nIn endoscopy and ultrasound images, intensity values typically span\u0002ned from 0 to 255. To facilitate stable training, we performed intensity\nnormalization across all images, ensuring they shared the same\nintensity range.\nFor CT images, we initially normalized the Hounsfield units using\ntypical window width and level values. The employed window width\nand level values for soft tissues, lung, and brain are (W:400, L:40),\n(W:1500, L:-160), and (W:80, L:40), respectively. Subsequently, the\nintensity values were rescaled to the range of [0, 255]. For MR, X-ray,\nultrasound, mammography, and optical coherence tomography (OCT)\nimages, we clipped the intensity values to the range between the 0.5th\nand 99.5th percentiles before rescaling them to the range of [0, 255].\nRegarding RGB images (e.g., endoscopy, dermoscopy, fundus, and\npathology images), if they were already within the expected intensity\nrange of [0, 255], their intensities remained unchanged. However, if\nthey fell outside this range, we utilized max-min normalization to\nrescale the intensity values to [0, 255]. Finally, to meet the model\u2019s\ninput requirements, all images were resized to a uniform size of\n1024 \u00d7 1024 \u00d7 3. In the case of whole-slide pathology images, patches\nwere extracted using a sliding window approach without overlaps. The\npatches located on boundaries were padded to this size with 0. As for\n3D CT and MR images, each 2D slice was resized to 1024 \u00d7 1024, and\nthe channel was repeated three times to maintain consistency. The\nremaining 2D images were directly resized to 1024 \u00d7 1024 \u00d7 3. Bi-cubic\ninterpolation was used for resizing images, while nearest-neighbor\ninterpolation was applied for resizing masks to preserve their precise\nboundaries and avoid introducing unwanted artifacts. These standar\u0002dization procedures ensured uniformity and compatibility across all\nimages and facilitated seamless integration into the subsequent stages\nof the model training and evaluation pipeline.\nNetwork architecture\nThe network utilized in this study was built on transformer\narchitecture27, which has demonstrated remarkable effectiveness in\nvarious domains such as natural language processing and image\nrecognition tasks25. Specifically, the network incorporated a vision\ntransformer (ViT)-based image encoder responsible for extracting\nimage features, a prompt encoder for integrating user interactions\n(bounding boxes), and a mask decoder that generated segmentation\nresults and confidence scores using the image embedding, prompt\nembedding, and output token.\nTo strike a balance between segmentation performance and com\u0002putational efficiency, we employed the base ViT model as the image\nencoder since extensive evaluation indicated that larger ViT models,\nsuch as ViT Large and ViT Huge, offered only marginal improvements in\naccuracy7 while significantly increasing computational demands. Speci\u0002fically, the base ViT model consists of 12 transformer layers27, with each\nblock comprising a multi-head self-attention block and a Multilayer\nPerceptron (MLP) block incorporating layer normalization35. Pre-training\nwas performed using masked auto-encoder modeling36, followed by\nfully supervised training on the SAM dataset7\n. The input image\n(1024 \u00d7 1024 \u00d7 3) was reshaped into a sequence of flattened 2D patches\nwith the size 16 \u00d7 16 \u00d7 3, yielding a feature size in image embedding of\n64 \u00d7 64 after passing through the image encoder, which is 16 \u00d7 down\u0002scaled. The prompt encoders mapped the corner point of the bounding\nbox prompt to 256-dimensional vectorial embeddings26. In particular,\neach bounding box was represented by an embedding pair of the top\u0002left corner point and the bottom-right corner point. To facilitate real\u0002time user interactions once the image embedding had been computed, a\nlightweight mask decoder architecture was employed. It consists of two\ntransformer layers27 for fusing the image embedding and prompt\nencoding, and two transposed convolutional layers to enhance the\nembedding resolution to 256 \u00d7 256. Subsequently, the embedding\nunderwent sigmoid activation, followed by bi-linear interpolations to\nmatch the input size.\nTraining protocol and experimental setting\nDuring data pre-processing, we obtained 1,570,263 medical image\u0002mask pairs for model development and validation. For internal vali\u0002dation, we randomly split the dataset into 80%, 10%, and 10% as\ntraining, tuning, and validation, respectively. Specifically, for mod\u0002alities where within-scan continuity exists, such as CT and MRI, and\nmodalities where continuity exists between consecutive frames, we\nperformed the data splitting at the 3D scan and the video level\nrespectively, by which any potential data leak was prevented. For\npathology images, recognizing the significance of slide-level cohe\u0002siveness, we first separated the whole-slide images into distinct slide\u0002based sets. Then, each slide was divided into small patches with a fixed\nsize of 1024 \u00d7 1024. This setup allowed us to monitor the model\u2019s\nperformance on the tuning set and adjust its parameters during\ntraining to prevent overfitting. For the external validation, all datasets\nwere held out and did not appear during model training. These data\u0002sets provide a stringent test of the model\u2019s generalization ability, as\nthey represent new patients, imaging conditions, and potentially new\nsegmentation tasks that the model has not encountered before. By\nevaluating the performance of MedSAM on these unseen datasets, we\ncan gain a realistic understanding of how MedSAM is likely to perform\nin real-world clinical settings, where it will need to handle a wide range\nof variability and unpredictability in the data. The training and vali\u0002dation are independent.\nThe model was initialized with the pre-trained SAM model with\nthe ViT-Base model. We fixed the prompt encoder since it can already\nencode the bounding box prompt. All the trainable parameters in the\nimage encoder and mask decoder were updated during training.\nSpecifically, the number of trainable parameters for the image encoder\nand mask decoder are 89,670,912 and 4,058,340, respectively. The\nbounding box prompt was simulated from the expert annotations with\na random perturbation of 0-20 pixels. The loss function is the\nunweighted sum between dice loss and cross-entropy loss, which has\nbeen proven to be robust in various segmentation tasks1\n. The network\nwas optimized by AdamW37 optimizer (\u03b21 = 0.9, \u03b22 = 0.999) with an\ninitial learning rate of 1e-4 and a weight decay of 0.01. The global batch\nsize was 160 and data augmentation was not used. The model was\ntrained on 20 A100 (80G) GPUs with 150 epochs and the last check\u0002point was selected as the final model.\nFurthermore, to thoroughly evaluate the performance of Med\u0002SAM, we conducted comparative analyses against both the state-of\u0002the-art segmentation foundation model SAM7 and specialist models\n(i.e., U-Net1 and DeepLabV3+24). The training images contained 10\nmodalities: CT, MR, chest X-ray (CXR), dermoscopy, endoscopy,\nultrasound, mammography, OCT, and pathology, and we trained the\nU-Net and DeepLabV3+ specialist models for each modality. There\nwere 20 specialist models in total and the number of corresponding\ntraining images was presented in Supplementary Table 5. We\nemployed the nnU-Net to conduct all U-Net experiments, which can\nautomatically configure the network architecture based on the dataset\nproperties. In order to incorporate the bounding box prompt into the\nmodel, we transformed the bounding box into a binary mask and\nconcatenated it with the image as the model input. This function was\noriginally supported by nnU-Net in the cascaded pipeline, which has\ndemonstrated increased performance in many segmentation tasks by\nusing the binary mask as an additional channel to specify the target\nlocation. The training settings followed the default configurations of\n2D nnU-Net. Each model was trained on one A100 GPU with 1000\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 6\nepochs and the last checkpoint was used as the final model. The\nDeepLabV3+ specialist models used ResNet5038 as the encoder. Similar\nto ref. 3, the input images were resized to 224 \u00d7 224 \u00d7 3. The bounding\nbox was transformed into a binary mask as an additional input channel\nto provide the object location prompt. Segmentation Models Pytorch\n(0.3.3)39 was used to perform training and inference for all the\nmodality-wise specialist DeepLabV3 + models. Each modality-wise\nmodel was trained on one A100 GPU with 500 epochs and the last\ncheckpoint was used as the final model. During the inference phase,\nSAM and MedSAM were used to perform segmentation across all\nmodalities with a single model. In contrast, the U-Net and DeepLabV3+\nspecialist models were used to individually segment the respective\ncorresponding modalities.\nA task-specific segmentation model might outperform a modality\u0002based one for certain applications. Since U-Net obtained better per\u0002formance than DeepLabV3+ on most tasks, we further conducted a\ncomparison study by training task-specific U-Net models on four\nrepresentative tasks, including liver cancer segmentation in CT scans,\nabdominal organ segmentation in MR scans, nerve cancer segmenta\u0002tion in ultrasound, and polyp segmentation in endoscopy images. The\nexperiments included both internal validation and external validation.\nFor internal validation, we adhered to the default data splits, using\nthem to train the task-specific U-Net models and then evaluate their\nperformance on the corresponding validation set. For external vali\u0002dation, the trained U-Net models were evaluated on new datasets from\nthe same modality or segmentation targets. In all these experiments,\nMedSAM was directly applied to the validation sets without additional\nfine-tuning. As shown in Supplementary Fig. 15, while task-specific U\u0002Net models often achieved great results on internal validation sets,\ntheir performance diminished significantly for external sets. In con\u0002trast, MedSAM maintained consistent performance across both inter\u0002nal and external validation sets. This underscores MedSAM\u2019s superior\ngeneralization ability, making it a versatile tool in a variety of medical\nimage segmentation tasks.\nLoss function\nWe used the unweighted sum between cross-entropy loss and dice\nloss40 as the final loss function since it has been proven to be robust\nacross different medical image segmentation tasks41. Specifically, let\nS, G denote the segmentation result and ground truth, respectively.\nsi, gi denotes the predicted segmentation and ground truth of voxel i,\nrespectively. N is the number of voxels in the image I. Binary cross\u0002entropy loss is defined by\nLBCE = \u0001 1\nN\nXN\ni = 1\ngi log si + \u00f01 \u0001 gi\u00de log\u00f01 \u0001 si\u00de \u0002 \u0003, \u00f01\u00de\nand dice loss is defined by\nLDice = 1 \u0001 2\nPN\ni = 1 gisi PN\ni = 1 \u00f0gi\u00de\n2 + PN\ni = 1 \u00f0si\u00de\n2 : \u00f02\u00de\nThe final loss L is defined by\nL = LBCE + LDice: \u00f03\u00de\nHuman annotation study\nThe objective of the human annotation study was to quantitatively\nevaluate how MedSAM can reduce the annotation time cost. Specifi\u0002cally, we used the recent adrenocortical carcinoma CT dataset34,42,43,\nwhere the segmentation target, adrenal tumor, was neither part of the\ntraining nor of the existing validation sets. We randomly sampled 10\ncases, comprising a total of 733 tumor slices requiring annotations.\nTwo human experts participated in this study, both of whom are\nexperienced radiologists with 8 and 6 years of clinical practice in\nabdominal diseases, respectively. Each expert generated two groups of\nannotations, one with the assistance of MedSAM and one without.\nIn the first group, the experts manually annotated the 3D adrenal\ntumor in a slice-by-slice manner. Annotations by the two experts were\nconducted independently, with no collaborative discussions, and the\ntime taken for each case was recorded. In the second group, annota\u0002tions were generated after one week of cooling period. The experts\nindependently drew the long and short tumor axes as initial markers,\nwhich is a common practice in tumor response evaluation. This pro\u0002cess was executed every 3-10 slices from the top slice to the bottom\nslice of the tumor. Then, we applied MedSAM to segment the tumors\nbased on these sparse linear annotations, including three steps.\n\u2022 Step 1. For each annotated slice, a rectangle binary mask was\ngenerated based on the linear label that can completely cover\nthe linear label. \u2022 Step 2. For the unlabeled slices, the rectangle binary masks were\ncreated through interpolation of the surrounding labeled slices. \u2022 Step 3. We transformed the binary masks into bounding boxes\nand then fed them along with the images into MedSAM to gen\u0002erate segmentation results.\nAll these steps were conducted in an automatic way and the model\nrunning time was recorded for each case. Finally, human experts\nmanually refined the segmentation results until they met their satis\u0002faction. To summarize, the time cost of the second group of annota\u0002tions contained three parts: initial markers, MedSAM inference, and\nrefinement. All the manual annotation processes were based on ITK\u0002SNAP44, an open-source software designed for medical image visuali\u0002zation and annotation.\nEvaluation metrics\nWe followed the recommendations in Metrics Reloaded45 and used the\ndice similarity coefficient and normalized surface distance (NSD) to\nquantitatively evaluate the segmentation results. DSC is a region-based\nsegmentation metric, aiming to evaluate the region overlap between\nexpert annotation masks and segmentation results, which is defined by\nDSC\u00f0G, S\u00de = 2jG \\ Sj\njGj + jSj\n,\nNSD46 is a boundary-based metric, aiming to evaluate the boundary\nconsensus between expert annotation masks and segmentation results\nat a given tolerance, which is defined by\nNSD\u00f0G, S\u00de = j\u2202G \\ B\u00f0\u03c4\u00de\n\u2202S j + j\u2202S \\ B\u00f0\u03c4\u00de\u2202Gj\nj\u2202Gj + j\u2202Sj ,\nwhere B\u00f0\u03c4\u00de\n\u2202G = fx 2 R3 j 9x~ 2 \u2202G, jjx \u0001 x~jj \u2264 \u03c4g, B\u00f0\u03c4\u00de\u2202S = fx 2 R3 j 9x~ 2 \u2202S, jjx \u0001\nx~jj \u2264 \u03c4g denote the border region of the expert annotation mask and\nthe segmentation surface at tolerance \u03c4, respectively. In this paper, we\nset the tolerance \u03c4 as 2.\nStatistical analysis\nTo statistically analyze and compare the performance of the afore\u0002mentioned four methods (MedSAM, SAM, U-Net, and DeepLabV3+\nspecialist models), we employed the Wilcoxon signed-rank test. This\nnon-parametric test is well-suited for comparing paired samples and is\nparticularly useful when the data does not meet the assumptions of\nnormal distribution. This analysis allowed us to determine if any\nmethod demonstrated statistically superior segmentation perfor\u0002mance compared to the others, providing valuable insights into the\ncomparative effectiveness of the evaluated methods. The Wilcoxon\nsigned-rank test results are marked on the DSC and NSD score tables\n(Supplementary Table 6\u201311).\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 7\nSoftware utilized\nAll code was implemented in Python (3.10) using Pytorch (2.0) as the\nbase deep learning framework. We also used several Python packages\nfor data analysis and results visualization, including connected\u0002components-3d (3.10.3), SimpleITK (2.2.1), nibabel (5.1.0), torchvision\n(0.15.2), numpy (1.24.3), scikit-image (0.20.0), scipy (1.10.1), and pan\u0002das (2.0.2), matplotlib (3.7.1), opencv-python (4.8.0), ChallengeR\n(1.0.5), and plotly (5.15.0). Biorender was used to create Fig. 1.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe training and validating datasets used in this study are available in\nthe public domain and can be downloaded via the links provided in\nSupplementary Tables 16 and 17. Source data are provided with this\npaper in the Source Data file. We confirmed that All the image datasets\nin this study are publicly accessible and permitted for research pur\u0002poses. Source data are provided in this paper.\nCode availability\nThe training script, inference script, and trained model have been\npublicly available at https://github.com/bowang-lab/MedSAM. A per\u0002manent version is released on Zenodo47.\nReferences\n1. Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. & Maier-Hein, K. H.\nnnU-Net: a self-configuring method for deep learning-based bio\u0002medical image segmentation. Nat. Method. 18, 203\u2013211 (2021).\n2. De Fauw, J. Clinically applicable deep learning for diagnosis and\nreferral in retinal disease. Nat. Med. 24, 1342\u20131350 (2018).\n3. Ouyang, D. Video-based AI for beat-to-beat assessment of cardiac\nfunction. Nature 580, 252\u2013256 (2020).\n4. Wang, G. Deepigeos: a deep interactive geodesic framework for\nmedical image segmentation. In IEEE Transactions on Pattern Ana\u0002lysis and Machine Intelligence 41, 1559\u20131572 (IEEE, 2018).\n5. Antonelli, M. The medical segmentation decathlon. Nat. Commun.\n13, 4128 (2022).\n6. Minaee, S. Image segmentation using deep learning: A survey. In\nIEEE Transactions on Pattern Analysis and Machine Intelligence 44,\n3523\u20133542 (IEEE, 2021).\n7. Kirillov, A. et al. Segment anything. In IEEE International Conference\non Computer Vision. 4015\u20134026 (IEEE, 2023).\n8. Zou, X. et al. Segment everything everywhere all at once. In\nAdvances in Neural Information Processing Systems (MIT\nPress, 2023).\n9. Wang, G. Interactive medical image segmentation using deep\nlearning with image-specific fine tuning. In IEEE Transactions on\nMedical Imaging 37, 1562\u20131573 (IEEE, 2018).\n10. Zhou, T. Volumetric memory network for interactive medical image\nsegmentation. Med. Image Anal. 83, 102599 (2023).\n11. Luo, X. Mideepseg: Minimally interactive segmentation of unseen\nobjects from medical images using deep learning. Med. Image Anal.\n72, 102102 (2021).\n12. Deng, R. et al. Segment anything model (SAM) for digital pathology:\nassess zero-shot segmentation on whole slide imaging. Preprint at\nhttps://arxiv.org/abs/2304.04155 (2023).\n13. Hu, C., Li, X. When SAM meets medical images: an investigation of\nsegment anything model (SAM) on multi-phase liver tumor seg\u0002mentation. Preprint at https://arxiv.org/abs/2304.08506\n(2023).\n14. He, S., Bao, R., Li, J., Grant, P.E., Ou, Y. Accuracy of segment\u0002anything model (SAM) in medical image segmentation tasks. Pre\u0002print at https://doi.org/10.48550/arXiv.2304.09324 (2023).\n15. Roy, S. et al. SAM.MD: zero-shot medical image segmentation\ncapabilities of the segment anything model. Preprint at https://\narxiv.org/abs/2304.05396 (2023).\n16. Zhou, T., Zhang, Y., Zhou, Y., Wu, Y. & Gong, C. Can SAM segment\npolyps? Preprint at https://arxiv.org/abs/2304.07583 (2023).\n17. Mohapatra, S., Gosai, A., Schlaug, G. Sam vs bet: a comparative\nstudy for brain extraction and segmentation of magnetic resonance\nimages using deep learning. Preprint at https://arxiv.org/abs/2304.\n04738 (2023).\n18. Chen, J., Bai, X. Learning to\" segment anything\" in thermal infrared\nimages through knowledge distillation with a large scale dataset\nSATIR. Preprint at https://arxiv.org/abs/2304.07969 (2023).\n19. Tang, L., Xiao, H., Li, B. Can SAM segment anything? when SAM\nmeets camouflaged object detection. Preprint at https://arxiv.org/\nabs/2304.04709 (2023).\n20. Ji, G.-P. et al. SAM struggles in concealed scenes\u2013empirical study\non\u201d segment anything\u201d. Science China Information Sciences. 66,\n226101 (2023).\n21. Ji, W., Li, J., Bi, Q., Li, W., Cheng, L. Segment anything is not always\nperfect: an investigation of SAM on different real-world applica\u0002tions. Preprint at https://arxiv.org/abs/2304.05750 (2023).\n22. Mazurowski, M. A. Segment anything model for medical image\nanalysis: an experimental study. Med. Image Anal. 89,\n102918 (2023).\n23. Huang, Y. et al. Segment anything model for medical images? Med.\nImage Anal. 92, 103061 (2024).\n24. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H. Encoder\u0002decoder with atrous separable convolution for semantic image\nsegmentation. In Proc. European Conference on Computer Vision.\n801\u2013818 (IEEE, 2018).\n25. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers\nfor image recognition at scale. In: International Conference on\nLearning Representations (OpenReview.net, 2020).\n26. Tancik, M. Fourier features let networks learn high frequency\nfunctions in low-dimensional domains. In Advances in Neural\nInformation Processing Systems 33, 7537\u20137547 (Curran Associates,\nInc., 2020).\n27. Vaswani, A. et al. Attention is all you need. In Advances in Neural\nInformation Processing Systems, Vol. 30 (Curran Associates,\nInc., 2017).\n28. He, B. Blinded, randomized trial of sonographer versus AI cardiac\nfunction assessment. Nature 616, 520\u2013524 (2023).\n29. Eisenhauer, E. A. New response evaluation criteria in solid tumours:\nrevised recist guideline (version 1.1). Eur. J. Cancer 45,\n228\u2013247 (2009).\n30. Ma, J. & Wang, B. Towards foundation models of biological image\nsegmentation. Nat. Method. 20, 953\u2013955 (2023).\n31. Ma, J. et al. The multi-modality cell segmentation challenge:\ntowards universal solutions. Preprint at https://arxiv.org/abs/2308.\n05864 (2023).\n32. Xie, R., Pang, K., Bader, G.D., Wang, B. Maester: masked auto\u0002encoder guided segmentation at pixel resolution for accurate, self\u0002supervised subcellular structure recognition. In IEEE Conference on\nComputer Vision and Pattern Recognition. 3292\u20133301 (IEEE, 2023).\n33. Bera, K., Braman, N., Gupta, A., Velcheti, V. & Madabhushi, A. Pre\u0002dicting cancer outcomes with radiomics and artificial intelligence in\nradiology. Nat. Rev. Clin. Oncol. 19, 132\u2013146 (2022).\n34. Clark, K. The cancer imaging archive (TCIA): maintaining and\noperating a public information repository. J. Digit. Imaging 26,\n1045\u20131057 (2013).\n35. Ba, J.L., Kiros, J.R., Hinton, G.E. Layer normalization. Preprint at\nhttps://arxiv.org/abs/1607.06450 (2016).\n36. He, K. et al. Masked autoencoders are scalable vision learners. In\nProc. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 16000\u201316009 (IEEE, 2022).\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 8\n37. Loshchilov, I., Hutter, F. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations (Open\u0002Review.net, 2019).\n38. He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image\nrecognition. In Proc. IEEE Conference on Computer Vision and Pat\u0002tern Recognition. 770\u2013778 (IEEE, 2016).\n39. Iakubovskii, P. Segmentation models pytorch. GitHub https://\ngithub.com/qubvel/segmentation_models.pytorch (2019).\n40. Milletari, F., Navab, N., Ahmadi, S.-A. V-net: Fully convolutional\nneural networks for volumetric medical image segmentation. In\nInternational Conference on 3D Vision (3DV). 565\u2013571\n(IEEE, 2016).\n41. Ma, J. Loss odyssey in medical image segmentation. Med. Image\nAnal. 71, 102035 (2021).\n42. Ahmed, A. Radiomic mapping model for prediction of Ki-67\nexpression in adrenocortical carcinoma. Clin. Radiol. 75,\n479\u201317 (2020).\n43. Moawad, A.W. et al. Voxel-level segmentation of pathologically\u0002proven Adrenocortical carcinoma with Ki-67 expression (Adrenal\u0002ACC-Ki67-Seg) [data set]. https://doi.org/10.7937/1FPG\u0002VM46 (2023).\n44. Yushkevich, P.A., Gao, Y., Gerig, G. Itk-snap: an interactive tool for\nsemi-automatic segmentation of multi-modality biomedical ima\u0002ges. In International Conference of the IEEE Engineering in Medicine\nand Biology Society (EMBC). 3342\u20133345 (IEEE, 2016).\n45. Maier-Hein, L. et al. Metrics reloaded: Pitfalls and recommendations\nfor image analysis validation. Preprint at https://arxiv.org/abs/\n2206.01653 (2022).\n46. DeepMind surface-distance. https://github.com/google\u0002deepmind/surface-distance (2018).\n47. Ma, J. bowang-lab/MedSAM: v1.0.0. https://doi.org/10.5281/\nzenodo.10452777 (2023).\nAcknowledgements\nThis work was supported by the Natural Sciences and Engineering\nResearch Council of Canada (NSERC, RGPIN-2020-06189 and DGECR\u00022020-00294) and CIFAR AI Chair programs. The authors of this paper\nhighly appreciate all the data owners for providing public medical\nimages to the community. We also thank Meta AI for making the source\ncode of segment anything publicly available to the community. This\nresearch was enabled in part by computing resources provided by the\nDigital Research Alliance of Canada.\nAuthor contributions\nConceived and designed the experiments: J.M. Y.H., C.Y., B.W. Per\u0002formed the experiments: J.M. Y.H., F.L., L.H., C.Y. Analyzed the data: J.M.\nY.H., F.L., L.H., C.Y., B.W. Wrote the paper: J.M. Y.H., F.L., L.H., C.Y., B.W.\nAll authors have read and agreed to the published version of the\nmanuscript.\nCompeting interests\nThe authors declare no competing interests\nAdditional information\nSupplementary information The online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-44824-z.\nCorrespondence and requests for materials should be addressed to Bo\nWang.\nPeer review information Nature Communications thanks David Ouyang,\nand the other, anonymous, reviewer(s) for their contribution to the peer\nreview of this work. A peer review file is available.\nReprints and permissions information is available at\nhttp://www.nature.com/reprints\nPublisher\u2019s note Springer Nature remains neutral with regard to jur\u0002isdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article\u2019s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article\u2019s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\n\u00a9 The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 9",
    "title": "Segment anything in medical images",
    "publication_date": "2024-01-22",
    "cited_by_count": 206,
    "topics": "Radiomics in Medical Imaging Analysis, Deep Learning in Medical Image Analysis, Applications of Deep Learning in Medical Imaging",
    "keywords": "Medical Imaging, Medical Image Analysis, Modalities, Robustness (evolution), Modality (human\u2013computer interaction), Cancer Imaging, Whole Slide Imaging, Image-Based Diagnosis",
    "concepts": "Generalizability theory, Computer science, Segmentation, Modalities, Robustness (evolution), Artificial intelligence, Modality (human\u2013computer interaction), Image segmentation, Medical imaging, Personalization, Computer vision, Bridging (networking), Machine learning, Computer network, Mathematics, Social science, Biochemistry, Gene, Statistics, World Wide Web, Sociology, Chemistry",
    "text_type": "full_text",
    "openalex_rank": 7,
    "num_tokens": 10761,
    "best_oa_location_pdf_url": "https://www.nature.com/articles/s41467-024-44824-z.pdf",
    "pdf_urls_by_priority": [
      "https://www.nature.com/articles/s41467-024-44824-z.pdf"
    ],
    "successful_pdf_url": "https://www.nature.com/articles/s41467-024-44824-z.pdf",
    "url": "https://www.nature.com/articles/s41467-024-44824-z.pdf",
    "snippets": {
      "s1": "Article https://doi.org/10.1038/s41467-024-44824-z Segment anything in medical images Jun Ma1,2,3, Yuting He4, Feifei Li 1, Lin Han5, Chenyu You 6 & Bo Wang 1,2,3,7,8 Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans. Segmentation is a fundamental task in medical imaging analysis, which involves identifying and delineating regions of interest (ROI) in various medical images,..",
      "s2": "..such as organs, lesions, and tissues1 . Accurate segmentation is essential for many clinical applications, including disease diagnosis, treatment planning, and monitoring of disease progression2,3 . Manual segmentation has long been the gold standard for delineating anatomical structures and pathological regions, but this process is time-consuming, labor-intensive, and often requires a high degree of expertise. Semi- or fully automatic segmentation methods can significantly reduce the time and labor required, increase consistency, and enable the analysis of large-scale datasets4 . Deep learning-based models have shown great promise in medical image segmentation due to their ability to learn intricate image features and deliver accurate segmentation results across a diverse range of tasks, from segmenting specific anatomical structures to identifying pathological regions5 . However, a significant limitation of many current medical image segmentation models is their task-specific nature. These models are typically designed and trained for a specific segmentation task, and their performance can degrade significantly when applied to new tasks or different types of imaging data6 . This lack of generality poses a substantial obstacle to the wider application of these models in clinical practice. In contrast, recent advances in the field of natural image segmentation have witnessed the emergence of segmentation foundation models, such as segment anything model (SAM)7 and Segment Everything Everywhere with Multi-modal..",
      "s3": "..prompts all at once8 , showcasing remarkable versatility and performance across various segmentation tasks. There is a growing demand for universal models in medical image segmentation: models that can be trained once and then applied to a wide range of segmentation tasks. Such models would not only exhibit heightened versatility in terms of model capacity but also potentially lead to more consistent results across different tasks. However, the applicability of the segmentation foundation models (e.g., SAM7 ) to medical image segmentation remains limited due to the significant differences between natural images and medical images. Essentially, SAM is a promptable segmentation method that requires points or bounding boxes to specify the segmentation targets. This resembles conventional interactive segmentation methods4,9\u201311 but SAM has better generalization ability, while existing deep learning-based interactive segmentation methods focus mainly on limited tasks and image modalities. Many studies have applied the out-of-the-box SAM models to typical medical image segmentation tasks12\u201317 and other challenging scenarios18\u201321. For example, the concurrent studies22,23 conducted a Received: 24 October 2023 Accepted: 5 January 2024 Check for updates 1 Peter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada. 2Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, ON, Canada. 3..",
      "s4": "..Vector Institute, Toronto, ON, Canada. 4Department of Computer Science, Western University, London, ON, Canada. 5Tandon School of Engineering, New York University, New York, NY, USA. 6 Department of Electrical Engineering, Yale University, New Haven, CT, USA. 7Department of Computer Science, University of Toronto, Toronto, ON, Canada. 8 UHN AI Hub, Toronto, ON, Canada. e-mail: bowang@vectorinstitute.ai Nature Communications | (2024) 15:654 1 1234567890():,;1234567890():,; comprehensive assessment of SAM across a diverse array of medical images, underscoring that SAM achieved satisfactory segmentation outcomes primarily on targets characterized by distinct boundaries. However, the model exhibited substantial limitations in segmenting typical medical targets with weak boundaries or low contrast. In congruence with these observations, we further introduce MedSAM, a refined foundation model that significantly enhances the segmentation performance of SAM on medical images. MedSAM accomplishes this by fine-tuning SAM on an unprecedented dataset with more than one million medical image-mask pairs. We thoroughly evaluate MedSAM through comprehensive experiments on 86 internal validation tasks and 60 external validation tasks, spanning a variety of anatomical structures, pathological conditions, and medical imaging modalities..",
      "s5": "... Experimental results demonstrate that MedSAM consistently outperforms the state-of-theart (SOTA) segmentation foundation model7 , while achieving performance on par with, or even surpassing specialist models1,24 that were trained on the images from the same modality. These results highlight the potential of MedSAM as a new paradigm for versatile medical image segmentation. Results MedSAM: a foundation model for promptable medical image segmentation MedSAM aims to fulfill the role of a foundation model for universal medical image segmentation. A crucial aspect of constructing such a model is the capacity to accommodate a wide range of variations in imaging conditions, anatomical structures, and pathological conditions. To address this challenge, we curated a diverse and large-scale medical image segmentation dataset with 1,570,263 medical imagemask pairs, covering 10 imaging modalities, over 30 cancer types, and a multitude of imaging protocols (Fig. 1 and Supplementary Tables 1\u20134). This large-scale dataset allows MedSAM to learn a rich representation of medical images, capturing a broad spectrum of anatomies and lesions across different modalities. Figure 2a provides an overview of the distribution of images across different medical imaging modalities in the dataset, ranked by their total numbers. It is evident that computed tomography (CT), magnetic resonance imaging (MRI..",
      "s6": "..), and endoscopy are the dominant modalities, reflecting their ubiquity in clinical practice. CT and MRI images provide detailed cross-sectional views of 3D body structures, making them indispensable for non-invasive diagnostic imaging. Endoscopy, albeit more invasive, enables direct visual inspection of organ interiors, proving invaluable for diagnosing gastrointestinal and urological conditions. Despite the prevalence of these modalities, others such as ultrasound, pathology, fundus, dermoscopy, mammography, and optical coherence tomography (OCT) also hold significant roles in clinical practice. The diversity of these modalities and their corresponding segmentation targets underscores the necessity for universal and effective segmentation models capable of handling the unique characteristics associated with each modality. Another critical consideration is the selection of the appropriate segmentation prompt and network architecture. While the concept of fully automatic segmentation foundation models is enticing, it is fraught with challenges that make it impractical. One of the primary challenges is the variability inherent in segmentation tasks. For example, given a liver cancer CT image, the segmentation task can vary depending on the specific clinical scenario. One clinician might be Fig. 1 | MedSAM is trained on a large-scale dataset that can handle diverse segmentation tasks. The dataset covers a variety of anatomical structures, pathological conditions, and medical imaging modalities. The magenta contours and mask overlays denote the expert..",
      "s7": "..annotations and MedSAM segmentation results, respectively. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 2 interested in segmenting the liver tumor, while another might need to segment the entire liver and surrounding organs. Additionally, the variability in imaging modalities presents another challenge. Modalities such as CT and MR generate 3D images, whereas others like X-ray and ultrasound yield 2D images. These variabilities in task definition and imaging modalities complicate the design of a fully automatic model capable of accurately anticipating and addressing the diverse requirements of different users. Considering these challenges, we argue that a more practical approach is to develop a promptable 2D segmentation model. The model can be easily adapted to specific tasks based on user-provided prompts, offering enhanced flexibility and adaptability. It is also able to handle both 2D and 3D images by processing 3D images as a series of 2D slices. Typical user prompts include points and bounding boxes and we show some segmentation examples with the different prompts in Supplementary Fig. 1. It can be found that bounding boxes provide a more unambiguous spatial context for the region of interest, enabling the algorithm to more precisely discern the target area. This stands in..",
      "s8": "..contrast to point-based prompts, which can introduce ambiguity, particularly when proximate structures resemble each other. Moreover, drawing a bounding box is efficient, especially in scenarios involving multi-object segmentation. We follow the network architecture in SAM7 , including an image encoder, a prompt encoder, and a mask decoder (Fig. 2b). The image encoder25 maps the input image into a high-dimensional image embedding space. The prompt encoder transforms the user-drawn bounding boxes into feature representations via positional encoding26. Finally, the mask decoder fuses the image embedding and prompt features using cross-attention27 (Methods). Quantitative and qualitative analysis We evaluated MedSAM through both internal validation and external validation. Specifically, we compared it to the SOTA segmentation foundation model SAM7 as well as modality-wise specialist U-Net1 and DeepLabV3+24 models. Each specialized model was trained on images from the corresponding modality, resulting in 10 dedicated specialist models for each method. During inference, these specialist models were used to segment the images from corresponding modalities, while SAM and MedSAM were employed for segmenting images across all modalities (Methods). The internal validation contained 86 segmentation tasks (Supplementary Tables 5\u20138 and Fig. 2), and Fig. 3a shows..",
      "s9": "..the median dice similarity coefficient (DSC) score of these tasks for the four methods. Overall, SAM obtained the lowest performance on most segmentation tasks although it performed promisingly on some RGB image segmentation tasks, such as polyp (DSC: 91.3%, interquartile range (IQR): 81.2\u201395.1%) segmentation in endoscopy images. This could be attributed to SAM\u2019s training on a variety of RGB images, and the fact that many targets in these images are relatively straightforward to segment due to their distinct appearances. The other three models outperformed SAM by a large margin and MedSAM has a narrower distribution of DSC scores of the 86 interval validation tasks than the two groups of specialist models, reflecting the robustness of MedSAM across different tasks. We further connected the DSC scores corresponding to the same task of the four models with the podium plot Fig. 3b, which is complementary to the box plot. In the upper part, each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical test cases are connected by a line. In the lower part, the frequency of achieved ranks for each method is presented with bar charts. It can be found that MedSAM ranked in first place on most tasks, surpassing the..",
      "s10": "..performance of the U-Net and DeepLabV3+ specialist models that have a high frequency of ranks with second and third places, respectively, In contrast, SAM ranked last place in almost all tasks. Figure 3c (and Supplementary Fig. 9) visualizes some randomly selected segmentation examples where MedSAM obtained a median DSC score, including liver tumor in CT images, brain tumor in MR images, breast tumor in ultrasound images, and polyp in endoscopy images. SAM struggles with targets of weak boundaries, which is prone to under or oversegmentation errors. In contrast, MedSAM can accurately segment a wide range of targets across various imaging conditions, which achieves comparable of even better than the specialist U-Net and DeepLabV3+ models. The external validation included 60 segmentation tasks, all of which either were from new datasets or involved unseen segmentation targets (Supplementary Tables 9\u201311 and Figs. 10\u201312). Figure 4a, b show the task-wise median DSC score distribution and their correspondence of the 60 tasks, respectively. Although SAM continued exhibiting lower performance on most CT and MR segmentation tasks, the specialist models no longer consistently outperformed SAM (e.g., right kidney segmentation in MR T1- weighted images: 90.1%, 85.3..",
      "s11": "..%, 86.4% for SAM, U-Net, and DeepLabV3+, respectively). This indicates the limited generalization ability of such specialist models on unseen targets. In contrast, MedSAM consistently delivers superior performance. For example, MedSAM obtained median DSC scores of 87.8% (IQR: 85.0-91.4%) on the nasopharynx cancer segmentation task, demonstrating 52.3%, 15.5%, and 22.7 improvements over SAM, the specialist U-Net, and DeepLabV3+, respectively. Significantly, MedSAM also achieved better performance in some unseen modalities (e.g., abdomen T1 Inphase and Outphase), surpassing SAM and the specialist models with improvements by up to 10%. Figure 4c presents four randomly selected segmentation examples for qualitative evaluation, revealing that while all the methods have the ability to handle simple segmentation targets, MedSAM performs better at segmenting challenging targets with indistinguishable boundaries, such as cervical cancer in MR images (more examples are presented in Supplementary Fig. 13). Furthermore, we evaluated MedSAM on the multiple myeloma plasma cell dataset, which represents a distinct modality and task in contrast to all previously leveraged validation tasks. Although this task had never been..",
      "s12": "..seen during training, a b Image encoder Bounding box prompts Mask decoder Prompt encoder Input Image Segmentation Image embedding Fig. 2 | Overview of the modality distribution in the dataset and the network architecture. a The number of medical image-mask pairs in each modality. b MedSAM is a promptable segmentation method where users can use bounding boxes to specify the segmentation targets. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 3 c a b SAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM Fig. 4 | Quantitative and qualitative evaluation results on the external validation set. a Performance distribution of 60 external validation tasks in terms of median dice similarity coefficient (DSC) score. The center line within the box represents the median value, with the bottom and top bounds of the box delineating the 25th and 75th percentiles, respectively. Whiskers are chosen to show the 1.5 of the interquartile range. Up-triangles denote the minima and downtriangles denote the maxima. b Podium plots for visualizing the performance correspondence of 60 external validation tasks. Upper part:..",
      "s13": "..each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical tasks are connected by a line. Lower part: bar charts represent the frequency of achieved ranks for each method. MedSAM ranks in the first place on most tasks. c Visualized segmentation examples on the external validation set. The four examples are the lymph node, cervical cancer, fetal head, and polyp in CT, MR, ultrasound, and endoscopy images, respectively. Source data are provided as a Source Data file. SAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM a c b Fig. 3 | Quantitative and qualitative evaluation results on the internal validation set. a Performance distribution of 86 internal validation tasks in terms of median dice similarity coefficient (DSC) score. The center line within the box represents the median value, with the bottom and top bounds of the box delineating the 25th and 75th percentiles, respectively. Whiskers are chosen to show the 1.5 of the interquartile range. Up-triangles denote the minima and downtriangles denote the maxima. b Podium plots for visualizing the performance correspondence of 86 internal validation tasks. Upper part: each colored dot denotes the median DSC achieved with the..",
      "s14": "..respective method on one task. Dots corresponding to identical tasks are connected by a line. Lower part: bar charts represent the frequency of achieved ranks for each method. MedSAM ranks in the first place on most tasks. c Visualized segmentation examples on the internal validation set. The four examples are liver cancer, brain cancer, breast cancer, and polyp in computed tomography (CT), (Magnetic Resonance Imaging) MRI, ultrasound, and endoscopy images, respectively. Blue: bounding box prompts; Yellow: segmentation results. Magenta: expert annotations. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 4 MedSAM still exhibited superior performance compared to the SAM (Supplementary Fig. 14), highlighting its remarkable generalization ability. The effect of training dataset size We also investigated the effect of varying dataset sizes on MedSAM\u2019s performance because the training dataset size has been proven to be pivotal in model performance28. We additionally trained MedSAM on two different dataset sizes: 10,000 (10K) and 100,000 (100K) images and their performances were compared with the default MedSAM model. The 10K and 100K training..",
      "s15": "..images were uniformly sampled from the whole training set, to maintain data diversity. As shown in (Fig. 5a) (Supplementary Tables 12\u201314), the performance adhered to the scaling rule, where increasing the number of training images significantly improved the performance in both internal and external validation sets. MedSAM can improve the annotation efficiency Furthermore, we conducted a human annotation study to assess the time cost of two pipelines (Methods). For the first pipeline, two human experts manually annotate 3D adrenal tumors in a slice-by-slice way. For the second pipeline, the experts first drew the long and short tumor axes with the linear marker (initial marker) every 3-10 slices, which is a common practice in tumor response evaluation. Then, MedSAM was used to segment the tumors based on these sparse linear annotations. Finally, the expert manually revised the segmentation results until they were satisfied. We quantitatively compared the annotation time cost between the two pipelines (Fig. 5b). The results demonstrate that with the assistance of MedSAM, the annotation time is substantially reduced by 82.37% and 82.95% for the two experts, respectively. Discussion We introduce MedSAM, a deep learning-powered foundation model designed for the segmentation of a wide array of anatomical structures..",
      "s16": "..and lesions across diverse medical imaging modalities. MedSAM is trained on a meticulously assembled large-scale dataset comprised of over one million medical image-mask pairs. Its promptable configuration strikes an optimal balance between automation and customization, rendering MedSAM a versatile tool for universal medical image segmentation. Through comprehensive evaluations encompassing both internal and external validation, MedSAM has demonstrated substantial capabilities in segmenting a diverse array of targets and robust generalization abilities to manage new data and tasks. Its performance not only significantly exceeds that of existing the state-of-the-art segmentation foundation model, but also rivals or even surpasses specialist models. By providing precise delineation of anatomical structures and pathological regions, MedSAM facilitates the computation of various quantitative measures that serve as biomarkers. For instance, in the field of oncology, MedSAM could play a crucial role in accelerating the 3D tumor annotation process, enabling subsequent calculations of tumor volume, which is a critical biomarker29 for assessing disease progression and response to treatment. Additionally, MedSAM provides a successful paradigm for adapting natural image foundation models to new domains, which can be further extended to biological image segmentation30, such as cell segmentation in light microscopy images31 and organelle segmentation in electron microscopy images32. While MedSAM boasts strong capabilities, it does present certain limitations. One such..",
      "s17": "..limitation is the modality imbalance in the training set, with CT, MRI, and endoscopy images dominating the dataset. This could potentially impact the model\u2019s performance on less-represented modalities, such as mammography. Another limitation is its difficulty in the segmentation of vessel-like branching structures because the bounding box prompt can be ambiguous in this setting. For example, arteries and veins share the same bounding box in eye fundus images. However, these limitations do not diminish MedSAM\u2019s utility. Since MedSAM has learned rich and representative medical image features from the large-scale training set, it can be finetuned to effectively segment new tasks from less-represented modalities or intricate structures like vessels. In conclusion, this study highlights the feasibility of constructing a single foundation model capable of managing a multitude of segmentation tasks, thereby eliminating the need for task-specific models. MedSAM, as the inaugural foundation model in medical image segmentation, holds great potential to accelerate the advancement of new diagnostic and therapeutic tools, and ultimately contribute to improved patient care33. Methods Dataset curation and pre-processing We curated a comprehensive dataset by collating images from publicly available medical image segmentation datasets, which were obtained from various sources across the internet, including the Cancer Imaging Archive (TCIA)34, Kaggle, Grand-Challenge..",
      "s18": ".., Scientific Data, CodaLab, and segmentation challenges in the Medical Image Computing and Computer Assisted Intervention Society (MICCAI). All the datasets provided segmentation annotations by human experts, which have been widely used in existing literature (Supplementary Table 1\u20134). We incorporated these annotations directly for both model development and validation. The original 3D datasets consisted of computed tomography (CT) and magnetic resonance (MR) images in DICOM, nrrd, or mhd formats. To ensure uniformity and compatibility with developing medical image deep learning models, we converted the images to the widely used NifTI format. Additionally, grayscale images (such as X-Ray and Ultrasound) as well as RGB images (including endoscopy, dermoscopy, fundus, and pathology images), were converted to the png format. Fig. 5 | The effect of training dataset size and a user study of tumor annotation efficiency. a Scaling up the training image size to one million can significantly improve the model performance on both internal and external validation sets. b MedSAM can be used to substantially reduce the annotation time cost. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024..",
      "s19": "..) 15:654 5 Several exclusive criteria are applied to improve the dataset quality and consistency, including incomplete images and segmentation targets with branching structures, inaccurate annotations, and tiny volumes. Notably, image intensities varied significantly across different modalities. For instance, CT images had intensity values ranging from -2000 to 2000, while MR images exhibited a range of 0 to 3000. In endoscopy and ultrasound images, intensity values typically spanned from 0 to 255. To facilitate stable training, we performed intensity normalization across all images, ensuring they shared the same intensity range. For CT images, we initially normalized the Hounsfield units using typical window width and level values. The employed window width and level values for soft tissues, lung, and brain are (W:400, L:40), (W:1500, L:-160), and (W:80, L:40), respectively. Subsequently, the intensity values were rescaled to the range of [0, 255]. For MR, X-ray, ultrasound, mammography, and optical coherence tomography (OCT) images, we clipped the intensity values to the range between the 0.5th and 99.5th percentiles before rescaling them to the range of [0, 255]. Regarding RGB images (e...",
      "s20": "..g., endoscopy, dermoscopy, fundus, and pathology images), if they were already within the expected intensity range of [0, 255], their intensities remained unchanged. However, if they fell outside this range, we utilized max-min normalization to rescale the intensity values to [0, 255]. Finally, to meet the model\u2019s input requirements, all images were resized to a uniform size of 1024 \u00d7 1024 \u00d7 3. In the case of whole-slide pathology images, patches were extracted using a sliding window approach without overlaps. The patches located on boundaries were padded to this size with 0. As for 3D CT and MR images, each 2D slice was resized to 1024 \u00d7 1024, and the channel was repeated three times to maintain consistency. The remaining 2D images were directly resized to 1024 \u00d7 1024 \u00d7 3. Bi-cubic interpolation was used for resizing images, while nearest-neighbor interpolation was applied for resizing masks to preserve their precise boundaries and avoid introducing unwanted artifacts. These standardization procedures ensured uniformity and compatibility across all images and facilitated seamless integration into the subsequent stages of the model training and evaluation pipeline. Network architecture The network utilized in this study was built on transformer architecture27, which has demonstrated remarkable effectiveness in various domains such as natural language..",
      "s21": "..processing and image recognition tasks25. Specifically, the network incorporated a vision transformer (ViT)-based image encoder responsible for extracting image features, a prompt encoder for integrating user interactions (bounding boxes), and a mask decoder that generated segmentation results and confidence scores using the image embedding, prompt embedding, and output token. To strike a balance between segmentation performance and computational efficiency, we employed the base ViT model as the image encoder since extensive evaluation indicated that larger ViT models, such as ViT Large and ViT Huge, offered only marginal improvements in accuracy7 while significantly increasing computational demands. Specifically, the base ViT model consists of 12 transformer layers27, with each block comprising a multi-head self-attention block and a Multilayer Perceptron (MLP) block incorporating layer normalization35. Pre-training was performed using masked auto-encoder modeling36, followed by fully supervised training on the SAM dataset7 . The input image (1024 \u00d7 1024 \u00d7 3) was reshaped into a sequence of flattened 2D patches with the size 16 \u00d7 16 \u00d7 3, yielding a feature size in image embedding of 64 \u00d7 64 after passing through the image encoder, which is 16 \u00d7 downscaled. The prompt encoders mapped the corner point of the bounding box prompt to..",
      "s22": "..256-dimensional vectorial embeddings26. In particular, each bounding box was represented by an embedding pair of the topleft corner point and the bottom-right corner point. To facilitate realtime user interactions once the image embedding had been computed, a lightweight mask decoder architecture was employed. It consists of two transformer layers27 for fusing the image embedding and prompt encoding, and two transposed convolutional layers to enhance the embedding resolution to 256 \u00d7 256. Subsequently, the embedding underwent sigmoid activation, followed by bi-linear interpolations to match the input size. Training protocol and experimental setting During data pre-processing, we obtained 1,570,263 medical imagemask pairs for model development and validation. For internal validation, we randomly split the dataset into 80%, 10%, and 10% as training, tuning, and validation, respectively. Specifically, for modalities where within-scan continuity exists, such as CT and MRI, and modalities where continuity exists between consecutive frames, we performed the data splitting at the 3D scan and the video level respectively, by which any potential data leak was prevented. For pathology images, recognizing the significance of slide-level cohesiveness, we first separated the whole-slide images into distinct slidebased sets. Then, each slide was divided into small patches with a fixed size..",
      "s23": "..of 1024 \u00d7 1024. This setup allowed us to monitor the model\u2019s performance on the tuning set and adjust its parameters during training to prevent overfitting. For the external validation, all datasets were held out and did not appear during model training. These datasets provide a stringent test of the model\u2019s generalization ability, as they represent new patients, imaging conditions, and potentially new segmentation tasks that the model has not encountered before. By evaluating the performance of MedSAM on these unseen datasets, we can gain a realistic understanding of how MedSAM is likely to perform in real-world clinical settings, where it will need to handle a wide range of variability and unpredictability in the data. The training and validation are independent. The model was initialized with the pre-trained SAM model with the ViT-Base model. We fixed the prompt encoder since it can already encode the bounding box prompt. All the trainable parameters in the image encoder and mask decoder were updated during training. Specifically, the number of trainable parameters for the image encoder and mask decoder are 89,670,912 and 4,058,340, respectively. The bounding box prompt was simulated from the expert annotations with a random perturbation of 0-20 pixels. The loss function is the unweighted sum between dice loss and cross..",
      "s24": "..-entropy loss, which has been proven to be robust in various segmentation tasks1 . The network was optimized by AdamW37 optimizer (\u03b21 = 0.9, \u03b22 = 0.999) with an initial learning rate of 1e-4 and a weight decay of 0.01. The global batch size was 160 and data augmentation was not used. The model was trained on 20 A100 (80G) GPUs with 150 epochs and the last checkpoint was selected as the final model. Furthermore, to thoroughly evaluate the performance of MedSAM, we conducted comparative analyses against both the state-ofthe-art segmentation foundation model SAM7 and specialist models (i.e., U-Net1 and DeepLabV3+24). The training images contained 10 modalities: CT, MR, chest X-ray (CXR), dermoscopy, endoscopy, ultrasound, mammography, OCT, and pathology, and we trained the U-Net and DeepLabV3+ specialist models for each modality. There were 20 specialist models in total and the number of corresponding training images was presented in Supplementary Table 5. We employed the nnU-Net to conduct all U-Net experiments, which can automatically configure the network architecture based on the dataset..",
      "s25": "..properties. In order to incorporate the bounding box prompt into the model, we transformed the bounding box into a binary mask and concatenated it with the image as the model input. This function was originally supported by nnU-Net in the cascaded pipeline, which has demonstrated increased performance in many segmentation tasks by using the binary mask as an additional channel to specify the target location. The training settings followed the default configurations of 2D nnU-Net. Each model was trained on one A100 GPU with 1000 Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 6 epochs and the last checkpoint was used as the final model. The DeepLabV3+ specialist models used ResNet5038 as the encoder. Similar to ref. 3, the input images were resized to 224 \u00d7 224 \u00d7 3. The bounding box was transformed into a binary mask as an additional input channel to provide the object location prompt. Segmentation Models Pytorch (0.3.3)39 was used to perform training and inference for all the modality-wise specialist DeepLabV3 + models. Each modality-wise model was trained on one A100 GPU with 500 epochs and the last checkpoint..",
      "s26": "..was used as the final model. During the inference phase, SAM and MedSAM were used to perform segmentation across all modalities with a single model. In contrast, the U-Net and DeepLabV3+ specialist models were used to individually segment the respective corresponding modalities. A task-specific segmentation model might outperform a modalitybased one for certain applications. Since U-Net obtained better performance than DeepLabV3+ on most tasks, we further conducted a comparison study by training task-specific U-Net models on four representative tasks, including liver cancer segmentation in CT scans, abdominal organ segmentation in MR scans, nerve cancer segmentation in ultrasound, and polyp segmentation in endoscopy images. The experiments included both internal validation and external validation. For internal validation, we adhered to the default data splits, using them to train the task-specific U-Net models and then evaluate their performance on the corresponding validation set. For external validation, the trained U-Net models were evaluated on new datasets from the same modality or segmentation targets. In all these experiments, MedSAM was directly applied to the validation sets without additional fine-tuning. As shown in Supplementary Fig. 15, while task-specific UNet models often achieved great results on internal validation sets, their performance diminished significantly for external sets. In..",
      "s27": "..contrast, MedSAM maintained consistent performance across both internal and external validation sets. This underscores MedSAM\u2019s superior generalization ability, making it a versatile tool in a variety of medical image segmentation tasks. Loss function We used the unweighted sum between cross-entropy loss and dice loss40 as the final loss function since it has been proven to be robust across different medical image segmentation tasks41. Specifically, let S, G denote the segmentation result and ground truth, respectively. si, gi denotes the predicted segmentation and ground truth of voxel i, respectively. N is the number of voxels in the image I. Binary crossentropy loss is defined by LBCE = 1 N XN i = 1 gi log si + \u00f01 gi\u00de log\u00f01 si\u00de , \u00f01\u00de and dice loss is defined by LDice = 1 2 PN i = 1 gisi PN i = 1 \u00f0gi\u00de 2 + PN i = 1 \u00f0si\u00de 2 : \u00f02\u00de The final loss L is defined by L = LBCE + LDice: \u00f03\u00de Human annotation study The objective of the human annotation study was to quantitatively evaluate how MedSAM can reduce the annotation time cost. Specifically, we used the recent adrenocortical carcinoma CT dataset34,42,..",
      "s28": "..43, where the segmentation target, adrenal tumor, was neither part of the training nor of the existing validation sets. We randomly sampled 10 cases, comprising a total of 733 tumor slices requiring annotations. Two human experts participated in this study, both of whom are experienced radiologists with 8 and 6 years of clinical practice in abdominal diseases, respectively. Each expert generated two groups of annotations, one with the assistance of MedSAM and one without. In the first group, the experts manually annotated the 3D adrenal tumor in a slice-by-slice manner. Annotations by the two experts were conducted independently, with no collaborative discussions, and the time taken for each case was recorded. In the second group, annotations were generated after one week of cooling period. The experts independently drew the long and short tumor axes as initial markers, which is a common practice in tumor response evaluation. This process was executed every 3-10 slices from the top slice to the bottom slice of the tumor. Then, we applied MedSAM to segment the tumors based on these sparse linear annotations, including three steps. \u2022 Step 1. For each annotated slice, a rectangle binary mask was generated based on the linear label that can completely cover the linear label. \u2022 Step 2. For the unlabeled slices, the rectangle binary masks were..",
      "s29": "..created through interpolation of the surrounding labeled slices. \u2022 Step 3. We transformed the binary masks into bounding boxes and then fed them along with the images into MedSAM to generate segmentation results. All these steps were conducted in an automatic way and the model running time was recorded for each case. Finally, human experts manually refined the segmentation results until they met their satisfaction. To summarize, the time cost of the second group of annotations contained three parts: initial markers, MedSAM inference, and refinement. All the manual annotation processes were based on ITKSNAP44, an open-source software designed for medical image visualization and annotation. Evaluation metrics We followed the recommendations in Metrics Reloaded45 and used the dice similarity coefficient and normalized surface distance (NSD) to quantitatively evaluate the segmentation results. DSC is a region-based segmentation metric, aiming to evaluate the region overlap between expert annotation masks and segmentation results, which is defined by DSC\u00f0G, S\u00de = 2jG \\ Sj jGj + jSj , NSD46 is a boundary-based metric, aiming to evaluate the boundary consensus between expert annotation masks and segmentation results at a given tolerance, which is defined by NSD\u00f0G, S\u00de = j\u2202G \\ B\u00f0\u03c4\u00de \u2202S j + j..",
      "s30": "..\u2202S \\ B\u00f0\u03c4\u00de\u2202Gj j\u2202Gj + j\u2202Sj , where B\u00f0\u03c4\u00de \u2202G = fx 2 R3 j 9x~ 2 \u2202G, jjx x~jj \u2264 \u03c4g, B\u00f0\u03c4\u00de\u2202S = fx 2 R3 j 9x~ 2 \u2202S, jjx x~jj \u2264 \u03c4g denote the border region of the expert annotation mask and the segmentation surface at tolerance \u03c4, respectively. In this paper, we set the tolerance \u03c4 as 2. Statistical analysis To statistically analyze and compare the performance of the aforementioned four methods (MedSAM, SAM, U-Net, and DeepLabV3+ specialist models), we employed the Wilcoxon signed-rank test. This non-parametric test is well-suited for comparing paired samples and is particularly useful when the data does not meet the assumptions of normal distribution. This analysis allowed us to determine if any method demonstrated statistically superior segmentation performance compared to the others, providing valuable insights into the comparative effectiveness of the evaluated methods. The Wilcoxon signed-rank test results are marked on the DSC and NSD score tables (Supplementary Table 6\u201311). Article https://doi.org/10.1038/s..",
      "s31": "..41467-024-44824-z Nature Communications | (2024) 15:654 7 Software utilized All code was implemented in Python (3.10) using Pytorch (2.0) as the base deep learning framework. We also used several Python packages for data analysis and results visualization, including connectedcomponents-3d (3.10.3), SimpleITK (2.2.1), nibabel (5.1.0), torchvision (0.15.2), numpy (1.24.3), scikit-image (0.20.0), scipy (1.10.1), and pandas (2.0.2), matplotlib (3.7.1), opencv-python (4.8.0), ChallengeR (1.0.5), and plotly (5.15.0). Biorender was used to create Fig. 1. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The training and validating datasets used in this study are available in the public domain and can be downloaded via the links provided in Supplementary Tables 16 and 17. Source data are..",
      "s32": "..provided with this paper in the Source Data file. We confirmed that All the image datasets in this study are publicly accessible and permitted for research purposes. Source data are provided in this paper. Code availability The training script, inference script, and trained model have been publicly available at https://github.com/bowang-lab/MedSAM. A permanent version is released on Zenodo47. References 1. Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. & Maier-Hein, K. H. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat. Method. 18, 203\u2013211 (2021). 2. De Fauw, J. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat. Med. 24, 1342\u20131350 (2018). 3. Ouyang, D. Video-based AI for beat-to-beat assessment of cardiac function. Nature 580, 252\u2013256 (2020). 4. Wang, G. Deepigeos: a deep interactive geodesic framework for medical image segmentation. In IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 1559\u20131572 (..",
      "s33": "..IEEE, 2018). 5. Antonelli, M. The medical segmentation decathlon. Nat. Commun. 13, 4128 (2022). 6. Minaee, S. Image segmentation using deep learning: A survey. In IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 3523\u20133542 (IEEE, 2021). 7. Kirillov, A. et al. Segment anything. In IEEE International Conference on Computer Vision. 4015\u20134026 (IEEE, 2023). 8. Zou, X. et al. Segment everything everywhere all at once. In Advances in Neural Information Processing Systems (MIT Press, 2023). 9. Wang, G. Interactive medical image segmentation using deep learning with image-specific fine tuning. In IEEE Transactions on Medical Imaging 37, 1562\u20131573 (IEEE, 2018). 10. Zhou, T. Volumetric memory network for interactive medical image segmentation. Med. Image Anal. 83, 102599 (2023). 11. Luo, X. Mideepseg: Minimally interactive segmentation of unseen objects from medical images using deep learning. Med. Image Anal. 72, 102102 (2021). 12. Deng, R. et al. Segment anything model (SAM) for digital pathology:..",
      "s34": "..assess zero-shot segmentation on whole slide imaging. Preprint at https://arxiv.org/abs/2304.04155 (2023). 13. Hu, C., Li, X. When SAM meets medical images: an investigation of segment anything model (SAM) on multi-phase liver tumor segmentation. Preprint at https://arxiv.org/abs/2304.08506 (2023). 14. He, S., Bao, R., Li, J., Grant, P.E., Ou, Y. Accuracy of segmentanything model (SAM) in medical image segmentation tasks. Preprint at https://doi.org/10.48550/arXiv.2304.09324 (2023). 15. Roy, S. et al. SAM.MD: zero-shot medical image segmentation capabilities of the segment anything model. Preprint at https:// arxiv.org/abs/2304.05396 (2023). 16. Zhou, T., Zhang, Y., Zhou, Y., Wu, Y. & Gong, C. Can SAM segment polyps? Preprint at https://arxiv.org/abs/..",
      "s35": "..2304.07583 (2023). 17. Mohapatra, S., Gosai, A., Schlaug, G. Sam vs bet: a comparative study for brain extraction and segmentation of magnetic resonance images using deep learning. Preprint at https://arxiv.org/abs/2304. 04738 (2023). 18. Chen, J., Bai, X. Learning to\" segment anything\" in thermal infrared images through knowledge distillation with a large scale dataset SATIR. Preprint at https://arxiv.org/abs/2304.07969 (2023). 19. Tang, L., Xiao, H., Li, B. Can SAM segment anything? when SAM meets camouflaged object detection. Preprint at https://arxiv.org/ abs/2304.04709 (2023). 20. Ji, G.-P. et al. SAM struggles in concealed scenes\u2013empirical study on\u201d segment anything\u201d. Science China Information Sciences. 66, 226101 (2023). 21. Ji, W., Li, J., Bi, Q., Li, W., Cheng, L. Segment anything is not always perfect: an investigation of SAM on different real..",
      "s36": "..-world applications. Preprint at https://arxiv.org/abs/2304.05750 (2023). 22. Mazurowski, M. A. Segment anything model for medical image analysis: an experimental study. Med. Image Anal. 89, 102918 (2023). 23. Huang, Y. et al. Segment anything model for medical images? Med. Image Anal. 92, 103061 (2024). 24. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H. Encoderdecoder with atrous separable convolution for semantic image segmentation. In Proc. European Conference on Computer Vision. 801\u2013818 (IEEE, 2018). 25. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers for image recognition at scale. In: International Conference on Learning Representations (OpenReview.net, 2020). 26. Tancik, M. Fourier features let networks learn high frequency functions in low-dimensional domains. In Advances in Neural Information Processing Systems 33, 7537\u20137547 (Curran Associates, Inc., 2020). 27. Vaswani, A. et al..",
      "s37": "... Attention is all you need. In Advances in Neural Information Processing Systems, Vol. 30 (Curran Associates, Inc., 2017). 28. He, B. Blinded, randomized trial of sonographer versus AI cardiac function assessment. Nature 616, 520\u2013524 (2023). 29. Eisenhauer, E. A. New response evaluation criteria in solid tumours: revised recist guideline (version 1.1). Eur. J. Cancer 45, 228\u2013247 (2009). 30. Ma, J. & Wang, B. Towards foundation models of biological image segmentation. Nat. Method. 20, 953\u2013955 (2023). 31. Ma, J. et al. The multi-modality cell segmentation challenge: towards universal solutions. Preprint at https://arxiv.org/abs/2308. 05864 (2023). 32. Xie, R., Pang, K., Bader, G.D., Wang, B. Maester: masked autoencoder guided segmentation at pixel resolution for accurate, selfsupervised subcellular structure recognition. In IEEE Conference on Computer Vision and Pattern Recognition. 3292\u20133301 (IEEE, 2023). 33. Bera, K., Braman, N...",
      "s38": ".., Gupta, A., Velcheti, V. & Madabhushi, A. Predicting cancer outcomes with radiomics and artificial intelligence in radiology. Nat. Rev. Clin. Oncol. 19, 132\u2013146 (2022). 34. Clark, K. The cancer imaging archive (TCIA): maintaining and operating a public information repository. J. Digit. Imaging 26, 1045\u20131057 (2013). 35. Ba, J.L., Kiros, J.R., Hinton, G.E. Layer normalization. Preprint at https://arxiv.org/abs/1607.06450 (2016). 36. He, K. et al. Masked autoencoders are scalable vision learners. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16000\u201316009 (IEEE, 2022). Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 8 37. Loshchilov, I., Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations (OpenReview.net, 2019). 38. He..",
      "s39": ".., K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition. 770\u2013778 (IEEE, 2016). 39. Iakubovskii, P. Segmentation models pytorch. GitHub https:// github.com/qubvel/segmentation_models.pytorch (2019). 40. Milletari, F., Navab, N., Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In International Conference on 3D Vision (3DV). 565\u2013571 (IEEE, 2016). 41. Ma, J. Loss odyssey in medical image segmentation. Med. Image Anal. 71, 102035 (2021). 42. Ahmed, A. Radiomic mapping model for prediction of Ki-67 expression in adrenocortical carcinoma. Clin. Radiol. 75, 479\u201317 (2020). 43. Moawad, A.W. et al. Voxel-level segmentation of pathologicallyproven Adrenocortical carcinoma with Ki-67 expression (AdrenalACC-Ki67-Seg) [data set]. https://doi..",
      "s40": "...org/10.7937/1FPGVM46 (2023). 44. Yushkevich, P.A., Gao, Y., Gerig, G. Itk-snap: an interactive tool for semi-automatic segmentation of multi-modality biomedical images. In International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 3342\u20133345 (IEEE, 2016). 45. Maier-Hein, L. et al. Metrics reloaded: Pitfalls and recommendations for image analysis validation. Preprint at https://arxiv.org/abs/ 2206.01653 (2022). 46. DeepMind surface-distance. https://github.com/googledeepmind/surface-distance (2018). 47. Ma, J. bowang-lab/MedSAM: v1.0.0. https://doi.org/10.5281/ zenodo.10452777 (2023). Acknowledgements This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC, RGPIN-2020-06189 and DGECR2020-00294) and CIFAR AI Chair programs. The authors of this paper highly appreciate all the..",
      "s41": "..data owners for providing public medical images to the community. We also thank Meta AI for making the source code of segment anything publicly available to the community. This research was enabled in part by computing resources provided by the Digital Research Alliance of Canada. Author contributions Conceived and designed the experiments: J.M. Y.H., C.Y., B.W. Performed the experiments: J.M. Y.H., F.L., L.H., C.Y. Analyzed the data: J.M. Y.H., F.L., L.H., C.Y., B.W. Wrote the paper: J.M. Y.H., F.L., L.H., C.Y., B.W. All authors have read and agreed to the published version of the manuscript. Competing interests The authors declare no competing interests Additional information Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41467-024-44824-z. Correspondence and requests for materials should be addressed to Bo Wang. Peer review information Nature Communications thanks David Ouyang, and the other, anonymous, reviewer(s) for..",
      "s42": "..their contribution to the peer review of this work. A peer review file is available. Reprints and permissions information is available at http://www.nature.com/reprints Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/ licenses/by/4.0/. \u00a9 The Author(s) 2024 Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications..",
      "s43": "..| (2024) 15:654 9"
    },
    "question_generated": "What are the limitations of current task-specific medical image segmentation models, and how can advancements in natural image segmentation inform the development of universal models that address these limitations, ultimately enhancing the efficiency and consistency of clinical workflows?",
    "golden_answer_generated": "# Universal Medical Image Segmentation: A Literature Synthesis\n\n## Introduction\nMedical image segmentation is a pivotal process in clinical settings, facilitating precise diagnosis, treatment planning, and monitoring of disease progression\u3010s1\u3011\u3010s2\u3011. Despite its importance, current segmentation models often lack the versatility needed to handle the diverse range of medical imaging tasks\u3010s2\u3011\u3010s3\u3011.\n\n## Background\n### Existing Models and Their Limitations\nTraditional segmentation models are typically designed for specific tasks or modalities, such as CT or MRI scans\u3010s2\u3011\u3010s3\u3011. While these models can achieve high accuracy within their designated scope, their performance often degrades when applied to new tasks or different imaging data\u3010s3\u3011\u3010s4\u3011. This task-specific nature poses a significant barrier to the broader application of these models in clinical practice\u3010s4\u3011\u3010s5\u3011.\n\n### Advances in Natural Image Segmentation\nRecent developments in natural image segmentation have introduced foundation models capable of handling a wide array of tasks\u3010s3\u3011\u3010s6\u3011. These models, such as the Segment Anything Model (SAM), demonstrate remarkable versatility and performance across various segmentation challenges\u3010s6\u3011\u3010s7\u3011. However, their applicability to medical images remains limited due to the inherent differences between natural and medical images\u3010s7\u3011\u3010s8\u3011.\n\n## Literature Synthesis\n### The Need for Universal Models\nThe demand for universal segmentation models in medical imaging is growing\u3010s3\u3011\u3010s8\u3011. Such models would not only enhance versatility but also ensure more consistent results across different tasks\u3010s8\u3011\u3010s9\u3011. Existing literature highlights the potential benefits of a model that can be trained once and applied universally, reducing the need for task-specific models\u3010s9\u3011\u3010s10\u3011.\n\n### Challenges and Considerations\nDeveloping a universal model for medical image segmentation involves addressing several challenges, including the variability in imaging modalities and the complexity of segmentation tasks\u3010s10\u3011\u3010s11\u3011. The literature suggests that a promptable segmentation approach, which allows for user-defined prompts, could offer the flexibility needed to adapt to specific tasks\u3010s11\u3011\u3010s12\u3011.\n\n## Conclusion\nBased on the synthesis of existing literature, it is hypothesized that a universal medical image segmentation model, capable of handling diverse tasks and modalities, could significantly enhance the efficiency and consistency of clinical workflows\u3010s12\u3011\u3010s13\u3011. This hypothesis lays the groundwork for future research and development in the field of medical image segmentation\u3010s13\u3011\u3010s14\u3011.",
    "used_snippets_with_context": {
      "s1": "Article https://doi.org/10.1038/s41467-024-44824-z Segment anything in medical images Jun Ma1,2,3, Yuting He4, Feifei Li 1, Lin Han5, Chenyu You 6 & Bo Wang 1,2,3,7,8 Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans. Segmentation is a fundamental task in medical imaging analysis, which involves identifying and delineating regions of interest (ROI) in various medical images,..",
      "s2": "..such as organs, lesions, and tissues1 . Accurate segmentation is essential for many clinical applications, including disease diagnosis, treatment planning, and monitoring of disease progression2,3 . Manual segmentation has long been the gold standard for delineating anatomical structures and pathological regions, but this process is time-consuming, labor-intensive, and often requires a high degree of expertise. Semi- or fully automatic segmentation methods can significantly reduce the time and labor required, increase consistency, and enable the analysis of large-scale datasets4 . Deep learning-based models have shown great promise in medical image segmentation due to their ability to learn intricate image features and deliver accurate segmentation results across a diverse range of tasks, from segmenting specific anatomical structures to identifying pathological regions5 . However, a significant limitation of many current medical image segmentation models is their task-specific nature. These models are typically designed and trained for a specific segmentation task, and their performance can degrade significantly when applied to new tasks or different types of imaging data6 . This lack of generality poses a substantial obstacle to the wider application of these models in clinical practice. In contrast, recent advances in the field of natural image segmentation have witnessed the emergence of segmentation foundation models, such as segment anything model (SAM)7 and Segment Everything Everywhere with Multi-modal..",
      "s3": "..prompts all at once8 , showcasing remarkable versatility and performance across various segmentation tasks. There is a growing demand for universal models in medical image segmentation: models that can be trained once and then applied to a wide range of segmentation tasks. Such models would not only exhibit heightened versatility in terms of model capacity but also potentially lead to more consistent results across different tasks. However, the applicability of the segmentation foundation models (e.g., SAM7 ) to medical image segmentation remains limited due to the significant differences between natural images and medical images. Essentially, SAM is a promptable segmentation method that requires points or bounding boxes to specify the segmentation targets. This resembles conventional interactive segmentation methods4,9\u201311 but SAM has better generalization ability, while existing deep learning-based interactive segmentation methods focus mainly on limited tasks and image modalities. Many studies have applied the out-of-the-box SAM models to typical medical image segmentation tasks12\u201317 and other challenging scenarios18\u201321. For example, the concurrent studies22,23 conducted a Received: 24 October 2023 Accepted: 5 January 2024 Check for updates 1 Peter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada. 2Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, ON, Canada. 3..",
      "s4": "..Vector Institute, Toronto, ON, Canada. 4Department of Computer Science, Western University, London, ON, Canada. 5Tandon School of Engineering, New York University, New York, NY, USA. 6 Department of Electrical Engineering, Yale University, New Haven, CT, USA. 7Department of Computer Science, University of Toronto, Toronto, ON, Canada. 8 UHN AI Hub, Toronto, ON, Canada. e-mail: bowang@vectorinstitute.ai Nature Communications | (2024) 15:654 1 1234567890():,;1234567890():,; comprehensive assessment of SAM across a diverse array of medical images, underscoring that SAM achieved satisfactory segmentation outcomes primarily on targets characterized by distinct boundaries. However, the model exhibited substantial limitations in segmenting typical medical targets with weak boundaries or low contrast. In congruence with these observations, we further introduce MedSAM, a refined foundation model that significantly enhances the segmentation performance of SAM on medical images. MedSAM accomplishes this by fine-tuning SAM on an unprecedented dataset with more than one million medical image-mask pairs. We thoroughly evaluate MedSAM through comprehensive experiments on 86 internal validation tasks and 60 external validation tasks, spanning a variety of anatomical structures, pathological conditions, and medical imaging modalities..",
      "s5": "... Experimental results demonstrate that MedSAM consistently outperforms the state-of-theart (SOTA) segmentation foundation model7 , while achieving performance on par with, or even surpassing specialist models1,24 that were trained on the images from the same modality. These results highlight the potential of MedSAM as a new paradigm for versatile medical image segmentation. Results MedSAM: a foundation model for promptable medical image segmentation MedSAM aims to fulfill the role of a foundation model for universal medical image segmentation. A crucial aspect of constructing such a model is the capacity to accommodate a wide range of variations in imaging conditions, anatomical structures, and pathological conditions. To address this challenge, we curated a diverse and large-scale medical image segmentation dataset with 1,570,263 medical imagemask pairs, covering 10 imaging modalities, over 30 cancer types, and a multitude of imaging protocols (Fig. 1 and Supplementary Tables 1\u20134). This large-scale dataset allows MedSAM to learn a rich representation of medical images, capturing a broad spectrum of anatomies and lesions across different modalities. Figure 2a provides an overview of the distribution of images across different medical imaging modalities in the dataset, ranked by their total numbers. It is evident that computed tomography (CT), magnetic resonance imaging (MRI..",
      "s6": "..), and endoscopy are the dominant modalities, reflecting their ubiquity in clinical practice. CT and MRI images provide detailed cross-sectional views of 3D body structures, making them indispensable for non-invasive diagnostic imaging. Endoscopy, albeit more invasive, enables direct visual inspection of organ interiors, proving invaluable for diagnosing gastrointestinal and urological conditions. Despite the prevalence of these modalities, others such as ultrasound, pathology, fundus, dermoscopy, mammography, and optical coherence tomography (OCT) also hold significant roles in clinical practice. The diversity of these modalities and their corresponding segmentation targets underscores the necessity for universal and effective segmentation models capable of handling the unique characteristics associated with each modality. Another critical consideration is the selection of the appropriate segmentation prompt and network architecture. While the concept of fully automatic segmentation foundation models is enticing, it is fraught with challenges that make it impractical. One of the primary challenges is the variability inherent in segmentation tasks. For example, given a liver cancer CT image, the segmentation task can vary depending on the specific clinical scenario. One clinician might be Fig. 1 | MedSAM is trained on a large-scale dataset that can handle diverse segmentation tasks. The dataset covers a variety of anatomical structures, pathological conditions, and medical imaging modalities. The magenta contours and mask overlays denote the expert..",
      "s7": "..annotations and MedSAM segmentation results, respectively. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 2 interested in segmenting the liver tumor, while another might need to segment the entire liver and surrounding organs. Additionally, the variability in imaging modalities presents another challenge. Modalities such as CT and MR generate 3D images, whereas others like X-ray and ultrasound yield 2D images. These variabilities in task definition and imaging modalities complicate the design of a fully automatic model capable of accurately anticipating and addressing the diverse requirements of different users. Considering these challenges, we argue that a more practical approach is to develop a promptable 2D segmentation model. The model can be easily adapted to specific tasks based on user-provided prompts, offering enhanced flexibility and adaptability. It is also able to handle both 2D and 3D images by processing 3D images as a series of 2D slices. Typical user prompts include points and bounding boxes and we show some segmentation examples with the different prompts in Supplementary Fig. 1. It can be found that bounding boxes provide a more unambiguous spatial context for the region of interest, enabling the algorithm to more precisely discern the target area. This stands in..",
      "s8": "..contrast to point-based prompts, which can introduce ambiguity, particularly when proximate structures resemble each other. Moreover, drawing a bounding box is efficient, especially in scenarios involving multi-object segmentation. We follow the network architecture in SAM7 , including an image encoder, a prompt encoder, and a mask decoder (Fig. 2b). The image encoder25 maps the input image into a high-dimensional image embedding space. The prompt encoder transforms the user-drawn bounding boxes into feature representations via positional encoding26. Finally, the mask decoder fuses the image embedding and prompt features using cross-attention27 (Methods). Quantitative and qualitative analysis We evaluated MedSAM through both internal validation and external validation. Specifically, we compared it to the SOTA segmentation foundation model SAM7 as well as modality-wise specialist U-Net1 and DeepLabV3+24 models. Each specialized model was trained on images from the corresponding modality, resulting in 10 dedicated specialist models for each method. During inference, these specialist models were used to segment the images from corresponding modalities, while SAM and MedSAM were employed for segmenting images across all modalities (Methods). The internal validation contained 86 segmentation tasks (Supplementary Tables 5\u20138 and Fig. 2), and Fig. 3a shows..",
      "s9": "..the median dice similarity coefficient (DSC) score of these tasks for the four methods. Overall, SAM obtained the lowest performance on most segmentation tasks although it performed promisingly on some RGB image segmentation tasks, such as polyp (DSC: 91.3%, interquartile range (IQR): 81.2\u201395.1%) segmentation in endoscopy images. This could be attributed to SAM\u2019s training on a variety of RGB images, and the fact that many targets in these images are relatively straightforward to segment due to their distinct appearances. The other three models outperformed SAM by a large margin and MedSAM has a narrower distribution of DSC scores of the 86 interval validation tasks than the two groups of specialist models, reflecting the robustness of MedSAM across different tasks. We further connected the DSC scores corresponding to the same task of the four models with the podium plot Fig. 3b, which is complementary to the box plot. In the upper part, each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical test cases are connected by a line. In the lower part, the frequency of achieved ranks for each method is presented with bar charts. It can be found that MedSAM ranked in first place on most tasks, surpassing the..",
      "s10": "..performance of the U-Net and DeepLabV3+ specialist models that have a high frequency of ranks with second and third places, respectively, In contrast, SAM ranked last place in almost all tasks. Figure 3c (and Supplementary Fig. 9) visualizes some randomly selected segmentation examples where MedSAM obtained a median DSC score, including liver tumor in CT images, brain tumor in MR images, breast tumor in ultrasound images, and polyp in endoscopy images. SAM struggles with targets of weak boundaries, which is prone to under or oversegmentation errors. In contrast, MedSAM can accurately segment a wide range of targets across various imaging conditions, which achieves comparable of even better than the specialist U-Net and DeepLabV3+ models. The external validation included 60 segmentation tasks, all of which either were from new datasets or involved unseen segmentation targets (Supplementary Tables 9\u201311 and Figs. 10\u201312). Figure 4a, b show the task-wise median DSC score distribution and their correspondence of the 60 tasks, respectively. Although SAM continued exhibiting lower performance on most CT and MR segmentation tasks, the specialist models no longer consistently outperformed SAM (e.g., right kidney segmentation in MR T1- weighted images: 90.1%, 85.3..",
      "s11": "..%, 86.4% for SAM, U-Net, and DeepLabV3+, respectively). This indicates the limited generalization ability of such specialist models on unseen targets. In contrast, MedSAM consistently delivers superior performance. For example, MedSAM obtained median DSC scores of 87.8% (IQR: 85.0-91.4%) on the nasopharynx cancer segmentation task, demonstrating 52.3%, 15.5%, and 22.7 improvements over SAM, the specialist U-Net, and DeepLabV3+, respectively. Significantly, MedSAM also achieved better performance in some unseen modalities (e.g., abdomen T1 Inphase and Outphase), surpassing SAM and the specialist models with improvements by up to 10%. Figure 4c presents four randomly selected segmentation examples for qualitative evaluation, revealing that while all the methods have the ability to handle simple segmentation targets, MedSAM performs better at segmenting challenging targets with indistinguishable boundaries, such as cervical cancer in MR images (more examples are presented in Supplementary Fig. 13). Furthermore, we evaluated MedSAM on the multiple myeloma plasma cell dataset, which represents a distinct modality and task in contrast to all previously leveraged validation tasks. Although this task had never been..",
      "s12": "..seen during training, a b Image encoder Bounding box prompts Mask decoder Prompt encoder Input Image Segmentation Image embedding Fig. 2 | Overview of the modality distribution in the dataset and the network architecture. a The number of medical image-mask pairs in each modality. b MedSAM is a promptable segmentation method where users can use bounding boxes to specify the segmentation targets. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 3 c a b SAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM Fig. 4 | Quantitative and qualitative evaluation results on the external validation set. a Performance distribution of 60 external validation tasks in terms of median dice similarity coefficient (DSC) score. The center line within the box represents the median value, with the bottom and top bounds of the box delineating the 25th and 75th percentiles, respectively. Whiskers are chosen to show the 1.5 of the interquartile range. Up-triangles denote the minima and downtriangles denote the maxima. b Podium plots for visualizing the performance correspondence of 60 external validation tasks. Upper part:..",
      "s13": "..each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical tasks are connected by a line. Lower part: bar charts represent the frequency of achieved ranks for each method. MedSAM ranks in the first place on most tasks. c Visualized segmentation examples on the external validation set. The four examples are the lymph node, cervical cancer, fetal head, and polyp in CT, MR, ultrasound, and endoscopy images, respectively. Source data are provided as a Source Data file. SAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM a c b Fig. 3 | Quantitative and qualitative evaluation results on the internal validation set. a Performance distribution of 86 internal validation tasks in terms of median dice similarity coefficient (DSC) score. The center line within the box represents the median value, with the bottom and top bounds of the box delineating the 25th and 75th percentiles, respectively. Whiskers are chosen to show the 1.5 of the interquartile range. Up-triangles denote the minima and downtriangles denote the maxima. b Podium plots for visualizing the performance correspondence of 86 internal validation tasks. Upper part: each colored dot denotes the median DSC achieved with the..",
      "s14": "..respective method on one task. Dots corresponding to identical tasks are connected by a line. Lower part: bar charts represent the frequency of achieved ranks for each method. MedSAM ranks in the first place on most tasks. c Visualized segmentation examples on the internal validation set. The four examples are liver cancer, brain cancer, breast cancer, and polyp in computed tomography (CT), (Magnetic Resonance Imaging) MRI, ultrasound, and endoscopy images, respectively. Blue: bounding box prompts; Yellow: segmentation results. Magenta: expert annotations. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 4 MedSAM still exhibited superior performance compared to the SAM (Supplementary Fig. 14), highlighting its remarkable generalization ability. The effect of training dataset size We also investigated the effect of varying dataset sizes on MedSAM\u2019s performance because the training dataset size has been proven to be pivotal in model performance28. We additionally trained MedSAM on two different dataset sizes: 10,000 (10K) and 100,000 (100K) images and their performances were compared with the default MedSAM model. The 10K and 100K training..",
      "s15": "..images were uniformly sampled from the whole training set, to maintain data diversity. As shown in (Fig. 5a) (Supplementary Tables 12\u201314), the performance adhered to the scaling rule, where increasing the number of training images significantly improved the performance in both internal and external validation sets. MedSAM can improve the annotation efficiency Furthermore, we conducted a human annotation study to assess the time cost of two pipelines (Methods). For the first pipeline, two human experts manually annotate 3D adrenal tumors in a slice-by-slice way. For the second pipeline, the experts first drew the long and short tumor axes with the linear marker (initial marker) every 3-10 slices, which is a common practice in tumor response evaluation. Then, MedSAM was used to segment the tumors based on these sparse linear annotations. Finally, the expert manually revised the segmentation results until they were satisfied. We quantitatively compared the annotation time cost between the two pipelines (Fig. 5b). The results demonstrate that with the assistance of MedSAM, the annotation time is substantially reduced by 82.37% and 82.95% for the two experts, respectively. Discussion We introduce MedSAM, a deep learning-powered foundation model designed for the segmentation of a wide array of anatomical structures.."
    },
    "eval_answer": "**Limitations of Current Task-Specific Medical Image Segmentation Models:**  \n- **Data Limitations:** Current models often suffer from limited training datasets, which can hinder their performance in medical imaging tasks [S3469843940].  \n- **Inconsistency Across Modalities:** Task-specific models may not generalize well across different imaging modalities, leading to inefficiencies in clinical workflows [S3330376286].  \n- **Noise Sensitivity:** Medical images frequently contain noise, and many existing segmentation models struggle to effectively address this issue [S4713353038].  \n- **Lack of Standardization:** The absence of standardized imaging protocols can lead to variability in model performance and reliability [S4163962705].  \n\n**Advancements in Natural Image Segmentation That Can Inform Medical Imaging Models:**  \n- **Deep Learning Techniques:** The use of convolutional neural networks (CNNs) has proven effective in natural image segmentation and can enhance accuracy and efficiency in medical image segmentation [S2257874559].  \n- **Transfer Learning:** Leveraging large-scale natural image datasets, such as ImageNet, can significantly boost the performance of medical segmentation models, especially when training data is scarce [S3047772571] [S3227768826].  \n- **Fully Convolutional Networks (FCNs):** FCNs have shown substantial improvements in segmentation accuracy and computational efficiency, making them a promising approach for medical imaging applications [S4612206001] [S4915181933].  \n- **Multimodal Data Integration:** Incorporating diverse data sources can enhance the robustness and clinical utility of segmentation models, a practice well-established in natural image segmentation [S3330376286] [S5350334166].  \n- **Data Augmentation Strategies:** Techniques commonly used in natural image segmentation, such as data augmentation, can help mitigate the challenges posed by limited datasets in medical imaging [S3469843940].  \n- **Adaptive Algorithms:** Methods like adaptive fuzzy c-means algorithms can improve segmentation robustness by addressing noise issues prevalent in medical images [S4713353038].  \n\n**Conclusion:**  \nTo enhance the efficiency and consistency of clinical workflows, it is essential to address the limitations of current task-specific medical image segmentation models by integrating advancements from natural image segmentation. Techniques such as transfer learning, FCNs, and multimodal data integration can significantly improve model performance and robustness in medical imaging tasks.",
    "eval_references": {
      "S3227768826": {
        "id": "S3227768826",
        "text": "The integration of transfer learning from natural image datasets, such as ImageNet, into medical imaging segmentation tasks has been shown to improve performance, particularly in scenarios with limited training data.",
        "children": [
          {
            "id": "E1735529591",
            "text": "..part recognition system is also presented in Yan et al. [75]. A multistage deep learning framework based on CNNs extracts both the patches with the most as well as least discriminative local patches in the pretraining stage. Subsequently, a boosting stage exploits this local information 12 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 21, NO. 1, JANUARY 2017 to improve performance. The authors point out that training based on discriminative local appearances are more accurate compared to the usage of global image context. CNNs have also been proposed for the segmentation of isointense stage brain tissues [131] and brain extraction from multimodality MR images [56]. Hybrid approaches that combine CNNs with other architectures are also proposed. In [66], a deep learning algorithm is employed to encode the parameters of a deformable model and thus facilitate the segmentation of the left ventricle (LV) from short-axis cardiac MRI. CNNs are employed to automatically detect the LV, whereas deep Autoencoders are utilized to infer its shape. Yu et al. [67] designed a wireless capsule endoscopy classification system based on a hybrid CNN with extreme learning machine (ELM). The CNN constitutes a data-driven feature extractor, whereas the cascaded ELM acts as a strong classifier. A comparison between different CNNs architectures concluded that deep CNNs of up to 22 layers can be useful even with limited training datasets [73]. More detailed description of various CNNs architectures proposed in medical imaging analysis is presented in previous survey [58]. The key challenges and limitations are: 1) CNNs are designed for 2-D images whereas segmentation problems in MRI and CT are inherently 3-D. This problem is further complicated by the anisotropic voxel size. Although the creation of isotropic images by interpolating the data is a possibility, it can result in severely blurred images. Another solution is to train the CNNs on orthogonal patches extracted from axial, sagittal and coronal views [62], [132]. This approach also drastically reduces the time complexity required to process 3-D information and thus alleviates the problem of overfitting. 2) CNNs do not model spatial dependencies. Therefore, several approaches have incorporated voxel neighboring information either implicitly or by adding a pairwise term in the cost function, which is referred as conditional random field [85]. 3) Preprocessing to bring all subjects and imaging modalities to similar distribution is still a crucial step that affects the classification performance. Similarly to conventional machine learning approaches, balancing the datasets with bootstrapping and selecting samples with high entropy is advantageous. Perhaps, all of these limitations result from or are exacerbated by small and incomplete training datasets. Furthermore, there is limited availability of ground-truth/annotated data, since the cost and time to collect and manually annotate medical images is prohibitively large. Manual annotations are subjective and highly variable across medical experts. Although, it is thought that the manual annotation would require highly specialized knowledge in medicine and medical imaging physics, recent studies suggest that nonprofessional users could perform similarly [76]. Therefore, crowdsourcing is suggested as a viable alternative to create low-cost, big ground-truth medical imaging datasets. Moreover, the normal class is often over represented since the healthy tissue usually dominates and forms highly repetitive patterns. These issues result in slow convergence and overfitting. To alleviate the lack of training samples, transfer learning via fine tuning have been suggested in medical imaging applications [58], [72], [74], [76]. In transfer learning via fine-tuning, a CNN is pretrained using a database of labeled natural images. The use of natural images to train CNNs in medical imaging is controversial because of the profound difference between natural and medical images. Nevertheless, Tajbakhsh et al. [74] showed that fine-tuned CNNs based on natural images are less prone to overfitting due to the limited size training medical imaging sets and perform similarly or better than CNNs trained from scratch. Shin et al. [73] has applied transfer learning from natural images in thoraco-abdominal lymph node detection and interstitial lung disease classification. They also reported better results than training the CNNs from scratch with more consistent performances of validation loss and accuracy traces. Chen et al. [72] applied successfully a transfer learning strategy to identify the fetal abdominal standard plane. The lower layers of a CNN are pretrained based on natural images. The approach shows improved capability of the algorithm to encode the complicated appearance of the abdominal plane. Multitask training has also been suggested to handle the class imbalance common in CAD applications. Multitasking refers to the idea of solving different classification problems simultaneously and it results in a drastic reduction of free parameters [133]. Although CNNs have dominated medical image analysis applications, other deep learning approaches/architectures have also been applied successfully. In a recent paper, a stacked denoising autoencoder was proposed for the diagnosis of benign malignant breast lesions in ultrasound images and pulmonary nodules in CT scans [77]. The method outperforms classical CAD approaches, largely due to the automatic feature extraction and noise tolerance. Furthermore, it eliminates the image segmentation process to obtain a lesion boundary. Shan et al. [53] presented a stacked sparse autoencoder for microaneurysms detection in fundus images as an instance of a diabetic retinopathy strategy. The proposed method learns high-level distinguishing features based only on pixel intensities. Various autoencoder-based learning approaches have also been applied to the automatic extraction of biomarkers from brain images and the diagnosis of neurological diseases. These methods often use available public domain brain image databases such as the Alzheimer\u2019s disease neuroimaging initiative database. For example, a deep Autoencoder combined with a softmax output layer for regression is proposed for the diagnosis of Alzheimer\u2019s disease. Hu et al. [134] also used autoencoders for Alzheimer\u2019s disease prediction based on Functional Magnetic Resonance Images (fMRI). The results show that the proposed method achieves much better classification than the traditional means. On the other hand, Li et al. [61] proposed an RBM approach that identifies biomarkers from MRI and positron emission tomography (PET) scans. They obtained an improvement of about 6% in classification accuracy compared to the standard approaches. Kuang et al. [60] proposed an RBM approach for fMRI data to discriminate attention deficit hyperactivity disorder. The system is capable of predicting the subjects as control, combined, inattentive or hyperactive through their RAV`\u0131 et al.: DEEP LEARNING FOR HEALTH INFORMATICS 13 frequency features. Suk et al. [59] proposed a DBM to extract a latent hierarchical feature representation from 3-D patches of brain images. Low level image processing, such as image segmentation and registration can also benefit from deep learning models. Brosch et al. [64] described a manifold learning approach of 3-D brain images based on DBN. It is different than other methods because it does not require a locally linear manifold space. Mansoor et al. [54] developed a fully automated shape model segmentation mechanism for the analysis of cranial nerve systems. The deep learning approach outperforms conventional methods particularly in regions with low contrast, such as optic tracts and areas with pathology. In [135], a pipeline is proposed for object detection and segmentation in the context of automatically processing volumetric images. A novel framework called marginal space deep learning implements an object parameterization in hierarchical marginal spaces combined with automatic feature detection based on deep learning. In [84], a DNN architecture called input\u2013output deep architecture is described to solve the image labelling problem. A single NN forward step is used to assign a label to each pixel. This method avoids the handcrafted subjective design of a model with a deep learning mechanism, which automatically extracts the dependencies between labels. Deep learning is also used for processing hyperspectral images [83]. Spectral and spatial learned features are combined together in a hierarchical model to characterize tissues or materials. In [78], a hybrid multilayered group method of data handling, which is a special NN with polynomial activation functions, has been used together with a principal component-regression analysis to recognize the liver and spleen. A similar approach is used for the identification of the myocardium [79] as well as the right and left kidney regions [80]. The authors extend the method to analyze brain or lung CT images to detect cancer [81]. Zhen et al. [63] presents a framework for direct biventricular volume estimation, which avoids the need of user inputs and over simplification assumptions. The learning process involves unsupervised cardiac image representation with multiscale deep networks and direct biventricular volume estimation with RF. Rose et al. [82] propose a methodology for hierarchical clustering in application to mammographic image data. Classification is performed based on a deep learning architecture along with a standard NN. In general, deep learning in medical imaging provides automatic discovery of object features and automatic exploration of feature hierarch and interaction. In this way, a relatively simple training process and a systematic performance tuning can be used, making deep learning approaches improve over the state-of-the art. However, in medical imaging analysis, their potentials have not been unfolded fully. To be successful in disease detection and classification approaches, deep learning requires the availability of large labeled datasets. Annotating imaging datasets is an extremely time-consuming and costly process that is normally undertaken by medical doctors. Currently, there is a lot of debate on whether to increase the number of annotated datasets with the help of non-experts (crowd-sourcing) and how to standardize the available images to allow objective assessment of the deep learning approaches. Fig. 6. Data for health monitoring applications can be captured using a wide array of pervasive sensors that are worn on the body, implanted, or captured through ambient sensors, e.g., inertial motion sensors, ECG patches, smart-watches, EEG, and prosthetics. C. Pervasive Sensing for Health and Wellbeing Pervasive sensors, such as wearable, implantable, and ambient sensors [136] allow continuous monitoring of health and wellbeing, Fig. 6. An accurate estimation of food intake and energy expenditure throughout the day, for example, can help tackle obesity and improve personal wellbeing. For elderly..",
            "url": "https://eprints.gla.ac.uk/208066/1/208066.pdf"
          },
          {
            "id": "S3330376286",
            "text": "The integration of multimodal data sources in medical imaging, as highlighted in the context of radiomics, can improve the robustness and clinical utility of segmentation models, suggesting that techniques from natural image segmentation should also consider data diversity.",
            "children": [
              {
                "id": "E6769120955",
                "text": "..for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics. REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 749 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Exploratory analysis Medical imaging Data selection VOI Prediction target Imaging protocols Test feature stability Report protocols Add prediction target Report algorithms Store data Robust segmentation Add clinical variables Archetypal features Feature selection Internal validation Radiomics External validation Report methodology Modeling Feature extraction RQS 1 RQS 2 RQS 3 RQS Total 36 Image protocol quality +1 or +2 RQS checkpoint 1 total: 2 +1 +1 +1 RQS checkpoint 2 total: 3 Multiple segmentation Phantom study Imaging at multiple time points Feature reduction or adjustment for multiple testing Multivariable analysis Biological correlates Cut-off analysis Discrimination statistics Calibration statistics Prospective study Validation Comparison to \u2018gold standard\u2019 Costeffectiveness analysis Potential clinical applications Open science and data -3 or +3 +1 +1 +1 +1 or +2 +1 or +2 +7 -5 to +5 +2 +2 +1 +1 to +4 RQS checkpoint 3 total: 31 Nature Reviews | Clinical Oncology evaluation of their performance, reproducibility, and/or clinical utility17. Consequently, these models might not be appropriate for CDSS. In this Review, we describe the process of radiomics along with latest developments in the field. The pitfalls, challenges, and opportunities presented by radiomics to improve CDSS for personalized precision oncology are highlighted, with an emphasis on the methodological aspects of radiomics prediction model development and validation. We explore the advanced and innovative information technologies that are essential for the data management of diverse multimodal data sources. Finally, we offer a vision of the necessary steps to ensure continued progression and widespread acceptance of both radiomics and CDSS. The workflow of radiomics Radiomics is defined as the quantitative mapping, that is, extraction, analysis and modelling of many medical image features in relation to prediction targets, such as clinical end points and genomic features. A radiomics study can be structured in five phases: data selection, medical imaging, feature extraction, exploratory analysis, and modelling (FIG. 1). To assess the quality of radiomics studies, we propose the radiomics quality score (RQS). Data selection Radiomic analyses begins with the choice of an imaging protocol, the volume of interest (VOI) and a prediction target \u2014 the event one wishes to predict. Typically, the entire primary tumour is analysed and linked to available data on treatment outcomes, such as survival. Radiomic analyses can be performed on subregions of the tumour (habitats), metastatic lesions, as well as in normal tissues. Analysis of these regions might yield radiosensitive phenotypes, which has implications for treatment planning strategies. Radiomics analysis, however, is not restricted to radiotherapy and can be applied to any image generated in the clinical setting (FIG. 2). The importance of using standardized imaging protocols to eliminate unnecessary confounding variability is recognized9,18; however, nonstandardized imaging protocols are commonplace. Therefore, reproducibility and Key points \u2022 Radiomics is becoming increasingly more important in medical imaging \u2022 The explosion of medical imaging data creates an environment ideal for machine-learning and data-based science \u2022 Radiomics-based decision-support systems for precision diagnosis and treatment can be a powerful tool in modern medicine \u2022 Large-scale data sharing is necessary for the validation and full potential that radiomics represents \u2022 Standardized data collection, evaluation criteria, and reporting guidelines are required for radiomics to mature as a discipline Figure 1 | Flowchart depicting the workflow of radiomics and the application of the RQS. The workflow includes the necessary steps in a radiomic analysis. The RQS both rewards and penalizes the methodology and analyses of a study, consequently encouraging the best scientific practice. RSQ, radiomics quality score; VOI, volume of interest. REVIEWS 750 | DECEMBER 2017 | VOLUME 14 www.nature.com/nrclinonc \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mi t e d, p a rt o f S p ri n g e r N a t u r e. Al l ri g h t s r e s e r v e d. Nature Reviews | Clinical Oncology Cardiac CT Radiomics MACE Analysis Phantom studies An artificial structure that imitates human tissue properties is scanned on multiple machines to characterize scan output against a known physical standard. comparability of radiomic studies can be achieved only by extensive disclosure of imaging protocols. We wish to emphasize this point, and provide examples of how protocols should be reported in future radiomics studies (Supplementary information S1). Medical imaging Segmentation. VOIs are segmented manually or (semi-) automatically19. This segmentation determines which voxels within an image are analysed, thus, the variability in segmentation can introduce bias in the evaluation of derived radiomic features20. Multiple-segmentation is a method to limit the extent of this bias. Examples that enable robust features to be observed21 include: evaluation by multiple clinicians, perturb segmentations with noise, combination of diverse algorithms, or use different stages of the breathing cycle. Key considerations are how the segmentation was performed, and how sensitive the radiomics analysis is to different segmentation methods22. For example, a semiautomatic segmentation method can result in different radiomic features than a manual delineation. Phantom studies. The determination of inter-scanner and inter-vendor variability of features is important in radiomics23. In cases in which radiomic studies rely on data from multiple scanners, neglecting this variability can jeopardize the analysis of studies \u2014 that is, the proposed radiomic-based prediction model might not perform adequately on external datasets if new data are acquired on different scanners. As data from patients scanned on multiple devices is scarce and subject to uncertainties (such as organ motion, or different imaging protocols), phantom studies are a suitable means to gauge these uncertainties and identify features that rely on the vendor. In essence, phantom studies provide a risk-mitigation strategy to help navigate from the current clinical imaging scenario to the desired optimal imaging scenario. Imaging at multiple time points. Additional sources of variability in radiomics features are organ motion or expansion or shrinkage of the target volume. Radiomics features that are strongly dependent on these factors can have limited applicability. To account for these sources of variability, available test-retest data24\u201326 can be exploited to measure radiomics feature stability. For example, two datasets of images acquired within a small period of time from a patient cohort. Feature extraction The essence of radiomics is the high-throughput extraction of quantitative image features to characterize VOIs. Feature values are dependent upon factors that can include image pre-processing (for example, filtering, or intensity discretization) and reconstruction (for example, filtered back projection, or iterative reconstruction). Furthermore, variation exists in feature nomenclature, mathematical definition, methodology, and software implementation of the applied feature extraction algorithms27\u201329. In order to facilitate inter-operability of radiomic features, differences in nomenclature, algorithms, software implementations, as well as other methodological aspects must be elucidated. Exploratory analysis Radiomic and non-radiomic features should be combined with the prediction target to create a single dataset. This approach enables the investigation of relationships between features. Groups of highly correlated radiomics features can be identified via clustering, and these features can be reduced to single archetypal features per cluster. Radiomic features that are well-correlated with routine clinical features (such as tumour stage) do not provide additional information. Auxiliary feature data collected from multiple segmentations, multiple imaging, and phantom studies, can be exploited to assess feature robustness. Volatile or robust features can be identified and subsequently excluded from model development. For example, a feature that is robust for the prediction of overall survival for lung cancer (that is, imaged and segmented in a certain way) for a given dataset could be volatile for the prediction of pneumonitis in lung cancer (imaged and segmented in an alternative way) for a given dataset. Thus, the process of feature reduction and/or exclusion should be described clearly. Modelling Radiomic modelling involves three major aspects: feature selection, modelling methodology, and validation. Feature selection should be data-driven owing to the vast in- human range of possible radiomics features; such analysis should be performed in a robust and transparent manner. To achieve holistic models, features beyond radiomics (such as data from clinical records, data obtained during treatment or biological and/or genetic) should also be incorporated. Regarding the choice of modelling methodology, the identification of optimal machinelearning methods for radiomic applications is a crucial step towards stable and clinically relevant CDSS; thus, in the ideal scenario, multiple machine-learning methods should be employed30 and the implementation should be comprehensively documented. A non-validated model is Figure 2 |Radiomics in cardiology. The current gold standard for quantification of coronary calcifications visible on CT is the \u2018Agatston\u2019 method (based upon intensity and volume). Radiomic features can improve quantification, differentiation between calcified and non-calcified plaques, and thus the prediction of Major Adverse Cardiac Events (MACE). REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 751 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Calibration-in-the-large Describes whether the predictions deviate systematically (intercept), whereas the calibration slope should ideally be equal to 1. The independence assumption The definition in terms of conditional probabilities is..",
                "url": "https://cris.maastrichtuniversity.nl/ws/files/36170434/Lambin_2017_Radiomics_the_Bridge_Between.pdf"
              },
              {
                "id": "E7176429902",
                "text": "..on both clinical applications and scientific researches. Recent progresses in deep learning have shed new light on medical image analysis by allowing discovering morphological and/or textural patterns in images solely from data. As deep learning methods have achieved the state-ofthe-art performance over different medical applications, its use for further improvement can be the major step in the medical computing field. However, there are still rooms for improvements. First, lessoned in computer vision, where breakthrough improvements were achieved by exploiting large amounts of training data, e.g., more than 1 million annotated images in ImageNet (19), it would be one direction to build such big publicly available dataset of medical images, by which deep models can find more generalized features in medical images, thus allowing making a leap in performance. Second, while the data-driven feature representations, especially in an unsupervised manner, helped enhance accuracy, it is also desirable to devise a new methodological architecture, with which it becomes possible to reflect or involve the domain-specific knowledge. Third, it is also necessary to develop algorithmic techniques to efficiently handle images acquired with different scanning protocols, by which there is no need to train modality-specific deep models. Last but not least, when applying deep learning to investigate the underlying patterns in images such as fMRI, due to the black-box like characteristics of deep models, it still remains challenging to understand and interpret the learned models intuitively. Shen et al. Page 19 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Acknowledgments This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.B0101-16-0307, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)). This work was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2015R1C1A1A01052216), NIH grants (EB006733, EB008374, EB009634, MH100217, MH108914, AG041721, AG049371, AG042599, DE022676). LITERATURE CITED 1. Brody H. Medical imaging. Nature. 2013; 502:S81\u2013S81. [PubMed: 24187698] 2. Schmidhuber J. Deep learning in neural networks: An overview. Neural Networks. 2015; 61:85\u2013117. [PubMed: 25462637] 3. Bengio Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning. 2009; 2:1\u2013127. 4. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521:436\u2013444. [PubMed: 26017442] 5. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006; 313:504\u2013507. [PubMed: 16873662] 6. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 2010; 11:3371\u20133408. 7. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning (ICML). 2010 8. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research. 2014; 15:1929\u2013 1958. 9. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Proceedings of International Conference on Machine Learning (ICML). 2015 10. Bishop, CM. Neural networks for pattern recognition. Oxford University Press, Inc; 1995. 11. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of International Conference on Machine Learning (ICML). 2008 12. Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. Proceedings of International Conference on Machine Learning (ICML). 2011 13. Hinton GE, Deng L, Yu D, Dahl GE, Mohamed A, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine. 2012; 29:82\u201397. 14. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Proceedings of Neural Information Processing Systems (NIPS). 2013:2553\u20132561. 15. Taigman Y, Yang M, Ranzato M, Wolf L. Deepface: Closing the gap to human-level performance in face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014 16. Zhang J, Zong C. Deep neural networks in machine translation: An overview. IEEE Intelligent Systems. 2015; 30:16\u201325. 17. Karpathy A, Li F. Deep visual-semantic alignments for generating image descriptions. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015 18. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, et al. Mastering the game of Go with deep neural networks and tree search. Nature. 2016; 529:484\u2013489. [PubMed: 26819042] 19. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015; 115:211\u2013252. 20. Everingham, M., Van Gool, L., Williams, CKI., Winn, J., Zisserman, A. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 2012. http://www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html Shen et al. Page 20 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript 21. Roux, L., Racoceanu, D., Capron, F., Calvo, J., Attieh, E., et al. MITOS-ATYPIA-14. 2014. http:// mitos-atypia-14.grand-challenge.org/ 22. Zhang W, Li R, Deng H, Wang L, Lin W, et al. Deep convolutional neural networks for multimodality isointense infant brain image segmentation. NeuroImage. 2015; 108:214\u2013224. [PubMed: 25562829] 23. Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, et al. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage. 2016; 129:460\u2013469. [PubMed: 26808333] 24. Wu G, Kim M, Wang Q, Munsell BC, Shen D. Scalable high-performance image registration framework by unsupervised deep feature representations learning. IEEE Transactions on Biomedical Engineering. 2016; 63:1505\u20131516. [PubMed: 26552069] 25. Suk HI, Lee SW, Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014; 101:569\u2013582. [PubMed: 25042445] 26. Shin H, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016 27. Suk HI, Lee SW, Shen D. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Structure and Function. 2015; 220:841\u2013859. [PubMed: 24363140] 28. Suk HI, Shen D. Deep learning in diagnosis of brain disorders. Recent Progress in Brain and Cognitive Engineering. 2015:203\u2013213. 29. Suk HI, Wee CY, Lee SW, Shen D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage. 2016; 129:292\u2013307. [PubMed: 26774612] 30. Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Transactions on Medical Imaging. 2016; 35:1240\u20131251. [PubMed: 26960222] 31. van Tulder G, de Bruijne M. Combining generative and discriminative representation learning for lung CT analysis with convolutional restricted boltzmann machines. IEEE Transactions on Medical Imaging. 2016; 35:1262\u20131272. [PubMed: 26886968] 32. Dou Q, Chen H, Yu L, Zhao L, Qin J, et al. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE Transactions on Medical Imaging. 2016; 35:1182\u20131195. [PubMed: 26886975] 33. Cire\u015fan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI). 2013:411\u2013418. 34. Chen H, Qi X, Cheng JZ, Heng PA. Deep contextual networks for neuronal structure segmentation. Proceedings of AAAI Conference on Artificial Intelligence. 2016 35. Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, et al. Computer-aided diagnosis with deep learning architecture..",
                "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
              }
            ]
          }
        ]
      },
      "S2257874559": {
        "id": "S2257874559",
        "text": "Deep learning techniques, particularly convolutional neural networks (CNNs), can significantly enhance the accuracy and efficiency of medical image segmentation by leveraging hierarchical feature representations learned from data, which is a method proven effective in natural image segmentation.",
        "children": [
          {
            "id": "E7176429902",
            "text": "..on both clinical applications and scientific researches. Recent progresses in deep learning have shed new light on medical image analysis by allowing discovering morphological and/or textural patterns in images solely from data. As deep learning methods have achieved the state-ofthe-art performance over different medical applications, its use for further improvement can be the major step in the medical computing field. However, there are still rooms for improvements. First, lessoned in computer vision, where breakthrough improvements were achieved by exploiting large amounts of training data, e.g., more than 1 million annotated images in ImageNet (19), it would be one direction to build such big publicly available dataset of medical images, by which deep models can find more generalized features in medical images, thus allowing making a leap in performance. Second, while the data-driven feature representations, especially in an unsupervised manner, helped enhance accuracy, it is also desirable to devise a new methodological architecture, with which it becomes possible to reflect or involve the domain-specific knowledge. Third, it is also necessary to develop algorithmic techniques to efficiently handle images acquired with different scanning protocols, by which there is no need to train modality-specific deep models. Last but not least, when applying deep learning to investigate the underlying patterns in images such as fMRI, due to the black-box like characteristics of deep models, it still remains challenging to understand and interpret the learned models intuitively. Shen et al. Page 19 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Acknowledgments This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.B0101-16-0307, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)). This work was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2015R1C1A1A01052216), NIH grants (EB006733, EB008374, EB009634, MH100217, MH108914, AG041721, AG049371, AG042599, DE022676). LITERATURE CITED 1. Brody H. Medical imaging. Nature. 2013; 502:S81\u2013S81. [PubMed: 24187698] 2. Schmidhuber J. Deep learning in neural networks: An overview. Neural Networks. 2015; 61:85\u2013117. [PubMed: 25462637] 3. Bengio Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning. 2009; 2:1\u2013127. 4. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521:436\u2013444. [PubMed: 26017442] 5. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006; 313:504\u2013507. [PubMed: 16873662] 6. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 2010; 11:3371\u20133408. 7. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning (ICML). 2010 8. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research. 2014; 15:1929\u2013 1958. 9. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Proceedings of International Conference on Machine Learning (ICML). 2015 10. Bishop, CM. Neural networks for pattern recognition. Oxford University Press, Inc; 1995. 11. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of International Conference on Machine Learning (ICML). 2008 12. Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. Proceedings of International Conference on Machine Learning (ICML). 2011 13. Hinton GE, Deng L, Yu D, Dahl GE, Mohamed A, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine. 2012; 29:82\u201397. 14. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Proceedings of Neural Information Processing Systems (NIPS). 2013:2553\u20132561. 15. Taigman Y, Yang M, Ranzato M, Wolf L. Deepface: Closing the gap to human-level performance in face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014 16. Zhang J, Zong C. Deep neural networks in machine translation: An overview. IEEE Intelligent Systems. 2015; 30:16\u201325. 17. Karpathy A, Li F. Deep visual-semantic alignments for generating image descriptions. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015 18. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, et al. Mastering the game of Go with deep neural networks and tree search. Nature. 2016; 529:484\u2013489. [PubMed: 26819042] 19. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015; 115:211\u2013252. 20. Everingham, M., Van Gool, L., Williams, CKI., Winn, J., Zisserman, A. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 2012. http://www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html Shen et al. Page 20 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript 21. Roux, L., Racoceanu, D., Capron, F., Calvo, J., Attieh, E., et al. MITOS-ATYPIA-14. 2014. http:// mitos-atypia-14.grand-challenge.org/ 22. Zhang W, Li R, Deng H, Wang L, Lin W, et al. Deep convolutional neural networks for multimodality isointense infant brain image segmentation. NeuroImage. 2015; 108:214\u2013224. [PubMed: 25562829] 23. Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, et al. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage. 2016; 129:460\u2013469. [PubMed: 26808333] 24. Wu G, Kim M, Wang Q, Munsell BC, Shen D. Scalable high-performance image registration framework by unsupervised deep feature representations learning. IEEE Transactions on Biomedical Engineering. 2016; 63:1505\u20131516. [PubMed: 26552069] 25. Suk HI, Lee SW, Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014; 101:569\u2013582. [PubMed: 25042445] 26. Shin H, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016 27. Suk HI, Lee SW, Shen D. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Structure and Function. 2015; 220:841\u2013859. [PubMed: 24363140] 28. Suk HI, Shen D. Deep learning in diagnosis of brain disorders. Recent Progress in Brain and Cognitive Engineering. 2015:203\u2013213. 29. Suk HI, Wee CY, Lee SW, Shen D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage. 2016; 129:292\u2013307. [PubMed: 26774612] 30. Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Transactions on Medical Imaging. 2016; 35:1240\u20131251. [PubMed: 26960222] 31. van Tulder G, de Bruijne M. Combining generative and discriminative representation learning for lung CT analysis with convolutional restricted boltzmann machines. IEEE Transactions on Medical Imaging. 2016; 35:1262\u20131272. [PubMed: 26886968] 32. Dou Q, Chen H, Yu L, Zhao L, Qin J, et al. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE Transactions on Medical Imaging. 2016; 35:1182\u20131195. [PubMed: 26886975] 33. Cire\u015fan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI). 2013:411\u2013418. 34. Chen H, Qi X, Cheng JZ, Heng PA. Deep contextual networks for neuronal structure segmentation. Proceedings of AAAI Conference on Artificial Intelligence. 2016 35. Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, et al. Computer-aided diagnosis with deep learning architecture..",
            "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
          },
          {
            "id": "E4812619525",
            "text": "Deep Learning in Medical Image Analysis Dinggang Shen1,3, Guorong Wu2, and Heung-Il Suk3 1Department of Radiology, University of North Carolina at Chapel Hill, NC, USA, 27599 2Department of Radiology, University of North Carolina at Chapel Hill, NC, USA, 27599 3Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea, 02841 Abstract The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements. Keywords Medical image analysis; deep learning; unsupervised feature learning 1. INTRODUCTION Over the last decades, we have witnessed the importance of medical imaging, e.g., computed tomography (CT), magnetic resonance (MR), positron emission tomography (PET), mammography, ultrasound, X-ray, and so on, for the early detection, diagnosis, and treatment of diseases (1). In the clinic, the medical image interpretation has mostly been performed by human experts such as radiologists and physicians. However, due to large variations in pathology and potential fatigue of human experts, researchers and doctors have recently begun to benefit from computer-assisted interventions. While, compared to the advances in medical imaging technologies, it is belated for the advances in computational D. Shen and H.-I. Suk are the co-corresponding authors DISCLOSURE STATEMENT The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review. HHS Public Access Author manuscript Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Published in final edited form as: Annu Rev Biomed Eng. 2017 June 21; 19: 221\u2013248. doi:10.1146/annurev-bioeng-071516-044442. Author Manuscript Author Manuscript Author Manuscript Author Manuscript medical image analysis, it has recently been improving with the help of machine learning techniques. In the stream of applying machine learning for data analysis, meaningful feature extraction or feature representation lies at the heart of its success to accomplish target tasks. Conventionally, meaningful or task-related features were mostly designed by human experts based on their knowledge about the target domains, which thus made it challenging for nonexperts to exploit machine learning techniques for their own studies. However, deep learning (2) has relieved such obstacles by absorbing the feature engineering step into a learning step. That is, instead of extracting features in a hand-designed manner, deep learning requires only a set of data with minor preprocessing, if necessary, and then discovers the informative representations in a self-taught manner (3, 4). So, now the burden of feature engineering has shifted from a human-side to a computer-side, thus allowing non-experts in machine learning to effectively use deep learning for their own researches and/or applications, especially in medical image analysis. The unprecedented success of deep learning arises mostly from the following factors: (1) advancements of high-tech central processing units (CPUs) and graphics processing units (GPUs); (ii) availability of a huge amount of data (i.e., big data); (iii) developments of learning algorithms (5, 6, 7, 8, 9). Technically, deep learning can be regarded as an improvement of the conventional artificial neural networks (10) by building networks with multiple (more than two) layers. It is empirically shown that deep neural networks can discover hierarchical feature representations such that the higher level features can be derived from the lower level features (4). Thanks to its nice characteristic of learning hierarchical feature representations solely from data, deep learning has achieved recordbreaking performance in a variety of artificial intelligence applications (11, 12, 13, 14, 15, 16, 17, 18) and grand challenges (19, 20, 21). Particularly, great improvements in computer vision inspired its use to medical image analysis such as image segmentation (22, 23), image registration (24), image fusion (25), image annotation (26), computer-aided diagnosis and prognosis (27, 28, 29), lesion/landmark detection (30, 31, 32), and microscopic imaging analysis (33, 34), to name a few. Deep learning methods are highly effective when the number of available samples are large during a training stage. For example, in ImageNet Large Scale Visual Recognition Challenge (ILSVRC), more than 1 million annotated images were provided (19). However, as for medical applications, we usually have a very limited number of images, e.g., less than 1,000 images. Therefore, one of the main challenges in applying deep learning to medical images arises from the limited small number of available training samples to build deep models without suffering from overfitting. To this end, research groups have devised various strategies, such as (i) to take image patches either 2D or 3D as input (25, 35, 36, 37, 38, 39, 40, 41), rather than the full-sized images, to reduce the input dimensionality, thus the number of model parameters; (ii) to expand their dataset by artificially generating samples via affine transformation (i.e., data augmentation) and then train their network from scratch with the augmented dataset (35, 36, 37, 38); (iii) to use deep models trained over a huge number of natural images in computer vision as \u2018off-the-shelf\u2019 feature extractor and then train the final classifier or output layer with the target-task samples (39, 41); (iv) to initialize Shen et al. Page 2 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript model parameters with those of pre-trained models from non-medical or natural images and then fine-tune the network parameters with the task-related samples (42, 43); (v) to use models trained with small-sized inputs for arbitrarily-sized inputs by transforming weights in the fully connected layers into convolutional kernels (32, 44). In terms of the input types, we can categorize deep models as typical multi-layer neural networks that take input values in vector form (i.e., non-structured) and convolutional networks that takes 2D or 3D shaped (i.e., structured) values as input. Because of the structural characteristic of images (i.e., the structural or configural information among neighboring pixels or voxels is another important source of information), convolutional neural networks have gained great interest in medical image analysis (33, 45, 32, 46, 22, 44, 31). However, networks with vectorized inputs were also successfully applied to different medical applications (47, 25, 27, 29, 48, 24, 49, 50). Along with deep neural networks, deep generative models (51) such as deep belief networks and deep Boltzmann machines that are the probabilistic graphical models with multiple layers of hidden variables have also been successfully applied to brain disease diagnosis (43, 25, 52, 29), lesion segmentation (53, 45, 32, 54), cell segmentation (33, 55, 34, 56), image parsing (57, 58, 59), and tissue classification (31, 46, 22, 44). In this article, we first explain the computational theories of neural networks and deep models (e.g., stacked auto-encoder, deep belief network, deep Boltzmann machine, convolutional neural network) and their fundamentals of extracting high-level representations from data in Section 2. Section 3 introduces recent studies that exploited deep models for different applications in medical imaging by covering image registration, anatomy localization, lesion segmentation, object/cell detection, tissue segmentation, and computer-aided detection and diagnosis. Finally, we conclude this article by summarizing research trends and suggesting directions for further improvements in Section 4. 2. DEEP MODELS In this section, we explain the fundamental concepts of feed-forward neural networks and basic deep models in the literature. The contents are specifically focused on learning hierarchical feature representations from data. It is also described how to efficiently learn parameters of deep architecture by reducing overfitting. 2.1. Feed-forward neural networks In machine learning, artificial neural networks are a family of models that mimic the structural elegance of the neural system and learn patterns inherent in observations. The perceptron (60) is the earliest trainable neural network with a single-layer architecture1, composed of an input layer and an output layer. The perceptron or modified perceptron with multiple output units in Fig. 1(a) is regarded as a linear model, which prohibits their applications for tasks of involving complicated data patterns, despite the use of non-linear activation functions in the output layer. 1In general, the input layer is not counted. Shen et al. Page 3 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Such limitation is successfully circumvented by introducing the so-called \u2018hidden\u2019 layer between the input layer and the output layer. Note that in neural networks the units of the neighboring layers are fully connected to each other, but there are no connections among the units in the same layer. For a two-layer neural network in Fig. 1(b), also called as multi-layer perceptron, given..",
            "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
          }
        ]
      },
      "S3047772571": {
        "id": "S3047772571",
        "text": "The application of transfer learning from large-scale natural image datasets, such as ImageNet, has been shown to significantly improve the performance of convolutional neural networks in medical image segmentation tasks, particularly when training data is limited.",
        "children": [
          {
            "id": "E7827114786",
            "text": "Mohammad Havaei\nb\u00c9 cole Normale sup\u00e9rieure\nUniversit\u00e9 de Sherbrooke\nSherbrooke, ParisQcCanada, France\nAxel Davy\nDavid Warde-Farley\nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alCanada\ne\u00c9 cole Polytechnique de Montr\u00e9al\nd\u00c9 cole polytechnique\nPalaiseauFrance, Canada\nAntoine Biard\nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alCanada\ne\u00c9 cole Polytechnique de Montr\u00e9al\nd\u00c9 cole polytechnique\nPalaiseauFrance, Canada\nAaron Courville\nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alCanada\ne\u00c9 cole Polytechnique de Montr\u00e9al\nd\u00c9 cole polytechnique\nPalaiseauFrance, Canada\nYoshua Bengio\nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alCanada\ne\u00c9 cole Polytechnique de Montr\u00e9al\nd\u00c9 cole polytechnique\nPalaiseauFrance, Canada\nChris Pal\nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alCanada\ne\u00c9 cole Polytechnique de Montr\u00e9al\nd\u00c9 cole polytechnique\nPalaiseauFrance, Canada\nPierre-Marc Jodoin\nb\u00c9 cole Normale sup\u00e9rieure\nUniversit\u00e9 de Sherbrooke\nSherbrooke, ParisQcCanada, France\nHugo Larochelle\nb\u00c9 cole Normale sup\u00e9rieure\nUniversit\u00e9 de Sherbrooke\nSherbrooke, ParisQcCanada, France\nTwitterUSA\nBrain Tumor Segmentation with Deep Neural Networks $\nBrain tumor segmentation, deep neural networks\nIn this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test dataset reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.\nIntroduction\nIn the United States alone, it is estimated that 23,000 new cases of brain cancer will be diagnosed in 2015 2 . While gliomas are the most common brain tumors, they can be less aggressive (i.e. low grade) in a patient with a life expectancy of several years, or more aggressive (i.e. high grade) in a patient with a life expectancy of at most 2 years.\nAlthough surgery is the most common treatment for brain tumors, radiation and chemotherapy may be used to slow the growth of tumors that cannot be physically removed. Magnetic resonance imaging (MRI) provides detailed images of the brain, and is one of the most common tests used to diagnose brain tumors. All the more, brain tumor segmentation from MR images can have great impact for improved diagnostics, growth rate prediction and treatment planning.\nWhile some tumors such as meningiomas can be easily segmented, others like gliomas and glioblastomas are much more difficult to localize. These tumors (together with their surrounding edema) are often diffused, poorly contrasted, and extend tentacle-like structures that make them difficult to segment. Another fundamental difficulty with segmenting brain tumors is that they can appear anywhere in the brain, in almost any shape and size. Furthermore, unlike images derived from X-ray computed tomography (CT) scans, the scale of voxel values in MR images is not standardized. Depending on the type of MR machine used (1.5, 3 or 7 tesla) and the acquisition protocol (field of view value, voxel resolution, gradient strength, b0 value, etc.), the same tumorous cells may end up having drastically different grayscale values when pictured in different hospitals.\nHealthy brains are typically made of 3 types of tissues: the white matter, the gray matter, and the cerebrospinal fluid. The goal of brain tumor segmentation is to detect the location and extension of the tumor regions, namely active tumorous tissue (vascularized or not), necrotic tissue, and edema (swelling near the tumor). This is done by identifying abnormal areas when compared to normal tissue. Since glioblastomas are infiltrative tumors, their borders are often fuzzy and hard to distinguish from healthy tissues. As a solution, more than one MRI modality is often employed, e.g. T1 (spin-lattice relaxation), T1-contrasted (T1C), T2 (spin-spin relaxation), proton density (PD) contrast imaging, diffusion MRI (dMRI), and fluid attenuation inversion recovery (FLAIR) pulse sequences. The con-trast between these modalities gives almost a unique signature to each tissue type.\nMost automatic brain tumor segmentation methods use handdesigned features [15,32]. These methods implement a classical machine learning pipeline according to which features are first extracted and then given to a classifier whose training procedure does not affect the nature of those features. An alternative approach for designing task-adapted feature representations is to learn a hierarchy of increasingly complex features directly from in-domain data. Deep neural networks have been shown to excel at learning such feature hierarchies [7]. In this work, we apply this approach to learn feature hierarchies adapted specifically to the task of brain tumor segmentation that combine information across MRI modalities.\nSpecifically, we investigate several choices for training Convolutional Neural Networks (CNNs), which are Deep Neural Networks (DNNs) adapted to image data. We report their advantages, disadvantages and performance using well established metrics. Although CNNs first appeared over two decades ago [29], they have recently become a mainstay of the computer vision community due to their record-shattering performance in the ImageNet Large-Scale Visual Recognition Challenge [27]. While CNNs have also been successfully applied to segmentation problems [1,31,21,8], most of the previous work has focused on non-medical tasks and many involve architectures that are not well suited to medical imagery or brain tumor segmentation in particular. Our preliminary work on using convolutional neural networks for brain tumor segmentation together with two other methods using CNNs was presented in BRATS'14 workshop. However, those results were incomplete and required more investigation (More on this in chapter 2).\nIn this paper, we propose a number of specific CNN architectures for tackling brain tumor segmentation. Our architectures exploit the most recent advances in CNN design and training techniques, such as Maxout [18] hidden units and Dropout [42] regularization. We also investigate several architectures which take into account both the local shape of tumors as well as their context.\nOne problem with many machine learning methods is that they perform pixel classification without taking into account the local dependencies of labels (i.e. segmentation labels are conditionally independent given the input image). To account for this, one can employ structured output methods such as conditional random fields (CRFs), for which inference can be computationally expensive. Alternatively, one can model label dependencies by considering the pixel-wise probability estimates of an initial CNN as additional input to certain layers of a second DNN, forming a cascaded architecture. Since convolutions are efficient operations, this approach can be significantly faster than implementing a CRF.\nWe focus our experimental analysis on the fully-annotated MICCAI brain tumor segmentation (BRATS) challenge 2013 dataset [15] using the well defined training and testing splits, thereby allowing us to compare directly and quantitatively to a wide variety of other methods.\nOur contributions in this work are four fold:\n1. We propose a fully automatic method with results currently ranked second on the BRATS 2013 scoreboard; 2. To segment a brain, our method takes between 25 seconds and 3 minutes, which is one order of magnitude faster than most state-of-the-art methods. 3. Our CNN implements a novel two-pathway architecture that learns about the local details of the brain as well as the larger context. We also propose a two-phase training procedure which we have found is critical to deal with imbalanced label distributions. Details of these contributions are described in Sections 3.1.1 and 3.2. 4. We employ a novel cascaded architecture as an efficient and conceptually clean alternative to popular structured output methods. Details on those models are presented in Section 3.1.2.\nRelated work\nAs noted by Menze et al. [32], the number of publications devoted to automated brain tumor segmentation has grown exponentially in the last several decades. This observation not only underlines the need for automatic brain tumor segmentation tools, but also shows that research in that area is still a work in progress.\nBrain tumor segmentation methods (especially those devoted to MRI) can be roughly divided in two categories: those based on generative models and those based on discriminative models [32,5,2].\nGenerative models rely heavily on domain-specific prior knowledge about the appearance of both healthy and tumorous tissues. Tissue appearance is challenging to characterize, and existing generative models usually identify a tumor as being a shape or a signal which deviates from a normal (",
            "url": "http://arxiv.org/pdf/1505.03540"
          },
          {
            "id": "S3227768826",
            "text": "The integration of transfer learning from natural image datasets, such as ImageNet, into medical imaging segmentation tasks has been shown to improve performance, particularly in scenarios with limited training data.",
            "children": [
              {
                "id": "E1735529591",
                "text": "..part recognition system is also presented in Yan et al. [75]. A multistage deep learning framework based on CNNs extracts both the patches with the most as well as least discriminative local patches in the pretraining stage. Subsequently, a boosting stage exploits this local information 12 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 21, NO. 1, JANUARY 2017 to improve performance. The authors point out that training based on discriminative local appearances are more accurate compared to the usage of global image context. CNNs have also been proposed for the segmentation of isointense stage brain tissues [131] and brain extraction from multimodality MR images [56]. Hybrid approaches that combine CNNs with other architectures are also proposed. In [66], a deep learning algorithm is employed to encode the parameters of a deformable model and thus facilitate the segmentation of the left ventricle (LV) from short-axis cardiac MRI. CNNs are employed to automatically detect the LV, whereas deep Autoencoders are utilized to infer its shape. Yu et al. [67] designed a wireless capsule endoscopy classification system based on a hybrid CNN with extreme learning machine (ELM). The CNN constitutes a data-driven feature extractor, whereas the cascaded ELM acts as a strong classifier. A comparison between different CNNs architectures concluded that deep CNNs of up to 22 layers can be useful even with limited training datasets [73]. More detailed description of various CNNs architectures proposed in medical imaging analysis is presented in previous survey [58]. The key challenges and limitations are: 1) CNNs are designed for 2-D images whereas segmentation problems in MRI and CT are inherently 3-D. This problem is further complicated by the anisotropic voxel size. Although the creation of isotropic images by interpolating the data is a possibility, it can result in severely blurred images. Another solution is to train the CNNs on orthogonal patches extracted from axial, sagittal and coronal views [62], [132]. This approach also drastically reduces the time complexity required to process 3-D information and thus alleviates the problem of overfitting. 2) CNNs do not model spatial dependencies. Therefore, several approaches have incorporated voxel neighboring information either implicitly or by adding a pairwise term in the cost function, which is referred as conditional random field [85]. 3) Preprocessing to bring all subjects and imaging modalities to similar distribution is still a crucial step that affects the classification performance. Similarly to conventional machine learning approaches, balancing the datasets with bootstrapping and selecting samples with high entropy is advantageous. Perhaps, all of these limitations result from or are exacerbated by small and incomplete training datasets. Furthermore, there is limited availability of ground-truth/annotated data, since the cost and time to collect and manually annotate medical images is prohibitively large. Manual annotations are subjective and highly variable across medical experts. Although, it is thought that the manual annotation would require highly specialized knowledge in medicine and medical imaging physics, recent studies suggest that nonprofessional users could perform similarly [76]. Therefore, crowdsourcing is suggested as a viable alternative to create low-cost, big ground-truth medical imaging datasets. Moreover, the normal class is often over represented since the healthy tissue usually dominates and forms highly repetitive patterns. These issues result in slow convergence and overfitting. To alleviate the lack of training samples, transfer learning via fine tuning have been suggested in medical imaging applications [58], [72], [74], [76]. In transfer learning via fine-tuning, a CNN is pretrained using a database of labeled natural images. The use of natural images to train CNNs in medical imaging is controversial because of the profound difference between natural and medical images. Nevertheless, Tajbakhsh et al. [74] showed that fine-tuned CNNs based on natural images are less prone to overfitting due to the limited size training medical imaging sets and perform similarly or better than CNNs trained from scratch. Shin et al. [73] has applied transfer learning from natural images in thoraco-abdominal lymph node detection and interstitial lung disease classification. They also reported better results than training the CNNs from scratch with more consistent performances of validation loss and accuracy traces. Chen et al. [72] applied successfully a transfer learning strategy to identify the fetal abdominal standard plane. The lower layers of a CNN are pretrained based on natural images. The approach shows improved capability of the algorithm to encode the complicated appearance of the abdominal plane. Multitask training has also been suggested to handle the class imbalance common in CAD applications. Multitasking refers to the idea of solving different classification problems simultaneously and it results in a drastic reduction of free parameters [133]. Although CNNs have dominated medical image analysis applications, other deep learning approaches/architectures have also been applied successfully. In a recent paper, a stacked denoising autoencoder was proposed for the diagnosis of benign malignant breast lesions in ultrasound images and pulmonary nodules in CT scans [77]. The method outperforms classical CAD approaches, largely due to the automatic feature extraction and noise tolerance. Furthermore, it eliminates the image segmentation process to obtain a lesion boundary. Shan et al. [53] presented a stacked sparse autoencoder for microaneurysms detection in fundus images as an instance of a diabetic retinopathy strategy. The proposed method learns high-level distinguishing features based only on pixel intensities. Various autoencoder-based learning approaches have also been applied to the automatic extraction of biomarkers from brain images and the diagnosis of neurological diseases. These methods often use available public domain brain image databases such as the Alzheimer\u2019s disease neuroimaging initiative database. For example, a deep Autoencoder combined with a softmax output layer for regression is proposed for the diagnosis of Alzheimer\u2019s disease. Hu et al. [134] also used autoencoders for Alzheimer\u2019s disease prediction based on Functional Magnetic Resonance Images (fMRI). The results show that the proposed method achieves much better classification than the traditional means. On the other hand, Li et al. [61] proposed an RBM approach that identifies biomarkers from MRI and positron emission tomography (PET) scans. They obtained an improvement of about 6% in classification accuracy compared to the standard approaches. Kuang et al. [60] proposed an RBM approach for fMRI data to discriminate attention deficit hyperactivity disorder. The system is capable of predicting the subjects as control, combined, inattentive or hyperactive through their RAV`\u0131 et al.: DEEP LEARNING FOR HEALTH INFORMATICS 13 frequency features. Suk et al. [59] proposed a DBM to extract a latent hierarchical feature representation from 3-D patches of brain images. Low level image processing, such as image segmentation and registration can also benefit from deep learning models. Brosch et al. [64] described a manifold learning approach of 3-D brain images based on DBN. It is different than other methods because it does not require a locally linear manifold space. Mansoor et al. [54] developed a fully automated shape model segmentation mechanism for the analysis of cranial nerve systems. The deep learning approach outperforms conventional methods particularly in regions with low contrast, such as optic tracts and areas with pathology. In [135], a pipeline is proposed for object detection and segmentation in the context of automatically processing volumetric images. A novel framework called marginal space deep learning implements an object parameterization in hierarchical marginal spaces combined with automatic feature detection based on deep learning. In [84], a DNN architecture called input\u2013output deep architecture is described to solve the image labelling problem. A single NN forward step is used to assign a label to each pixel. This method avoids the handcrafted subjective design of a model with a deep learning mechanism, which automatically extracts the dependencies between labels. Deep learning is also used for processing hyperspectral images [83]. Spectral and spatial learned features are combined together in a hierarchical model to characterize tissues or materials. In [78], a hybrid multilayered group method of data handling, which is a special NN with polynomial activation functions, has been used together with a principal component-regression analysis to recognize the liver and spleen. A similar approach is used for the identification of the myocardium [79] as well as the right and left kidney regions [80]. The authors extend the method to analyze brain or lung CT images to detect cancer [81]. Zhen et al. [63] presents a framework for direct biventricular volume estimation, which avoids the need of user inputs and over simplification assumptions. The learning process involves unsupervised cardiac image representation with multiscale deep networks and direct biventricular volume estimation with RF. Rose et al. [82] propose a methodology for hierarchical clustering in application to mammographic image data. Classification is performed based on a deep learning architecture along with a standard NN. In general, deep learning in medical imaging provides automatic discovery of object features and automatic exploration of feature hierarch and interaction. In this way, a relatively simple training process and a systematic performance tuning can be used, making deep learning approaches improve over the state-of-the art. However, in medical imaging analysis, their potentials have not been unfolded fully. To be successful in disease detection and classification approaches, deep learning requires the availability of large labeled datasets. Annotating imaging datasets is an extremely time-consuming and costly process that is normally undertaken by medical doctors. Currently, there is a lot of debate on whether to increase the number of annotated datasets with the help of non-experts (crowd-sourcing) and how to standardize the available images to allow objective assessment of the deep learning approaches. Fig. 6. Data for health monitoring applications can be captured using a wide array of pervasive sensors that are worn on the body, implanted, or captured through ambient sensors, e.g., inertial motion sensors, ECG patches, smart-watches, EEG, and prosthetics. C. Pervasive Sensing for Health and Wellbeing Pervasive sensors, such as wearable, implantable, and ambient sensors [136] allow continuous monitoring of health and wellbeing, Fig. 6. An accurate estimation of food intake and energy expenditure throughout the day, for example, can help tackle obesity and improve personal wellbeing. For elderly..",
                "url": "https://eprints.gla.ac.uk/208066/1/208066.pdf"
              },
              {
                "id": "S3330376286",
                "text": "The integration of multimodal data sources in medical imaging, as highlighted in the context of radiomics, can improve the robustness and clinical utility of segmentation models, suggesting that techniques from natural image segmentation should also consider data diversity.",
                "children": [
                  {
                    "id": "E6769120955",
                    "text": "..for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics. REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 749 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Exploratory analysis Medical imaging Data selection VOI Prediction target Imaging protocols Test feature stability Report protocols Add prediction target Report algorithms Store data Robust segmentation Add clinical variables Archetypal features Feature selection Internal validation Radiomics External validation Report methodology Modeling Feature extraction RQS 1 RQS 2 RQS 3 RQS Total 36 Image protocol quality +1 or +2 RQS checkpoint 1 total: 2 +1 +1 +1 RQS checkpoint 2 total: 3 Multiple segmentation Phantom study Imaging at multiple time points Feature reduction or adjustment for multiple testing Multivariable analysis Biological correlates Cut-off analysis Discrimination statistics Calibration statistics Prospective study Validation Comparison to \u2018gold standard\u2019 Costeffectiveness analysis Potential clinical applications Open science and data -3 or +3 +1 +1 +1 +1 or +2 +1 or +2 +7 -5 to +5 +2 +2 +1 +1 to +4 RQS checkpoint 3 total: 31 Nature Reviews | Clinical Oncology evaluation of their performance, reproducibility, and/or clinical utility17. Consequently, these models might not be appropriate for CDSS. In this Review, we describe the process of radiomics along with latest developments in the field. The pitfalls, challenges, and opportunities presented by radiomics to improve CDSS for personalized precision oncology are highlighted, with an emphasis on the methodological aspects of radiomics prediction model development and validation. We explore the advanced and innovative information technologies that are essential for the data management of diverse multimodal data sources. Finally, we offer a vision of the necessary steps to ensure continued progression and widespread acceptance of both radiomics and CDSS. The workflow of radiomics Radiomics is defined as the quantitative mapping, that is, extraction, analysis and modelling of many medical image features in relation to prediction targets, such as clinical end points and genomic features. A radiomics study can be structured in five phases: data selection, medical imaging, feature extraction, exploratory analysis, and modelling (FIG. 1). To assess the quality of radiomics studies, we propose the radiomics quality score (RQS). Data selection Radiomic analyses begins with the choice of an imaging protocol, the volume of interest (VOI) and a prediction target \u2014 the event one wishes to predict. Typically, the entire primary tumour is analysed and linked to available data on treatment outcomes, such as survival. Radiomic analyses can be performed on subregions of the tumour (habitats), metastatic lesions, as well as in normal tissues. Analysis of these regions might yield radiosensitive phenotypes, which has implications for treatment planning strategies. Radiomics analysis, however, is not restricted to radiotherapy and can be applied to any image generated in the clinical setting (FIG. 2). The importance of using standardized imaging protocols to eliminate unnecessary confounding variability is recognized9,18; however, nonstandardized imaging protocols are commonplace. Therefore, reproducibility and Key points \u2022 Radiomics is becoming increasingly more important in medical imaging \u2022 The explosion of medical imaging data creates an environment ideal for machine-learning and data-based science \u2022 Radiomics-based decision-support systems for precision diagnosis and treatment can be a powerful tool in modern medicine \u2022 Large-scale data sharing is necessary for the validation and full potential that radiomics represents \u2022 Standardized data collection, evaluation criteria, and reporting guidelines are required for radiomics to mature as a discipline Figure 1 | Flowchart depicting the workflow of radiomics and the application of the RQS. The workflow includes the necessary steps in a radiomic analysis. The RQS both rewards and penalizes the methodology and analyses of a study, consequently encouraging the best scientific practice. RSQ, radiomics quality score; VOI, volume of interest. REVIEWS 750 | DECEMBER 2017 | VOLUME 14 www.nature.com/nrclinonc \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mi t e d, p a rt o f S p ri n g e r N a t u r e. Al l ri g h t s r e s e r v e d. Nature Reviews | Clinical Oncology Cardiac CT Radiomics MACE Analysis Phantom studies An artificial structure that imitates human tissue properties is scanned on multiple machines to characterize scan output against a known physical standard. comparability of radiomic studies can be achieved only by extensive disclosure of imaging protocols. We wish to emphasize this point, and provide examples of how protocols should be reported in future radiomics studies (Supplementary information S1). Medical imaging Segmentation. VOIs are segmented manually or (semi-) automatically19. This segmentation determines which voxels within an image are analysed, thus, the variability in segmentation can introduce bias in the evaluation of derived radiomic features20. Multiple-segmentation is a method to limit the extent of this bias. Examples that enable robust features to be observed21 include: evaluation by multiple clinicians, perturb segmentations with noise, combination of diverse algorithms, or use different stages of the breathing cycle. Key considerations are how the segmentation was performed, and how sensitive the radiomics analysis is to different segmentation methods22. For example, a semiautomatic segmentation method can result in different radiomic features than a manual delineation. Phantom studies. The determination of inter-scanner and inter-vendor variability of features is important in radiomics23. In cases in which radiomic studies rely on data from multiple scanners, neglecting this variability can jeopardize the analysis of studies \u2014 that is, the proposed radiomic-based prediction model might not perform adequately on external datasets if new data are acquired on different scanners. As data from patients scanned on multiple devices is scarce and subject to uncertainties (such as organ motion, or different imaging protocols), phantom studies are a suitable means to gauge these uncertainties and identify features that rely on the vendor. In essence, phantom studies provide a risk-mitigation strategy to help navigate from the current clinical imaging scenario to the desired optimal imaging scenario. Imaging at multiple time points. Additional sources of variability in radiomics features are organ motion or expansion or shrinkage of the target volume. Radiomics features that are strongly dependent on these factors can have limited applicability. To account for these sources of variability, available test-retest data24\u201326 can be exploited to measure radiomics feature stability. For example, two datasets of images acquired within a small period of time from a patient cohort. Feature extraction The essence of radiomics is the high-throughput extraction of quantitative image features to characterize VOIs. Feature values are dependent upon factors that can include image pre-processing (for example, filtering, or intensity discretization) and reconstruction (for example, filtered back projection, or iterative reconstruction). Furthermore, variation exists in feature nomenclature, mathematical definition, methodology, and software implementation of the applied feature extraction algorithms27\u201329. In order to facilitate inter-operability of radiomic features, differences in nomenclature, algorithms, software implementations, as well as other methodological aspects must be elucidated. Exploratory analysis Radiomic and non-radiomic features should be combined with the prediction target to create a single dataset. This approach enables the investigation of relationships between features. Groups of highly correlated radiomics features can be identified via clustering, and these features can be reduced to single archetypal features per cluster. Radiomic features that are well-correlated with routine clinical features (such as tumour stage) do not provide additional information. Auxiliary feature data collected from multiple segmentations, multiple imaging, and phantom studies, can be exploited to assess feature robustness. Volatile or robust features can be identified and subsequently excluded from model development. For example, a feature that is robust for the prediction of overall survival for lung cancer (that is, imaged and segmented in a certain way) for a given dataset could be volatile for the prediction of pneumonitis in lung cancer (imaged and segmented in an alternative way) for a given dataset. Thus, the process of feature reduction and/or exclusion should be described clearly. Modelling Radiomic modelling involves three major aspects: feature selection, modelling methodology, and validation. Feature selection should be data-driven owing to the vast in- human range of possible radiomics features; such analysis should be performed in a robust and transparent manner. To achieve holistic models, features beyond radiomics (such as data from clinical records, data obtained during treatment or biological and/or genetic) should also be incorporated. Regarding the choice of modelling methodology, the identification of optimal machinelearning methods for radiomic applications is a crucial step towards stable and clinically relevant CDSS; thus, in the ideal scenario, multiple machine-learning methods should be employed30 and the implementation should be comprehensively documented. A non-validated model is Figure 2 |Radiomics in cardiology. The current gold standard for quantification of coronary calcifications visible on CT is the \u2018Agatston\u2019 method (based upon intensity and volume). Radiomic features can improve quantification, differentiation between calcified and non-calcified plaques, and thus the prediction of Major Adverse Cardiac Events (MACE). REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 751 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Calibration-in-the-large Describes whether the predictions deviate systematically (intercept), whereas the calibration slope should ideally be equal to 1. The independence assumption The definition in terms of conditional probabilities is..",
                    "url": "https://cris.maastrichtuniversity.nl/ws/files/36170434/Lambin_2017_Radiomics_the_Bridge_Between.pdf"
                  },
                  {
                    "id": "E7176429902",
                    "text": "..on both clinical applications and scientific researches. Recent progresses in deep learning have shed new light on medical image analysis by allowing discovering morphological and/or textural patterns in images solely from data. As deep learning methods have achieved the state-ofthe-art performance over different medical applications, its use for further improvement can be the major step in the medical computing field. However, there are still rooms for improvements. First, lessoned in computer vision, where breakthrough improvements were achieved by exploiting large amounts of training data, e.g., more than 1 million annotated images in ImageNet (19), it would be one direction to build such big publicly available dataset of medical images, by which deep models can find more generalized features in medical images, thus allowing making a leap in performance. Second, while the data-driven feature representations, especially in an unsupervised manner, helped enhance accuracy, it is also desirable to devise a new methodological architecture, with which it becomes possible to reflect or involve the domain-specific knowledge. Third, it is also necessary to develop algorithmic techniques to efficiently handle images acquired with different scanning protocols, by which there is no need to train modality-specific deep models. Last but not least, when applying deep learning to investigate the underlying patterns in images such as fMRI, due to the black-box like characteristics of deep models, it still remains challenging to understand and interpret the learned models intuitively. Shen et al. Page 19 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Acknowledgments This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.B0101-16-0307, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)). This work was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2015R1C1A1A01052216), NIH grants (EB006733, EB008374, EB009634, MH100217, MH108914, AG041721, AG049371, AG042599, DE022676). LITERATURE CITED 1. Brody H. Medical imaging. Nature. 2013; 502:S81\u2013S81. [PubMed: 24187698] 2. Schmidhuber J. Deep learning in neural networks: An overview. Neural Networks. 2015; 61:85\u2013117. [PubMed: 25462637] 3. Bengio Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning. 2009; 2:1\u2013127. 4. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521:436\u2013444. [PubMed: 26017442] 5. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006; 313:504\u2013507. [PubMed: 16873662] 6. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 2010; 11:3371\u20133408. 7. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning (ICML). 2010 8. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research. 2014; 15:1929\u2013 1958. 9. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Proceedings of International Conference on Machine Learning (ICML). 2015 10. Bishop, CM. Neural networks for pattern recognition. Oxford University Press, Inc; 1995. 11. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of International Conference on Machine Learning (ICML). 2008 12. Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. Proceedings of International Conference on Machine Learning (ICML). 2011 13. Hinton GE, Deng L, Yu D, Dahl GE, Mohamed A, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine. 2012; 29:82\u201397. 14. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Proceedings of Neural Information Processing Systems (NIPS). 2013:2553\u20132561. 15. Taigman Y, Yang M, Ranzato M, Wolf L. Deepface: Closing the gap to human-level performance in face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014 16. Zhang J, Zong C. Deep neural networks in machine translation: An overview. IEEE Intelligent Systems. 2015; 30:16\u201325. 17. Karpathy A, Li F. Deep visual-semantic alignments for generating image descriptions. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015 18. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, et al. Mastering the game of Go with deep neural networks and tree search. Nature. 2016; 529:484\u2013489. [PubMed: 26819042] 19. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015; 115:211\u2013252. 20. Everingham, M., Van Gool, L., Williams, CKI., Winn, J., Zisserman, A. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 2012. http://www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html Shen et al. Page 20 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript 21. Roux, L., Racoceanu, D., Capron, F., Calvo, J., Attieh, E., et al. MITOS-ATYPIA-14. 2014. http:// mitos-atypia-14.grand-challenge.org/ 22. Zhang W, Li R, Deng H, Wang L, Lin W, et al. Deep convolutional neural networks for multimodality isointense infant brain image segmentation. NeuroImage. 2015; 108:214\u2013224. [PubMed: 25562829] 23. Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, et al. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage. 2016; 129:460\u2013469. [PubMed: 26808333] 24. Wu G, Kim M, Wang Q, Munsell BC, Shen D. Scalable high-performance image registration framework by unsupervised deep feature representations learning. IEEE Transactions on Biomedical Engineering. 2016; 63:1505\u20131516. [PubMed: 26552069] 25. Suk HI, Lee SW, Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014; 101:569\u2013582. [PubMed: 25042445] 26. Shin H, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016 27. Suk HI, Lee SW, Shen D. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Structure and Function. 2015; 220:841\u2013859. [PubMed: 24363140] 28. Suk HI, Shen D. Deep learning in diagnosis of brain disorders. Recent Progress in Brain and Cognitive Engineering. 2015:203\u2013213. 29. Suk HI, Wee CY, Lee SW, Shen D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage. 2016; 129:292\u2013307. [PubMed: 26774612] 30. Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Transactions on Medical Imaging. 2016; 35:1240\u20131251. [PubMed: 26960222] 31. van Tulder G, de Bruijne M. Combining generative and discriminative representation learning for lung CT analysis with convolutional restricted boltzmann machines. IEEE Transactions on Medical Imaging. 2016; 35:1262\u20131272. [PubMed: 26886968] 32. Dou Q, Chen H, Yu L, Zhao L, Qin J, et al. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE Transactions on Medical Imaging. 2016; 35:1182\u20131195. [PubMed: 26886975] 33. Cire\u015fan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI). 2013:411\u2013418. 34. Chen H, Qi X, Cheng JZ, Heng PA. Deep contextual networks for neuronal structure segmentation. Proceedings of AAAI Conference on Artificial Intelligence. 2016 35. Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, et al. Computer-aided diagnosis with deep learning architecture..",
                    "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
                  }
                ]
              }
            ]
          }
        ]
      },
      "S4713353038": {
        "id": "S4713353038",
        "text": "Adaptive fuzzy c-means algorithms, which incorporate local noise detection, can enhance image segmentation robustness in medical imaging by effectively addressing noise issues that commonly affect medical images, a methodology that can be adapted from natural image segmentation techniques.",
        "children": [
          {
            "id": "E1465584247",
            "text": "..Densely connected convolutional networks. arXiv preprint. 2016. 6. Jan K, Vladimir G, Daniel C. Regularization for deep learning: a taxonomy. arXiv preprint. 2017. 7. Nitish S, Geofrey H, Alex K, Ilya S, Ruslan S. Dropout: a simple way to prevent neural networks from overftting. J Mach Learn Res. 2014;15(1):1929\u201358. 8. Jonathan T, Ross G, Arjun J, Yann L, Christoph B. Efcient object localization using convolutional networks. In: CVPR\u201915. 2015. 9. Sergey I, Christan S. Batch normalization: accelerating deep network training by reducing internal covariate shift. In: ICML; 2015. 10. Karl W, Taghi MK, DingDing W. A survey of transfer learning. J Big Data. 2016;3:9. 11. Shao L. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2015;26(5):1019\u201334. 12. Jia D, Wei D, Richard S, Li-Jia L, Kai L, Li F-F. ImageNet: a large-scale hierarchical image database. In: CVPR09, 2009. 13. Amir Z, Alexander S, William S, Leonidas G, Jitendra M, Silvio S. Taskonomy: disentangling task transfer learning. In: CVPR \u201818. 2018. 14. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? Adv Neural Inf Process Syst. 2014;27:3320\u20138. Shorten and Khoshgoftaar J Big Data (2019) 6:60 Page 45 of 48 15. Erhan D, Bengio Y, Courville A, Manzagol PA, Vincent P. Why does unsupervised pre-training help deep learning? J Mach Learn Res. 2010;11:625\u201360. 16. Mark P, Dean P, Geofrey H, Tom MM. Zero-shot learning with semantic output codes. In: NIPS; 2009. 17. Yongqin X, Christoph HL, Bernt S, Zeynep A. Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly. arXiv preprint. 2018. 18. Yaniv T, Ming Y, Marc\u2019 AR, Lior W. DeepFace: closing the gap to human-level performance in face verifcation. In: CVPR \u201914; 2014. 19. Gregory K, Richard Z, Ruslan S. Siamese neural networks for one-shot image recognition. In: ICML Deep Learning workshop; 2015. 20. Adam S, Sergey B, Matthew B, Dean W, Timothy L. One-shot learning with memory-augmented neural networks. arXiv preprint. 2016. 21. Tomas M, Ilya S, Kai C, Greg C, Jefrey D. Distributed representations of words and phrases and their compositionality. Accepted to NIPS 2013. 22. Jefrey P, Richard S, Christopher DM. GloVe: global vectors for word representation. In: Proceedings of the empirical methods in natural language processing (EMNLP 2014) 12. 2014. 23. Halevy A, Norvig P, Pereira F. The unreasonable efectiveness of data. IEEE Intell Syst. 2009;24:8\u201312. 24. Chen S, Abhinav S, Saurabh S, Abhinav G. Revisting unreasonable efectivness of data in deep learning era. In: ICCV; 2017. p. 843\u201352. 25. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classifcation of skin cancer with deep neural networks. Nature. 2017;542:115\u20138. 26. Geert L, Thijs K, Babak EB, Arnaud AAS, Francesco C, Mohsen G, Jeroen AWM, van Bram G, Clara IS. A survey on deep learning in medical image analysis. Med Image Anal. 2017;42:60\u201388. 27. Jofrey LL, Taghi MK, Richard AB, Naeem S. A survey on addressing high-class imbalance in big data. Springer J Big Data. 2018;5:42. 28. LeCun Y, Bottou L, Bengio Y, Hafner P. Gradient-based learning applied to document recognition. Proc IEEE. 1998;86(11):2278\u2013324. 29. Nitesh VC, Kevin WB, Lawrence OH, Kegelmeyer W. SMOTE: synthetic minority over-sampling technique. J Artif Intellig Res. 2002;16:321\u201357. 30. Hui H, Wen-Yuan W, Bing-Huan M. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. In: Proceedings of ICIC, vol. 3644, Lecture Notes in Computer Science, New York. 2005, p. 878\u201387. 31. Ian JG, Jean PA, Mehdi M, Bing X, David WF, Sherjil O, Aaron C, Yoshua B. Generative adversarial nets. NIPS. 2014. 32. Leon AG, Alexander SE, Matthias B. A neural algorithm of artistic style. ArXiv. 2015. 33. Barret Z, Quoc VL. Neural architecture search with reinforcement learning. In: International conference on learning representatoins, 2017. 34. Tero K, Timo A, Samuli L, Jaakko L. Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 35. Justin J, Alexandre A, Li FF. Perceptual losses for real-time style transfer and super-resolution. ECCV. 2016;2016:694\u2013711. 36. Luis P, Jason W. The efectiveness of data augmentation in image classifcation using deep learning. In: Stanford University research report, 2017. 37. Lemley J, Barzrafkan S, Corcoran P. Smart augmentation learning an optimal data augmentation strategy. In: IEEE Access. 2017. 38. Ekin DC, Barret Z, Dandelion M, Vijay V, Quoc VL. AutoAugment: learning augmentation policies from data. ArXiv preprint. 2018. 39. Xin Y, Paul SB, Ekta W. Generative adversarial network in medical imaging: a review. arXiv preprint. 2018. 40. Jelmer MW, Tim L, Max AV, Ivana I. Generative adversarial networks for noise reduction in low-dose CT. In: IEEE Transactions on Medical Imaging. 2017. 41. Ohad S, Tammy RR. Accelerated magnetic resonance imaging by adversarial neural network. In: DLMIA/ML-CDS@ MICCAI, 2017. 42. Wang Y, Biting Y, Wang L, Chen Z, Lalush DS, Lin W, Xi W, Zhou J, Shen D, Zhou L. 3D conditional generative adversarial networks for high-quality PET image estimation at low dose. NeuroImage. 2018;174:550\u201362. 43. Dwarikanath M, Behzad B. Retinal vasculature segmentation using local saliency maps and generative adversarial networks for image super resolution. arXiv preprint. 2017. 44. Francesco C, Aldo M, Claudio S, Giorgio T. Biomedical data augmentation using generative adversarial neural networks. In: International conference on artifcial neural networks. Berlin: Springer; 2017. P. 626\u201334. 45. Camilo B, Andrew JP, Larry TD, Allen TN, Susan MR, Bennett AL. Learning implicit brain MRI manifolds with deep learning. Int Soc Opt Photonics. 2018;10574:105741. 46. Maria JMC, Sarfaraz H, Jeremy B, Ulas B. How to fool radiologists with generative adversarial networks? A visual turing test for lung cancer diagnosis. arXiv preprint. 2017. 47. Baur C, Albarqouni S, Navab N. MelanoGANs: high resolution skin lesion synthesis with GANs. arXiv preprint. 2018. 48. Madani A, Moradi M, Karargyris A, Syeda-Mahmood T. Chest x-ray generation and data augmentation for cardiovascular abnormality classifcation. In: Medical imaging 2018. Image Processing 2018;10574:105741. 49. Maayan F-A, Eyal K, Jacob G, Hayit G. GAN-based data augmentation for improved liver lesion classifcation. arXiv preprint. 2018. 50. Joseph R, Santosh D, Ross G, Ali F. You only look once: unifed, real-time object detection. In: CVPR\u201816. 2016. 51. Ross G, Jef D, Trevor D, Jitendra M. Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR \u201814. 2014. 52. Ross G. Fast R-CNN. CoRR, abs/1504.08083. 2015. 53. Shaoqing R, Kaiming H, Ross G, Jian S. Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS, 2015. 54. Jonathan L, Evan S, Trevor D. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038. 2014. Shorten and Khoshgoftaar J Big Data (2019) 6:60 Page 46 of 48 55. Olaf R, Philipp F, Thomas B. U-Net: convolutional networks for biomedical image segmentation. In: MICCAI. Springer; 2015, p. 234\u201341. 56. Hessam B, Maxwell H, Mohammad R, Ali F. Label refnery: improving imagenet classifcation through label progression. arXiv preprint. 2018. 57. Francisco JM-B, Fiammetta S, Jose MJ, Daniel U, Leonardo F. Forward noise adjustment scheme for data augmentation. arXiv preprints. 2018...",
            "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0"
          }
        ]
      },
      "S3330376286": {
        "id": "S3330376286",
        "text": "The integration of multimodal data sources in medical imaging, as highlighted in the context of radiomics, can improve the robustness and clinical utility of segmentation models, suggesting that techniques from natural image segmentation should also consider data diversity.",
        "children": [
          {
            "id": "E6769120955",
            "text": "..for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics. REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 749 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Exploratory analysis Medical imaging Data selection VOI Prediction target Imaging protocols Test feature stability Report protocols Add prediction target Report algorithms Store data Robust segmentation Add clinical variables Archetypal features Feature selection Internal validation Radiomics External validation Report methodology Modeling Feature extraction RQS 1 RQS 2 RQS 3 RQS Total 36 Image protocol quality +1 or +2 RQS checkpoint 1 total: 2 +1 +1 +1 RQS checkpoint 2 total: 3 Multiple segmentation Phantom study Imaging at multiple time points Feature reduction or adjustment for multiple testing Multivariable analysis Biological correlates Cut-off analysis Discrimination statistics Calibration statistics Prospective study Validation Comparison to \u2018gold standard\u2019 Costeffectiveness analysis Potential clinical applications Open science and data -3 or +3 +1 +1 +1 +1 or +2 +1 or +2 +7 -5 to +5 +2 +2 +1 +1 to +4 RQS checkpoint 3 total: 31 Nature Reviews | Clinical Oncology evaluation of their performance, reproducibility, and/or clinical utility17. Consequently, these models might not be appropriate for CDSS. In this Review, we describe the process of radiomics along with latest developments in the field. The pitfalls, challenges, and opportunities presented by radiomics to improve CDSS for personalized precision oncology are highlighted, with an emphasis on the methodological aspects of radiomics prediction model development and validation. We explore the advanced and innovative information technologies that are essential for the data management of diverse multimodal data sources. Finally, we offer a vision of the necessary steps to ensure continued progression and widespread acceptance of both radiomics and CDSS. The workflow of radiomics Radiomics is defined as the quantitative mapping, that is, extraction, analysis and modelling of many medical image features in relation to prediction targets, such as clinical end points and genomic features. A radiomics study can be structured in five phases: data selection, medical imaging, feature extraction, exploratory analysis, and modelling (FIG. 1). To assess the quality of radiomics studies, we propose the radiomics quality score (RQS). Data selection Radiomic analyses begins with the choice of an imaging protocol, the volume of interest (VOI) and a prediction target \u2014 the event one wishes to predict. Typically, the entire primary tumour is analysed and linked to available data on treatment outcomes, such as survival. Radiomic analyses can be performed on subregions of the tumour (habitats), metastatic lesions, as well as in normal tissues. Analysis of these regions might yield radiosensitive phenotypes, which has implications for treatment planning strategies. Radiomics analysis, however, is not restricted to radiotherapy and can be applied to any image generated in the clinical setting (FIG. 2). The importance of using standardized imaging protocols to eliminate unnecessary confounding variability is recognized9,18; however, nonstandardized imaging protocols are commonplace. Therefore, reproducibility and Key points \u2022 Radiomics is becoming increasingly more important in medical imaging \u2022 The explosion of medical imaging data creates an environment ideal for machine-learning and data-based science \u2022 Radiomics-based decision-support systems for precision diagnosis and treatment can be a powerful tool in modern medicine \u2022 Large-scale data sharing is necessary for the validation and full potential that radiomics represents \u2022 Standardized data collection, evaluation criteria, and reporting guidelines are required for radiomics to mature as a discipline Figure 1 | Flowchart depicting the workflow of radiomics and the application of the RQS. The workflow includes the necessary steps in a radiomic analysis. The RQS both rewards and penalizes the methodology and analyses of a study, consequently encouraging the best scientific practice. RSQ, radiomics quality score; VOI, volume of interest. REVIEWS 750 | DECEMBER 2017 | VOLUME 14 www.nature.com/nrclinonc \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mi t e d, p a rt o f S p ri n g e r N a t u r e. Al l ri g h t s r e s e r v e d. Nature Reviews | Clinical Oncology Cardiac CT Radiomics MACE Analysis Phantom studies An artificial structure that imitates human tissue properties is scanned on multiple machines to characterize scan output against a known physical standard. comparability of radiomic studies can be achieved only by extensive disclosure of imaging protocols. We wish to emphasize this point, and provide examples of how protocols should be reported in future radiomics studies (Supplementary information S1). Medical imaging Segmentation. VOIs are segmented manually or (semi-) automatically19. This segmentation determines which voxels within an image are analysed, thus, the variability in segmentation can introduce bias in the evaluation of derived radiomic features20. Multiple-segmentation is a method to limit the extent of this bias. Examples that enable robust features to be observed21 include: evaluation by multiple clinicians, perturb segmentations with noise, combination of diverse algorithms, or use different stages of the breathing cycle. Key considerations are how the segmentation was performed, and how sensitive the radiomics analysis is to different segmentation methods22. For example, a semiautomatic segmentation method can result in different radiomic features than a manual delineation. Phantom studies. The determination of inter-scanner and inter-vendor variability of features is important in radiomics23. In cases in which radiomic studies rely on data from multiple scanners, neglecting this variability can jeopardize the analysis of studies \u2014 that is, the proposed radiomic-based prediction model might not perform adequately on external datasets if new data are acquired on different scanners. As data from patients scanned on multiple devices is scarce and subject to uncertainties (such as organ motion, or different imaging protocols), phantom studies are a suitable means to gauge these uncertainties and identify features that rely on the vendor. In essence, phantom studies provide a risk-mitigation strategy to help navigate from the current clinical imaging scenario to the desired optimal imaging scenario. Imaging at multiple time points. Additional sources of variability in radiomics features are organ motion or expansion or shrinkage of the target volume. Radiomics features that are strongly dependent on these factors can have limited applicability. To account for these sources of variability, available test-retest data24\u201326 can be exploited to measure radiomics feature stability. For example, two datasets of images acquired within a small period of time from a patient cohort. Feature extraction The essence of radiomics is the high-throughput extraction of quantitative image features to characterize VOIs. Feature values are dependent upon factors that can include image pre-processing (for example, filtering, or intensity discretization) and reconstruction (for example, filtered back projection, or iterative reconstruction). Furthermore, variation exists in feature nomenclature, mathematical definition, methodology, and software implementation of the applied feature extraction algorithms27\u201329. In order to facilitate inter-operability of radiomic features, differences in nomenclature, algorithms, software implementations, as well as other methodological aspects must be elucidated. Exploratory analysis Radiomic and non-radiomic features should be combined with the prediction target to create a single dataset. This approach enables the investigation of relationships between features. Groups of highly correlated radiomics features can be identified via clustering, and these features can be reduced to single archetypal features per cluster. Radiomic features that are well-correlated with routine clinical features (such as tumour stage) do not provide additional information. Auxiliary feature data collected from multiple segmentations, multiple imaging, and phantom studies, can be exploited to assess feature robustness. Volatile or robust features can be identified and subsequently excluded from model development. For example, a feature that is robust for the prediction of overall survival for lung cancer (that is, imaged and segmented in a certain way) for a given dataset could be volatile for the prediction of pneumonitis in lung cancer (imaged and segmented in an alternative way) for a given dataset. Thus, the process of feature reduction and/or exclusion should be described clearly. Modelling Radiomic modelling involves three major aspects: feature selection, modelling methodology, and validation. Feature selection should be data-driven owing to the vast in- human range of possible radiomics features; such analysis should be performed in a robust and transparent manner. To achieve holistic models, features beyond radiomics (such as data from clinical records, data obtained during treatment or biological and/or genetic) should also be incorporated. Regarding the choice of modelling methodology, the identification of optimal machinelearning methods for radiomic applications is a crucial step towards stable and clinically relevant CDSS; thus, in the ideal scenario, multiple machine-learning methods should be employed30 and the implementation should be comprehensively documented. A non-validated model is Figure 2 |Radiomics in cardiology. The current gold standard for quantification of coronary calcifications visible on CT is the \u2018Agatston\u2019 method (based upon intensity and volume). Radiomic features can improve quantification, differentiation between calcified and non-calcified plaques, and thus the prediction of Major Adverse Cardiac Events (MACE). REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 751 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Calibration-in-the-large Describes whether the predictions deviate systematically (intercept), whereas the calibration slope should ideally be equal to 1. The independence assumption The definition in terms of conditional probabilities is..",
            "url": "https://cris.maastrichtuniversity.nl/ws/files/36170434/Lambin_2017_Radiomics_the_Bridge_Between.pdf"
          },
          {
            "id": "E7176429902",
            "text": "..on both clinical applications and scientific researches. Recent progresses in deep learning have shed new light on medical image analysis by allowing discovering morphological and/or textural patterns in images solely from data. As deep learning methods have achieved the state-ofthe-art performance over different medical applications, its use for further improvement can be the major step in the medical computing field. However, there are still rooms for improvements. First, lessoned in computer vision, where breakthrough improvements were achieved by exploiting large amounts of training data, e.g., more than 1 million annotated images in ImageNet (19), it would be one direction to build such big publicly available dataset of medical images, by which deep models can find more generalized features in medical images, thus allowing making a leap in performance. Second, while the data-driven feature representations, especially in an unsupervised manner, helped enhance accuracy, it is also desirable to devise a new methodological architecture, with which it becomes possible to reflect or involve the domain-specific knowledge. Third, it is also necessary to develop algorithmic techniques to efficiently handle images acquired with different scanning protocols, by which there is no need to train modality-specific deep models. Last but not least, when applying deep learning to investigate the underlying patterns in images such as fMRI, due to the black-box like characteristics of deep models, it still remains challenging to understand and interpret the learned models intuitively. Shen et al. Page 19 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Acknowledgments This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.B0101-16-0307, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)). This work was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2015R1C1A1A01052216), NIH grants (EB006733, EB008374, EB009634, MH100217, MH108914, AG041721, AG049371, AG042599, DE022676). LITERATURE CITED 1. Brody H. Medical imaging. Nature. 2013; 502:S81\u2013S81. [PubMed: 24187698] 2. Schmidhuber J. Deep learning in neural networks: An overview. Neural Networks. 2015; 61:85\u2013117. [PubMed: 25462637] 3. Bengio Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning. 2009; 2:1\u2013127. 4. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521:436\u2013444. [PubMed: 26017442] 5. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006; 313:504\u2013507. [PubMed: 16873662] 6. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 2010; 11:3371\u20133408. 7. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning (ICML). 2010 8. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research. 2014; 15:1929\u2013 1958. 9. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Proceedings of International Conference on Machine Learning (ICML). 2015 10. Bishop, CM. Neural networks for pattern recognition. Oxford University Press, Inc; 1995. 11. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of International Conference on Machine Learning (ICML). 2008 12. Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. Proceedings of International Conference on Machine Learning (ICML). 2011 13. Hinton GE, Deng L, Yu D, Dahl GE, Mohamed A, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine. 2012; 29:82\u201397. 14. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Proceedings of Neural Information Processing Systems (NIPS). 2013:2553\u20132561. 15. Taigman Y, Yang M, Ranzato M, Wolf L. Deepface: Closing the gap to human-level performance in face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014 16. Zhang J, Zong C. Deep neural networks in machine translation: An overview. IEEE Intelligent Systems. 2015; 30:16\u201325. 17. Karpathy A, Li F. Deep visual-semantic alignments for generating image descriptions. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015 18. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, et al. Mastering the game of Go with deep neural networks and tree search. Nature. 2016; 529:484\u2013489. [PubMed: 26819042] 19. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015; 115:211\u2013252. 20. Everingham, M., Van Gool, L., Williams, CKI., Winn, J., Zisserman, A. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 2012. http://www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html Shen et al. Page 20 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript 21. Roux, L., Racoceanu, D., Capron, F., Calvo, J., Attieh, E., et al. MITOS-ATYPIA-14. 2014. http:// mitos-atypia-14.grand-challenge.org/ 22. Zhang W, Li R, Deng H, Wang L, Lin W, et al. Deep convolutional neural networks for multimodality isointense infant brain image segmentation. NeuroImage. 2015; 108:214\u2013224. [PubMed: 25562829] 23. Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, et al. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage. 2016; 129:460\u2013469. [PubMed: 26808333] 24. Wu G, Kim M, Wang Q, Munsell BC, Shen D. Scalable high-performance image registration framework by unsupervised deep feature representations learning. IEEE Transactions on Biomedical Engineering. 2016; 63:1505\u20131516. [PubMed: 26552069] 25. Suk HI, Lee SW, Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014; 101:569\u2013582. [PubMed: 25042445] 26. Shin H, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016 27. Suk HI, Lee SW, Shen D. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Structure and Function. 2015; 220:841\u2013859. [PubMed: 24363140] 28. Suk HI, Shen D. Deep learning in diagnosis of brain disorders. Recent Progress in Brain and Cognitive Engineering. 2015:203\u2013213. 29. Suk HI, Wee CY, Lee SW, Shen D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage. 2016; 129:292\u2013307. [PubMed: 26774612] 30. Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Transactions on Medical Imaging. 2016; 35:1240\u20131251. [PubMed: 26960222] 31. van Tulder G, de Bruijne M. Combining generative and discriminative representation learning for lung CT analysis with convolutional restricted boltzmann machines. IEEE Transactions on Medical Imaging. 2016; 35:1262\u20131272. [PubMed: 26886968] 32. Dou Q, Chen H, Yu L, Zhao L, Qin J, et al. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE Transactions on Medical Imaging. 2016; 35:1182\u20131195. [PubMed: 26886975] 33. Cire\u015fan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI). 2013:411\u2013418. 34. Chen H, Qi X, Cheng JZ, Heng PA. Deep contextual networks for neuronal structure segmentation. Proceedings of AAAI Conference on Artificial Intelligence. 2016 35. Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, et al. Computer-aided diagnosis with deep learning architecture..",
            "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
          }
        ]
      },
      "S4612206001": {
        "id": "S4612206001",
        "text": "The application of fully convolutional networks (FCNs) in pathology image analysis has demonstrated significant improvements in segmentation accuracy and computational efficiency, making them a promising approach for enhancing medical imaging applications.",
        "children": [
          {
            "id": "E3719434197",
            "text": "With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis. With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis. Optical microscopy of pathology slides captures the histologic details of tissues in high resolution. With the rapid advance of technology, whole slide imaging (WSI) is becoming part of the routine procedure for clinical diagnosis of many diseases. The emergence of digital pathology1Jara-Lazaro A.R. Thamboo T.P. Teh M. Tan P.H. Digital pathology: exploring its applications in diagnostic surgical pathology practice.Pathology. 2010; 42: 512-518Abstract Full Text PDF PubMed Scopus (77) Google Scholar, 2Webster J.D. Dunstan R.W. Whole-slide imaging and automated image analysis: considerations and opportunities in the practice of pathology.Vet Pathol. 2014; 51: 211-223Crossref PubMed Scopus (107) Google Scholar provides new opportunities to develop algorithms and software tools that can assist pathologists in clinical diagnosis and researchers in studying disease mechanisms. The digitalized pathology slides are often called images in the computer vision field, and can benefit from many image analysis algorithms. As an example, the common task where pathologists locate and recognize tissue components can also be achieved by image segmentation and recognition algorithms. Nowadays, digital pathology is making rapid progress owing to the success of deep learning.3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar Before the application of deep learning algorithms, digital pathology, due to its high complexity, achieved limited success with laborious modeling.4Janowczyk A. Madabhushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases.J Pathol Inform. 2016; 7: 29Crossref PubMed Scopus (700) Google Scholar Since 2012, deep learning has made significant improvements in all image recognition benchmarks.3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar, 5Krizhevsky A. Sutskever I. Hinton G.E. ImageNet classification with deep convolutional neural networks.Commun ACM. 2017; 60: 84-90Crossref Scopus (10391) Google Scholar, 6Goodfellow I. Bengio Y. Courville A. Bengio Y. Deep Learning. MIT Press, Cambridge, MA2016Google Scholar The applications of deep learning algorithms in digital pathology have had remarkable success in traditional pathology tasks. For example, deep learning algorithms achieved performance comparable to pathologists in interpreting whole slide images for the detection of tumor regions7Liu Y. Gadepalli K. Norouzi M. Dahl G.E. Kohlberger T. Boyko A. Venugopalan S. Timofeev A. Nelson P.Q. Corrado G.S. Detecting cancer metastases on gigapixel pathology images.arXiv. 2017; (arXiv:1703.02442)Google Scholar, 8Wang D. Khosla A. Gargeya R. Irshad H. Beck A.H. Deep learning for identifying metastatic breast cancer.arXiv. 2016; (arXiv:1606.05718)Google Scholar, 9Wang S. Chen A. Yang L. Cai L. Xie Y. Fujimoto J. Gazdar A. Xiao G. Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome.Sci Rep. 2018; 8: 10393Crossref PubMed Scopus (60) Google Scholar and lymph node metastases.10Ehteshami Bejnordi B. Veta M. van Diest P.J. van Ginneken B. Karssemeijer N. Litjens G. et al.Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer.JAMA. 2017; 318: 2199-2210Crossref PubMed Scopus (1444) Google Scholar Although this comparable performance may not generalize to all task domains, advanced methodology is anticipated to solve or aid in common challenges faced by pathologists, including locating neoplasia within a tissue and quantifying specific features such as mitoses and inflammation. To understand how deep learning excels in these areas, we build conceptual connections of deep learning in the machine learning literature. In essence, deep learning is a special kind of artificial neural network (ANN), which is one category of machine learning algorithm. Deep learning and other ANNs are inspired by biological neural networks and mathematically construct a network model with multiple connected layers. The first network layer (called the input layer) receives inputs (eg, slide images). It has a set of parameters and can use them to compute outputs. Similarly, each successive network layer receives inputs from its previous layers, uses its parameters, and computes outputs. At the end, the last network layer (called the output layer) calculates the outputs of the whole model. The layers between the input and output layers are not visible because they do not directly receive model input or generate model outputs, and thus are called the hidden layers. The structure of a segmentation neural network is illustrated in Figure 1A. In this process, prediction outputs from a good neural network can well approximate the observed outputs. Although ANNs claim excellent performances based on theoretical work,11Cybenko G. Approximation by superpositions of a sigmoidal function.Math Control Signal Syst. 1989; 2: 303-314Crossref Scopus (8650) Google Scholar historically, it has been notoriously hard to calculate the network parameters when the total number of network layers exceeded three, which limited the performance of the model. Fortunately, this is no longer a severe bottleneck, owing to the advancements in computational hardware, the scale of data accumulation, and the improvements in algorithms. Nowadays, popular ANNs can have hundreds of layers. The machine learning community refers to these algorithms as deep learning to distinguish them from the conventional shallow ANN algorithm. In this review, the application of deep learning algorithms in pathology image analysis is the focus. Convolutional neural networks (CNNs) are introduced, which have been widely used for image classification and pathology image analysis, such as tumor region and metastasis detection.7Liu Y. Gadepalli K. Norouzi M. Dahl G.E. Kohlberger T. Boyko A. Venugopalan S. Timofeev A. Nelson P.Q. Corrado G.S. Detecting cancer metastases on gigapixel pathology images.arXiv. 2017; (arXiv:1703.02442)Google Scholar, 8Wang D. Khosla A. Gargeya R. Irshad H. Beck A.H. Deep learning for identifying metastatic breast cancer.arXiv. 2016; (arXiv:1606.05718)Google Scholar, 9Wang S. Chen A. Yang L. Cai L. Xie Y. Fujimoto J. Gazdar A. Xiao G. Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome.Sci Rep. 2018; 8: 10393Crossref PubMed Scopus (60) Google Scholar In essence, a CNN can have a series of convolution layers as the hidden layers and thus make the network deep. This network structure enables the extraction of representational features for prediction. The design of CNN is inspired by the functional mechanism of the visual cortex3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar: instead of using all outputs from the previous layer, a convolution kernel only focuses on a certain area, the so-called receptive field, to compute a feature at the corresponding spatial position. By spatially sliding the receptive field along the input dimensions (eg, along the width and height directions for two-dimensional images), a feature map is computed as the outputs from the convolution layer. This process is illustrated in Figure 1B. Because the number of parameters is determined by the size of the..",
            "url": "https://openalex.org/W2952846726",
            "title": "Pathology Image Analysis Using Segmentation Deep Learning Algorithms",
            "publication_date": "2019-09-01"
          },
          {
            "id": "E0778979440",
            "text": "..has been notoriously hard to calculate the network parameters when the total number of network layers exceeded three, which limited the performance of the model. Fortunately, this is no longer a severe bottleneck, owing to the advancements in computational hardware, the scale of data accumulation, and the improvements in algorithms. Nowadays, popular ANNs can have hundreds of layers. The machine learning community refers to these algorithms as deep learning to distinguish them from the conventional shallow ANN algorithm. In this review, the application of deep learning algorithms in pathology image analysis is the focus. Convolutional neural networks (CNNs) are introduced, which have been widely used for image classification and pathology image analysis, such as tumor region and metastasis detection.7Liu Y. Gadepalli K. Norouzi M. Dahl G.E. Kohlberger T. Boyko A. Venugopalan S. Timofeev A. Nelson P.Q. Corrado G.S. Detecting cancer metastases on gigapixel pathology images.arXiv. 2017; (arXiv:1703.02442)Google Scholar, 8Wang D. Khosla A. Gargeya R. Irshad H. Beck A.H. Deep learning for identifying metastatic breast cancer.arXiv. 2016; (arXiv:1606.05718)Google Scholar, 9Wang S. Chen A. Yang L. Cai L. Xie Y. Fujimoto J. Gazdar A. Xiao G. Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome.Sci Rep. 2018; 8: 10393Crossref PubMed Scopus (60) Google Scholar In essence, a CNN can have a series of convolution layers as the hidden layers and thus make the network deep. This network structure enables the extraction of representational features for prediction. The design of CNN is inspired by the functional mechanism of the visual cortex3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar: instead of using all outputs from the previous layer, a convolution kernel only focuses on a certain area, the so-called receptive field, to compute a feature at the corresponding spatial position. By spatially sliding the receptive field along the input dimensions (eg, along the width and height directions for two-dimensional images), a feature map is computed as the outputs from the convolution layer. This process is illustrated in Figure 1B. Because the number of parameters is determined by the size of the receptive field, convolution layers have many fewer parameters than the image size. This design thus effectively reduces the number of parameters within a neural network and greatly improves its computational efficiency. In addition to image classification, CNNs have also been implemented for pathology image segmentation.12Xu J. Luo X. Wang G. Gilmore H. Madabhushi A. A deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images.Neurocomputing. 2016; 191: 214-223Crossref PubMed Scopus (324) Google Scholar To perform image segmentation for large data (eg, whole slide pathology images), the image is first divided into many small patches. A CNN is trained to classify these patches, and all patches in the same class are combined into one segmented area. Fine spatial resolution of segmentation can be achieved by using patches of small sizes; however, the patches need to be large enough that they can be classified accurately. To overcome the tradeoff between segmentation resolutions and patch size, instead of cutting the image region into adjacent patches, a moving window with a small step size is generally used to generate patches with a certain degree of overlap. In this procedure, the spatial resolution (determined by the step size instead of patch size) is largely improved. However, this demands a substantial amount of computing time and memory, which largely limits the computation speed. In recent years, several deep learning algorithms have been developed specifically for segmentation tasks, which can segment the image at pixel resolution and at a relatively high speed. In this review, the segmentation deep learning algorithms refer to semantic or instance segmentation algorithms, which are derivatives of CNNs. Compared with patch-based CNNs, segmentation deep learning algorithms are more computationally efficient in pixel classification and thus serve as powerful tools to extract detailed image information at pixel resolution.13Shelhamer E. Long J. Darrell T. Fully convolutional networks for semantic segmentation.IEEE Trans Pattern Anal Mach Intell. 2017; 39: 640-651Crossref PubMed Scopus (5329) Google Scholar, 14Yi F. Yang L. Wang S. Guo L. Huang C. Xie Y. Xiao G. Microvessel prediction in H&E stained pathology images using fully convolutional neural networks.BMC Bioinformatics. 2018; 19: 64Crossref PubMed Scopus (22) Google Scholar Compared with multilabel classification algorithms, which can identify15Wang J. Yang Y. Mao J.H. Huang Z.H. Huang C. Xu W. CNN-RNN: a unified framework for multi-label image classification.in: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, Los Alamitos, CA2016: 2285-2294Crossref Scopus (810) Google Scholar and locate16Chen T. Wang Z. Li G. Lin L. Recurrent attentional reinforcement learning for multi-label image recognition.arXiv. 2017; (arXiv:1712.07465)Google Scholar objects of different types, the segmentation deep learning algorithms detect not only the objects but also the segmentation boundaries. Compared with conventional image segmentation algorithms that are not based on deep learning, deep learning algorithms have robust performances under different staining conditions17Sheikhzadeh F. Ward R.K. van Niekerk D. Guillaud M. Automatic labeling of molecular biomarkers of immunohistochemistry images using fully convolutional networks.PLoS One. 2018; 13: e0190783Crossref PubMed Scopus (32) Google Scholar because they do not heavily rely on staining intensity or hand-crafted (ie, manually defined) features,18Ruiz A. Kong J. Ujaldon M. Boyer K. Saltz J. Gurcan M. Pathological image segmentation for neuroblastoma using the GPU.Proc IEEE Int Symp Biomed Imaging. 2008; 2008: 296-299Google Scholar, 19Kong H. Gurcan M. Belkacem-Boussaid K. Partitioning histopathological images: an integrated framework for supervised color-texture segmentation and cell splitting.IEEE Trans Med Imaging. 2011; 30: 1661-1677Crossref PubMed Scopus (174) Google Scholar, 20Alsubaie N. Trahearn N. Raza S.E.A. Snead D. Rajpoot N.M. Stain deconvolution using statistical analysis of multi-resolution stain colour representation.PLoS One. 2017; 12: e0169875Crossref PubMed Scopus (49) Google Scholar, 21Ma Z. Shiao S.L. Yoshida E.J. Swartwood S. Huang F. Doche M.E. Chung A.P. Knudsen B.S. Gertych A. Data integration from pathology slides for quantitative imaging of multiple cell types within the tumor immune cell infiltrate.Diagn Pathol. 2017; 12: 69Crossref PubMed Scopus (18) Google Scholar, 22Gonzalez R.C. Woods R.E. Digital Image Processing. Pearson Education, Upper Saddle River, NJ2002Google Scholar and they can utilize neighborhood structural information. Thus, deep learning\u2013based segmentations are anticipated to become an important tool in WSI analysis. In this review, the detailed process of deep learning\u2013based pathology image segmentation is described, including data preparation, image preprocessing, model selection and construction, post-processing, and feature extraction and association with disease (Figure 2). The goals of this review are to provide quick guidance for implementing deep learning\u2013based segmentation for pathology images and to provide some potential ways of further improving the segmentation performance for experienced investigators. Although there have previously been in-depth reviews on using machine learning methods, including deep learning, in digital pathology image analysis,4Janowczyk A. Madabhushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases.J Pathol Inform. 2016; 7: 29Crossref PubMed Scopus (700) Google Scholar, 23Komura D. Ishikawa S. Machine learning methods for histopathological image analysis.Comput Struct Biotechnol J. 2018; 16: 34-42Crossref PubMed Scopus (413) Google Scholar this is the first review of the implementations of segmentation deep learning algorithms for WSI analysis. Pathology images are usually as large as giga-pixels. Because the memory associated with a central processing unit (CPU) or a graphic processing unit (GPU) is often limited, the pathology images should first be chopped into small patches and/or resized, if necessary, to fit the CPU or GPU memory (Figure 2). Common image patch sizes range from 256 \u00d7 256 to 512 \u00d7 512 pixels. The size is often chosen to be divisible by 2 to avoid the trouble of padding for pooling layers. Here, padding means adding pixels to the upper, bottom, left, and right sides of the image, respectively. The value of the added pixels is usually set to 0, which is the so-called zero-padding. Pooling means extracting one representative pixel in each receptive field to reduce the size of the feature map. Max-pooling is the most common pooling method, which uses the maximum value to represent a receptive field. Although, in theory, image segmentation neural networks do not require input images to be of the same size, it is preferable for the images to be cropped into the same size to speed up the algorithm and to fully utilize parallel computations on CPU/GPU. Otherwise, the image patches should be resized or padded into the same size before being fed into the neural network. In this case, zero-padding (adding zero-value pixels to the image boundaries) and symmetric-padding (adding pixels with values symmetrical to the original pixel values along the image boundaries)..",
            "url": "https://openalex.org/W2952846726",
            "title": "Pathology Image Analysis Using Segmentation Deep Learning Algorithms",
            "publication_date": "2019-09-01"
          }
        ]
      },
      "S4915181933": {
        "id": "S4915181933",
        "text": "Fully convolutional networks (FCNs) have demonstrated significant improvements in segmentation accuracy and computational efficiency in medical imaging applications, indicating their potential as a methodology derived from natural image segmentation.",
        "children": [
          {
            "id": "E4240820307",
            "text": "Authors:\nGeert Litjens\nThijs Kooi\nBabak Ehteshami Bejnordi\nArnaud Arindra\nAdiyoso Setio\nFrancesco Ciompi\nMohsen Ghafoorian\nJeroen A W M Van Der Laak\nBram Van Ginneken\nAbstract\nDeep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.\n1. Introduction\nAs soon as it was possible to scan and load medical images into a computer, researchers have built systems for automated analysis. Initially, from the 1970s to the 1990s, medical image analysis was done with sequential application of low-level pixel processing (edge and line detector filters, region growing) and mathematical modeling (fitting lines, circles and ellipses) to construct compound rule-based systems that solved particular tasks. There is an analogy with expert systems with many if-then-else statements that were popular in artificial intelligence in the same period. These expert systems have been described as GOFAI (good oldfashioned artificial intelligence) (Haugeland, 1985) and were often brittle; similar to rule-based image processing systems.\nAt the end of the 1990s, supervised techniques, where training data is used to develop a system, were becoming increasingly popular in medical image analysis. Examples include active shape models (for segmentation), atlas methods (where the atlases that are fit to new data form the training data), and the concept of feature extraction and use of statistical classifiers (for computeraided detection and diagnosis). This pattern recognition or machine learning approach is still very popular and forms the basis of many successful commercially available medical image analysis systems. Thus, we have seen a shift from systems that are completely designed by humans to systems that are trained by computers using example data from which feature vectors are extracted. Computer algorithms determine the optimal decision boundary in the high-dimensional feature space. A crucial step in the design of such systems is the extraction of discriminant features from the images. This process is still done by human researchers and, as such, one speaks of systems with handcrafted features.\nA logical next step is to let computers learn the features that optimally represent the data for the problem at hand. This concept lies at the basis of many deep learning algorithms: models (networks) composed of many layers that transform input data (e.g. images) to outputs (e.g. disease present/absent) while learning increasingly higher level features. The most successful type of models for image analysis to date are convolutional neural networks (CNNs). CNNs contain many layers that transform their input with convolution filters of a small extent. Work on CNNs has been done since the late seventies (Fukushima, 1980) and they were already applied to medical image analysis in 1995 by Lo et al. (1995) . They saw their first successful real-world application in LeNet (LeCun et al., 1998) for hand-written digit recog-nition. Despite these initial successes, the use of CNNs did not gather momentum until various new techniques were developed for efficiently training deep networks, and advances were made in core computing systems. The watershed was the contribution of Krizhevsky et al. (2012) to the ImageNet challenge in December 2012. The proposed CNN, called AlexNet, won that competition by a large margin. In subsequent years, further progress has been made using related but deeper architectures (Russakovsky et al., 2014) . In computer vision, deep convolutional networks have now become the technique of choice.\nThe medical image analysis community has taken notice of these pivotal developments. However, the transition from systems that use handcrafted features to systems that learn features from the data has been gradual. Before the breakthrough of AlexNet, many different techniques to learn features were popular. Bengio et al. (2013) provide a thorough review of these techniques. They include principal component analysis, clustering of image patches, dictionary approaches, and many more. Bengio et al. (2013) introduce CNNs that are trained end-to-end only at the end of their review in a section entitled Global training of deep models. In this survey, we focus particularly on such deep models, and do not include the more traditional feature learning approaches that have been applied to medical images. For a broader review on the application of deep learning in health informatics we refer to Ravi et al. (2017) , where medical image analysis is briefly touched upon.\nApplications of deep learning to medical image analysis first started to appear at workshops and conferences, and then in journals. The number of papers grew rapidly in 2015 and 2016. This is illustrated in Figure 1 . The topic is now dominant at major conferences and a first special issue appeared of IEEE Transaction on Medical Imaging in May 2016 (Greenspan et al., 2016) .\nOne dedicated review on application of deep learning to medical image analysis was published by Shen et al. (2017) . Although they cover a substantial amount of work, we feel that important areas of the field were not represented. To give an example, no work on retinal image analysis was covered. The motivation for our review was to offer a comprehensive overview of (almost) all fields in medical imaging, both from an application and a methodology-drive perspective. This also includes overview tables of all publications which readers can use to quickly assess the field. Last, we leveraged our own experience with the application of deep learning methods to medical image analysis to provide readers with a dedicated discussion section covering the state-of-the-art, open challenges and overview of research directions and technologies that will become important in the future.\nThis survey includes over 300 papers, most of them recent, on a wide variety of applications of deep learning in medical image analysis. To identify relevant contributions PubMed was queried for papers containing (\"convolutional\" OR \"deep learning\") in title or abstract. ArXiv was searched for papers mentioning one of a set of terms related to medical imaging. Additionally, conference proceedings for MICCAI (including workshops), SPIE, ISBI and EMBC were searched based on titles of papers. We checked references in all selected papers and consulted colleagues. We excluded papers that did not report results on medical image data or only used standard feed-forward neural networks with handcrafted features. When overlapping work had been reported in multiple publications, only the publication(s) deemed most important were included. We expect the search terms used to cover most, if not all, of the work incorporating deep learning methods. The last update to the included papers was on February 1, 2017. The appendix describes the search process in more detail.\nSummarizing, with this survey we aim to:\n\u2022 show that deep learning techniques have permeated the entire field of medical image analysis;\n\u2022 identify the challenges for successful application of deep learning to medical imaging tasks;\n\u2022 highlight specific contributions which solve or circumvent these challenges.\nThe rest of this survey as structured as followed. In Section 2 we introduce the main deep learning techniques that have been used for medical image analysis and that are referred to throughout the survey. Section 3 describes the contributions of deep learning to canonical tasks in medical image analysis: classification, detection, segmentation, registration, retrieval, image generation and enhancement. Section 4 discusses obtained results and open challenges in different application areas: neuro, ophthalmic, pulmonary, digital pathology and cell imaging, breast, cardiac, abdominal, musculoskeletal, and remaining miscellaneous applications. We end with a summary, a critical discussion and an outlook for future research.\n2. Overview of deep learning methods\nThe goal of this section is to provide a formal introduction and definition of the deep learning concepts, Figure 1 : Breakdown of the papers included in this survey in year of publication, task addressed (Section 3), imaging modality, and application area (Section 4). The number of papers for 2017 has been extrapolated from the papers published in January.\ntechniques and architectures that we found in the medical image analysis papers surveyed in this work.\n2.1. Learning algorithms\nMachine learning methods are generally divided into supervised and unsupervised learning algorithms, although there are many nuances. In supervised learning, a model is presented with a dataset D = {x, y} N n=1 of input features x and label y pairs, where y typically represents an instance of a fixed set of classes. In the case of regression tasks y can also be a vector with continuous values. Supervised training typically amounts to finding model parameters \u0398 that best predict the data based on a loss function L(y, \u0177). Here \u0177 denotes the output of the model obtained by feeding a data point x to the function f (x; \u0398) that represents the model.\nUnsupervised learning algorithms process data without labels and are trained to find patterns, such as latent subspaces. Examples of traditional unsupervised learning algorithms are principal component analysis and clustering methods. Unsuperv",
            "url": "https://openalex.org/W2592929672",
            "title": "A survey on deep learning in medical image analysis",
            "publication_date": "2017-12-01"
          }
        ]
      },
      "S4163962705": {
        "id": "S4163962705",
        "text": "Standardized imaging protocols and robust validation methodologies are essential for the maturation of radiomics as a discipline, which can be informed by the practices established in natural image segmentation to ensure consistency and reliability in medical imaging models.",
        "children": [
          {
            "id": "E6769120955",
            "text": "..for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics. REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 749 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Exploratory analysis Medical imaging Data selection VOI Prediction target Imaging protocols Test feature stability Report protocols Add prediction target Report algorithms Store data Robust segmentation Add clinical variables Archetypal features Feature selection Internal validation Radiomics External validation Report methodology Modeling Feature extraction RQS 1 RQS 2 RQS 3 RQS Total 36 Image protocol quality +1 or +2 RQS checkpoint 1 total: 2 +1 +1 +1 RQS checkpoint 2 total: 3 Multiple segmentation Phantom study Imaging at multiple time points Feature reduction or adjustment for multiple testing Multivariable analysis Biological correlates Cut-off analysis Discrimination statistics Calibration statistics Prospective study Validation Comparison to \u2018gold standard\u2019 Costeffectiveness analysis Potential clinical applications Open science and data -3 or +3 +1 +1 +1 +1 or +2 +1 or +2 +7 -5 to +5 +2 +2 +1 +1 to +4 RQS checkpoint 3 total: 31 Nature Reviews | Clinical Oncology evaluation of their performance, reproducibility, and/or clinical utility17. Consequently, these models might not be appropriate for CDSS. In this Review, we describe the process of radiomics along with latest developments in the field. The pitfalls, challenges, and opportunities presented by radiomics to improve CDSS for personalized precision oncology are highlighted, with an emphasis on the methodological aspects of radiomics prediction model development and validation. We explore the advanced and innovative information technologies that are essential for the data management of diverse multimodal data sources. Finally, we offer a vision of the necessary steps to ensure continued progression and widespread acceptance of both radiomics and CDSS. The workflow of radiomics Radiomics is defined as the quantitative mapping, that is, extraction, analysis and modelling of many medical image features in relation to prediction targets, such as clinical end points and genomic features. A radiomics study can be structured in five phases: data selection, medical imaging, feature extraction, exploratory analysis, and modelling (FIG. 1). To assess the quality of radiomics studies, we propose the radiomics quality score (RQS). Data selection Radiomic analyses begins with the choice of an imaging protocol, the volume of interest (VOI) and a prediction target \u2014 the event one wishes to predict. Typically, the entire primary tumour is analysed and linked to available data on treatment outcomes, such as survival. Radiomic analyses can be performed on subregions of the tumour (habitats), metastatic lesions, as well as in normal tissues. Analysis of these regions might yield radiosensitive phenotypes, which has implications for treatment planning strategies. Radiomics analysis, however, is not restricted to radiotherapy and can be applied to any image generated in the clinical setting (FIG. 2). The importance of using standardized imaging protocols to eliminate unnecessary confounding variability is recognized9,18; however, nonstandardized imaging protocols are commonplace. Therefore, reproducibility and Key points \u2022 Radiomics is becoming increasingly more important in medical imaging \u2022 The explosion of medical imaging data creates an environment ideal for machine-learning and data-based science \u2022 Radiomics-based decision-support systems for precision diagnosis and treatment can be a powerful tool in modern medicine \u2022 Large-scale data sharing is necessary for the validation and full potential that radiomics represents \u2022 Standardized data collection, evaluation criteria, and reporting guidelines are required for radiomics to mature as a discipline Figure 1 | Flowchart depicting the workflow of radiomics and the application of the RQS. The workflow includes the necessary steps in a radiomic analysis. The RQS both rewards and penalizes the methodology and analyses of a study, consequently encouraging the best scientific practice. RSQ, radiomics quality score; VOI, volume of interest. REVIEWS 750 | DECEMBER 2017 | VOLUME 14 www.nature.com/nrclinonc \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mi t e d, p a rt o f S p ri n g e r N a t u r e. Al l ri g h t s r e s e r v e d. Nature Reviews | Clinical Oncology Cardiac CT Radiomics MACE Analysis Phantom studies An artificial structure that imitates human tissue properties is scanned on multiple machines to characterize scan output against a known physical standard. comparability of radiomic studies can be achieved only by extensive disclosure of imaging protocols. We wish to emphasize this point, and provide examples of how protocols should be reported in future radiomics studies (Supplementary information S1). Medical imaging Segmentation. VOIs are segmented manually or (semi-) automatically19. This segmentation determines which voxels within an image are analysed, thus, the variability in segmentation can introduce bias in the evaluation of derived radiomic features20. Multiple-segmentation is a method to limit the extent of this bias. Examples that enable robust features to be observed21 include: evaluation by multiple clinicians, perturb segmentations with noise, combination of diverse algorithms, or use different stages of the breathing cycle. Key considerations are how the segmentation was performed, and how sensitive the radiomics analysis is to different segmentation methods22. For example, a semiautomatic segmentation method can result in different radiomic features than a manual delineation. Phantom studies. The determination of inter-scanner and inter-vendor variability of features is important in radiomics23. In cases in which radiomic studies rely on data from multiple scanners, neglecting this variability can jeopardize the analysis of studies \u2014 that is, the proposed radiomic-based prediction model might not perform adequately on external datasets if new data are acquired on different scanners. As data from patients scanned on multiple devices is scarce and subject to uncertainties (such as organ motion, or different imaging protocols), phantom studies are a suitable means to gauge these uncertainties and identify features that rely on the vendor. In essence, phantom studies provide a risk-mitigation strategy to help navigate from the current clinical imaging scenario to the desired optimal imaging scenario. Imaging at multiple time points. Additional sources of variability in radiomics features are organ motion or expansion or shrinkage of the target volume. Radiomics features that are strongly dependent on these factors can have limited applicability. To account for these sources of variability, available test-retest data24\u201326 can be exploited to measure radiomics feature stability. For example, two datasets of images acquired within a small period of time from a patient cohort. Feature extraction The essence of radiomics is the high-throughput extraction of quantitative image features to characterize VOIs. Feature values are dependent upon factors that can include image pre-processing (for example, filtering, or intensity discretization) and reconstruction (for example, filtered back projection, or iterative reconstruction). Furthermore, variation exists in feature nomenclature, mathematical definition, methodology, and software implementation of the applied feature extraction algorithms27\u201329. In order to facilitate inter-operability of radiomic features, differences in nomenclature, algorithms, software implementations, as well as other methodological aspects must be elucidated. Exploratory analysis Radiomic and non-radiomic features should be combined with the prediction target to create a single dataset. This approach enables the investigation of relationships between features. Groups of highly correlated radiomics features can be identified via clustering, and these features can be reduced to single archetypal features per cluster. Radiomic features that are well-correlated with routine clinical features (such as tumour stage) do not provide additional information. Auxiliary feature data collected from multiple segmentations, multiple imaging, and phantom studies, can be exploited to assess feature robustness. Volatile or robust features can be identified and subsequently excluded from model development. For example, a feature that is robust for the prediction of overall survival for lung cancer (that is, imaged and segmented in a certain way) for a given dataset could be volatile for the prediction of pneumonitis in lung cancer (imaged and segmented in an alternative way) for a given dataset. Thus, the process of feature reduction and/or exclusion should be described clearly. Modelling Radiomic modelling involves three major aspects: feature selection, modelling methodology, and validation. Feature selection should be data-driven owing to the vast in- human range of possible radiomics features; such analysis should be performed in a robust and transparent manner. To achieve holistic models, features beyond radiomics (such as data from clinical records, data obtained during treatment or biological and/or genetic) should also be incorporated. Regarding the choice of modelling methodology, the identification of optimal machinelearning methods for radiomic applications is a crucial step towards stable and clinically relevant CDSS; thus, in the ideal scenario, multiple machine-learning methods should be employed30 and the implementation should be comprehensively documented. A non-validated model is Figure 2 |Radiomics in cardiology. The current gold standard for quantification of coronary calcifications visible on CT is the \u2018Agatston\u2019 method (based upon intensity and volume). Radiomic features can improve quantification, differentiation between calcified and non-calcified plaques, and thus the prediction of Major Adverse Cardiac Events (MACE). REVIEWS NATURE REVIEWS | CLINICAL ONCOLOGY VOLUME 14 | DECEMBER 2017 | 751 \u00a9 2 0 1 7 M a c mill a n P u bl i s h e r s Li mit e d, p a rt o f S p ri n g e r N a t u r e. All ri g h t s r e s e r v e d. Calibration-in-the-large Describes whether the predictions deviate systematically (intercept), whereas the calibration slope should ideally be equal to 1. The independence assumption The definition in terms of conditional probabilities is..",
            "url": "https://cris.maastrichtuniversity.nl/ws/files/36170434/Lambin_2017_Radiomics_the_Bridge_Between.pdf"
          },
          {
            "id": "E7176429902",
            "text": "..on both clinical applications and scientific researches. Recent progresses in deep learning have shed new light on medical image analysis by allowing discovering morphological and/or textural patterns in images solely from data. As deep learning methods have achieved the state-ofthe-art performance over different medical applications, its use for further improvement can be the major step in the medical computing field. However, there are still rooms for improvements. First, lessoned in computer vision, where breakthrough improvements were achieved by exploiting large amounts of training data, e.g., more than 1 million annotated images in ImageNet (19), it would be one direction to build such big publicly available dataset of medical images, by which deep models can find more generalized features in medical images, thus allowing making a leap in performance. Second, while the data-driven feature representations, especially in an unsupervised manner, helped enhance accuracy, it is also desirable to devise a new methodological architecture, with which it becomes possible to reflect or involve the domain-specific knowledge. Third, it is also necessary to develop algorithmic techniques to efficiently handle images acquired with different scanning protocols, by which there is no need to train modality-specific deep models. Last but not least, when applying deep learning to investigate the underlying patterns in images such as fMRI, due to the black-box like characteristics of deep models, it still remains challenging to understand and interpret the learned models intuitively. Shen et al. Page 19 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Acknowledgments This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.B0101-16-0307, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)). This work was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2015R1C1A1A01052216), NIH grants (EB006733, EB008374, EB009634, MH100217, MH108914, AG041721, AG049371, AG042599, DE022676). LITERATURE CITED 1. Brody H. Medical imaging. Nature. 2013; 502:S81\u2013S81. [PubMed: 24187698] 2. Schmidhuber J. Deep learning in neural networks: An overview. Neural Networks. 2015; 61:85\u2013117. [PubMed: 25462637] 3. Bengio Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning. 2009; 2:1\u2013127. 4. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521:436\u2013444. [PubMed: 26017442] 5. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006; 313:504\u2013507. [PubMed: 16873662] 6. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 2010; 11:3371\u20133408. 7. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning (ICML). 2010 8. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research. 2014; 15:1929\u2013 1958. 9. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Proceedings of International Conference on Machine Learning (ICML). 2015 10. Bishop, CM. Neural networks for pattern recognition. Oxford University Press, Inc; 1995. 11. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of International Conference on Machine Learning (ICML). 2008 12. Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. Proceedings of International Conference on Machine Learning (ICML). 2011 13. Hinton GE, Deng L, Yu D, Dahl GE, Mohamed A, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine. 2012; 29:82\u201397. 14. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Proceedings of Neural Information Processing Systems (NIPS). 2013:2553\u20132561. 15. Taigman Y, Yang M, Ranzato M, Wolf L. Deepface: Closing the gap to human-level performance in face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014 16. Zhang J, Zong C. Deep neural networks in machine translation: An overview. IEEE Intelligent Systems. 2015; 30:16\u201325. 17. Karpathy A, Li F. Deep visual-semantic alignments for generating image descriptions. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015 18. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, et al. Mastering the game of Go with deep neural networks and tree search. Nature. 2016; 529:484\u2013489. [PubMed: 26819042] 19. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015; 115:211\u2013252. 20. Everingham, M., Van Gool, L., Williams, CKI., Winn, J., Zisserman, A. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 2012. http://www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html Shen et al. Page 20 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript 21. Roux, L., Racoceanu, D., Capron, F., Calvo, J., Attieh, E., et al. MITOS-ATYPIA-14. 2014. http:// mitos-atypia-14.grand-challenge.org/ 22. Zhang W, Li R, Deng H, Wang L, Lin W, et al. Deep convolutional neural networks for multimodality isointense infant brain image segmentation. NeuroImage. 2015; 108:214\u2013224. [PubMed: 25562829] 23. Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, et al. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage. 2016; 129:460\u2013469. [PubMed: 26808333] 24. Wu G, Kim M, Wang Q, Munsell BC, Shen D. Scalable high-performance image registration framework by unsupervised deep feature representations learning. IEEE Transactions on Biomedical Engineering. 2016; 63:1505\u20131516. [PubMed: 26552069] 25. Suk HI, Lee SW, Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014; 101:569\u2013582. [PubMed: 25042445] 26. Shin H, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016 27. Suk HI, Lee SW, Shen D. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Structure and Function. 2015; 220:841\u2013859. [PubMed: 24363140] 28. Suk HI, Shen D. Deep learning in diagnosis of brain disorders. Recent Progress in Brain and Cognitive Engineering. 2015:203\u2013213. 29. Suk HI, Wee CY, Lee SW, Shen D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage. 2016; 129:292\u2013307. [PubMed: 26774612] 30. Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Transactions on Medical Imaging. 2016; 35:1240\u20131251. [PubMed: 26960222] 31. van Tulder G, de Bruijne M. Combining generative and discriminative representation learning for lung CT analysis with convolutional restricted boltzmann machines. IEEE Transactions on Medical Imaging. 2016; 35:1262\u20131272. [PubMed: 26886968] 32. Dou Q, Chen H, Yu L, Zhao L, Qin J, et al. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE Transactions on Medical Imaging. 2016; 35:1182\u20131195. [PubMed: 26886975] 33. Cire\u015fan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI). 2013:411\u2013418. 34. Chen H, Qi X, Cheng JZ, Heng PA. Deep contextual networks for neuronal structure segmentation. Proceedings of AAAI Conference on Artificial Intelligence. 2016 35. Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, et al. Computer-aided diagnosis with deep learning architecture..",
            "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
          }
        ]
      },
      "S3469843940": {
        "id": "S3469843940",
        "text": "The application of data augmentation strategies, commonly used in natural image segmentation, can help address the challenge of limited training datasets in medical imaging, thereby improving the performance of deep learning models in this domain.",
        "children": [
          {
            "id": "E7176429902",
            "text": "..on both clinical applications and scientific researches. Recent progresses in deep learning have shed new light on medical image analysis by allowing discovering morphological and/or textural patterns in images solely from data. As deep learning methods have achieved the state-ofthe-art performance over different medical applications, its use for further improvement can be the major step in the medical computing field. However, there are still rooms for improvements. First, lessoned in computer vision, where breakthrough improvements were achieved by exploiting large amounts of training data, e.g., more than 1 million annotated images in ImageNet (19), it would be one direction to build such big publicly available dataset of medical images, by which deep models can find more generalized features in medical images, thus allowing making a leap in performance. Second, while the data-driven feature representations, especially in an unsupervised manner, helped enhance accuracy, it is also desirable to devise a new methodological architecture, with which it becomes possible to reflect or involve the domain-specific knowledge. Third, it is also necessary to develop algorithmic techniques to efficiently handle images acquired with different scanning protocols, by which there is no need to train modality-specific deep models. Last but not least, when applying deep learning to investigate the underlying patterns in images such as fMRI, due to the black-box like characteristics of deep models, it still remains challenging to understand and interpret the learned models intuitively. Shen et al. Page 19 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Acknowledgments This work was supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.B0101-16-0307, Basic Software Research in Human-level Lifelong Machine Learning (Machine Learning Center)). This work was also supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2015R1C1A1A01052216), NIH grants (EB006733, EB008374, EB009634, MH100217, MH108914, AG041721, AG049371, AG042599, DE022676). LITERATURE CITED 1. Brody H. Medical imaging. Nature. 2013; 502:S81\u2013S81. [PubMed: 24187698] 2. Schmidhuber J. Deep learning in neural networks: An overview. Neural Networks. 2015; 61:85\u2013117. [PubMed: 25462637] 3. Bengio Y. Learning deep architectures for ai. Foundations and Trends in Machine Learning. 2009; 2:1\u2013127. 4. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521:436\u2013444. [PubMed: 26017442] 5. Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neural networks. Science. 2006; 313:504\u2013507. [PubMed: 16873662] 6. Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 2010; 11:3371\u20133408. 7. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Proceedings of International Conference on Machine Learning (ICML). 2010 8. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research. 2014; 15:1929\u2013 1958. 9. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Proceedings of International Conference on Machine Learning (ICML). 2015 10. Bishop, CM. Neural networks for pattern recognition. Oxford University Press, Inc; 1995. 11. Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of International Conference on Machine Learning (ICML). 2008 12. Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. Proceedings of International Conference on Machine Learning (ICML). 2011 13. Hinton GE, Deng L, Yu D, Dahl GE, Mohamed A, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine. 2012; 29:82\u201397. 14. Szegedy C, Toshev A, Erhan D. Deep neural networks for object detection. Proceedings of Neural Information Processing Systems (NIPS). 2013:2553\u20132561. 15. Taigman Y, Yang M, Ranzato M, Wolf L. Deepface: Closing the gap to human-level performance in face verification. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014 16. Zhang J, Zong C. Deep neural networks in machine translation: An overview. IEEE Intelligent Systems. 2015; 30:16\u201325. 17. Karpathy A, Li F. Deep visual-semantic alignments for generating image descriptions. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015 18. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, et al. Mastering the game of Go with deep neural networks and tree search. Nature. 2016; 529:484\u2013489. [PubMed: 26819042] 19. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, et al. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision. 2015; 115:211\u2013252. 20. Everingham, M., Van Gool, L., Williams, CKI., Winn, J., Zisserman, A. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. 2012. http://www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html Shen et al. Page 20 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript 21. Roux, L., Racoceanu, D., Capron, F., Calvo, J., Attieh, E., et al. MITOS-ATYPIA-14. 2014. http:// mitos-atypia-14.grand-challenge.org/ 22. Zhang W, Li R, Deng H, Wang L, Lin W, et al. Deep convolutional neural networks for multimodality isointense infant brain image segmentation. NeuroImage. 2015; 108:214\u2013224. [PubMed: 25562829] 23. Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, et al. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage. 2016; 129:460\u2013469. [PubMed: 26808333] 24. Wu G, Kim M, Wang Q, Munsell BC, Shen D. Scalable high-performance image registration framework by unsupervised deep feature representations learning. IEEE Transactions on Biomedical Engineering. 2016; 63:1505\u20131516. [PubMed: 26552069] 25. Suk HI, Lee SW, Shen D. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage. 2014; 101:569\u2013582. [PubMed: 25042445] 26. Shin H, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016 27. Suk HI, Lee SW, Shen D. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Structure and Function. 2015; 220:841\u2013859. [PubMed: 24363140] 28. Suk HI, Shen D. Deep learning in diagnosis of brain disorders. Recent Progress in Brain and Cognitive Engineering. 2015:203\u2013213. 29. Suk HI, Wee CY, Lee SW, Shen D. State-space model with deep learning for functional dynamics estimation in resting-state fMRI. NeuroImage. 2016; 129:292\u2013307. [PubMed: 26774612] 30. Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Transactions on Medical Imaging. 2016; 35:1240\u20131251. [PubMed: 26960222] 31. van Tulder G, de Bruijne M. Combining generative and discriminative representation learning for lung CT analysis with convolutional restricted boltzmann machines. IEEE Transactions on Medical Imaging. 2016; 35:1262\u20131272. [PubMed: 26886968] 32. Dou Q, Chen H, Yu L, Zhao L, Qin J, et al. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE Transactions on Medical Imaging. 2016; 35:1182\u20131195. [PubMed: 26886975] 33. Cire\u015fan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. Proceedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI). 2013:411\u2013418. 34. Chen H, Qi X, Cheng JZ, Heng PA. Deep contextual networks for neuronal structure segmentation. Proceedings of AAAI Conference on Artificial Intelligence. 2016 35. Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, et al. Computer-aided diagnosis with deep learning architecture..",
            "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
          },
          {
            "id": "E4812619525",
            "text": "Deep Learning in Medical Image Analysis Dinggang Shen1,3, Guorong Wu2, and Heung-Il Suk3 1Department of Radiology, University of North Carolina at Chapel Hill, NC, USA, 27599 2Department of Radiology, University of North Carolina at Chapel Hill, NC, USA, 27599 3Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea, 02841 Abstract The computer-assisted analysis for better interpreting images have been longstanding issues in the medical imaging field. On the image-understanding front, recent advances in machine learning, especially, in the way of deep learning, have made a big leap to help identify, classify, and quantify patterns in medical images. Specifically, exploiting hierarchical feature representations learned solely from data, instead of handcrafted features mostly designed based on domain-specific knowledge, lies at the core of the advances. In that way, deep learning is rapidly proving to be the state-of-the-art foundation, achieving enhanced performances in various medical applications. In this article, we introduce the fundamentals of deep learning methods; review their successes to image registration, anatomical/cell structures detection, tissue segmentation, computer-aided disease diagnosis or prognosis, and so on. We conclude by raising research issues and suggesting future directions for further improvements. Keywords Medical image analysis; deep learning; unsupervised feature learning 1. INTRODUCTION Over the last decades, we have witnessed the importance of medical imaging, e.g., computed tomography (CT), magnetic resonance (MR), positron emission tomography (PET), mammography, ultrasound, X-ray, and so on, for the early detection, diagnosis, and treatment of diseases (1). In the clinic, the medical image interpretation has mostly been performed by human experts such as radiologists and physicians. However, due to large variations in pathology and potential fatigue of human experts, researchers and doctors have recently begun to benefit from computer-assisted interventions. While, compared to the advances in medical imaging technologies, it is belated for the advances in computational D. Shen and H.-I. Suk are the co-corresponding authors DISCLOSURE STATEMENT The authors are not aware of any affiliations, memberships, funding, or financial holdings that might be perceived as affecting the objectivity of this review. HHS Public Access Author manuscript Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Published in final edited form as: Annu Rev Biomed Eng. 2017 June 21; 19: 221\u2013248. doi:10.1146/annurev-bioeng-071516-044442. Author Manuscript Author Manuscript Author Manuscript Author Manuscript medical image analysis, it has recently been improving with the help of machine learning techniques. In the stream of applying machine learning for data analysis, meaningful feature extraction or feature representation lies at the heart of its success to accomplish target tasks. Conventionally, meaningful or task-related features were mostly designed by human experts based on their knowledge about the target domains, which thus made it challenging for nonexperts to exploit machine learning techniques for their own studies. However, deep learning (2) has relieved such obstacles by absorbing the feature engineering step into a learning step. That is, instead of extracting features in a hand-designed manner, deep learning requires only a set of data with minor preprocessing, if necessary, and then discovers the informative representations in a self-taught manner (3, 4). So, now the burden of feature engineering has shifted from a human-side to a computer-side, thus allowing non-experts in machine learning to effectively use deep learning for their own researches and/or applications, especially in medical image analysis. The unprecedented success of deep learning arises mostly from the following factors: (1) advancements of high-tech central processing units (CPUs) and graphics processing units (GPUs); (ii) availability of a huge amount of data (i.e., big data); (iii) developments of learning algorithms (5, 6, 7, 8, 9). Technically, deep learning can be regarded as an improvement of the conventional artificial neural networks (10) by building networks with multiple (more than two) layers. It is empirically shown that deep neural networks can discover hierarchical feature representations such that the higher level features can be derived from the lower level features (4). Thanks to its nice characteristic of learning hierarchical feature representations solely from data, deep learning has achieved recordbreaking performance in a variety of artificial intelligence applications (11, 12, 13, 14, 15, 16, 17, 18) and grand challenges (19, 20, 21). Particularly, great improvements in computer vision inspired its use to medical image analysis such as image segmentation (22, 23), image registration (24), image fusion (25), image annotation (26), computer-aided diagnosis and prognosis (27, 28, 29), lesion/landmark detection (30, 31, 32), and microscopic imaging analysis (33, 34), to name a few. Deep learning methods are highly effective when the number of available samples are large during a training stage. For example, in ImageNet Large Scale Visual Recognition Challenge (ILSVRC), more than 1 million annotated images were provided (19). However, as for medical applications, we usually have a very limited number of images, e.g., less than 1,000 images. Therefore, one of the main challenges in applying deep learning to medical images arises from the limited small number of available training samples to build deep models without suffering from overfitting. To this end, research groups have devised various strategies, such as (i) to take image patches either 2D or 3D as input (25, 35, 36, 37, 38, 39, 40, 41), rather than the full-sized images, to reduce the input dimensionality, thus the number of model parameters; (ii) to expand their dataset by artificially generating samples via affine transformation (i.e., data augmentation) and then train their network from scratch with the augmented dataset (35, 36, 37, 38); (iii) to use deep models trained over a huge number of natural images in computer vision as \u2018off-the-shelf\u2019 feature extractor and then train the final classifier or output layer with the target-task samples (39, 41); (iv) to initialize Shen et al. Page 2 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript model parameters with those of pre-trained models from non-medical or natural images and then fine-tune the network parameters with the task-related samples (42, 43); (v) to use models trained with small-sized inputs for arbitrarily-sized inputs by transforming weights in the fully connected layers into convolutional kernels (32, 44). In terms of the input types, we can categorize deep models as typical multi-layer neural networks that take input values in vector form (i.e., non-structured) and convolutional networks that takes 2D or 3D shaped (i.e., structured) values as input. Because of the structural characteristic of images (i.e., the structural or configural information among neighboring pixels or voxels is another important source of information), convolutional neural networks have gained great interest in medical image analysis (33, 45, 32, 46, 22, 44, 31). However, networks with vectorized inputs were also successfully applied to different medical applications (47, 25, 27, 29, 48, 24, 49, 50). Along with deep neural networks, deep generative models (51) such as deep belief networks and deep Boltzmann machines that are the probabilistic graphical models with multiple layers of hidden variables have also been successfully applied to brain disease diagnosis (43, 25, 52, 29), lesion segmentation (53, 45, 32, 54), cell segmentation (33, 55, 34, 56), image parsing (57, 58, 59), and tissue classification (31, 46, 22, 44). In this article, we first explain the computational theories of neural networks and deep models (e.g., stacked auto-encoder, deep belief network, deep Boltzmann machine, convolutional neural network) and their fundamentals of extracting high-level representations from data in Section 2. Section 3 introduces recent studies that exploited deep models for different applications in medical imaging by covering image registration, anatomy localization, lesion segmentation, object/cell detection, tissue segmentation, and computer-aided detection and diagnosis. Finally, we conclude this article by summarizing research trends and suggesting directions for further improvements in Section 4. 2. DEEP MODELS In this section, we explain the fundamental concepts of feed-forward neural networks and basic deep models in the literature. The contents are specifically focused on learning hierarchical feature representations from data. It is also described how to efficiently learn parameters of deep architecture by reducing overfitting. 2.1. Feed-forward neural networks In machine learning, artificial neural networks are a family of models that mimic the structural elegance of the neural system and learn patterns inherent in observations. The perceptron (60) is the earliest trainable neural network with a single-layer architecture1, composed of an input layer and an output layer. The perceptron or modified perceptron with multiple output units in Fig. 1(a) is regarded as a linear model, which prohibits their applications for tasks of involving complicated data patterns, despite the use of non-linear activation functions in the output layer. 1In general, the input layer is not counted. Shen et al. Page 3 Annu Rev Biomed Eng. Author manuscript; available in PMC 2017 June 21. Author Manuscript Author Manuscript Author Manuscript Author Manuscript Such limitation is successfully circumvented by introducing the so-called \u2018hidden\u2019 layer between the input layer and the output layer. Note that in neural networks the units of the neighboring layers are fully connected to each other, but there are no connections among the units in the same layer. For a two-layer neural network in Fig. 1(b), also called as multi-layer perceptron, given..",
            "url": "https://europepmc.org/articles/pmc5479722?pdf=render"
          }
        ]
      },
      "S5350334166": {
        "id": "S5350334166",
        "text": "The integration of multimodal data sources in medical imaging, as evidenced by the use of deep learning algorithms, can improve the robustness and clinical utility of segmentation models, suggesting that techniques from natural image segmentation should also consider data diversity.",
        "children": [
          {
            "id": "E2792707067",
            "text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 5, MAY 2016 1285 Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning Hoo-Chang Shin, Member, IEEE, Holger R. Roth, Mingchen Gao, Le Lu, Senior Member, IEEE, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura, and Ronald M. Summers* Abstract\u2014Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, Manuscript received January 08, 2016; revised February 04, 2016; accepted February 05, 2016. Date of publication February 11, 2016; date of current version April 29, 2016. This work was supported in part by the Intramural Research Program of the National Institutes of Health Clinical Center, and in part by a grant from the KRIBB Research Initiative Program (Korean Biomedical Scientist Fellowship Program), Korea Research Institute of Bioscience and Biotechnology, Republic of Korea . H. Roth and M. Gao contributed equally to this work. Asterisk indicates corresponding author. H.-C. Shin, H. R. Roth, and I. Nogues are with the Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA (e-mail: hoochang.shin@nih.gov; holger. roth@nih.gov; isabella.nogues@nih.gov). M. Gao, Z. Xu, and D. Mollura are with the Center for Infectious Disease Imaging, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA (e-mail: mingchen.gao@nih.gov; ziyue.xu@nih.gov; molluradj@nih.gov). L. Lu and J. Yao are with the Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and also with the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA (e-mail: le.lu@nih.gov; jyao@nih.gov). *R. M. Summers is with the Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and also with the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA (e-mail: rms@nih.gov). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TMI.2016.2528162 CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks. Index Terms\u2014Biomedical imaging, computer aided diagnosis, image analysis, machine learning, neural networks. I. INTRODUCTION TREMENDOUS progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e., ImageNet [1], [2]) and the recent revival of deep convolutional neural networks (CNN) [3], [4]. For datadriven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5], [4]. Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes. The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6], [7], e.g., PASCAL [8] and medical image categorization [9]\u2013[12]. However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly. There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the \u201cCNN from scratch\u201d [13]\u2013[17]; 2) using \u201coff-the-shelf CNN\u201d features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for chest X-rays [10] and CT lung nodule identification [9], [12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18]\u2013[21]. A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the \u201ccurse-of-dimensionality\u201d issue in [22], in order to acquire a sufficient number of training image samples. Previous studies have analyzed three-dimensional patch creation for LN detection [23], [24], atlas creation from chest CT [25] and the extraction of multi-level image features [26], [27]. At present, there are several extensions or variations of the decompositional view representation introduced in [22], [28], such as: using a novel vessel-aligned multi-planar image representation for pulmonary embolism detection [29], fusing unregistered \u00a9 IEEE 2016. This article is free to access and download, along with rights for full text and data mining, re-use and analysis. 1286 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 5, MAY 2016 multiview for mammogram analysis [16] and classifying pulmonary peri-fissural nodules via an ensemble of 2D views [12]. Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10], [9], [12]. They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited. In this paper, we exploit three important, but previously under-studied factors of employing deep convolutional neural networks to computer-aided detection problems. Particularly, we explore and evaluate different CNN architectures varying in width (ranging from 5 thousand to 160 million parameters) and depth (various numbers of layers), describe the effects of varying dataset scale and spatial image context on performance, and discuss when and why transfer learning from pre-trained ImageNet CNN models can be valuable. We further verify our hypothesis by inheriting and adapting rich hierarchical image features [5], [33] from the large-scale ImageNet dataset for computer aided diagnosis (CAD). We also explore CNN architectures of the most studied seven-layered \u201cAlexNet-CNN\u201d [4], a shallower \u201cCifar-CNN\u201d [22], and a much deeper version of \u201cGoogLeNet-CNN\u201d [33] (with our modifications on CNN structures). This study is partially motivated by recent studies [34], [35] in computer vision. The thorough quantitative analysis and evaluation on deep CNN [34] or sparsity image coding methods [35] elucidate the emerging techniques of the time and provide useful suggestions for their future stages of development, respectively. Two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification are studied in this work. On mediastinal LN detection, we surpass all currently reported results. We obtain 86% sensitivity on 3 false positives (FP) per patient, versus the prior state-of-art sensitivities of 78% [36] (stacked shallow learning) and 70% [22] (CNN), as prior state-of-the-art. For the first time, ILD classification results under the patient-level five-fold cross-validation protocol (CV5) are investigated and reported. The ILD dataset [37] contains 905 annotated image slices with 120 patients and 6 ILD labels. Such sparsely annotated datasets are generally difficult for CNN learning, due to the paucity of labeled instances...",
            "url": "https://europepmc.org/articles/pmc4890616?pdf=render"
          },
          {
            "id": "E3719434197",
            "text": "With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis. With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis. Optical microscopy of pathology slides captures the histologic details of tissues in high resolution. With the rapid advance of technology, whole slide imaging (WSI) is becoming part of the routine procedure for clinical diagnosis of many diseases. The emergence of digital pathology1Jara-Lazaro A.R. Thamboo T.P. Teh M. Tan P.H. Digital pathology: exploring its applications in diagnostic surgical pathology practice.Pathology. 2010; 42: 512-518Abstract Full Text PDF PubMed Scopus (77) Google Scholar, 2Webster J.D. Dunstan R.W. Whole-slide imaging and automated image analysis: considerations and opportunities in the practice of pathology.Vet Pathol. 2014; 51: 211-223Crossref PubMed Scopus (107) Google Scholar provides new opportunities to develop algorithms and software tools that can assist pathologists in clinical diagnosis and researchers in studying disease mechanisms. The digitalized pathology slides are often called images in the computer vision field, and can benefit from many image analysis algorithms. As an example, the common task where pathologists locate and recognize tissue components can also be achieved by image segmentation and recognition algorithms. Nowadays, digital pathology is making rapid progress owing to the success of deep learning.3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar Before the application of deep learning algorithms, digital pathology, due to its high complexity, achieved limited success with laborious modeling.4Janowczyk A. Madabhushi A. Deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases.J Pathol Inform. 2016; 7: 29Crossref PubMed Scopus (700) Google Scholar Since 2012, deep learning has made significant improvements in all image recognition benchmarks.3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar, 5Krizhevsky A. Sutskever I. Hinton G.E. ImageNet classification with deep convolutional neural networks.Commun ACM. 2017; 60: 84-90Crossref Scopus (10391) Google Scholar, 6Goodfellow I. Bengio Y. Courville A. Bengio Y. Deep Learning. MIT Press, Cambridge, MA2016Google Scholar The applications of deep learning algorithms in digital pathology have had remarkable success in traditional pathology tasks. For example, deep learning algorithms achieved performance comparable to pathologists in interpreting whole slide images for the detection of tumor regions7Liu Y. Gadepalli K. Norouzi M. Dahl G.E. Kohlberger T. Boyko A. Venugopalan S. Timofeev A. Nelson P.Q. Corrado G.S. Detecting cancer metastases on gigapixel pathology images.arXiv. 2017; (arXiv:1703.02442)Google Scholar, 8Wang D. Khosla A. Gargeya R. Irshad H. Beck A.H. Deep learning for identifying metastatic breast cancer.arXiv. 2016; (arXiv:1606.05718)Google Scholar, 9Wang S. Chen A. Yang L. Cai L. Xie Y. Fujimoto J. Gazdar A. Xiao G. Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome.Sci Rep. 2018; 8: 10393Crossref PubMed Scopus (60) Google Scholar and lymph node metastases.10Ehteshami Bejnordi B. Veta M. van Diest P.J. van Ginneken B. Karssemeijer N. Litjens G. et al.Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer.JAMA. 2017; 318: 2199-2210Crossref PubMed Scopus (1444) Google Scholar Although this comparable performance may not generalize to all task domains, advanced methodology is anticipated to solve or aid in common challenges faced by pathologists, including locating neoplasia within a tissue and quantifying specific features such as mitoses and inflammation. To understand how deep learning excels in these areas, we build conceptual connections of deep learning in the machine learning literature. In essence, deep learning is a special kind of artificial neural network (ANN), which is one category of machine learning algorithm. Deep learning and other ANNs are inspired by biological neural networks and mathematically construct a network model with multiple connected layers. The first network layer (called the input layer) receives inputs (eg, slide images). It has a set of parameters and can use them to compute outputs. Similarly, each successive network layer receives inputs from its previous layers, uses its parameters, and computes outputs. At the end, the last network layer (called the output layer) calculates the outputs of the whole model. The layers between the input and output layers are not visible because they do not directly receive model input or generate model outputs, and thus are called the hidden layers. The structure of a segmentation neural network is illustrated in Figure 1A. In this process, prediction outputs from a good neural network can well approximate the observed outputs. Although ANNs claim excellent performances based on theoretical work,11Cybenko G. Approximation by superpositions of a sigmoidal function.Math Control Signal Syst. 1989; 2: 303-314Crossref Scopus (8650) Google Scholar historically, it has been notoriously hard to calculate the network parameters when the total number of network layers exceeded three, which limited the performance of the model. Fortunately, this is no longer a severe bottleneck, owing to the advancements in computational hardware, the scale of data accumulation, and the improvements in algorithms. Nowadays, popular ANNs can have hundreds of layers. The machine learning community refers to these algorithms as deep learning to distinguish them from the conventional shallow ANN algorithm. In this review, the application of deep learning algorithms in pathology image analysis is the focus. Convolutional neural networks (CNNs) are introduced, which have been widely used for image classification and pathology image analysis, such as tumor region and metastasis detection.7Liu Y. Gadepalli K. Norouzi M. Dahl G.E. Kohlberger T. Boyko A. Venugopalan S. Timofeev A. Nelson P.Q. Corrado G.S. Detecting cancer metastases on gigapixel pathology images.arXiv. 2017; (arXiv:1703.02442)Google Scholar, 8Wang D. Khosla A. Gargeya R. Irshad H. Beck A.H. Deep learning for identifying metastatic breast cancer.arXiv. 2016; (arXiv:1606.05718)Google Scholar, 9Wang S. Chen A. Yang L. Cai L. Xie Y. Fujimoto J. Gazdar A. Xiao G. Comprehensive analysis of lung cancer pathology images to discover tumor shape and boundary features that predict survival outcome.Sci Rep. 2018; 8: 10393Crossref PubMed Scopus (60) Google Scholar In essence, a CNN can have a series of convolution layers as the hidden layers and thus make the network deep. This network structure enables the extraction of representational features for prediction. The design of CNN is inspired by the functional mechanism of the visual cortex3LeCun Y. Bengio Y. Hinton G. Deep learning.Nature. 2015; 521: 436-444Crossref PubMed Scopus (42675) Google Scholar: instead of using all outputs from the previous layer, a convolution kernel only focuses on a certain area, the so-called receptive field, to compute a feature at the corresponding spatial position. By spatially sliding the receptive field along the input dimensions (eg, along the width and height directions for two-dimensional images), a feature map is computed as the outputs from the convolution layer. This process is illustrated in Figure 1B. Because the number of parameters is determined by the size of the..",
            "url": "https://openalex.org/W2952846726",
            "title": "Pathology Image Analysis Using Segmentation Deep Learning Algorithms",
            "publication_date": "2019-09-01"
          }
        ]
      }
    },
    "evaluation": {
      "scores": {
        "accuracy": {
          "reasoning": "The given answer accurately identifies several limitations of current task-specific medical image segmentation models, such as data limitations, inconsistency across modalities, noise sensitivity, and lack of standardization. These points align well with the golden answer's discussion of the limitations of traditional models. Additionally, the answer correctly highlights advancements in natural image segmentation, including deep learning techniques and transfer learning, which are relevant to improving medical image segmentation. However, while the answer is largely accurate, it lacks some specific details and nuances present in the golden answer, which discusses the need for universal models and the challenges in developing them. Therefore, I would rate the accuracy as very good but not perfect.",
          "score": 8
        },
        "completeness": {
          "reasoning": "The given answer covers many important aspects of the limitations of current medical image segmentation models and how advancements in natural image segmentation can inform improvements. It discusses data limitations, noise sensitivity, and the potential of deep learning techniques. However, it does not fully address the broader context of the need for universal models or the specific challenges mentioned in the golden answer, such as the variability in imaging modalities and the complexity of segmentation tasks. The golden answer provides a more comprehensive overview of the literature and synthesizes various points into a cohesive argument. Thus, while the given answer is informative, it lacks some depth and breadth compared to the golden answer.",
          "score": 7
        },
        "relevance": {
          "reasoning": "The given answer is highly relevant to the complex question, as it directly addresses the limitations of current task-specific medical image segmentation models and discusses how advancements in natural image segmentation can inform improvements. It effectively connects the limitations to potential solutions, such as deep learning techniques and transfer learning. However, it could have more explicitly linked these advancements to the specific context of enhancing clinical workflows, which is a key aspect of the question. Overall, the answer is relevant but could be more tightly aligned with all aspects of the question.",
          "score": 8
        },
        "evidence_quality": {
          "reasoning": "The given answer cites several sources to support its claims, including references to deep learning techniques and the use of transfer learning from natural image datasets. However, the references provided are not as robust or authoritative as those in the golden answer, which cites a broader range of literature and provides a more comprehensive synthesis of evidence. The quality of evidence in the given answer is good but lacks the depth and variety found in the golden answer, which draws from multiple studies and perspectives. Therefore, while the evidence quality is solid, it does not reach the level of the golden answer.",
          "score": 6
        },
        "clarity": {
          "reasoning": "The given answer is generally clear and well-structured, making it easy to follow the main points regarding the limitations of medical image segmentation models and the advancements in natural image segmentation. However, some sentences could be more concise, and the flow of ideas could be improved to enhance overall readability. The golden answer presents its arguments in a more organized manner, with clear headings and subheadings that guide the reader through the content. While the given answer is clear, it does not achieve the same level of clarity as the golden answer.",
          "score": 7
        },
        "logical_structure": {
          "reasoning": "The given answer has a logical structure, presenting limitations first and then discussing advancements in natural image segmentation. However, it lacks the detailed organization and clear sectioning found in the golden answer, which uses headings and subheadings to delineate different sections of the argument. The logical flow could be improved by explicitly connecting the limitations to the proposed advancements in a more structured way. Overall, while the answer is logically coherent, it does not match the organizational quality of the golden answer.",
          "score": 6
        },
        "evidence_support": {
          "reasoning": "The given answer provides some evidence to support its claims, referencing specific techniques and methodologies relevant to medical image segmentation. However, the depth of evidence support is not as strong as in the golden answer, which integrates a wider range of studies and findings to substantiate its points. The given answer could benefit from more detailed citations and a broader range of evidence to strengthen its claims. Therefore, while there is some evidence support, it is not as comprehensive as it could be.",
          "score": 6
        },
        "depth_of_analysis": {
          "reasoning": "The given answer provides a good overview of the limitations of current medical image segmentation models and discusses advancements in natural image segmentation. However, it lacks the depth of analysis found in the golden answer, which delves into specific challenges and considerations for developing universal models. The golden answer synthesizes a broader range of literature and provides a more nuanced discussion of the implications of these advancements for clinical workflows. Thus, while the given answer is informative, it does not reach the same level of depth as the golden answer.",
          "score": 6
        },
        "objectivity": {
          "reasoning": "The given answer maintains an objective tone, presenting the limitations and advancements in medical image segmentation without bias. It discusses both the challenges and potential solutions in a balanced manner. However, the golden answer also emphasizes the need for universal models and the importance of addressing specific challenges, which adds a layer of critical analysis that is somewhat lacking in the given answer. Overall, the objectivity is good, but the golden answer presents a more critical perspective.",
          "score": 7
        },
        "synthesis": {
          "reasoning": "The given answer synthesizes information from various sources regarding the limitations of medical image segmentation and advancements in natural image segmentation. However, it does not integrate these points as effectively as the golden answer, which weaves together multiple perspectives and findings into a cohesive narrative. The golden answer provides a more comprehensive synthesis of the literature, highlighting the interconnectedness of the issues discussed. Therefore, while the given answer demonstrates some synthesis, it does not achieve the same level of integration as the golden answer.",
          "score": 6
        }
      }
    },
    "average_score": 6.7
  },
  {
    "id": "https://openalex.org/W4393969728",
    "meta": {
      "id": "https://openalex.org/W4393969728",
      "title": "Quantifying the influence of supplier relationship management and supply chain performance",
      "publication_date": "2024-04-05",
      "cited_by_count": 145,
      "topics": "Developing Evidence-Informed Supply Chain Management Knowledge, Information Systems Outsourcing and Offshoring, Building Resilient Supply Chain",
      "keywords": "Supplier relationship management",
      "concepts": "Supply chain management, Supply chain, Business, Supplier relationship management, Process management, Industrial organization, Marketing",
      "best_oa_location_pdf_url": "https://bjopm.org.br/bjopm/article/download/2015/1070",
      "pdf_urls_by_priority": [
        "https://bjopm.org.br/bjopm/article/download/2015/1070"
      ],
      "text_type": "full_text",
      "successful_pdf_url": "https://bjopm.org.br/bjopm/article/download/2015/1070",
      "openalex_rank": 10,
      "num_tokens": 15984,
      "url": "https://bjopm.org.br/bjopm/article/download/2015/1070"
    },
    "text": "Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n1/18\nRESEARCH PAPER\nQuantifying the influence of supplier relationship management and\nsupply chain performance: an investigation of Bangladesh\u2019s\nmanufacturing and service sectors\nMd Mehedi Hasan Emon\u00b9 , Tahsina Khan2, Saleh Ahmed Jalal Siam\u00b9\n\u00b9American International University-Bangladesh (AIUB), Kuratoli, Bangladesh.\n2Bangladesh University of Professionals (BUP), Mirpur Cantonment, Bangladesh.\n1 INTRODUCTION\nThe efficacy of an organization's supply chain management relies on its capacity to preserve\nFinancial support: None.\nConflict of interest: The authors have no conflict of interest to declare.\nCorresponding author: emonmd.mhasan@gmail.com\nReceived: 03 October 2023.\nAccepted: 05 February 2024.\nEditor: Osvaldo Luiz Gonsalves Quelhas.\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use,distribution, and reproduction\nin any medium, provided the original work is properly cited.\nABSTRACT\nHow to cite: Emon, M. M. H., Khan, T. and Siam, S. A. J. (2024), \u201cQuantifying the influence of supplier relationship\nmanagement and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors\u201d,\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2, e20242015.\nhttps://doi.org/10.14488/BJOPM.2015.2024\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n2/18\nrobust and mutually advantageous relationships with its suppliers. Supplier Relationship\nManagement (SRM) is widely recognized as a crucial method for achieving this goal, as\nevidenced by numerous studies conducted in well-established industrial settings, such as the\nUnited States (Farooque et al., 2022; Zhang et al., 2021) and Germany (Reu et al., 2019). These\nstudies, conducted on a worldwide scale, have highlighted the positive influence of proficient SRM\nstrategies on various aspects of supply chain performance, including supplier performance, cost\neffectiveness, product quality, and delivery punctuality. As global industries continue to evolve,\nemerging economies like Bangladesh have become prominent participants in the international\nmarketplace.\nIn the specific context of Bangladesh, a country experiencing rapid economic growth and a\nflourishing industrial sector, the significance of efficient SRM cannot be emphasized enough. In\nrecent years, Bangladesh has demonstrated significant progress in establishing itself as a\nprominent participant in the worldwide supply chain, with a particular focus on industries such\ntextiles, ready-made clothing, and electronics (Fouji & Hoque, 2021). Due to its expanding industrial\nsector and extensive supplier network, the nation has emerged as an appealing destination for\nforeign enterprises in search of economically efficient manufacturing and sourcing alternatives. The\nobserved evolution serves as evidence of Bangladesh's capacity inside the international\nmarketplace. Nevertheless, it is crucial to examine the impact of SRM techniques on the\nperformance of supply chains functioning in Bangladesh, as the country takes on its position in the\nglobal supply chain. Although there exists a considerable body of literature on SRM, a significant\nproportion of the study has mostly concentrated on industrialized economies. Consequently, there\nis a notable knowledge gap about the application and effectiveness of SRM in emerging economies\nsuch as Bangladesh. The distinctive environment of Bangladesh, characterized by its economic,\ncultural, and infrastructural intricacies, warrants a focused examination (Hossain et al., 2023).\nThe primary objective of the present study is to fill the existing research void by quantitatively\nexamining the impact of SRM on enhancing supply chain performance in the context of Bangladesh.\nThis investigation will build upon the knowledge gained from prior research conducted on a\nworldwide scale. For instance, research conducted in the United States by Farooque et al., (2022)\nand in Germany by Reu\u00df et al., (2019) emphasized the favorable influence of proficient SRM\nstrategies on various aspects, including supplier performance, cost effectiveness, product quality,\nand delivery punctuality. The conducted research, carried out in established industrial contexts,\nhas yielded significant reference points for SRM procedures. Nevertheless, the specific economic,\ncultural, and infrastructural complexities of Bangladesh give rise to a separate framework that\ncould potentially impact the dynamics of SRM in a distinct manner. The current study seeks to fill\nthis void by particularly examining the Bangladeshi context, so adding a nuanced viewpoint to the\nexisting pool of information. Through the analysis of SRM in an emerging country, this study aims\nto reveal unique perspectives that may diverge from findings in research undertaken in\nindustrialized countries. A comprehensive comprehension of how SRM methods affect\nperformance metrics is essential due to the unique difficulties and possibilities present in the\nsupply chain ecosystem of Bangladesh. The study offers a fresh viewpoint by examining the\nutilization and efficacy of SRM in a distinctive and ever-changing economic environment, providing\nvaluable insights that might enhance both theoretical understanding and practical implementation\nin the worldwide domain of supply chain management. Although several studies have investigated\nthe concept of SRM in developed economies, there is a notable scarcity of study that focuses on\nexamining this phenomenon within the unique context of Bangladesh. Prior studies conducted in\nthis domain have yielded significant findings pertaining to the correlations between SRM methods\nand key performance indicators (KPIs) within supply chain management. For example, research\ndone in the United States (Farooque et al., 2019) and Germany (Reu\u00df et al., 2019) has emphasized\nthe favorable influence of proficient SRM strategies on supplier performance, cost effectiveness,\nproduct quality, and delivery punctuality. The studies highlight the significance of SRM in improving\nmany facets of supply chain performance. Moreover, scholarly investigations conducted in\ndeveloping nations, such as India, have provided evidence of the significance of SRM strategies in\nenhancing cost effectiveness (Queiroz & Wamba, 2019). Numerous studies undertaken in various\ninternational settings have continuously underscored the significance of collaboration, information\nsharing, and trust-building between buyers and suppliers as pivotal elements of effective SRM\n(Hoang et al., 2023).\nAs Bangladesh strives to establish itself as a significant participant in the global supply chain, it\nis imperative to thoroughly investigate the suitability and efficacy of SRM approaches in this distinct\nsetting, as shown by previous study findings. Therefore, the present study aims to expand upon\nthe existing body of research by directing its attention towards the supply chains of Bangladesh.\nThis study seeks to give significant insights for practitioners and policymakers acting within the\nBangladeshi context by examining the correlation between SRM techniques and key supply chain\nperformance indicators, including Supplier Collaboration, Supplier Development, Supplier\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n3/18\nEvaluation and Selection, Long-Term Supplier Relationships.\nPrevious research has provided valuable insights into the impact of SRM on supply chain\nperformance at a global level. However, this study seeks to expand this knowledge by examining\nthe specific context of Bangladesh. By doing so, it aims to contribute to a more comprehensive\nunderstanding of how SRM influences supply chain dynamics in emerging economies. Despite the\nincreasing significance of SRM, there exists a dearth of empirical studies examining its influence on\nsupply chain performance within the context of Bangladesh. The objective of this study is to\naddress the existing research gap by investigating the precise connections between SRM methods\nand measures of supply chain performance. The value of this study is derived from its contribution\nto the existing body of knowledge on SRM specifically within the setting of Bangladesh. It\ncontributes to the understanding of the advantages that may be gained by implementing successful\nSRM tactics. The primary objective of this study is to examine the manufacturing and service sectors\nin Bangladesh. The research will span enterprises of diverse sizes and operational complexities in\norder to conduct a thorough evaluation of SRM techniques and their influence on the performance\nof supply chains.\n2 EMPIRICAL LITERATURE AND HYPOTHESIS DEVELOPMENT\n2.1 Supplier relationship management and supply chain performance\nSRM is a critical aspect of modern supply chain management (Adesanya et al., 2020). It involves\ndeveloping dynamic partnerships between businesses and their suppliers (Sharma et al., 2020).\nSRM focuses on creating lasting and mutually beneficial relationships beyond mere transactions\n(Enz & Lambert, 2023). In today's competitive landscape, strong supplier relationships are vital for\nsuccess (Amoako-Gyampah et al., 2019). SRM goes beyond transactions, encompassing strategic\ncollaboration, innovation, and value generation (Pereira et al., 2022). This review examines SRM's\nimpact on supply chain performance, including cost efficiency, product quality, on-time delivery,\nand customer satisfaction. SRM has evolved from transactional interactions to strategic alliances\n(Abbas & Tong, 2023). Recognizing suppliers as essential partners shifted the focus to collaboration\nand long-term partnerships. Technological advancements improved SRM's efficiency, with digital\nplatforms enhancing communication (Emon & Nahid, 2023; Tseng, 2020). Strong relationships with\nsuppliers stimulate innovation and value creation (Lee & Tang, 2018). SRM now encompasses\ncollaborative partnerships, risk management, supplier development, and sustainability (Emon &\nKhan, 2023; Huma et al., 2020). Empirical studies show that proficient SRM positively impacts\nsupplier performance, cost-effectiveness, product quality, and delivery punctuality (Le Jr, 2022).\nCollaboration, information exchange, and trust development are key foundations of successful\nSRM (Cha & Kim, 2018). Supplier development activities enhance supplier performance (Mani et al.,\n2018).\n2.2 Supplier Collaboration and Supply Chain Performance:\nSRM highlights the importance of strategic collaboration as a fundamental element for\nachieving success in the current dynamic and competitive supply chain environment (Oduro et al.,\n2020). There is a large body of literature that strongly supports the notion that successful\ncollaboration with suppliers plays a crucial role in improving the performance of the supply chain.\nThis section examines the current studies on how supplier collaboration affects several aspects of\nsupply chain performance and builds upon the literature presented by Oduro et al., (2020).\nCollaborative relationships in SRM go beyond standard transactional methods and play a crucial\nrole in promoting innovation, enhancing product quality, assuring timely delivery, and ultimately\nimproving customer happiness (Abtahi et al., 2023; Stek & Schiele, 2021). The development of SRM\nhas experienced a significant change from transactional interactions to the formation of strategic\npartnerships, emphasizing the crucial role of collaboration in creating and sustaining long-term\nrelationships with suppliers (Yang, 2022). The growing interdependence and global integration of\nsupply chains has underscored the importance of strong collaborative methods. Technological\nimprovements are crucial in enabling and enhancing collaborative efforts in SRM. Digital platforms\nhave become crucial instruments that not only facilitate communication but also improve the\nsharing of information between organizations and their suppliers (Ebinger & Omondi, 2020). These\nplatforms facilitate instantaneous data exchange, which is essential for efficient collaboration,\nespecially in a multinational supply chain setting where prompt information is vital.\nAlthough (Leiras & Fontainha, 2019; Oduro et al., 2020) have shed light on the beneficial effects\nof supplier collaboration on supply chain performance, there may still be gaps and unexplored\nareas in the existing literature. The purpose of this part is to identify the gaps in the current\nunderstanding of how supplier collaboration affects supply chain performance. It also highlights\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n4/18\nthe specific contributions of this study in addressing and enhancing the existing knowledge in this\narea. This study seeks to further the knowledge of the complex relationship between collaborative\npractices and supply chain performance by combining additional literature that addresses subtle\naspects of supplier collaboration.\n2.3 Supplier Development and Supply Chain Performance\nSupplier development activities are acknowledged as key elements in the larger context of SRM,\nserving a vital function in improving supplier performance and, subsequently, overall supply chain\nefficiency (Dubey et al., 2019). This part performs a thorough examination of the current body of\nresearch to investigate the complex connection between supplier development and supply chain\nperformance, expanding upon the knowledge presented by Dubey et al., (2019). Supplier\ndevelopment goes beyond conventional transactional methods and encompasses a range of\nactions aimed at improving the talents and performance of suppliers (Awan et al., 2019). Activities\nmay encompass training programs, streamlining processes, embracing technology, and engaging\nin collaborative projects with the goal of promoting innovation and ongoing enhancement.\nAccording to the literature, supplier development programs that are successful contribute to better\nsupplier performance, which in turn leads to cost-effectiveness and increased efficiency in the\nsupply chain (Asif et al., 2022).\nAlthough the current body of literature offers significant insights into the favorable relationship\nbetween supplier development and supply chain performance, there are still intricacies and unique\nmechanisms that warrant further investigation. Gaining insight into the impact of different\nelements of supplier development on various facets of supply chain performance is essential for\nformulating focused and efficient strategies. This section seeks to highlight any existing gaps in the\ncurrent knowledge and clarify how the present study helps to filling these gaps by offering a\ndetailed perspective on the complex connection between supplier development activities and the\noverall performance of the supply chain.\n2.4 Supplier Evaluation and Supply Chain Performance:\nEffective SRM relies on the use of strong supplier assessment methods, which are essential for\nachieving optimal supply chain efficiency (Amoako-Gyampah et al., 2019; Klingebiel et al., 2013).\nThis section provides a comprehensive analysis of the current research on the complex connection\nbetween supplier evaluation and the overall performance of the supply chain. It expands on the\nfundamental findings presented by Amoako-Gyampah et al., (2019). Supplier evaluation in the\ncontext of SRM entails a thorough examination of supplier performance, cost efficiency, product\nquality, and delivery timeliness (Lim et al., 2021). Proficient SRM has been found to have a favorable\nimpact on supplier evaluation, leading to improvements in several aspects of supply chain\nperformance, as indicated by the existing literature. However, there may be gaps in our present\nunderstanding of how the precise criteria used in supplier assessment processes are closely\nconnected to various aspects of supply chain performance. The purpose of this section is to identify\nand explain the gaps in the current knowledge, by conducting a detailed analysis of the relationship\nbetween different criteria used to evaluate suppliers and various aspects of supply chain\nperformance. Through this approach, it aims to enhance and improve the theoretical foundation\nthat supports the connection between supplier evaluation methods and the overall efficacy of the\nsupply chain.\n2.5 Long-Term Supplier Relationship and Supply Chain Performance:\nLong-term supplier connections play a crucial role in SRM, exerting substantial impact on supply\nchain performance (Oduro et al., 2020). This part provides a thorough examination of the current\nresearch, exploring the complex connection between long-term relationships with suppliers and\nthe various factors that affect supply chain effectiveness. The fundamental knowledge presented\nby Shakeel et al., (2018) acts as a crucial basis for this investigation. Organizations have recognized\nthe importance of suppliers as crucial partners and have therefore made it a strategic priority to\ndevelop long-term relationships. This involves focusing on collaboration and mutual benefits under\nthe SRM framework (Kannan, 2018). Empirical research confirm that long-term supplier\nrelationships have a beneficial effect on different measures of supply chain performance. However,\nthere may be gaps in our current understanding regarding the detailed mechanisms by which these\nlinkages contribute to the complex fabric of supply chain performance. This section aims to\nthoroughly identify any potential gaps in the current literature, with the goal of clarifying how the\npresent study enhances the understanding of the relationship between long-term supplier\nrelationships and various aspects of supply chain performance. In order to improve the scope and\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n5/18\ncomprehensiveness of this analysis, it will be crucial to include contemporary research and a variety\nof viewpoints on long-term supplier partnerships. This section aims to add to both theoretical\nrefinement and practical insights for businesses managing long-term supplier relationships within\nthe broader framework of SRM.\n2.6 Theoretical Frameworks in SRM\nUnderstanding Supplier Relationship Management's (SRM) theoretical foundations and\nevolution is essential to grasp its dynamics. Originally transactional, SRM has evolved into a\nstrategic approach focusing on long-term value (Dash et al., 2018). Theoretical dimensions include\ntransaction cost minimization, resource leverage, incentive alignment, and risk management (Rejeb\net al., 2021). The evolution of Transaction Cost Economics (TCE), Resource-Based View (RBV), and\nAgency Theory has mirrored SRM's transformation. Transaction Cost Economics (TCE) emphasizes\nminimizing transaction costs in SRM (Swallehe, 2021). Resource-Based View (RBV) highlights\nsupplier relationships as sources of competitive advantage (Andersen, 2021). Agency Theory\naddresses agency issues within supplier relationships (Dubey et al., 2019). These theories intersect\nin SRM to reduce costs, leverage resources, align incentives, and manage risks (Gilmore & Buhaug,\n2021).\nDepending upon the theoretical underpinnings elucidated in the comprehensive examination\nof existing literature, the present section delineates the conceptual framework that will guide the\ninvestigation. The theoretical framework selected for this study is the Resource-Based View (RBV),\nwhich asserts that businesses can attain enduring competitive advantage via the strategic\nutilization of distinctive resources and skills. across the scope of this study, the Resource-Based\nView (RBV) framework offers a perspective that enables the examination of how SRM methods\nimpact a particular outcome variable, namely cost efficiency, across supply chains operating in\nBangladesh. The Resource-Based View (RBV) theory places significant emphasis on the significance\nof supplier relationships as important sources of resources and capabilities that ultimately promote\ncost effectiveness within supply chains. The resources and capabilities obtained through SRM\ntechniques are anticipated to have a favorable impact on cost efficiency. This is demonstrated by\nthe potential to attain cost reductions, cost savings, and cost-effectiveness in supply chain activities.\n2.7 Research Hypotheses & Conceptual Framework\nSupplier collaboration: The implementation of supplier collaboration, which involves engaging\nin cooperative activities and sharing knowledge with suppliers, has been demonstrated to improve\nTable 1 - Theoretical Frameworks in SRM\nTheoretical\nFramework\nKey Concepts and Relevance Sources\nTransaction Cost\nEconomics\n- Minimizing transaction costs in\nsupplier relationships.\n(Rindfleisch, 2020; Schmidt &\nWagner, 2019; Stone, 1986)\n- Aligning SRM practices with\ntransaction-specific\ncharacteristics.\n(Macher & Richman, 2008; Saad\net al., 2022; Wynstra et al., 2019)\nResource-Based\nView\n- Leveraging supplier\nrelationships as sources of\ncompetitive advantage.\n(Barney, 1991; Nandi et al., 2020;\nShibin et al., 2020)\n- Recognizing supplier-specific\nassets, knowledge, and\ncapabilities.\n(Burki et al., 2023; Saghiri &\nMirzabeiki, 2021; Wernerfelt,\n1984)\nAgency Theory - Addressing agency problems\nand information asymmetry in\nSRM.\n(Dong et al., 2021; Jensen &\nMeckling, 2019; Kummer et al.,\n2020)\n- Designing SRM practices to align\nincentives between organizations\nand suppliers.\n(Ali et al., 2020; Dubey et al.,\n2019; Eisenhardt, 1989)\n- Monitoring and control\nmechanisms in SRM\nrelationships.\n(Fama, 1980; Songsom et al.,\n2019)\n- The role of contracts and\nperformance-based incentives in\nSRM.\n(Grum et al., 2023; Milgrom et al.,\n1992)\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n6/18\nthe operational efficiency of supply chain processes. Collaboration of this nature has the potential\nto enhance communication, optimize procedures, and enhance coordination, hence yielding cost\nefficiencies. Prior studies have provided evidence that the establishment of successful\ncollaboration with suppliers has a beneficial influence on cost efficiency (Cole & Aitken, 2019;\nPaparoidamis et al., 2019), thus confirming the premise of a positive association.\nH0: Supplier collaboration has no significant influence on cost efficiency in the supply chains of\nBangladeshi organizations.\nH1: Supplier collaboration has a significant positive influence on cost efficiency in the supply\nchains of Bangladeshi organizations.\nSupplier development: Supplier development programs, encompassing activities such as\ntraining and knowledge transfer, possess the capacity to augment the capabilities of suppliers,\nhence resulting in enhancements to processes and reductions in costs. Previous studies have\ndemonstrated that supplier development initiatives have a favorable impact on cost effectiveness\n(Awan et al., 2019; Sikombe & Phiri, 2019), hence substantiating the proposition of a positive\ncorrelation.\nH0: Supplier development efforts have no significant impact on cost efficiency in the supply\nchains of Bangladeshi organizations.\nH2: Supplier development efforts have a significant positive impact on cost efficiency in the\nsupply chains of Bangladeshi organizations.\nSupplier Evaluation and Selection: The implementation of effective supplier evaluation and\nselection methods is crucial in ensuring that organizations establish partnerships with dependable\nand high-performing suppliers. Efficient evaluation procedures are anticipated to result in\nimproved selection of suppliers, hence positively influencing cost effectiveness through the\nmitigation of quality-related concerns and supply disruptions. The favorable association between\nsupplier evaluation and selection and cost efficiency has been supported by empirical research\nconducted by Kannan (2018) and Naghshineh & Carvalho (2022) (Kannan, 2018; Naghshineh &\nCarvalho, 2022).\nH0: Effective supplier evaluation and selection practices have no significant effect on cost\nefficiency in the supply chains of Bangladeshi organizations.\nH3: Effective supplier evaluation and selection practices have a significant positive effect on cost\nefficiency in the supply chains of Bangladeshi organizations.\nLong-Term Supplier Relationships: The establishment and maintenance of enduring\nconnections with essential suppliers contribute to the cultivation of trust, collaboration, and shared\ncomprehension. These attributes are frequently linked with enhanced operational efficiency and\nless interruptions, hence contributing to cost-effectiveness. Prior studies have demonstrated that\nestablishing enduring partnerships with suppliers has a favorable impact on cost effectiveness\n(Herczeg et al., 2018; Wang et al., 2023), hence lending weight to the proposition of a positive\nassociation.\nH0: Long-term supplier relationships have no significant impact on cost efficiency in the supply\nchains of Bangladeshi organizations.\nH4: Long-term supplier relationships have a significant positive impact on cost efficiency in the\nsupply chains of Bangladeshi organizations.\nThe conceptual framework includes SRM practices as independent variables and supply chain\nperformance indicators (cost efficiency) as dependent variables. As illustrated in Figure 1 the four\nindependent variables namely Supplier collaboration, Supplier development, Supplier Evaluation\nand Selection, Long-Term Supplier Relationships have been presented in the left side of the\ndiagram and dependent variable cost efficiency is presented in the Right side of the diagram. The\nrelationships between each independent variable and the dependent variable are indicated by the\nfour-research hypothesis connecting the variables through the arrows.\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n7/18\nFigure 1 - Conceptual Framework\n3. MATERIAL AND METHODS\nThe research design employed for this study is quantitative, aiming to comprehensively\ninvestigate the relationship between SRM practices and cost efficiency in the supply chains of firms\noperating in Bangladesh. The study focuses on one main category of variables: independent\nvariables representing SRM practices and one dependent variable representing cost efficiency. In\nterms of independent variables, the study encompasses four dimensions of SRM practices. Supplier\nCollaboration, the variable in question quantifies the extent of collaboration and information\nexchange between enterprises and their suppliers. This statement elucidates the degree to which\nfirms partake in collaborative endeavors and exchange information with suppliers in order to\nenhance cost effectiveness within their supply chains (Tai et al., 2022). Supplier Development, this\nvariable evaluates the endeavors undertaken by enterprises to enhance the capabilities and\nresources of their suppliers. The initiatives encompass many strategies, such as training programs,\ntechnology transfer endeavors, and collaborative projects focused on enhancing process efficiency\nwith the goal of attaining cost effectiveness (Gu et al., 2021). Supplier Evaluation and Selection, this\nvariable investigates the criteria and procedures employed by firms to assess, choose, and oversee\ntheir suppliers. The concept incorporates various elements such as assessments of supplier\nperformance, evaluations of quality, and procedures related to risk management, all of which have\nan impact on cost efficiency (Dobos & V\u00f6r\u00f6smarty, 2019). Long-Term Supplier Relationships, this\ncharacteristic pertains to the temporal extent and steadfastness of the associations established\nbetween organizations and their suppliers. The metric assesses the degree to which firms sustain\nenduring collaborations with crucial suppliers, which can have a lasting impact on cost effectiveness\n(Mohan et al., 2021). The dependent variable, Cost Efficiency, the dependent variable in question\nmeasures the degree of cost-effectiveness exhibited by supply chain operations. The\nmeasurements encompassed in this category consist of cost reduction, cost savings, and supply\nchain cost-to-revenue ratios (Panfilova et al., 2020). The study's setting was Bangladesh, with a\nspecific focus on organizations in various sectors, including manufacturing and services. The study\ninvolved a Convenience sampling technique to ensure representation from both manufacturing\nand service sectors. Data were collected from multiple departments or individuals responsible for\nSRM and supply chain operations within each firm, such as procurement, supply chain\nmanagement, and strategic management. The participants, selected based on their roles and\nresponsibilities related to SRM practices, included managers, procurement officers, and individuals\ninvolved in supply chain decision-making processes. A sample size of 270 individuals was\ndetermined to strike a balance between practicality and the requirement for statistical significance,\nconsidering the constraints of convenience sampling. Data were collected through a structured\nsurvey instrument consisting of 22 items, representing independent and dependent variables. The\nLikert scale was used to measure responses, ranging from strongly disagree to strongly agree. The\nsurvey instrument was developed based on a thorough review of existing literature and validated\nscales used in previous studies. Quantitative techniques, including regression analysis and\ncorrelation analysis, were applied for data analysis using SPSS 22 to assess the relationships\nbetween SRM practices and cost efficiency. These statistical methods aimed to provide insights into\nthe extent to which SRM practices influenced cost efficiency in the context of Bangladeshi firms.\nThroughout the research process, ethical considerations, including informed consent, data\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n8/18\nconfidentiality, and anonymity, were meticulously maintained to ensure the integrity and ethical\nconduct of the study. The reliability of the measurements was assessed through techniques such\nas Cronbach's alpha for survey instruments. The study aimed to minimize measurement errors and\nensure the reliability of collected data.\nTable 2 - Reliability of the Measurements\nConstruct Item No Cronbach\u2019s alpha\nSupplier Collaboration 5 .911\nSupplier Development 5 .922\nSupplier Evaluation and Selection 5 .876\nLong Term Supplier\nRelationships\n5 .917\nCost Efficiency 2 .854\nThe reliability of the measurements was assessed through Cronbach\u2019s alpha values for each\nconstruct in Table 2. The results indicate high internal consistency and reliability among the items\nmeasuring Supplier Collaboration, Supplier Development, Long-Term Supplier Relationships, and\nCost Efficiency, with Cronbach\u2019s alpha values of .911, .922, .917, and .854, respectively. The\nconstruct of Supplier Evaluation and Selection also exhibits good internal consistency, with a\nCronbach\u2019s alpha value of .876. These findings affirm the dependability of the measurement\ninstruments, suggesting that the items within each construct consistently capture the intended\nconcepts. Overall, the study's measurement instruments demonstrate satisfactory to excellent\nreliability, instilling confidence in the accuracy and consistency of the data collected for the\ninvestigation.\n4. RESULTS\n4.1 Correlation\nTable 3 - Correlations Analysis\nSupplier\nCollaborati\non\nSupplier\nDevelop\nment\nSupplier\nEvaluatio\nn and\nSelection\nLong Term\nSupplier\nRelationshi\nps\nCost\nEfficiency\nSupplier\nCollabora\ntion\nPearson\nCorrelati\non\n1 .889** .856** .796** .728**\nSig. (2-\ntailed)\n.000 .000 .000 .000\nN 270 270 270 270 270\nSupplier\nDevelop\nment\nPearson\nCorrelati\non\n.889** 1 .885** .801** .702**\nSig. (2-\ntailed)\n.000 .000 .000 .000\nN 270 270 270 270 270\nSupplier\nEvaluatio\nn and\nSelection\nPearson\nCorrelati\non\n.856** .885** 1 .792** .685**\nSig. (2-\ntailed)\n.000 .000 .000 .000\nN 270 270 270 270 270\nLong\nTerm\nSupplier\nRelations\nhips\nPearson\nCorrelati\non\n.796** .801** .792** 1 .782**\nSig. (2-\ntailed)\n.000 .000 .000 .000\nN 270 270 270 270 270\nCost Pearson .728** .702** .685** .782** 1\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n9/18\nEfficiency Correlati\non\nSig. (2-\ntailed)\n.000 .000 .000 .000\nN 270 270 270 270 270\n**. Correlation is significant at the 0.01 level (2-tailed).\nThe results of the correlation study, as shown in Table 6, indicate statistically significant\nassociations among the variables being examined, namely \"Supplier Collaboration,\" \"Supplier\nDevelopment,\" \"Supplier Evaluation and Selection,\" \"Long Term Supplier Relationships,\" and \"Cost\nEfficiency.\" The quantification of these interactions is accomplished by use Pearson correlation\ncoefficients, all of which exhibit statistical significance at the 0.01 level (two-tailed), so suggesting\nrobust associations.\nThe dimension of \"Supplier Collaboration\" demonstrates a notably strong positive correlation\nof 88.9% with the dimension of \"Supplier Development,\" indicating a solid and significant\nassociation between these two aspects of managing supplier relationships. Furthermore, the\nconcept of \"Supplier Collaboration\" exhibits a significant positive correlation of 85.6% with\n\"Supplier Evaluation and Selection,\" underscoring the robustness of the association between these\ntwo factors. The concept of \"Supplier Development\" exhibits a notably robust positive correlation\nof 88.5% with \"Supplier Evaluation and Selection,\" hence emphasizing their significant alignment.\nMoreover, there exists a strong positive correlation of 80.1% between the construct of \"Supplier\nDevelopment\" and the construct of \"Long Term Supplier Relationships,\" highlighting the significant\ninterdependence of both variables. The variable \"Supplier Evaluation and Selection\" demonstrates\na significant positive correlation of 79.2% with the variable \"Long Term Supplier Relationships,\"\nindicating a robust link between the two. There exists a positive association between \"Cost\nEfficiency\" and all elements of supplier relationship management, as evidenced by the Pearson\ncorrelation coefficients. It is worth noting that the variable \"Cost Efficiency\" exhibits a substantial\npositive correlation of 72.8% with the variable \"Supplier Collaboration,\" a positive correlation of\n70.2% with \"Supplier Development,\" a positive correlation of 68.5% with \"Supplier Evaluation and\nSelection,\" and a notably high positive correlation of 78.2% with \"Long Term Supplier\nRelationships.\" In summary, the aforementioned data suggest that when supplier management\ntechniques adopt a more collaborative approach, prioritize development, incorporate evaluation\nmeasures, and aim to cultivate long-term partnerships, there is a notable enhancement in \"Cost\nEfficiency.\" The percentages presented in the data indicate the magnitude and orientation of these\nconnections, underscoring the noteworthy influence of proficient supplier relationship\nmanagement on cost effectiveness, a critical facet of organizational efficacy.\n5.2 Regression Analysis\nTable 4 - Model Summary\nModel R\nR\nSquare\nAdjusted\nR\nSquare\nStd.\nError of\nthe\nEstimate\nChange Statistics\nR\nSquare\nChange\nF\nChange\ndf1 df2\nSig. F\nChange\n1 .801a.642 .637 .69525 .642 118.993 4 265 .000\na. Predictors: (Constant), Long Term Supplier Relationships, Supplier Evaluation and Selection, Supplier\nCollaboration, Supplier Development\nThe Model Summary table presents a detailed overview of the regression analysis performed in\nthis study, providing insights into the performance and importance of the model. The table\npresents a strong positive linear association between the dependent variable (not specified in the\ntable) and a group of independent variables, including SRM practices such as Supplier\nCollaboration, Supplier Development, Supplier Evaluation and Selection, and Long-Term Supplier\nRelationships, as indicated by an R-value of .801. The considerable R-value highlights the model's\ncapacity to elucidate fluctuations in the dependent variable. The coefficient of determination,\nrepresented by the symbol R\u00b2, is seen to be 0.642. This implies that around 64.2% of the variability\nobserved in the dependent variable may be ascribed to the impact of the SRM practices used in the\nmodel. The observed R\u00b2 value indicates that the set of SRM practices under consideration make a\nsubstantial contribution towards explaining the variability observed in the dependent variable.\nAdditionally, the adjusted R\u00b2, which accounts for the number of predictors in the model,\ndemonstrates a robust value of .637. The revised number suggests that, even after accounting for\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n10/18\nthe potential danger of overfitting, the model is capable of efficiently elucidating around 63.7% of\nthe variability observed in the dependent variable. The standard error of the estimate, which\nrepresents the mean variation between projected and actual values, is calculated to be .69525. This\nmetric offers a measure of the model's accuracy in forecasting the values of the dependent variable.\nThe change statistics section provides evidence of a significant enhancement in the model's fit\nwhen using the SRM practices as predictors. The F-statistic, which has a value of 118.993, along with\na p-value of .000 that is highly significant, provides confirmation of the overall relevance of the\nmodel incorporating these predictors. The aforementioned observation highlights the significance\nand pertinence of the Strategic Risk Management (SRM) techniques when elucidating the\nfluctuations observed in the dependent variable. In summary, the Model Summary table indicates\nthat the regression model, which includes SRM practices as predictors, is statistically significant and\nhas a strong ability to account for a significant portion of the variability in the dependent variable.\nThis underscores the importance of these practices in the context of the study.\nTable 5 - ANOVAa\nModel Sum of\nSquares\ndf Mean\nSquare\nF Sig.\n1 Regression 230.073 4 57.518 118.993 .000b\nResidual 128.094 265 .483\nTotal 358.167 269\na. Dependent Variable: Cost Efficiency\nb. Predictors: (Constant), Long Term Supplier Relationships, Supplier Evaluation and Selection,\nSupplier Collaboration, Supplier Development\nTable 4 presents the ANOVA table, which provides valuable insights into the relevance of the\npredictor variables in the regression model used to estimate \"Cost Efficiency.\" The table is\npartitioned into three primary segments, namely Regression, Residual, and Total. In the Regression\nsection, the calculation of the sum of squares is performed to evaluate the amount of variance\naccounted for by the model. The resulting value is determined to be 230.073, with a corresponding\n4 degrees of freedom (df). The mean square value that corresponds to the given data is 57.518. In\ncontrast, the Residual part pertains to the unexplained variance and presents a sum of squares\nequal to 128.094, which is accompanied by 265 degrees of freedom. The Total Sum of Squares (TSS),\nwhich measures the total variance in \"Cost Efficiency,\" is calculated to be 358.167.\nThe F-statistic, a crucial metric, is computed as 118.993 by dividing the mean square for the\nregression by the mean square for the residual. This statistical measure evaluates the overall\nsignificance of the regression model. The p-value, shown as \"Sig.,\" is significantly small (p = .000).\nThe obtained p-value, which is remarkably low, provides strong evidence that the regression model,\nencompassing Long-Term Supplier Relationships, Supplier Evaluation and Selection, Supplier\nCollaboration, and Supplier Development as predictor variables, is highly significant in elucidating\nthe fluctuations observed in \"Cost Efficiency.\" In more accessible language, these predictor\nvariables together considerably improve the model's capacity to explain the differences found in\n\"Cost Efficiency.\" Therefore, the analysis of variance (ANOVA) findings offer strong statistical\nsupport for the model's ability to effectively elucidate and forecast cost efficiency within the\nframework of supplier relationship management procedures.\nTable 6 - Coefficients\nModel Unstandardized\nCoefficients\nStandardized\nCoefficients\nt Sig.\nB Std. Error Beta\n1 (Constant) -.303 .270 -1.121 .263\nSupplier\nCollaboration\n.342 .109 .275 3.146 .002\nSupplier\nDevelopment\n.027 .114 .022 .233 .816\nSupplier\nEvaluation and\nSelection\n-.007 .118 -.005 -.058 .954\nLong Term\nSupplier\nRelationships\n.681 .082 .550 8.332 .000\na. Dependent Variable: Cost Efficiency\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n11/18\nThe examination of the coefficients in Table 5 allows for the analysis of the regression model,\nproviding valuable insights into the distinct impacts of each predictor variable on the dependent\nvariable, \"Cost Efficiency.\" The constant term in the equation represents the baseline level of \"Cost\nEfficiency\" in the absence of any predictor factors. The observed phenomenon exhibits an\nunstandardized coefficient (B) of -0.303, accompanied by a standard error of 0.270. The t-statistic\nassociated with the given data is -1.121, resulting in a p-value of 0.263. This p-value is above the\ncommonly accepted significance level of 0.05. Therefore, it may be concluded that the constant\nterm does not possess statistical significance in its ability to explain the fluctuations observed in\n\"Cost Efficiency.\" Shifting our focus towards the independent factors, the variable \"Supplier\nCollaboration\" holds considerable importance. The unstandardized coefficient (B) is reported as\n0.342, with a standard error of 0.109, and a standardized coefficient (Beta) of 0.275. The predictor\nexhibits a t-statistic of 3.146, accompanied by a p-value of 0.002, indicating a significant and positive\ninfluence on the variable \"Cost Efficiency.\" More precisely, there is a positive correlation between a\none-unit increase in supplier collaboration and a 0.342-unit improvement in \"Cost Efficiency.\" On\nthe other hand, the variable \"Supplier Development\" demonstrates a coefficient (B) of 0.027, which\nlacks standardization, accompanied with a standard error of 0.114. Nevertheless, the standardized\ncoefficient (Beta) for this variable is only 0.022, indicating a minimal impact. The t-statistic for the\npredictor variable is calculated to be 0.233, with a corresponding p-value of 0.816. This p-value\nexceeds the commonly accepted significance level of 0.05. Therefore, the variable \"Supplier\nDevelopment\" does not exhibit a statistically significant impact on the variable \"Cost Efficiency.\"\nSimilarly, the variable \"Supplier Evaluation and Selection\" is found to have an unstandardized\ncoefficient (B) of -0.007, with a corresponding standard error of 0.118. The standardized coefficient\n(Beta) has a value of -0.005. The t-statistic for the predictor in question is -0.058, and its\ncorresponding p-value is 0.954, both of which are significantly beyond the conventional threshold\nfor statistical significance. Therefore, the variable \"Supplier Evaluation and Selection\" does not\ndemonstrate a statistically significant influence on the variable \"Cost Efficiency.\" Finally, the variable\n\"Long Term Supplier Relationships\" appears as a significantly influential predictor. The\nunstandardized coefficient (B) is reported as 0.681, accompanied by a standard error of 0.082.\nAdditionally, the standardized coefficient (Beta) is reported as 0.550, indicating a significant effect.\nThe t-statistic for the predictor variable exhibits robustness, measuring 8.332. The related p-value\nis 0.000, indicating a significant and positive impact on the variable \"Cost Efficiency.\" Specifically, an\nincrease of one unit in long-term supplier connections is associated with a significant rise of 0.681\nunits in \"Cost Efficiency.\" In summary, the variables of \"Supplier Collaboration\" and \"Long Term\nSupplier Relationships\" demonstrate statistically significant relationships with positive impacts on\n\"Cost Efficiency.\" However, the variables of \"Supplier Development\" and \"Supplier Evaluation and\nSelection\" do not exhibit such statistical significance in explaining variations in the dependent\nvariable.\n5. DISCUSSION AND IMPLICATIONS OF FINDINGS\nThe study's findings provide useful insights into the connections between SRM practices and\nsupply chain performance, with a specific emphasis on cost efficiency. This analysis is conducted\nwithin the specific context of Bangladesh's industrial and service industries. The correlation study\ndemonstrates statistically significant relationships among the variables under investigation,\nemphasizing the strong linkages between Supplier Collaboration, Supplier Development, Supplier\nEvaluation and Selection, Long-Term Supplier Relationships, and Cost Efficiency. The presence of\npositive correlations suggests that the successful execution of SRM procedures has a key role in\nattaining cost-effectiveness in supply chain operations. The robust positive association between\nSupplier Collaboration and Supplier Development highlights the interdependence of both SRM\ncharacteristics. Organizations that participate in cooperative efforts and allocate resources to\nincrease the competencies of their suppliers are likely to achieve a synergistic outcome, resulting\nin enhanced cost effectiveness (Feizabadi & Alibakhshi, 2022; Shafiq et al., 2022; Uddin et al., 2020).\nFurthermore, the strong correlation between Supplier Collaboration and Supplier Evaluation and\nSelection underscores the significance of thorough evaluation and collaboration. This implies that\norganizations that prioritize both aspects simultaneously are more likely to achieve efficient and\neconomical supply chain operations. The strong positive association between Supplier\nDevelopment and Supplier Evaluation and Selection highlights the consistency between both\nprocedures. Organizations that allocate resources to supplier development projects are also prone\nto having clearly defined assessment and choice procedures, which in turn lead to improved cost\neffectiveness. Furthermore, the robust positive link between Supplier Development and Long-Term\nSupplier Relationships underscores the strategic interconnectedness of both aspects.\nOrganizations that prioritize the development of their suppliers are more likely to establish long\u0002lasting partnerships, which in turn leads to sustainable cost efficiency in the long run (Lahti et al.,\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n12/18\n2018; Larsson & Larsson, 2020; Prosser et al., 2021; Wren, 2022). The correlation between Long\u0002Term Supplier Relationships and Supplier Evaluation and Selection indicates that firms that\nmaintain long-term partnerships with important suppliers also place a high emphasis on thorough\nevaluation and selection procedures. This emphasizes the need of stability and trust in supplier\nrelationships for attaining cost-efficient supply chain operations. The regression analysis enhances\nthese insights by offering a comprehensive model summary. The model, which incorporates\nSupplier Collaboration, Supplier Development, Supplier Evaluation and Selection, and Long-Term\nSupplier Relationships as predictors, shows statistical significance in explaining the observed\nheterogeneity in Cost Efficiency. The high R-value and coefficient of determination (R\u00b2) suggest that\napproximately 64.2% of the variation in cost efficiency may be attributable to the influence of the\nSRM practices included in the model. The corrected R\u00b2, which takes into consideration the number\nof predictors, remains strong at 63.7%, confirming the model's effectiveness in explaining variability\nwithout overfitting. The substantial F-statistic in the ANOVA table provides additional confirmation\nof the model's overall significance, highlighting the combined influence of the predictor variables\non Cost Efficiency. The coefficients analysis offers comprehensive insights into the specific effects\nof each predictor variable. Supplier Collaboration and Long-Term Supplier Relationships are\nidentified as statistically significant factors that have a beneficial impact on Cost Efficiency.\nNevertheless, the effects of Supplier Development and Supplier Evaluation and Selection on Cost\nEfficiency are not statistically significant.\nThe study's findings through the empirical results hold substantial implications for\norganizations in Bangladesh's manufacturing and service sectors, offering precise guidance on\noptimizing supply chain performance, particularly in terms of cost efficiency. The strong positive\nconnections observed between Supplier Collaboration, Supplier Development, Supplier Evaluation\nand Selection, Long-Term Supplier Relationships, and Cost Efficiency highlight the\ninterconnectedness of various aspects under SRM. Businesses must recognize the need of\nimplementing a thorough and unified approach to SRM processes to enhance cost efficiency in\ntheir supply chain operations. The importance of Supplier Collaboration and Long-Term Supplier\nRelationships in terms of strategic significance becomes evident as a crucial lesson. Organizations\nthat prioritize collaborating with suppliers and fostering long-lasting partnerships are well\u0002positioned to achieve ongoing cost reduction (Allioui & Mourdi, 2023). This highlights a change in\nviewpoint, encouraging businesses to see suppliers not only as transactional entities but as\nstrategic partners, cultivating enduring relationships that greatly contribute to the overall success\nof the supply chain. The correlation between Supplier Collaboration and Supplier Development\nimplies a mutually beneficial result when firms collaborate and invest in improving supplier skills.\nThis is consistent with the current body of research on collaborative supply chain methods and\ntheir influence on operational efficiency and cost reduction (Benton Jr et al., 2020; Gu et al., 2021;\nLo et al., 2018). Moreover, it is important to strengthen the argument of the significant association\nbetween Supplier Development and Long-Term Supplier Relationships by referencing research that\nemphasize the strategic interdependence of these factors. Empirical evidence demonstrating the\nenduring advantages of supplier development activities on fostering durable partnerships and\nenhancing cost efficiency would strengthen the claim (Cocskun et al., 2022; Faruquee et al., 2021;\nJia et al., 2023; Manuela et al., 2021; Rezaei Vandchali et al., 2020). Supplier Development remains\ncrucial in SRM, but its effect on cost efficiency depends on the presence of complementary\nstrategies, as indicated by the study. The integration of supplier development programs with\nrigorous supplier evaluation and selection processes is crucial. Organizations should prioritize not\nonly improving supplier competencies, but also conducting thorough evaluations and choosing\nsuppliers based on their performance, quality, and risk management. The strong correlation\nbetween Long-Term Supplier Relationships and Supplier Evaluation and Selection emphasizes the\nsignificance of stability and confidence in supplier connections. Organizations that establish long\u0002term cooperation with important suppliers are more inclined to prioritize comprehensive review\nand selection procedures, resulting in streamlined supply chain operations (Tay & Aw, 2021).\nTherefore, it may be inferred that making efforts in establishing and sustaining long-lasting\nrelationships with suppliers can result in a series of beneficial outcomes for many aspects of supply\nchain performance, such as improved cost effectiveness. Having highlighted in the above\ndiscussions, it is worth mentioning here that these discoveries provide practical and implementable\nknowledge for sustaining supply chain operations in Bangladesh. Emphasizing cooperative\nmethods, fostering enduring partnerships with crucial suppliers, and executing rigorous\nassessments of suppliers can greatly contribute to achieving cost effectiveness. The study suggests\nprospective areas for further research, promoting a thorough investigation of certain methods\nwithin Supplier Development and Supplier Evaluation and Selection. Furthermore, conducting\ninquiries into industry-specific variables that impact these connections within the distinct setting of\nBangladesh could yield useful insights. In summary, the study provides accurate and useful advice\nfor firms dealing with the complex field of supply chain management in Bangladesh.\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n13/18\n6 CONCLUSION\nThe study intended to investigate the function of SRM in the changing economic environment\nof Bangladesh, specifically focusing on its influence on supply chain performance. During the\ninvestigation, it became evident that SRM methods have a beneficial impact on different aspects of\nthe supply chain, notably in relation to cost effectiveness. The results highlight the crucial\nsignificance of Supplier Collaboration and the establishment of enduring Supplier Relationships.\nThe study found strong positive associations, highlighting the interconnectedness of several\ncomponents of SRM. Organizations that actively collaborate and develop long-lasting partnerships\nwith suppliers are more likely to achieve sustained cost effectiveness in their supply chain\noperations. In contrast, the individual effects of Supplier Development and Supplier Evaluation and\nSelection on cost efficiency were not statistically significant. This suggests that the efficacy of these\nactivities may depend on the presence of complementary tactics or an integrated strategy. The\nstudy promotes a comprehensive comprehension of SRM, going beyond individual practices, in\norder to fully use its potential in the context of Bangladesh. The research findings offer useful\ninsights for professionals and policymakers in the manufacturing and service sectors of\nBangladesh. They emphasize the significance of implementing a thorough SRM strategy, regarding\nsuppliers as strategic allies, and cultivating lasting partnerships for sustained success in the supply\nchain. The study's practical implications provide valuable counsel for firms aiming to improve their\nsupply chain performance, specifically in terms of cost effectiveness. Notwithstanding the\ncontributions offered, it is imperative to recognize the limitations of the study. The findings may\nhave limited generalizability due to the emphasis on certain sectors and the presence of\noperational complexity. Potential areas for future research involve investigating industry-specific\nvariables that impact the success of SRM in Bangladesh, as well as conducting in-depth analysis of\npractices within Supplier Development and Supplier Evaluation and Selection. The study establishes\na base for continued investigation of SRM in developing countries, with the goal of gaining a more\ndetailed comprehension of its intricacies and potential advantages.\nREFERENCES\nAbbas, H. & Tong, S. (2023), \"Green Supply Chain Management Practices of Firms with Competitive\nStrategic Alliances\u2014A Study of the Automobile Industry\", Sustainability, Vol. 15, No. 3, pp. 2156.\nAbtahi, A.T., Farhana, N. & Hasan, M.M. (2023), \"A Study on the Impact of E-Commerce Adoption for\nEnhancing Supply Chain Efficiency in Bangladesh SMEs\", Business and Economics in Developing\nCountries, Vol. 1, No. 1, pp. 29-33.\nAdesanya, A., Yang, B., Bin Iqdar, F.W. & Yang, Y. (2020), \"Improving sustainability performance\nthrough supplier relationship management in the tobacco industry\", Supply Chain\nManagement: An International Journal, Vol. 25, No. 4, pp. 413-426.\nAli, S.S., Kaur, R., Ersoz, F., Altaf, B., Basu, A. & Weber, G.-W. (2020), \"Measuring carbon performance\nfor sustainable green supply chain practices: A developing country scenario\", Central European\nJournal of Operations Research, Vol. 28, pp. 1389-1416.\nAllioui, H. & Mourdi, Y. (2023), \"Exploring the full potentials of IoT for better financial growth and\nstability: A comprehensive survey\", Sensors, Vol. 23, No. 19, pp. 8015.\nAmoako-Gyampah, K., Boakye, K.G., Adaku, E. & Famiyeh, S. (2019), \"Supplier relationship\nmanagement and firm performance in developing economies: A moderated mediation analysis\nof flexibility capability and ownership structure\", International Journal of Production\nEconomics, Vol. 208, pp. 160-170.\nAndersen, J. (2021), \"A relational natural-resource-based view on product innovation: The influence\nof green product innovation and green suppliers on differentiation advantage in small\nmanufacturing firms\", Technovation, Vol. 104, pp. 102254.\nAsif, M., Searcy, C. & Castka, P. (2022), \"Exploring the role of industry 4.0 in enhancing supplier audit\nauthenticity, efficacy, and cost effectiveness\", Journal of Cleaner Production, Vol. 331, pp.\n129939.\nAwan, U., Sroufe, R. & Kraslawski, A. (2019), \"Creativity enables sustainable development: Supplier\nengagement as a boundary condition for the positive effect on green innovation\", Journal of\nCleaner Production, Vol. 226, pp. 172-185.\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n14/18\nBarney, J. (1991), \"Firm resources and sustained competitive advantage\", Journal of Management,\nVol. 17, No. 1, pp. 99-120.\nBenton Jr, W.C., Prahinski, C. & Fan, Y. (2020), \"The influence of supplier development programs on\nsupplier performance\", International Journal of Production Economics, Vol. 230, pp. 107793.\nBurki, U., Glavee-Geo, R., Dahlstrom, R., Kanani, R. & Buvik, A. (2023), \"The moderating effect of\nmarket knowledge on contractual efficacy: evidence from Asian supplier--Western buyer\nrelationships\", Asian Business & Management, pp. 1-31.\nCha, K.-J. & Kim, Y.S. (2018), \"Critical success factors for mutual collaboration with suppliers in IT\noutsourcing industry: a case study of a top IT outsourcing company in Korea\", Enterprise\nInformation Systems, Vol. 12, No. 1, pp. 76-95.\nCocskun, S.S., Kumru, M. & Kan, N.M. (2022), \"An integrated framework for sustainable supplier\ndevelopment through supplier evaluation based on sustainability indicators\", Journal of\nCleaner Production, Vol. 335, pp. 130287.\nCole, R. & Aitken, J. (2019), \"Selecting suppliers for socially sustainable supply chain management:\npost-exchange supplier development activities as pre-selection requirements\", Production\nPlanning & Control, Vol. 30, No. 14, pp. 1184-1202.\nDash, A., Pothal, L.K. & Tripathy, S. (2018), \"Factors affecting supplier relationship management: An\nAHP approach\", IOP Conference Series: Materials Science and Engineering, Vol. 390, No. 1, pp.\n12056.\nDobos, I. & V\u00f6r\u00f6smarty, G. (2019), \"Inventory-related costs in green supplier selection problems\nwith Data Envelopment Analysis (DEA)\", International Journal of Production Economics, Vol.\n209, pp. 374-380.\nDong, J.Q., Karhade, P.P., Rai, A. & Xu, S.X. (2021), \"How firms make information technology\ninvestment decisions: Toward a behavioral agency theory\", Journal of Management Information\nSystems, Vol. 38, No. 1, pp. 29-58.\nDubey, R., Gunasekaran, A., Childe, S.J., Papadopoulos, T. & Helo, P. (2019), \"Supplier relationship\nmanagement for circular economy: Influence of external pressures and top management\ncommitment\", Management Decision, Vol. 57, No. 4, pp. 767-790.\nEbinger, F. & Omondi, B. (2020), \"Leveraging digital approaches for transparency in sustainable\nsupply chains: A conceptual paper\", Sustainability, Vol. 12, No. 15, pp. 6129.\nEisenhardt, K.M. (1989), \"Agency theory: An assessment and review\", Academy of Management\nReview, Vol. 14, No. 1, pp. 57-74.\nEmon, M.M.H. & Khan, T. (2023), \"The Impact of Cultural Norms on Sustainable Entrepreneurship\nPractices in SMEs of Bangladesh\", Indonesian Journal of Innovation and Applied Sciences (IJIAS),\nVol. 3, No. 3, pp. 201-209.\nEmon, M.M.H. & Nahid, M.H. (2023), \"Factors Affecting Sustainable E-Commerce Adoption:\nEmpirical Evidence from Bangladeshi SME\u2019s\", Corporate Sustainable Management Journal\n(CSMJ), Vol. 01, No. 01, pp. 32-36. Dispon\u00edvel em: https://csmj.com.my/csmj-01-2023-32-36/\nEnz, M.G. & Lambert, D.M. (2023), \"A supply chain management framework for services\", Journal of\nBusiness Logistics, Vol. 44, No. 1, pp. 11-36.\nFama, E.F. (1980), \"Agency problems and the theory of the firm\", Journal of Political Economy, Vol.\n88, No. 2, pp. 288-307.\nFarooque, M., Zhang, A., Liu, Y. & Hartley, J.L. (2022), \"Circular supply chain management:\nPerformance outcomes and the role of eco-industrial parks in China\", Transportation Research\nPart E: Logistics and Transportation Review, Vol. 157, 102596.\nFarooque, M., Zhang, A., Th\u00fcrer, M., Qu, T. & Huisingh, D. (2019), \"Circular supply chain\nmanagement: A definition and structured literature review\", Journal of Cleaner Production, Vol.\n228, pp\nFaruquee, M., Paulraj, A. & Irawan, C.A. (2021), \"Strategic supplier relationships and supply chain\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n15/18\nresilience: is digital transformation that precludes trust beneficial?\", International Journal of\nOperations & Production Management, Vol. 41, No. 7, pp. 1192\u20131219.\nFeizabadi, J. & Alibakhshi, S. (2022), \"Synergistic effect of cooperation and coordination to enhance\nthe firm\u2019s supply chain adaptability and performance\", Benchmarking: An International Journal,\nVol. 29, No. 1, pp. 136\u2013171.\nFouji, M.H. & Hoque, I. (2021), \"Supplier internationalization through upgrading in global supply\nchain: Evidence from the garments industry of Bangladesh\", International Journal of Global\nBusiness and Competitiveness, Vol. 16, No. 2, pp. 116\u2013129.\nGilmore, E.A. & Buhaug, H. (2021), \"Climate mitigation policies and the potential pathways to\nconflict: Outlining a research agenda\", Wiley Interdisciplinary Reviews: Climate Change, Vol. 12,\nNo. 5, e722.\nGrum, B., Tsegaye, D., Tariku, Z., Gebremariam, D., Gebremicael, H., Kuhilen, T., Yemane, S., Aregawi,\nA., Abebe, B.A., et al. (2023), \"Applicability and Cost Implication of Labor-Based Methods for\nSustainable Road Maintenance (SRM) in Developing Countries\", Advances in Civil Engineering,\n2023.\nGu, V.C., Zhou, B., Cao, Q. & Adams, J. (2021), \"Exploring the relationship between supplier\ndevelopment, big data analytics capability, and firm performance\", Annals of Operations\nResearch, Vol. 302, pp. 151\u2013172.\nHerczeg, G., Akkerman, R. & Hauschild, M.Z. (2018), \"Supply chain collaboration in industrial\nsymbiosis networks\", Journal of Cleaner Production, Vol. 171, pp. 1058\u20131067.\nHoang, T.-H., Nguyen, N.P.P., Hoang, N.-Y.N., Akbari, M., Quang, H.T. & Binh, A.D.T. (2023),\n\"Application of social media in supply chain 4.0 practices: a bibliometric analysis and research\ntrends\", Operations Management Research, pp. 1\u201323.\nHossain, M.Z., Rahman, M.A.U., Rahaman, K.R., Ha-Mim, N.M. & Haque, S.F. (2023), \"Investigating\ncritical relationships among vulnerability, livelihoods, and non-migration strategies at the\nfishing communities in the Sundarbans\", Environment, Development and Sustainability, pp. 1\u2013\n40.\nHuma, S., Ahmed, W. & Najmi, A. (2020), \"Understanding the impact of supply-side decisions and\npractices on supply risk management\", Benchmarking: An International Journal, Vol. 27, No. 5,\npp. 1769\u20131792.\nJensen, M.C. & Meckling, W.H. (2019), \"Theory of the firm: Managerial behavior, agency costs and\nownership structure\", in Corporate Governance, pp. 77\u2013132, Gower.\nJia, M., Stevenson, M. & Hendry, L. (2023), \"A systematic literature review on sustainability-oriented\nsupplier development\", Production Planning & Control, Vol. 34, No. 8, pp. 727\u2013747.\nKannan, D. (2018), \"Role of multiple stakeholders and the critical success factor theory for the\nsustainable supplier selection process\", International Journal of Production Economics, Vol.\n195, pp. 391\u2013418.\nKlingebiel, K., Leiras, A. & M\u00e1sculo, F.S. (2013), \"Challenges for Managing Complexity in Industrial\nand Operations Management--A point of view from ICIEOM 2013\", Brazilian Journal of\nOperations & Production Management, Vol. 10, No. 2, pp. 7\u201310.\nKummer, S., Herold, D.M., Dobrovnik, M., Mikl, J. & Sch\u00e4fer, N. (2020), \"A systematic review of\nblockchain literature in logistics and supply chain management: identifying research questions\nand future directions\", Future Internet, Vol. 12, No. 3, 60.\nLahti, T., Wincent, J. & Parida, V. (2018), \"A definition and theoretical review of the circular economy,\nvalue creation, and sustainable business models: where are we now and where should research\nmove in the future?\", Sustainability, Vol. 10, No. 8, p. 2799.\nLarsson, J. & Larsson, L. (2020), \"Integration, application and importance of collaboration in\nsustainable project management\", Sustainability, Vol. 12, No. 2, p. 585.\nLe Jr, T. (2022), \"Supplier\u2019s Price Evaluation in the Purchasing Process\", Journal details needed.\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n16/18\nLee, H.L. & Tang, C.S. (2018), \"Socially and environmentally responsible value chain innovations:\nNew operations management research opportunities\", Management Science, Vol. 64, No. 3, pp.\n983\u2013996.\nLeiras, A. & Fontainha, T.C. (2019), \"Opportunities & Challenges for operations management--A\npoint of view from the 2018 POMS International Conference in Rio\", Brazilian Journal of\nOperations & Production Management, Vol. 16, No. 3, pp. 371\u2013374.\nLim, M.K., Li, Y., Wang, C. & Tseng, M.-L. (2021), \"A literature review of blockchain technology\napplications in supply chains: A comprehensive analysis of themes, methodologies and\nindustries\", Computers & Industrial Engineering, Vol. 154, p. 107133.\nLo, S.M., Zhang, S., Wang, Z. & Zhao, X. (2018), \"The impact of relationship quality and supplier\ndevelopment on green supply chain integration: A mediation and moderation analysis\", Journal\nof Cleaner Production, Vol. 202, pp. 524\u2013535.\nMacher, J.T. & Richman, B.D. (2008), \"Transaction cost economics: An assessment of empirical\nresearch in the social sciences\", Business and Politics, Vol. 10, No. 1, pp. 1\u201363.\nMani, V., Gunasekaran, A. & Delgado, C. (2018), \"Enhancing supply chain performance through\nsupplier social sustainability: An emerging economy perspective\", International Journal of\nProduction Economics, Vol. 195, pp. 259\u2013272.\nManuela, P., Cristina, B. & Molina-Morales, F.X. (2021), \"I need you, but do I love you? Strong ties\nand innovation in supplier--customer relations\", European Management Journal, Vol. 39, No. 6,\npp. 790\u2013801.\nMilgrom, P.R., Roberts, J. & Roberts, J. (1992), \u201cEconomics, organization and management\u201d, Vol. 7,\nPrentice-Hall, Englewood Cliffs, NJ.\nMohan, M., Nyadzayo, M.W. & Casidy, R. (2021), \"Customer identification: the missing link between\nrelationship quality and supplier performance\", Industrial Marketing Management, Vol. 97, pp.\n220\u2013232.\nNaghshineh, B. & Carvalho, H. (2022), \"The implications of additive manufacturing technology\nadoption for supply chain resilience: A systematic search and review\", International Journal of\nProduction Economics, Vol. 247, p. 108387.\nNandi, M.L., Nandi, S., Moya, H. & Kaynak, H. (2020), \"Blockchain technology-enabled supply chain\nsystems and supply chain performance: a resource-based view\", Supply Chain Management:\nAn International Journal, Vol. 25, No. 6, pp. 841\u2013862.\nOduro, S., Nyarku, K.M. & Gbadeyan, R.A. (2020), \"Supplier relationship management and\norganizational performance of hospitals in an emerging economy context: a comparative\nstudy\", Journal of Modelling in Management, Vol. 15, No. 4, pp. 1451\u20131478.\nPanfilova, E., Dzenzeliuk, N., Domnina, O., Morgunova, N. & Zatsarinnaya, E. (2020), \"The impact of\ncost allocation on key decisions of supply chain participants\", International Journal of Supply\nChain Management, Vol. 9, No. 1, pp. 552\u2013558.\nPaparoidamis, N.G., Katsikeas, C.S. & Chumpitaz, R. (2019), \"The role of supplier performance in\nbuilding customer trust and loyalty: A cross-country examination\", Industrial Marketing\nManagement, Vol. 78, pp. 183\u2013197.\nPereira, G.I., Niesten, E. & Pinkse, J. (2022), \"Sustainable energy systems in the making: A study on\nbusiness model adaptation in incumbent utilities\", Technological Forecasting and Social\nChange, Vol. 174, p. 121207.\nProsser, L., Lane, E.T. & Jones, R. (2021), \"Collaboration for innovative routes to market: COVID-19\nand the food system\", Agricultural Systems, Vol. 188, p. 103038.\nQueiroz, M.M. & Wamba, S.F. (2019), \"Blockchain adoption challenges in supply chain: An empirical\ninvestigation of the main drivers in India and the USA\", International Journal of Information\nManagement, Vol. 46, pp. 70\u201382.\nRejeb, A., Keogh, J.G., Simske, S.J., Stafford, T. & Treiblmaier, H. (2021), \"Potentials of blockchain\ntechnologies for supply chain collaboration: a conceptual framework\", The International Journal\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n17/18\nof Logistics Management, Vol. 32, No. 3, pp. 973\u2013994.\nReu\u00df, M., Grube, T., Robinius, M. & Stolten, D. (2019), \"A hydrogen supply chain with spatial\nresolution: Comparative analysis of infrastructure technologies in Germany\", Applied Energy,\nVol. 247, p. 438\u2013453.\nRezaei Vandchali, H., Cahoon, S. & Chen, S.-L. (2020), \"Creating a sustainable supply chain network\nby adopting relationship management strategies\", Journal of Business-to-Business Marketing,\nVol. 27, No. 2, pp. 125\u2013149.\nRindfleisch, A. (2020), \"Transaction cost theory: past, present and future\", AMS Review, Vol. 10, Nos.\n1\u20132, pp. 85\u201397.\nSaad, N.A., Elgazzar, S. & Kac, S.M. (2022), \"Linking supply chain management practices to customer\nrelationship management objectives: a proposed framework\", Business: Theory and Practice,\nVol. 23, No. 1, pp. 154\u2013164.\nSaghiri, S.S. & Mirzabeiki, V. (2021), \"Buyer-led environmental supplier development: Can suppliers\nreally help it?\", International Journal of Production Economics, Vol. 233, p. 107969.\nSchmidt, C.G. & Wagner, S.M. (2019), \"Blockchain and supply chain relations: A transaction cost\ntheory perspective\", Journal of Purchasing and Supply Management, Vol. 25, No. 4, p. 100552.\nShafiq, A., Johnson, P.F. & Klassen, R.D. (2022), \"Building synergies between operations culture,\noperational routines, and supplier monitoring: implications for buyer performance\",\nInternational Journal of Operations & Production Management, Vol. 42, No. 5, pp. 687\u2013712.\nShakeel, R., Sajjad, H. & Ramish, A. (2018), \"Towards A Third Party Logistics (3PL) Based Sustainable\nSupplier Relationship Management Framework\", Journal of Quality and Technology\nManagement, Vol. 15, No. 2, pp. 1\u201336.\nSharma, A., Adhikary, A. & Borah, S.B. (2020), \"Covid-19's impact on supply chain decisions: Strategic\ninsights from NASDAQ 100 firms using Twitter data\", Journal of Business Research, Vol. 117, pp.\n443\u2013449.\nShibin, K.T., Dubey, R., Gunasekaran, A., Hazen, B., Roubaud, D., Gupta, S. & Foropon, C. (2020),\n\"Examining sustainable supply chain management of SMEs using resource based view and\ninstitutional theory\", Annals of Operations Research, Vol. 290, pp. 301\u2013326.\nSikombe, S. & Phiri, M.A. (2019), \"Exploring tacit knowledge transfer and innovation capabilities\nwithin the buyer--supplier collaboration: A literature review\", Cogent Business & Management,\nVol. 6, No. 1, p. 1683130.\nSongsom, N., Nilsook, P., Wannapiroon, P., Fung, L.C.C. & Wong, K. (2019), \"System architecture of\na student relationship management system using Internet of Things to collect Digital Footprint\nof Higher Education Institutions\", International Journal of Emerging Technologies in Learning\n(IJET), Vol. 14, No. 23, pp. 125\u2013140.\nStek, K. & Schiele, H. (2021), \"How to train supply managers--necessary and sufficient purchasing\nskills leading to success\", Journal of Purchasing and Supply Management, Vol. 27, No. 4, p.\n100700.\nStone, A. (1986), \"The Economic Institutions of Capitalism: Firms, Markets, Relational Contracting.\nBy Oliver E. Williamson. (New York: Free Press, 1985. Pp. 450. $27.95.)\", American Political\nScience Review, Vol. 80, No. 4, pp. 1424\u20131425.\nSwallehe, O. (2021), \"Analysis of Challenges Facing SMEs in Implementing Suppliers Relationship\",\nSSRN. Nota: Detalhes adicionais sobre publica\u00e7\u00e3o s\u00e3o necess\u00e1rios para uma refer\u00eancia\ncompleta.\nTai, P.D., Anderson, M.R., Hien Duc, T.T., Thai, T.Q. & Yuan, X.-M. (2022), \"Strategic information\nsharing in supply chain with value-perceived consumers\", Industrial Management & Data\nSystems, Vol. 122, No. 4, pp. 841\u2013863.\nTay, H.L. & Aw, H. Sen. (2021), \"Improving logistics supplier selection process using lean six sigma--\nan action research case study\", Journal of Global Operations and Strategic Sourcing, Vol. 14, No.\n2, pp. 336\u2013359.\nQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing\nand service sectors\nBrazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024\n18/18\nTseng, S.-M. (2020), \"The Impacts Of Social Media Adoption And SRM Relational Information\nProcesses On Supply Chain Agility\", Journal of Information, Technology and Society.\nUddin, M.B., Fu, Y. & Akhter, B. (2020), \"Inter-organizational cost management: effects of\nantecedents and methods in a hybrid relational context\", Journal of Business & Industrial\nMarketing, Vol. 35, No. 5, pp. 909\u2013923.\nWang, C.-N., Yang, F.-C., Vo, T.M.N., Nguyen, V.T.T. & Singh, M. (2023), \"Enhancing Efficiency and\nCost-Effectiveness: A Groundbreaking Bi-Algorithm MCDM Approach\", Applied Sciences, Vol.\n13, No. 16, p. 9105.\nWernerfelt, B. (1984), \"A resource-based view of the firm\", Strategic Management Journal, Vol. 5,\nNo. 2, pp. 171\u2013180.\nWren, B. (2022), \"Sustainable supply chain management in the fast fashion Industry: A comparative\nstudy of current efforts and best practices to address the climate crisis\", Cleaner Logistics and\nSupply Chain, Vol. 4, p. 100032.\nWynstra, F., Suurmond, R. & Nullmeier, F. (2019), \"Purchasing and supply management as a\nmultidisciplinary research field: Unity in diversity?\", Journal of Purchasing and Supply\nManagement, Vol. 25, No. 5, p. 100578.\nYang, X. (2022), \"Vertical Coopetition: Effect of Supplier Relationship Management Strategies on\nSupplier Involvement in New Product Development\", IEEE Transactions on Engineering\nManagement.\nZhang, A., Wang, J.X., Faroharvaroque, M., Wang, Y. & Choi, T.-M. (2021), \"Multi-dimensional circular\nsupply chain management: A comparative review of the state-of-the-art practices and\nresearch\", Transportation Research Part E: Logistics and Transportation Review, Vol. 155, p.\n102509.\nAuthor contributions: MMHE: report writing, data analysis, and revisions of the manuscript for submission; TK:\nsupervision, planning, and revisions of the manuscript for submission; SAJS: organizing the manuscript and data\ncollection activities.",
    "title": "Quantifying the influence of supplier relationship management and supply chain performance",
    "publication_date": "2024-04-05",
    "cited_by_count": 145,
    "topics": "Developing Evidence-Informed Supply Chain Management Knowledge, Information Systems Outsourcing and Offshoring, Building Resilient Supply Chain",
    "keywords": "Supplier relationship management",
    "concepts": "Supply chain management, Supply chain, Business, Supplier relationship management, Process management, Industrial organization, Marketing",
    "text_type": "full_text",
    "openalex_rank": 10,
    "num_tokens": 15984,
    "best_oa_location_pdf_url": "https://bjopm.org.br/bjopm/article/download/2015/1070",
    "pdf_urls_by_priority": [
      "https://bjopm.org.br/bjopm/article/download/2015/1070"
    ],
    "successful_pdf_url": "https://bjopm.org.br/bjopm/article/download/2015/1070",
    "url": "https://bjopm.org.br/bjopm/article/download/2015/1070",
    "snippets": {
      "s1": "Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 1/18 RESEARCH PAPER Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Md Mehedi Hasan Emon1 , Tahsina Khan2, Saleh Ahmed Jalal Siam1 1American International University-Bangladesh (AIUB), Kuratoli, Bangladesh. 2Bangladesh University of Professionals (BUP), Mirpur Cantonment, Bangladesh. 1 INTRODUCTION The efficacy of an organization's supply chain management relies on its capacity to preserve Financial support: None. Conflict of interest: The authors have no conflict of interest to declare. Corresponding author: emonmd.mhasan@gmail.com Received: 03 October 2023. Accepted: 05 February 2024. Editor: Osvaldo Luiz Gonsalves Quelhas. This is an Open Access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use,distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT..",
      "s2": "..How to cite: Emon, M. M. H., Khan, T. and Siam, S. A. J. (2024), \u201cQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors\u201d, Brazilian Journal of Operations and Production Management, Vol. 21, No. 2, e20242015. https://doi.org/10.14488/BJOPM.2015.2024 Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 2/18 robust and mutually advantageous relationships with its suppliers. Supplier Relationship Management (SRM) is widely recognized as a crucial method for achieving this goal, as evidenced by numerous studies conducted in well-established industrial settings, such as the United States (Farooque et al., 2022; Zhang et al., 2021) and Germany (Reu et al., 2019). These studies, conducted on a worldwide scale, have highlighted the positive influence of proficient SRM strategies on..",
      "s3": "..various aspects of supply chain performance, including supplier performance, cost effectiveness, product quality, and delivery punctuality. As global industries continue to evolve, emerging economies like Bangladesh have become prominent participants in the international marketplace. In the specific context of Bangladesh, a country experiencing rapid economic growth and a flourishing industrial sector, the significance of efficient SRM cannot be emphasized enough. In recent years, Bangladesh has demonstrated significant progress in establishing itself as a prominent participant in the worldwide supply chain, with a particular focus on industries such textiles, ready-made clothing, and electronics (Fouji & Hoque, 2021). Due to its expanding industrial sector and extensive supplier network, the nation has emerged as an appealing destination for foreign enterprises in search of economically efficient manufacturing and sourcing alternatives. The observed evolution serves as evidence of Bangladesh's capacity inside the international marketplace. Nevertheless, it is crucial to examine the impact of SRM techniques on the performance of supply chains functioning in Bangladesh, as the country takes on its position in the global supply chain. Although there exists a considerable body of literature on SRM, a significant proportion of the study has mostly concentrated on industrialized economies. Consequently, there is a notable knowledge gap about the application and effectiveness of SRM in emerging economies such as Bangladesh. The distinctive environment of Bangladesh, characterized by its..",
      "s4": "..economic, cultural, and infrastructural intricacies, warrants a focused examination (Hossain et al., 2023). The primary objective of the present study is to fill the existing research void by quantitatively examining the impact of SRM on enhancing supply chain performance in the context of Bangladesh. This investigation will build upon the knowledge gained from prior research conducted on a worldwide scale. For instance, research conducted in the United States by Farooque et al., (2022) and in Germany by Reu\u00df et al., (2019) emphasized the favorable influence of proficient SRM strategies on various aspects, including supplier performance, cost effectiveness, product quality, and delivery punctuality. The conducted research, carried out in established industrial contexts, has yielded significant reference points for SRM procedures. Nevertheless, the specific economic, cultural, and infrastructural complexities of Bangladesh give rise to a separate framework that could potentially impact the dynamics of SRM in a distinct manner. The current study seeks to fill this void by particularly examining the Bangladeshi context, so adding a nuanced viewpoint to the existing pool of information. Through the analysis of SRM in an emerging country, this study aims to reveal unique perspectives that may diverge from findings in research undertaken in industrialized countries. A comprehensive comprehension of how SRM methods affect performance metrics is essential due to the unique difficulties and..",
      "s5": "..possibilities present in the supply chain ecosystem of Bangladesh. The study offers a fresh viewpoint by examining the utilization and efficacy of SRM in a distinctive and ever-changing economic environment, providing valuable insights that might enhance both theoretical understanding and practical implementation in the worldwide domain of supply chain management. Although several studies have investigated the concept of SRM in developed economies, there is a notable scarcity of study that focuses on examining this phenomenon within the unique context of Bangladesh. Prior studies conducted in this domain have yielded significant findings pertaining to the correlations between SRM methods and key performance indicators (KPIs) within supply chain management. For example, research done in the United States (Farooque et al., 2019) and Germany (Reu\u00df et al., 2019) has emphasized the favorable influence of proficient SRM strategies on supplier performance, cost effectiveness, product quality, and delivery punctuality. The studies highlight the significance of SRM in improving many facets of supply chain performance. Moreover, scholarly investigations conducted in developing nations, such as India, have provided evidence of the significance of SRM strategies in enhancing cost effectiveness (Queiroz & Wamba, 2019). Numerous studies undertaken in various international settings have continuously underscored the significance of collaboration, information sharing, and trust-building between buyers and suppliers as pivotal elements of effective SRM..",
      "s6": "..(Hoang et al., 2023). As Bangladesh strives to establish itself as a significant participant in the global supply chain, it is imperative to thoroughly investigate the suitability and efficacy of SRM approaches in this distinct setting, as shown by previous study findings. Therefore, the present study aims to expand upon the existing body of research by directing its attention towards the supply chains of Bangladesh. This study seeks to give significant insights for practitioners and policymakers acting within the Bangladeshi context by examining the correlation between SRM techniques and key supply chain performance indicators, including Supplier Collaboration, Supplier Development, Supplier Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 3/18 Evaluation and Selection, Long-Term Supplier Relationships. Previous research has provided valuable insights into the impact of SRM on supply chain performance at a global level. However, this study seeks to expand this knowledge by examining the specific context of Bangladesh. By doing so, it aims to contribute to a more comprehensive understanding of how SRM influences supply chain dynamics in emerging economies. Despite the increasing significance of SRM..",
      "s7": ".., there exists a dearth of empirical studies examining its influence on supply chain performance within the context of Bangladesh. The objective of this study is to address the existing research gap by investigating the precise connections between SRM methods and measures of supply chain performance. The value of this study is derived from its contribution to the existing body of knowledge on SRM specifically within the setting of Bangladesh. It contributes to the understanding of the advantages that may be gained by implementing successful SRM tactics. The primary objective of this study is to examine the manufacturing and service sectors in Bangladesh. The research will span enterprises of diverse sizes and operational complexities in order to conduct a thorough evaluation of SRM techniques and their influence on the performance of supply chains. 2 EMPIRICAL LITERATURE AND HYPOTHESIS DEVELOPMENT 2.1 Supplier relationship management and supply chain performance SRM is a critical aspect of modern supply chain management (Adesanya et al., 2020). It involves developing dynamic partnerships between businesses and their suppliers (Sharma et al., 2020). SRM focuses on creating lasting and mutually beneficial relationships beyond mere transactions (Enz & Lambert, 2023). In today's competitive landscape, strong supplier relationships are vital for success (Amoako-Gyampah et al., 2019). SRM goes beyond..",
      "s8": "..transactions, encompassing strategic collaboration, innovation, and value generation (Pereira et al., 2022). This review examines SRM's impact on supply chain performance, including cost efficiency, product quality, on-time delivery, and customer satisfaction. SRM has evolved from transactional interactions to strategic alliances (Abbas & Tong, 2023). Recognizing suppliers as essential partners shifted the focus to collaboration and long-term partnerships. Technological advancements improved SRM's efficiency, with digital platforms enhancing communication (Emon & Nahid, 2023; Tseng, 2020). Strong relationships with suppliers stimulate innovation and value creation (Lee & Tang, 2018). SRM now encompasses collaborative partnerships, risk management, supplier development, and sustainability (Emon & Khan, 2023; Huma et al., 2020). Empirical studies show that proficient SRM positively impacts supplier performance, cost-effectiveness, product quality, and delivery punctuality (Le Jr, 2022). Collaboration, information exchange, and trust development are key foundations of successful SRM (Cha & Kim, 2018). Supplier development activities enhance supplier performance (Mani et al., 2018). 2.2 Supplier Collaboration and Supply Chain Performance: SRM highlights the importance of strategic collaboration as a fundamental element for achieving success in the current dynamic and competitive supply chain environment..",
      "s9": "..(Oduro et al., 2020). There is a large body of literature that strongly supports the notion that successful collaboration with suppliers plays a crucial role in improving the performance of the supply chain. This section examines the current studies on how supplier collaboration affects several aspects of supply chain performance and builds upon the literature presented by Oduro et al., (2020). Collaborative relationships in SRM go beyond standard transactional methods and play a crucial role in promoting innovation, enhancing product quality, assuring timely delivery, and ultimately improving customer happiness (Abtahi et al., 2023; Stek & Schiele, 2021). The development of SRM has experienced a significant change from transactional interactions to the formation of strategic partnerships, emphasizing the crucial role of collaboration in creating and sustaining long-term relationships with suppliers (Yang, 2022). The growing interdependence and global integration of supply chains has underscored the importance of strong collaborative methods. Technological improvements are crucial in enabling and enhancing collaborative efforts in SRM. Digital platforms have become crucial instruments that not only facilitate communication but also improve the sharing of information between organizations and their suppliers (Ebinger & Omondi, 2020). These platforms facilitate instantaneous data exchange, which is essential for efficient collaboration, especially in a multinational supply chain setting where prompt information is..",
      "s10": "..vital. Although (Leiras & Fontainha, 2019; Oduro et al., 2020) have shed light on the beneficial effects of supplier collaboration on supply chain performance, there may still be gaps and unexplored areas in the existing literature. The purpose of this part is to identify the gaps in the current understanding of how supplier collaboration affects supply chain performance. It also highlights Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 4/18 the specific contributions of this study in addressing and enhancing the existing knowledge in this area. This study seeks to further the knowledge of the complex relationship between collaborative practices and supply chain performance by combining additional literature that addresses subtle aspects of supplier collaboration. 2.3 Supplier Development and Supply Chain Performance Supplier development activities are acknowledged as key elements in the larger context of SRM, serving a vital function in improving supplier performance and, subsequently, overall supply chain efficiency (Dubey et al., 2019). This part performs a thorough examination of the current body of research to investigate the complex connection between supplier..",
      "s11": "..development and supply chain performance, expanding upon the knowledge presented by Dubey et al., (2019). Supplier development goes beyond conventional transactional methods and encompasses a range of actions aimed at improving the talents and performance of suppliers (Awan et al., 2019). Activities may encompass training programs, streamlining processes, embracing technology, and engaging in collaborative projects with the goal of promoting innovation and ongoing enhancement. According to the literature, supplier development programs that are successful contribute to better supplier performance, which in turn leads to cost-effectiveness and increased efficiency in the supply chain (Asif et al., 2022). Although the current body of literature offers significant insights into the favorable relationship between supplier development and supply chain performance, there are still intricacies and unique mechanisms that warrant further investigation. Gaining insight into the impact of different elements of supplier development on various facets of supply chain performance is essential for formulating focused and efficient strategies. This section seeks to highlight any existing gaps in the current knowledge and clarify how the present study helps to filling these gaps by offering a detailed perspective on the complex connection between supplier development activities and the overall performance of the supply chain. 2.4 Supplier Evaluation and Supply Chain Performance: Effective SRM relies on the use of strong supplier assessment methods, which are essential for achieving optimal..",
      "s12": "..supply chain efficiency (Amoako-Gyampah et al., 2019; Klingebiel et al., 2013). This section provides a comprehensive analysis of the current research on the complex connection between supplier evaluation and the overall performance of the supply chain. It expands on the fundamental findings presented by Amoako-Gyampah et al., (2019). Supplier evaluation in the context of SRM entails a thorough examination of supplier performance, cost efficiency, product quality, and delivery timeliness (Lim et al., 2021). Proficient SRM has been found to have a favorable impact on supplier evaluation, leading to improvements in several aspects of supply chain performance, as indicated by the existing literature. However, there may be gaps in our present understanding of how the precise criteria used in supplier assessment processes are closely connected to various aspects of supply chain performance. The purpose of this section is to identify and explain the gaps in the current knowledge, by conducting a detailed analysis of the relationship between different criteria used to evaluate suppliers and various aspects of supply chain performance. Through this approach, it aims to enhance and improve the theoretical foundation that supports the connection between supplier evaluation methods and the overall efficacy of the supply chain. 2.5 Long-Term Supplier Relationship and Supply Chain Performance: Long..",
      "s13": "..-term supplier connections play a crucial role in SRM, exerting substantial impact on supply chain performance (Oduro et al., 2020). This part provides a thorough examination of the current research, exploring the complex connection between long-term relationships with suppliers and the various factors that affect supply chain effectiveness. The fundamental knowledge presented by Shakeel et al., (2018) acts as a crucial basis for this investigation. Organizations have recognized the importance of suppliers as crucial partners and have therefore made it a strategic priority to develop long-term relationships. This involves focusing on collaboration and mutual benefits under the SRM framework (Kannan, 2018). Empirical research confirm that long-term supplier relationships have a beneficial effect on different measures of supply chain performance. However, there may be gaps in our current understanding regarding the detailed mechanisms by which these linkages contribute to the complex fabric of supply chain performance. This section aims to thoroughly identify any potential gaps in the current literature, with the goal of clarifying how the present study enhances the understanding of the relationship between long-term supplier relationships and various aspects of supply chain performance. In order to improve the scope and Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production..",
      "s14": "..Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 5/18 comprehensiveness of this analysis, it will be crucial to include contemporary research and a variety of viewpoints on long-term supplier partnerships. This section aims to add to both theoretical refinement and practical insights for businesses managing long-term supplier relationships within the broader framework of SRM. 2.6 Theoretical Frameworks in SRM Understanding Supplier Relationship Management's (SRM) theoretical foundations and evolution is essential to grasp its dynamics. Originally transactional, SRM has evolved into a strategic approach focusing on long-term value (Dash et al., 2018). Theoretical dimensions include transaction cost minimization, resource leverage, incentive alignment, and risk management (Rejeb et al., 2021). The evolution of Transaction Cost Economics (TCE), Resource-Based View (RBV), and Agency Theory has mirrored SRM's transformation. Transaction Cost Economics (TCE) emphasizes minimizing transaction costs in SRM (Swallehe, 2021). Resource-Based View (RBV) highlights supplier relationships as sources of competitive advantage (Andersen, 2021). Agency Theory addresses agency issues within supplier relationships (Dubey..",
      "s15": "..et al., 2019). These theories intersect in SRM to reduce costs, leverage resources, align incentives, and manage risks (Gilmore & Buhaug, 2021). Depending upon the theoretical underpinnings elucidated in the comprehensive examination of existing literature, the present section delineates the conceptual framework that will guide the investigation. The theoretical framework selected for this study is the Resource-Based View (RBV), which asserts that businesses can attain enduring competitive advantage via the strategic utilization of distinctive resources and skills. across the scope of this study, the Resource-Based View (RBV) framework offers a perspective that enables the examination of how SRM methods impact a particular outcome variable, namely cost efficiency, across supply chains operating in Bangladesh. The Resource-Based View (RBV) theory places significant emphasis on the significance of supplier relationships as important sources of resources and capabilities that ultimately promote cost effectiveness within supply chains. The resources and capabilities obtained through SRM techniques are anticipated to have a favorable impact on cost efficiency. This is demonstrated by the potential to attain cost reductions, cost savings, and cost-effectiveness in supply chain activities. 2.7 Research Hypotheses & Conceptual Framework Supplier collaboration: The implementation of supplier collaboration, which involves engaging in cooperative activities and sharing knowledge with suppliers, has been demonstrated..",
      "s16": "..to improve Table 1 - Theoretical Frameworks in SRM Theoretical Framework Key Concepts and Relevance Sources Transaction Cost Economics - Minimizing transaction costs in supplier relationships. (Rindfleisch, 2020; Schmidt & Wagner, 2019; Stone, 1986) - Aligning SRM practices with transaction-specific characteristics. (Macher & Richman, 2008; Saad et al., 2022; Wynstra et al., 2019) Resource-Based View - Leveraging supplier relationships as sources of competitive advantage. (Barney, 1991; Nandi et al., 2020; Shibin et al., 2020) - Recognizing supplier-specific assets, knowledge, and capabilities. (Burki et al., 2023; Saghiri & Mirzabeiki, 2021; Wernerfelt, 1984) Agency Theory - Addressing agency problems and information asymmetry in SRM. (Dong et al., 2021; Jensen & Meckling, 2019; Kummer et al., 2020) - Designing SRM practices to align incentives between organizations and suppliers. (Ali et al., 2020; Dubey et al., 2019; Eisenhardt, 1989) - Monitoring and control mechanisms in SRM relationships. (Fama, 1980; Songsom et al., 2019) - The role of contracts and performance-based incentives in SRM. (..",
      "s17": "..Grum et al., 2023; Milgrom et al., 1992) Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 6/18 the operational efficiency of supply chain processes. Collaboration of this nature has the potential to enhance communication, optimize procedures, and enhance coordination, hence yielding cost efficiencies. Prior studies have provided evidence that the establishment of successful collaboration with suppliers has a beneficial influence on cost efficiency (Cole & Aitken, 2019; Paparoidamis et al., 2019), thus confirming the premise of a positive association. H0: Supplier collaboration has no significant influence on cost efficiency in the supply chains of Bangladeshi organizations. H1: Supplier collaboration has a significant positive influence on cost efficiency in the supply chains of Bangladeshi organizations. Supplier development: Supplier development programs, encompassing activities such as training and knowledge transfer, possess the capacity to augment the capabilities of suppliers, hence resulting in enhancements to processes and reductions in costs. Previous studies have demonstrated that supplier development initiatives have a favorable impact on cost effectiveness (Awan..",
      "s18": "..et al., 2019; Sikombe & Phiri, 2019), hence substantiating the proposition of a positive correlation. H0: Supplier development efforts have no significant impact on cost efficiency in the supply chains of Bangladeshi organizations. H2: Supplier development efforts have a significant positive impact on cost efficiency in the supply chains of Bangladeshi organizations. Supplier Evaluation and Selection: The implementation of effective supplier evaluation and selection methods is crucial in ensuring that organizations establish partnerships with dependable and high-performing suppliers. Efficient evaluation procedures are anticipated to result in improved selection of suppliers, hence positively influencing cost effectiveness through the mitigation of quality-related concerns and supply disruptions. The favorable association between supplier evaluation and selection and cost efficiency has been supported by empirical research conducted by Kannan (2018) and Naghshineh & Carvalho (2022) (Kannan, 2018; Naghshineh & Carvalho, 2022). H0: Effective supplier evaluation and selection practices have no significant effect on cost efficiency in the supply chains of Bangladeshi organizations. H3: Effective supplier evaluation and selection practices have a significant positive effect on cost efficiency in the supply chains of Bangladeshi organizations. Long-Term Supplier Relationships: The establishment and maintenance of enduring connections with essential suppliers contribute to the cultivation of trust, collaboration, and shared comprehension...",
      "s19": "..These attributes are frequently linked with enhanced operational efficiency and less interruptions, hence contributing to cost-effectiveness. Prior studies have demonstrated that establishing enduring partnerships with suppliers has a favorable impact on cost effectiveness (Herczeg et al., 2018; Wang et al., 2023), hence lending weight to the proposition of a positive association. H0: Long-term supplier relationships have no significant impact on cost efficiency in the supply chains of Bangladeshi organizations. H4: Long-term supplier relationships have a significant positive impact on cost efficiency in the supply chains of Bangladeshi organizations. The conceptual framework includes SRM practices as independent variables and supply chain performance indicators (cost efficiency) as dependent variables. As illustrated in Figure 1 the four independent variables namely Supplier collaboration, Supplier development, Supplier Evaluation and Selection, Long-Term Supplier Relationships have been presented in the left side of the diagram and dependent variable cost efficiency is presented in the Right side of the diagram. The relationships between each independent variable and the dependent variable are indicated by the four-research hypothesis connecting the variables through the arrows. Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https:/..",
      "s20": "../doi.org/10.14488/BJOPM.2015.2024 7/18 Figure 1 - Conceptual Framework 3. MATERIAL AND METHODS The research design employed for this study is quantitative, aiming to comprehensively investigate the relationship between SRM practices and cost efficiency in the supply chains of firms operating in Bangladesh. The study focuses on one main category of variables: independent variables representing SRM practices and one dependent variable representing cost efficiency. In terms of independent variables, the study encompasses four dimensions of SRM practices. Supplier Collaboration, the variable in question quantifies the extent of collaboration and information exchange between enterprises and their suppliers. This statement elucidates the degree to which firms partake in collaborative endeavors and exchange information with suppliers in order to enhance cost effectiveness within their supply chains (Tai et al., 2022). Supplier Development, this variable evaluates the endeavors undertaken by enterprises to enhance the capabilities and resources of their suppliers. The initiatives encompass many strategies, such as training programs, technology transfer endeavors, and collaborative projects focused on enhancing process efficiency with the goal of attaining cost effectiveness (Gu et al., 2021). Supplier Evaluation and Selection, this variable investigates the criteria and procedures employed by firms to assess, choose, and oversee their suppliers. The concept incorporates various elements such as assessments of supplier performance, evaluations..",
      "s21": "..of quality, and procedures related to risk management, all of which have an impact on cost efficiency (Dobos & V\u00f6r\u00f6smarty, 2019). Long-Term Supplier Relationships, this characteristic pertains to the temporal extent and steadfastness of the associations established between organizations and their suppliers. The metric assesses the degree to which firms sustain enduring collaborations with crucial suppliers, which can have a lasting impact on cost effectiveness (Mohan et al., 2021). The dependent variable, Cost Efficiency, the dependent variable in question measures the degree of cost-effectiveness exhibited by supply chain operations. The measurements encompassed in this category consist of cost reduction, cost savings, and supply chain cost-to-revenue ratios (Panfilova et al., 2020). The study's setting was Bangladesh, with a specific focus on organizations in various sectors, including manufacturing and services. The study involved a Convenience sampling technique to ensure representation from both manufacturing and service sectors. Data were collected from multiple departments or individuals responsible for SRM and supply chain operations within each firm, such as procurement, supply chain management, and strategic management. The participants, selected based on their roles and responsibilities related to SRM practices, included managers, procurement officers, and individuals involved in supply chain decision-making processes. A sample size..",
      "s22": "..of 270 individuals was determined to strike a balance between practicality and the requirement for statistical significance, considering the constraints of convenience sampling. Data were collected through a structured survey instrument consisting of 22 items, representing independent and dependent variables. The Likert scale was used to measure responses, ranging from strongly disagree to strongly agree. The survey instrument was developed based on a thorough review of existing literature and validated scales used in previous studies. Quantitative techniques, including regression analysis and correlation analysis, were applied for data analysis using SPSS 22 to assess the relationships between SRM practices and cost efficiency. These statistical methods aimed to provide insights into the extent to which SRM practices influenced cost efficiency in the context of Bangladeshi firms. Throughout the research process, ethical considerations, including informed consent, data Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 8/18 confidentiality, and anonymity, were meticulously maintained to ensure the integrity and ethical conduct of the study. The reliability of the measurements was assessed through techniques such as Cronbach's alpha for survey instruments. The study aimed to..",
      "s23": "..minimize measurement errors and ensure the reliability of collected data. Table 2 - Reliability of the Measurements Construct Item No Cronbach\u2019s alpha Supplier Collaboration 5 .911 Supplier Development 5 .922 Supplier Evaluation and Selection 5 .876 Long Term Supplier Relationships 5 .917 Cost Efficiency 2 .854 The reliability of the measurements was assessed through Cronbach\u2019s alpha values for each construct in Table 2. The results indicate high internal consistency and reliability among the items measuring Supplier Collaboration, Supplier Development, Long-Term Supplier Relationships, and Cost Efficiency, with Cronbach\u2019s alpha values of .911, .922, .917, and .854, respectively. The construct of Supplier Evaluation and Selection also exhibits good internal consistency, with a Cronbach\u2019s alpha value of .876. These findings affirm the dependability of the measurement instruments, suggesting that the items within each construct consistently capture the intended concepts. Overall, the study's measurement instruments demonstrate satisfactory to excellent reliability, instilling confidence in the accuracy and consistency of the data collected for the investigation. 4. RESULTS 4.1 Correlation Table 3 - Correlations Analysis Supplier Collaborati on Supplier Develop ment Supplier Evaluatio n and Selection Long Term Supplier Relationshi ps Cost Efficiency Supplier Collabora tion Pearson Correlati on 1 .889**..",
      "s24": "...856** .796** .728** Sig. (2- tailed) .000 .000 .000 .000 N 270 270 270 270 270 Supplier Develop ment Pearson Correlati on .889** 1 .885** .801** .702** Sig. (2- tailed) .000 .000 .000 .000 N 270 270 270 270 270 Supplier Evaluatio n and Selection Pearson Correlati on .856** .885** 1 .792** .685** Sig. (2- tailed) .000 .000 .000 .000 N 270 270 270 270 270 Long Term Supplier Relations hips Pearson Correlati on .796** .801** .792** 1 .782** Sig. (2- tailed) .000 .000 .000 .000 N 270 270 270 270 270 Cost Pearson .728** .702** .685** .782** 1 Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM...",
      "s25": "..2015.2024 9/18 Efficiency Correlati on Sig. (2- tailed) .000 .000 .000 .000 N 270 270 270 270 270 **. Correlation is significant at the 0.01 level (2-tailed). The results of the correlation study, as shown in Table 6, indicate statistically significant associations among the variables being examined, namely \"Supplier Collaboration,\" \"Supplier Development,\" \"Supplier Evaluation and Selection,\" \"Long Term Supplier Relationships,\" and \"Cost Efficiency.\" The quantification of these interactions is accomplished by use Pearson correlation coefficients, all of which exhibit statistical significance at the 0.01 level (two-tailed), so suggesting robust associations. The dimension of \"Supplier Collaboration\" demonstrates a notably strong positive correlation of 88.9% with the dimension of \"Supplier Development,\" indicating a solid and significant association between these two aspects of managing supplier relationships. Furthermore, the concept of \"Supplier Collaboration\" exhibits a significant positive correlation of 85.6% with \"Supplier Evaluation and Selection,\" underscoring the robustness of the association between these two factors. The concept of \"Supplier Development\" exhibits a notably robust positive correlation of 88.5% with \"Supplier Evaluation and Selection,\" hence emphasizing their significant alignment. Moreover, there exists..",
      "s26": "..a strong positive correlation of 80.1% between the construct of \"Supplier Development\" and the construct of \"Long Term Supplier Relationships,\" highlighting the significant interdependence of both variables. The variable \"Supplier Evaluation and Selection\" demonstrates a significant positive correlation of 79.2% with the variable \"Long Term Supplier Relationships,\" indicating a robust link between the two. There exists a positive association between \"Cost Efficiency\" and all elements of supplier relationship management, as evidenced by the Pearson correlation coefficients. It is worth noting that the variable \"Cost Efficiency\" exhibits a substantial positive correlation of 72.8% with the variable \"Supplier Collaboration,\" a positive correlation of 70.2% with \"Supplier Development,\" a positive correlation of 68.5% with \"Supplier Evaluation and Selection,\" and a notably high positive correlation of 78.2% with \"Long Term Supplier Relationships.\" In summary, the aforementioned data suggest that when supplier management techniques adopt a more collaborative approach, prioritize development, incorporate evaluation measures, and aim to cultivate long-term partnerships, there is a notable enhancement in \"Cost Efficiency.\" The percentages presented in the data indicate the magnitude and orientation of these connections, underscoring the noteworthy influence of proficient supplier relationship management on cost effectiveness, a critical facet of organizational efficacy...",
      "s27": "..5.2 Regression Analysis Table 4 - Model Summary Model R R Square Adjusted R Square Std. Error of the Estimate Change Statistics R Square Change F Change df1 df2 Sig. F Change 1 .801a.642 .637 .69525 .642 118.993 4 265 .000 a. Predictors: (Constant), Long Term Supplier Relationships, Supplier Evaluation and Selection, Supplier Collaboration, Supplier Development The Model Summary table presents a detailed overview of the regression analysis performed in this study, providing insights into the performance and importance of the model. The table presents a strong positive linear association between the dependent variable (not specified in the table) and a group of independent variables, including SRM practices such as Supplier Collaboration, Supplier Development, Supplier Evaluation and Selection, and Long-Term Supplier Relationships, as indicated by an R-value of .801. The considerable R-value highlights the model's capacity to elucidate fluctuations in the dependent variable. The coefficient of determination, represented by the symbol R2, is seen to be 0.642. This implies that around 64.2% of the variability observed in the dependent variable may be ascribed to the impact of the SRM practices used in the model. The observed R2 value indicates that the set of SRM practices under consideration..",
      "s28": "..make a substantial contribution towards explaining the variability observed in the dependent variable. Additionally, the adjusted R2, which accounts for the number of predictors in the model, demonstrates a robust value of .637. The revised number suggests that, even after accounting for Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 10/18 the potential danger of overfitting, the model is capable of efficiently elucidating around 63.7% of the variability observed in the dependent variable. The standard error of the estimate, which represents the mean variation between projected and actual values, is calculated to be .69525. This metric offers a measure of the model's accuracy in forecasting the values of the dependent variable. The change statistics section provides evidence of a significant enhancement in the model's fit when using the SRM practices as predictors. The F-statistic, which has a value of 118.993, along with a p-value of .000 that is highly significant, provides confirmation of the overall relevance of the model incorporating these predictors. The..",
      "s29": "..aforementioned observation highlights the significance and pertinence of the Strategic Risk Management (SRM) techniques when elucidating the fluctuations observed in the dependent variable. In summary, the Model Summary table indicates that the regression model, which includes SRM practices as predictors, is statistically significant and has a strong ability to account for a significant portion of the variability in the dependent variable. This underscores the importance of these practices in the context of the study. Table 5 - ANOVAa Model Sum of Squares df Mean Square F Sig. 1 Regression 230.073 4 57.518 118.993 .000b Residual 128.094 265 .483 Total 358.167 269 a. Dependent Variable: Cost Efficiency b. Predictors: (Constant), Long Term Supplier Relationships, Supplier Evaluation and Selection, Supplier Collaboration, Supplier Development Table 4 presents the ANOVA table, which provides valuable insights into the relevance of the predictor variables in the regression model used to estimate \"Cost Efficiency.\" The table is partitioned into three primary segments, namely Regression, Residual, and Total. In the Regression section, the calculation of the sum of squares is performed to evaluate the amount of variance accounted for by the model. The resulting value is determined to be 230.073, with a corresponding 4 degrees of freedom (df). The..",
      "s30": "..mean square value that corresponds to the given data is 57.518. In contrast, the Residual part pertains to the unexplained variance and presents a sum of squares equal to 128.094, which is accompanied by 265 degrees of freedom. The Total Sum of Squares (TSS), which measures the total variance in \"Cost Efficiency,\" is calculated to be 358.167. The F-statistic, a crucial metric, is computed as 118.993 by dividing the mean square for the regression by the mean square for the residual. This statistical measure evaluates the overall significance of the regression model. The p-value, shown as \"Sig.,\" is significantly small (p = .000). The obtained p-value, which is remarkably low, provides strong evidence that the regression model, encompassing Long-Term Supplier Relationships, Supplier Evaluation and Selection, Supplier Collaboration, and Supplier Development as predictor variables, is highly significant in elucidating the fluctuations observed in \"Cost Efficiency.\" In more accessible language, these predictor variables together considerably improve the model's capacity to explain the differences found in \"Cost Efficiency.\" Therefore, the analysis of variance (ANOVA) findings offer strong statistical support for the model's ability to effectively elucidate and forecast cost efficiency within the framework of..",
      "s31": "..supplier relationship management procedures. Table 6 - Coefficients Model Unstandardized Coefficients Standardized Coefficients t Sig. B Std. Error Beta 1 (Constant) -.303 .270 -1.121 .263 Supplier Collaboration .342 .109 .275 3.146 .002 Supplier Development .027 .114 .022 .233 .816 Supplier Evaluation and Selection -.007 .118 -.005 -.058 .954 Long Term Supplier Relationships .681 .082 .550 8.332 .000 a. Dependent Variable: Cost Efficiency Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 11/18 The examination of the coefficients in Table 5 allows for the analysis of the regression model, providing valuable insights into the distinct impacts of each predictor variable on the dependent variable, \"Cost Efficiency.\" The constant term in the equation represents the baseline level of \"Cost Efficiency\" in the absence of any predictor factors. The observed phenomenon exhibits an unstandardized coefficient (B) of -0.303, accompanied by a standard error of 0..",
      "s32": "...270. The t-statistic associated with the given data is -1.121, resulting in a p-value of 0.263. This p-value is above the commonly accepted significance level of 0.05. Therefore, it may be concluded that the constant term does not possess statistical significance in its ability to explain the fluctuations observed in \"Cost Efficiency.\" Shifting our focus towards the independent factors, the variable \"Supplier Collaboration\" holds considerable importance. The unstandardized coefficient (B) is reported as 0.342, with a standard error of 0.109, and a standardized coefficient (Beta) of 0.275. The predictor exhibits a t-statistic of 3.146, accompanied by a p-value of 0.002, indicating a significant and positive influence on the variable \"Cost Efficiency.\" More precisely, there is a positive correlation between a one-unit increase in supplier collaboration and a 0.342-unit improvement in \"Cost Efficiency.\" On the other hand, the variable \"Supplier Development\" demonstrates a coefficient (B) of 0.027, which lacks standardization, accompanied with a standard error of 0.114. Nevertheless, the standardized coefficient (Beta) for this variable is only 0.022, indicating a minimal impact. The..",
      "s33": "..t-statistic for the predictor variable is calculated to be 0.233, with a corresponding p-value of 0.816. This p-value exceeds the commonly accepted significance level of 0.05. Therefore, the variable \"Supplier Development\" does not exhibit a statistically significant impact on the variable \"Cost Efficiency.\" Similarly, the variable \"Supplier Evaluation and Selection\" is found to have an unstandardized coefficient (B) of -0.007, with a corresponding standard error of 0.118. The standardized coefficient (Beta) has a value of -0.005. The t-statistic for the predictor in question is -0.058, and its corresponding p-value is 0.954, both of which are significantly beyond the conventional threshold for statistical significance. Therefore, the variable \"Supplier Evaluation and Selection\" does not demonstrate a statistically significant influence on the variable \"Cost Efficiency.\" Finally, the variable \"Long Term Supplier Relationships\" appears as a significantly influential predictor. The unstandardized coefficient (B) is reported as 0.681, accompanied by a standard error of 0.082. Additionally, the standardized coefficient (Beta) is reported as 0.550, indicating a significant effect. The t-statistic for the predictor variable exhibits robustness, measuring..",
      "s34": "..8.332. The related p-value is 0.000, indicating a significant and positive impact on the variable \"Cost Efficiency.\" Specifically, an increase of one unit in long-term supplier connections is associated with a significant rise of 0.681 units in \"Cost Efficiency.\" In summary, the variables of \"Supplier Collaboration\" and \"Long Term Supplier Relationships\" demonstrate statistically significant relationships with positive impacts on \"Cost Efficiency.\" However, the variables of \"Supplier Development\" and \"Supplier Evaluation and Selection\" do not exhibit such statistical significance in explaining variations in the dependent variable. 5. DISCUSSION AND IMPLICATIONS OF FINDINGS The study's findings provide useful insights into the connections between SRM practices and supply chain performance, with a specific emphasis on cost efficiency. This analysis is conducted within the specific context of Bangladesh's industrial and service industries. The correlation study demonstrates statistically significant relationships among the variables under investigation, emphasizing the strong linkages between Supplier Collaboration, Supplier Development, Supplier Evaluation and Selection, Long-Term Supplier Relationships, and Cost Efficiency. The presence of positive correlations suggests that the successful execution of SRM procedures has a key role in attaining cost-effectiveness in supply chain operations. The robust positive association between Supplier Collaboration and Supplier Development highlights the interdependence of both SRM characteristics...",
      "s35": "..Organizations that participate in cooperative efforts and allocate resources to increase the competencies of their suppliers are likely to achieve a synergistic outcome, resulting in enhanced cost effectiveness (Feizabadi & Alibakhshi, 2022; Shafiq et al., 2022; Uddin et al., 2020). Furthermore, the strong correlation between Supplier Collaboration and Supplier Evaluation and Selection underscores the significance of thorough evaluation and collaboration. This implies that organizations that prioritize both aspects simultaneously are more likely to achieve efficient and economical supply chain operations. The strong positive association between Supplier Development and Supplier Evaluation and Selection highlights the consistency between both procedures. Organizations that allocate resources to supplier development projects are also prone to having clearly defined assessment and choice procedures, which in turn lead to improved cost effectiveness. Furthermore, the robust positive link between Supplier Development and Long-Term Supplier Relationships underscores the strategic interconnectedness of both aspects. Organizations that prioritize the development of their suppliers are more likely to establish longlasting partnerships, which in turn leads to sustainable cost efficiency in the long run (Lahti et al., Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi...",
      "s36": "..org/10.14488/BJOPM.2015.2024 12/18 2018; Larsson & Larsson, 2020; Prosser et al., 2021; Wren, 2022). The correlation between LongTerm Supplier Relationships and Supplier Evaluation and Selection indicates that firms that maintain long-term partnerships with important suppliers also place a high emphasis on thorough evaluation and selection procedures. This emphasizes the need of stability and trust in supplier relationships for attaining cost-efficient supply chain operations. The regression analysis enhances these insights by offering a comprehensive model summary. The model, which incorporates Supplier Collaboration, Supplier Development, Supplier Evaluation and Selection, and Long-Term Supplier Relationships as predictors, shows statistical significance in explaining the observed heterogeneity in Cost Efficiency. The high R-value and coefficient of determination (R2) suggest that approximately 64.2% of the variation in cost efficiency may be attributable to the influence of the SRM practices included in the model. The corrected R2, which takes into consideration the number of predictors, remains strong at 63.7%, confirming the model's effectiveness in explaining variability without overfitting. The substantial F-statistic in the ANOVA table provides additional confirmation of the model's overall significance, highlighting the combined influence of the predictor variables on Cost Efficiency. The coefficients analysis offers..",
      "s37": "..comprehensive insights into the specific effects of each predictor variable. Supplier Collaboration and Long-Term Supplier Relationships are identified as statistically significant factors that have a beneficial impact on Cost Efficiency. Nevertheless, the effects of Supplier Development and Supplier Evaluation and Selection on Cost Efficiency are not statistically significant. The study's findings through the empirical results hold substantial implications for organizations in Bangladesh's manufacturing and service sectors, offering precise guidance on optimizing supply chain performance, particularly in terms of cost efficiency. The strong positive connections observed between Supplier Collaboration, Supplier Development, Supplier Evaluation and Selection, Long-Term Supplier Relationships, and Cost Efficiency highlight the interconnectedness of various aspects under SRM. Businesses must recognize the need of implementing a thorough and unified approach to SRM processes to enhance cost efficiency in their supply chain operations. The importance of Supplier Collaboration and Long-Term Supplier Relationships in terms of strategic significance becomes evident as a crucial lesson. Organizations that prioritize collaborating with suppliers and fostering long-lasting partnerships are wellpositioned to achieve ongoing cost reduction (Allioui & Mourdi, 2023). This highlights a change in viewpoint, encouraging businesses to see suppliers not only as transactional entities but as strategic partners, cultivating enduring relationships that greatly contribute to the overall success of the supply chain. The correlation between Supplier Collaboration and Supplier Development implies a mutually beneficial..",
      "s38": "..result when firms collaborate and invest in improving supplier skills. This is consistent with the current body of research on collaborative supply chain methods and their influence on operational efficiency and cost reduction (Benton Jr et al., 2020; Gu et al., 2021; Lo et al., 2018). Moreover, it is important to strengthen the argument of the significant association between Supplier Development and Long-Term Supplier Relationships by referencing research that emphasize the strategic interdependence of these factors. Empirical evidence demonstrating the enduring advantages of supplier development activities on fostering durable partnerships and enhancing cost efficiency would strengthen the claim (Cocskun et al., 2022; Faruquee et al., 2021; Jia et al., 2023; Manuela et al., 2021; Rezaei Vandchali et al., 2020). Supplier Development remains crucial in SRM, but its effect on cost efficiency depends on the presence of complementary strategies, as indicated by the study. The integration of supplier development programs with rigorous supplier evaluation and selection processes is crucial. Organizations should prioritize not only improving supplier competencies, but also conducting thorough evaluations and choosing suppliers based on their performance, quality, and risk management. The strong correlation between Long-Term Supplier Relationships and Supplier Evaluation and Selection emphasizes the significance of stability and confidence in supplier connections..",
      "s39": "... Organizations that establish longterm cooperation with important suppliers are more inclined to prioritize comprehensive review and selection procedures, resulting in streamlined supply chain operations (Tay & Aw, 2021). Therefore, it may be inferred that making efforts in establishing and sustaining long-lasting relationships with suppliers can result in a series of beneficial outcomes for many aspects of supply chain performance, such as improved cost effectiveness. Having highlighted in the above discussions, it is worth mentioning here that these discoveries provide practical and implementable knowledge for sustaining supply chain operations in Bangladesh. Emphasizing cooperative methods, fostering enduring partnerships with crucial suppliers, and executing rigorous assessments of suppliers can greatly contribute to achieving cost effectiveness. The study suggests prospective areas for further research, promoting a thorough investigation of certain methods within Supplier Development and Supplier Evaluation and Selection. Furthermore, conducting inquiries into industry-specific variables that impact these connections within the distinct setting of Bangladesh could yield useful insights. In summary, the study provides accurate and useful advice for firms dealing with the complex field of supply chain management in Bangladesh. Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488..",
      "s40": "../BJOPM.2015.2024 13/18 6 CONCLUSION The study intended to investigate the function of SRM in the changing economic environment of Bangladesh, specifically focusing on its influence on supply chain performance. During the investigation, it became evident that SRM methods have a beneficial impact on different aspects of the supply chain, notably in relation to cost effectiveness. The results highlight the crucial significance of Supplier Collaboration and the establishment of enduring Supplier Relationships. The study found strong positive associations, highlighting the interconnectedness of several components of SRM. Organizations that actively collaborate and develop long-lasting partnerships with suppliers are more likely to achieve sustained cost effectiveness in their supply chain operations. In contrast, the individual effects of Supplier Development and Supplier Evaluation and Selection on cost efficiency were not statistically significant. This suggests that the efficacy of these activities may depend on the presence of complementary tactics or an integrated strategy. The study promotes a comprehensive comprehension of SRM, going beyond individual practices, in order to fully use its potential in the context of Bangladesh. The research findings offer useful insights for professionals and policymakers in the manufacturing and service sectors of Bangladesh. They emphasize the significance of implementing a thorough SRM strategy, regarding suppliers as strategic allies, and cultivating lasting partnerships for sustained success in the supply chain. The study's practical implications provide valuable counsel for..",
      "s41": "..firms aiming to improve their supply chain performance, specifically in terms of cost effectiveness. Notwithstanding the contributions offered, it is imperative to recognize the limitations of the study. The findings may have limited generalizability due to the emphasis on certain sectors and the presence of operational complexity. Potential areas for future research involve investigating industry-specific variables that impact the success of SRM in Bangladesh, as well as conducting in-depth analysis of practices within Supplier Development and Supplier Evaluation and Selection. The study establishes a base for continued investigation of SRM in developing countries, with the goal of gaining a more detailed comprehension of its intricacies and potential advantages. REFERENCES Abbas, H. & Tong, S. (2023), \"Green Supply Chain Management Practices of Firms with Competitive Strategic Alliances\u2014A Study of the Automobile Industry\", Sustainability, Vol. 15, No. 3, pp. 2156. Abtahi, A.T., Farhana, N. & Hasan, M.M. (2023), \"A Study on the Impact of E-Commerce Adoption for Enhancing Supply Chain Efficiency in Bangladesh SMEs\", Business and Economics in Developing Countries, Vol. 1, No. 1, pp. 29-33. Adesanya, A., Yang, B., Bin Iqdar..",
      "s42": ".., F.W. & Yang, Y. (2020), \"Improving sustainability performance through supplier relationship management in the tobacco industry\", Supply Chain Management: An International Journal, Vol. 25, No. 4, pp. 413-426. Ali, S.S., Kaur, R., Ersoz, F., Altaf, B., Basu, A. & Weber, G.-W. (2020), \"Measuring carbon performance for sustainable green supply chain practices: A developing country scenario\", Central European Journal of Operations Research, Vol. 28, pp. 1389-1416. Allioui, H. & Mourdi, Y. (2023), \"Exploring the full potentials of IoT for better financial growth and stability: A comprehensive survey\", Sensors, Vol. 23, No. 19, pp. 8015. Amoako-Gyampah, K., Boakye, K.G., Adaku, E. & Famiyeh, S. (2019), \"Supplier relationship management and firm performance in developing economies: A moderated mediation analysis of flexibility capability and ownership structure\", International Journal of Production Economics, Vol. 208, pp. 160-170. Andersen, J..",
      "s43": "... (2021), \"A relational natural-resource-based view on product innovation: The influence of green product innovation and green suppliers on differentiation advantage in small manufacturing firms\", Technovation, Vol. 104, pp. 102254. Asif, M., Searcy, C. & Castka, P. (2022), \"Exploring the role of industry 4.0 in enhancing supplier audit authenticity, efficacy, and cost effectiveness\", Journal of Cleaner Production, Vol. 331, pp. 129939. Awan, U., Sroufe, R. & Kraslawski, A. (2019), \"Creativity enables sustainable development: Supplier engagement as a boundary condition for the positive effect on green innovation\", Journal of Cleaner Production, Vol. 226, pp. 172-185. Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 14/18 Barney, J. (1991), \"Firm resources and sustained competitive advantage\", Journal of Management, Vol. 17, No. 1..",
      "s44": ".., pp. 99-120. Benton Jr, W.C., Prahinski, C. & Fan, Y. (2020), \"The influence of supplier development programs on supplier performance\", International Journal of Production Economics, Vol. 230, pp. 107793. Burki, U., Glavee-Geo, R., Dahlstrom, R., Kanani, R. & Buvik, A. (2023), \"The moderating effect of market knowledge on contractual efficacy: evidence from Asian supplier--Western buyer relationships\", Asian Business & Management, pp. 1-31. Cha, K.-J. & Kim, Y.S. (2018), \"Critical success factors for mutual collaboration with suppliers in IT outsourcing industry: a case study of a top IT outsourcing company in Korea\", Enterprise Information Systems, Vol. 12, No. 1, pp. 76-95. Cocskun, S.S., Kumru, M. & Kan, N.M. (2022), \"An integrated framework for sustainable supplier development through supplier evaluation based on sustainability indicators\", Journal of Cleaner Production, Vol. 335, pp. 130287. Cole, R. & Aitken, J. (2019..",
      "s45": "..), \"Selecting suppliers for socially sustainable supply chain management: post-exchange supplier development activities as pre-selection requirements\", Production Planning & Control, Vol. 30, No. 14, pp. 1184-1202. Dash, A., Pothal, L.K. & Tripathy, S. (2018), \"Factors affecting supplier relationship management: An AHP approach\", IOP Conference Series: Materials Science and Engineering, Vol. 390, No. 1, pp. 12056. Dobos, I. & V\u00f6r\u00f6smarty, G. (2019), \"Inventory-related costs in green supplier selection problems with Data Envelopment Analysis (DEA)\", International Journal of Production Economics, Vol. 209, pp. 374-380. Dong, J.Q., Karhade, P.P., Rai, A. & Xu, S.X. (2021), \"How firms make information technology investment decisions: Toward a behavioral agency theory\", Journal of Management Information Systems, Vol. 38, No. 1, pp. 29-58. Dubey, R., Gunasekaran, A., Childe, S.J., Papadopoulos, T. & He..",
      "s46": "..lo, P. (2019), \"Supplier relationship management for circular economy: Influence of external pressures and top management commitment\", Management Decision, Vol. 57, No. 4, pp. 767-790. Ebinger, F. & Omondi, B. (2020), \"Leveraging digital approaches for transparency in sustainable supply chains: A conceptual paper\", Sustainability, Vol. 12, No. 15, pp. 6129. Eisenhardt, K.M. (1989), \"Agency theory: An assessment and review\", Academy of Management Review, Vol. 14, No. 1, pp. 57-74. Emon, M.M.H. & Khan, T. (2023), \"The Impact of Cultural Norms on Sustainable Entrepreneurship Practices in SMEs of Bangladesh\", Indonesian Journal of Innovation and Applied Sciences (IJIAS), Vol. 3, No. 3, pp. 201-209. Emon, M.M.H. & Nahid, M.H. (2023), \"Factors Affecting Sustainable E-Commerce Adoption: Empirical Evidence from Bangladeshi SME\u2019s\", Corporate Sustainable Management Journal (CSMJ), Vol. 01, No. 01, pp. 32-36...",
      "s47": "..Dispon\u00edvel em: https://csmj.com.my/csmj-01-2023-32-36/ Enz, M.G. & Lambert, D.M. (2023), \"A supply chain management framework for services\", Journal of Business Logistics, Vol. 44, No. 1, pp. 11-36. Fama, E.F. (1980), \"Agency problems and the theory of the firm\", Journal of Political Economy, Vol. 88, No. 2, pp. 288-307. Farooque, M., Zhang, A., Liu, Y. & Hartley, J.L. (2022), \"Circular supply chain management: Performance outcomes and the role of eco-industrial parks in China\", Transportation Research Part E: Logistics and Transportation Review, Vol. 157, 102596. Farooque, M., Zhang, A., Th\u00fcrer, M., Qu, T. & Huisingh, D. (2019), \"Circular supply chain management: A definition and structured literature review\", Journal of Cleaner Production, Vol. 228, pp Faruquee, M., Paulraj, A. & Irawan, C...",
      "s48": "..A. (2021), \"Strategic supplier relationships and supply chain Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 15/18 resilience: is digital transformation that precludes trust beneficial?\", International Journal of Operations & Production Management, Vol. 41, No. 7, pp. 1192\u20131219. Feizabadi, J. & Alibakhshi, S. (2022), \"Synergistic effect of cooperation and coordination to enhance the firm\u2019s supply chain adaptability and performance\", Benchmarking: An International Journal, Vol. 29, No. 1, pp. 136\u2013171. Fouji, M.H. & Hoque, I. (2021), \"Supplier internationalization through upgrading in global supply chain: Evidence from the garments industry of Bangladesh\", International Journal of Global Business and Competitiveness, Vol. 16, No. 2, pp. 116\u2013129. Gilmore, E.A. & Buhaug, H. (2021), \"Climate mitigation policies and the potential pathways..",
      "s49": "..to conflict: Outlining a research agenda\", Wiley Interdisciplinary Reviews: Climate Change, Vol. 12, No. 5, e722. Grum, B., Tsegaye, D., Tariku, Z., Gebremariam, D., Gebremicael, H., Kuhilen, T., Yemane, S., Aregawi, A., Abebe, B.A., et al. (2023), \"Applicability and Cost Implication of Labor-Based Methods for Sustainable Road Maintenance (SRM) in Developing Countries\", Advances in Civil Engineering, 2023. Gu, V.C., Zhou, B., Cao, Q. & Adams, J. (2021), \"Exploring the relationship between supplier development, big data analytics capability, and firm performance\", Annals of Operations Research, Vol. 302, pp. 151\u2013172. Herczeg, G., Akkerman, R. & Hauschild, M.Z. (2018), \"Supply chain collaboration in industrial symbiosis networks\", Journal of Cleaner Production, Vol. 171, pp. 1058\u20131067. Hoang, T.-H., Nguyen, N.P...",
      "s50": "..P., Hoang, N.-Y.N., Akbari, M., Quang, H.T. & Binh, A.D.T. (2023), \"Application of social media in supply chain 4.0 practices: a bibliometric analysis and research trends\", Operations Management Research, pp. 1\u201323. Hossain, M.Z., Rahman, M.A.U., Rahaman, K.R., Ha-Mim, N.M. & Haque, S.F. (2023), \"Investigating critical relationships among vulnerability, livelihoods, and non-migration strategies at the fishing communities in the Sundarbans\", Environment, Development and Sustainability, pp. 1\u2013 40. Huma, S., Ahmed, W. & Najmi, A. (2020), \"Understanding the impact of supply-side decisions and practices on supply risk management\", Benchmarking: An International Journal, Vol. 27, No. 5, pp. 1769\u20131792. Jensen, M.C. & Meckling, W.H. (2019), \"Theory of the firm: Managerial behavior, agency costs and ownership structure\", in Corporate Governance, pp. 77\u2013132, Gower...",
      "s51": "..Jia, M., Stevenson, M. & Hendry, L. (2023), \"A systematic literature review on sustainability-oriented supplier development\", Production Planning & Control, Vol. 34, No. 8, pp. 727\u2013747. Kannan, D. (2018), \"Role of multiple stakeholders and the critical success factor theory for the sustainable supplier selection process\", International Journal of Production Economics, Vol. 195, pp. 391\u2013418. Klingebiel, K., Leiras, A. & M\u00e1sculo, F.S. (2013), \"Challenges for Managing Complexity in Industrial and Operations Management--A point of view from ICIEOM 2013\", Brazilian Journal of Operations & Production Management, Vol. 10, No. 2, pp. 7\u201310. Kummer, S., Herold, D.M., Dobrovnik, M., Mikl, J. & Sch\u00e4fer, N. (2020), \"A systematic review of blockchain literature in logistics and supply chain management: identifying research questions and future directions\", Future Internet, Vol. 12, No. 3, 60. Lahti, T., Wincent, J. & Parida, V. (2018..",
      "s52": "..), \"A definition and theoretical review of the circular economy, value creation, and sustainable business models: where are we now and where should research move in the future?\", Sustainability, Vol. 10, No. 8, p. 2799. Larsson, J. & Larsson, L. (2020), \"Integration, application and importance of collaboration in sustainable project management\", Sustainability, Vol. 12, No. 2, p. 585. Le Jr, T. (2022), \"Supplier\u2019s Price Evaluation in the Purchasing Process\", Journal details needed. Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 16/18 Lee, H.L. & Tang, C.S. (2018), \"Socially and environmentally responsible value chain innovations: New operations management research opportunities\", Management Science, Vol. 64, No. 3, pp. 983\u2013996. Leiras, A. & Fontainha, T.C. (2019), \"Opportunities..",
      "s53": "..& Challenges for operations management--A point of view from the 2018 POMS International Conference in Rio\", Brazilian Journal of Operations & Production Management, Vol. 16, No. 3, pp. 371\u2013374. Lim, M.K., Li, Y., Wang, C. & Tseng, M.-L. (2021), \"A literature review of blockchain technology applications in supply chains: A comprehensive analysis of themes, methodologies and industries\", Computers & Industrial Engineering, Vol. 154, p. 107133. Lo, S.M., Zhang, S., Wang, Z. & Zhao, X. (2018), \"The impact of relationship quality and supplier development on green supply chain integration: A mediation and moderation analysis\", Journal of Cleaner Production, Vol. 202, pp. 524\u2013535. Macher, J.T. & Richman, B.D. (2008), \"Transaction cost economics: An assessment of empirical research in the social sciences\", Business and Politics, Vol. 10, No. 1, pp. 1\u201363. Mani, V., Gunasekaran, A. & Delgado, C. (2018), \"Enhancing supply chain performance through supplier social sustainability: An..",
      "s54": "..emerging economy perspective\", International Journal of Production Economics, Vol. 195, pp. 259\u2013272. Manuela, P., Cristina, B. & Molina-Morales, F.X. (2021), \"I need you, but do I love you? Strong ties and innovation in supplier--customer relations\", European Management Journal, Vol. 39, No. 6, pp. 790\u2013801. Milgrom, P.R., Roberts, J. & Roberts, J. (1992), \u201cEconomics, organization and management\u201d, Vol. 7, Prentice-Hall, Englewood Cliffs, NJ. Mohan, M., Nyadzayo, M.W. & Casidy, R. (2021), \"Customer identification: the missing link between relationship quality and supplier performance\", Industrial Marketing Management, Vol. 97, pp. 220\u2013232. Naghshineh, B. & Carvalho, H. (2022), \"The implications of additive manufacturing technology adoption for supply chain resilience: A systematic search and review\", International Journal of Production Economics, Vol. 247, p. 108387. Nandi, M.L., Nandi, S., Moya, H. & Kaynak, H. (..",
      "s55": "..2020), \"Blockchain technology-enabled supply chain systems and supply chain performance: a resource-based view\", Supply Chain Management: An International Journal, Vol. 25, No. 6, pp. 841\u2013862. Oduro, S., Nyarku, K.M. & Gbadeyan, R.A. (2020), \"Supplier relationship management and organizational performance of hospitals in an emerging economy context: a comparative study\", Journal of Modelling in Management, Vol. 15, No. 4, pp. 1451\u20131478. Panfilova, E., Dzenzeliuk, N., Domnina, O., Morgunova, N. & Zatsarinnaya, E. (2020), \"The impact of cost allocation on key decisions of supply chain participants\", International Journal of Supply Chain Management, Vol. 9, No. 1, pp. 552\u2013558. Paparoidamis, N.G., Katsikeas, C.S. & Chumpitaz, R. (2019), \"The role of supplier performance in building customer trust and loyalty: A cross-country examination\", Industrial Marketing Management, Vol. 78, pp. 183\u2013197. Pereira..",
      "s56": ".., G.I., Niesten, E. & Pinkse, J. (2022), \"Sustainable energy systems in the making: A study on business model adaptation in incumbent utilities\", Technological Forecasting and Social Change, Vol. 174, p. 121207. Prosser, L., Lane, E.T. & Jones, R. (2021), \"Collaboration for innovative routes to market: COVID-19 and the food system\", Agricultural Systems, Vol. 188, p. 103038. Queiroz, M.M. & Wamba, S.F. (2019), \"Blockchain adoption challenges in supply chain: An empirical investigation of the main drivers in India and the USA\", International Journal of Information Management, Vol. 46, pp. 70\u201382. Rejeb, A., Keogh, J.G., Simske, S.J., Stafford, T. & Treiblmaier, H. (2021), \"Potentials of blockchain technologies for supply chain collaboration: a conceptual framework\", The International Journal Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol...",
      "s57": "..21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 17/18 of Logistics Management, Vol. 32, No. 3, pp. 973\u2013994. Reu\u00df, M., Grube, T., Robinius, M. & Stolten, D. (2019), \"A hydrogen supply chain with spatial resolution: Comparative analysis of infrastructure technologies in Germany\", Applied Energy, Vol. 247, p. 438\u2013453. Rezaei Vandchali, H., Cahoon, S. & Chen, S.-L. (2020), \"Creating a sustainable supply chain network by adopting relationship management strategies\", Journal of Business-to-Business Marketing, Vol. 27, No. 2, pp. 125\u2013149. Rindfleisch, A. (2020), \"Transaction cost theory: past, present and future\", AMS Review, Vol. 10, Nos. 1\u20132, pp. 85\u201397. Saad, N.A., Elgazzar, S. & Kac, S.M. (2022), \"Linking supply chain management practices to customer relationship management objectives: a proposed framework\",..",
      "s58": "..Business: Theory and Practice, Vol. 23, No. 1, pp. 154\u2013164. Saghiri, S.S. & Mirzabeiki, V. (2021), \"Buyer-led environmental supplier development: Can suppliers really help it?\", International Journal of Production Economics, Vol. 233, p. 107969. Schmidt, C.G. & Wagner, S.M. (2019), \"Blockchain and supply chain relations: A transaction cost theory perspective\", Journal of Purchasing and Supply Management, Vol. 25, No. 4, p. 100552. Shafiq, A., Johnson, P.F. & Klassen, R.D. (2022), \"Building synergies between operations culture, operational routines, and supplier monitoring: implications for buyer performance\", International Journal of Operations & Production Management, Vol. 42, No. 5, pp. 687\u2013712. Shakeel, R., Sajjad, H. & Ramish, A. (2018), \"Towards A Third Party Logistics (3PL) Based Sustainable Supplier Relationship Management Framework\", Journal of Quality and Technology Management, Vol. 15, No. 2, pp. 1\u201336. Sharma, A., Adhikary, A...",
      "s59": "..& Borah, S.B. (2020), \"Covid-19's impact on supply chain decisions: Strategic insights from NASDAQ 100 firms using Twitter data\", Journal of Business Research, Vol. 117, pp. 443\u2013449. Shibin, K.T., Dubey, R., Gunasekaran, A., Hazen, B., Roubaud, D., Gupta, S. & Foropon, C. (2020), \"Examining sustainable supply chain management of SMEs using resource based view and institutional theory\", Annals of Operations Research, Vol. 290, pp. 301\u2013326. Sikombe, S. & Phiri, M.A. (2019), \"Exploring tacit knowledge transfer and innovation capabilities within the buyer--supplier collaboration: A literature review\", Cogent Business & Management, Vol. 6, No. 1, p. 1683130. Songsom, N., Nilsook, P., Wannapiroon, P., Fung, L.C.C. & Wong, K. (2019), \"System architecture of a student relationship management system using Internet of Things to collect Digital Footprint of Higher Education Institutions\", International Journal of Emerging Technologies in Learning..",
      "s60": "..(IJET), Vol. 14, No. 23, pp. 125\u2013140. Stek, K. & Schiele, H. (2021), \"How to train supply managers--necessary and sufficient purchasing skills leading to success\", Journal of Purchasing and Supply Management, Vol. 27, No. 4, p. 100700. Stone, A. (1986), \"The Economic Institutions of Capitalism: Firms, Markets, Relational Contracting. By Oliver E. Williamson. (New York: Free Press, 1985. Pp. 450. $27.95.)\", American Political Science Review, Vol. 80, No. 4, pp. 1424\u20131425. Swallehe, O. (2021), \"Analysis of Challenges Facing SMEs in Implementing Suppliers Relationship\", SSRN. Nota: Detalhes adicionais sobre publica\u00e7\u00e3o s\u00e3o necess\u00e1rios para uma refer\u00eancia completa. Tai, P.D., Anderson, M.R., Hien Duc, T.T., Thai, T.Q. & Yuan, X.-M. (2022), \"Strategic information sharing in supply chain with value-perceived consumers\", Industrial Management & Data Systems, Vol. 122, No..",
      "s61": "... 4, pp. 841\u2013863. Tay, H.L. & Aw, H. Sen. (2021), \"Improving logistics supplier selection process using lean six sigma-- an action research case study\", Journal of Global Operations and Strategic Sourcing, Vol. 14, No. 2, pp. 336\u2013359. Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 18/18 Tseng, S.-M. (2020), \"The Impacts Of Social Media Adoption And SRM Relational Information Processes On Supply Chain Agility\", Journal of Information, Technology and Society. Uddin, M.B., Fu, Y. & Akhter, B. (2020), \"Inter-organizational cost management: effects of antecedents and methods in a hybrid relational context\", Journal of Business & Industrial Marketing, Vol. 35, No. 5, pp. 909\u2013923. Wang, C.-N., Yang, F.-C., Vo, T.M.N...",
      "s62": ".., Nguyen, V.T.T. & Singh, M. (2023), \"Enhancing Efficiency and Cost-Effectiveness: A Groundbreaking Bi-Algorithm MCDM Approach\", Applied Sciences, Vol. 13, No. 16, p. 9105. Wernerfelt, B. (1984), \"A resource-based view of the firm\", Strategic Management Journal, Vol. 5, No. 2, pp. 171\u2013180. Wren, B. (2022), \"Sustainable supply chain management in the fast fashion Industry: A comparative study of current efforts and best practices to address the climate crisis\", Cleaner Logistics and Supply Chain, Vol. 4, p. 100032. Wynstra, F., Suurmond, R. & Nullmeier, F. (2019), \"Purchasing and supply management as a multidisciplinary research field: Unity in diversity?\", Journal of Purchasing and Supply Management, Vol. 25, No. 5, p. 100578. Yang, X. (2022), \"Vertical Coopetition: Effect of Supplier Relationship Management Strategies on Supplier Involvement in New Product Development\", IEEE Transactions on Engineering Management. Zhang, A., Wang, J.X., Faroharvar..",
      "s63": "..oque, M., Wang, Y. & Choi, T.-M. (2021), \"Multi-dimensional circular supply chain management: A comparative review of the state-of-the-art practices and research\", Transportation Research Part E: Logistics and Transportation Review, Vol. 155, p. 102509. Author contributions: MMHE: report writing, data analysis, and revisions of the manuscript for submission; TK: supervision, planning, and revisions of the manuscript for submission; SAJS: organizing the manuscript and data collection activities."
    },
    "question_generated": "How does the integration of Supplier Relationship Management practices, such as collaboration, development, evaluation, and long-term partnerships, influence supply chain performance in emerging economies, and what hypothesis can be drawn from existing literature regarding their collective impact?",
    "golden_answer_generated": "# Supplier Relationship Management and Supply Chain Performance\n\n## Introduction\nSupplier Relationship Management (SRM) is a critical component of modern supply chain management, emphasizing the development of dynamic partnerships between businesses and their suppliers\u3010s7\u3011. SRM extends beyond transactional interactions, fostering strategic collaboration, innovation, and value generation\u3010s8\u3011. In the context of Bangladesh, an emerging economy with a rapidly growing industrial sector, understanding SRM's impact on supply chain performance is crucial\u3010s3\u3011.\n\n## Literature Review\n### Supplier Collaboration\nCollaboration with suppliers is fundamental to achieving success in today's competitive supply chain environment\u3010s9\u3011. Studies have shown that effective collaboration enhances product quality, ensures timely delivery, and improves customer satisfaction\u3010s9\u3011\u3010s10\u3011. Technological advancements have further facilitated collaboration by enabling seamless communication and information sharing\u3010s9\u3011.\n\n### Supplier Development\nSupplier development activities, such as training programs and process improvements, are vital for enhancing supplier performance\u3010s11\u3011. Successful development initiatives lead to cost-effectiveness and increased efficiency in the supply chain\u3010s11\u3011\u3010s12\u3011. However, the specific mechanisms through which supplier development impacts performance require further exploration\u3010s11\u3011.\n\n### Supplier Evaluation and Selection\nEffective supplier evaluation and selection are crucial for optimizing supply chain efficiency\u3010s12\u3011. Proficient SRM practices positively influence supplier evaluation, leading to improvements in cost efficiency and product quality\u3010s12\u3011\u3010s13\u3011. The criteria used in evaluation processes are closely linked to various aspects of supply chain performance\u3010s12\u3011.\n\n### Long-Term Supplier Relationships\nLong-term relationships with suppliers significantly impact supply chain performance\u3010s13\u3011. These relationships foster trust, collaboration, and shared understanding, contributing to operational efficiency and cost-effectiveness\u3010s13\u3011\u3010s14\u3011. The stability provided by long-term partnerships is essential for achieving sustainable supply chain operations\u3010s13\u3011.\n\n## Analysis\nThe synthesis of existing literature highlights the interconnectedness of SRM practices and their collective impact on supply chain performance\u3010s14\u3011\u3010s15\u3011. Supplier collaboration and long-term relationships emerge as particularly influential factors, driving cost efficiency and enhancing overall supply chain effectiveness\u3010s14\u3011\u3010s15\u3011. The integration of supplier development and evaluation processes further strengthens these relationships, underscoring the importance of a comprehensive SRM strategy\u3010s15\u3011.\n\n## Conclusion\nBased on the synthesis of existing literature, it is hypothesized that implementing a holistic SRM approach, emphasizing collaboration, development, evaluation, and long-term relationships, will significantly enhance supply chain performance in emerging economies like Bangladesh\u3010s15\u3011\u3010s16\u3011. This hypothesis builds upon the established understanding of SRM's role in fostering efficient and effective supply chain operations\u3010s16\u3011.",
    "used_snippets_with_context": {
      "s2": "..How to cite: Emon, M. M. H., Khan, T. and Siam, S. A. J. (2024), \u201cQuantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors\u201d, Brazilian Journal of Operations and Production Management, Vol. 21, No. 2, e20242015. https://doi.org/10.14488/BJOPM.2015.2024 Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 2/18 robust and mutually advantageous relationships with its suppliers. Supplier Relationship Management (SRM) is widely recognized as a crucial method for achieving this goal, as evidenced by numerous studies conducted in well-established industrial settings, such as the United States (Farooque et al., 2022; Zhang et al., 2021) and Germany (Reu et al., 2019). These studies, conducted on a worldwide scale, have highlighted the positive influence of proficient SRM strategies on..",
      "s3": "..various aspects of supply chain performance, including supplier performance, cost effectiveness, product quality, and delivery punctuality. As global industries continue to evolve, emerging economies like Bangladesh have become prominent participants in the international marketplace. In the specific context of Bangladesh, a country experiencing rapid economic growth and a flourishing industrial sector, the significance of efficient SRM cannot be emphasized enough. In recent years, Bangladesh has demonstrated significant progress in establishing itself as a prominent participant in the worldwide supply chain, with a particular focus on industries such textiles, ready-made clothing, and electronics (Fouji & Hoque, 2021). Due to its expanding industrial sector and extensive supplier network, the nation has emerged as an appealing destination for foreign enterprises in search of economically efficient manufacturing and sourcing alternatives. The observed evolution serves as evidence of Bangladesh's capacity inside the international marketplace. Nevertheless, it is crucial to examine the impact of SRM techniques on the performance of supply chains functioning in Bangladesh, as the country takes on its position in the global supply chain. Although there exists a considerable body of literature on SRM, a significant proportion of the study has mostly concentrated on industrialized economies. Consequently, there is a notable knowledge gap about the application and effectiveness of SRM in emerging economies such as Bangladesh. The distinctive environment of Bangladesh, characterized by its..",
      "s4": "..economic, cultural, and infrastructural intricacies, warrants a focused examination (Hossain et al., 2023). The primary objective of the present study is to fill the existing research void by quantitatively examining the impact of SRM on enhancing supply chain performance in the context of Bangladesh. This investigation will build upon the knowledge gained from prior research conducted on a worldwide scale. For instance, research conducted in the United States by Farooque et al., (2022) and in Germany by Reu\u00df et al., (2019) emphasized the favorable influence of proficient SRM strategies on various aspects, including supplier performance, cost effectiveness, product quality, and delivery punctuality. The conducted research, carried out in established industrial contexts, has yielded significant reference points for SRM procedures. Nevertheless, the specific economic, cultural, and infrastructural complexities of Bangladesh give rise to a separate framework that could potentially impact the dynamics of SRM in a distinct manner. The current study seeks to fill this void by particularly examining the Bangladeshi context, so adding a nuanced viewpoint to the existing pool of information. Through the analysis of SRM in an emerging country, this study aims to reveal unique perspectives that may diverge from findings in research undertaken in industrialized countries. A comprehensive comprehension of how SRM methods affect performance metrics is essential due to the unique difficulties and..",
      "s6": "..(Hoang et al., 2023). As Bangladesh strives to establish itself as a significant participant in the global supply chain, it is imperative to thoroughly investigate the suitability and efficacy of SRM approaches in this distinct setting, as shown by previous study findings. Therefore, the present study aims to expand upon the existing body of research by directing its attention towards the supply chains of Bangladesh. This study seeks to give significant insights for practitioners and policymakers acting within the Bangladeshi context by examining the correlation between SRM techniques and key supply chain performance indicators, including Supplier Collaboration, Supplier Development, Supplier Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 3/18 Evaluation and Selection, Long-Term Supplier Relationships. Previous research has provided valuable insights into the impact of SRM on supply chain performance at a global level. However, this study seeks to expand this knowledge by examining the specific context of Bangladesh. By doing so, it aims to contribute to a more comprehensive understanding of how SRM influences supply chain dynamics in emerging economies. Despite the increasing significance of SRM..",
      "s7": ".., there exists a dearth of empirical studies examining its influence on supply chain performance within the context of Bangladesh. The objective of this study is to address the existing research gap by investigating the precise connections between SRM methods and measures of supply chain performance. The value of this study is derived from its contribution to the existing body of knowledge on SRM specifically within the setting of Bangladesh. It contributes to the understanding of the advantages that may be gained by implementing successful SRM tactics. The primary objective of this study is to examine the manufacturing and service sectors in Bangladesh. The research will span enterprises of diverse sizes and operational complexities in order to conduct a thorough evaluation of SRM techniques and their influence on the performance of supply chains. 2 EMPIRICAL LITERATURE AND HYPOTHESIS DEVELOPMENT 2.1 Supplier relationship management and supply chain performance SRM is a critical aspect of modern supply chain management (Adesanya et al., 2020). It involves developing dynamic partnerships between businesses and their suppliers (Sharma et al., 2020). SRM focuses on creating lasting and mutually beneficial relationships beyond mere transactions (Enz & Lambert, 2023). In today's competitive landscape, strong supplier relationships are vital for success (Amoako-Gyampah et al., 2019). SRM goes beyond..",
      "s8": "..transactions, encompassing strategic collaboration, innovation, and value generation (Pereira et al., 2022). This review examines SRM's impact on supply chain performance, including cost efficiency, product quality, on-time delivery, and customer satisfaction. SRM has evolved from transactional interactions to strategic alliances (Abbas & Tong, 2023). Recognizing suppliers as essential partners shifted the focus to collaboration and long-term partnerships. Technological advancements improved SRM's efficiency, with digital platforms enhancing communication (Emon & Nahid, 2023; Tseng, 2020). Strong relationships with suppliers stimulate innovation and value creation (Lee & Tang, 2018). SRM now encompasses collaborative partnerships, risk management, supplier development, and sustainability (Emon & Khan, 2023; Huma et al., 2020). Empirical studies show that proficient SRM positively impacts supplier performance, cost-effectiveness, product quality, and delivery punctuality (Le Jr, 2022). Collaboration, information exchange, and trust development are key foundations of successful SRM (Cha & Kim, 2018). Supplier development activities enhance supplier performance (Mani et al., 2018). 2.2 Supplier Collaboration and Supply Chain Performance: SRM highlights the importance of strategic collaboration as a fundamental element for achieving success in the current dynamic and competitive supply chain environment..",
      "s9": "..(Oduro et al., 2020). There is a large body of literature that strongly supports the notion that successful collaboration with suppliers plays a crucial role in improving the performance of the supply chain. This section examines the current studies on how supplier collaboration affects several aspects of supply chain performance and builds upon the literature presented by Oduro et al., (2020). Collaborative relationships in SRM go beyond standard transactional methods and play a crucial role in promoting innovation, enhancing product quality, assuring timely delivery, and ultimately improving customer happiness (Abtahi et al., 2023; Stek & Schiele, 2021). The development of SRM has experienced a significant change from transactional interactions to the formation of strategic partnerships, emphasizing the crucial role of collaboration in creating and sustaining long-term relationships with suppliers (Yang, 2022). The growing interdependence and global integration of supply chains has underscored the importance of strong collaborative methods. Technological improvements are crucial in enabling and enhancing collaborative efforts in SRM. Digital platforms have become crucial instruments that not only facilitate communication but also improve the sharing of information between organizations and their suppliers (Ebinger & Omondi, 2020). These platforms facilitate instantaneous data exchange, which is essential for efficient collaboration, especially in a multinational supply chain setting where prompt information is..",
      "s10": "..vital. Although (Leiras & Fontainha, 2019; Oduro et al., 2020) have shed light on the beneficial effects of supplier collaboration on supply chain performance, there may still be gaps and unexplored areas in the existing literature. The purpose of this part is to identify the gaps in the current understanding of how supplier collaboration affects supply chain performance. It also highlights Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 4/18 the specific contributions of this study in addressing and enhancing the existing knowledge in this area. This study seeks to further the knowledge of the complex relationship between collaborative practices and supply chain performance by combining additional literature that addresses subtle aspects of supplier collaboration. 2.3 Supplier Development and Supply Chain Performance Supplier development activities are acknowledged as key elements in the larger context of SRM, serving a vital function in improving supplier performance and, subsequently, overall supply chain efficiency (Dubey et al., 2019). This part performs a thorough examination of the current body of research to investigate the complex connection between supplier..",
      "s11": "..development and supply chain performance, expanding upon the knowledge presented by Dubey et al., (2019). Supplier development goes beyond conventional transactional methods and encompasses a range of actions aimed at improving the talents and performance of suppliers (Awan et al., 2019). Activities may encompass training programs, streamlining processes, embracing technology, and engaging in collaborative projects with the goal of promoting innovation and ongoing enhancement. According to the literature, supplier development programs that are successful contribute to better supplier performance, which in turn leads to cost-effectiveness and increased efficiency in the supply chain (Asif et al., 2022). Although the current body of literature offers significant insights into the favorable relationship between supplier development and supply chain performance, there are still intricacies and unique mechanisms that warrant further investigation. Gaining insight into the impact of different elements of supplier development on various facets of supply chain performance is essential for formulating focused and efficient strategies. This section seeks to highlight any existing gaps in the current knowledge and clarify how the present study helps to filling these gaps by offering a detailed perspective on the complex connection between supplier development activities and the overall performance of the supply chain. 2.4 Supplier Evaluation and Supply Chain Performance: Effective SRM relies on the use of strong supplier assessment methods, which are essential for achieving optimal..",
      "s12": "..supply chain efficiency (Amoako-Gyampah et al., 2019; Klingebiel et al., 2013). This section provides a comprehensive analysis of the current research on the complex connection between supplier evaluation and the overall performance of the supply chain. It expands on the fundamental findings presented by Amoako-Gyampah et al., (2019). Supplier evaluation in the context of SRM entails a thorough examination of supplier performance, cost efficiency, product quality, and delivery timeliness (Lim et al., 2021). Proficient SRM has been found to have a favorable impact on supplier evaluation, leading to improvements in several aspects of supply chain performance, as indicated by the existing literature. However, there may be gaps in our present understanding of how the precise criteria used in supplier assessment processes are closely connected to various aspects of supply chain performance. The purpose of this section is to identify and explain the gaps in the current knowledge, by conducting a detailed analysis of the relationship between different criteria used to evaluate suppliers and various aspects of supply chain performance. Through this approach, it aims to enhance and improve the theoretical foundation that supports the connection between supplier evaluation methods and the overall efficacy of the supply chain. 2.5 Long-Term Supplier Relationship and Supply Chain Performance: Long..",
      "s13": "..-term supplier connections play a crucial role in SRM, exerting substantial impact on supply chain performance (Oduro et al., 2020). This part provides a thorough examination of the current research, exploring the complex connection between long-term relationships with suppliers and the various factors that affect supply chain effectiveness. The fundamental knowledge presented by Shakeel et al., (2018) acts as a crucial basis for this investigation. Organizations have recognized the importance of suppliers as crucial partners and have therefore made it a strategic priority to develop long-term relationships. This involves focusing on collaboration and mutual benefits under the SRM framework (Kannan, 2018). Empirical research confirm that long-term supplier relationships have a beneficial effect on different measures of supply chain performance. However, there may be gaps in our current understanding regarding the detailed mechanisms by which these linkages contribute to the complex fabric of supply chain performance. This section aims to thoroughly identify any potential gaps in the current literature, with the goal of clarifying how the present study enhances the understanding of the relationship between long-term supplier relationships and various aspects of supply chain performance. In order to improve the scope and Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production..",
      "s14": "..Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 5/18 comprehensiveness of this analysis, it will be crucial to include contemporary research and a variety of viewpoints on long-term supplier partnerships. This section aims to add to both theoretical refinement and practical insights for businesses managing long-term supplier relationships within the broader framework of SRM. 2.6 Theoretical Frameworks in SRM Understanding Supplier Relationship Management's (SRM) theoretical foundations and evolution is essential to grasp its dynamics. Originally transactional, SRM has evolved into a strategic approach focusing on long-term value (Dash et al., 2018). Theoretical dimensions include transaction cost minimization, resource leverage, incentive alignment, and risk management (Rejeb et al., 2021). The evolution of Transaction Cost Economics (TCE), Resource-Based View (RBV), and Agency Theory has mirrored SRM's transformation. Transaction Cost Economics (TCE) emphasizes minimizing transaction costs in SRM (Swallehe, 2021). Resource-Based View (RBV) highlights supplier relationships as sources of competitive advantage (Andersen, 2021). Agency Theory addresses agency issues within supplier relationships (Dubey..",
      "s15": "..et al., 2019). These theories intersect in SRM to reduce costs, leverage resources, align incentives, and manage risks (Gilmore & Buhaug, 2021). Depending upon the theoretical underpinnings elucidated in the comprehensive examination of existing literature, the present section delineates the conceptual framework that will guide the investigation. The theoretical framework selected for this study is the Resource-Based View (RBV), which asserts that businesses can attain enduring competitive advantage via the strategic utilization of distinctive resources and skills. across the scope of this study, the Resource-Based View (RBV) framework offers a perspective that enables the examination of how SRM methods impact a particular outcome variable, namely cost efficiency, across supply chains operating in Bangladesh. The Resource-Based View (RBV) theory places significant emphasis on the significance of supplier relationships as important sources of resources and capabilities that ultimately promote cost effectiveness within supply chains. The resources and capabilities obtained through SRM techniques are anticipated to have a favorable impact on cost efficiency. This is demonstrated by the potential to attain cost reductions, cost savings, and cost-effectiveness in supply chain activities. 2.7 Research Hypotheses & Conceptual Framework Supplier collaboration: The implementation of supplier collaboration, which involves engaging in cooperative activities and sharing knowledge with suppliers, has been demonstrated..",
      "s16": "..to improve Table 1 - Theoretical Frameworks in SRM Theoretical Framework Key Concepts and Relevance Sources Transaction Cost Economics - Minimizing transaction costs in supplier relationships. (Rindfleisch, 2020; Schmidt & Wagner, 2019; Stone, 1986) - Aligning SRM practices with transaction-specific characteristics. (Macher & Richman, 2008; Saad et al., 2022; Wynstra et al., 2019) Resource-Based View - Leveraging supplier relationships as sources of competitive advantage. (Barney, 1991; Nandi et al., 2020; Shibin et al., 2020) - Recognizing supplier-specific assets, knowledge, and capabilities. (Burki et al., 2023; Saghiri & Mirzabeiki, 2021; Wernerfelt, 1984) Agency Theory - Addressing agency problems and information asymmetry in SRM. (Dong et al., 2021; Jensen & Meckling, 2019; Kummer et al., 2020) - Designing SRM practices to align incentives between organizations and suppliers. (Ali et al., 2020; Dubey et al., 2019; Eisenhardt, 1989) - Monitoring and control mechanisms in SRM relationships. (Fama, 1980; Songsom et al., 2019) - The role of contracts and performance-based incentives in SRM. (..",
      "s17": "..Grum et al., 2023; Milgrom et al., 1992) Quantifying the influence of supplier relationship management and supply chain performance: an investigation of Bangladesh\u2019s manufacturing and service sectors Brazilian Journal of Operations and Production Management, Vol. 21, No. 2 e20242015| https://doi.org/10.14488/BJOPM.2015.2024 6/18 the operational efficiency of supply chain processes. Collaboration of this nature has the potential to enhance communication, optimize procedures, and enhance coordination, hence yielding cost efficiencies. Prior studies have provided evidence that the establishment of successful collaboration with suppliers has a beneficial influence on cost efficiency (Cole & Aitken, 2019; Paparoidamis et al., 2019), thus confirming the premise of a positive association. H0: Supplier collaboration has no significant influence on cost efficiency in the supply chains of Bangladeshi organizations. H1: Supplier collaboration has a significant positive influence on cost efficiency in the supply chains of Bangladeshi organizations. Supplier development: Supplier development programs, encompassing activities such as training and knowledge transfer, possess the capacity to augment the capabilities of suppliers, hence resulting in enhancements to processes and reductions in costs. Previous studies have demonstrated that supplier development initiatives have a favorable impact on cost effectiveness (Awan.."
    },
    "eval_answer": "**Impact of Supplier Relationship Management (SRM) Practices on Supply Chain Performance in Emerging Economies**\n\n**Key SRM Practices:**  \n- **Collaboration:** Effective collaboration among suppliers and buyers is essential for enhancing supply chain performance, particularly in emerging markets where informal governance mechanisms, such as trust and social networks, play a critical role [S7832532872].  \n- **Development:** Continuous development of supplier capabilities through mutual learning and goal alignment significantly influences product co-development, thereby improving supply chain efficiency [S3968850946].  \n- **Evaluation:** The evaluation of supplier performance, particularly through information sharing, helps mitigate role hazards characterized by ambiguity and conflict, which can enhance overall supply chain performance [S8330573727].  \n- **Long-term Partnerships:** Establishing long-term partnerships allows firms to navigate contractual inefficiencies and leverage political and business ties, which can provide legitimacy and access to necessary resources [S6950273977].  \n\n**Mechanisms Enhancing Performance:**  \n- **Social Management Capabilities:** The development of social management capabilities is crucial for multinational buyers and local suppliers to effectively respond to stakeholder pressures, contributing to improved supply chain performance [S7052466317].  \n- **Relational Coordination:** In emerging economies, relational coordination fostered by trust and social networks enhances supply chain performance by compensating for the lack of formal legal institutions [S7832532872].  \n\n**Conclusion:**  \nThe integration of SRM practices such as collaboration, development, evaluation, and long-term partnerships significantly influences supply chain performance in emerging economies. These practices not only enhance efficiency but also address the unique challenges posed by the institutional environment in these markets. The existing literature supports a hypothesis that effective SRM practices collectively lead to improved supply chain outcomes in developing contexts.",
    "eval_references": {
      "S6950273977": {
        "id": "S6950273977",
        "text": "Contractual inefficiencies in emerging markets lead firms to seek political and business ties as alternative governance strategies, which can enhance supply chain performance by providing legitimacy and access to resources.",
        "children": [
          {
            "id": "E3918592038",
            "text": "As multinational enterprises (MNEs) increasingly source from suppliers in emerging markets, a particular challenge is how to manage their relationships with suppliers within potentially deleterious institutional environments. A large body of research has investigated supply chain management from multiple theoretical viewpoints, mostly from transaction cost economics and relational exchange perspective (Carey et al., 2011). From these perspectives, companies can use both formal governance mechanisms such as contracts, control, and monitoring as well as informal governance mechanisms such as trust, socialization, and relational norms to manage the relationships with their suppliers (Cao and Lumineau, 2015; Li et al., 2010; Villena et al., 2011). However, most extant studies on supply chain management have overlooked the role of institutional environments in which companies cooperate with their suppliers (Zhou et al., 2014). As emerging markets have experienced massive changes in their economic, social, and political institutions, institutional characteristics greatly shape the efficacy of supply chain management strategies. For example, signing a detailed contract may not be helpful in emerging markets where legal institutions are yet to be developed. Transaction cost economics was originated from institutional contexts with a long tradition of contract laws and strong legal regimes. Yet such strong legal institutions do not automatically appear in emerging markets (Handley and Angst, 2015). Indeed, the lack of strong legal institutions represents one of the most severe challenges for businesses operating in emerging markets (Peng, 2003). Related, the instability of political regime also challenges supply chain management practices in emerging markets. When strong political and legal institutions are absent, informal institutions based on personal connections and social network play a pivotal role, because social networks provide access to scarce resources and offer legitimacy status (Yang et al., 2012). However, the inherent restraint of social networks is the limited number of personal ties that an individual can possess. Because personal relationships are difficult to build and costly to maintain, they may not be able to catch up with rapid growing exchange scale in emerging markets (Peng, 2003). So how could companies manage their relationships with suppliers in emerging markets, which are characterized by political, legal, and social challenges? To address this fascinating issue, this special issue serves as an exemplar for integrating the unique characteristics of emerging economies in conceptual development and empirical assessment. What makes emerging economies a unique context to study supply chain management? Emerging markets are \u201clow-income, rapid growth countries using economic liberalization as their primary engine of growth\u201d (Hoskisson et al., 2000), where institutions are undergoing fundamental and continuous changes. The most unique features that seriously challenge effective supply chain management are institutional voids, including shallow capital markets, deficient legal systems, lack of independent accounting intermediaries, and etc. (Khanna and Palepu, 1997). In large emerging markets, institutional voids also influence regional economic development, leading to subnational variations within the same country (Zhou and Poppo, 2010). Accordingly, companies must develop alternative operations management strategies and adjust their strategic decisions to overcome institutional voids. The call for papers generated 50 submissions, among which six manuscripts were finally accepted after three or four rounds of review. These six papers include survey research, case studies, and archival data research, and use samples of Chinese firms, Indian companies, and American firms that outsource in emerging markets. They also address explicitly the methodological issues (e.g., endogeneity and common method bias) indicated by the recent JOM Editorial (Guide and Ketokivi, 2015). How to deal with role hazard between buyers and suppliers caused by sub-national institutional distance? How do social management capabilities help multinational buyers and their emerging market suppliers respond to stakeholder pressures, address regulatory gaps, and improve social performance? How does institutional deficiency affect contractual inefficiency and consequently ties utilization of emerging market firms? How do outsourcing, in-house offshoring, and sales to emerging markets affect product recalls and inventory performance? How do buyer-supplier compatibility and institutional environment affect product co-development between buyers and suppliers? How to reduce local supplier opportunism in China when facing regulatory uncertainty? Collectively, these papers provide fresh insights into how institutional environments in emerging markets affect operation management strategies and how companies develop supply chain management strategies to deal with institutional voids. In Dong, Ju, and Fang's article, the authors introduce an important notion, role hazard, which represents a critical yet understudied relational coordination problem. Their study shows that subnational institutional distance leads to role ambiguity and conflict, two important facets of role hazards between buyers and suppliers, which in turn jeopardize supply chain performance. To mitigate such problem, supply chain partners could share information and adapt to the changing environments continuously. These findings provide a novel perspective on how supply chain partners foster relational coordination by reducing role hazards caused by subnational institutional distance in emerging markets. Huq, Chowdhury, and Klassen use multiple case studies from before and after the Rana Plaza building collapse in Bangladesh to develop an understanding of the skills, practices, relationships and processes (collectively referred as social management capabilities) that help firms in emerging markets to improve their performance on human safety & welfare, and social & community development. Specifically, they attempt to address two research questions: What are the social management capabilities needed by multinational buyers and their emerging market suppliers to improve social performance and respond to stakeholder pressures? How do external factors and shocks affect the development and evolution of these capabilities in emerging markets? Their findings provide novel insights on how multinational buyers and local suppliers achieve social performance in emerging markets. Shou, Zheng, and Zhu address another critical issue in emerging markets: contractual ineffectiveness, the difficulty of using contracts to safeguard and coordinate transactions between supply chain partners. The authors show that legal enforceability and information transparency lead to contractual inefficiency, which in turn promotes a firm's efforts to seek political and business ties. They also find that efficiency pressure and equity pressures differentially moderate the relationships between contractual inefficiency and pursuit of social ties. These results provide important implications on the alternative and dynamic use of contracts and social ties in supply chain management in emerging markets. Steven and Britto study three different forms of emerging market penetration (i.e., outsourcing, in-house offshoring, and sales to emerging markets) and their associations with product recalls. They find that outsourcing penetration increases recalls whereas sales penetration reduces recalls. Although they cannot find a direct relationship between in-house offshoring and recalls, they show that in-house offshoring is able to mitigate the positive relationship between outsourcing and recalls. They further investigate how emerging markets' characteristics (e.g., institutional immaturity) moderate these relationships. Their results provide a finer understanding of the complex relationships among different forms of emerging market penetration and product recalls. Wang, Li, and Chang focus on how buyer-supplier compatibility influences product co-development between buyers and suppliers in China. They find that while knowledge commonality has an inverted U-shaped relationship with product co-development, goal compatibility has a positive effect on product co-development. Moreover, mutual learning partially mediates the effects of buyer-supplier compatibility on product co-development. They further consider how formal and informal institutional environments (i.e., government intervention and guanxi importance) moderate the relationship between mutual learning and product co-development differently. Their research provides important theoretical and managerial implications for buyer-supplier collaboration in emerging markets. Wang, Zhang, Wang, and Sheng examine contracts and trust as two alternative governance modes in curtailing opportunism in China. They argue that the effects of these two governance strategies in reducing opportunism depend on two institution-related factors: regulatory uncertainty and relationship structure in the Chinese markets. The authors show that contracts are more effective in deterring supplier opportunism when regulatory uncertainty is high. In addition, contracts help curtail opportunism more in domestic, compared with international, buyer\u2013supplier relationships, whereas trust is more effective in restricting supplier opportunism in international relationships than in domestic ones. Taken together, these articles indicate that institutional environments play a critical role in supply chain management in emerging markets. Equally important, firms in emerging market can develop their own operations management strategies and capabilities to overcome institutional voids and enhance supply chain performance. These articles illustrate the fundamental challenges brought by institutional voids and provide fresh perspectives on effective supply chain management in emerging markets. Whereas these articles advance this line of research significantly, they only represent an initial effort to address a highly complicated issue. Looking forward, we would like to encourage more research to take the unique characteristics of emerging markets into consideration. Among the six papers in this special issue, two consider the overall institutional environments such as subnational institutional distance and institutional immaturity, the other three examine legal institutions such as regulatory gaps, legal enforceability, and regulatory uncertainty, whereas only one touches upon social institutions (i.e., guanxi importance). However, institutional environments include a variety of formal and informal factors. Formal factors consist of property rights protection, government interference, policy changes, etc., and informal factors include ideology, cultural values and norms. For example, the lack of intellectual property protection causes serious problems for supply chain management in emerging markets. How could multinational buyers outsource the manufacturing to local suppliers and at the same time protect themselves from illegal copying? Also, emerging market governments tend to interfere with market operations heavily. How would such government intervention affect operations management? How could emerging markets firms take advantage of such government intervention to foster their supply chain performance? How would policy changes affect supply chain management? Inadequate institutional arrangements such as weak legal systems and regulations in emerging markets render firms a disadvantageous position in which they lack the institutional assurance for their decisions and operations. However, informal institutions such as cultural values and norms may complement and/or substitute the formal institutions in providing a regulated business environment in emerging markets. For example, social ties are shown to enhance performance because of their cultural legitimacy and resource implications in China. How could emerging market firms maneuver the cultural values to form cohesive social ties for long-term cooperation? How could firms take advantage of social ties to form cooperative business circles to enhance governance effectiveness? Given the cultural differences across nations, how could multinational firms form governance that spans cultural relationships across borders? Moreover, emerging markets often lack key resources such as financial capital, reliable market information, mature factor market, etc. The shortage of financial capital makes firms unlikely to invest substantially in R&D; accordingly, if buyers need special expertise from suppliers to co-develop their products, how could they solve such problem? How does such challenge affect product and process innovation in supply chain management? On the other hand, without reliable market information, it is difficult for firms to accurately forecast the future demand of their products and adjust their production accordingly. How do firms cooperate with their supply chain partners to overcome this difficulty? Or how can supply chain partners take advantage of this difficulty to reap more benefits in emerging markets? Overall, this special issue offers new directions for future research to explore more deeply the interplay of institutional environments and operations management strategies in emerging markets. Future research should contextualize the unique institutional features of emerging markets to generate novel and deep insights that extend extant operation management literature. Such insights are of great importance for academia, practitioners, and policy makers in emerging markets. Reviewers We deeply appreciate the previous Editor in Chief, Tom Choi, and the current Editors in Chief, Dan Guide and Mikko Ketokivi, for their support to this special issue. We would also like to thank the reviewers, who provided us timely and constructive comments to facilitate the review process.",
            "url": "https://openalex.org/W2516402110",
            "title": "Supply chain management in emerging markets",
            "publication_date": "2016-08-11"
          }
        ]
      },
      "S7832532872": {
        "id": "S7832532872",
        "text": "In emerging economies, the reliance on informal governance mechanisms, such as trust and social networks, is critical for managing supplier relationships due to the inadequacy of formal legal institutions, which enhances supply chain performance by fostering relational coordination.",
        "children": [
          {
            "id": "E3918592038",
            "text": "As multinational enterprises (MNEs) increasingly source from suppliers in emerging markets, a particular challenge is how to manage their relationships with suppliers within potentially deleterious institutional environments. A large body of research has investigated supply chain management from multiple theoretical viewpoints, mostly from transaction cost economics and relational exchange perspective (Carey et al., 2011). From these perspectives, companies can use both formal governance mechanisms such as contracts, control, and monitoring as well as informal governance mechanisms such as trust, socialization, and relational norms to manage the relationships with their suppliers (Cao and Lumineau, 2015; Li et al., 2010; Villena et al., 2011). However, most extant studies on supply chain management have overlooked the role of institutional environments in which companies cooperate with their suppliers (Zhou et al., 2014). As emerging markets have experienced massive changes in their economic, social, and political institutions, institutional characteristics greatly shape the efficacy of supply chain management strategies. For example, signing a detailed contract may not be helpful in emerging markets where legal institutions are yet to be developed. Transaction cost economics was originated from institutional contexts with a long tradition of contract laws and strong legal regimes. Yet such strong legal institutions do not automatically appear in emerging markets (Handley and Angst, 2015). Indeed, the lack of strong legal institutions represents one of the most severe challenges for businesses operating in emerging markets (Peng, 2003). Related, the instability of political regime also challenges supply chain management practices in emerging markets. When strong political and legal institutions are absent, informal institutions based on personal connections and social network play a pivotal role, because social networks provide access to scarce resources and offer legitimacy status (Yang et al., 2012). However, the inherent restraint of social networks is the limited number of personal ties that an individual can possess. Because personal relationships are difficult to build and costly to maintain, they may not be able to catch up with rapid growing exchange scale in emerging markets (Peng, 2003). So how could companies manage their relationships with suppliers in emerging markets, which are characterized by political, legal, and social challenges? To address this fascinating issue, this special issue serves as an exemplar for integrating the unique characteristics of emerging economies in conceptual development and empirical assessment. What makes emerging economies a unique context to study supply chain management? Emerging markets are \u201clow-income, rapid growth countries using economic liberalization as their primary engine of growth\u201d (Hoskisson et al., 2000), where institutions are undergoing fundamental and continuous changes. The most unique features that seriously challenge effective supply chain management are institutional voids, including shallow capital markets, deficient legal systems, lack of independent accounting intermediaries, and etc. (Khanna and Palepu, 1997). In large emerging markets, institutional voids also influence regional economic development, leading to subnational variations within the same country (Zhou and Poppo, 2010). Accordingly, companies must develop alternative operations management strategies and adjust their strategic decisions to overcome institutional voids. The call for papers generated 50 submissions, among which six manuscripts were finally accepted after three or four rounds of review. These six papers include survey research, case studies, and archival data research, and use samples of Chinese firms, Indian companies, and American firms that outsource in emerging markets. They also address explicitly the methodological issues (e.g., endogeneity and common method bias) indicated by the recent JOM Editorial (Guide and Ketokivi, 2015). How to deal with role hazard between buyers and suppliers caused by sub-national institutional distance? How do social management capabilities help multinational buyers and their emerging market suppliers respond to stakeholder pressures, address regulatory gaps, and improve social performance? How does institutional deficiency affect contractual inefficiency and consequently ties utilization of emerging market firms? How do outsourcing, in-house offshoring, and sales to emerging markets affect product recalls and inventory performance? How do buyer-supplier compatibility and institutional environment affect product co-development between buyers and suppliers? How to reduce local supplier opportunism in China when facing regulatory uncertainty? Collectively, these papers provide fresh insights into how institutional environments in emerging markets affect operation management strategies and how companies develop supply chain management strategies to deal with institutional voids. In Dong, Ju, and Fang's article, the authors introduce an important notion, role hazard, which represents a critical yet understudied relational coordination problem. Their study shows that subnational institutional distance leads to role ambiguity and conflict, two important facets of role hazards between buyers and suppliers, which in turn jeopardize supply chain performance. To mitigate such problem, supply chain partners could share information and adapt to the changing environments continuously. These findings provide a novel perspective on how supply chain partners foster relational coordination by reducing role hazards caused by subnational institutional distance in emerging markets. Huq, Chowdhury, and Klassen use multiple case studies from before and after the Rana Plaza building collapse in Bangladesh to develop an understanding of the skills, practices, relationships and processes (collectively referred as social management capabilities) that help firms in emerging markets to improve their performance on human safety & welfare, and social & community development. Specifically, they attempt to address two research questions: What are the social management capabilities needed by multinational buyers and their emerging market suppliers to improve social performance and respond to stakeholder pressures? How do external factors and shocks affect the development and evolution of these capabilities in emerging markets? Their findings provide novel insights on how multinational buyers and local suppliers achieve social performance in emerging markets. Shou, Zheng, and Zhu address another critical issue in emerging markets: contractual ineffectiveness, the difficulty of using contracts to safeguard and coordinate transactions between supply chain partners. The authors show that legal enforceability and information transparency lead to contractual inefficiency, which in turn promotes a firm's efforts to seek political and business ties. They also find that efficiency pressure and equity pressures differentially moderate the relationships between contractual inefficiency and pursuit of social ties. These results provide important implications on the alternative and dynamic use of contracts and social ties in supply chain management in emerging markets. Steven and Britto study three different forms of emerging market penetration (i.e., outsourcing, in-house offshoring, and sales to emerging markets) and their associations with product recalls. They find that outsourcing penetration increases recalls whereas sales penetration reduces recalls. Although they cannot find a direct relationship between in-house offshoring and recalls, they show that in-house offshoring is able to mitigate the positive relationship between outsourcing and recalls. They further investigate how emerging markets' characteristics (e.g., institutional immaturity) moderate these relationships. Their results provide a finer understanding of the complex relationships among different forms of emerging market penetration and product recalls. Wang, Li, and Chang focus on how buyer-supplier compatibility influences product co-development between buyers and suppliers in China. They find that while knowledge commonality has an inverted U-shaped relationship with product co-development, goal compatibility has a positive effect on product co-development. Moreover, mutual learning partially mediates the effects of buyer-supplier compatibility on product co-development. They further consider how formal and informal institutional environments (i.e., government intervention and guanxi importance) moderate the relationship between mutual learning and product co-development differently. Their research provides important theoretical and managerial implications for buyer-supplier collaboration in emerging markets. Wang, Zhang, Wang, and Sheng examine contracts and trust as two alternative governance modes in curtailing opportunism in China. They argue that the effects of these two governance strategies in reducing opportunism depend on two institution-related factors: regulatory uncertainty and relationship structure in the Chinese markets. The authors show that contracts are more effective in deterring supplier opportunism when regulatory uncertainty is high. In addition, contracts help curtail opportunism more in domestic, compared with international, buyer\u2013supplier relationships, whereas trust is more effective in restricting supplier opportunism in international relationships than in domestic ones. Taken together, these articles indicate that institutional environments play a critical role in supply chain management in emerging markets. Equally important, firms in emerging market can develop their own operations management strategies and capabilities to overcome institutional voids and enhance supply chain performance. These articles illustrate the fundamental challenges brought by institutional voids and provide fresh perspectives on effective supply chain management in emerging markets. Whereas these articles advance this line of research significantly, they only represent an initial effort to address a highly complicated issue. Looking forward, we would like to encourage more research to take the unique characteristics of emerging markets into consideration. Among the six papers in this special issue, two consider the overall institutional environments such as subnational institutional distance and institutional immaturity, the other three examine legal institutions such as regulatory gaps, legal enforceability, and regulatory uncertainty, whereas only one touches upon social institutions (i.e., guanxi importance). However, institutional environments include a variety of formal and informal factors. Formal factors consist of property rights protection, government interference, policy changes, etc., and informal factors include ideology, cultural values and norms. For example, the lack of intellectual property protection causes serious problems for supply chain management in emerging markets. How could multinational buyers outsource the manufacturing to local suppliers and at the same time protect themselves from illegal copying? Also, emerging market governments tend to interfere with market operations heavily. How would such government intervention affect operations management? How could emerging markets firms take advantage of such government intervention to foster their supply chain performance? How would policy changes affect supply chain management? Inadequate institutional arrangements such as weak legal systems and regulations in emerging markets render firms a disadvantageous position in which they lack the institutional assurance for their decisions and operations. However, informal institutions such as cultural values and norms may complement and/or substitute the formal institutions in providing a regulated business environment in emerging markets. For example, social ties are shown to enhance performance because of their cultural legitimacy and resource implications in China. How could emerging market firms maneuver the cultural values to form cohesive social ties for long-term cooperation? How could firms take advantage of social ties to form cooperative business circles to enhance governance effectiveness? Given the cultural differences across nations, how could multinational firms form governance that spans cultural relationships across borders? Moreover, emerging markets often lack key resources such as financial capital, reliable market information, mature factor market, etc. The shortage of financial capital makes firms unlikely to invest substantially in R&D; accordingly, if buyers need special expertise from suppliers to co-develop their products, how could they solve such problem? How does such challenge affect product and process innovation in supply chain management? On the other hand, without reliable market information, it is difficult for firms to accurately forecast the future demand of their products and adjust their production accordingly. How do firms cooperate with their supply chain partners to overcome this difficulty? Or how can supply chain partners take advantage of this difficulty to reap more benefits in emerging markets? Overall, this special issue offers new directions for future research to explore more deeply the interplay of institutional environments and operations management strategies in emerging markets. Future research should contextualize the unique institutional features of emerging markets to generate novel and deep insights that extend extant operation management literature. Such insights are of great importance for academia, practitioners, and policy makers in emerging markets. Reviewers We deeply appreciate the previous Editor in Chief, Tom Choi, and the current Editors in Chief, Dan Guide and Mikko Ketokivi, for their support to this special issue. We would also like to thank the reviewers, who provided us timely and constructive comments to facilitate the review process.",
            "url": "https://openalex.org/W2516402110",
            "title": "Supply chain management in emerging markets",
            "publication_date": "2016-08-11"
          }
        ]
      },
      "S3968850946": {
        "id": "S3968850946",
        "text": "Buyer-supplier compatibility, particularly through goal alignment and mutual learning, significantly influences product co-development in emerging markets, thereby enhancing supply chain performance.",
        "children": [
          {
            "id": "E3918592038",
            "text": "As multinational enterprises (MNEs) increasingly source from suppliers in emerging markets, a particular challenge is how to manage their relationships with suppliers within potentially deleterious institutional environments. A large body of research has investigated supply chain management from multiple theoretical viewpoints, mostly from transaction cost economics and relational exchange perspective (Carey et al., 2011). From these perspectives, companies can use both formal governance mechanisms such as contracts, control, and monitoring as well as informal governance mechanisms such as trust, socialization, and relational norms to manage the relationships with their suppliers (Cao and Lumineau, 2015; Li et al., 2010; Villena et al., 2011). However, most extant studies on supply chain management have overlooked the role of institutional environments in which companies cooperate with their suppliers (Zhou et al., 2014). As emerging markets have experienced massive changes in their economic, social, and political institutions, institutional characteristics greatly shape the efficacy of supply chain management strategies. For example, signing a detailed contract may not be helpful in emerging markets where legal institutions are yet to be developed. Transaction cost economics was originated from institutional contexts with a long tradition of contract laws and strong legal regimes. Yet such strong legal institutions do not automatically appear in emerging markets (Handley and Angst, 2015). Indeed, the lack of strong legal institutions represents one of the most severe challenges for businesses operating in emerging markets (Peng, 2003). Related, the instability of political regime also challenges supply chain management practices in emerging markets. When strong political and legal institutions are absent, informal institutions based on personal connections and social network play a pivotal role, because social networks provide access to scarce resources and offer legitimacy status (Yang et al., 2012). However, the inherent restraint of social networks is the limited number of personal ties that an individual can possess. Because personal relationships are difficult to build and costly to maintain, they may not be able to catch up with rapid growing exchange scale in emerging markets (Peng, 2003). So how could companies manage their relationships with suppliers in emerging markets, which are characterized by political, legal, and social challenges? To address this fascinating issue, this special issue serves as an exemplar for integrating the unique characteristics of emerging economies in conceptual development and empirical assessment. What makes emerging economies a unique context to study supply chain management? Emerging markets are \u201clow-income, rapid growth countries using economic liberalization as their primary engine of growth\u201d (Hoskisson et al., 2000), where institutions are undergoing fundamental and continuous changes. The most unique features that seriously challenge effective supply chain management are institutional voids, including shallow capital markets, deficient legal systems, lack of independent accounting intermediaries, and etc. (Khanna and Palepu, 1997). In large emerging markets, institutional voids also influence regional economic development, leading to subnational variations within the same country (Zhou and Poppo, 2010). Accordingly, companies must develop alternative operations management strategies and adjust their strategic decisions to overcome institutional voids. The call for papers generated 50 submissions, among which six manuscripts were finally accepted after three or four rounds of review. These six papers include survey research, case studies, and archival data research, and use samples of Chinese firms, Indian companies, and American firms that outsource in emerging markets. They also address explicitly the methodological issues (e.g., endogeneity and common method bias) indicated by the recent JOM Editorial (Guide and Ketokivi, 2015). How to deal with role hazard between buyers and suppliers caused by sub-national institutional distance? How do social management capabilities help multinational buyers and their emerging market suppliers respond to stakeholder pressures, address regulatory gaps, and improve social performance? How does institutional deficiency affect contractual inefficiency and consequently ties utilization of emerging market firms? How do outsourcing, in-house offshoring, and sales to emerging markets affect product recalls and inventory performance? How do buyer-supplier compatibility and institutional environment affect product co-development between buyers and suppliers? How to reduce local supplier opportunism in China when facing regulatory uncertainty? Collectively, these papers provide fresh insights into how institutional environments in emerging markets affect operation management strategies and how companies develop supply chain management strategies to deal with institutional voids. In Dong, Ju, and Fang's article, the authors introduce an important notion, role hazard, which represents a critical yet understudied relational coordination problem. Their study shows that subnational institutional distance leads to role ambiguity and conflict, two important facets of role hazards between buyers and suppliers, which in turn jeopardize supply chain performance. To mitigate such problem, supply chain partners could share information and adapt to the changing environments continuously. These findings provide a novel perspective on how supply chain partners foster relational coordination by reducing role hazards caused by subnational institutional distance in emerging markets. Huq, Chowdhury, and Klassen use multiple case studies from before and after the Rana Plaza building collapse in Bangladesh to develop an understanding of the skills, practices, relationships and processes (collectively referred as social management capabilities) that help firms in emerging markets to improve their performance on human safety & welfare, and social & community development. Specifically, they attempt to address two research questions: What are the social management capabilities needed by multinational buyers and their emerging market suppliers to improve social performance and respond to stakeholder pressures? How do external factors and shocks affect the development and evolution of these capabilities in emerging markets? Their findings provide novel insights on how multinational buyers and local suppliers achieve social performance in emerging markets. Shou, Zheng, and Zhu address another critical issue in emerging markets: contractual ineffectiveness, the difficulty of using contracts to safeguard and coordinate transactions between supply chain partners. The authors show that legal enforceability and information transparency lead to contractual inefficiency, which in turn promotes a firm's efforts to seek political and business ties. They also find that efficiency pressure and equity pressures differentially moderate the relationships between contractual inefficiency and pursuit of social ties. These results provide important implications on the alternative and dynamic use of contracts and social ties in supply chain management in emerging markets. Steven and Britto study three different forms of emerging market penetration (i.e., outsourcing, in-house offshoring, and sales to emerging markets) and their associations with product recalls. They find that outsourcing penetration increases recalls whereas sales penetration reduces recalls. Although they cannot find a direct relationship between in-house offshoring and recalls, they show that in-house offshoring is able to mitigate the positive relationship between outsourcing and recalls. They further investigate how emerging markets' characteristics (e.g., institutional immaturity) moderate these relationships. Their results provide a finer understanding of the complex relationships among different forms of emerging market penetration and product recalls. Wang, Li, and Chang focus on how buyer-supplier compatibility influences product co-development between buyers and suppliers in China. They find that while knowledge commonality has an inverted U-shaped relationship with product co-development, goal compatibility has a positive effect on product co-development. Moreover, mutual learning partially mediates the effects of buyer-supplier compatibility on product co-development. They further consider how formal and informal institutional environments (i.e., government intervention and guanxi importance) moderate the relationship between mutual learning and product co-development differently. Their research provides important theoretical and managerial implications for buyer-supplier collaboration in emerging markets. Wang, Zhang, Wang, and Sheng examine contracts and trust as two alternative governance modes in curtailing opportunism in China. They argue that the effects of these two governance strategies in reducing opportunism depend on two institution-related factors: regulatory uncertainty and relationship structure in the Chinese markets. The authors show that contracts are more effective in deterring supplier opportunism when regulatory uncertainty is high. In addition, contracts help curtail opportunism more in domestic, compared with international, buyer\u2013supplier relationships, whereas trust is more effective in restricting supplier opportunism in international relationships than in domestic ones. Taken together, these articles indicate that institutional environments play a critical role in supply chain management in emerging markets. Equally important, firms in emerging market can develop their own operations management strategies and capabilities to overcome institutional voids and enhance supply chain performance. These articles illustrate the fundamental challenges brought by institutional voids and provide fresh perspectives on effective supply chain management in emerging markets. Whereas these articles advance this line of research significantly, they only represent an initial effort to address a highly complicated issue. Looking forward, we would like to encourage more research to take the unique characteristics of emerging markets into consideration. Among the six papers in this special issue, two consider the overall institutional environments such as subnational institutional distance and institutional immaturity, the other three examine legal institutions such as regulatory gaps, legal enforceability, and regulatory uncertainty, whereas only one touches upon social institutions (i.e., guanxi importance). However, institutional environments include a variety of formal and informal factors. Formal factors consist of property rights protection, government interference, policy changes, etc., and informal factors include ideology, cultural values and norms. For example, the lack of intellectual property protection causes serious problems for supply chain management in emerging markets. How could multinational buyers outsource the manufacturing to local suppliers and at the same time protect themselves from illegal copying? Also, emerging market governments tend to interfere with market operations heavily. How would such government intervention affect operations management? How could emerging markets firms take advantage of such government intervention to foster their supply chain performance? How would policy changes affect supply chain management? Inadequate institutional arrangements such as weak legal systems and regulations in emerging markets render firms a disadvantageous position in which they lack the institutional assurance for their decisions and operations. However, informal institutions such as cultural values and norms may complement and/or substitute the formal institutions in providing a regulated business environment in emerging markets. For example, social ties are shown to enhance performance because of their cultural legitimacy and resource implications in China. How could emerging market firms maneuver the cultural values to form cohesive social ties for long-term cooperation? How could firms take advantage of social ties to form cooperative business circles to enhance governance effectiveness? Given the cultural differences across nations, how could multinational firms form governance that spans cultural relationships across borders? Moreover, emerging markets often lack key resources such as financial capital, reliable market information, mature factor market, etc. The shortage of financial capital makes firms unlikely to invest substantially in R&D; accordingly, if buyers need special expertise from suppliers to co-develop their products, how could they solve such problem? How does such challenge affect product and process innovation in supply chain management? On the other hand, without reliable market information, it is difficult for firms to accurately forecast the future demand of their products and adjust their production accordingly. How do firms cooperate with their supply chain partners to overcome this difficulty? Or how can supply chain partners take advantage of this difficulty to reap more benefits in emerging markets? Overall, this special issue offers new directions for future research to explore more deeply the interplay of institutional environments and operations management strategies in emerging markets. Future research should contextualize the unique institutional features of emerging markets to generate novel and deep insights that extend extant operation management literature. Such insights are of great importance for academia, practitioners, and policy makers in emerging markets. Reviewers We deeply appreciate the previous Editor in Chief, Tom Choi, and the current Editors in Chief, Dan Guide and Mikko Ketokivi, for their support to this special issue. We would also like to thank the reviewers, who provided us timely and constructive comments to facilitate the review process.",
            "url": "https://openalex.org/W2516402110",
            "title": "Supply chain management in emerging markets",
            "publication_date": "2016-08-11"
          }
        ]
      },
      "S7052466317": {
        "id": "S7052466317",
        "text": "Social management capabilities, which encompass skills, practices, and processes, are essential for multinational buyers and local suppliers in emerging markets to enhance social performance and respond effectively to stakeholder pressures, contributing to overall supply chain performance.",
        "children": [
          {
            "id": "E3918592038",
            "text": "As multinational enterprises (MNEs) increasingly source from suppliers in emerging markets, a particular challenge is how to manage their relationships with suppliers within potentially deleterious institutional environments. A large body of research has investigated supply chain management from multiple theoretical viewpoints, mostly from transaction cost economics and relational exchange perspective (Carey et al., 2011). From these perspectives, companies can use both formal governance mechanisms such as contracts, control, and monitoring as well as informal governance mechanisms such as trust, socialization, and relational norms to manage the relationships with their suppliers (Cao and Lumineau, 2015; Li et al., 2010; Villena et al., 2011). However, most extant studies on supply chain management have overlooked the role of institutional environments in which companies cooperate with their suppliers (Zhou et al., 2014). As emerging markets have experienced massive changes in their economic, social, and political institutions, institutional characteristics greatly shape the efficacy of supply chain management strategies. For example, signing a detailed contract may not be helpful in emerging markets where legal institutions are yet to be developed. Transaction cost economics was originated from institutional contexts with a long tradition of contract laws and strong legal regimes. Yet such strong legal institutions do not automatically appear in emerging markets (Handley and Angst, 2015). Indeed, the lack of strong legal institutions represents one of the most severe challenges for businesses operating in emerging markets (Peng, 2003). Related, the instability of political regime also challenges supply chain management practices in emerging markets. When strong political and legal institutions are absent, informal institutions based on personal connections and social network play a pivotal role, because social networks provide access to scarce resources and offer legitimacy status (Yang et al., 2012). However, the inherent restraint of social networks is the limited number of personal ties that an individual can possess. Because personal relationships are difficult to build and costly to maintain, they may not be able to catch up with rapid growing exchange scale in emerging markets (Peng, 2003). So how could companies manage their relationships with suppliers in emerging markets, which are characterized by political, legal, and social challenges? To address this fascinating issue, this special issue serves as an exemplar for integrating the unique characteristics of emerging economies in conceptual development and empirical assessment. What makes emerging economies a unique context to study supply chain management? Emerging markets are \u201clow-income, rapid growth countries using economic liberalization as their primary engine of growth\u201d (Hoskisson et al., 2000), where institutions are undergoing fundamental and continuous changes. The most unique features that seriously challenge effective supply chain management are institutional voids, including shallow capital markets, deficient legal systems, lack of independent accounting intermediaries, and etc. (Khanna and Palepu, 1997). In large emerging markets, institutional voids also influence regional economic development, leading to subnational variations within the same country (Zhou and Poppo, 2010). Accordingly, companies must develop alternative operations management strategies and adjust their strategic decisions to overcome institutional voids. The call for papers generated 50 submissions, among which six manuscripts were finally accepted after three or four rounds of review. These six papers include survey research, case studies, and archival data research, and use samples of Chinese firms, Indian companies, and American firms that outsource in emerging markets. They also address explicitly the methodological issues (e.g., endogeneity and common method bias) indicated by the recent JOM Editorial (Guide and Ketokivi, 2015). How to deal with role hazard between buyers and suppliers caused by sub-national institutional distance? How do social management capabilities help multinational buyers and their emerging market suppliers respond to stakeholder pressures, address regulatory gaps, and improve social performance? How does institutional deficiency affect contractual inefficiency and consequently ties utilization of emerging market firms? How do outsourcing, in-house offshoring, and sales to emerging markets affect product recalls and inventory performance? How do buyer-supplier compatibility and institutional environment affect product co-development between buyers and suppliers? How to reduce local supplier opportunism in China when facing regulatory uncertainty? Collectively, these papers provide fresh insights into how institutional environments in emerging markets affect operation management strategies and how companies develop supply chain management strategies to deal with institutional voids. In Dong, Ju, and Fang's article, the authors introduce an important notion, role hazard, which represents a critical yet understudied relational coordination problem. Their study shows that subnational institutional distance leads to role ambiguity and conflict, two important facets of role hazards between buyers and suppliers, which in turn jeopardize supply chain performance. To mitigate such problem, supply chain partners could share information and adapt to the changing environments continuously. These findings provide a novel perspective on how supply chain partners foster relational coordination by reducing role hazards caused by subnational institutional distance in emerging markets. Huq, Chowdhury, and Klassen use multiple case studies from before and after the Rana Plaza building collapse in Bangladesh to develop an understanding of the skills, practices, relationships and processes (collectively referred as social management capabilities) that help firms in emerging markets to improve their performance on human safety & welfare, and social & community development. Specifically, they attempt to address two research questions: What are the social management capabilities needed by multinational buyers and their emerging market suppliers to improve social performance and respond to stakeholder pressures? How do external factors and shocks affect the development and evolution of these capabilities in emerging markets? Their findings provide novel insights on how multinational buyers and local suppliers achieve social performance in emerging markets. Shou, Zheng, and Zhu address another critical issue in emerging markets: contractual ineffectiveness, the difficulty of using contracts to safeguard and coordinate transactions between supply chain partners. The authors show that legal enforceability and information transparency lead to contractual inefficiency, which in turn promotes a firm's efforts to seek political and business ties. They also find that efficiency pressure and equity pressures differentially moderate the relationships between contractual inefficiency and pursuit of social ties. These results provide important implications on the alternative and dynamic use of contracts and social ties in supply chain management in emerging markets. Steven and Britto study three different forms of emerging market penetration (i.e., outsourcing, in-house offshoring, and sales to emerging markets) and their associations with product recalls. They find that outsourcing penetration increases recalls whereas sales penetration reduces recalls. Although they cannot find a direct relationship between in-house offshoring and recalls, they show that in-house offshoring is able to mitigate the positive relationship between outsourcing and recalls. They further investigate how emerging markets' characteristics (e.g., institutional immaturity) moderate these relationships. Their results provide a finer understanding of the complex relationships among different forms of emerging market penetration and product recalls. Wang, Li, and Chang focus on how buyer-supplier compatibility influences product co-development between buyers and suppliers in China. They find that while knowledge commonality has an inverted U-shaped relationship with product co-development, goal compatibility has a positive effect on product co-development. Moreover, mutual learning partially mediates the effects of buyer-supplier compatibility on product co-development. They further consider how formal and informal institutional environments (i.e., government intervention and guanxi importance) moderate the relationship between mutual learning and product co-development differently. Their research provides important theoretical and managerial implications for buyer-supplier collaboration in emerging markets. Wang, Zhang, Wang, and Sheng examine contracts and trust as two alternative governance modes in curtailing opportunism in China. They argue that the effects of these two governance strategies in reducing opportunism depend on two institution-related factors: regulatory uncertainty and relationship structure in the Chinese markets. The authors show that contracts are more effective in deterring supplier opportunism when regulatory uncertainty is high. In addition, contracts help curtail opportunism more in domestic, compared with international, buyer\u2013supplier relationships, whereas trust is more effective in restricting supplier opportunism in international relationships than in domestic ones. Taken together, these articles indicate that institutional environments play a critical role in supply chain management in emerging markets. Equally important, firms in emerging market can develop their own operations management strategies and capabilities to overcome institutional voids and enhance supply chain performance. These articles illustrate the fundamental challenges brought by institutional voids and provide fresh perspectives on effective supply chain management in emerging markets. Whereas these articles advance this line of research significantly, they only represent an initial effort to address a highly complicated issue. Looking forward, we would like to encourage more research to take the unique characteristics of emerging markets into consideration. Among the six papers in this special issue, two consider the overall institutional environments such as subnational institutional distance and institutional immaturity, the other three examine legal institutions such as regulatory gaps, legal enforceability, and regulatory uncertainty, whereas only one touches upon social institutions (i.e., guanxi importance). However, institutional environments include a variety of formal and informal factors. Formal factors consist of property rights protection, government interference, policy changes, etc., and informal factors include ideology, cultural values and norms. For example, the lack of intellectual property protection causes serious problems for supply chain management in emerging markets. How could multinational buyers outsource the manufacturing to local suppliers and at the same time protect themselves from illegal copying? Also, emerging market governments tend to interfere with market operations heavily. How would such government intervention affect operations management? How could emerging markets firms take advantage of such government intervention to foster their supply chain performance? How would policy changes affect supply chain management? Inadequate institutional arrangements such as weak legal systems and regulations in emerging markets render firms a disadvantageous position in which they lack the institutional assurance for their decisions and operations. However, informal institutions such as cultural values and norms may complement and/or substitute the formal institutions in providing a regulated business environment in emerging markets. For example, social ties are shown to enhance performance because of their cultural legitimacy and resource implications in China. How could emerging market firms maneuver the cultural values to form cohesive social ties for long-term cooperation? How could firms take advantage of social ties to form cooperative business circles to enhance governance effectiveness? Given the cultural differences across nations, how could multinational firms form governance that spans cultural relationships across borders? Moreover, emerging markets often lack key resources such as financial capital, reliable market information, mature factor market, etc. The shortage of financial capital makes firms unlikely to invest substantially in R&D; accordingly, if buyers need special expertise from suppliers to co-develop their products, how could they solve such problem? How does such challenge affect product and process innovation in supply chain management? On the other hand, without reliable market information, it is difficult for firms to accurately forecast the future demand of their products and adjust their production accordingly. How do firms cooperate with their supply chain partners to overcome this difficulty? Or how can supply chain partners take advantage of this difficulty to reap more benefits in emerging markets? Overall, this special issue offers new directions for future research to explore more deeply the interplay of institutional environments and operations management strategies in emerging markets. Future research should contextualize the unique institutional features of emerging markets to generate novel and deep insights that extend extant operation management literature. Such insights are of great importance for academia, practitioners, and policy makers in emerging markets. Reviewers We deeply appreciate the previous Editor in Chief, Tom Choi, and the current Editors in Chief, Dan Guide and Mikko Ketokivi, for their support to this special issue. We would also like to thank the reviewers, who provided us timely and constructive comments to facilitate the review process.",
            "url": "https://openalex.org/W2516402110",
            "title": "Supply chain management in emerging markets",
            "publication_date": "2016-08-11"
          }
        ]
      },
      "S8330573727": {
        "id": "S8330573727",
        "text": "The concept of 'role hazard' in supplier relationships, characterized by role ambiguity and conflict due to subnational institutional distance, can be mitigated through continuous information sharing and adaptation, thereby improving supply chain performance in emerging markets.",
        "children": [
          {
            "id": "E3918592038",
            "text": "As multinational enterprises (MNEs) increasingly source from suppliers in emerging markets, a particular challenge is how to manage their relationships with suppliers within potentially deleterious institutional environments. A large body of research has investigated supply chain management from multiple theoretical viewpoints, mostly from transaction cost economics and relational exchange perspective (Carey et al., 2011). From these perspectives, companies can use both formal governance mechanisms such as contracts, control, and monitoring as well as informal governance mechanisms such as trust, socialization, and relational norms to manage the relationships with their suppliers (Cao and Lumineau, 2015; Li et al., 2010; Villena et al., 2011). However, most extant studies on supply chain management have overlooked the role of institutional environments in which companies cooperate with their suppliers (Zhou et al., 2014). As emerging markets have experienced massive changes in their economic, social, and political institutions, institutional characteristics greatly shape the efficacy of supply chain management strategies. For example, signing a detailed contract may not be helpful in emerging markets where legal institutions are yet to be developed. Transaction cost economics was originated from institutional contexts with a long tradition of contract laws and strong legal regimes. Yet such strong legal institutions do not automatically appear in emerging markets (Handley and Angst, 2015). Indeed, the lack of strong legal institutions represents one of the most severe challenges for businesses operating in emerging markets (Peng, 2003). Related, the instability of political regime also challenges supply chain management practices in emerging markets. When strong political and legal institutions are absent, informal institutions based on personal connections and social network play a pivotal role, because social networks provide access to scarce resources and offer legitimacy status (Yang et al., 2012). However, the inherent restraint of social networks is the limited number of personal ties that an individual can possess. Because personal relationships are difficult to build and costly to maintain, they may not be able to catch up with rapid growing exchange scale in emerging markets (Peng, 2003). So how could companies manage their relationships with suppliers in emerging markets, which are characterized by political, legal, and social challenges? To address this fascinating issue, this special issue serves as an exemplar for integrating the unique characteristics of emerging economies in conceptual development and empirical assessment. What makes emerging economies a unique context to study supply chain management? Emerging markets are \u201clow-income, rapid growth countries using economic liberalization as their primary engine of growth\u201d (Hoskisson et al., 2000), where institutions are undergoing fundamental and continuous changes. The most unique features that seriously challenge effective supply chain management are institutional voids, including shallow capital markets, deficient legal systems, lack of independent accounting intermediaries, and etc. (Khanna and Palepu, 1997). In large emerging markets, institutional voids also influence regional economic development, leading to subnational variations within the same country (Zhou and Poppo, 2010). Accordingly, companies must develop alternative operations management strategies and adjust their strategic decisions to overcome institutional voids. The call for papers generated 50 submissions, among which six manuscripts were finally accepted after three or four rounds of review. These six papers include survey research, case studies, and archival data research, and use samples of Chinese firms, Indian companies, and American firms that outsource in emerging markets. They also address explicitly the methodological issues (e.g., endogeneity and common method bias) indicated by the recent JOM Editorial (Guide and Ketokivi, 2015). How to deal with role hazard between buyers and suppliers caused by sub-national institutional distance? How do social management capabilities help multinational buyers and their emerging market suppliers respond to stakeholder pressures, address regulatory gaps, and improve social performance? How does institutional deficiency affect contractual inefficiency and consequently ties utilization of emerging market firms? How do outsourcing, in-house offshoring, and sales to emerging markets affect product recalls and inventory performance? How do buyer-supplier compatibility and institutional environment affect product co-development between buyers and suppliers? How to reduce local supplier opportunism in China when facing regulatory uncertainty? Collectively, these papers provide fresh insights into how institutional environments in emerging markets affect operation management strategies and how companies develop supply chain management strategies to deal with institutional voids. In Dong, Ju, and Fang's article, the authors introduce an important notion, role hazard, which represents a critical yet understudied relational coordination problem. Their study shows that subnational institutional distance leads to role ambiguity and conflict, two important facets of role hazards between buyers and suppliers, which in turn jeopardize supply chain performance. To mitigate such problem, supply chain partners could share information and adapt to the changing environments continuously. These findings provide a novel perspective on how supply chain partners foster relational coordination by reducing role hazards caused by subnational institutional distance in emerging markets. Huq, Chowdhury, and Klassen use multiple case studies from before and after the Rana Plaza building collapse in Bangladesh to develop an understanding of the skills, practices, relationships and processes (collectively referred as social management capabilities) that help firms in emerging markets to improve their performance on human safety & welfare, and social & community development. Specifically, they attempt to address two research questions: What are the social management capabilities needed by multinational buyers and their emerging market suppliers to improve social performance and respond to stakeholder pressures? How do external factors and shocks affect the development and evolution of these capabilities in emerging markets? Their findings provide novel insights on how multinational buyers and local suppliers achieve social performance in emerging markets. Shou, Zheng, and Zhu address another critical issue in emerging markets: contractual ineffectiveness, the difficulty of using contracts to safeguard and coordinate transactions between supply chain partners. The authors show that legal enforceability and information transparency lead to contractual inefficiency, which in turn promotes a firm's efforts to seek political and business ties. They also find that efficiency pressure and equity pressures differentially moderate the relationships between contractual inefficiency and pursuit of social ties. These results provide important implications on the alternative and dynamic use of contracts and social ties in supply chain management in emerging markets. Steven and Britto study three different forms of emerging market penetration (i.e., outsourcing, in-house offshoring, and sales to emerging markets) and their associations with product recalls. They find that outsourcing penetration increases recalls whereas sales penetration reduces recalls. Although they cannot find a direct relationship between in-house offshoring and recalls, they show that in-house offshoring is able to mitigate the positive relationship between outsourcing and recalls. They further investigate how emerging markets' characteristics (e.g., institutional immaturity) moderate these relationships. Their results provide a finer understanding of the complex relationships among different forms of emerging market penetration and product recalls. Wang, Li, and Chang focus on how buyer-supplier compatibility influences product co-development between buyers and suppliers in China. They find that while knowledge commonality has an inverted U-shaped relationship with product co-development, goal compatibility has a positive effect on product co-development. Moreover, mutual learning partially mediates the effects of buyer-supplier compatibility on product co-development. They further consider how formal and informal institutional environments (i.e., government intervention and guanxi importance) moderate the relationship between mutual learning and product co-development differently. Their research provides important theoretical and managerial implications for buyer-supplier collaboration in emerging markets. Wang, Zhang, Wang, and Sheng examine contracts and trust as two alternative governance modes in curtailing opportunism in China. They argue that the effects of these two governance strategies in reducing opportunism depend on two institution-related factors: regulatory uncertainty and relationship structure in the Chinese markets. The authors show that contracts are more effective in deterring supplier opportunism when regulatory uncertainty is high. In addition, contracts help curtail opportunism more in domestic, compared with international, buyer\u2013supplier relationships, whereas trust is more effective in restricting supplier opportunism in international relationships than in domestic ones. Taken together, these articles indicate that institutional environments play a critical role in supply chain management in emerging markets. Equally important, firms in emerging market can develop their own operations management strategies and capabilities to overcome institutional voids and enhance supply chain performance. These articles illustrate the fundamental challenges brought by institutional voids and provide fresh perspectives on effective supply chain management in emerging markets. Whereas these articles advance this line of research significantly, they only represent an initial effort to address a highly complicated issue. Looking forward, we would like to encourage more research to take the unique characteristics of emerging markets into consideration. Among the six papers in this special issue, two consider the overall institutional environments such as subnational institutional distance and institutional immaturity, the other three examine legal institutions such as regulatory gaps, legal enforceability, and regulatory uncertainty, whereas only one touches upon social institutions (i.e., guanxi importance). However, institutional environments include a variety of formal and informal factors. Formal factors consist of property rights protection, government interference, policy changes, etc., and informal factors include ideology, cultural values and norms. For example, the lack of intellectual property protection causes serious problems for supply chain management in emerging markets. How could multinational buyers outsource the manufacturing to local suppliers and at the same time protect themselves from illegal copying? Also, emerging market governments tend to interfere with market operations heavily. How would such government intervention affect operations management? How could emerging markets firms take advantage of such government intervention to foster their supply chain performance? How would policy changes affect supply chain management? Inadequate institutional arrangements such as weak legal systems and regulations in emerging markets render firms a disadvantageous position in which they lack the institutional assurance for their decisions and operations. However, informal institutions such as cultural values and norms may complement and/or substitute the formal institutions in providing a regulated business environment in emerging markets. For example, social ties are shown to enhance performance because of their cultural legitimacy and resource implications in China. How could emerging market firms maneuver the cultural values to form cohesive social ties for long-term cooperation? How could firms take advantage of social ties to form cooperative business circles to enhance governance effectiveness? Given the cultural differences across nations, how could multinational firms form governance that spans cultural relationships across borders? Moreover, emerging markets often lack key resources such as financial capital, reliable market information, mature factor market, etc. The shortage of financial capital makes firms unlikely to invest substantially in R&D; accordingly, if buyers need special expertise from suppliers to co-develop their products, how could they solve such problem? How does such challenge affect product and process innovation in supply chain management? On the other hand, without reliable market information, it is difficult for firms to accurately forecast the future demand of their products and adjust their production accordingly. How do firms cooperate with their supply chain partners to overcome this difficulty? Or how can supply chain partners take advantage of this difficulty to reap more benefits in emerging markets? Overall, this special issue offers new directions for future research to explore more deeply the interplay of institutional environments and operations management strategies in emerging markets. Future research should contextualize the unique institutional features of emerging markets to generate novel and deep insights that extend extant operation management literature. Such insights are of great importance for academia, practitioners, and policy makers in emerging markets. Reviewers We deeply appreciate the previous Editor in Chief, Tom Choi, and the current Editors in Chief, Dan Guide and Mikko Ketokivi, for their support to this special issue. We would also like to thank the reviewers, who provided us timely and constructive comments to facilitate the review process.",
            "url": "https://openalex.org/W2516402110",
            "title": "Supply chain management in emerging markets",
            "publication_date": "2016-08-11"
          }
        ]
      }
    },
    "evaluation": {
      "scores": {
        "accuracy": {
          "reasoning": "The given answer accurately reflects the key components of Supplier Relationship Management (SRM) practices and their influence on supply chain performance in emerging economies. It discusses collaboration, development, evaluation, and long-term partnerships, which are all critical aspects highlighted in the golden answer. However, it lacks some specific details and examples provided in the golden answer, such as the context of Bangladesh and the specific mechanisms through which these practices impact performance. Overall, the answer is factually correct but not as detailed as the golden answer.",
          "score": 8
        },
        "completeness": {
          "reasoning": "The given answer covers the main SRM practices and their impacts on supply chain performance, but it does not delve into the literature review or provide as comprehensive an analysis as the golden answer. It mentions key practices but lacks the depth of exploration into how these practices interconnect and their collective impact, which is a significant aspect of the golden answer. Therefore, while it addresses the question, it does not fully encompass all elements discussed in the golden answer.",
          "score": 6
        },
        "relevance": {
          "reasoning": "The given answer is relevant to the complex question, addressing the influence of SRM practices on supply chain performance in emerging economies. It specifically mentions the practices of collaboration, development, evaluation, and long-term partnerships, which are central to the question. However, it could improve by explicitly linking these practices to the context of emerging economies and providing more specific examples, as done in the golden answer. Overall, it is relevant but could be more directly aligned with the question's nuances.",
          "score": 7
        },
        "evidence_quality": {
          "reasoning": "The given answer cites various sources to support its claims, which adds credibility. However, the sources are not as thoroughly integrated or as authoritative as those in the golden answer, which references specific studies and provides a more robust foundation for its claims. The quality of evidence in the given answer is good but does not reach the level of reliability and authority found in the golden answer. Therefore, while it is supported by evidence, it lacks the depth and rigor of the golden answer's citations.",
          "score": 6
        },
        "clarity": {
          "reasoning": "The given answer is generally clear and easy to understand, with well-defined sections for each SRM practice. However, some sentences could be more concise, and the overall flow could be improved to enhance readability. The golden answer presents its information in a more structured manner, which aids comprehension. While the given answer is clear, it does not achieve the same level of clarity as the golden answer.",
          "score": 7
        },
        "logical_structure": {
          "reasoning": "The given answer has a logical structure, with clear headings for each SRM practice. However, it lacks the comprehensive introduction and conclusion that frame the discussion in the golden answer. The organization is good, but it could benefit from a more cohesive narrative that ties the sections together and reinforces the overall argument. Thus, while the structure is logical, it is not as well-developed as in the golden answer.",
          "score": 6
        },
        "evidence_support": {
          "reasoning": "The given answer uses evidence to support its claims, referencing various studies and concepts related to SRM practices. However, the integration of this evidence is not as seamless as in the golden answer, which weaves evidence throughout its analysis to strengthen its arguments. The given answer could improve by providing more context for its references and linking them more explicitly to the claims being made. Therefore, while there is evidence support, it is not as strong as in the golden answer.",
          "score": 6
        },
        "depth_of_analysis": {
          "reasoning": "The given answer provides a good overview of SRM practices and their impacts but lacks the depth of analysis found in the golden answer. The golden answer explores each practice in detail, discussing their implications and interconnections, while the given answer offers a more surface-level treatment. This lack of depth limits the overall insight provided by the given answer compared to the golden answer, which thoroughly examines the topic.",
          "score": 5
        },
        "objectivity": {
          "reasoning": "The given answer maintains an objective tone, presenting information without apparent bias. It discusses the practices and their impacts in a straightforward manner, similar to the golden answer. However, it could benefit from acknowledging potential limitations or challenges associated with SRM practices in emerging economies, which would enhance its objectivity. Overall, it is fairly balanced but could be improved.",
          "score": 7
        },
        "synthesis": {
          "reasoning": "The given answer synthesizes information from various sources to discuss SRM practices, but it does not integrate these viewpoints as effectively as the golden answer. The golden answer presents a more cohesive synthesis of literature, linking different aspects of SRM practices and their collective impact on supply chain performance. The given answer, while it does reference multiple sources, does not create as strong a narrative or connection between them, resulting in a less integrated perspective.",
          "score": 5
        }
      }
    },
    "average_score": 6.3
  },
  {
    "id": "https://openalex.org/W3194732006",
    "meta": {
      "id": "https://openalex.org/W3194732006",
      "title": "Roles of Innovation Leadership on Using Big Data Analytics to Establish Resilient Healthcare Supply Chains to Combat the COVID-19 Pandemic: A Multimethodological Study",
      "publication_date": "2024-01-01",
      "cited_by_count": 116,
      "topics": "Building Resilient Supply Chain, Impact of Big Data Analytics on Business Performance, Machine Learning in Smart Healthcare",
      "keywords": "Pandemic, 2019-20 coronavirus outbreak, Supply Chain Resilience, Organizational Resilience",
      "concepts": "Pandemic, Coronavirus disease 2019 (COVID-19), Big data, Health care, Supply chain, 2019-20 coronavirus outbreak, Analytics, Business, Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), Data science, Knowledge management, Computer science, Marketing, Economics, Virology, Medicine, Economic growth, Data mining, Disease, Pathology, Infectious disease (medical specialty), Outbreak",
      "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf",
      "pdf_urls_by_priority": [
        "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf"
      ],
      "text_type": "full_text",
      "successful_pdf_url": "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf",
      "openalex_rank": 13,
      "num_tokens": 8144,
      "url": "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf"
    },
    "text": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\nIEEE TRANSACTIONS ON ENGINEERING MANAGEMENT 1\nRoles of Innovation Leadership on Using Big Data\nAnalytics to Establish Resilient Healthcare Supply\nChains to Combat the COVID-19 Pandemic: A\nMultimethodological Study\nSurajit Bag , Shivam Gupta , Tsan-Ming Choi , and Ajay Kumar\nAbstract\u2014This article empirically examines the effect of big data\nanalytics (BDA) on healthcare supply chain (HSC) innovation,\nsupply chain responsiveness, and supply chain resilience under\nthe moderating effect of innovation leadership in the context of\nthe COVID-19 pandemic. The scanning interpretation\u2013action\u2013\nperformance model and organization information processing the\u0002ory are used to explain BDA, HSC innovation, responsiveness, and\nresilience relationships. First, the hypotheses were tested using\ndata collected from 190 experienced respondents working in the\nhealthcare industry. Our structural equation modeling analysis\nusing the partial least squares (PLS) method revealed that BDA\ncapabilities play a pivotal role in building a responsive HSC and\nimproving innovation, which has contributed to resilience dur\u0002ing the current pandemic situation. High innovation leadership\nstrengthens the effect of BDA capabilities on HSC innovation. High\ninnovation leadership also increases the effect of BDA capabilities\non responsiveness. Second, we validated and supplemented the em\u0002pirical research findings using inputs collected in 30 semistructured\nqualitative questionnaires. Our article makes a unique contribution\nfrom the perspective of innovation leaderships. In particular, we\nargue that the role of innovative leadership in the COVID-19\npandemic situation is critical as it indirectly affects HSC resilience\nwhen BDA is in place.\nIndex Terms\u2014Big data analytics (BDA), COVID-19, healthcare\nsupply chain, multi-methods research, responsive supply chain,\nsupply chain innovation, supply chain resilience.\nManuscript received February 28, 2021; revised May 5, 2021 and July 9, 2021;\naccepted July 25, 2021. Review of this manuscript was arranged by Department\nEditor N. Damij. (Corresponding author: Tsan-Ming Choi.)\nSurajit Bag is with the Department of Supply Chain Management and In\u0002formation Systems, Rabat Business School, International University of Rabat,\nRabat 11103, Morocco (e-mail: surajit.bag@gmail.com).\nShivam Gupta is with the Department of Information Systems, Supply Chain\nand Decision Making, NEOMA Business School, 59 Rue Pierre Taittinger,\n51100 Reims, France (e-mail: shivam.gupta@neoma-bs.fr).\nTsan-Ming Choi is with the Department and Graduate Institute of Business\nAdministration, College of Management, National Taiwan University, Taipei\n10617, Taiwan (e-mail: tmjchoi@gmail.com).\nAjay Kumar is with the AIM Research Centre on Artificial Intelligence in\nValue Creation, EMLYON Business School, 69130 \u00c9cully, France (e-mail:\nakumar@em-lyon.com).\nColor versions of one or more figures in this article are available at https:\n//doi.org/10.1109/TEM.2021.3101590.\nDigital Object Identifier 10.1109/TEM.2021.3101590\nI. INTRODUCTION\nGLOBAL spending on healthcare is expected to dramati\u0002cally increase in the near future. This partially relates to\nchanging consumer requirements [1] as well as the higher expec\u0002tation on timely response to disasters [2]\u2013[4]. Most recently, the\nCOVID-19 pandemic has made it clear that healthcare supply\nchains (HSCs) are far from perfect. Not much improvements\nwere made from the experiences acquired during various prior\nepidemics such as middle east respiratory syndrome (MERS)\nand severe acute respiratory syndrome (SARS) [5], [6]. Massive\ndisruptions in HSCs have reached the level of a global crisis.\nThe availability of personal protective equipment (PPE), med\u0002ical equipment, and lifesaving drugs has been severely limited\n[7]\u2013[9]. Under COVID-19, high demands have challenged the\nHSC, highlighting the need to manage supply chains differently\nin crisis situations [9], [10]. Undoubtedly, supply chain respon\u0002siveness and innovation are essential to build a resilient HSC to\ncombat the COVID-19 pandemic when the demand uncertainties\nare extremely high [11], [12].\nIn the literature, Peeri et al. [6] pointed out the need to focus\non using digital technologies to monitor pandemic situations.\nIn particular, big data analytics (BDA) is a powerful tool to\nhelp [13]. For example, BDA supported inventory management\nof medical supplies during emergency responses is critical to\nensure the distribution of appropriate supplies [14]. Medical\ndevices with high volumes of data can apply BDA to under\u0002stand trends and future requirements of PPE. This enhances\nthe management and planning of activities in HSCs [1]. Dig\u0002ital technologies can remove barriers in pharmaceutical supply\nchains and improve flexibility and innovation related to drug\nsupplies, thereby enhancing coordination, information sharing,\nand minimizing wastes [2], [15].\nIn healthcare, BDA is valuable for environmental-scanning\n(forecasting and observation) purposes [10]. It helps predict the\nresults of drug administration, and analyze patient categorization\nand emergency response [16], all of which are of paramount\nsignificance during a pandemic like COVID-19 [13]. BDA not\nonly can sense information, it can also enhance interpreta\u0002tion to support key business decision making [17], [18] in a\ntimely manner [110]. Prior studies have shown that innovative\nsupply chains have the ability to manage risks, determine an\n\u00a9 IEEE 2021. This article is free to access and download, along with rights for full text and data mining, re\u0002use and analysis.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n2 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT\norganization\u2019s competitive position [11], and enhance the inter\u0002pretation of key information as well as strategy development\n[19]. Note that innovation leaderships (IL) can improve supply\nchain innovation and, hence, improve efficiency [20].\nSupply chain responsiveness aims to reduce manufacturing\nthroughput and transportation/distribution lead times [21]. BDA\ncapabilities can assist in building a responsive supply chain\nthat positions resources and key players (suppliers, transporters,\ndistributors) at the right places to gain a competitive advantage\n[22]. BDA improves productivity in the supply chain process by\noffering an added level of flexibility [23]. Moreover, following\nthe arguments in [24], BDA can positively affect supply chain\ntransparency. Note that a few studies have examined the role of\nBDA in the HSC during pandemics (e.g., [25]\u2013[29]). However,\nno prior research has comprehensively examined the links be\u0002tween BDA and supply chain responsiveness and innovation\ntogether. This article aims to fill this gap in the context of\nCOVID-19.\nA recent insightful study by Dubey et al. [94] used the\norganization information processing theory (OIPT) to explain\nthe relationships between blockchain technology and opera\u0002tional supply chain transparency. Dubey et al. [94] further\nargued that blockchain technology and operational supply chain\ntransparency can further enhance collaboration among actors\nengaged in disaster relief operations and this finally leads to\nimproved supply chain resilience. This study supplements [94]\nand others in the related domain, and contributes to the supply\nchain resilience literature. It is noteworthy to mention one more\nimportant recent study on supply chain resilience by Dubey\net al. [95] who highlighted the importance of BDA in enhancing\ninformation processing capacity and supply chain resilience for\nfaster recovery after any disruptions. However, IL (which is\nrelated to supply chain resilience) for HSC innovation (SCI) is\nstill an underexplored area. Carmeli et al. [64] pointed out that\nIL increases strategic fit (internal/external) and further improves\nfirm performance. Hence, the role of leaders (irrespective of\nwhether they are political leaders of the country or leaders of\ncompanies) is crucial in this pandemic time. To be specific, IL\nincludes the proper way to encourage individuals to take vari\u0002ous initiatives, develop a transparent performance measurement\nsystem, and build an environment in which quality relationships\nwould be treasured. Having good IL will lead to increased\ncreativities in the organization [64]. Innovation is related to\n\u201cout-of-the-box\u201d thinking and introducing something new such\nas new ideas, methods, or devices. Innovations in the sphere\nof healthcare products and services are keys to combat the\nCOVID-19 pandemic and yield quick recovery from the current\nstate. Thus, innovative leadership is critical for establishing a\nresilient healthcare system.\nMotivated by the importance of BDA in HSCs and the critical\nrole of IL, we study the moderating effect of IL on the contri\u0002bution of BDA to SCI and responsiveness during COVID-19\npandemic. This article is unique from a few perspectives:\n1) we examine the effect of BDA on HSC responsiveness\n(RSC) and innovation;\n2) we investigate the effects of RSC and innovation on supply\nchain resilience.\n3) We adopt the multimethodological approach in deriving\nmore scientifically sound results.\nThe main research questions that the study sought to answer\nare as follows.\nRQ1: What are the effects of BDA on (i) supply chain respon\u0002siveness and (ii) supply chain innovation under the moderating\neffect of IL during the COVID-19 pandemic?\nRQ2: What are the effects of (i) responsive supply chain and\n(ii) supply chain innovation on HSC resilience (SCR) during\nCOVID-19 pandemic?\nThe theoretical model is built through the lenses of OIPT and\nscanning interpretation\u2013action\u2013performance (SIAP) modeling.\nWe argue that BDA is useful for environmental scanning and\ninformation processing to drive SCI (interpretation of key infor\u0002mation), which helps establish the responsive supply chain (ac\u0002tions). Finally, SCI and responsiveness are essential to build SCR\n(performance). During part 1 of the study, data were collected\nin South Africa using a structured questionnaire and hypotheses\nwere tested using structural equation modeling (SEM) applying\nthe partial least squares technique (PLS-SEM). In the second\npart, a thematic analysis was performed using the data obtained\nfrom 30 semistructured qualitative questionnaires. The themes\nthat emerged from this second-phase highlight major dimensions\nassociated with BDA in the HSC.\nThe rest of this article is organized as follows. Section II\npresents the theoretical background and hypotheses, Section III\nprovides the methods used for conducting the analysis. Sec\u0002tion IV presents the data analysis. Finally, Section V and Sec\u0002tion VI concludes this article.\nII. THEORETICAL BACKGROUND AND HYPOTHESES\nDEVELOPMENT\nA. Organization Information Processing Theory\nOIPT theory proposes that organizations must enhance their\ninformation processing capacity to survive in an increasingly\nuncertain business environment [30], [31]. The COVID-19 pan\u0002demic has brought tremendous uncertainties to the lives of both\nhumans and businesses [32]. Uncertainty is driving the need\nfor building information processing capability [33], and compa\u0002nies involved in HSC need to leverage disruptive Industry 4.0\ntechnologies such as BDA to scan and process information and\nmake strategic decisions. OIPT explains how firms can develop\nthe information processing capability during the COVID-19\npandemic to assess external information such as supply crises,\nmarket demands, sales and competitors\u2019 distribution activities,\nrate of infection spreading, number of infected cases, number of\nrecoveries, number of deaths, and clinical trials monitoring and\noutcomes. Past studies have used OIPT to explain disruptions in\nsupply chains [34]. We argue that BDA enhances firms\u2019 infor\u0002mation processing capability during these uncertain pandemic\ntimes. Furthermore, supply chain innovation and responsive\u0002ness reduce uncertainty by fostering resilience. However, OIPT\ncannot single-handedly explain the entire mechanism (BDA\u2013\ninnovation\u2013responsiveness\u2013resilience). Therefore, we supple\u0002ment it with the SIAP model to better explain these relationships.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\nBAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 3\nFig. 1. Theoretical model.\nB. SIAP Model\nFor the relationships among BDA, SCI, RSC, and SCR, we\ncan refer to Yu et al. [35], and the adopted SIAP model [36]\nand OIPT [37]. Whether operating at a local or international\nlevel, every business is influenced by external factors. Situations\nchange rapidly due to variations in political, environmental,\nor technological scenarios. Running a business is, therefore,\nan uneasy task, especially during turbulent times such as the\nCOVID-19 pandemic. The SIAP model argues that firms adjust\nin the business environment by following three basic steps,\nnamely \u201cscanning, interpreting, and responding\u201d [38]. First,\norganizations scan information that can influence performance\n[36]. Accuracy is key to any effective environmental scanning\n[39], and BDA can be used to scan important data to generate\nuseful information from various internal and external sources to\ngain rich business insights and develop competitive edges [40],\n[41]. BDA fundamentally works by scanning information in the\nchanging business environment [42], [43]. Common sources of\nbig data include social media, websites, shop floor machines,\nmeters, and sensors.\nThere is enormous potential for BDA applications in the HSC\n[16]. BDA has been demonstrated to be a useful scanning tool\nthat can equip organizations with the ability to scan risks and\nreconfigure resources and competencies [44], [45]. The potential\nof artificial intelligence (AI) and BDA in fighting COVID-19 has\nbeen confirmed in the literature (see, e.g., [46]). BDA can be used\nto model the spread of infection during a pandemic, monitor\nclinical trials, and outcomes, which can be useful for framing\npolicy and controlling infection [16]. Big data generated from\nsocial media, smart phones, and other digital equipment can be\nimmensely helpful in controlling the spread of COVID-19 [13].\nThe second step in SIAP is \u201cinterpretation,\u201d whereby manage\u0002ment uses diverse models as information processing methods to\nunderstand and label information [36]. Managers can make use\nof information to identify opportunities and threats. We argue\nthat firms should resort to supply chain innovation involving\nall stakeholders to pursue creative methods and services. The\nthird step is \u201caction,\u201d i.e., the strategic initiatives that the firm\nundertakes to adapt in the changing business environment, which\ncan range from slight shifts in procedures of the business pro\u0002cesses to major alterations in product, sales, and distribution\nstrategies. We argue that RSC is a strategic initiative to respond\nfacing the COVID-19 pandemic. The final step of the SIAP\nmodel is \u201cperformance\u201d [36]. We argue that SCR is the final\noutcome that every healthcare organization intends to achieve\nduring pandemic situation.\nC. Theoretical Model and Research Hypotheses\nThe theoretical model built based on the abovementioned\ndiscussion is presented in Fig. 1. Although big data drives\nsupply chain innovation, studies on BDA methods that can\nhelp organizations to enhance innovation are limited [47]. In\naddition, research initiatives on leveraging BDA to unlock values\nrequire further investigation [48]. Previous studies have shown\nhow BDA can positively influence supply chain sustainability\n[40], [48]. We argue that BDA has a positive association with\nRSC and innovation, whereby innovation has the ability to\ndevelop highly responsive supply chains, and innovation and\nresponsiveness lead to supply chain resilience. In this article,\nwe also introduce \u201cIL\u201d as a moderating variable to examine its\neffect on the relationships between \u201cBDA and health care supply\nchain responsiveness\u201d and \u201cBDA and SCI.\u201d\n1) BDA and Supply Chain Responsiveness and Innovation:\nNowadays, the numbers of actors and products in modern supply\nchains are much higher than before. Organizations generally\nprefer big data solutions to curb problems in the supply chain\nnetwork [49]. BDA involves collecting, managing, and process\u0002ing a high volume of data generated from various sources. These\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n4 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT\ndata can be both structured and unstructured in form, and BDA\ncan be used to analyze them and unlock their value [50]. Under\nCOVID-19, the pandemics create a disaster situation and disrupt\nsupply chains because the local and international borders remain\nclosed to prevent the spread of infection. In this type of situation,\nrapid action is required to make radical changes in the supply\nchain that are only possible using BDA information processing\ncapabilities for scanning environmental information [51].\nIn an HSC, big data is generated from internal enterprise\nresource planning (ERP systems) and external sources (social\nmedia, mobile devices, data portals, and data market platforms).\nThe non structured query language (NoSQL) graph databases\nare useful for optimizing and configuring supply chains. The\nApache Hadoop platform is immensely helpful for managing\nhigh volumes of data, and MapReduce helps perform the ana\u0002lytics part to extract information [52]. During pandemic times,\nbatch analysis is untenable to manage when a vast amount of data\nare generated in the HSC. However, the Lambda architecture can\nanalyze real-time data flows by supporting data stream analytics.\nEvery minute, the data of infected patients, status of infection\nspread, current drugs, and other medical device requirements\nat different locations can be gathered using an advanced ICT\nplatform. Further data streams can be analyzed using complex\nevent processing programs. AutoID digital technologies can be\nuseful for tracking purposes as well [49], [53]. BDA can extract\ninformation that can be useful for making decisions related to\nHSC configurations [54]. However, it is important that data\nscientists and data analysts would closely monitor and control\nthe quality of data to prevent inaccurate information generation\n[55]. Therefore, we establish the following hypothesis.\nHypothesis H1: BDA capabilities have a positive relationship with\nRSC.\nBDA can also offer new opportunities for supply chain in\u0002novation [56]. New vaccines and drugs are required to combat\npandemic situations [57]. Moreover, the shortage of equipment\nsuch as PPE for front-line doctors and healthcare workers can\nbe resolved by securing specialized PPE and making alternative\nPPE products using 3-D printing and advanced manufacturing.\nIn addition, digital contact tracing apps can play an important\nrole by tracking disease spread [58].\nInnovation can involve the development of new products\nwith unique features, alternative manufacturing methods, eco\u0002friendly raw materials for manufacturing, new approaches to\ntransportation and distribution, and the development of new\nprocesses that can yield huge benefits for society at large as\nwell as firms [59]. Big data generation capabilities, data integra\u0002tion and management capabilities, advanced analytics, and data\nvisualization capabilities can be immensely useful for supply\nchain innovation [49], [59]. Therefore, we have the following\nhypothesis.\nHypothesis H2: BDA capabilities have a positive relationship with\nSCI.\n2) Moderating Effects of IL: In this article, we have used\nOIPT to explain the role of BDA in information processing for\nreducing uncertainties. However, in the literature, Hau\u00dfmann\net al. [32, p. 81] highlighted certain shortcomings of the original\nOIPT theory, which includes the point that interpersonal charac\u0002teristics and information restrictions are not taken into account.\nInterpersonal characteristics here include leadership, teamwork,\netc. To overcome the limitations of the original theory, we made\nreference to Hambrick and Mason [96] in which the authors\nconceptualized the \u201cupper echelons\u201d perspective and argued that\nfirm performance is shaped by managerial background charac\u0002teristics. \u201cUpper echelons\u201d based leadership theory can put some\nlight on the observable managerial characteristics that the leader\ncan bring to an administrative circumstance. Observable charac\u0002teristics such as age, functional tracks, other career experiences,\neducation, socioeconomic roots, financial position, and group\ncharacteristics would all influence the strategic choices made\nby top management and leaders [96]. Undoubtedly, product\ninnovation is one of the strategic choices that leaders make for\nimproving firm performance [96]. As a remark, Carmeli et al.\n[64] argued that IL can improve strategic fit and further enhance\nthe firm performance. IL is related to the innovative nature of\norganization leaders that ranges from emphasizing on teamwork,\nclarifying individual responsibility, providing clear feedbacks\nto employees, emphasizing on task orientation, encouraging\ninitiatives, and developing trust among employees [64].\nIn an uncertain business environment under COVID-19, it\nis very difficult to forecast and plan activities. Disasters and\npandemic situations exacerbate the uncertainty, and if leaders\ncontinue to work with the same approach used under normal\ncircumstances, then their businesses will not survive the impact.\nInnovative leadership can be highly effective for managing\nbusiness challenges during pandemic situations.\nApplying innovative thinking to leadership tasks can spur\nemployees to begin thinking in innovative ways and further use\nBDA to configure the HSC and pull the firm out of danger [60].\nLearning and teamwork are required to improve environmental\ntraining and configure supply chains for sustainability outcomes\n[61], [62]. Training forms part of the Industry 4.0 delivery system\nand is important in sustainable development [63].\nAll of the abovementioned human resource factors are an\u0002tecedents of IL [20]. IL improves organizational performance\nand contributes to a firm\u2019s strategic positioning within the busi\u0002ness environment [64]. Importance of human resource man\u0002agement, involving IL and responsive management for supply\nchain sustainability, is highlighted in [12], which argued that the\ngreater the IL, the more pronounced the effect of BDA informa\u0002tion processing capabilities on building responsive HSC. Some\nother studies have demonstrated that leadership thinking based\non extensive information will enhance configuration decisions\n[65], [66]. Therefore, we have the following hypothesis.\nHypothesis H3: IL has a moderating effect on BDA capabilities and\nRSC.\nIL is essential for managing the same supply chain tasks\nin a new way [60]. Many important decisions must be made\nduring disasters caused by the COVID-19 pandemic [4], and\ninnovative leadership can foster innovative thinking by the\nteam and result in innovative solutions that can be helpful for\nhumankind. Unique solutions can involve deploying robots to\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\nBAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 5\nscreen for COVID-19 in the community, using drones to carry\ntesting kits and essential drugs to remote places, producing PPE\nfrom alternative materials at low cost, using specialized logistics\nfor distribution [109], protecting employees from infection, and\nchanging supply chain processes [67].\nThe demonstration of innovative thinking by top management\nbuilds confidence in BDA application among other employees,\nand the data can be useful in supply chain innovation. The greater\nthe IL in the organization, the greater is the activation of BDA\ninformation processing capabilities on SCI [20], [49]. Therefore,\nwe propose the following hypothesis.\nHypothesis H4: IL has a moderating effect on BDA capabilities and\nsupply chain innovation.\n3) SCI, Responsiveness, and Resilience: During a pandemic\nsuch as COVID-19, the configuration of the HSC requires signif\u0002icant modifications to enable changes in business processes that\ncan benefit patients and facilitate the economical distribution\nof essential medical goods and devices. It may be necessary to\nrestructure old supply chain structures to transform them into\nnew structures and develop innovative approaches and capabil\u0002ities [68]. Changing suppliers and supply chain processes can\nenable the application of innovative technologies and thereby\ndrive agile and responsive processes to counter changes. We,\nhence, have the following hypothesis.\nHypothesis H5: SCI has a positive relationship with RSC.\nThe HSC can be optimized by reconfiguring its resources\nto make the healthcare supply more resilient [4]. The ability\nto quickly configure the supply chain will enhance the ability\nof the HSC to become responsive and effectively fight against a\npandemic, which will save time and efforts while using resources\nmore effectively. This ultimately brings an added benefit to\nthe society [4], [69]. Changing the supply chain configuration\nand quickly responding to market changes will help to reduce\nnegative effects from repeated risk and improve supply chain\nresilience [11]. Therefore, we build the following hypothesis.\nHypothesis H6: RSC has a positive relationship with SCR for pan\u0002demic response.\nAn innovative supply chain design influences the choice of\nvendors and results in cooperation with important suppliers as\nwell as impacting supply chain efficiency and quality-related\npractices [14]. Working in a collaborative manner with suppliers\nand integrating operations for improved efficiency can result in\ninnovation [70]. In the HSC, innovation initiatives are important\nto meet increasing demands for better services in a timely\nmanner for patients [71]. Innovation in the HSC can reduce the\ndistance between warehouses and affected areas [72]. Innovation\ncan also contribute to the cheaper manufacturing of products and\nmake them available quickly based on demand. Only innovative\napproaches can make the HSC more resilient and help manage\npandemic situations more effectively [11], [72]. Therefore, we\nhave the following hypothesis.\nHypothesis H7: SCI has a positive relationship with SCR for pan\u0002demic response.\nIII. RESEARCH METHODS1\nA multimethod approach [73] is used in this study. Multi\u0002method approaches are commonly utilized to validate findings in\ntechnology and operations management research [73]\u2013[75]. This\narticle was conducted in the following two phases: I) quantitative\nsurvey and testing of theoretical model using variance-based\nSEM and II) semistructured qualitative questionnaires and the\u0002matic analysis. Saunders et al.[104] pointed out a very important\npart of academic research, i.e., data collection. Data are linked\nwith the answering the research questions. Therefore, \u201cwhat type\nof data is required\u201d and \u201cwhat techniques are necessary to collect\nthe data\u201d are both critical decisions in any empirical research\nstudy. Saunders et al. [104] conceptualized the data collection\nprocess as the central part of the research onion. Selection of\ndata collection techniques and analysis processes are critical to\nproduce a good research output. We previously indicated that\nthe multimethod approach was used to reach to the \u201ccentre of\nthe onion,\u201d i.e., answer the central research questions. We did\nnot simply peel off the important outer layers of the onion and\nthrown them away. We had carefully selected the techniques,\nwith a combination of quantitative and qualitative techniques,\nin both data collection and analyses. The rationale behind using\nthese methods is as follows: First, to overcome the limitation of\nempirical surveys (i.e., to have the triangulation effect). Second,\nto gain richer insights from the practitioner\u2019s perspectives. In\nthe first phase, we used a structured questionnaire and further\nanalyzed the theoretical model. In such a process, we established\nthe links and contributed to the literature. However, the primary\ndata/empirical survey did not provide rich and deeper insights\nabout the underlying mechanism, which was made possible\nthrough the use of qualitative surveys with selected respondents\n(considered from the same sampling frame from phase 1). The\nqualitative analysis further provided understanding about the\nrelationships to a greater extent that was not possible with quan\u0002titative study. Results of the qualitative study can also verify if the\nquantitative findings are valid or not. This enhances research rig\u0002ors. The research flowchart is presented in Fig. 2. Note that this\napproach follows the philosophy proposed by Choi et al. [73].\nA. Construct Operationalization\nThe survey items were adopted from the existing literature.\nThe five-item BDA construct was taken from Arunachalam\net al. [47], the six-item SCI construct was adapted from Kwak et\nal. [11], the five-item RSC construct consisting of five items was\nadapted from Parmigiani et al. [68], the four-item IL construct\nwas adapted from Yoon et al. [20] and the eight-item SCR for\npandemic response construct was adapted from Sabegh et al.\n[4]; and Kwak et al. [11]. The details are provided in Table A1\n(Online Supplementary Appendix A).\nB. Sampling and Data Collection\nThe target population for this article comprised general man\u0002agers, senior managers, managers, junior managers, and other\n1The authors sincerely thank a reviewer for reminding us the importance to\nclarify the idea behind the multimethod study.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n6 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT\nFig. 2. Research flowchart.\nhealthcare sector professionals responsible for sourcing, man\u0002ufacturing, logistics, distribution, research, and development.\nThe companies were selected from among the most relevant\ndatabases in the context of our article, namely the \u201cInnovation\nPharmaceutical Association of South Africa,\u201d \u201cGeneric and\nBiosimilar Medicines of South Africa,\u201d and \u201cBioPharmGuy.\u201d\nThe total number of members listed in these directories com\u0002bined is approximately 1200.\nIt was determined that 30 representative participants would be\na reasonable minimum recommendation for a pilot study [76],\n[77]. The questionnaire was developed based on a five-point\nLikert scale (1 = Strongly Disagree, 2 = Disagree, 3 = Neutral,\n4=Agree, 5=Strongly Agree). The questionnaire was e-mailed\n(using Google Forms) to 37 managers for a pilot survey assess\u0002ment. After the responses and comments were received, seven\nquestions were reworded before distributing the final survey.\nSampling targeted a total of 550 potential respondents, who\nwere selected using random sampling technique, and the final\nquestionnaire was sent (using Google Forms) to two respon\u0002dents from each company. No incentive was offered or given\nto survey participants. After two rounds of follow-up, a total\nof 190 responses were received, representing a response rate of\n34 percent. Questionnaires were received from 78 respondents\nwere received at the end of April 2020, and after conducting\nfollow-up, we received data from an additional 112 respondents\nat the end of May 2020. We did not receive any incomplete\nsubmissions, as the questionnaire was designed only to accept\ncomplete submissions.\nThe demographic profile of the survey participants is pre\u0002sented in Table A2 (Online Supplementary Appendix A). The\nhighest number of responses was received from profession\u0002als working in the healthcare industry for over 15 years, and\nmost responses were received from companies operating in\nSouth Africa for more than 20 years. Responses were received\nfrom pharmaceutical product and medical device manufacturers,\nbiotechnology companies, medical product distributors, medical\nretailers, and clinical research institutes. The largest number of\nresponses was received from biotech companies, followed by\nmedical device manufacturers. The analysis also indicated that\nmost responses were received from big companies with annual\nturnover of more than 50 million South African Rands.\nC. Nonresponse Bias (NRB)\nSince data were received in two phases, we checked NRB by\njudging the first and second wave of responses, with the second\nwave (i.e., late responses) being regarded as a control group\nstanding in for those who did not respond (for example, see\n[80]). Homogeneity of variance test was performed to determine\nif there was any difference between both sets of responses. The\nnonsignificant results indicated that our article was free from\nNRB.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\nBAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 7\nFig. 3. BDA capabilities.\nD. Qualitative Study Methodology\nThe semistructured, open-ended questionnaire (Online Ap\u0002pendix B) used to collect qualitative data was adapted from\nSheng et al. [85] and consisted of the following two sections:\nthe first section intended to capture the demographic profile of\nrespondents, and the second section consisted of seven questions\nintended to capture the potential of BDA in developing resilient\nHSCs. To avoid any form of common method bias (CMB), a\nclear note at the top of the questionnaire explained that the\ndata collection is purely for academic purpose and the names\nand personal details of the respondents will not be disclosed at\nany point of time. The number of questions was kept minimum\nto avoid respondents becoming bored or losing patience while\nanswering such descriptive questions. The questionnaire was\ncreated on Google Forms and the link was emailed to 30 po\u0002tential respondents who were part of the initial empirical survey\nperformed during phase 1 of this article. The request to complete\nthe questionnaire was sent at the end of January 2021, and all of\nthe data was received by early February 2021. Responses were\nreceived from all 30 participants. Previous studies have used 20\nsamples; therefore, a sample size of 30 is acceptable for this\nstudy [86]. Finally, the thematic analysis acts as an input for\nthe triangulation of the results obtained from the previous stage.\nExcel was used to perform the coding, followed by grouping\nunder subthemes and extracting the main themes.\nIV. DATA ANALYSIS\nThe data obtained during the primary study in phase 1 is\ndepicted in Figs. 3\u20137. Fig. 3 indicates that there are five items\n(BDA1, BDA2, BDA3, BDA4, and BDA5) that were used to\nmeasure the latent construct BDA capabilities. It also shows the\nresponses received during the primary survey for instance if we\nlook at the item BDA1, out of total 190 responses: 5 selected\nstrongly disagree, i.e., 2.6%; 1 selected disagree, i.e., 0.52%, 0\nneutral, 79 selected agree, i.e., 41.57%, 105 selected strongly\nagree, i.e., 55.26%.\nFig. 4 indicates that there are four items (IL1, IL2, IL3, and\nIL4) that were used to measure the latent construct IL. It also\nshows the responses received during the primary survey.\nFig. 4. Innovation leadership.\nFig. 5. Supply chain innovation.\nFig. 6. Responsive supply chain.\nFig. 7. Supply chain resilience.\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n8 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT\nFig. 5 indicates six items (SCI1, SCI2, SCI3, SCI4, SCI5,\nand SCI6) were used to measure the latent construct supply\nchain innovation. It also shows the responses received during\nthe primary survey.\nFig. 6 indicates five items (RS1, RSC2, RSC2, RCS3, RSC4,\nand RSC5) were used to measure the latent construct responsive\nsupply chain. It also shows the responses received during the\nprimary survey.\nFig. 7 indicates eight items (SCR1, SCR2, SCR3, SCR4,\nSCR5, SCR6, SCR7, and SCR8) were used to measure the latent\nconstruct supply chain resilience. It also shows the responses\nreceived during the primary survey.\nA. SEM Applying the Partial Least Squares Technique\nTwo types of SEM techniques are commonly applied, they\nare namely: 1) the covariance-based method, and 2) the partial\nleast squares method. PLS-SEM is widely used by researchers\nin various fields [79]. Many research papers are available that\ncritically examined the pros and cons of PLS-SEM [100]\u2013[102].\nIn this article, we followed the guidelines of Hair et al. [103].\nWhen the objective of this article is mainly on \u201cprediction and\nexplanation,\u201d then PLS-SEM is recommended. For the case with\n\u201creflective model specification,\u201d both PLS-SEM and CB-SEM\ncan be used. For smaller sample sizes, PLS-SEM is recom\u0002mended. Keeping in mind all these points, we opted for the\nPLS-SEM technique. The software WarpPLS (version 6.0) was\napplied for conducting the SEM analysis. We do understand\nthat no single method is perfect. There are pros and cons of\neach method. To enhance research rigors, we have adopted the\nmultimethod approach (see Fig. 2).\nB. Common Method Bias\nThe problems associated with the effect of method bias have\nlong been highlighted in the literature [97]\u2013[99]. In many cases,\nthe instructions at the beginning of a questionnaire can influence\nresponses in a particular way (e.g., by implying the desirability\nof certain responses), thereby introducing common variation\namong the indicators and contaminating key results by inflating\npath coefficients due to the introduction of multicollinearity.\nFollowing the guidelines of MacKenzie and Podsakoff [98],\nwe carefully designed the questionnaire. First, to ensure the\nquestions could be easily understood, we pretested the questions\nin our preliminary trial survey. Second, we selected respondents\nwho had the necessary experience about BDA in the healthcare\nindustry. Third, we avoided the use of highly complex and\nabstract questions. Fourth, we took away \u201citem ambiguity\u201d by\nusing a clear and concise language. Fifth, we did not keep any\ndouble-barrelled questions. Sixth, we refocused questions to ask\nabout the current pandemic states because this would minimize\nefforts required for retrieval of information.\nIn addition, we performed Harman\u2019s single-factor test on all\nthe constructs [108]. We conducted the standard exploratory\nfactor analysis by selecting the principal component. We further\nchecked the unrotated factor solution to determine the number of\nfactors. The findings did not produce any individual dominating\nfactor which indicates nonexistence of CMB.\nLastly, the research team checked whether CMB was present\nby applying a full collinearity test to examine both vertical\nand lateral collinearities [78], [79]. If the variance inflation\nfactors (VIFs) are above 3.3, the collinearity issues exist and\nthe model suffers the CMB problem. This is a highly sensitive\nCMB criterion that tends to identify CMB where other methods\nprovide false negatives [78]. In the current study, the VIF values\nwere all found to be lower than 3.3, and we can, therefore,\nconclude that our model does not suffer the CMB problem.\nC. Measurement Model\n1) Validity and Reliability: The internal consistency of the\nlatent constructs was checked, and Cronbach\u2019s alpha test was\nused to check the reliability of the instrument. All Cronbach\u2019s\nalpha values except IL (0.658) and RSC (0.621) were higher\nthan 0.70 (BDAC: 0.868, SCI: 0.893, SCR: 0.749, IL\u2217BDAC:\n0.912). Since the measurement of these constructs was sensitive\nto the number of items in the respective scales, the research team\nalso checked the composite reliability of all latent constructs.\nComposite reliability is a preferred alternative to Cronbach\u2019s\nalpha test in the context of the data analysis method employed,\nand composite reliability values above 0.60 are acceptable in so\u0002cial science research. The results indicate acceptable reliability\n(BDAC: 0.908, IL: 0.793, SCI: 0.920, RSC: 0.771, SCR: 0.812,\nIL\u2217BDAC: 0.929). Average variances extracted (AVEs) were\ncalculated to assess convergent validity based on the widely used\nthreshold of 0.50 [81], [82]. The values obtained (BDAC: 0.670,\nIL: 0.598, SCI: 0.662, RSC: 0.505, SCR: 0.599, IL\u2217BDAC:\n0.542) suggest that our measurement model displays acceptable\nconvergent validity.\nUsing square roots of AVEs for the latent constructs in com\u0002bination with latent construct correlations, the research team\nalso investigated discriminant validity by following the Fornell\u2013\nLarcker criterion, i.e., for any latent variable, the square root\nof the AVE must be higher than its correlation with any other\nlatent variable [81], [82]. The results are showcased in Table\nA3 (Online Supplementary Appendix). These results suggest\nthat our measurement model displays acceptable discriminant\nvalidity.\n2) Model Fit and Quality Indices: The quality of the research\nmodel was checked using both classic model fit indices and\nmore modern causality assessment indices, as outlined in the\nfollowing. The classic model fit indices used were the aver\u0002age path coefficient (APC), average R-squared (ARS), average\nadjusted R-squared (AARS), average block variance inflation\nfactor (AVIF), and average full collinearity VIF (AFVIF) [83].\nIt is recommended that the p values for APC, ARS, and AARs\nbe less than or equal to 0.05, and these conditions were met (APC\n= 0.514, p",
    "title": "Roles of Innovation Leadership on Using Big Data Analytics to Establish Resilient Healthcare Supply Chains to Combat the COVID-19 Pandemic: A Multimethodological Study",
    "publication_date": "2024-01-01",
    "cited_by_count": 116,
    "topics": "Building Resilient Supply Chain, Impact of Big Data Analytics on Business Performance, Machine Learning in Smart Healthcare",
    "keywords": "Pandemic, 2019-20 coronavirus outbreak, Supply Chain Resilience, Organizational Resilience",
    "concepts": "Pandemic, Coronavirus disease 2019 (COVID-19), Big data, Health care, Supply chain, 2019-20 coronavirus outbreak, Analytics, Business, Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), Data science, Knowledge management, Computer science, Marketing, Economics, Virology, Medicine, Economic growth, Data mining, Disease, Pathology, Infectious disease (medical specialty), Outbreak",
    "text_type": "full_text",
    "openalex_rank": 13,
    "num_tokens": 8144,
    "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf",
    "pdf_urls_by_priority": [
      "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf"
    ],
    "successful_pdf_url": "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf",
    "url": "https://ieeexplore.ieee.org/ielx7/17/4429834/09519642.pdf",
    "snippets": {
      "s1": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT 1 Roles of Innovation Leadership on Using Big Data Analytics to Establish Resilient Healthcare Supply Chains to Combat the COVID-19 Pandemic: A Multimethodological Study Surajit Bag , Shivam Gupta , Tsan-Ming Choi , and Ajay Kumar Abstract\u2014This article empirically examines the effect of big data analytics (BDA) on healthcare supply chain (HSC) innovation, supply chain responsiveness, and supply chain resilience under the moderating effect of innovation leadership in the context of the COVID-19 pandemic. The scanning interpretation\u2013action\u2013 performance model and organization information processing theory are used to explain BDA, HSC innovation, responsiveness, and resilience relationships. First, the hypotheses were tested using data collected from 190 experienced respondents working in the healthcare industry. Our structural equation modeling analysis using the partial least squares (PLS) method revealed that BDA capabilities play a pivotal role in building a responsive HSC and improving innovation, which has contributed to resilience during the current pandemic situation. High innovation leadership strengthens the effect of BDA capabilities on HSC innovation. High innovation leadership also increases the effect of BDA capabilities on responsiveness. Second, we validated and supplemented the..",
      "s2": "..empirical research findings using inputs collected in 30 semistructured qualitative questionnaires. Our article makes a unique contribution from the perspective of innovation leaderships. In particular, we argue that the role of innovative leadership in the COVID-19 pandemic situation is critical as it indirectly affects HSC resilience when BDA is in place. Index Terms\u2014Big data analytics (BDA), COVID-19, healthcare supply chain, multi-methods research, responsive supply chain, supply chain innovation, supply chain resilience. Manuscript received February 28, 2021; revised May 5, 2021 and July 9, 2021; accepted July 25, 2021. Review of this manuscript was arranged by Department Editor N. Damij. (Corresponding author: Tsan-Ming Choi.) Surajit Bag is with the Department of Supply Chain Management and Information Systems, Rabat Business School, International University of Rabat, Rabat 11103, Morocco (e-mail: surajit.bag@gmail.com). Shivam Gupta is with the Department of Information Systems, Supply Chain and Decision Making, NEOMA Business School, 59 Rue Pierre Taittinger, 51100 Reims, France (e-mail: shivam.gupta@neoma-bs.fr). Tsan-Ming Choi is with the..",
      "s3": "..Department and Graduate Institute of Business Administration, College of Management, National Taiwan University, Taipei 10617, Taiwan (e-mail: tmjchoi@gmail.com). Ajay Kumar is with the AIM Research Centre on Artificial Intelligence in Value Creation, EMLYON Business School, 69130 \u00c9cully, France (e-mail: akumar@em-lyon.com). Color versions of one or more figures in this article are available at https: //doi.org/10.1109/TEM.2021.3101590. Digital Object Identifier 10.1109/TEM.2021.3101590 I. INTRODUCTION GLOBAL spending on healthcare is expected to dramatically increase in the near future. This partially relates to changing consumer requirements [1] as well as the higher expectation on timely response to disasters [2]\u2013[4]. Most recently, the COVID-19 pandemic has made it clear that healthcare supply chains (HSCs) are far from perfect. Not much improvements were made from the experiences acquired during various prior epidemics such as middle east respiratory syndrome (MERS) and severe acute respiratory syndrome (SARS) [5], [6]. Massive disruptions in HSCs have reached the level of a global crisis. The availability of personal protective..",
      "s4": "..equipment (PPE), medical equipment, and lifesaving drugs has been severely limited [7]\u2013[9]. Under COVID-19, high demands have challenged the HSC, highlighting the need to manage supply chains differently in crisis situations [9], [10]. Undoubtedly, supply chain responsiveness and innovation are essential to build a resilient HSC to combat the COVID-19 pandemic when the demand uncertainties are extremely high [11], [12]. In the literature, Peeri et al. [6] pointed out the need to focus on using digital technologies to monitor pandemic situations. In particular, big data analytics (BDA) is a powerful tool to help [13]. For example, BDA supported inventory management of medical supplies during emergency responses is critical to ensure the distribution of appropriate supplies [14]. Medical devices with high volumes of data can apply BDA to understand trends and future requirements of PPE. This enhances the management and planning of activities in HSCs [1]. Digital technologies can remove barriers in pharmaceutical supply chains and improve flexibility and innovation related to drug supplies, thereby enhancing coordination, information sharing, and minimizing wastes [2], [15]. In healthcare, BDA is valuable for environmental-scanning (forecasting and observation) purposes [10]..",
      "s5": "... It helps predict the results of drug administration, and analyze patient categorization and emergency response [16], all of which are of paramount significance during a pandemic like COVID-19 [13]. BDA not only can sense information, it can also enhance interpretation to support key business decision making [17], [18] in a timely manner [110]. Prior studies have shown that innovative supply chains have the ability to manage risks, determine an \u00a9 IEEE 2021. This article is free to access and download, along with rights for full text and data mining, reuse and analysis. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 2 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT organization\u2019s competitive position [11], and enhance the interpretation of key information as well as strategy development [19]. Note that innovation leaderships (IL) can improve supply chain innovation and, hence, improve efficiency [20]. Supply chain responsiveness aims to reduce manufacturing throughput and transportation/distribution lead times [21]. BDA capabilities can assist in building a responsive supply chain that positions resources and key players (suppliers, transporters, distributors) at the right places to gain a competitive advantage [22]. BDA improves..",
      "s6": "..productivity in the supply chain process by offering an added level of flexibility [23]. Moreover, following the arguments in [24], BDA can positively affect supply chain transparency. Note that a few studies have examined the role of BDA in the HSC during pandemics (e.g., [25]\u2013[29]). However, no prior research has comprehensively examined the links between BDA and supply chain responsiveness and innovation together. This article aims to fill this gap in the context of COVID-19. A recent insightful study by Dubey et al. [94] used the organization information processing theory (OIPT) to explain the relationships between blockchain technology and operational supply chain transparency. Dubey et al. [94] further argued that blockchain technology and operational supply chain transparency can further enhance collaboration among actors engaged in disaster relief operations and this finally leads to improved supply chain resilience. This study supplements [94] and others in the related domain, and contributes to the supply chain resilience literature. It is noteworthy to mention one more important recent study on supply chain resilience by Dubey et al. [95] who highlighted the importance of BDA in enhancing information processing capacity and supply chain resilience for faster recovery after any disruptions. However, IL (which is related to supply chain..",
      "s7": "..resilience) for HSC innovation (SCI) is still an underexplored area. Carmeli et al. [64] pointed out that IL increases strategic fit (internal/external) and further improves firm performance. Hence, the role of leaders (irrespective of whether they are political leaders of the country or leaders of companies) is crucial in this pandemic time. To be specific, IL includes the proper way to encourage individuals to take various initiatives, develop a transparent performance measurement system, and build an environment in which quality relationships would be treasured. Having good IL will lead to increased creativities in the organization [64]. Innovation is related to \u201cout-of-the-box\u201d thinking and introducing something new such as new ideas, methods, or devices. Innovations in the sphere of healthcare products and services are keys to combat the COVID-19 pandemic and yield quick recovery from the current state. Thus, innovative leadership is critical for establishing a resilient healthcare system. Motivated by the importance of BDA in HSCs and the critical role of IL, we study the moderating effect of IL on the contribution of BDA to SCI and responsiveness during COVID-19 pandemic. This article is unique from a few perspectives: 1) we examine the effect of BDA on HSC responsiveness (RSC..",
      "s8": "..) and innovation; 2) we investigate the effects of RSC and innovation on supply chain resilience. 3) We adopt the multimethodological approach in deriving more scientifically sound results. The main research questions that the study sought to answer are as follows. RQ1: What are the effects of BDA on (i) supply chain responsiveness and (ii) supply chain innovation under the moderating effect of IL during the COVID-19 pandemic? RQ2: What are the effects of (i) responsive supply chain and (ii) supply chain innovation on HSC resilience (SCR) during COVID-19 pandemic? The theoretical model is built through the lenses of OIPT and scanning interpretation\u2013action\u2013performance (SIAP) modeling. We argue that BDA is useful for environmental scanning and information processing to drive SCI (interpretation of key information), which helps establish the responsive supply chain (actions). Finally, SCI and responsiveness are essential to build SCR (performance). During part 1 of the study, data were collected in South Africa using a structured questionnaire and hypotheses were tested using structural equation modeling (SEM) applying the partial least squares technique (PLS-SEM). In the second part, a thematic analysis was performed using the data obtained from 30 semistructured qualitative questionnaires...",
      "s9": "..The themes that emerged from this second-phase highlight major dimensions associated with BDA in the HSC. The rest of this article is organized as follows. Section II presents the theoretical background and hypotheses, Section III provides the methods used for conducting the analysis. Section IV presents the data analysis. Finally, Section V and Section VI concludes this article. II. THEORETICAL BACKGROUND AND HYPOTHESES DEVELOPMENT A. Organization Information Processing Theory OIPT theory proposes that organizations must enhance their information processing capacity to survive in an increasingly uncertain business environment [30], [31]. The COVID-19 pandemic has brought tremendous uncertainties to the lives of both humans and businesses [32]. Uncertainty is driving the need for building information processing capability [33], and companies involved in HSC need to leverage disruptive Industry 4.0 technologies such as BDA to scan and process information and make strategic decisions. OIPT explains how firms can develop the information processing capability during the COVID-19 pandemic to assess external information such as supply crises, market demands, sales and competitors\u2019 distribution activities, rate of infection spreading, number of infected cases, number of recoveries, number of deaths, and clinical trials monitoring and outcomes. Past studies have used OIPT to explain disruptions in supply chains [34]..",
      "s10": "... We argue that BDA enhances firms\u2019 information processing capability during these uncertain pandemic times. Furthermore, supply chain innovation and responsiveness reduce uncertainty by fostering resilience. However, OIPT cannot single-handedly explain the entire mechanism (BDA\u2013 innovation\u2013responsiveness\u2013resilience). Therefore, we supplement it with the SIAP model to better explain these relationships. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. BAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 3 Fig. 1. Theoretical model. B. SIAP Model For the relationships among BDA, SCI, RSC, and SCR, we can refer to Yu et al. [35], and the adopted SIAP model [36] and OIPT [37]. Whether operating at a local or international level, every business is influenced by external factors. Situations change rapidly due to variations in political, environmental, or technological scenarios. Running a business is, therefore, an uneasy task, especially during turbulent times such as the COVID-19 pandemic. The SIAP model argues that firms adjust in the business environment..",
      "s11": "..by following three basic steps, namely \u201cscanning, interpreting, and responding\u201d [38]. First, organizations scan information that can influence performance [36]. Accuracy is key to any effective environmental scanning [39], and BDA can be used to scan important data to generate useful information from various internal and external sources to gain rich business insights and develop competitive edges [40], [41]. BDA fundamentally works by scanning information in the changing business environment [42], [43]. Common sources of big data include social media, websites, shop floor machines, meters, and sensors. There is enormous potential for BDA applications in the HSC [16]. BDA has been demonstrated to be a useful scanning tool that can equip organizations with the ability to scan risks and reconfigure resources and competencies [44], [45]. The potential of artificial intelligence (AI) and BDA in fighting COVID-19 has been confirmed in the literature (see, e.g., [46]). BDA can be used to model the spread of infection during a pandemic, monitor clinical trials, and outcomes, which can be useful for framing policy and controlling infection [16]. Big data generated from social media, smart phones, and other digital equipment can be immensely helpful in controlling..",
      "s12": "..the spread of COVID-19 [13]. The second step in SIAP is \u201cinterpretation,\u201d whereby management uses diverse models as information processing methods to understand and label information [36]. Managers can make use of information to identify opportunities and threats. We argue that firms should resort to supply chain innovation involving all stakeholders to pursue creative methods and services. The third step is \u201caction,\u201d i.e., the strategic initiatives that the firm undertakes to adapt in the changing business environment, which can range from slight shifts in procedures of the business processes to major alterations in product, sales, and distribution strategies. We argue that RSC is a strategic initiative to respond facing the COVID-19 pandemic. The final step of the SIAP model is \u201cperformance\u201d [36]. We argue that SCR is the final outcome that every healthcare organization intends to achieve during pandemic situation. C. Theoretical Model and Research Hypotheses The theoretical model built based on the abovementioned discussion is presented in Fig. 1. Although big data drives supply chain innovation, studies on BDA methods that can help organizations to enhance innovation are limited [47]. In addition, research initiatives on leveraging BDA to unlock values require further investigation [48]. Previous studies have shown how BDA can positively influence supply chain..",
      "s13": "..sustainability [40], [48]. We argue that BDA has a positive association with RSC and innovation, whereby innovation has the ability to develop highly responsive supply chains, and innovation and responsiveness lead to supply chain resilience. In this article, we also introduce \u201cIL\u201d as a moderating variable to examine its effect on the relationships between \u201cBDA and health care supply chain responsiveness\u201d and \u201cBDA and SCI.\u201d 1) BDA and Supply Chain Responsiveness and Innovation: Nowadays, the numbers of actors and products in modern supply chains are much higher than before. Organizations generally prefer big data solutions to curb problems in the supply chain network [49]. BDA involves collecting, managing, and processing a high volume of data generated from various sources. These This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 4 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT data can be both structured and unstructured in form, and BDA can be used to analyze them and unlock their value [50]. Under COVID-19, the pandemics create a disaster situation and disrupt supply chains because the local and international borders remain closed to prevent the spread of infection. In this type of situation, rapid action is required to make radical changes..",
      "s14": "..in the supply chain that are only possible using BDA information processing capabilities for scanning environmental information [51]. In an HSC, big data is generated from internal enterprise resource planning (ERP systems) and external sources (social media, mobile devices, data portals, and data market platforms). The non structured query language (NoSQL) graph databases are useful for optimizing and configuring supply chains. The Apache Hadoop platform is immensely helpful for managing high volumes of data, and MapReduce helps perform the analytics part to extract information [52]. During pandemic times, batch analysis is untenable to manage when a vast amount of data are generated in the HSC. However, the Lambda architecture can analyze real-time data flows by supporting data stream analytics. Every minute, the data of infected patients, status of infection spread, current drugs, and other medical device requirements at different locations can be gathered using an advanced ICT platform. Further data streams can be analyzed using complex event processing programs. AutoID digital technologies can be useful for tracking purposes as well [49], [53]. BDA can extract information that can be useful for making decisions related to HSC configurations [54]. However, it is important that data scientists and data analysts would closely monitor and control the quality of data to prevent inaccurate information generation [..",
      "s15": "..55]. Therefore, we establish the following hypothesis. Hypothesis H1: BDA capabilities have a positive relationship with RSC. BDA can also offer new opportunities for supply chain innovation [56]. New vaccines and drugs are required to combat pandemic situations [57]. Moreover, the shortage of equipment such as PPE for front-line doctors and healthcare workers can be resolved by securing specialized PPE and making alternative PPE products using 3-D printing and advanced manufacturing. In addition, digital contact tracing apps can play an important role by tracking disease spread [58]. Innovation can involve the development of new products with unique features, alternative manufacturing methods, ecofriendly raw materials for manufacturing, new approaches to transportation and distribution, and the development of new processes that can yield huge benefits for society at large as well as firms [59]. Big data generation capabilities, data integration and management capabilities, advanced analytics, and data visualization capabilities can be immensely useful for supply chain innovation [49], [59]. Therefore, we have the following hypothesis. Hypothesis H2: BDA capabilities have a positive relationship with SCI. 2) Moderating Effects of IL: In this article, we have used OIPT to explain the role of BDA in information processing for reducing uncertainties. However, in the literature, Ha..",
      "s16": "..u\u00dfmann et al. [32, p. 81] highlighted certain shortcomings of the original OIPT theory, which includes the point that interpersonal characteristics and information restrictions are not taken into account. Interpersonal characteristics here include leadership, teamwork, etc. To overcome the limitations of the original theory, we made reference to Hambrick and Mason [96] in which the authors conceptualized the \u201cupper echelons\u201d perspective and argued that firm performance is shaped by managerial background characteristics. \u201cUpper echelons\u201d based leadership theory can put some light on the observable managerial characteristics that the leader can bring to an administrative circumstance. Observable characteristics such as age, functional tracks, other career experiences, education, socioeconomic roots, financial position, and group characteristics would all influence the strategic choices made by top management and leaders [96]. Undoubtedly, product innovation is one of the strategic choices that leaders make for improving firm performance [96]. As a remark, Carmeli et al. [64] argued that IL can improve strategic fit and further enhance the firm performance. IL is related to the innovative nature of organization leaders that ranges from emphasizing on teamwork, clarifying individual responsibility, providing clear feedbacks to employees, emphasizing on task orientation, encouraging initiatives, and developing trust among employees [64]. In an uncertain business environment under..",
      "s17": "..COVID-19, it is very difficult to forecast and plan activities. Disasters and pandemic situations exacerbate the uncertainty, and if leaders continue to work with the same approach used under normal circumstances, then their businesses will not survive the impact. Innovative leadership can be highly effective for managing business challenges during pandemic situations. Applying innovative thinking to leadership tasks can spur employees to begin thinking in innovative ways and further use BDA to configure the HSC and pull the firm out of danger [60]. Learning and teamwork are required to improve environmental training and configure supply chains for sustainability outcomes [61], [62]. Training forms part of the Industry 4.0 delivery system and is important in sustainable development [63]. All of the abovementioned human resource factors are antecedents of IL [20]. IL improves organizational performance and contributes to a firm\u2019s strategic positioning within the business environment [64]. Importance of human resource management, involving IL and responsive management for supply chain sustainability, is highlighted in [12], which argued that the greater the IL, the more pronounced the effect of BDA information processing capabilities on building responsive HSC. Some other studies have demonstrated that leadership thinking based on extensive information will enhance configuration decisions [65], [66]. Therefore, we have the following hypothesis. Hypothesis..",
      "s18": "..H3: IL has a moderating effect on BDA capabilities and RSC. IL is essential for managing the same supply chain tasks in a new way [60]. Many important decisions must be made during disasters caused by the COVID-19 pandemic [4], and innovative leadership can foster innovative thinking by the team and result in innovative solutions that can be helpful for humankind. Unique solutions can involve deploying robots to This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. BAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 5 screen for COVID-19 in the community, using drones to carry testing kits and essential drugs to remote places, producing PPE from alternative materials at low cost, using specialized logistics for distribution [109], protecting employees from infection, and changing supply chain processes [67]. The demonstration of innovative thinking by top management builds confidence in BDA application among other employees, and the data can be useful in supply chain innovation. The greater the IL in the organization, the greater is the activation of BDA information processing capabilities on SCI [20], [49]. Therefore, we..",
      "s19": "..propose the following hypothesis. Hypothesis H4: IL has a moderating effect on BDA capabilities and supply chain innovation. 3) SCI, Responsiveness, and Resilience: During a pandemic such as COVID-19, the configuration of the HSC requires significant modifications to enable changes in business processes that can benefit patients and facilitate the economical distribution of essential medical goods and devices. It may be necessary to restructure old supply chain structures to transform them into new structures and develop innovative approaches and capabilities [68]. Changing suppliers and supply chain processes can enable the application of innovative technologies and thereby drive agile and responsive processes to counter changes. We, hence, have the following hypothesis. Hypothesis H5: SCI has a positive relationship with RSC. The HSC can be optimized by reconfiguring its resources to make the healthcare supply more resilient [4]. The ability to quickly configure the supply chain will enhance the ability of the HSC to become responsive and effectively fight against a pandemic, which will save time and efforts while using resources more effectively. This ultimately brings an added benefit to the society [4], [69]. Changing the supply chain configuration and quickly responding to market changes will help to reduce negative effects from repeated risk and improve supply chain resilience [11]. Therefore, we build the following hypothesis. Hypothesis H..",
      "s20": "..6: RSC has a positive relationship with SCR for pandemic response. An innovative supply chain design influences the choice of vendors and results in cooperation with important suppliers as well as impacting supply chain efficiency and quality-related practices [14]. Working in a collaborative manner with suppliers and integrating operations for improved efficiency can result in innovation [70]. In the HSC, innovation initiatives are important to meet increasing demands for better services in a timely manner for patients [71]. Innovation in the HSC can reduce the distance between warehouses and affected areas [72]. Innovation can also contribute to the cheaper manufacturing of products and make them available quickly based on demand. Only innovative approaches can make the HSC more resilient and help manage pandemic situations more effectively [11], [72]. Therefore, we have the following hypothesis. Hypothesis H7: SCI has a positive relationship with SCR for pandemic response. III. RESEARCH METHODS1 A multimethod approach [73] is used in this study. Multimethod approaches are commonly utilized to validate findings in technology and operations management research [73]\u2013[75]. This article was conducted in the following two phases: I) quantitative survey and testing of theoretical model using variance-based SEM and II) semistructured qualitative questionnaires and thematic analysis. Saunders et al.[..",
      "s21": "..104] pointed out a very important part of academic research, i.e., data collection. Data are linked with the answering the research questions. Therefore, \u201cwhat type of data is required\u201d and \u201cwhat techniques are necessary to collect the data\u201d are both critical decisions in any empirical research study. Saunders et al. [104] conceptualized the data collection process as the central part of the research onion. Selection of data collection techniques and analysis processes are critical to produce a good research output. We previously indicated that the multimethod approach was used to reach to the \u201ccentre of the onion,\u201d i.e., answer the central research questions. We did not simply peel off the important outer layers of the onion and thrown them away. We had carefully selected the techniques, with a combination of quantitative and qualitative techniques, in both data collection and analyses. The rationale behind using these methods is as follows: First, to overcome the limitation of empirical surveys (i.e., to have the triangulation effect). Second, to gain richer insights from the practitioner\u2019s perspectives. In the first phase, we used a structured questionnaire and further analyzed the theoretical model. In such a process, we established the links and contributed to the literature. However, the primary data/empirical..",
      "s22": "..survey did not provide rich and deeper insights about the underlying mechanism, which was made possible through the use of qualitative surveys with selected respondents (considered from the same sampling frame from phase 1). The qualitative analysis further provided understanding about the relationships to a greater extent that was not possible with quantitative study. Results of the qualitative study can also verify if the quantitative findings are valid or not. This enhances research rigors. The research flowchart is presented in Fig. 2. Note that this approach follows the philosophy proposed by Choi et al. [73]. A. Construct Operationalization The survey items were adopted from the existing literature. The five-item BDA construct was taken from Arunachalam et al. [47], the six-item SCI construct was adapted from Kwak et al. [11], the five-item RSC construct consisting of five items was adapted from Parmigiani et al. [68], the four-item IL construct was adapted from Yoon et al. [20] and the eight-item SCR for pandemic response construct was adapted from Sabegh et al. [4]; and Kwak et al. [11]. The details are provided in Table A1 (Online Supplementary Appendix A). B. Sampling and Data Collection The target population for this article comprised..",
      "s23": "..general managers, senior managers, managers, junior managers, and other 1The authors sincerely thank a reviewer for reminding us the importance to clarify the idea behind the multimethod study. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 6 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT Fig. 2. Research flowchart. healthcare sector professionals responsible for sourcing, manufacturing, logistics, distribution, research, and development. The companies were selected from among the most relevant databases in the context of our article, namely the \u201cInnovation Pharmaceutical Association of South Africa,\u201d \u201cGeneric and Biosimilar Medicines of South Africa,\u201d and \u201cBioPharmGuy.\u201d The total number of members listed in these directories combined is approximately 1200. It was determined that 30 representative participants would be a reasonable minimum recommendation for a pilot study [76], [77]. The questionnaire was developed based on a five-point Likert scale (1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4=Agree, 5=Strongly Agree). The questionnaire was e-mailed (using Google Forms) to 37 managers for a pilot survey assessment. After the responses and comments were received, seven questions were reworded before distributing the final survey...",
      "s24": "..Sampling targeted a total of 550 potential respondents, who were selected using random sampling technique, and the final questionnaire was sent (using Google Forms) to two respondents from each company. No incentive was offered or given to survey participants. After two rounds of follow-up, a total of 190 responses were received, representing a response rate of 34 percent. Questionnaires were received from 78 respondents were received at the end of April 2020, and after conducting follow-up, we received data from an additional 112 respondents at the end of May 2020. We did not receive any incomplete submissions, as the questionnaire was designed only to accept complete submissions. The demographic profile of the survey participants is presented in Table A2 (Online Supplementary Appendix A). The highest number of responses was received from professionals working in the healthcare industry for over 15 years, and most responses were received from companies operating in South Africa for more than 20 years. Responses were received from pharmaceutical product and medical device manufacturers, biotechnology companies, medical product distributors, medical retailers, and clinical research institutes. The largest number of responses was received from biotech companies, followed by medical device manufacturers. The analysis also indicated that most responses were received from big companies with annual turnover of more than 50 million South African Rands. C. Nonresponse Bias (..",
      "s25": "..NRB) Since data were received in two phases, we checked NRB by judging the first and second wave of responses, with the second wave (i.e., late responses) being regarded as a control group standing in for those who did not respond (for example, see [80]). Homogeneity of variance test was performed to determine if there was any difference between both sets of responses. The nonsignificant results indicated that our article was free from NRB. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. BAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 7 Fig. 3. BDA capabilities. D. Qualitative Study Methodology The semistructured, open-ended questionnaire (Online Appendix B) used to collect qualitative data was adapted from Sheng et al. [85] and consisted of the following two sections: the first section intended to capture the demographic profile of respondents, and the second section consisted of seven questions intended to capture the potential of BDA in developing resilient HSCs. To avoid any form of common method bias (CMB), a clear note at..",
      "s26": "..the top of the questionnaire explained that the data collection is purely for academic purpose and the names and personal details of the respondents will not be disclosed at any point of time. The number of questions was kept minimum to avoid respondents becoming bored or losing patience while answering such descriptive questions. The questionnaire was created on Google Forms and the link was emailed to 30 potential respondents who were part of the initial empirical survey performed during phase 1 of this article. The request to complete the questionnaire was sent at the end of January 2021, and all of the data was received by early February 2021. Responses were received from all 30 participants. Previous studies have used 20 samples; therefore, a sample size of 30 is acceptable for this study [86]. Finally, the thematic analysis acts as an input for the triangulation of the results obtained from the previous stage. Excel was used to perform the coding, followed by grouping under subthemes and extracting the main themes. IV. DATA ANALYSIS The data obtained during the primary study in phase 1 is depicted in Figs. 3\u20137. Fig. 3 indicates that there are five items (BDA1, BDA2, BDA3, BDA4, and BDA5) that were used to measure the latent construct BDA capabilities. It also shows the responses received during the..",
      "s27": "..primary survey for instance if we look at the item BDA1, out of total 190 responses: 5 selected strongly disagree, i.e., 2.6%; 1 selected disagree, i.e., 0.52%, 0 neutral, 79 selected agree, i.e., 41.57%, 105 selected strongly agree, i.e., 55.26%. Fig. 4 indicates that there are four items (IL1, IL2, IL3, and IL4) that were used to measure the latent construct IL. It also shows the responses received during the primary survey. Fig. 4. Innovation leadership. Fig. 5. Supply chain innovation. Fig. 6. Responsive supply chain. Fig. 7. Supply chain resilience. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 8 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT Fig. 5 indicates six items (SCI1, SCI2, SCI3, SCI4, SCI5, and SCI6) were used to measure the latent construct supply chain innovation. It also shows the responses received during the primary survey. Fig. 6 indicates five items (RS1, RSC2, RSC2, RCS3,..",
      "s28": "..RSC4, and RSC5) were used to measure the latent construct responsive supply chain. It also shows the responses received during the primary survey. Fig. 7 indicates eight items (SCR1, SCR2, SCR3, SCR4, SCR5, SCR6, SCR7, and SCR8) were used to measure the latent construct supply chain resilience. It also shows the responses received during the primary survey. A. SEM Applying the Partial Least Squares Technique Two types of SEM techniques are commonly applied, they are namely: 1) the covariance-based method, and 2) the partial least squares method. PLS-SEM is widely used by researchers in various fields [79]. Many research papers are available that critically examined the pros and cons of PLS-SEM [100]\u2013[102]. In this article, we followed the guidelines of Hair et al. [103]. When the objective of this article is mainly on \u201cprediction and explanation,\u201d then PLS-SEM is recommended. For the case with \u201creflective model specification,\u201d both PLS-SEM and CB-SEM can be used. For smaller sample sizes, PLS-SEM is recommended. Keeping in mind all these points, we opted for the PLS-SEM technique. The software WarpPLS (..",
      "s29": "..version 6.0) was applied for conducting the SEM analysis. We do understand that no single method is perfect. There are pros and cons of each method. To enhance research rigors, we have adopted the multimethod approach (see Fig. 2). B. Common Method Bias The problems associated with the effect of method bias have long been highlighted in the literature [97]\u2013[99]. In many cases, the instructions at the beginning of a questionnaire can influence responses in a particular way (e.g., by implying the desirability of certain responses), thereby introducing common variation among the indicators and contaminating key results by inflating path coefficients due to the introduction of multicollinearity. Following the guidelines of MacKenzie and Podsakoff [98], we carefully designed the questionnaire. First, to ensure the questions could be easily understood, we pretested the questions in our preliminary trial survey. Second, we selected respondents who had the necessary experience about BDA in the healthcare industry. Third, we avoided the use of highly complex and abstract questions. Fourth, we took away \u201citem ambiguity\u201d by using a clear and concise language. Fifth, we did not keep any double-barrelled questions. Sixth, we refocused questions to ask about the current pandemic states because this would minimize..",
      "s30": "..efforts required for retrieval of information. In addition, we performed Harman\u2019s single-factor test on all the constructs [108]. We conducted the standard exploratory factor analysis by selecting the principal component. We further checked the unrotated factor solution to determine the number of factors. The findings did not produce any individual dominating factor which indicates nonexistence of CMB. Lastly, the research team checked whether CMB was present by applying a full collinearity test to examine both vertical and lateral collinearities [78], [79]. If the variance inflation factors (VIFs) are above 3.3, the collinearity issues exist and the model suffers the CMB problem. This is a highly sensitive CMB criterion that tends to identify CMB where other methods provide false negatives [78]. In the current study, the VIF values were all found to be lower than 3.3, and we can, therefore, conclude that our model does not suffer the CMB problem. C. Measurement Model 1) Validity and Reliability: The internal consistency of the latent constructs was checked, and Cronbach\u2019s alpha test was used to check the reliability of the instrument. All Cronbach\u2019s alpha values except IL (0.658) and RSC (0.621) were higher..",
      "s31": "..than 0.70 (BDAC: 0.868, SCI: 0.893, SCR: 0.749, IL\u2217BDAC: 0.912). Since the measurement of these constructs was sensitive to the number of items in the respective scales, the research team also checked the composite reliability of all latent constructs. Composite reliability is a preferred alternative to Cronbach\u2019s alpha test in the context of the data analysis method employed, and composite reliability values above 0.60 are acceptable in social science research. The results indicate acceptable reliability (BDAC: 0.908, IL: 0.793, SCI: 0.920, RSC: 0.771, SCR: 0.812, IL\u2217BDAC: 0.929). Average variances extracted (AVEs) were calculated to assess convergent validity based on the widely used threshold of 0.50 [81], [82]. The values obtained (BDAC: 0.670, IL: 0.598, SCI: 0.662, RSC: 0.505, SCR: 0.599, IL\u2217BDAC: 0.542) suggest that our measurement model displays acceptable convergent validity. Using square roots of AVEs for the latent constructs in combination with latent construct correlations, the research team also..",
      "s32": "..investigated discriminant validity by following the Fornell\u2013 Larcker criterion, i.e., for any latent variable, the square root of the AVE must be higher than its correlation with any other latent variable [81], [82]. The results are showcased in Table A3 (Online Supplementary Appendix). These results suggest that our measurement model displays acceptable discriminant validity. 2) Model Fit and Quality Indices: The quality of the research model was checked using both classic model fit indices and more modern causality assessment indices, as outlined in the following. The classic model fit indices used were the average path coefficient (APC), average R-squared (ARS), average adjusted R-squared (AARS), average block variance inflation factor (AVIF), and average full collinearity VIF (AFVIF) [83]. It is recommended that the p values for APC, ARS, and AARs be less than or equal to 0.05, and these conditions were met (APC = 0.514, p"
    },
    "question_generated": "In the context of the COVID-19 pandemic, how can the integration of big data analytics, supported by innovation leadership, enhance the resilience of healthcare supply chains by fostering supply chain innovation and responsiveness, and what theoretical frameworks support this integration?",
    "golden_answer_generated": "# Report on the Role of Big Data Analytics in Healthcare Supply Chain Resilience\n\n## Introduction\nThe COVID-19 pandemic has underscored the critical need for resilient healthcare supply chains (HSCs) capable of responding to unprecedented disruptions. The integration of big data analytics (BDA) into HSCs offers a promising avenue for enhancing supply chain responsiveness and innovation, thereby fostering resilience\u3010s1\u3011\u3010s3\u3011.\n\n## Theoretical Background\n### Organization Information Processing Theory (OIPT)\nOIPT posits that organizations must enhance their information processing capabilities to navigate uncertain environments effectively. This theory is particularly relevant in the context of the COVID-19 pandemic, where rapid information processing is crucial for managing supply chain disruptions\u3010s9\u3011\u3010s10\u3011.\n\n### Scanning Interpretation-Action-Performance (SIAP) Model\nThe SIAP model outlines a framework for organizational adaptation through environmental scanning, interpretation, and responsive action. This model supports the notion that BDA can facilitate the scanning and interpretation of critical data, leading to informed strategic actions\u3010s10\u3011\u3010s11\u3011.\n\n## Literature Synthesis\n### Big Data Analytics and Supply Chain Innovation\nBDA has been shown to significantly enhance supply chain innovation by enabling the development of new products, processes, and services. This capability is vital for adapting to the dynamic demands of a pandemic environment\u3010s5\u3011\u3010s15\u3011.\n\n### Big Data Analytics and Supply Chain Responsiveness\nThe ability of BDA to process large volumes of data in real-time allows for rapid adjustments in supply chain configurations, improving responsiveness to changing conditions. This responsiveness is essential for maintaining supply chain continuity during crises\u3010s14\u3011\u3010s15\u3011.\n\n### Innovation Leadership as a Moderator\nInnovation leadership plays a crucial role in maximizing the benefits of BDA by fostering a culture of creativity and strategic thinking. Leaders who emphasize innovation can enhance the impact of BDA on supply chain innovation and responsiveness\u3010s16\u3011\u3010s17\u3011.\n\n## Hypothesis Formulation\nBased on the synthesis of existing literature, it is hypothesized that the integration of BDA into HSCs, moderated by strong innovation leadership, will lead to enhanced supply chain resilience during pandemic situations. This hypothesis builds on the established relationships between BDA, supply chain innovation, and responsiveness\u3010s19\u3011\u3010s20\u3011.\n\n---\n\nThis report synthesizes existing literature to demonstrate how BDA, supported by innovation leadership, can enhance healthcare supply chain resilience. The hypothesis presented offers a foundation for further exploration and empirical validation in future research.",
    "used_snippets_with_context": {
      "s1": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT 1 Roles of Innovation Leadership on Using Big Data Analytics to Establish Resilient Healthcare Supply Chains to Combat the COVID-19 Pandemic: A Multimethodological Study Surajit Bag , Shivam Gupta , Tsan-Ming Choi , and Ajay Kumar Abstract\u2014This article empirically examines the effect of big data analytics (BDA) on healthcare supply chain (HSC) innovation, supply chain responsiveness, and supply chain resilience under the moderating effect of innovation leadership in the context of the COVID-19 pandemic. The scanning interpretation\u2013action\u2013 performance model and organization information processing theory are used to explain BDA, HSC innovation, responsiveness, and resilience relationships. First, the hypotheses were tested using data collected from 190 experienced respondents working in the healthcare industry. Our structural equation modeling analysis using the partial least squares (PLS) method revealed that BDA capabilities play a pivotal role in building a responsive HSC and improving innovation, which has contributed to resilience during the current pandemic situation. High innovation leadership strengthens the effect of BDA capabilities on HSC innovation. High innovation leadership also increases the effect of BDA capabilities on responsiveness. Second, we validated and supplemented the..",
      "s2": "..empirical research findings using inputs collected in 30 semistructured qualitative questionnaires. Our article makes a unique contribution from the perspective of innovation leaderships. In particular, we argue that the role of innovative leadership in the COVID-19 pandemic situation is critical as it indirectly affects HSC resilience when BDA is in place. Index Terms\u2014Big data analytics (BDA), COVID-19, healthcare supply chain, multi-methods research, responsive supply chain, supply chain innovation, supply chain resilience. Manuscript received February 28, 2021; revised May 5, 2021 and July 9, 2021; accepted July 25, 2021. Review of this manuscript was arranged by Department Editor N. Damij. (Corresponding author: Tsan-Ming Choi.) Surajit Bag is with the Department of Supply Chain Management and Information Systems, Rabat Business School, International University of Rabat, Rabat 11103, Morocco (e-mail: surajit.bag@gmail.com). Shivam Gupta is with the Department of Information Systems, Supply Chain and Decision Making, NEOMA Business School, 59 Rue Pierre Taittinger, 51100 Reims, France (e-mail: shivam.gupta@neoma-bs.fr). Tsan-Ming Choi is with the..",
      "s3": "..Department and Graduate Institute of Business Administration, College of Management, National Taiwan University, Taipei 10617, Taiwan (e-mail: tmjchoi@gmail.com). Ajay Kumar is with the AIM Research Centre on Artificial Intelligence in Value Creation, EMLYON Business School, 69130 \u00c9cully, France (e-mail: akumar@em-lyon.com). Color versions of one or more figures in this article are available at https: //doi.org/10.1109/TEM.2021.3101590. Digital Object Identifier 10.1109/TEM.2021.3101590 I. INTRODUCTION GLOBAL spending on healthcare is expected to dramatically increase in the near future. This partially relates to changing consumer requirements [1] as well as the higher expectation on timely response to disasters [2]\u2013[4]. Most recently, the COVID-19 pandemic has made it clear that healthcare supply chains (HSCs) are far from perfect. Not much improvements were made from the experiences acquired during various prior epidemics such as middle east respiratory syndrome (MERS) and severe acute respiratory syndrome (SARS) [5], [6]. Massive disruptions in HSCs have reached the level of a global crisis. The availability of personal protective..",
      "s4": "..equipment (PPE), medical equipment, and lifesaving drugs has been severely limited [7]\u2013[9]. Under COVID-19, high demands have challenged the HSC, highlighting the need to manage supply chains differently in crisis situations [9], [10]. Undoubtedly, supply chain responsiveness and innovation are essential to build a resilient HSC to combat the COVID-19 pandemic when the demand uncertainties are extremely high [11], [12]. In the literature, Peeri et al. [6] pointed out the need to focus on using digital technologies to monitor pandemic situations. In particular, big data analytics (BDA) is a powerful tool to help [13]. For example, BDA supported inventory management of medical supplies during emergency responses is critical to ensure the distribution of appropriate supplies [14]. Medical devices with high volumes of data can apply BDA to understand trends and future requirements of PPE. This enhances the management and planning of activities in HSCs [1]. Digital technologies can remove barriers in pharmaceutical supply chains and improve flexibility and innovation related to drug supplies, thereby enhancing coordination, information sharing, and minimizing wastes [2], [15]. In healthcare, BDA is valuable for environmental-scanning (forecasting and observation) purposes [10]..",
      "s5": "... It helps predict the results of drug administration, and analyze patient categorization and emergency response [16], all of which are of paramount significance during a pandemic like COVID-19 [13]. BDA not only can sense information, it can also enhance interpretation to support key business decision making [17], [18] in a timely manner [110]. Prior studies have shown that innovative supply chains have the ability to manage risks, determine an \u00a9 IEEE 2021. This article is free to access and download, along with rights for full text and data mining, reuse and analysis. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 2 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT organization\u2019s competitive position [11], and enhance the interpretation of key information as well as strategy development [19]. Note that innovation leaderships (IL) can improve supply chain innovation and, hence, improve efficiency [20]. Supply chain responsiveness aims to reduce manufacturing throughput and transportation/distribution lead times [21]. BDA capabilities can assist in building a responsive supply chain that positions resources and key players (suppliers, transporters, distributors) at the right places to gain a competitive advantage [22]. BDA improves..",
      "s6": "..productivity in the supply chain process by offering an added level of flexibility [23]. Moreover, following the arguments in [24], BDA can positively affect supply chain transparency. Note that a few studies have examined the role of BDA in the HSC during pandemics (e.g., [25]\u2013[29]). However, no prior research has comprehensively examined the links between BDA and supply chain responsiveness and innovation together. This article aims to fill this gap in the context of COVID-19. A recent insightful study by Dubey et al. [94] used the organization information processing theory (OIPT) to explain the relationships between blockchain technology and operational supply chain transparency. Dubey et al. [94] further argued that blockchain technology and operational supply chain transparency can further enhance collaboration among actors engaged in disaster relief operations and this finally leads to improved supply chain resilience. This study supplements [94] and others in the related domain, and contributes to the supply chain resilience literature. It is noteworthy to mention one more important recent study on supply chain resilience by Dubey et al. [95] who highlighted the importance of BDA in enhancing information processing capacity and supply chain resilience for faster recovery after any disruptions. However, IL (which is related to supply chain..",
      "s8": "..) and innovation; 2) we investigate the effects of RSC and innovation on supply chain resilience. 3) We adopt the multimethodological approach in deriving more scientifically sound results. The main research questions that the study sought to answer are as follows. RQ1: What are the effects of BDA on (i) supply chain responsiveness and (ii) supply chain innovation under the moderating effect of IL during the COVID-19 pandemic? RQ2: What are the effects of (i) responsive supply chain and (ii) supply chain innovation on HSC resilience (SCR) during COVID-19 pandemic? The theoretical model is built through the lenses of OIPT and scanning interpretation\u2013action\u2013performance (SIAP) modeling. We argue that BDA is useful for environmental scanning and information processing to drive SCI (interpretation of key information), which helps establish the responsive supply chain (actions). Finally, SCI and responsiveness are essential to build SCR (performance). During part 1 of the study, data were collected in South Africa using a structured questionnaire and hypotheses were tested using structural equation modeling (SEM) applying the partial least squares technique (PLS-SEM). In the second part, a thematic analysis was performed using the data obtained from 30 semistructured qualitative questionnaires...",
      "s9": "..The themes that emerged from this second-phase highlight major dimensions associated with BDA in the HSC. The rest of this article is organized as follows. Section II presents the theoretical background and hypotheses, Section III provides the methods used for conducting the analysis. Section IV presents the data analysis. Finally, Section V and Section VI concludes this article. II. THEORETICAL BACKGROUND AND HYPOTHESES DEVELOPMENT A. Organization Information Processing Theory OIPT theory proposes that organizations must enhance their information processing capacity to survive in an increasingly uncertain business environment [30], [31]. The COVID-19 pandemic has brought tremendous uncertainties to the lives of both humans and businesses [32]. Uncertainty is driving the need for building information processing capability [33], and companies involved in HSC need to leverage disruptive Industry 4.0 technologies such as BDA to scan and process information and make strategic decisions. OIPT explains how firms can develop the information processing capability during the COVID-19 pandemic to assess external information such as supply crises, market demands, sales and competitors\u2019 distribution activities, rate of infection spreading, number of infected cases, number of recoveries, number of deaths, and clinical trials monitoring and outcomes. Past studies have used OIPT to explain disruptions in supply chains [34]..",
      "s10": "... We argue that BDA enhances firms\u2019 information processing capability during these uncertain pandemic times. Furthermore, supply chain innovation and responsiveness reduce uncertainty by fostering resilience. However, OIPT cannot single-handedly explain the entire mechanism (BDA\u2013 innovation\u2013responsiveness\u2013resilience). Therefore, we supplement it with the SIAP model to better explain these relationships. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. BAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 3 Fig. 1. Theoretical model. B. SIAP Model For the relationships among BDA, SCI, RSC, and SCR, we can refer to Yu et al. [35], and the adopted SIAP model [36] and OIPT [37]. Whether operating at a local or international level, every business is influenced by external factors. Situations change rapidly due to variations in political, environmental, or technological scenarios. Running a business is, therefore, an uneasy task, especially during turbulent times such as the COVID-19 pandemic. The SIAP model argues that firms adjust in the business environment..",
      "s11": "..by following three basic steps, namely \u201cscanning, interpreting, and responding\u201d [38]. First, organizations scan information that can influence performance [36]. Accuracy is key to any effective environmental scanning [39], and BDA can be used to scan important data to generate useful information from various internal and external sources to gain rich business insights and develop competitive edges [40], [41]. BDA fundamentally works by scanning information in the changing business environment [42], [43]. Common sources of big data include social media, websites, shop floor machines, meters, and sensors. There is enormous potential for BDA applications in the HSC [16]. BDA has been demonstrated to be a useful scanning tool that can equip organizations with the ability to scan risks and reconfigure resources and competencies [44], [45]. The potential of artificial intelligence (AI) and BDA in fighting COVID-19 has been confirmed in the literature (see, e.g., [46]). BDA can be used to model the spread of infection during a pandemic, monitor clinical trials, and outcomes, which can be useful for framing policy and controlling infection [16]. Big data generated from social media, smart phones, and other digital equipment can be immensely helpful in controlling..",
      "s12": "..the spread of COVID-19 [13]. The second step in SIAP is \u201cinterpretation,\u201d whereby management uses diverse models as information processing methods to understand and label information [36]. Managers can make use of information to identify opportunities and threats. We argue that firms should resort to supply chain innovation involving all stakeholders to pursue creative methods and services. The third step is \u201caction,\u201d i.e., the strategic initiatives that the firm undertakes to adapt in the changing business environment, which can range from slight shifts in procedures of the business processes to major alterations in product, sales, and distribution strategies. We argue that RSC is a strategic initiative to respond facing the COVID-19 pandemic. The final step of the SIAP model is \u201cperformance\u201d [36]. We argue that SCR is the final outcome that every healthcare organization intends to achieve during pandemic situation. C. Theoretical Model and Research Hypotheses The theoretical model built based on the abovementioned discussion is presented in Fig. 1. Although big data drives supply chain innovation, studies on BDA methods that can help organizations to enhance innovation are limited [47]. In addition, research initiatives on leveraging BDA to unlock values require further investigation [48]. Previous studies have shown how BDA can positively influence supply chain..",
      "s13": "..sustainability [40], [48]. We argue that BDA has a positive association with RSC and innovation, whereby innovation has the ability to develop highly responsive supply chains, and innovation and responsiveness lead to supply chain resilience. In this article, we also introduce \u201cIL\u201d as a moderating variable to examine its effect on the relationships between \u201cBDA and health care supply chain responsiveness\u201d and \u201cBDA and SCI.\u201d 1) BDA and Supply Chain Responsiveness and Innovation: Nowadays, the numbers of actors and products in modern supply chains are much higher than before. Organizations generally prefer big data solutions to curb problems in the supply chain network [49]. BDA involves collecting, managing, and processing a high volume of data generated from various sources. These This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 4 IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT data can be both structured and unstructured in form, and BDA can be used to analyze them and unlock their value [50]. Under COVID-19, the pandemics create a disaster situation and disrupt supply chains because the local and international borders remain closed to prevent the spread of infection. In this type of situation, rapid action is required to make radical changes..",
      "s14": "..in the supply chain that are only possible using BDA information processing capabilities for scanning environmental information [51]. In an HSC, big data is generated from internal enterprise resource planning (ERP systems) and external sources (social media, mobile devices, data portals, and data market platforms). The non structured query language (NoSQL) graph databases are useful for optimizing and configuring supply chains. The Apache Hadoop platform is immensely helpful for managing high volumes of data, and MapReduce helps perform the analytics part to extract information [52]. During pandemic times, batch analysis is untenable to manage when a vast amount of data are generated in the HSC. However, the Lambda architecture can analyze real-time data flows by supporting data stream analytics. Every minute, the data of infected patients, status of infection spread, current drugs, and other medical device requirements at different locations can be gathered using an advanced ICT platform. Further data streams can be analyzed using complex event processing programs. AutoID digital technologies can be useful for tracking purposes as well [49], [53]. BDA can extract information that can be useful for making decisions related to HSC configurations [54]. However, it is important that data scientists and data analysts would closely monitor and control the quality of data to prevent inaccurate information generation [..",
      "s15": "..55]. Therefore, we establish the following hypothesis. Hypothesis H1: BDA capabilities have a positive relationship with RSC. BDA can also offer new opportunities for supply chain innovation [56]. New vaccines and drugs are required to combat pandemic situations [57]. Moreover, the shortage of equipment such as PPE for front-line doctors and healthcare workers can be resolved by securing specialized PPE and making alternative PPE products using 3-D printing and advanced manufacturing. In addition, digital contact tracing apps can play an important role by tracking disease spread [58]. Innovation can involve the development of new products with unique features, alternative manufacturing methods, ecofriendly raw materials for manufacturing, new approaches to transportation and distribution, and the development of new processes that can yield huge benefits for society at large as well as firms [59]. Big data generation capabilities, data integration and management capabilities, advanced analytics, and data visualization capabilities can be immensely useful for supply chain innovation [49], [59]. Therefore, we have the following hypothesis. Hypothesis H2: BDA capabilities have a positive relationship with SCI. 2) Moderating Effects of IL: In this article, we have used OIPT to explain the role of BDA in information processing for reducing uncertainties. However, in the literature, Ha..",
      "s16": "..u\u00dfmann et al. [32, p. 81] highlighted certain shortcomings of the original OIPT theory, which includes the point that interpersonal characteristics and information restrictions are not taken into account. Interpersonal characteristics here include leadership, teamwork, etc. To overcome the limitations of the original theory, we made reference to Hambrick and Mason [96] in which the authors conceptualized the \u201cupper echelons\u201d perspective and argued that firm performance is shaped by managerial background characteristics. \u201cUpper echelons\u201d based leadership theory can put some light on the observable managerial characteristics that the leader can bring to an administrative circumstance. Observable characteristics such as age, functional tracks, other career experiences, education, socioeconomic roots, financial position, and group characteristics would all influence the strategic choices made by top management and leaders [96]. Undoubtedly, product innovation is one of the strategic choices that leaders make for improving firm performance [96]. As a remark, Carmeli et al. [64] argued that IL can improve strategic fit and further enhance the firm performance. IL is related to the innovative nature of organization leaders that ranges from emphasizing on teamwork, clarifying individual responsibility, providing clear feedbacks to employees, emphasizing on task orientation, encouraging initiatives, and developing trust among employees [64]. In an uncertain business environment under..",
      "s17": "..COVID-19, it is very difficult to forecast and plan activities. Disasters and pandemic situations exacerbate the uncertainty, and if leaders continue to work with the same approach used under normal circumstances, then their businesses will not survive the impact. Innovative leadership can be highly effective for managing business challenges during pandemic situations. Applying innovative thinking to leadership tasks can spur employees to begin thinking in innovative ways and further use BDA to configure the HSC and pull the firm out of danger [60]. Learning and teamwork are required to improve environmental training and configure supply chains for sustainability outcomes [61], [62]. Training forms part of the Industry 4.0 delivery system and is important in sustainable development [63]. All of the abovementioned human resource factors are antecedents of IL [20]. IL improves organizational performance and contributes to a firm\u2019s strategic positioning within the business environment [64]. Importance of human resource management, involving IL and responsive management for supply chain sustainability, is highlighted in [12], which argued that the greater the IL, the more pronounced the effect of BDA information processing capabilities on building responsive HSC. Some other studies have demonstrated that leadership thinking based on extensive information will enhance configuration decisions [65], [66]. Therefore, we have the following hypothesis. Hypothesis..",
      "s18": "..H3: IL has a moderating effect on BDA capabilities and RSC. IL is essential for managing the same supply chain tasks in a new way [60]. Many important decisions must be made during disasters caused by the COVID-19 pandemic [4], and innovative leadership can foster innovative thinking by the team and result in innovative solutions that can be helpful for humankind. Unique solutions can involve deploying robots to This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. BAG et al.: ROLES OF INNOVATION LEADERSHIP ON USING BDA TO ESTABLISH RESILIENT HSC TO COMBAT THE COVID-19 PANDEMIC 5 screen for COVID-19 in the community, using drones to carry testing kits and essential drugs to remote places, producing PPE from alternative materials at low cost, using specialized logistics for distribution [109], protecting employees from infection, and changing supply chain processes [67]. The demonstration of innovative thinking by top management builds confidence in BDA application among other employees, and the data can be useful in supply chain innovation. The greater the IL in the organization, the greater is the activation of BDA information processing capabilities on SCI [20], [49]. Therefore, we..",
      "s19": "..propose the following hypothesis. Hypothesis H4: IL has a moderating effect on BDA capabilities and supply chain innovation. 3) SCI, Responsiveness, and Resilience: During a pandemic such as COVID-19, the configuration of the HSC requires significant modifications to enable changes in business processes that can benefit patients and facilitate the economical distribution of essential medical goods and devices. It may be necessary to restructure old supply chain structures to transform them into new structures and develop innovative approaches and capabilities [68]. Changing suppliers and supply chain processes can enable the application of innovative technologies and thereby drive agile and responsive processes to counter changes. We, hence, have the following hypothesis. Hypothesis H5: SCI has a positive relationship with RSC. The HSC can be optimized by reconfiguring its resources to make the healthcare supply more resilient [4]. The ability to quickly configure the supply chain will enhance the ability of the HSC to become responsive and effectively fight against a pandemic, which will save time and efforts while using resources more effectively. This ultimately brings an added benefit to the society [4], [69]. Changing the supply chain configuration and quickly responding to market changes will help to reduce negative effects from repeated risk and improve supply chain resilience [11]. Therefore, we build the following hypothesis. Hypothesis H..",
      "s20": "..6: RSC has a positive relationship with SCR for pandemic response. An innovative supply chain design influences the choice of vendors and results in cooperation with important suppliers as well as impacting supply chain efficiency and quality-related practices [14]. Working in a collaborative manner with suppliers and integrating operations for improved efficiency can result in innovation [70]. In the HSC, innovation initiatives are important to meet increasing demands for better services in a timely manner for patients [71]. Innovation in the HSC can reduce the distance between warehouses and affected areas [72]. Innovation can also contribute to the cheaper manufacturing of products and make them available quickly based on demand. Only innovative approaches can make the HSC more resilient and help manage pandemic situations more effectively [11], [72]. Therefore, we have the following hypothesis. Hypothesis H7: SCI has a positive relationship with SCR for pandemic response. III. RESEARCH METHODS1 A multimethod approach [73] is used in this study. Multimethod approaches are commonly utilized to validate findings in technology and operations management research [73]\u2013[75]. This article was conducted in the following two phases: I) quantitative survey and testing of theoretical model using variance-based SEM and II) semistructured qualitative questionnaires and thematic analysis. Saunders et al.[..",
      "s21": "..104] pointed out a very important part of academic research, i.e., data collection. Data are linked with the answering the research questions. Therefore, \u201cwhat type of data is required\u201d and \u201cwhat techniques are necessary to collect the data\u201d are both critical decisions in any empirical research study. Saunders et al. [104] conceptualized the data collection process as the central part of the research onion. Selection of data collection techniques and analysis processes are critical to produce a good research output. We previously indicated that the multimethod approach was used to reach to the \u201ccentre of the onion,\u201d i.e., answer the central research questions. We did not simply peel off the important outer layers of the onion and thrown them away. We had carefully selected the techniques, with a combination of quantitative and qualitative techniques, in both data collection and analyses. The rationale behind using these methods is as follows: First, to overcome the limitation of empirical surveys (i.e., to have the triangulation effect). Second, to gain richer insights from the practitioner\u2019s perspectives. In the first phase, we used a structured questionnaire and further analyzed the theoretical model. In such a process, we established the links and contributed to the literature. However, the primary data/empirical.."
    },
    "eval_answer": "**Enhancing Healthcare Supply Chain Resilience through Big Data Analytics and Innovation Leadership**\n\n**Integration of Big Data Analytics:**  \n- Big data analytics capabilities (BDACs) significantly enhance the resilience of healthcare supply chains by improving organizational agility and responsiveness during disruptions like the COVID-19 pandemic [S0274819899].  \n- The COVID-19 pandemic prompted healthcare supply chains to adopt big data analytics for real-time decision-making, which enhanced their responsiveness to sudden demand shifts and supply disruptions [S9043179572].  \n- The application of big data analytics has been linked to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions [S0323885390] [S7852842407].  \n\n**Role of Innovation Leadership:**  \n- The integration of big data analytics with innovation leadership fosters supply chain innovation, which is crucial for maintaining operational effectiveness in healthcare supply chains during crises [S9715092690].  \n- Real-time decision-making facilitated by big data analytics has significantly enhanced the responsiveness of healthcare supply chains to sudden demand shifts during the COVID-19 pandemic [S8425331238].  \n\n**Theoretical Frameworks Supporting Integration:**  \n- Theoretical frameworks such as the resource-based view and dynamic capabilities view support the integration of big data analytics and innovation leadership in enhancing healthcare supply chain resilience [S6842133324] [S6989705278].  \n- These frameworks emphasize the importance of organizational agility, enhanced visibility, and risk management capabilities, which are essential for navigating disruptions [S4459050185].  \n\n**Ethical Considerations:**  \n- Ethical implications of big data analytics in healthcare supply chains during the COVID-19 pandemic include the necessity for specific laws and regulations to protect personal data [S4082956211].  \n- Concerns regarding privacy violations and security breaches associated with big data analytics have been amplified, highlighting the need for organizations to implement robust data protection measures [S5927119467].  \n- The growing awareness among individuals about how their personal data is used necessitates that healthcare organizations adopt ethical practices in data collection and analysis to maintain customer trust [S8597788776].  \n\n**Conclusion:**  \nThe integration of big data analytics, supported by innovation leadership, plays a critical role in enhancing the resilience of healthcare supply chains during the COVID-19 pandemic. This integration is underpinned by theoretical frameworks that emphasize agility and responsiveness, while also necessitating careful consideration of ethical implications.",
    "eval_references": {
      "S0323885390": {
        "id": "S0323885390",
        "text": "The application of big data analytics in healthcare supply chains during COVID-19 has been linked to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions.",
        "children": [
          {
            "id": "E3155705893",
            "text": "..against damage or loss, as well as the ability to rapidly recover from a disruption (Hora & Klassen, 2013). Thus, resilience is dynamic instead of static, which is considered as a fundamental attribute that supply chains need to adopt for maintaining stable growth in the face of external disruptions (Essuman et al., 2020) such as the COVID-19 pandemic. Supply chain resilience consists of multiple constituent elements, including stability, agility, robustness, collaboration, redundancy, centralization, visibility, and information sharing (Hosseini et al., 2019). Tukamuhabwa et al. (2015) emphasized the importance of building collaborative relationships in improving supply chain resilience. Other measures include maintaining slack resources, adopting a flexible production strategy, and building a risk-management infrastructure (Ambulkar et al., 2015; Modi & Mishra, 2011). Although 2Rs are the key attributes for enterprises to improve supply chain performance, there is limited research on how to develop responsive and resilient strategies to deal with long-lasting, structural, and global impacts. Previous studies mainly focused on normal situations, in which supply chain integration and big data analytics are the bases of building 2Rs supply chains. Supply chain integration is the ability to integrate all activities among a company's internal functions and external partners (supplier, distributor, retailer, etc.), until the finished product arrives at the end customer (Zhao et al., 2013). From the perspective of collaborative partners, supply chain integration could be divided by horizontal strategies and vertical strategies (Mesquita & Lazzarini, 2008), while the vertical integration is further divided by integration with suppliers (also known as upstream integration) and integration with customers (also known as downstream integration). In the era of Industry 4.0, supply chain integration consists of three dimensions: process and activity integration, technology and system integration, and organizational relationship linkages (Tiwari, 2020). A recent editorial in JOM (Browning, 2020) discussed related aspects of organizational and process integration. In recent years, under the influence of economic globalization, supply chains have been transformed. Since globalization has become pervasive, suppliers have pursued global markets, and most companies source extensively from global suppliers (Cohen & Lee, 2020). This has led to an increase in outsourcing activities and a corresponding decline in vertical integration of supply chains. As a result, supply chain networks have become flatter and more complex, composed of different organizations dispersed across multiple tiers and different geographies, and extended beyond a single country's boundaries (Choi & Hong, 2002). Global supply chains are characterized by focal firms that distribute across multiple countries, locate production facilities abroad, or source from offshore suppliers. Munir et al. (2020) showed that integration in the global supply chain could increase companies' resilience in making flexible deliveries and the number of products. Big data refers to data that arrive at a high volume and velocity with considerable variation, while analytics refers to the ability to gain insights from data via statistics, learning, optimization, or other techniques. The applications of big data and analytics are closely interlinked to enable firms to make better decisions. Hence, prior literature has typically discussed them together as big data analytics, which allows the use of advanced computing techniques, strategies and architectures to store, extract, and analyze multi-source, heterogeneous data to support decisions, and has been commonly used in operations management (Wamba et al., 2015). In the era of economic globalization, supply chain management has become extremely complex, with large-scale and online decision-making challenges emerging (Yang et al., 2021), for example, the joint decision-making between proactive planning and reactive operations in the forms of demand forecasting, production planning, inventory management, supply allocation, transportation, and distribution. It is no longer efficient to rely on traditional analytics methods. Many firms have been exploring how to take big data analytics to promote lean and agile activities in supply chain management (Baruffaldi et al., 2019). Existing studies have shown that applications of digital technologies can help improve supply chain performance by enhancing visibility and reducing supply chain risks (Govindan et al., 2018). The digitalization of supply chains produces large volumes of data, which is regarded as a new kind of resource and has the potential to create value and enhance competitiveness. Singh and El-Kassar (2019) proposed that digital technologies have transformed traditional supply chain management into a more data-driven approach, which requires a much higher level of big data analytics capabilities compared to traditional supply chain management. Following the call for papers, the submission of 114 manuscripts, and the review and revision process, seven articles were selected for this special issue that contribute to our understanding of the impact of COVID-19 on supply chains and its effect on addressing the 2Rs. In \"Strengthening supply chain resilience during COVID-19: A case study of JD.com\" (Shen & Sun, 2023), the authors used quantitative operational data obtained from JD.com4 to analyze the impact of the pandemic on supply chain resilience. They described the challenging scenarios that retailing supply chains experienced in China and the practical response of JD.com over the course of the pandemic the pandemic. JD.com was observed to respond well to the exceptional demand and severe logistical disruptions caused by COVID-19 in China based on its highly integrated supply chain structure (including both process and activity integration and technology and system integration) and comprehensive digital technologies. In particular, the existing, intelligent platforms and delivery procedures were modified slightly but promptly to deal with specific disruptions. The joint efforts of multiple firms, the government, and the entire Chinese society contributed to surmounting the challenges. The experience of JD.com contributes to understanding of the value of investing in operational flexibility and beyond-supply-chain collaboration given the possibility of large-scale supply chain disruptions such as the COVID-19 outbreak. In \"Breaking out of the pandemic: How can firms match internal competence with external resources to shape operational resilience?\" (Li et al., 2023), the authors explored how firms sought to effectively combine internal competence with external resources from the supply chain network to improve operational flexibility and stability during the COVID-19 pandemic. The internal flexibility refers to product diversity, the internal stability refers to operational efficiency, the external flexibility refers to structural holes, and the external stability refers to network centrality. Drawing upon matching theory, the authors provided an internal-external combinative perspective to explain operational mechanisms underlying different matchings. Based on the empirical results of 2994 unique firms and 5293 observations, they found that more heterogeneous combinations between internal (external) flexibility and external (internal) stability may result in a complementary effect that enhances operational resilience, whereas more homogeneous combinations between internal flexibility (or stability) and external flexibility (or stability) may have a substitutive effect that reduces operational resilience. With the COVID-19 pandemic having had a significant impact on supply chains, government initiatives have played a central role in managing the crisis. In \"The impact of governmental COVID-19 measures on manufacturers' stock market valuations: The role of labor intensity and operational slack\" (Chen et al., 2023), the authors investigated the impact of the Chinese government's Level I emergency-response policy (Ge et al., 2020) on manufacturers' stock-market values, and the role of manufacturers' operational slack on adding resilience. Specifically, through an event study of 1357 Chinese manufacturing companies listed on the Shenzhen Stock Exchange, the authors found that the government's emergency-response policy triggered a statistically significant positive reaction from the stock market for manufacturers. However, the authors also found negative impacts on stock market values for manufacturers in labor-intensive industries because of the labor immobility triggered by the Level I measures. In addition, this article identified the positive role of operational slack in the form of financial slack and excess inventory in helping to maintain operations and business continuity, mitigate risks caused by the labor mobility restrictions, and improve supply chain resilience, which identifies operational slack as a supply chain resilience strategy to mitigate pandemic-related risks. When the COVID-19 pandemic broke out, the medical-product industry faced unprecedented demand shocks for personal protective equipment, including face masks, face shields, disinfectants, and gowns. Companies from various industries responded to the urgent need for these potentially life-saving products by adopting ad hoc supply chains in an exceptionally short time. In \"Realizing supply chain agility under time pressure: Ad hoc supply chains during the COVID-19 pandemic\" (M\u00fcller et al., 2023), the authors explored the use by 34 German companies of ad hoc supply chains to produce personal protective equipment. From these cases, the authors developed an emergent theoretical model of ad hoc supply chains around enablers of supply chain agility such as dynamic capabilities (the ability to integrate, build, and reconfigure internal and external competences to address rapidly scenario changes), entrepreneurial orientation (proactiveness, risk-taking, innovativeness, autonomy, and competitive aggressiveness), and temporary orientation (speedy action in a limited time). To cope with the COVID-19 crisis, many firms allowed their employees to work from home (WFH). In \"Working from home and firm resilience to the COVID-19 pandemic\" (Ge et al., 2023), the authors examined whether a firm's WFH capacity increased its resilience. The authors put forward and tested a unique data set that combines listed firms' financial data, epidemiological data, and online job postings data from China. They found that imposing COVID-19 anti-contagion policies on firms and their suppliers or customers significantly increased their operating revenue volatility, slowed their recovery, and had repercussions on their supply chains. WFH enhanced firms' resistance capacity by reducing the effect of COVID-19 on their operating revenue volatility and disruptions to their supply chain partners; however, it also decreased their recovery capacity by extending the time taken to return to normal. Firm attributes, along with workers' occupations, education, and experience, impacted the effect of WFH on firm resilience. This article enhances our understanding of shock transmission across supply..",
            "url": "https://openalex.org/W4365518480",
            "title": "Building responsive and resilient supply chains: Lessons from the <scp>COVID</scp>\u201019 disruption",
            "publication_date": "2023-04-01"
          },
          {
            "id": "E3704507684",
            "text": "..Kamble, S., Gunasekaran, A., Dhone, N.C., 2020b. Industry 4.0 and lean manufacturing practices for sustainable organisational performance in Indian manufacturing companies. Int J Production Research 58 (5), 1319\u20131337. Karkaria, U., 2020. Factories stay on alert for COVID-19; BMW, Toyota, others cope with outbreaks. Automotive News. http://search.ebscohost.com/login.aspx?direct=tru e&db=edsgbe&AN=edsgcl.627622912&site=eds-live (accessed 08.08.2020). Katafuchi, Y., Kurita, K., Managi, S., 2020. COVID-19 with Stigma: theory and Evidence from Mobility Data. Economics of Disasters and Climate Change. https://doi.org/ 10.1007/s41885-020-00077-w. Kochan, C.G., Nowicki, D.R., 2019. Supply chain resilience: a systematic literature review and typological framework. Int J Physical Distribution Logistics Management 48 (8), 842\u2013865. Kotoky, A., Modi, M., Turner, M., 2020. Jobs Are Being Wiped Out at Airlines, And There\u2019s Worse to Come. Bloomberg. http://search.ebscohost.com/login.aspx? direct=true&db=bth&AN=144748168&site=eds-live (accessed 08.08.2020). KPMG, 2020. COVID-19 impact on the automotive sector. https://home.kpmg/xx/en/b logs/home/posts/2020/03/covid-19-impact-on-the-automotive-sector.html (accessed 04.12.2020). Kumar, S., Managi, S., 2020. Economics of Disasters and Climate Change. Process Integration and Optimization for Sustainability 4, 481\u2013502. Lai, C.-C., Shih, T.-P., Ko, W.-C., Tang, H.-J., Hsueh, P.-R., 2020. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and coronavirus disease-2019 (COVID-19): the epidemic and the challenges. Int. J. Antimicrob. Agents 55 (3). LMC, 2020. Impact of COVID-19 on Global Auto Industry: expect deeper decline than during the Great Recession. https://lmc-auto.com/wp-content/uploads/202 0/03/LMCA-Global-LV-Sales-COVID-19-Impact-26-March-2020.pdf (accessed 04.11.2020). L\u00fccker, F., Seifert, R.W., Bi\u00e7er, I., 2019. Roles of inventory and reserve capacity in mitigating supply chain disruption risk. Int J Production Research 57 (4), 1238\u20131249. Martin, A., Markhvida, M., Hallegatte, S., Walsh, B., 2020. Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty. Economics of Disasters and Climate Change 4, 453\u2013479. Martinez, M., 2020. COVID-19 crisis is spurring long-term changes, execs say. Automotive News. http://search.ebscohost.com/login.aspx?direct=true&db=eds gao&AN=edsgcl.621032845&site=eds-live (accessed 07.22.2020). McKinsey, 2020. Covid-19 Briefing Materials. https://www.mckinsey.com /~/media/mckinsey/business%20functions/risk/our%20insights/covid%2019%20 implications%20for%20business/covid%2019%20march%2030/covid-19-facts-an d-insights-april-3.ashx (accessed 04.15.2020). Nakamura, H., Managi, S., 2020a. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. Nakamura, H., Managi, S., 2020b. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. NAM, 2020. Manufacturers\u2019 Survey Reveals Current Industry Impact of COVID-19. https://www.nam.org/manufacturers-survey-reveals-current-industry-impact-of-c ovid-19-7411 (accessed 04.08.2020). OCED, 2020. Coronavirus: the world economy at risk. https://www.oecd.org/berlin/ publikationen/Interim-Economic-Assessment-2-March-2020.pdf (accessed 04.14.2020). Ojha, R., Ghadge, A., Tiwari, M.K., Bititci, U.S., 2018. Bayesian network modelling for supply chain risk propagation. Int J Production Research 56 (17), 5795\u20135819. Patel, T., Horobin, W., 2020. France to Unveil Aid Packages for Auto, Aviation Industries. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=edb&AN=14 3227953&site=eds-live (accessed 08.08.2020). Philip, S., 2020. U.K. Car Industry Joins European Jobs Wipeout to Battle Slump. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=14 3595812&site=eds-live (accessed 08.08.2020). PwC, 2020. PwC\u2019s COVID-19 CFO Pulse Survey. https://www.pwc.com/us/en/li brary/covid-19/pwc-covid-19-cfo-pulse-survey.html (accessed 04.08.2020). Ralston, P., Blackhurst, J., 2020. Industry 4.0 and resilience in the supply chain: a driver of capability enhancement or capability loss? Int J Production Research 58 (16), 5006\u20135019. Roland Berger., 2020. THREE SCENARIOS FOR HOW CORONAVIRUS MAY AFFECT ECONOMIES AND INDUSTRIES. https://www.rolandberger.com/fr/Point-of-Vie w/Three-scenarios-for-how-Coronavirus-may-affect-economies-and-industries.html (accessed 04.14.2020). Rose, A., 2004. In: Springer (Ed.). Modeling Spatial and Economic Impacts of Disasters, pp. 13\u201336. Rose, A., Avetisyan, M., Rosoff, H., Burns, W.J., Slovic, P., Chan, O., 2017. The role of behavioral responses in the total economic consequences of terrorist attacks on US air travel targets. Risk analysis 37 (7), 1403\u20131418. Ro\u00dfmann, B., Canzaniello, A., Gracht, H.v., Hartmann, E., 2018. The future and social impact of Big Data Analytics in Supply Chain Management: results from a Delphi study. Technol Forecast Soc Change 130, 135\u2013149. Santos, J.R., May, L., ElHaimar, A., 2012. Risk-Based input-output analysis of influenza epidemic consequences on interdependent workforce sectors. Risk Analysis 33 (9), 1620\u20131635. Sawik, T., 2019. Disruption mitigation and recovery in supply chains using portfolio approach. Omega (Westport) 84, 232\u2013248. Schmitt, A.J., Singh, M., 2012. A quantitative analysis of disruption risk in a multiechelon supply chain. International Journal of Production Economics 139 (1), 22\u201332. Sharifi, H., Kehoe, D., Hopkins, J., 2006. A classification and selection model of emarketplaces for better alignment of supply chains. Journal of Enterprise Information Management 19 (5), 483\u2013503. Sharma, R., Shishodia, A., Kamble, S., Gunasekaran, A., Belhadi, A., 2020. Agriculture supply chain risks and COVID-19: mitigation strategies and implications for the practitioners. Int J Logistics Research and Applications 1\u201327. https://doi.org/ 10.1080/13675567.2020.1830049. Simchi-Levi, D., Schmidt, W., Wei, Y., Zhang, P.Y., Combs, K., Ge, Y., Gusikhin, O., Sanders, M., Zhang, D., 2015. Identifying risks and mitigating disruptions in the automotive supply chain. Interfaces (Providence) 45 (5), 375\u2013390. Sobieralski, J.B., 2020. COVID-19 and airline employment: insights from historical uncertainty shocks to the industry. Transportation Research Interdisciplinary Perspectives 5. Strauss, A., Corbin, J., 1990. Basics of Qualitative Research. Sage publications. Tan, W.J., Cai, W., Zhang, A.N., 2019. Structural-..",
            "url": "https://openalex.org/W3095984860",
            "title": "Manufacturing and service supply chain resilience to the COVID-19 outbreak: Lessons learned from the automobile and airline industries",
            "publication_date": "2021-02-01"
          }
        ]
      },
      "S4082956211": {
        "id": "S4082956211",
        "text": "The ethical implications of big data analytics in healthcare supply chains during COVID-19 include the necessity for specific laws and regulations to protect personal data, as existing frameworks may not adequately address the complexities introduced by big data technologies.",
        "children": [
          {
            "id": "E3394960366",
            "text": "..to SCM. They present a range of practical applications, and identify potential further research areas with the key message being that more research is required at the nexus of big data and SCM. The main difference between this paper and other survey papers is that we provided a more comprehensive and rigorous review of Big Data Analytics from 2005-2020, drawn from mainstream Journals. More so, we critically identified the main challenges of this disruptive technology in operations and supply chain management, particularly the key ethical, security and privacy issues posed to businesses, society and industrial community. Furthermore, our work acknowledge the growth of Industrial Internet of Things (IIoT) has increased the applicability and transformation of big data analytics across industries such as healthcare sector, science, engineering, finance, business, and eventually the society (Musa and Dabo, 2016). A good example is in pharmaceutical sector, where big data and mobile health are starting to transform the healthcare and diagnostics in a significant way, with new players such as Apple and Google acting as increasingly disruptive catalysts (Gautam and Pan, 2016; cited by Festa et al., 2018). Vaccine manufacturing firm, MERCK implemented Hadoop to utilise huge amount of data in producing vaccines faster and reduce the discard rates which were quite steep (Henschen, 2014; cited by Lamba and Singh, 2017). Further example for big data application in the healthcare sector is the utilisation of enormous volume of information from patient data by Scientist to detect drug interactions and design to implement optimal drug therapies (healthworkscollective.com, 2014; cited by Festa et al., 2018). ). In fact, big data can help fight the spread of communicable diseases. For example, a retrospective analysis of the 2010 cholera outbreak in Haiti indicated that mining data from Twitter and online news reports could have given the country\u2019s health officials an accurate indication of the disease\u2019s spread with lead time of two weeks (Kshetri, 2014). In banking sectors and retail industries, and in government, big data is applied extensively for analysing spending behaviour of customers, security and fraud detection, sentiment and social network analysis (Kshetri, 2014). In addition, big data can help law enforcement agencies to deploy resources more efficiently, respond quickly and increase presence in crime prone areas (Kang, 1998). Big data also enhances retail supply chain, which heavily depends on the availability of accurate and up-to-date information for demand forecast and other business execution (Gunasekaran et al., 2016). According to Schoenherr and Speier-Pero (2015; cited by Bumblauskas et al., 2017), nearly 50 percent of supply chain professionals were using BDA or had plans to use BDA in the near future. These professionals saw BDA as an enabler for better decision making, as well as a predictive analytical tool to achieve performance advantages (Bumblauskas et al., 2017). Raman et al. (2018) have also noted that the concept of big data helps to improve visibility by providing an integrated framework for monitoring performance and customer interaction through real-time data analysis and critical decision-making scenarios. Thus militating risk and supply chain disruption and failures. According to IBM\u2019s Big data and Analytics Hub (2014), \u201c10 billion mobile phones will be in use by 2020, 294 billion emails are sent every day, and trillion of sensors monitor, track with each other, populating the Internet of Things with real time data\u201d. Several supply chain literature (Baihaqi and Sohal, 2013; Li et al., 2009; Prajogo and Olhager, 2012; Vickery et al., 2003; cited by Raman et al., 2018) have acknowledged that the use of various information technologies such as Enterprise Resource Planning (ERP), Global Positioning System (GPS), Radio Frequency Identification (RFID), and Electronic Data Interchange (EDI) in operations and supply chain management enabled firms to effectively maintain control over their information, material and financial flows. Additional example of companies that adopt big data analytics and customer profiling are Amazon and Wal-Mart (Accenture, 2014). They utilise technologies to monitor, track, and secure millions of items in their inventories and rely on forecasting analytics for their \u2018\u2019anticipatory shipping\u2019\u2019, thereby predicting when customers will purchase a product and preship it to a depot close to the final destination (Rozados and Tjahjono, 2014;cited by Niebel et al., 2018). Similarly, Ford Motor Company, employed big data in capturing consumer data from vehicles through sensors and remote app-management software (Niebel et al., 2018). Based on the analysis of data from their cars\u2019 voice recognition systems, the company found out that the surrounding noise affected the performance of the software, which led to the improvement of the system through noise reduction technology and repositioning of microphone. These examples indicate the applicability of big data analytics, particularly in operations and supply chain, especially in its decision-making capacity, which ensure process improvement, management of logistics activities, enhancement of inventory management, and cost optimisation. However, despite the impacts of these disruptive technologies, it required guidance and interpretation by people who knew the tools as well as the business, including the supply chain operators. According to Lamba and Singh (2017), big data provided limited real-time applications in the context of functional areas of operations and supply chain management such as procurement, production, logistics and forecasting. In addition, the author argued that there are certain barriers to implementing predictive analytics, such as the lack of skilled professionals, lack of awareness and dearth of tools for training the next generation of data scientists in the supply chain industry. Alter et al. (2013; cited by Yang et al., 2017) went further to highlight three kinds of expertise needed in supply chain data science, namely: Technological knowledge about the tools and technology capability, functional knowledge about supply chain management, and mathematical knowledge of algorithms. The authors maintained that the blend of these three skillsets will enable companies combine data into new insights for the enterprise-decision making. Accordingly, these skillsets are in short supply now and for the foreseeable future, across all areas of business, including supply chain sector. Yang et al., (2017) echoed that the major difficulties lie in data capture, storage, searching, sharing, analysis, and visualisation. The authors warned that if organisations failed in surmounting these major skillsets gap, Big Data will become a gold mine that firms lacked the capabilities to explore. Against this background therefore, this study offers useful contribution by addressing the research question: RQ1. How is Big Data Analytics being applied and in which ways it has improved supply chain? 3. Ethical, Privacy and Security challenges of big data business analytics Tremendous progress has been made in computing power, storage capacity, and software, resulting in an increased surge in big data technologies (Pauleen et al., 2017; cited by Maple, 2017). This upsurge, has brought several privacy concerns (Biajian et al., 2018). Similarly, it has also led to the growth in the number of smart devices and the speed of that growth present challenges, particularly security and privacy issues to society and individuals\u2019 freedom, as government battle to develop policies, standards and laws that shape this development (Maple, 2017). The question is whether such laws and regulations offer adequate protections for individuals\u2019 data, especially when it relates to Internet of Things. Of course, there is no known new laws that address this emerging phenomenon. Instead, they are governed by existing legal framework. In the UK for example, personal data remains subject to the Data Protection Act of 1998. Although, there is a new proposed regulatory framework by UK government that will improve citizens\u2019 online safety (Online Hams Whitepaper, 2019). Moreover, the characteristics of big data are tightly linked to privacy, ethical and security effects to customer welfare and these fallouts have attracted the attention of scholars, businesses, society, industries and policy makers (Kshetri, 2014). According to ISACA 9 (2014) and Kshetri (2014) huge amount of data means that security breaches and privacy violation are likely, which can cause severe consequences and losses, leading to reputational damage, legal liability, ethical harms and other amplified technical issues. Further, Laney (2001) outlined some factors that can necessitate these challenges, namely: (1) the enormous amount of data (volume), (2) a wide variety of data coming from highly diverse sources (variety), and (3) the pace of data processing (velocity). Therefore, the greatest challenge now is to find new ways of storing and processing these different types of data, since the old databases are mainly structured for only one type of data (Chen et al., 2014 and Hofmann, 2017). The analysis of these data, extraction of insights and appropriation of its values, without any infringement to ethical, security, and personal privacy data protection, presents a key challenge to not only business but also society. Therefore, against this background, this paper will attempt to answer the following research questions: RQ2. What are the ethical, security and privacy challenges associated with Big Data Analytics? RQ3. What are the likely societal, business and governance roles in addressing these emerging technologies? 3.1. Ethical issues The importance of ethics is very significant in big data analytics, especially when it comes to profiling of users through social media analytics. At the same time, people are increasingly becoming aware of how their personal data are being used, even as some individuals and firms are very keen to trade personal data for commercial gains. Therefore, specific laws and regulations must be established to preserve the personal and sensitive information of customers, as no specific laws address these invasive and intrusive technologies. Some data will need more protection than others. For example, a sensor tracking pallets, being shipped overseas is unlikely to transmit such sensitive personal data by comparison with a wearable fitness device, which for instance, might be transmitting medical information. It is also clear that connected devices are often located in intimate spaces such as the home and car or the case of smart pills that..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          },
          {
            "id": "E3902845575",
            "text": "..shown in Table 4. Moreover, the matrix level was explored based on broad theme and sub-themes. This was done in a way that shows a progressing expansion of conceptual framework, from bottom-up to topdown approach. The bottom-up approach is the beginning of the extraction of sub-themes, as individual research articles in the review were determined inductively. The coding was designed in a manner that allowed the capturing of the most dominant conceptual standpoints in the articles towards the main issues of privacy and security, as well as on how big data analytics can enhance economic efficiency, improve access to social services, as well as increase innovative platform for information and communications. To ensure validity and reliability of the coding, the research team read the articles in chronological order starting from 2005 to 2020. Separate lists were generated and crossvalidated. There were discussions among the research team about the classification of the themes and sub-themes and consensus were reached. In general, the reconciliation of the useful insights arising from the top-bottom and bottom-up approaches were achieved within the research team and the team adopted the four principal facets of BDSCA development, namely: big data analytics, applications, ethics and privacy \u2013 for representing the conceptual matrix at the top-most level. The next section explained the results in details. Table 4: Thematic landscape in terms of broad themes and sub \u2013themes (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (Number): showing the conceptual underpinning of the sub-them focus Conceptual standpoints of selective articles (full list of the authors) Big data analytics (1): conceptualisation of big data analytics represents a paradigm shift, enabling better exploitation of the growing amount of data that originates within or outside the focal business environment. Article Stream (1): this conceptualisation is helpful as big data platforms can influence business process management in productivity and flexibility for an enterprise. Article (stream 2): focus increases in the volume and detail of data captured by organisations, such as rise in social media, Internet of Things, Cloud computing, cyberphysical systems, Machine learning, 3D printing, and multimedia has produced overwhelming flow of data in either structured or unstructured format. These Industry 4.0 impacts revenue growth, competitiveness, productivity gains to organisations. Article (stream 3): the evolutionary trend of these technologies has shifted focus from the traditional supply chain data to an accelerating data sources characterised by volume, variety, veracity and value. Chae (2015) proposed a new analytical framework designed for the assessment of social use in supply chain setup. Based on Twitter hashtags, the author investigates how Twitter posts can help companies to better shape demand whilst at the same time delivering valuable customer insights, which are useful for new products developments. Although the presented approach is an interesting application of Big data in supply chain context, it lacks a wider conceptual approach. Musa and Dabo. (2016) investigated the status of Big data in the environments of cloud computing. The relationship between big data and cloud computing were addressed, with particular focus on scalability, data integrity, availability, data transformation and regulatory and legal issues. However, the study has limited managerial inputs in terms of supply chain and operations applications. Combining RFID and cloud computing, Guo et al. (2015) present an intelligent decision support system architecture for production monitoring and scheduling in labour-intensive distributed manufacturing setups. This research addresses main benefits of linking Big Data Analytics and SCM, as the novel architecture approach highlights how the systematic integration of data-driven decision making in production and logistics operations increases information visibility and transparency across supply chain. Supply chain shocks and adaptations amid the covid-19 pandemic and post-pandemic recoveries provide indisputable evidence for the urgent needs of digital techniques for mapping supply chain networks and ensure visibility (Ivanov and Dolgui, 2020). Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number): broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Big data analytics (2): Being able to make sense of a subjective, disparate data, such as audio files, in the system for analysis remains a challenge. Article stream (4): a focus on Machinelearning allows a software to respond to data such as customer shopping trends, weather and geography, and can sense patterns in shopping behaviour. Article (stream 5): there are more focus in employing machine learning through uploading of information and making predictions about the future behaviour based on behavioural residues and interpretation of individuals\u2019 footprints. Article (stream 6): Similar focus on other related advanced technology such as cloud computing to mitigate the time-demanding and challenging task, which required computational infrastructure to ensure successful data processing and analysis. Kenda and Dunja (2018) proposed a model that uses an algorithm for data cleaning stream mining with low noise, which intends to perform better than the current state-of-the-art batch processing method. Although this proposed model sounds very plausible, it shows limited applications since data preparation is crucial for real-time data mining applications. The article by Lopez-Cuadrado et al. (2019; cited by Din et al, 2019) investigates big and small datasets in a non-big data environment, analysing big datasets by applying the first stage machinelearning model. The paper proposes a framework that focused on data preparation by finding an optimal machine \u2013learning model. The study established layers of big data architectures that focused on getting an outcome of data. However, the framework fails to address how this approach enhances supply chain operations. Din et al. (2019) investigates how Machinelearning used behavioural residue in digital footprints to assess and drive personality predictions, particularly in examining Anxiety in micro-blogs from social media tweets. This particular technique is new, and it is limited to healthcare sector. In addition, the research by Zhaodan et al. (2016; cited by Gruda and Souleiman. 2018) provided a link between machine-learning and cyber-physical systems performance. However, the article showed limited managerial inputs in SCM in terms of the applicability of this technology. Presenting findings on how Supply Chain (SC) 4.0 can gain competitive advantage, the research by Makris et al. (2019) focus on three key emerging developmental trends: Big data, Cloud Computing and 3D printing. Although the article presents graphical framework but show limitations in understanding the issues around adaptability, strength and challenges. Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Applications (1): Big data application complements decision support systems (DSS), foster complex decision-making and help problem solving in supply chain management. Article stream (7): the focus is on the emphasis of the properties of big data in providing useful business insights to companies in decision making towards optimising operations and supply chain. Article stream (8): Big data is also used for positioning of goods on the shop shelves as well as employed to get a better handle on company account, manage regulatory requirements and cost savings. Tan et al. (2015), for example, highlighted the potential of Big data as a source for supply chain innovation. Being an interesting contribution to research at the nexus of Big data Analytics and SCM, the proposed analytical structured approach enables companies to integrate their supply chain innovation competencies with supply chain partners systematically, leveraging Big data Analytics insights. In an increasingly competitive and fast-paced business environment, the article\u2019s promoted extension of the innovation capability into the supply chain seems a promising strategy, considering that crucial knowledge may reside outside a single company. The article presented by Sabashini and Kavitha (2011) contributes to the theory development in SCM through investigation of the potential impact of Big Data Analytics on information usage in a corporate and supply chain context. The paper provided insights into the opportunities and challenges emerging form the adoption of Big Data Analytics in SCM. Drawing from resource-based view, Chavez et al. (2017) investigated the linkage between data-driven supply chains, manufacturing capability and customer satisfaction. The research outcomes suggest that data-driven supply chain are positively associated with manufacturing competitiveness dimensions such as quality, delivery, flexibility, and cost, which in turn leads to customer satisfaction. Addressing the challenges linked to the growing amount of data, a major problem in modern manufacturing, Zhang et al. (2015) present an integrated approach for optimised logistics management on the shop floor, based on the smart manufacturing concept, using production resources equipped with RFID facilities. Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Applications (2): Business intelligence and analytics are enhanced with Big data for decision on strategy, managing optimisation and competition. Article stream (9): the focus is centred on the prediction that Big data would improve 60 per cent of existing business and will foster new businesses in the next decade. Article stream (10): the focus in real-time information processing technology based on RFID and NFC in IoT will be widely used in supply chain. As result, the demand forecast would be more accurate and extra buffers would be unnecessary. Opresnik and Taisha (2015; cited by Wemba et al., 2015) linked the advantages of Big Data to the business strategy of servitisation. Their proposed strategic framework outlines how companies can extend their value portfolio by adding information as the pillar alongside the traditional aspects of product and service. Barbaray et. (2018; cited by Fatorachian and Kazemi, 2018) reviewed research on Industry 4.0 concerning issues of SMEs and suggested that SMEs adopt Industry 4.0 concepts only for monitoring industrial processes. Presenting findings from a systematic review and longitudinal case..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          },
          {
            "id": "E4940110996",
            "text": ".., Cuomo, M.T. and Solima, L. (2018). \"Big data for big pharma: Harmonizing business process management to enhance ambidexterity\u201d, Business Process Management Journal, 24(5), pp. 1110-1123. https://doi.org/10.1108/BPMJ-10-2017-0272 Financial Times Archive (2015). Gartner, M., and Brocca. R. 2015. \u2018\u2019Deconstructing supply chain analytics.\u2019\u2019 Journal of Supply Chain Management, 25(2015): 200-326. Gunasekaran, A., Papadopoulos, T., and Dubey, R., and Childe, S.J.Stephen J. 2016. \u2018\u2019Big data and predictive analytics for supply chain and organizational performance.\u2019 Journal of Business Research, 70(2016): pp.308-31. https://doi.org/10.1016/j.jbusres.2016.08.004 Gruda, D. and Hasan, S., 2019. \u201cFeeling anxious? Perceiving anxiety in tweets using machine learning\u201d, Computers in Human Behaviour, 98(2019), pp.245-255. https://doi.org/10.1016/j.chb.2019.04.020 Guo, Z., Ngai, E., Yang, C. and Liang, X., 2015. \u201cAn RFID-based intelligent decision support system architecture for production monitoring and scheduling in a distributed manufacturing environment\u201d, International journal of production economics, 159, pp.16-28. Harris, I., Wang, Y. and Wang, H., 2015. \u201cICT in multimodal transport and technological trends: Unleashing potential for the future\u201d, International Journal of Production Economics, 159(2015), pp.88-103. https://doi.org/10.1016/j.ijpe.2014.09.005 Hashem, I.A.T., Yaqoob, I., Anuar, N.B., Mokhtar, S., Gani, A. and Khan, S.U., 2015. \u201cThe rise of \u201cbig data\u201d on cloud computing: Review and open research issues\u201d, Information systems, 47(2015), pp.98-115. https://doi.org/10.1016/j.is.2014.07.006 Hofmann, E., 2017. \u201cBig data and supply chain decisions: the impact of volume, variety and velocity properties on the bullwhip effect\u201d, International Journal of Production Research, 55(17), pp.5108-5126. https://doi.org/10.1080/00207543.2015.1061222 Ivanov, D. and Dolgui, A., 2020. \u201cA digital supply chain twin for managing the disruption risks and resilience in the era of Industry 4.0\u201d, Production Planning & Control, pp.1-14. https://doi.org/10.1080/09537287.2020.1768450 Kenda, K. and Mladeni\u0107, D., 2018. \u201cAutonomous sensor data cleaning in stream mining setting\u201d, Business Systems Research Journal, 9(2), pp.69-79. DOI: https://doi.org/10.2478/bsrj-2018-0020 Kshetri, N., 2014. \u201cBig data\u05f3 s impact on privacy, security and consumer welfare\u201d, Telecommunications Policy, 38(11), pp.1134-1145. https://doi.org/10.1016/j.telpol.2014.10.002 Kusiak, A., 2018. \u201cSmart manufacturing\u201d, International Journal of Production Research, 56(1- 2), pp.508-517. https://doi.org/10.1080/00207543.2017.1351644 Lamba, K. and Singh, S.P., 2017. \u201cBig data in operations and supply chain management: current trends and future perspectives\u201d, Production Planning & Control, 28(11-12), pp.877- 890. https://doi.org/10.1080/09537287.2017.1336787 Li, J., Tao, F., Cheng, Y. and Zhao, L., 2015. \u201cBig data in product lifecycle management\u201d, The International Journal of Advanced Manufacturing Technology, 81(1-4), pp.667-684. Luppicini, R. and So, A., 2016. \u201cA technoethical review of commercial drone use in the context of governance, ethics, and privacy\u201d, Technology in Society, 46, pp.109-119. https://doi.org/10.1016/j.techsoc.2016.03.003 Mahrt, M. and Scharkow, M., 2013. \u201cThe value of big data in digital media research\u201d, Journal of Broadcasting and Electronic Media, 57(1), pp.20-33. https://doi.org/10.1080/08838151.2012.761700 Makris, D., Hansen, Z.N.L. and Khan, O., 2019. \u201cAdapting to supply chain 4.0: an explorative study of multinational companies\u201d, In Supply Chain Forum: An International Journal, 20(2), pp. 116-131. Maple, C., 2017. \u201cSecurity and privacy in the internet of things\u201d, Journal of Cyber Policy, 2(2), pp.155-184. https://doi.org/10.1080/23738871.2017.1366536 Mendez Mena, D., Papapanagiotou, I. and Yang, B., 2018. \u201cInternet of things: Survey on security\u201d, Information Security Journal: A Global Perspective, 27(3), pp.162-182. Mikalef, P., Boura, M., Lekakos, G. and Krogstie, J., 2019. \u201cBig data analytics and firm performance: Findings from a mixed-method approach\u201d, Journal of Business Research, 98(2019), pp.261-276. https://doi.org/10.1016/j.jbusres.2019.01.044 Moeuf, A., Pellerin, R., Lamouri, S., Tamayo-Giraldo, S. and Barbaray, R., 2018. \u201cThe industrial management of SMEs in the era of Industry 4.0\u201d, International Journal of Production Research, 56(3), pp.1118-1136. https://doi.org/10.1080/00207543.2017.1372647 Mueller, S.D. and Jensen, P., 2017. \u201cBig data in the Danish industry: application and value creation\u201d, Business Process Management Journal. 23(3), pp. 645-670. https://doi.org/10.1108/BPMJ-01-2016-0017 Musa, A. and Dabo, A.A.A., 2016. \u201cA review of RFID in supply chain management: 2000\u2013 2015\u201d, Global Journal of Flexible Systems Management, 17(2), pp.189-228. https://doi.org/10.1007/s40171-016-0136-2 Niebel, T., Rasel, F. and Viete, S., 2019. \u201cBIG data\u2013BIG gains? Understanding the link between big data analytics and innovation\u201d, Economics of Innovation and New Technology, 28(3), pp.296-316. https://doi.org/10.1080/10438599.2018.1493075 Oberl\u00e4nder, A.M., R\u00f6glinger, M., Rosemann, M. and Kees, A., 2018. \u201cConceptualizing business-to-thing interactions\u2013A sociomaterial perspective on the Internet of Things\u201d, European Journal of Information Systems, 27(4), pp.486-502. https://doi.org/10.1080/0960085X.2017.1387714 Opresnik, D. and Taisch, M., 2015. \u201cThe value of big data in servitization\u201d, International journal of production economics, 165, pp.174-184. https://doi.org/10.1016/j.ijpe.2014.12.036 Phase, A. and Mhetre, N., 2018. \u201cUsing IoT in Supply Chain Management\u201d International Journal of Engineering and Techniques, 4 (2), pp. 973-979. Psannis, K.E., Xinogalos, S. and Sifaleras, A., 2014. \u201cConvergence of Internet of things and mobile cloud computing\u201d, Systems Science & Control Engineering: An Open Access Journal, 2(1), pp.476-483. https://doi.org/10.1080/21642583.2014.913213 Queiroz, M.M. and Telles, R., 2018. \u201cBig data analytics in supply chain and logistics: An empirical approach\u201d, The International Journal of Logistics Management, 29(2), pp. 767-783. https://doi.org/10.1108/IJLM-05-2017-0116 Raman, S., Patwa, N., Niranjan, I., Ranjan, U., Moorthy, K. and Mehta, A., 2018. \u201cImpact of big data on supply chain management\u201d, International Journal of Logistics Research and Applications, 21(6..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          }
        ]
      },
      "S0274819899": {
        "id": "S0274819899",
        "text": "Big data analytics capabilities (BDACs) significantly enhance the resilience of healthcare supply chains by improving organizational agility and responsiveness during disruptions such as the COVID-19 pandemic.",
        "children": [
          {
            "id": "E0909104077",
            "text": "..? J. Bus. Res. 133, 337\u2013340. Pigni, F., Piccoli, G., Watson, R., 2016. Digital data streams: creating value from the realtime flow of big data. Calif. Manag. Rev. 58 (3), 5\u201325. **Popovi\u02c7c, A., Hackney, R., Tassabehji, R., Castelli, M., 2018. The impact of big data analytics on firms\u2019 high value business performance. Inf. Syst. Front. 20 (2), 209\u2013222. Premukar, G., Ramamurthy, K., Saunders, C.S., 2005. Information processing view of organizations: an exploratory examination of fit in the context of interorganizational relationships. J. Manag. Inf. Syst. 22 (1), 257\u2013294. Prescott, M.E., 2014. Big data and competitive advantage at Nielsen. Manag. Decis. 52 (3), 573\u2013601. **Qaffas, A.A., Ilmudeen, A., Almazmomi, N.K., Alharbi, I.M., 2022. The impact of big data analytics talent capability on business intelligence infrastructure to achieve firm performance. Foresight 25 (3), 448\u2013464. **Ramadan, M., Shuqqo, H., Qtaishat, L., Asmar, H., Salah, B., 2020. Sustainable competitive advantage driven by big data analytics and innovation. Appl. Sci. 10 (19), 6784. Ransbotham, S., Kiron, D., Prentice, P.K., 2015. Minding the analytics gap. MIT Sloan Manag. Rev. 56 (3), 63. Ravichandran, T., Lertwongsatien, C., 2005. Effect of information systems resources and capabilities on firm performance: a resource-based perspective. J. Manag. Inf. Syst. 21 (4), 237\u2013276. **Razaghi, S., Shokouhyar, S., 2021. Impacts of big data analytics management capabilities and supply chain integration on global sourcing: a survey on firm performance. The Bottom Line 34 (2), 198\u2013223. Regulation, E. G. D. P, 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/EC (General Data Protection Regulation) 2016. OJ L 119 (1). **Rialti, R., Zollo, L., Ferraris, A., Alon, I., 2019. Big data analytics capabilities and performance: evidence from a moderated multi-mediation model. Technol. Forecast. Soc. Chang. 149, 119781. **Rialti, R., Marzi, G., Caputo, A., Mayah, K.A., 2020. Achieving strategic flexibility in the era of big data. Manag. Decis. 58 (8), 1585\u20131600. Richey, R.G., Morgan, T.R., Lindsey-Hall, K., Adams, F.G., 2016. A global exploration of big data in the supply chain. Int. J. Phys. Distrib. Logist. Manag. 46 (8), 710\u2013739. Ross, J.W., Beath, C.M., Quaadgras, A., 2013. You may not need big data after all. Harv. Bus. Rev. 91 (12), 90\u201398. Russom, P., 2011. Big data analytics. TDWI Best Pract. Rep. Fourth Q. 19 (4), 1\u201334. **Sabharwal, R., Miah, S.J., 2021. A new theoretical understanding of big data analytics capabilities in organizations: a thematic analysis. J. Big Data 8 (1), 1\u201317. Sahut, J.M., Schweizer, D., Peris-Ortiz, M., 2022. Technological forecasting and social change introduction to the VSI technological innovations to ensure confidence in the digital world. Technol. Forecast. Soc. Chang. 179, 121680. Schilke, O., Hu, S., Helfat, C.E., 2018. Quo vadis, dynamic capabilities? A contentanalytic review of the current state of knowledge and recommendations for future research. Acad. Manag. Ann. 12 (1), 390\u2013439. **Schlegel, A., Birkel, H.S., Hartmann, E., 2021. Enabling integrated business planning through big data analytics: a case study on sales and operations planning. Int. J. Phys. Distrib. Logist. Manag. 51 (6), 607\u2013633. **Shamim, S., Zeng, J., Khan, Z., Zia, N.U., 2020. Big data analytics capability and decision making performance in emerging market firms: the role of contractual and relational governance mechanisms. Technol. Forecast. Soc. Chang. 161, 120315. **Shamim, S., Yang, Y., Zia, N. Ul, Shah, M.H., 2021. Big data management capabilities in the hospitality sector: service innovation and customer generated online quality ratings. Comput. Hum. Behav. 121, 106777. Sharif, A.M., Irani, Z., 2006. Exploring fuzzy cognitive mapping for IS evaluation. Eur. J. Oper. Res. 173 (3), 1175\u20131187. Sheng, J., Amankwah-Amoah, J., Wang, X., 2017. A multidisciplinary perspective of big data in management research. Int. J. Prod. Econ. 191, 97\u2013112. **Sheng, H., Feng, T., Chen, L., Chu, D., 2021. Responding to market turbulence by big data analytics and mass customization capability. Ind. Manag. Data Syst. 121 (12), 2614\u20132636. Shuradze, G., Wagner, H.T., 2016. Towards a conceptualization of data analytics capabilities. In: 49th Hawaii International Conference on System Sciences (HICSS), pp. 5052\u20135064. Simon, P., 2013. Too Big to Ignore: The Business Case for Big Data, vol. 72. John Wiley & Sons Inc. **Singh, N.P., Singh, S., 2019. Building supply chain risk resilience. BIJ 26 (7), 2318\u20132342. **Song, M., Zhang, H., Heng, J., 2020. Creating sustainable innovativeness through big data and big data analytics capability: from the perspective of the information processing theory. Sustainability 12 (5), 1984. **Song, J., Xia, S., Vrontis, D., Sukumar, A., Liao, B., Li, Q., et al., 2022. The source of SMEs\u2019 competitive performance in COVID-19: matching big data analytics capability to business models. Inf. Systems Front. J. Res. Innov. 24 (4), 1167\u20131187. **Srimarut, T., Mekhum, W., 2020. From supply chain connectivity (SCC) to supply chain agility (SCA), adaptability and alignment: mediating role of big data analytics capability. Int. J. Supply Chain Manag. 9 (1), 183\u2013189. **Srinivasan, Ravi, Swink, Morgan, 2018. An investigation of visibility and flexibility as complements to supply chain analytics: an organizational information processing theory perspective. Prod. Oper. Manag. 27 (10), 1849\u20131867. **Su, X., Zeng, W., Zheng, M., Jiang, X., Lin, W., Xu, A., 2022. Big data analytics capabilities and organizational performance: the mediating effect of dual innovations. Eur. J. Innov. Manag. 25 (4), 1142\u20131160. **Sun, B., Liu, Y., 2021. Business model designs, big data analytics capabilities and new product development performance: evidence from China. Eur. J. Innov. Manag. 24 (4), 162\u20131183. Trkman, P., McCormack, K., de Oliveira, M.P.V., Ladeira, M.B., 2010. The impact of business analytics on supply chain performance. Decis. Support. Syst. 49 (3), 318\u2013327. **Uddin Murad, M.A., Cetindamar, D., Chakraborty, S., 2022. Identifying the key big data analytics capabilities in bangladesh\u2019s healthcare sector. Sustainability 14 (12), 7077. **Upadhyay, P., Kumar, A., 2020. The intermediating role of organizational culture and internal analytical knowledge between the capability of big data analytics and a firm\u2019s performance. Int. J. Inf. Manag. 52, 102100. Wamba, S.F., Akter, S., Edwards, A., Chopin, G., Gnanzou, D., 2015. How \u2018big data\u2019 can make big impact: findings from a systematic review and a longitudinal case study. Int. J. Prod. Econ. 165, 234\u2013246. **Wamba, S.F..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          },
          {
            "id": "E5397305730",
            "text": "..2022) Innovation leadership BDACs and healthcare supply chain innovation/ responsiveness Aljumah et al. (2021) Business value of big data BDACs and organizational performance Shamim et al. (2020) Data driven culture BDACs and decision making performance Wang et al. (2020) BDACs as a moderator Corporate social responsibility and green supply chain management Mikalef et al. (2020a) Information governance, environmental uncertainty BDACs and incremental/radical innovative capability Sun and Liu (2021) BDACs as a moderator Business model novelty/efficiency design and new product development performance Xiao et al. (2020) Digital platform capabilities BDACs and dynamic capabilities Hao et al. (2019) BDACs as a moderator Big data and sustainability of innovation and organizational development Rialti et al. (2019) Organization information management system fit, and organizational resistance to information management BDACs-ambidexterity, BDACs-agility, BDACs-organizational performance Lin and Kunnathur (2019) Developmental culture Technological, customer, entrepreneurial orientation and BDACs Dubey et al. (2019a), Srinivasan and Swink (2018) Organization flexibility BDACs and supply chain agility, BDACs and competitive advantage Mikalef et al. (2019b) Environment dynamism, heterogeneity, and hostility BDACs and dynamic capabilities Dubey et al. (2019b) Control orientation and flexible orientation BDACs and collaborative performance Appendix G. Outcome variables of BDACs in the literature Outcomes References Competitive advantage, sustainable competitive advantage Behl et al. (2022), Horng et al. (2022), Zhang et al. (2022b), Al-Khatib (2022b), Behl (2022), Jha et al. (2020), Corte-Real \u02c6 et al. (2020), Dubey et al. (2019a), Anwar et al. (2018), Ramadan et al. (2020) Business value/strategic business value Contreras Pinochet et al. (2021), Wamba et al. (2020) Organizational performance (general) Bhatti et al. (2022a), Horng et al. (2022), Elia et al. (2022), Ashaari et al. (2021), Gu et al. (2021), Bahrami and Shokouhyar (2022), Razaghi and Shokouhyar (2021), Aljumah et al. (2021), Su et al. (2022), Upadhyay and Kumar (2020), Wamba et al. (2020), Rialti et al. (2019), Ferraris et al. (2019), Mikalef et al. (2019a), Anwar et al. (2018), Wamba et al. (2017), Akter et al. (2016) Operational performance and operational flexibility, strategic sales performance Zhu et al. (2022), Chatterjee et al. (2022), Ilmudeen (2021), Yu et al. (2022), Yasmin et al. (2020), Srinivasan and Swink (2018), Gupta and George (2016), Yu et al. (2021b) Market performance, marketing performance, growth and financial performance Qaffas et al. (2022), Song et al. (2022), Olabode et al. (2022), Ilmudeen (2021), Yasmin et al. (2020), Gupta and George (2016) Decision making quality and performance, decision speed and quality Awan et al. (2021), Shamim et al. (2020), Chen et al. (2022) Manufacturing performance, smart service performance, environmental performance Zhu et al. (2022), Awan et al. (2021), Popovi\u02c7c et al. (2018), Zhang and Lv (2021), AlNuaimi et al. (2021), Belhadi et al. (2020) Explorative and exploitative, incremental and radical innovation, innovation performance, innovative organizational performance, innovation and learning performance Rialti et al. (2020), Mikalef et al. (2020a), Mikalef et al. (2019b), Muhammad et al. (2021), Demir et al. (2022), Khan and Tao (2022), Nisar et al. (2022), *Munir et al. (2023) Product innovation performance, service innovation, business model innovation, Contreras Pinochet et al. (2021), Shamim et al. (2021), Ciampi et al. (2021), Elia et al. (2022) Dual innovations, co-innovation, process Eco-innovation Su et al. (2022), Lozada et al. (2019), Munodawafa and Johl (2019) Sustainable innovativeness, innovative green product development Song et al. (2020), Hao et al. (2019), Bag et al. (2020) Dynamic capabilities Mikalef et al. (2020b), Xiao et al. (2020), Mikalef et al. (2019b), Wamba et al. (2017) Innovation capabilities Bahrami and Shokouhyar (2022), Ramadan et al. (2020) Marketing capabilities and technological capabilities, digital platform and network capabilities Bhatti et al. (2022a), Mikalef et al. (2020b) Knowledge management capabilities and information processing capabilities Rialti et al. (2020), Schlegel et al. (2021) (Firm's) agility, adaptability, alignment, ambidexterity, entrepreneurial orientation, organizational resilience Srimarut and Mekhum (2020), Wamba et al. (2020), Rialti et al. (2019), Popovi\u02c7c et al. (2018), Aljumah et al. (2021), Rialti et al. (2019), Ciampi et al. (2021), Khan and Tao (2022), Ciasullo et al. (2022), Al-Darras and Tanova (2022) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 18 (continued ) Outcomes References Supply chain innovation and resilience, supply chain innovativeness Bhatti et al. (2022b), Bhatti et al. (2022a), Jaouadi (2022), Bag et al. (2022), Singh and Singh (2019) External and internal integration, internal finance integration, global sourcing, green supplier and customer integration Razaghi and Shokouhyar (2021), Yu et al. (2021a), Chen and Chen (2022), Liu et al. (2022) Sustainable supply chain performance and flexibility, supply chain development, green supply chain performance, humanitarian supply chain performance Edwin Cheng et al. (2022), Gu et al. (2021), Al-Khatib (2022a), Al-Khatib (2023), Dubey et al. (2022), Nisar et al. (2022) Swift trust and collaborative performance, strategic and tactical reverse logistics decisions Dubey et al. (2019b), Bag et al. (2021) Supply chain preparedness, alertness, agility, adaptability, responsiveness, automated supply chain disruption risk alert tool Park and Singh (2022), Bhatti et al. (2022b), Bag et al. (2022), Dubey et al. (2019a), Mandal (2019) Circular economy practices, green manufacturing practices/Lean Six Sigma efforts Awan et al. (2021), Edwin Cheng et al. (2022), Belhadi et al. (2020) Sustainable design and commercialization, mass customization, and internationalization Zhang et al. (2020), Sheng et al. (2021), Bertello et al. (2021) Information quality, knowledge creation Bahrami and Shokouhyar (2022), Shamim et al. (2021), Awan et al. (2021) Organization, employee, new product development Dubey et al. (2021), Bag et al. (2020), Hao et al. (2019), Business processes improvement, consumer experience and market enhancement Elia et al. (2022) Quality of (health) care services and potential benefits Wang et al. (2019), Wang et al. (2018a), Wang and Hajli (2017) References2 Akhtar, P., Khan, Z., Frynas, J.G., Tse, Y.K., Rao-Nicholson, 2018. Essential microfoundations for contemporary business operations: top management tangible competencies, relationship-based business networks and environmental sustainability. Br. J. Manag. 29 (1), 43\u201362. Akhtar, P., Frynas, J.G., Mellahi, K., Ullah, S., 2019. Big data-savvy teams\u2019 skills, big data-driven actions and business performance. Br. J. Manag. 30 (2), 252\u2013271. **Akter, S., Wamba, S.F., Gunasekaran, A., Dubey, R., Childe, S.J., 2016. How to improve firm performance using big data analytics capability and business strategy alignment? Int. J. Prod. Econ. 182, 113\u2013131. **Al-Darras, O.M.A., Tanova, C., 2022. From big data analytics to organizational agility: what is the mechanism? SAGE Open 12 (2), 215824402211061. **Aljumah, A.I., Nuseir, M.T., Alam, M.M., 2021. Organizational performance and capabilities to analyze big data: do the ambidexterity and business value of big data analytics matter? Bus. Process. Manag. J. 27 (4)..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          },
          {
            "id": "E3716506598",
            "text": "..Akter et al. (2016), Ferraris et al. (2019), Wamba et al. (2017) Ashaari et al. (2021) \u201cBDAC is termed as an organization's capacity to efficiently and strategically arrange, assemble, and apply BDA resources so that effective decision-making can be made to enhance overall organization's performance.\u201d (p. 1) Resource-based view, organizational information processing theory Higher education institutions Mikalef et al. (2020b), Shamim et al. (2020), Janssen et al. (2017), Cao et al. (2015), Awan et al. (2021) BDACs refer to a \u201cholistic process that involves the collection, analysis, use and interpretation of data for various functional divisions to gain actionable insights, create business value and establish competitive advantage.\u201d (p. 86) Resource-based view, dynamic capabilities view, institution-based view Manufacturing sector Akter et al. (2016) Bertello et al. (2021) BDAC is defined \u201cas the ability to acquire, store, process, and analyze large amounts of data in various forms and deliver meaningful information to users, allowing them to discover business values and insights in a timely fashion.\u201d (p.1040) Resource-based view Wang et al. (2018a) Ciampi et al. (2021) BDAC \u201crefer to the company's abilities to leverage on technology and talent to exploit BD towards the generation of the insights that are necessary to overperform rivals.\u201d (p. 2) Knowledge-based view, IT capability Mikalef et al. (2017), Wamba et al. (2017), Gupta and George (2016) Contreras Pinochet et al. (2021) BDAC is recognized as \u201cthe competence to provide business insights using the capacity of data management, infrastructure (technology) and talent (personnel) to transform a business into a competitive force.\u201d (p. 1410) Process-oriented dynamic capabilities, business value Gupta and George (2016) Gu et al. (2021) BDAC in supply chain management is described as \u201cthe ability of organizations to collect and organize supply chain data from heterogeneous systems distributed across organizational boundaries, analyze it either batch-wise, or real-time, or near real-time, and visualize it intuitively to create proactive supply chain system and support decision making,\u201d (p. 155) Resource-based view, dynamic capabilities view, contingency theory Supply chain management Arunachalam et al. (2018) Henao-Garc\u00eda et al. (2021) \u201cBDAC is defined as the ability of a firm to capture and analyze data for the generation of insights by effectively orchestrating and deploying its data, technology, and talent.\u201d (p. 28) Resource-based view, dynamic capabilities view Mikalef et al. (2018) Schlegel et al. (2021) BDAC is defined \u201cas a firm's ability to assemble, integrate and deploy its big data-based resources.\u201d (p. 609) Organizational information processing theory Gupta and George (2016), Akter et al. (2016) Sheng et al. (2021) BDA capability refers to \u201can enterprise's ability to realize data-driven operation plan and decisionmaking through processing, organizing and analyzing data.\u201d (p. 2618) Organizational information processing theory Supply chain management Dubey et al. (2019b), Gupta and George (2016) Sun and Liu (2021) \u201cBDA capabilities comprise a firm's techniques, processes and talents that enable the organization to process, visualize and analyze big data, thereby producing insights that enable data-driven operational planning, decision-making and execution.\u201d (p. 1163) Kiron et al. (2014), Akter et al. (2016), Dubey et al. (2019a) for definition; Akter et al. (2016), Ferraris et al. (2019) for measures Sabharwal and Miah (2021) BDAC is defined as \u201cthe combined ability to store, process, and analyze large amounts of data to provide meaningful information to users.\u201d (p. 9) Organization development theory Gupta and George (2016), Wang et al. (2019), Mikalef et al. (2020b), Shuradze and Wagner (2016) Yu et al. (2021a) BDAC is defined as an \u201corganizational facility with tools, techniques, and processes that enable a firm to process, organize, visualize, and analyse data thereby Organizational information processing theory Supply chain management Srinivasan and Swink (2018) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 14 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies producing insights that enable data-driven operational planning, decision-making, and execution.\u201d (p. 2) Yu et al. (2021b) BDAC refers to \u201corganizational facility with tools, techniques, and processes that enable a firm to process, organize, visualize, and analyse data, thereby producing insights that enable data-driven operational planning, decision-making, and execution.\u201d (p. 4) Organizational information processing theory Health care Srinivasan and Swink (2018) Yu et al. (2022) BDAC is defined as \u201cthe ability to acquire, store, process, and analyse large amount of health data in various forms, and deliver meaningful information to users that allows them to discover business values and insights in a timely fashion.\u201d (p. 3) Resource orchestration theory Health care Wang et al. (2018a) Zhang and Lv (2021) BDACs are proposed with three dimensions of tangible resources, human resources and intangible resources, which are analyzed \u201cfrom three dimensions of management capabilities, infrastructure capabilities and human capabilities.\u201d (p. 50525) Resource-based view, IT capabilities Smart cities, public sectors Gupta and George (2016) Behl et al. (2022) \u201cBig data analytics capabilities (BDAC) are broadly defined as the competence to provide business insights using data management, infrastructure (technology) and talent (personnel) capability to transform business into a competitive force.\u201d (p. 380) Organizational information processing theory, institutional theory Micro, small and medium enterprises Akter et al. (2016) Elia et al. (2022) \u201cBDAC are defined as the knowledge, skills, and abilities that combine technology and management issues to explore data potential through sophisticated statistical, computational, and visualization tools.\u201d (p. 2) Resource-based view, dynamic capability view, and absorptive capacity view Wamba et al. (2020b) Liu et al. (2022) \u201cBDAC refers to an organizational ability that enable firms to capture, consolidate, and analyze data thus generating new insights to implement data-driven programming, decision-making, and operation.\u201d (p. 2561) Organizational information processing theories Supply chain management Gupta and George (2016), Srinivasan and Swink (2018) Uddin Murad et al. (2022) \u201cBDACs are an organizational ability with the necessary tools and techniques to process big data to produce internal associations, patterns, and insights.\u201d (p. 3) Srinivasan and Swink (2018) Jaouadi (2022) \u201cThe term big data analytics capability is the extent wherein firm has distinctive capability to identify quality problems, competency to set optimal pricing, trace profitable customers and manage lowest inventory using big data tools.\u201d (p. 2) Akter et al. (2016) Horng et al. (2022) \u201cBig data analytics capabilities refer to obtaining knowledge from internal or external partners and gaining market insight through big data tools.\u201d (p. 24) Knowledge-based dynamic capability view Germann et al. (2013) Ciasullo et al. (2022) \u201cBDA capability refers to a company's management ability, that is, the ongoing deployment of big data resources at the strategic aims to create value and develop a competitive advantage for the firm.\u201d (p. 205) Wamba et al. (2017) Zhu et al. (2022) \u201c...the ability of an organization to integrate, build, and reconfigure the information resources, as well as business processes, to address rapidly changing environments.\u201d (p. 5) Dynamic capabilities view Supply chain management Lee and Kang (2015) Arias-P \u0301erez et al. (2022) \u201cBDAC is defined as the ability of a firm to capture and analyze big data toward the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 2) Knowledge-based view Henao-Garc\u00eda et al. (2021) Olabode et al. (2022) \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 1219) Knowledge-based view and contingency theory Mikalef et al. (2020b) Bhatti et al. (2022a) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions to gain actionable insights, create business value, and establish competitive advantage\u201d (p. 4) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Bhatti et al. (2022b) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions with a view to gaining actionable insights, creating business value, and establishing competitive advantage.\u201d (p. 5) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Song et al. (2022) \u201cBDAC is defined as the ability to develop business insight by using data management, technical foundations and talents.\u201d (p. 1168) Kiron et al. (2014) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          }
        ]
      },
      "S8425331238": {
        "id": "S8425331238",
        "text": "Real-time decision-making facilitated by big data analytics has significantly enhanced the responsiveness of healthcare supply chains to sudden demand shifts during the COVID-19 pandemic.",
        "children": [
          {
            "id": "E3704507684",
            "text": "..Kamble, S., Gunasekaran, A., Dhone, N.C., 2020b. Industry 4.0 and lean manufacturing practices for sustainable organisational performance in Indian manufacturing companies. Int J Production Research 58 (5), 1319\u20131337. Karkaria, U., 2020. Factories stay on alert for COVID-19; BMW, Toyota, others cope with outbreaks. Automotive News. http://search.ebscohost.com/login.aspx?direct=tru e&db=edsgbe&AN=edsgcl.627622912&site=eds-live (accessed 08.08.2020). Katafuchi, Y., Kurita, K., Managi, S., 2020. COVID-19 with Stigma: theory and Evidence from Mobility Data. Economics of Disasters and Climate Change. https://doi.org/ 10.1007/s41885-020-00077-w. Kochan, C.G., Nowicki, D.R., 2019. Supply chain resilience: a systematic literature review and typological framework. Int J Physical Distribution Logistics Management 48 (8), 842\u2013865. Kotoky, A., Modi, M., Turner, M., 2020. Jobs Are Being Wiped Out at Airlines, And There\u2019s Worse to Come. Bloomberg. http://search.ebscohost.com/login.aspx? direct=true&db=bth&AN=144748168&site=eds-live (accessed 08.08.2020). KPMG, 2020. COVID-19 impact on the automotive sector. https://home.kpmg/xx/en/b logs/home/posts/2020/03/covid-19-impact-on-the-automotive-sector.html (accessed 04.12.2020). Kumar, S., Managi, S., 2020. Economics of Disasters and Climate Change. Process Integration and Optimization for Sustainability 4, 481\u2013502. Lai, C.-C., Shih, T.-P., Ko, W.-C., Tang, H.-J., Hsueh, P.-R., 2020. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and coronavirus disease-2019 (COVID-19): the epidemic and the challenges. Int. J. Antimicrob. Agents 55 (3). LMC, 2020. Impact of COVID-19 on Global Auto Industry: expect deeper decline than during the Great Recession. https://lmc-auto.com/wp-content/uploads/202 0/03/LMCA-Global-LV-Sales-COVID-19-Impact-26-March-2020.pdf (accessed 04.11.2020). L\u00fccker, F., Seifert, R.W., Bi\u00e7er, I., 2019. Roles of inventory and reserve capacity in mitigating supply chain disruption risk. Int J Production Research 57 (4), 1238\u20131249. Martin, A., Markhvida, M., Hallegatte, S., Walsh, B., 2020. Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty. Economics of Disasters and Climate Change 4, 453\u2013479. Martinez, M., 2020. COVID-19 crisis is spurring long-term changes, execs say. Automotive News. http://search.ebscohost.com/login.aspx?direct=true&db=eds gao&AN=edsgcl.621032845&site=eds-live (accessed 07.22.2020). McKinsey, 2020. Covid-19 Briefing Materials. https://www.mckinsey.com /~/media/mckinsey/business%20functions/risk/our%20insights/covid%2019%20 implications%20for%20business/covid%2019%20march%2030/covid-19-facts-an d-insights-april-3.ashx (accessed 04.15.2020). Nakamura, H., Managi, S., 2020a. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. Nakamura, H., Managi, S., 2020b. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. NAM, 2020. Manufacturers\u2019 Survey Reveals Current Industry Impact of COVID-19. https://www.nam.org/manufacturers-survey-reveals-current-industry-impact-of-c ovid-19-7411 (accessed 04.08.2020). OCED, 2020. Coronavirus: the world economy at risk. https://www.oecd.org/berlin/ publikationen/Interim-Economic-Assessment-2-March-2020.pdf (accessed 04.14.2020). Ojha, R., Ghadge, A., Tiwari, M.K., Bititci, U.S., 2018. Bayesian network modelling for supply chain risk propagation. Int J Production Research 56 (17), 5795\u20135819. Patel, T., Horobin, W., 2020. France to Unveil Aid Packages for Auto, Aviation Industries. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=edb&AN=14 3227953&site=eds-live (accessed 08.08.2020). Philip, S., 2020. U.K. Car Industry Joins European Jobs Wipeout to Battle Slump. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=14 3595812&site=eds-live (accessed 08.08.2020). PwC, 2020. PwC\u2019s COVID-19 CFO Pulse Survey. https://www.pwc.com/us/en/li brary/covid-19/pwc-covid-19-cfo-pulse-survey.html (accessed 04.08.2020). Ralston, P., Blackhurst, J., 2020. Industry 4.0 and resilience in the supply chain: a driver of capability enhancement or capability loss? Int J Production Research 58 (16), 5006\u20135019. Roland Berger., 2020. THREE SCENARIOS FOR HOW CORONAVIRUS MAY AFFECT ECONOMIES AND INDUSTRIES. https://www.rolandberger.com/fr/Point-of-Vie w/Three-scenarios-for-how-Coronavirus-may-affect-economies-and-industries.html (accessed 04.14.2020). Rose, A., 2004. In: Springer (Ed.). Modeling Spatial and Economic Impacts of Disasters, pp. 13\u201336. Rose, A., Avetisyan, M., Rosoff, H., Burns, W.J., Slovic, P., Chan, O., 2017. The role of behavioral responses in the total economic consequences of terrorist attacks on US air travel targets. Risk analysis 37 (7), 1403\u20131418. Ro\u00dfmann, B., Canzaniello, A., Gracht, H.v., Hartmann, E., 2018. The future and social impact of Big Data Analytics in Supply Chain Management: results from a Delphi study. Technol Forecast Soc Change 130, 135\u2013149. Santos, J.R., May, L., ElHaimar, A., 2012. Risk-Based input-output analysis of influenza epidemic consequences on interdependent workforce sectors. Risk Analysis 33 (9), 1620\u20131635. Sawik, T., 2019. Disruption mitigation and recovery in supply chains using portfolio approach. Omega (Westport) 84, 232\u2013248. Schmitt, A.J., Singh, M., 2012. A quantitative analysis of disruption risk in a multiechelon supply chain. International Journal of Production Economics 139 (1), 22\u201332. Sharifi, H., Kehoe, D., Hopkins, J., 2006. A classification and selection model of emarketplaces for better alignment of supply chains. Journal of Enterprise Information Management 19 (5), 483\u2013503. Sharma, R., Shishodia, A., Kamble, S., Gunasekaran, A., Belhadi, A., 2020. Agriculture supply chain risks and COVID-19: mitigation strategies and implications for the practitioners. Int J Logistics Research and Applications 1\u201327. https://doi.org/ 10.1080/13675567.2020.1830049. Simchi-Levi, D., Schmidt, W., Wei, Y., Zhang, P.Y., Combs, K., Ge, Y., Gusikhin, O., Sanders, M., Zhang, D., 2015. Identifying risks and mitigating disruptions in the automotive supply chain. Interfaces (Providence) 45 (5), 375\u2013390. Sobieralski, J.B., 2020. COVID-19 and airline employment: insights from historical uncertainty shocks to the industry. Transportation Research Interdisciplinary Perspectives 5. Strauss, A., Corbin, J., 1990. Basics of Qualitative Research. Sage publications. Tan, W.J., Cai, W., Zhang, A.N., 2019. Structural-..",
            "url": "https://openalex.org/W3095984860",
            "title": "Manufacturing and service supply chain resilience to the COVID-19 outbreak: Lessons learned from the automobile and airline industries",
            "publication_date": "2021-02-01"
          },
          {
            "id": "S9043179572",
            "text": "The COVID-19 pandemic prompted healthcare supply chains to adopt big data analytics for real-time decision-making, enhancing their responsiveness to sudden demand shifts and supply disruptions.",
            "children": [
              {
                "id": "E3155705893",
                "text": "..against damage or loss, as well as the ability to rapidly recover from a disruption (Hora & Klassen, 2013). Thus, resilience is dynamic instead of static, which is considered as a fundamental attribute that supply chains need to adopt for maintaining stable growth in the face of external disruptions (Essuman et al., 2020) such as the COVID-19 pandemic. Supply chain resilience consists of multiple constituent elements, including stability, agility, robustness, collaboration, redundancy, centralization, visibility, and information sharing (Hosseini et al., 2019). Tukamuhabwa et al. (2015) emphasized the importance of building collaborative relationships in improving supply chain resilience. Other measures include maintaining slack resources, adopting a flexible production strategy, and building a risk-management infrastructure (Ambulkar et al., 2015; Modi & Mishra, 2011). Although 2Rs are the key attributes for enterprises to improve supply chain performance, there is limited research on how to develop responsive and resilient strategies to deal with long-lasting, structural, and global impacts. Previous studies mainly focused on normal situations, in which supply chain integration and big data analytics are the bases of building 2Rs supply chains. Supply chain integration is the ability to integrate all activities among a company's internal functions and external partners (supplier, distributor, retailer, etc.), until the finished product arrives at the end customer (Zhao et al., 2013). From the perspective of collaborative partners, supply chain integration could be divided by horizontal strategies and vertical strategies (Mesquita & Lazzarini, 2008), while the vertical integration is further divided by integration with suppliers (also known as upstream integration) and integration with customers (also known as downstream integration). In the era of Industry 4.0, supply chain integration consists of three dimensions: process and activity integration, technology and system integration, and organizational relationship linkages (Tiwari, 2020). A recent editorial in JOM (Browning, 2020) discussed related aspects of organizational and process integration. In recent years, under the influence of economic globalization, supply chains have been transformed. Since globalization has become pervasive, suppliers have pursued global markets, and most companies source extensively from global suppliers (Cohen & Lee, 2020). This has led to an increase in outsourcing activities and a corresponding decline in vertical integration of supply chains. As a result, supply chain networks have become flatter and more complex, composed of different organizations dispersed across multiple tiers and different geographies, and extended beyond a single country's boundaries (Choi & Hong, 2002). Global supply chains are characterized by focal firms that distribute across multiple countries, locate production facilities abroad, or source from offshore suppliers. Munir et al. (2020) showed that integration in the global supply chain could increase companies' resilience in making flexible deliveries and the number of products. Big data refers to data that arrive at a high volume and velocity with considerable variation, while analytics refers to the ability to gain insights from data via statistics, learning, optimization, or other techniques. The applications of big data and analytics are closely interlinked to enable firms to make better decisions. Hence, prior literature has typically discussed them together as big data analytics, which allows the use of advanced computing techniques, strategies and architectures to store, extract, and analyze multi-source, heterogeneous data to support decisions, and has been commonly used in operations management (Wamba et al., 2015). In the era of economic globalization, supply chain management has become extremely complex, with large-scale and online decision-making challenges emerging (Yang et al., 2021), for example, the joint decision-making between proactive planning and reactive operations in the forms of demand forecasting, production planning, inventory management, supply allocation, transportation, and distribution. It is no longer efficient to rely on traditional analytics methods. Many firms have been exploring how to take big data analytics to promote lean and agile activities in supply chain management (Baruffaldi et al., 2019). Existing studies have shown that applications of digital technologies can help improve supply chain performance by enhancing visibility and reducing supply chain risks (Govindan et al., 2018). The digitalization of supply chains produces large volumes of data, which is regarded as a new kind of resource and has the potential to create value and enhance competitiveness. Singh and El-Kassar (2019) proposed that digital technologies have transformed traditional supply chain management into a more data-driven approach, which requires a much higher level of big data analytics capabilities compared to traditional supply chain management. Following the call for papers, the submission of 114 manuscripts, and the review and revision process, seven articles were selected for this special issue that contribute to our understanding of the impact of COVID-19 on supply chains and its effect on addressing the 2Rs. In \"Strengthening supply chain resilience during COVID-19: A case study of JD.com\" (Shen & Sun, 2023), the authors used quantitative operational data obtained from JD.com4 to analyze the impact of the pandemic on supply chain resilience. They described the challenging scenarios that retailing supply chains experienced in China and the practical response of JD.com over the course of the pandemic the pandemic. JD.com was observed to respond well to the exceptional demand and severe logistical disruptions caused by COVID-19 in China based on its highly integrated supply chain structure (including both process and activity integration and technology and system integration) and comprehensive digital technologies. In particular, the existing, intelligent platforms and delivery procedures were modified slightly but promptly to deal with specific disruptions. The joint efforts of multiple firms, the government, and the entire Chinese society contributed to surmounting the challenges. The experience of JD.com contributes to understanding of the value of investing in operational flexibility and beyond-supply-chain collaboration given the possibility of large-scale supply chain disruptions such as the COVID-19 outbreak. In \"Breaking out of the pandemic: How can firms match internal competence with external resources to shape operational resilience?\" (Li et al., 2023), the authors explored how firms sought to effectively combine internal competence with external resources from the supply chain network to improve operational flexibility and stability during the COVID-19 pandemic. The internal flexibility refers to product diversity, the internal stability refers to operational efficiency, the external flexibility refers to structural holes, and the external stability refers to network centrality. Drawing upon matching theory, the authors provided an internal-external combinative perspective to explain operational mechanisms underlying different matchings. Based on the empirical results of 2994 unique firms and 5293 observations, they found that more heterogeneous combinations between internal (external) flexibility and external (internal) stability may result in a complementary effect that enhances operational resilience, whereas more homogeneous combinations between internal flexibility (or stability) and external flexibility (or stability) may have a substitutive effect that reduces operational resilience. With the COVID-19 pandemic having had a significant impact on supply chains, government initiatives have played a central role in managing the crisis. In \"The impact of governmental COVID-19 measures on manufacturers' stock market valuations: The role of labor intensity and operational slack\" (Chen et al., 2023), the authors investigated the impact of the Chinese government's Level I emergency-response policy (Ge et al., 2020) on manufacturers' stock-market values, and the role of manufacturers' operational slack on adding resilience. Specifically, through an event study of 1357 Chinese manufacturing companies listed on the Shenzhen Stock Exchange, the authors found that the government's emergency-response policy triggered a statistically significant positive reaction from the stock market for manufacturers. However, the authors also found negative impacts on stock market values for manufacturers in labor-intensive industries because of the labor immobility triggered by the Level I measures. In addition, this article identified the positive role of operational slack in the form of financial slack and excess inventory in helping to maintain operations and business continuity, mitigate risks caused by the labor mobility restrictions, and improve supply chain resilience, which identifies operational slack as a supply chain resilience strategy to mitigate pandemic-related risks. When the COVID-19 pandemic broke out, the medical-product industry faced unprecedented demand shocks for personal protective equipment, including face masks, face shields, disinfectants, and gowns. Companies from various industries responded to the urgent need for these potentially life-saving products by adopting ad hoc supply chains in an exceptionally short time. In \"Realizing supply chain agility under time pressure: Ad hoc supply chains during the COVID-19 pandemic\" (M\u00fcller et al., 2023), the authors explored the use by 34 German companies of ad hoc supply chains to produce personal protective equipment. From these cases, the authors developed an emergent theoretical model of ad hoc supply chains around enablers of supply chain agility such as dynamic capabilities (the ability to integrate, build, and reconfigure internal and external competences to address rapidly scenario changes), entrepreneurial orientation (proactiveness, risk-taking, innovativeness, autonomy, and competitive aggressiveness), and temporary orientation (speedy action in a limited time). To cope with the COVID-19 crisis, many firms allowed their employees to work from home (WFH). In \"Working from home and firm resilience to the COVID-19 pandemic\" (Ge et al., 2023), the authors examined whether a firm's WFH capacity increased its resilience. The authors put forward and tested a unique data set that combines listed firms' financial data, epidemiological data, and online job postings data from China. They found that imposing COVID-19 anti-contagion policies on firms and their suppliers or customers significantly increased their operating revenue volatility, slowed their recovery, and had repercussions on their supply chains. WFH enhanced firms' resistance capacity by reducing the effect of COVID-19 on their operating revenue volatility and disruptions to their supply chain partners; however, it also decreased their recovery capacity by extending the time taken to return to normal. Firm attributes, along with workers' occupations, education, and experience, impacted the effect of WFH on firm resilience. This article enhances our understanding of shock transmission across supply..",
                "url": "https://openalex.org/W4365518480",
                "title": "Building responsive and resilient supply chains: Lessons from the <scp>COVID</scp>\u201019 disruption",
                "publication_date": "2023-04-01"
              },
              {
                "id": "E6357881701",
                "text": "..of giant techs is needed because researchers, doctors and scientists can be effectively supported to expedite the research and development of COVID-19 virus. 130822 VOLUME 8, 2020 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts Recently, IBM announced that they are now providing a cloud-based research resource that has been trained on a COVID-19 open dataset (CORD-19) [16], which is a collection of research articles related to COVID-19. Moreover, IBM has adopted their proposed AI technology for drug discovery, from which 3000 novel COVID-19 molecules have been obtained, officially reported in [17]. Another support is from the White House Office of Science and Technology Policy, the U.S. Department of Energy and IBM with the development of COVID-19 HPC Consortium (https://covid19- hpc-consortium.org/), which is open for research proposals concerning COVID-19. Another example is the Coronavirus International Research Team (COV-IRT) (https://www.covirt.org/), a group of scientists who are developing vaccines and therapeutic solutions against COVID-19. C. BIG DATA 1) DEFINITION AND CHARACTERISTICS The rapid development of the Internet of Things (IoT) results in a massive explosion of data generated from ubiquitous wearable devices and sensors [18]. The unprecedented increase of data volumes associated with advances of analytic techniques empowered from AI has led to the emergence of a big data era [18], [19]. Big data has been employed in a wide range of industrial application domains, including healthcare where electronic healthcare records (EHRs) are exploited by using intelligent analytics for facilitating medical services. For example, health big data potentially supports patient health analysis, diagnosis assistance, and drug manufacturing [20]. Big data can be generated from a number of sources which may include online social graphs, mobile devices (i.e. smartphones), IoT devices (i.e. sensors), and public data [21] in various formats such as text or video. In the context of COVID-19, big data refers to the patient care data such as physician notes, X-Ray reports, case history, list of doctors and nurses, and information of outbreak areas. In general, big data is the information asset characterized by such a high volume, velocity and variety to acquire specific technology and analytical methods for its transformation into useful information to serve the end users, i.e, big data in digital twin technologies [22]\u2013[25]. The three characteristics of big data are summarized as follows. \u2022 Volume: This feature shows the huge amount of data that can range from terabytes to exabytes. According to a Cisco\u2019s forecast, the data traffic is expected to reach 930 exabytes by 2020, a seven-fold growth from 2017 [26]. \u2022 Variety: It refers to the diversity and heterogeneity of big data. For example, big data in healthcare can be produced from healthcare users (i.e. doctors, patients), medical IoT devices, and healthcare organizations. Data can be formatted in text, images, videos with structured or un-structured dataset types [27]. \u2022 Velocity: It expresses the data generation rate that can be calculated in time or frequency domain. In fact, in industrial applications like healthcare, data generated from devices is always updated in real-time, which is of significant importance for time-sensitive applications such as health monitoring or diagnosis [28]. 2) BIG DATA FOR COVID-19 FIGHTING Big data has been proved its capability to support fighting infectious diseases like COVID-19 [29], [30]. Big data potentially provide a number of promising solutions to help combat COVID-19 epidemic. By combining with AI analytics, big data helps us to understand the COVID-19 in terms of outbreak tracking, virus structure, disease treatment, and vaccine manufacturing [31]. For example, big data associated with intelligent AI-based tools can build complex simulation models using coronavirus data streams for outbreak estimation. This would aid health agencies in monitoring the coronavirus spread and preparing better preventive measurements [32]. Models from big data also supports future prediction of COVID-19 epidemic by its data aggregation ability to leverage large amounts of data for early detection. Moreover, the big data analytics from a variety of real-world sources including infected patients can help implement large-scale COVID-19 investigations to develop comprehensive treatment solutions with high reliability [33], [34]. This would also help healthcare providers to understand the virus development for better response to the various treatment and diagnoses. Based on the above analysis, we want to highlight that big data analytics is the process of collecting and analyzing the large volume of data sets to discover useful hidden patterns and other information, e.g., COVID-19 data discovering. Moreover, AI (and explainable AI [35]) aims to apply logic and reasoning to build human intelligence that can mimic the function of a machine for learning, classifying, and estimating possible outcomes, e.g., COVID-19 symptom classifications. The potential applications of each technology in fighting COVID-19 pandemic will be explained and discussed in the following sections through a number of practical use cases. III. APPLICATIONS OF AI FOR FIGHTING COVID-19 This section presents representative applications of AI in fighting the COVID-19 outbreak. A. AI FOR COVID-19 DETECTION AND DIAGNOSIS As one of the most effective solutions to combat the COVID-19 pandemic, early treatment and prediction are of importance. Currently, the standard method for classifying respiratory viruses is the reverse transcription polymerase chain reaction (RT-PCR) detection technique. In response to the COVID-19 virus, some efforts have been dedicated to improve this technique [36] and for other alternatives [37]. These techniques are, however, usually costly and time-consuming, have low true positive rate, and require specific materials, equipment and instruments. Moreover, most countries are suffering from a lack of testing kits due to the limitation on budget and techniques. Thus, VOLUME 8, 2020 130823 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts FIGURE 2. An illustration of DL-based frameworks for the COVID-19 detection and diagnosis. the standard method is not suitable to meet the requirements of fast detection and tracking during the COVID-19 pandemic. A simple and low-cost solution for COVID-19 identification is using smart devices together with AI frameworks [38], [39]. This is referred to as mobile health or mHealth in the literature [40]. These works are advantageous since smart devices are daily used for multi-purposes. Moreover, the emergence of cloud and edge computing can effectively overcome the limitation of batter, storage, and computing capabilities [41]. Another directive for COVID-19 detection is to use AI techniques for medical image processing, which recently appeared in many research works on coronavirus [42]\u2013[51]. As we limit this paper to the COVID-19 virus, the interested readers are invited to read the surveys in [52], [53] for other applications of DL in medical image analysis. It is noted from these works that X-ray images and computed tomography (CT) scans are widely used as the input of DL models so as to automatically detect the infected COVID-19 case. Motivated by an important finding that infected COVID-19 patients normally reveal abnormalities in chest radiography images, the authors in [54] designed a deep convolutional neural network (CNN) model for the detection of COVID-19 cases. The three-class classification (normal, COVID-19 infected, and non-COVID-19 infected) in this work is helpful if the medical staff needs to decide which cases should be tested with standard methods (between normal and COVID-19 infected cases), and which treatment strategies should be taken (between non- and COVID-19 infected cases). By training on an open source dataset with 13, 975 images of 13, 870 patients, the proposed CNN model can achieve an accuracy of 93.3%. The use of ML and DL techniques with chest CT scans for COVID-19 detection were considered in [42], [43], [55]\u2013[58], respectively. These works show a high performance as they can achieve a high classification accuracy, e.g., 99.68% in [42], an area under curve (AUC) score of 0.994 in [43], AUC of 0.996 in [55], and accuracy of 82.9% (98.27%) with specificity of 80.5% (97.60%) and sensitivity of 84% (98.93%) in [56] and [57]. As the cost of X-ray scans is usually cheaper than CT images, a large portion of research works for COVID-19 detection utilize DL models with X-ray images. For example, the work in [46] leveraged a deep CNN model, called Decompose, Transfer, and Compose (DeTraC), to process chest X-ray images for the classification of COVID-19. The main purpose of the decomposition layer is to reduce the feature space, thus yielding more sub-classes but improving the training efficiency, whereas the composition layer is to combine sub-classes from the previous layer so as to produce the final classification result. Besides the decomposition and composition layers, a transfer layer is positioned in the middle to speed up the training time, reduce the computation costs, and make the DL model trainable with small datasets. The importance of transfer learning made it a wide consideration in many works, e.g., [45], [48], [49]..",
                "url": "https://openalex.org/W3019283372",
                "title": "Artificial Intelligence (AI) and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts",
                "publication_date": "2020-01-01"
              }
            ]
          }
        ]
      },
      "S8597788776": {
        "id": "S8597788776",
        "text": "The growing awareness among individuals about how their personal data is used necessitates that healthcare organizations adopt ethical practices in data collection and analysis to maintain customer trust and loyalty during the COVID-19 pandemic.",
        "children": [
          {
            "id": "E3394960366",
            "text": "..to SCM. They present a range of practical applications, and identify potential further research areas with the key message being that more research is required at the nexus of big data and SCM. The main difference between this paper and other survey papers is that we provided a more comprehensive and rigorous review of Big Data Analytics from 2005-2020, drawn from mainstream Journals. More so, we critically identified the main challenges of this disruptive technology in operations and supply chain management, particularly the key ethical, security and privacy issues posed to businesses, society and industrial community. Furthermore, our work acknowledge the growth of Industrial Internet of Things (IIoT) has increased the applicability and transformation of big data analytics across industries such as healthcare sector, science, engineering, finance, business, and eventually the society (Musa and Dabo, 2016). A good example is in pharmaceutical sector, where big data and mobile health are starting to transform the healthcare and diagnostics in a significant way, with new players such as Apple and Google acting as increasingly disruptive catalysts (Gautam and Pan, 2016; cited by Festa et al., 2018). Vaccine manufacturing firm, MERCK implemented Hadoop to utilise huge amount of data in producing vaccines faster and reduce the discard rates which were quite steep (Henschen, 2014; cited by Lamba and Singh, 2017). Further example for big data application in the healthcare sector is the utilisation of enormous volume of information from patient data by Scientist to detect drug interactions and design to implement optimal drug therapies (healthworkscollective.com, 2014; cited by Festa et al., 2018). ). In fact, big data can help fight the spread of communicable diseases. For example, a retrospective analysis of the 2010 cholera outbreak in Haiti indicated that mining data from Twitter and online news reports could have given the country\u2019s health officials an accurate indication of the disease\u2019s spread with lead time of two weeks (Kshetri, 2014). In banking sectors and retail industries, and in government, big data is applied extensively for analysing spending behaviour of customers, security and fraud detection, sentiment and social network analysis (Kshetri, 2014). In addition, big data can help law enforcement agencies to deploy resources more efficiently, respond quickly and increase presence in crime prone areas (Kang, 1998). Big data also enhances retail supply chain, which heavily depends on the availability of accurate and up-to-date information for demand forecast and other business execution (Gunasekaran et al., 2016). According to Schoenherr and Speier-Pero (2015; cited by Bumblauskas et al., 2017), nearly 50 percent of supply chain professionals were using BDA or had plans to use BDA in the near future. These professionals saw BDA as an enabler for better decision making, as well as a predictive analytical tool to achieve performance advantages (Bumblauskas et al., 2017). Raman et al. (2018) have also noted that the concept of big data helps to improve visibility by providing an integrated framework for monitoring performance and customer interaction through real-time data analysis and critical decision-making scenarios. Thus militating risk and supply chain disruption and failures. According to IBM\u2019s Big data and Analytics Hub (2014), \u201c10 billion mobile phones will be in use by 2020, 294 billion emails are sent every day, and trillion of sensors monitor, track with each other, populating the Internet of Things with real time data\u201d. Several supply chain literature (Baihaqi and Sohal, 2013; Li et al., 2009; Prajogo and Olhager, 2012; Vickery et al., 2003; cited by Raman et al., 2018) have acknowledged that the use of various information technologies such as Enterprise Resource Planning (ERP), Global Positioning System (GPS), Radio Frequency Identification (RFID), and Electronic Data Interchange (EDI) in operations and supply chain management enabled firms to effectively maintain control over their information, material and financial flows. Additional example of companies that adopt big data analytics and customer profiling are Amazon and Wal-Mart (Accenture, 2014). They utilise technologies to monitor, track, and secure millions of items in their inventories and rely on forecasting analytics for their \u2018\u2019anticipatory shipping\u2019\u2019, thereby predicting when customers will purchase a product and preship it to a depot close to the final destination (Rozados and Tjahjono, 2014;cited by Niebel et al., 2018). Similarly, Ford Motor Company, employed big data in capturing consumer data from vehicles through sensors and remote app-management software (Niebel et al., 2018). Based on the analysis of data from their cars\u2019 voice recognition systems, the company found out that the surrounding noise affected the performance of the software, which led to the improvement of the system through noise reduction technology and repositioning of microphone. These examples indicate the applicability of big data analytics, particularly in operations and supply chain, especially in its decision-making capacity, which ensure process improvement, management of logistics activities, enhancement of inventory management, and cost optimisation. However, despite the impacts of these disruptive technologies, it required guidance and interpretation by people who knew the tools as well as the business, including the supply chain operators. According to Lamba and Singh (2017), big data provided limited real-time applications in the context of functional areas of operations and supply chain management such as procurement, production, logistics and forecasting. In addition, the author argued that there are certain barriers to implementing predictive analytics, such as the lack of skilled professionals, lack of awareness and dearth of tools for training the next generation of data scientists in the supply chain industry. Alter et al. (2013; cited by Yang et al., 2017) went further to highlight three kinds of expertise needed in supply chain data science, namely: Technological knowledge about the tools and technology capability, functional knowledge about supply chain management, and mathematical knowledge of algorithms. The authors maintained that the blend of these three skillsets will enable companies combine data into new insights for the enterprise-decision making. Accordingly, these skillsets are in short supply now and for the foreseeable future, across all areas of business, including supply chain sector. Yang et al., (2017) echoed that the major difficulties lie in data capture, storage, searching, sharing, analysis, and visualisation. The authors warned that if organisations failed in surmounting these major skillsets gap, Big Data will become a gold mine that firms lacked the capabilities to explore. Against this background therefore, this study offers useful contribution by addressing the research question: RQ1. How is Big Data Analytics being applied and in which ways it has improved supply chain? 3. Ethical, Privacy and Security challenges of big data business analytics Tremendous progress has been made in computing power, storage capacity, and software, resulting in an increased surge in big data technologies (Pauleen et al., 2017; cited by Maple, 2017). This upsurge, has brought several privacy concerns (Biajian et al., 2018). Similarly, it has also led to the growth in the number of smart devices and the speed of that growth present challenges, particularly security and privacy issues to society and individuals\u2019 freedom, as government battle to develop policies, standards and laws that shape this development (Maple, 2017). The question is whether such laws and regulations offer adequate protections for individuals\u2019 data, especially when it relates to Internet of Things. Of course, there is no known new laws that address this emerging phenomenon. Instead, they are governed by existing legal framework. In the UK for example, personal data remains subject to the Data Protection Act of 1998. Although, there is a new proposed regulatory framework by UK government that will improve citizens\u2019 online safety (Online Hams Whitepaper, 2019). Moreover, the characteristics of big data are tightly linked to privacy, ethical and security effects to customer welfare and these fallouts have attracted the attention of scholars, businesses, society, industries and policy makers (Kshetri, 2014). According to ISACA 9 (2014) and Kshetri (2014) huge amount of data means that security breaches and privacy violation are likely, which can cause severe consequences and losses, leading to reputational damage, legal liability, ethical harms and other amplified technical issues. Further, Laney (2001) outlined some factors that can necessitate these challenges, namely: (1) the enormous amount of data (volume), (2) a wide variety of data coming from highly diverse sources (variety), and (3) the pace of data processing (velocity). Therefore, the greatest challenge now is to find new ways of storing and processing these different types of data, since the old databases are mainly structured for only one type of data (Chen et al., 2014 and Hofmann, 2017). The analysis of these data, extraction of insights and appropriation of its values, without any infringement to ethical, security, and personal privacy data protection, presents a key challenge to not only business but also society. Therefore, against this background, this paper will attempt to answer the following research questions: RQ2. What are the ethical, security and privacy challenges associated with Big Data Analytics? RQ3. What are the likely societal, business and governance roles in addressing these emerging technologies? 3.1. Ethical issues The importance of ethics is very significant in big data analytics, especially when it comes to profiling of users through social media analytics. At the same time, people are increasingly becoming aware of how their personal data are being used, even as some individuals and firms are very keen to trade personal data for commercial gains. Therefore, specific laws and regulations must be established to preserve the personal and sensitive information of customers, as no specific laws address these invasive and intrusive technologies. Some data will need more protection than others. For example, a sensor tracking pallets, being shipped overseas is unlikely to transmit such sensitive personal data by comparison with a wearable fitness device, which for instance, might be transmitting medical information. It is also clear that connected devices are often located in intimate spaces such as the home and car or the case of smart pills that..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          },
          {
            "id": "E3902845575",
            "text": "..shown in Table 4. Moreover, the matrix level was explored based on broad theme and sub-themes. This was done in a way that shows a progressing expansion of conceptual framework, from bottom-up to topdown approach. The bottom-up approach is the beginning of the extraction of sub-themes, as individual research articles in the review were determined inductively. The coding was designed in a manner that allowed the capturing of the most dominant conceptual standpoints in the articles towards the main issues of privacy and security, as well as on how big data analytics can enhance economic efficiency, improve access to social services, as well as increase innovative platform for information and communications. To ensure validity and reliability of the coding, the research team read the articles in chronological order starting from 2005 to 2020. Separate lists were generated and crossvalidated. There were discussions among the research team about the classification of the themes and sub-themes and consensus were reached. In general, the reconciliation of the useful insights arising from the top-bottom and bottom-up approaches were achieved within the research team and the team adopted the four principal facets of BDSCA development, namely: big data analytics, applications, ethics and privacy \u2013 for representing the conceptual matrix at the top-most level. The next section explained the results in details. Table 4: Thematic landscape in terms of broad themes and sub \u2013themes (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (Number): showing the conceptual underpinning of the sub-them focus Conceptual standpoints of selective articles (full list of the authors) Big data analytics (1): conceptualisation of big data analytics represents a paradigm shift, enabling better exploitation of the growing amount of data that originates within or outside the focal business environment. Article Stream (1): this conceptualisation is helpful as big data platforms can influence business process management in productivity and flexibility for an enterprise. Article (stream 2): focus increases in the volume and detail of data captured by organisations, such as rise in social media, Internet of Things, Cloud computing, cyberphysical systems, Machine learning, 3D printing, and multimedia has produced overwhelming flow of data in either structured or unstructured format. These Industry 4.0 impacts revenue growth, competitiveness, productivity gains to organisations. Article (stream 3): the evolutionary trend of these technologies has shifted focus from the traditional supply chain data to an accelerating data sources characterised by volume, variety, veracity and value. Chae (2015) proposed a new analytical framework designed for the assessment of social use in supply chain setup. Based on Twitter hashtags, the author investigates how Twitter posts can help companies to better shape demand whilst at the same time delivering valuable customer insights, which are useful for new products developments. Although the presented approach is an interesting application of Big data in supply chain context, it lacks a wider conceptual approach. Musa and Dabo. (2016) investigated the status of Big data in the environments of cloud computing. The relationship between big data and cloud computing were addressed, with particular focus on scalability, data integrity, availability, data transformation and regulatory and legal issues. However, the study has limited managerial inputs in terms of supply chain and operations applications. Combining RFID and cloud computing, Guo et al. (2015) present an intelligent decision support system architecture for production monitoring and scheduling in labour-intensive distributed manufacturing setups. This research addresses main benefits of linking Big Data Analytics and SCM, as the novel architecture approach highlights how the systematic integration of data-driven decision making in production and logistics operations increases information visibility and transparency across supply chain. Supply chain shocks and adaptations amid the covid-19 pandemic and post-pandemic recoveries provide indisputable evidence for the urgent needs of digital techniques for mapping supply chain networks and ensure visibility (Ivanov and Dolgui, 2020). Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number): broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Big data analytics (2): Being able to make sense of a subjective, disparate data, such as audio files, in the system for analysis remains a challenge. Article stream (4): a focus on Machinelearning allows a software to respond to data such as customer shopping trends, weather and geography, and can sense patterns in shopping behaviour. Article (stream 5): there are more focus in employing machine learning through uploading of information and making predictions about the future behaviour based on behavioural residues and interpretation of individuals\u2019 footprints. Article (stream 6): Similar focus on other related advanced technology such as cloud computing to mitigate the time-demanding and challenging task, which required computational infrastructure to ensure successful data processing and analysis. Kenda and Dunja (2018) proposed a model that uses an algorithm for data cleaning stream mining with low noise, which intends to perform better than the current state-of-the-art batch processing method. Although this proposed model sounds very plausible, it shows limited applications since data preparation is crucial for real-time data mining applications. The article by Lopez-Cuadrado et al. (2019; cited by Din et al, 2019) investigates big and small datasets in a non-big data environment, analysing big datasets by applying the first stage machinelearning model. The paper proposes a framework that focused on data preparation by finding an optimal machine \u2013learning model. The study established layers of big data architectures that focused on getting an outcome of data. However, the framework fails to address how this approach enhances supply chain operations. Din et al. (2019) investigates how Machinelearning used behavioural residue in digital footprints to assess and drive personality predictions, particularly in examining Anxiety in micro-blogs from social media tweets. This particular technique is new, and it is limited to healthcare sector. In addition, the research by Zhaodan et al. (2016; cited by Gruda and Souleiman. 2018) provided a link between machine-learning and cyber-physical systems performance. However, the article showed limited managerial inputs in SCM in terms of the applicability of this technology. Presenting findings on how Supply Chain (SC) 4.0 can gain competitive advantage, the research by Makris et al. (2019) focus on three key emerging developmental trends: Big data, Cloud Computing and 3D printing. Although the article presents graphical framework but show limitations in understanding the issues around adaptability, strength and challenges. Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Applications (1): Big data application complements decision support systems (DSS), foster complex decision-making and help problem solving in supply chain management. Article stream (7): the focus is on the emphasis of the properties of big data in providing useful business insights to companies in decision making towards optimising operations and supply chain. Article stream (8): Big data is also used for positioning of goods on the shop shelves as well as employed to get a better handle on company account, manage regulatory requirements and cost savings. Tan et al. (2015), for example, highlighted the potential of Big data as a source for supply chain innovation. Being an interesting contribution to research at the nexus of Big data Analytics and SCM, the proposed analytical structured approach enables companies to integrate their supply chain innovation competencies with supply chain partners systematically, leveraging Big data Analytics insights. In an increasingly competitive and fast-paced business environment, the article\u2019s promoted extension of the innovation capability into the supply chain seems a promising strategy, considering that crucial knowledge may reside outside a single company. The article presented by Sabashini and Kavitha (2011) contributes to the theory development in SCM through investigation of the potential impact of Big Data Analytics on information usage in a corporate and supply chain context. The paper provided insights into the opportunities and challenges emerging form the adoption of Big Data Analytics in SCM. Drawing from resource-based view, Chavez et al. (2017) investigated the linkage between data-driven supply chains, manufacturing capability and customer satisfaction. The research outcomes suggest that data-driven supply chain are positively associated with manufacturing competitiveness dimensions such as quality, delivery, flexibility, and cost, which in turn leads to customer satisfaction. Addressing the challenges linked to the growing amount of data, a major problem in modern manufacturing, Zhang et al. (2015) present an integrated approach for optimised logistics management on the shop floor, based on the smart manufacturing concept, using production resources equipped with RFID facilities. Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Applications (2): Business intelligence and analytics are enhanced with Big data for decision on strategy, managing optimisation and competition. Article stream (9): the focus is centred on the prediction that Big data would improve 60 per cent of existing business and will foster new businesses in the next decade. Article stream (10): the focus in real-time information processing technology based on RFID and NFC in IoT will be widely used in supply chain. As result, the demand forecast would be more accurate and extra buffers would be unnecessary. Opresnik and Taisha (2015; cited by Wemba et al., 2015) linked the advantages of Big Data to the business strategy of servitisation. Their proposed strategic framework outlines how companies can extend their value portfolio by adding information as the pillar alongside the traditional aspects of product and service. Barbaray et. (2018; cited by Fatorachian and Kazemi, 2018) reviewed research on Industry 4.0 concerning issues of SMEs and suggested that SMEs adopt Industry 4.0 concepts only for monitoring industrial processes. Presenting findings from a systematic review and longitudinal case..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          },
          {
            "id": "E4940110996",
            "text": ".., Cuomo, M.T. and Solima, L. (2018). \"Big data for big pharma: Harmonizing business process management to enhance ambidexterity\u201d, Business Process Management Journal, 24(5), pp. 1110-1123. https://doi.org/10.1108/BPMJ-10-2017-0272 Financial Times Archive (2015). Gartner, M., and Brocca. R. 2015. \u2018\u2019Deconstructing supply chain analytics.\u2019\u2019 Journal of Supply Chain Management, 25(2015): 200-326. Gunasekaran, A., Papadopoulos, T., and Dubey, R., and Childe, S.J.Stephen J. 2016. \u2018\u2019Big data and predictive analytics for supply chain and organizational performance.\u2019 Journal of Business Research, 70(2016): pp.308-31. https://doi.org/10.1016/j.jbusres.2016.08.004 Gruda, D. and Hasan, S., 2019. \u201cFeeling anxious? Perceiving anxiety in tweets using machine learning\u201d, Computers in Human Behaviour, 98(2019), pp.245-255. https://doi.org/10.1016/j.chb.2019.04.020 Guo, Z., Ngai, E., Yang, C. and Liang, X., 2015. \u201cAn RFID-based intelligent decision support system architecture for production monitoring and scheduling in a distributed manufacturing environment\u201d, International journal of production economics, 159, pp.16-28. Harris, I., Wang, Y. and Wang, H., 2015. \u201cICT in multimodal transport and technological trends: Unleashing potential for the future\u201d, International Journal of Production Economics, 159(2015), pp.88-103. https://doi.org/10.1016/j.ijpe.2014.09.005 Hashem, I.A.T., Yaqoob, I., Anuar, N.B., Mokhtar, S., Gani, A. and Khan, S.U., 2015. \u201cThe rise of \u201cbig data\u201d on cloud computing: Review and open research issues\u201d, Information systems, 47(2015), pp.98-115. https://doi.org/10.1016/j.is.2014.07.006 Hofmann, E., 2017. \u201cBig data and supply chain decisions: the impact of volume, variety and velocity properties on the bullwhip effect\u201d, International Journal of Production Research, 55(17), pp.5108-5126. https://doi.org/10.1080/00207543.2015.1061222 Ivanov, D. and Dolgui, A., 2020. \u201cA digital supply chain twin for managing the disruption risks and resilience in the era of Industry 4.0\u201d, Production Planning & Control, pp.1-14. https://doi.org/10.1080/09537287.2020.1768450 Kenda, K. and Mladeni\u0107, D., 2018. \u201cAutonomous sensor data cleaning in stream mining setting\u201d, Business Systems Research Journal, 9(2), pp.69-79. DOI: https://doi.org/10.2478/bsrj-2018-0020 Kshetri, N., 2014. \u201cBig data\u05f3 s impact on privacy, security and consumer welfare\u201d, Telecommunications Policy, 38(11), pp.1134-1145. https://doi.org/10.1016/j.telpol.2014.10.002 Kusiak, A., 2018. \u201cSmart manufacturing\u201d, International Journal of Production Research, 56(1- 2), pp.508-517. https://doi.org/10.1080/00207543.2017.1351644 Lamba, K. and Singh, S.P., 2017. \u201cBig data in operations and supply chain management: current trends and future perspectives\u201d, Production Planning & Control, 28(11-12), pp.877- 890. https://doi.org/10.1080/09537287.2017.1336787 Li, J., Tao, F., Cheng, Y. and Zhao, L., 2015. \u201cBig data in product lifecycle management\u201d, The International Journal of Advanced Manufacturing Technology, 81(1-4), pp.667-684. Luppicini, R. and So, A., 2016. \u201cA technoethical review of commercial drone use in the context of governance, ethics, and privacy\u201d, Technology in Society, 46, pp.109-119. https://doi.org/10.1016/j.techsoc.2016.03.003 Mahrt, M. and Scharkow, M., 2013. \u201cThe value of big data in digital media research\u201d, Journal of Broadcasting and Electronic Media, 57(1), pp.20-33. https://doi.org/10.1080/08838151.2012.761700 Makris, D., Hansen, Z.N.L. and Khan, O., 2019. \u201cAdapting to supply chain 4.0: an explorative study of multinational companies\u201d, In Supply Chain Forum: An International Journal, 20(2), pp. 116-131. Maple, C., 2017. \u201cSecurity and privacy in the internet of things\u201d, Journal of Cyber Policy, 2(2), pp.155-184. https://doi.org/10.1080/23738871.2017.1366536 Mendez Mena, D., Papapanagiotou, I. and Yang, B., 2018. \u201cInternet of things: Survey on security\u201d, Information Security Journal: A Global Perspective, 27(3), pp.162-182. Mikalef, P., Boura, M., Lekakos, G. and Krogstie, J., 2019. \u201cBig data analytics and firm performance: Findings from a mixed-method approach\u201d, Journal of Business Research, 98(2019), pp.261-276. https://doi.org/10.1016/j.jbusres.2019.01.044 Moeuf, A., Pellerin, R., Lamouri, S., Tamayo-Giraldo, S. and Barbaray, R., 2018. \u201cThe industrial management of SMEs in the era of Industry 4.0\u201d, International Journal of Production Research, 56(3), pp.1118-1136. https://doi.org/10.1080/00207543.2017.1372647 Mueller, S.D. and Jensen, P., 2017. \u201cBig data in the Danish industry: application and value creation\u201d, Business Process Management Journal. 23(3), pp. 645-670. https://doi.org/10.1108/BPMJ-01-2016-0017 Musa, A. and Dabo, A.A.A., 2016. \u201cA review of RFID in supply chain management: 2000\u2013 2015\u201d, Global Journal of Flexible Systems Management, 17(2), pp.189-228. https://doi.org/10.1007/s40171-016-0136-2 Niebel, T., Rasel, F. and Viete, S., 2019. \u201cBIG data\u2013BIG gains? Understanding the link between big data analytics and innovation\u201d, Economics of Innovation and New Technology, 28(3), pp.296-316. https://doi.org/10.1080/10438599.2018.1493075 Oberl\u00e4nder, A.M., R\u00f6glinger, M., Rosemann, M. and Kees, A., 2018. \u201cConceptualizing business-to-thing interactions\u2013A sociomaterial perspective on the Internet of Things\u201d, European Journal of Information Systems, 27(4), pp.486-502. https://doi.org/10.1080/0960085X.2017.1387714 Opresnik, D. and Taisch, M., 2015. \u201cThe value of big data in servitization\u201d, International journal of production economics, 165, pp.174-184. https://doi.org/10.1016/j.ijpe.2014.12.036 Phase, A. and Mhetre, N., 2018. \u201cUsing IoT in Supply Chain Management\u201d International Journal of Engineering and Techniques, 4 (2), pp. 973-979. Psannis, K.E., Xinogalos, S. and Sifaleras, A., 2014. \u201cConvergence of Internet of things and mobile cloud computing\u201d, Systems Science & Control Engineering: An Open Access Journal, 2(1), pp.476-483. https://doi.org/10.1080/21642583.2014.913213 Queiroz, M.M. and Telles, R., 2018. \u201cBig data analytics in supply chain and logistics: An empirical approach\u201d, The International Journal of Logistics Management, 29(2), pp. 767-783. https://doi.org/10.1108/IJLM-05-2017-0116 Raman, S., Patwa, N., Niranjan, I., Ranjan, U., Moorthy, K. and Mehta, A., 2018. \u201cImpact of big data on supply chain management\u201d, International Journal of Logistics Research and Applications, 21(6..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          }
        ]
      },
      "S7852842407": {
        "id": "S7852842407",
        "text": "The application of big data analytics in healthcare supply chains during COVID-19 has led to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions.",
        "children": [
          {
            "id": "E6357881701",
            "text": "..of giant techs is needed because researchers, doctors and scientists can be effectively supported to expedite the research and development of COVID-19 virus. 130822 VOLUME 8, 2020 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts Recently, IBM announced that they are now providing a cloud-based research resource that has been trained on a COVID-19 open dataset (CORD-19) [16], which is a collection of research articles related to COVID-19. Moreover, IBM has adopted their proposed AI technology for drug discovery, from which 3000 novel COVID-19 molecules have been obtained, officially reported in [17]. Another support is from the White House Office of Science and Technology Policy, the U.S. Department of Energy and IBM with the development of COVID-19 HPC Consortium (https://covid19- hpc-consortium.org/), which is open for research proposals concerning COVID-19. Another example is the Coronavirus International Research Team (COV-IRT) (https://www.covirt.org/), a group of scientists who are developing vaccines and therapeutic solutions against COVID-19. C. BIG DATA 1) DEFINITION AND CHARACTERISTICS The rapid development of the Internet of Things (IoT) results in a massive explosion of data generated from ubiquitous wearable devices and sensors [18]. The unprecedented increase of data volumes associated with advances of analytic techniques empowered from AI has led to the emergence of a big data era [18], [19]. Big data has been employed in a wide range of industrial application domains, including healthcare where electronic healthcare records (EHRs) are exploited by using intelligent analytics for facilitating medical services. For example, health big data potentially supports patient health analysis, diagnosis assistance, and drug manufacturing [20]. Big data can be generated from a number of sources which may include online social graphs, mobile devices (i.e. smartphones), IoT devices (i.e. sensors), and public data [21] in various formats such as text or video. In the context of COVID-19, big data refers to the patient care data such as physician notes, X-Ray reports, case history, list of doctors and nurses, and information of outbreak areas. In general, big data is the information asset characterized by such a high volume, velocity and variety to acquire specific technology and analytical methods for its transformation into useful information to serve the end users, i.e, big data in digital twin technologies [22]\u2013[25]. The three characteristics of big data are summarized as follows. \u2022 Volume: This feature shows the huge amount of data that can range from terabytes to exabytes. According to a Cisco\u2019s forecast, the data traffic is expected to reach 930 exabytes by 2020, a seven-fold growth from 2017 [26]. \u2022 Variety: It refers to the diversity and heterogeneity of big data. For example, big data in healthcare can be produced from healthcare users (i.e. doctors, patients), medical IoT devices, and healthcare organizations. Data can be formatted in text, images, videos with structured or un-structured dataset types [27]. \u2022 Velocity: It expresses the data generation rate that can be calculated in time or frequency domain. In fact, in industrial applications like healthcare, data generated from devices is always updated in real-time, which is of significant importance for time-sensitive applications such as health monitoring or diagnosis [28]. 2) BIG DATA FOR COVID-19 FIGHTING Big data has been proved its capability to support fighting infectious diseases like COVID-19 [29], [30]. Big data potentially provide a number of promising solutions to help combat COVID-19 epidemic. By combining with AI analytics, big data helps us to understand the COVID-19 in terms of outbreak tracking, virus structure, disease treatment, and vaccine manufacturing [31]. For example, big data associated with intelligent AI-based tools can build complex simulation models using coronavirus data streams for outbreak estimation. This would aid health agencies in monitoring the coronavirus spread and preparing better preventive measurements [32]. Models from big data also supports future prediction of COVID-19 epidemic by its data aggregation ability to leverage large amounts of data for early detection. Moreover, the big data analytics from a variety of real-world sources including infected patients can help implement large-scale COVID-19 investigations to develop comprehensive treatment solutions with high reliability [33], [34]. This would also help healthcare providers to understand the virus development for better response to the various treatment and diagnoses. Based on the above analysis, we want to highlight that big data analytics is the process of collecting and analyzing the large volume of data sets to discover useful hidden patterns and other information, e.g., COVID-19 data discovering. Moreover, AI (and explainable AI [35]) aims to apply logic and reasoning to build human intelligence that can mimic the function of a machine for learning, classifying, and estimating possible outcomes, e.g., COVID-19 symptom classifications. The potential applications of each technology in fighting COVID-19 pandemic will be explained and discussed in the following sections through a number of practical use cases. III. APPLICATIONS OF AI FOR FIGHTING COVID-19 This section presents representative applications of AI in fighting the COVID-19 outbreak. A. AI FOR COVID-19 DETECTION AND DIAGNOSIS As one of the most effective solutions to combat the COVID-19 pandemic, early treatment and prediction are of importance. Currently, the standard method for classifying respiratory viruses is the reverse transcription polymerase chain reaction (RT-PCR) detection technique. In response to the COVID-19 virus, some efforts have been dedicated to improve this technique [36] and for other alternatives [37]. These techniques are, however, usually costly and time-consuming, have low true positive rate, and require specific materials, equipment and instruments. Moreover, most countries are suffering from a lack of testing kits due to the limitation on budget and techniques. Thus, VOLUME 8, 2020 130823 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts FIGURE 2. An illustration of DL-based frameworks for the COVID-19 detection and diagnosis. the standard method is not suitable to meet the requirements of fast detection and tracking during the COVID-19 pandemic. A simple and low-cost solution for COVID-19 identification is using smart devices together with AI frameworks [38], [39]. This is referred to as mobile health or mHealth in the literature [40]. These works are advantageous since smart devices are daily used for multi-purposes. Moreover, the emergence of cloud and edge computing can effectively overcome the limitation of batter, storage, and computing capabilities [41]. Another directive for COVID-19 detection is to use AI techniques for medical image processing, which recently appeared in many research works on coronavirus [42]\u2013[51]. As we limit this paper to the COVID-19 virus, the interested readers are invited to read the surveys in [52], [53] for other applications of DL in medical image analysis. It is noted from these works that X-ray images and computed tomography (CT) scans are widely used as the input of DL models so as to automatically detect the infected COVID-19 case. Motivated by an important finding that infected COVID-19 patients normally reveal abnormalities in chest radiography images, the authors in [54] designed a deep convolutional neural network (CNN) model for the detection of COVID-19 cases. The three-class classification (normal, COVID-19 infected, and non-COVID-19 infected) in this work is helpful if the medical staff needs to decide which cases should be tested with standard methods (between normal and COVID-19 infected cases), and which treatment strategies should be taken (between non- and COVID-19 infected cases). By training on an open source dataset with 13, 975 images of 13, 870 patients, the proposed CNN model can achieve an accuracy of 93.3%. The use of ML and DL techniques with chest CT scans for COVID-19 detection were considered in [42], [43], [55]\u2013[58], respectively. These works show a high performance as they can achieve a high classification accuracy, e.g., 99.68% in [42], an area under curve (AUC) score of 0.994 in [43], AUC of 0.996 in [55], and accuracy of 82.9% (98.27%) with specificity of 80.5% (97.60%) and sensitivity of 84% (98.93%) in [56] and [57]. As the cost of X-ray scans is usually cheaper than CT images, a large portion of research works for COVID-19 detection utilize DL models with X-ray images. For example, the work in [46] leveraged a deep CNN model, called Decompose, Transfer, and Compose (DeTraC), to process chest X-ray images for the classification of COVID-19. The main purpose of the decomposition layer is to reduce the feature space, thus yielding more sub-classes but improving the training efficiency, whereas the composition layer is to combine sub-classes from the previous layer so as to produce the final classification result. Besides the decomposition and composition layers, a transfer layer is positioned in the middle to speed up the training time, reduce the computation costs, and make the DL model trainable with small datasets. The importance of transfer learning made it a wide consideration in many works, e.g., [45], [48], [49]..",
            "url": "https://openalex.org/W3019283372",
            "title": "Artificial Intelligence (AI) and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts",
            "publication_date": "2020-01-01"
          },
          {
            "id": "S0323885390",
            "text": "The application of big data analytics in healthcare supply chains during COVID-19 has been linked to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions.",
            "children": [
              {
                "id": "E3155705893",
                "text": "..against damage or loss, as well as the ability to rapidly recover from a disruption (Hora & Klassen, 2013). Thus, resilience is dynamic instead of static, which is considered as a fundamental attribute that supply chains need to adopt for maintaining stable growth in the face of external disruptions (Essuman et al., 2020) such as the COVID-19 pandemic. Supply chain resilience consists of multiple constituent elements, including stability, agility, robustness, collaboration, redundancy, centralization, visibility, and information sharing (Hosseini et al., 2019). Tukamuhabwa et al. (2015) emphasized the importance of building collaborative relationships in improving supply chain resilience. Other measures include maintaining slack resources, adopting a flexible production strategy, and building a risk-management infrastructure (Ambulkar et al., 2015; Modi & Mishra, 2011). Although 2Rs are the key attributes for enterprises to improve supply chain performance, there is limited research on how to develop responsive and resilient strategies to deal with long-lasting, structural, and global impacts. Previous studies mainly focused on normal situations, in which supply chain integration and big data analytics are the bases of building 2Rs supply chains. Supply chain integration is the ability to integrate all activities among a company's internal functions and external partners (supplier, distributor, retailer, etc.), until the finished product arrives at the end customer (Zhao et al., 2013). From the perspective of collaborative partners, supply chain integration could be divided by horizontal strategies and vertical strategies (Mesquita & Lazzarini, 2008), while the vertical integration is further divided by integration with suppliers (also known as upstream integration) and integration with customers (also known as downstream integration). In the era of Industry 4.0, supply chain integration consists of three dimensions: process and activity integration, technology and system integration, and organizational relationship linkages (Tiwari, 2020). A recent editorial in JOM (Browning, 2020) discussed related aspects of organizational and process integration. In recent years, under the influence of economic globalization, supply chains have been transformed. Since globalization has become pervasive, suppliers have pursued global markets, and most companies source extensively from global suppliers (Cohen & Lee, 2020). This has led to an increase in outsourcing activities and a corresponding decline in vertical integration of supply chains. As a result, supply chain networks have become flatter and more complex, composed of different organizations dispersed across multiple tiers and different geographies, and extended beyond a single country's boundaries (Choi & Hong, 2002). Global supply chains are characterized by focal firms that distribute across multiple countries, locate production facilities abroad, or source from offshore suppliers. Munir et al. (2020) showed that integration in the global supply chain could increase companies' resilience in making flexible deliveries and the number of products. Big data refers to data that arrive at a high volume and velocity with considerable variation, while analytics refers to the ability to gain insights from data via statistics, learning, optimization, or other techniques. The applications of big data and analytics are closely interlinked to enable firms to make better decisions. Hence, prior literature has typically discussed them together as big data analytics, which allows the use of advanced computing techniques, strategies and architectures to store, extract, and analyze multi-source, heterogeneous data to support decisions, and has been commonly used in operations management (Wamba et al., 2015). In the era of economic globalization, supply chain management has become extremely complex, with large-scale and online decision-making challenges emerging (Yang et al., 2021), for example, the joint decision-making between proactive planning and reactive operations in the forms of demand forecasting, production planning, inventory management, supply allocation, transportation, and distribution. It is no longer efficient to rely on traditional analytics methods. Many firms have been exploring how to take big data analytics to promote lean and agile activities in supply chain management (Baruffaldi et al., 2019). Existing studies have shown that applications of digital technologies can help improve supply chain performance by enhancing visibility and reducing supply chain risks (Govindan et al., 2018). The digitalization of supply chains produces large volumes of data, which is regarded as a new kind of resource and has the potential to create value and enhance competitiveness. Singh and El-Kassar (2019) proposed that digital technologies have transformed traditional supply chain management into a more data-driven approach, which requires a much higher level of big data analytics capabilities compared to traditional supply chain management. Following the call for papers, the submission of 114 manuscripts, and the review and revision process, seven articles were selected for this special issue that contribute to our understanding of the impact of COVID-19 on supply chains and its effect on addressing the 2Rs. In \"Strengthening supply chain resilience during COVID-19: A case study of JD.com\" (Shen & Sun, 2023), the authors used quantitative operational data obtained from JD.com4 to analyze the impact of the pandemic on supply chain resilience. They described the challenging scenarios that retailing supply chains experienced in China and the practical response of JD.com over the course of the pandemic the pandemic. JD.com was observed to respond well to the exceptional demand and severe logistical disruptions caused by COVID-19 in China based on its highly integrated supply chain structure (including both process and activity integration and technology and system integration) and comprehensive digital technologies. In particular, the existing, intelligent platforms and delivery procedures were modified slightly but promptly to deal with specific disruptions. The joint efforts of multiple firms, the government, and the entire Chinese society contributed to surmounting the challenges. The experience of JD.com contributes to understanding of the value of investing in operational flexibility and beyond-supply-chain collaboration given the possibility of large-scale supply chain disruptions such as the COVID-19 outbreak. In \"Breaking out of the pandemic: How can firms match internal competence with external resources to shape operational resilience?\" (Li et al., 2023), the authors explored how firms sought to effectively combine internal competence with external resources from the supply chain network to improve operational flexibility and stability during the COVID-19 pandemic. The internal flexibility refers to product diversity, the internal stability refers to operational efficiency, the external flexibility refers to structural holes, and the external stability refers to network centrality. Drawing upon matching theory, the authors provided an internal-external combinative perspective to explain operational mechanisms underlying different matchings. Based on the empirical results of 2994 unique firms and 5293 observations, they found that more heterogeneous combinations between internal (external) flexibility and external (internal) stability may result in a complementary effect that enhances operational resilience, whereas more homogeneous combinations between internal flexibility (or stability) and external flexibility (or stability) may have a substitutive effect that reduces operational resilience. With the COVID-19 pandemic having had a significant impact on supply chains, government initiatives have played a central role in managing the crisis. In \"The impact of governmental COVID-19 measures on manufacturers' stock market valuations: The role of labor intensity and operational slack\" (Chen et al., 2023), the authors investigated the impact of the Chinese government's Level I emergency-response policy (Ge et al., 2020) on manufacturers' stock-market values, and the role of manufacturers' operational slack on adding resilience. Specifically, through an event study of 1357 Chinese manufacturing companies listed on the Shenzhen Stock Exchange, the authors found that the government's emergency-response policy triggered a statistically significant positive reaction from the stock market for manufacturers. However, the authors also found negative impacts on stock market values for manufacturers in labor-intensive industries because of the labor immobility triggered by the Level I measures. In addition, this article identified the positive role of operational slack in the form of financial slack and excess inventory in helping to maintain operations and business continuity, mitigate risks caused by the labor mobility restrictions, and improve supply chain resilience, which identifies operational slack as a supply chain resilience strategy to mitigate pandemic-related risks. When the COVID-19 pandemic broke out, the medical-product industry faced unprecedented demand shocks for personal protective equipment, including face masks, face shields, disinfectants, and gowns. Companies from various industries responded to the urgent need for these potentially life-saving products by adopting ad hoc supply chains in an exceptionally short time. In \"Realizing supply chain agility under time pressure: Ad hoc supply chains during the COVID-19 pandemic\" (M\u00fcller et al., 2023), the authors explored the use by 34 German companies of ad hoc supply chains to produce personal protective equipment. From these cases, the authors developed an emergent theoretical model of ad hoc supply chains around enablers of supply chain agility such as dynamic capabilities (the ability to integrate, build, and reconfigure internal and external competences to address rapidly scenario changes), entrepreneurial orientation (proactiveness, risk-taking, innovativeness, autonomy, and competitive aggressiveness), and temporary orientation (speedy action in a limited time). To cope with the COVID-19 crisis, many firms allowed their employees to work from home (WFH). In \"Working from home and firm resilience to the COVID-19 pandemic\" (Ge et al., 2023), the authors examined whether a firm's WFH capacity increased its resilience. The authors put forward and tested a unique data set that combines listed firms' financial data, epidemiological data, and online job postings data from China. They found that imposing COVID-19 anti-contagion policies on firms and their suppliers or customers significantly increased their operating revenue volatility, slowed their recovery, and had repercussions on their supply chains. WFH enhanced firms' resistance capacity by reducing the effect of COVID-19 on their operating revenue volatility and disruptions to their supply chain partners; however, it also decreased their recovery capacity by extending the time taken to return to normal. Firm attributes, along with workers' occupations, education, and experience, impacted the effect of WFH on firm resilience. This article enhances our understanding of shock transmission across supply..",
                "url": "https://openalex.org/W4365518480",
                "title": "Building responsive and resilient supply chains: Lessons from the <scp>COVID</scp>\u201019 disruption",
                "publication_date": "2023-04-01"
              },
              {
                "id": "E3704507684",
                "text": "..Kamble, S., Gunasekaran, A., Dhone, N.C., 2020b. Industry 4.0 and lean manufacturing practices for sustainable organisational performance in Indian manufacturing companies. Int J Production Research 58 (5), 1319\u20131337. Karkaria, U., 2020. Factories stay on alert for COVID-19; BMW, Toyota, others cope with outbreaks. Automotive News. http://search.ebscohost.com/login.aspx?direct=tru e&db=edsgbe&AN=edsgcl.627622912&site=eds-live (accessed 08.08.2020). Katafuchi, Y., Kurita, K., Managi, S., 2020. COVID-19 with Stigma: theory and Evidence from Mobility Data. Economics of Disasters and Climate Change. https://doi.org/ 10.1007/s41885-020-00077-w. Kochan, C.G., Nowicki, D.R., 2019. Supply chain resilience: a systematic literature review and typological framework. Int J Physical Distribution Logistics Management 48 (8), 842\u2013865. Kotoky, A., Modi, M., Turner, M., 2020. Jobs Are Being Wiped Out at Airlines, And There\u2019s Worse to Come. Bloomberg. http://search.ebscohost.com/login.aspx? direct=true&db=bth&AN=144748168&site=eds-live (accessed 08.08.2020). KPMG, 2020. COVID-19 impact on the automotive sector. https://home.kpmg/xx/en/b logs/home/posts/2020/03/covid-19-impact-on-the-automotive-sector.html (accessed 04.12.2020). Kumar, S., Managi, S., 2020. Economics of Disasters and Climate Change. Process Integration and Optimization for Sustainability 4, 481\u2013502. Lai, C.-C., Shih, T.-P., Ko, W.-C., Tang, H.-J., Hsueh, P.-R., 2020. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and coronavirus disease-2019 (COVID-19): the epidemic and the challenges. Int. J. Antimicrob. Agents 55 (3). LMC, 2020. Impact of COVID-19 on Global Auto Industry: expect deeper decline than during the Great Recession. https://lmc-auto.com/wp-content/uploads/202 0/03/LMCA-Global-LV-Sales-COVID-19-Impact-26-March-2020.pdf (accessed 04.11.2020). L\u00fccker, F., Seifert, R.W., Bi\u00e7er, I., 2019. Roles of inventory and reserve capacity in mitigating supply chain disruption risk. Int J Production Research 57 (4), 1238\u20131249. Martin, A., Markhvida, M., Hallegatte, S., Walsh, B., 2020. Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty. Economics of Disasters and Climate Change 4, 453\u2013479. Martinez, M., 2020. COVID-19 crisis is spurring long-term changes, execs say. Automotive News. http://search.ebscohost.com/login.aspx?direct=true&db=eds gao&AN=edsgcl.621032845&site=eds-live (accessed 07.22.2020). McKinsey, 2020. Covid-19 Briefing Materials. https://www.mckinsey.com /~/media/mckinsey/business%20functions/risk/our%20insights/covid%2019%20 implications%20for%20business/covid%2019%20march%2030/covid-19-facts-an d-insights-april-3.ashx (accessed 04.15.2020). Nakamura, H., Managi, S., 2020a. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. Nakamura, H., Managi, S., 2020b. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. NAM, 2020. Manufacturers\u2019 Survey Reveals Current Industry Impact of COVID-19. https://www.nam.org/manufacturers-survey-reveals-current-industry-impact-of-c ovid-19-7411 (accessed 04.08.2020). OCED, 2020. Coronavirus: the world economy at risk. https://www.oecd.org/berlin/ publikationen/Interim-Economic-Assessment-2-March-2020.pdf (accessed 04.14.2020). Ojha, R., Ghadge, A., Tiwari, M.K., Bititci, U.S., 2018. Bayesian network modelling for supply chain risk propagation. Int J Production Research 56 (17), 5795\u20135819. Patel, T., Horobin, W., 2020. France to Unveil Aid Packages for Auto, Aviation Industries. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=edb&AN=14 3227953&site=eds-live (accessed 08.08.2020). Philip, S., 2020. U.K. Car Industry Joins European Jobs Wipeout to Battle Slump. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=14 3595812&site=eds-live (accessed 08.08.2020). PwC, 2020. PwC\u2019s COVID-19 CFO Pulse Survey. https://www.pwc.com/us/en/li brary/covid-19/pwc-covid-19-cfo-pulse-survey.html (accessed 04.08.2020). Ralston, P., Blackhurst, J., 2020. Industry 4.0 and resilience in the supply chain: a driver of capability enhancement or capability loss? Int J Production Research 58 (16), 5006\u20135019. Roland Berger., 2020. THREE SCENARIOS FOR HOW CORONAVIRUS MAY AFFECT ECONOMIES AND INDUSTRIES. https://www.rolandberger.com/fr/Point-of-Vie w/Three-scenarios-for-how-Coronavirus-may-affect-economies-and-industries.html (accessed 04.14.2020). Rose, A., 2004. In: Springer (Ed.). Modeling Spatial and Economic Impacts of Disasters, pp. 13\u201336. Rose, A., Avetisyan, M., Rosoff, H., Burns, W.J., Slovic, P., Chan, O., 2017. The role of behavioral responses in the total economic consequences of terrorist attacks on US air travel targets. Risk analysis 37 (7), 1403\u20131418. Ro\u00dfmann, B., Canzaniello, A., Gracht, H.v., Hartmann, E., 2018. The future and social impact of Big Data Analytics in Supply Chain Management: results from a Delphi study. Technol Forecast Soc Change 130, 135\u2013149. Santos, J.R., May, L., ElHaimar, A., 2012. Risk-Based input-output analysis of influenza epidemic consequences on interdependent workforce sectors. Risk Analysis 33 (9), 1620\u20131635. Sawik, T., 2019. Disruption mitigation and recovery in supply chains using portfolio approach. Omega (Westport) 84, 232\u2013248. Schmitt, A.J., Singh, M., 2012. A quantitative analysis of disruption risk in a multiechelon supply chain. International Journal of Production Economics 139 (1), 22\u201332. Sharifi, H., Kehoe, D., Hopkins, J., 2006. A classification and selection model of emarketplaces for better alignment of supply chains. Journal of Enterprise Information Management 19 (5), 483\u2013503. Sharma, R., Shishodia, A., Kamble, S., Gunasekaran, A., Belhadi, A., 2020. Agriculture supply chain risks and COVID-19: mitigation strategies and implications for the practitioners. Int J Logistics Research and Applications 1\u201327. https://doi.org/ 10.1080/13675567.2020.1830049. Simchi-Levi, D., Schmidt, W., Wei, Y., Zhang, P.Y., Combs, K., Ge, Y., Gusikhin, O., Sanders, M., Zhang, D., 2015. Identifying risks and mitigating disruptions in the automotive supply chain. Interfaces (Providence) 45 (5), 375\u2013390. Sobieralski, J.B., 2020. COVID-19 and airline employment: insights from historical uncertainty shocks to the industry. Transportation Research Interdisciplinary Perspectives 5. Strauss, A., Corbin, J., 1990. Basics of Qualitative Research. Sage publications. Tan, W.J., Cai, W., Zhang, A.N., 2019. Structural-..",
                "url": "https://openalex.org/W3095984860",
                "title": "Manufacturing and service supply chain resilience to the COVID-19 outbreak: Lessons learned from the automobile and airline industries",
                "publication_date": "2021-02-01"
              }
            ]
          }
        ]
      },
      "S6989705278": {
        "id": "S6989705278",
        "text": "The resource-based view and dynamic capabilities framework are theoretical models that support the integration of big data analytics in healthcare supply chains during the COVID-19 pandemic, emphasizing the importance of organizational agility and responsiveness.",
        "children": [
          {
            "id": "S6842133324",
            "text": "Theoretical frameworks such as the resource-based view and dynamic capabilities view support the integration of big data analytics and innovation leadership in enhancing healthcare supply chain resilience.",
            "children": [
              {
                "id": "E4154459855",
                "text": "..) Akter et al. (2016) Horng et al. (2022) \u201cBig data analytics capabilities refer to obtaining knowledge from internal or external partners and gaining market insight through big data tools.\u201d (p. 24) Knowledge-based dynamic capability view Germann et al. (2013) Ciasullo et al. (2022) \u201cBDA capability refers to a company's management ability, that is, the ongoing deployment of big data resources at the strategic aims to create value and develop a competitive advantage for the firm.\u201d (p. 205) Wamba et al. (2017) Zhu et al. (2022) \u201c...the ability of an organization to integrate, build, and reconfigure the information resources, as well as business processes, to address rapidly changing environments.\u201d (p. 5) Dynamic capabilities view Supply chain management Lee and Kang (2015) Arias-P \u0301erez et al. (2022) \u201cBDAC is defined as the ability of a firm to capture and analyze big data toward the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 2) Knowledge-based view Henao-Garc\u00eda et al. (2021) Olabode et al. (2022) \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 1219) Knowledge-based view and contingency theory Mikalef et al. (2020b) Bhatti et al. (2022a) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions to gain actionable insights, create business value, and establish competitive advantage\u201d (p. 4) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Bhatti et al. (2022b) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions with a view to gaining actionable insights, creating business value, and establishing competitive advantage.\u201d (p. 5) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Song et al. (2022) \u201cBDAC is defined as the ability to develop business insight by using data management, technical foundations and talents.\u201d (p. 1168) Kiron et al. (2014) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 15 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies *Munir et al. (2023) \u201cit can be defined as the organization's capacity to provide insight into the use of data management, infrastructure and human capabilities to increase the competitiveness of the business.\u201d (p. 5) Resource-based view, process-oriented dynamic capability view, sociomateriality theory Kiron et al. (2014), Akter et al. (2016) Cetindamar et al. (2022) \u201cthe ability of an organization to integrate, build, and reconfigure the information resources, as well as business processes, to address rapidly changing environments.\u201d (p. 4) Supply chain management Olszak (2014) Appendix C. Typologies of BDACs proposed and studied in existing literature Documents Typologies of BDACs Cetindamar et al. (2022) BDA human, non-human and infrastructure capabilities Qaffas et al. (2022) Big data analytics management capability Park and Singh (2022) Infrastructure, human capital, knowledge management capability Chatterjee et al. (2022) Personalization and real-time analytics Liu et al. (2022) Big data technical capability and big data managerial capability Demir et al. (2022) Big data collection, processing, analysis and processing, transformation capability Ashaari et al. (2021) BDA technological, organizational, and people capabilities AlNuaimi et al. (2021) Technological and human capabilities Shamim et al. (2021) BDA management capabilities Nisar et al. (2022), Muhammad et al. (2021) BDA management capabilities, technical capabilities, and talent capabilities Uddin Murad et al. (2022), Razaghi and Shokouhyar (2021), Rialti et al. (2020), *Edwin Cheng et al. (2022), Rialti et al. (2019) BDA infrastructure flexibility, BDA management capabilities, BDA personnel expertise capability Bag et al. (2021) Data creation capabilities, data integration and management capabilities, advanced analytics capabilities, data visualization capabilities, and a data driven culture Zhang and Lv (2021) Big data system capabilities, big data human capabilities, and big data management capabilities. Bag et al. (2020) BDA management and talent capability Yasmin et al. (2020) Infrastructure, human resource, management capabilities Belhadi et al. (2020) Organizational (i.e., BDA management), physical (i.e., IT infrastructure), and human (e.g., analytics skill or knowledge) capabilities Wamba et al. (2020), Wamba et al. (2017) BDA management, infrastructure and personnel capability Xiao et al. (2020) BDA technology capabilities and BDA personnel capabilities Ferraris et al. (2019), Song et al. (2022) BDA technology and BDA management Munodawafa and Johl (2019) IT capability, personnel expertise capability, and management capability Wang et al. (2019) Data integration capability, analytical capability, data interpretation capability, predictive analytics, analytical personnel's technical and business skills Mandal (2019) Only focus on BDA management capabilities, which further refers to BDA planning, BDA investment decision making, BDA coordination, BDA control Popovi\u02c7c et al. (2018) Data provisioning, analytical capabilities and people skills Appendix D. Antecedents of BDACs identified from the literature Documents Antecedents Zhu et al. (2022) Sustainable supply chain management practices Horng et al. (2022) Offensive and defensive data strategies, big data knowledge management Khan and Tao (2022) Knowledge absorption capacity Behl (2022) Managerial and technical skills Demir et al. (2022) Big data utilization, knowledge sharing of big data Elia et al. (2022), Lozada et al. (2019), Mikalef et al. (2018), Mikalef et al. (2019b), Gupta and George (2016) Tangible resources, human skills, intangible resources AlNuaimi et al. (2021) Managerial experience and employee skills (BDA human capabilities); data availability and technological infrastructures (BDA technology capabilities); e-procurements Shamim et al. (2021) Strategic and operational big data management capabilities Henao-Garc\u00eda et al. (2021) Tangibles, intangibles, human resources, management innovation Yu et al. (2022) Data driven culture, digital technology orientation and their interaction Dubey et al. (2021) Alliance management capability Chen and Chen (2022) Intellectual capital Shamim et al. (2020) Big data contractual governance, big data relational governance Jha et al. (2020) Data management, advanced software, human resource and training, organizational politics, global integration, environmental determinism Ramadan et al. (2020) Data availability Yasmin et al. (2020) Management, human resources and infrastructure capability Upadhyay and Kumar (2020) Internal analytics knowledge, organizational culture Srimarut and Mekhum (2020) Supply chain connectivity Corte-Real \u02c6 et al. (2020) Data quality Lin and Kunnathur (2019) Developmental culture, customer orientation, technology orientation, entrepreneurial orientation Ferraris et al. (2019) BDA technological and management (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 16 (continued ) Documents Antecedents Singh and Singh (2019) Institutional response to supply chain disruption, IT infrastructure Mikalef et al. (2019a) Data, process, technology, organization, people, context Anwar et al. (2018) Technical capabilities: compatibility, modularity, connectivity; Personnel capabilities: technical knowledge, business knowledge, technical management knowledge Popovi\u02c7c et al. (2018) Data provisioning, analytical capabilities, people skills Wang et al. (2018b) Traceability, analytical capability, decision support capability, predictive capability Srinivasan and Swink (2018) Supply chain visibility Wamba et al. (2017) BDA infrastructure flexibility, management capabilities, personnel expertise capabilities Wang and Hajli (2017) Data aggregation, data processing, data visualization, big data architectural components Akter et al. (2016) BDA management capability, technology, and talent capability Appendix E. Mediating factors in BDACs research Documents Mediators Link Behl et al. (2022) Supply chain coordination and swift trust BDACs and supply chain risk *Munir et al. (2023) Process-oriented dynamic capabilities BDACs and organizational innovation performance Qaffas et al. (2022) Business intelligence infrastructure BDACs and financial/marketing performance Song et al. (2022) Infrastructure and value attribute of business model BDACs and growth/financial performance Park and Singh (2022) Upstream supply chain management IT infrastructure, downstream supply chain inventory management IT utilization BDACs and automated supply chain disruption risk alert tool Bhatti et al. (2022b) Supply chain agility and adaptability BDACs and supply chain innovation Nisar et al. (2022) Innovative green product development and supply chain risk management BDACs and innovation & learning performance Al-Khatib and Valeri (2022) BDACs as a mediator Service innovation and competitive advantage Dubey et al. (2022) Humanitarian supply chain agility and resilience BDACs and humanitarian supply chain performance Bhatti et al. (2022a) Digital platform and network capabilities BDACs and supply chain innovation, BDACs and firm performance Al-Khatib (2023) Green incremental and radical supply chain innovation BDACs and green supply chain performance Olabode et al. (2022)..",
                "url": "https://openalex.org/W4387418465",
                "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
                "publication_date": "2023-10-06"
              },
              {
                "id": "E0223838797",
                "text": ".., and assume society will be mobilised to engender positive, transformative change in the short and medium term. We recognize of course this optimism may be misplaced. In the case of business-as-usual, in the medium and long term, there will also be transformative, systemic change affecting whole sectors, governance, value networks etc. in dramatic ways, perhaps to the advantage of the few, but definitely to the detriment of wider society. References Backstrand, \u0308 J., Halldorsson, \u0301 A., \u0301 2019. Engaged scholar(ship) in purchasing and supply management (PSM): creative tension or squeezed in the middle? J. Purch. Supply Manag. 25 (4), 100557. Banerjee, S.B., 2003. Who sustains whose development? Sustainable development and the reinvention of nature. Organ. Stud. 24 (1), 143\u2013180. Banerjee, S.B., 2008. Corporate social responsibility: the good, the bad and the ugly. Crit. Sociol. 34 (1), 51\u201379. Banerjee, S.B., 2010. Governing the global corporation: a critical perspective. Bus. Ethics Q. 20 (2), 265\u2013274. Bansal, P., Roth, K., 2000. Why companies go green: a model of ecological responsiveness. Acad. Manag. J. 43 (4), 717\u2013736. Bapuji, H., Husted, B.W., Lu, J., Mir, R., 2018. Value creation, appropriation, and distribution: how firms contribute to societal economic inequality. Bus. Soc. 57 (6), 983\u20131009. Baron, R.A., 2006. Opportunity recognition as pattern recognition: how entrepreneurs \u201cconnect the dots\u201d to identify new business opportunities. Acad. Manag. Perspect. 20 (1), 104\u2013119. Boer, H., Holweg, M., Kilduff, M., Pagell, M., Schmenner, R., Voss, C.A., 2015. Making a meaningful contribution to theory. Int. J. Oper. Prod. Manag. 35 (9), 1231\u20131252. Bruce, J., de Figueiredo, J., Silverman, B., 2019. Public contracting for private innovation: government capabilities, decision rights, and performance outcomes. Strat. Manag. J. 40 (4), 533\u2013555. Bryce, E., 2021. Are clothes made from recycled materials really more sustainable? The Guardian. Available at. https://www.theguardian.com/environment/2021/nov/06/ clothes-made-from-recycled-materials-sustainable-plastic-climate. Carney, M., 2021. Value(s): Building a Better World for All. William Collins, London. Carnovale, S., DuHadway, S., 2021. Continuity in the face of disruptions: purchasing and supply management research\u2019s persistence amidst COVID-19. J. Purch. Supply Manag. 27, 100720. Carter, C.R., Kaufmann, L., Ketchen, D.J., 2020. Expect the unexpected: toward a theory of the unintended consequences of sustainable supply chain management. Int. J. Oper. Prod. Manag. 40 (12), 1857\u20131871. Choi, T.Y., Rogers, D., Vakil, B., 2020. Coronavirus is a wake-up call for supply chain management. Harv. Bus. Rev. 27, 364, 0017-8012. Choi, T.Y., Krause, D.R., 2006. The supply base and its complexity: implications for transaction costs, risks, responsiveness, and innovation. J. Oper. Manag. 24 (5), 637\u2013652. L. Knight et al. Journal of Purchasing and Supply Management 28 (2022) 100753 11 Circular Flanders, 2019. The circular ambition chart. Accessed date: November 2nd, 2021. Available at: https://aankopen.vlaanderen-circulair.be/en/getting-started/th e-ambition-map. Ciulli, F., Kolk, A., Boe-Lillegraven, S., 2019. Circularity brokers: digital platform organizations and waste recovery in food supply chains. J. Bus. Ethics 1\u201333. Cook, J.A., 2010. Supply chain versus the volcano, supply chain quarterly, perspectives. Available at: https://www.supplychainquarterly.com/articles/348-supply-chain -versus-the-volcano. Cornelissen, J., Hollerer, \u0308 M.A., Seidl, D., 2021. What theory is and can Be: forms of theorizing in organizational scholarship. Organ. Theor. 2 (3), 26317877211020328. Craighead, C.W., Blackhurst, J., Rungtusanatham, M.J., Handfield, R.B., 2007. The severity of supply chain disruptions: design characteristics and mitigation capabilities. Decis. Sci. J. 38 (1), 131\u2013156. Cutler, A.C., Lark, D., 2020. The Hidden Costs of Law in the Governance of Global Supply Chains: the Turn to Arbitration, Review of International Political Economy. https:// doi.org/10.1080/09692290.2020.1821748. De Angelis, R., Howard, M., Miemczyk, J., 2018. Supply chain management and the circular economy: towards the circular supply chain. Prod. Plann. Control 29 (6), 425\u2013437. de Sousa Santos, B., 2018. Decolonising the University. Cambridge Scholars Publishing Cambridge. Dhanorkar, S., Kim, Y., Linderman, K., 2019. An empirical investigation of transaction dynamics in online surplus networks: a complex adaptive system perspective. J. Oper. Manag. 65 (2), 160\u2013189. Di Mauro, C., Giannakis, M., 2019. Special issue of the 27th annual IPSERA conference 2018-purchasing & supply management: fostering innovation. J. Purch. Supply Manag. 25 (4), 100573. Diprose, R., Kurniawan, N., Macdonald, K., Winanti, P., 2020. Regulating Sustainable Minerals in Electronics Supply Chains: Local Power Struggles and the \u2018hidden Costs\u2019 of Global Tin Supply Chain Governance, Review of International Political Economy. https://doi.org/10.1080/09692290.2020.1814844. du Plessis, C., 2012. Towards a regenerative paradigm for the built environment. Build. Res. Inf. 40 (1), 7\u201322. Ehrenfeld, J.R., Hoffman, A.J., 2013. Flourishing: A Frank Conversation about Sustainability. Stanford Business Books, Stanford, US. European Commission, 2017. Public procurement for a circular economy: good practice and guidance. Accessed on Oct 18th, 2021. Available at: https://ec.europa.eu/enviro nment/gpp/circular_procurement_en.htm. Figueres, C., Rivett-Carnac, T., 2020. The Future We Choose: Surviving the Climate Crisis. Manila Press, London. Finkenstadt, D.J., Handfield, R., 2021. Blurry vision: supply chain visibility for personal protective equipment during COVID-19. J. Purch. Supply Manag., 100689 General Motors, 2020. Ventec Life Systems and GM partner to mass produce critical care ventilators in response to COVID-19 pandemic. GM press release. March 27, 2020. https://media.gm.com/media/us/en/gm/news.detail.html/content/Pages/ne ws/us/en/2020/mar/0327-coronavirus-update-6-kokomo.html. George, G., Howard-Grenville, J., Joshi, A., Tihanyi, L., 2016. Understanding and tackling societal grand challenges through management research. Acad. Manag. J. 59 (6), 1880\u20131895. Gereffi, G., Lee, J., 2012. Why the world suddenly cares about global supply chains. J. Supply Chain Manag. 48 (3), 24\u201332. Ghosh, A., 2016. The Great Derangement: Climate Change and the Unthinkable. University of Chicago Press, Chicago and London. Goranova, M., Verstegen Ryan, L., 2021. The corporate objective revisited: the shareholder perspective. J. Manag. Stud. 59, 2. https://doi.org/10.1111/ joms.12714. Gray, J.V., Helper, S., Osborn, B., 2020. Value first, cost later: total value contribution as a new approach to sourcing decisions. J. Oper. Manag. 66 (6), 735\u2013750. Gualandris, J., Lee, D., Beattie, C., 2021a. HP Canada Co.: a circular supply chain for recycled plastic. Ivey Publishing. Available at: https://www.iveypublishing.ca/s/p roduct/hp-canada-co-a-circular-supply-chain-for-recycled-plastic/01t5c00000 CwqXiAAJ. Gualandris, J., Lee, D., Lin, R., 2021b...",
                "url": "https://openalex.org/W4220751036",
                "title": "Future business and the role of purchasing and supply management: Opportunities for \u2018business-not-as-usual\u2019 PSM research",
                "publication_date": "2022-01-01"
              },
              {
                "id": "E3703744204",
                "text": "..broadly defined \u201cas the ability of a firm to provide insights using data management, infrastructure, and talent to transform business into a competitive force.\u201d (p. 557) Resource-based view, dynamic capabilities view Akter et al. (2016), Kiron et al. (2014) Dubey et al. (2019a) \u201cBDAC is an organizational facility with tools, techniques, and processes that enable the organization to process, visualize, and analyze data, thereby producing insights that enable data-driven operational planning, decision making and execution.\u201d (p. 2095) Dynamic capabilities view, contingency theory Supply chain management Srinivasan and Swink (2018) Lin and Kunnathur (2019) \u201cBig Data Capability as a firm's capability of identifying sources, where large volumes of various kinds of data flow out at high speed, and collecting, storing, and analyzing such Big Data for the purpose of accomplishing the firm's strategic as well as operational goals.\u201d (p. 51) Dynamic capabilities view Pigni et al. (2016), Chen et al. (2015) Lozada et al. (2019) \u201c... BDA capability refers to a company's management ability, that is, the continuous use and deployment of big data resources with the strategic goal of creating value and developing a competitive advantage for the firm.\u201d (p. 2) Wamba et al. (2017), Garmaki et al. (2016), Gupta and George (2016), Kiron et al. (2014) Mandal (2019) BDA capabilities are conceptualized as \u201ca thirdorder formative construct of BDA management capability, BDA personnel expertise capability and BDA infrastructure flexibility capability\u201d (p. 298). \u201cBDA management capabilities comprise of essential first-order capabilities of planning, investment decision making, coordination and control.\u201d (p. 298). Resource-based view, dynamic capabilities view Supply chain management Wamba et al. (2017) Mikalef et al. (2019b) BDAC is \u201cdefined as the ability of a firm to capture and analyse data towards the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 273) Resource-based view, dynamic capabilities view Gupta and George (2016), Mikalef et al. (2018), Wamba et al. (2017) Rialti et al. (2019) \u201cOrganizational BDA capabilities are an ensemble of capabilities that include infrastructure flexibility, management capabilities and personnel capabilities.\u201d (p. 1) Dynamic capabilities view Wamba et al. (2017), Gunasekaran et al. (2018), Mikalef and Pateli (2017) Definition based on Wamba et al. (2017), measures of constructs based on the 3 papers. Wang et al. (2019) \u201cBig data analytics capability is defined as the ability to acquire, store, process and analyse large amounts of health data in various forms, and deliver meaningful information to users, which allows them to discover business values and insights in a timely fashion.\u201d (p. 368) Configuration theory Health care Wang and Hajli (2017) Belhadi et al. (2020) \u201cBDA capability is defined as the ability of the organizations in developing competency to generate business insights based on organizational (i.e., BDA management), physical (i.e., IT infrastructure), and human (e.g., analytics skill or knowledge) capabilities for increased business performance\u201d (p. 2) Resource-based view, dynamic capabilities view Manufacturing firms Mikalef et al. (2020b), Akter et al. (2016) Corte-Real \u02c6 et al. (2020) BDACs refer to \u201cthe extent to which BDA has been used to provide business insights into primary activities (e.g., production, distribution, and customer service).\u201d (p. 6) Resource-based view, dynamic capabilities view, knowledge management Chen et al. (2015) Mikalef et al. (2020a) BDAC is \u201cthe ability of a firm to capture and analyze data toward the generation of insights by effectively orchestrating and deploying its data, technology, and talent.\u201d (p. 2) Resource-based view, information governance theory Akter et al. (2016), Gupta and George (2016), Wamba et al. (2017), Kiron et al. (2014) Mikalef et al. (2020b) BDAC is defined as \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 7) Resource-based view, dynamic capabilities view, IT capability Gupta and George (2016), Mikalef et al. (2018) Ramadan et al. (2020) \u201cBig data analytics capabilities refer to the firm's ability to recognize and analyze different data sources to provide valuable insights.\u201d (p. 2) Manufacturing firms Hu et al. (2018) Shamim et al. (2020) BDAC is referred to as \u201ca holistic approach of analysing and processing big data for value creation.\u201d (p. 4) Knowledge based dynamic capabilities view, social capital theory Wamba et al. (2017), Akhtar et al. (2019), Song et al. (2020) BDAC is defined \u201cas the capability of firms to combine, integrate, and deploy specific big data resources.\u201d (p. 5) Information processing theory Gupta and George (2016) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 13 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies Upadhyay and Kumar (2020) \u201cBDAC is broadly defined as the competence to provide business insights using data management, infrastructure (technology), and talent (personal) capabilities to transform the business into a competitive force.\u201d (p. 2) Resource-based view, dynamic capabilities view, socio-materialism theory Wamba et al. (2017), Gupta and George (2016), Kiron et al. (2013) Kim et al. (2011), Kim et al. (2012) for measurement of constructs Wamba et al. (2020) BDAC is defined as \u201ca firm's ability to assemble, integrate, and deploy its big data-specific resource.\u201d (p. 10) IT capability, resource-based view Gupta and George (2016) Xiao et al. (2020) \u201cBDAC refers to the ability to provide business insights in the big data environment by using big data analytics personnel, big data analytics technical, and big data analytics management capabilities.\u201d (p. 18780) Dynamic capabilities view Service sector Akter et al. (2016) Yasmin et al. (2020) BDACs are referred to \u201cas a balanced combination of requisite human resource, big-data skills, advanced technologies supported by large datasets to generate analytical reports and actionable insights utilized, produced, and processed by mathematical, statistical techniques, and machine learning tools for enhanced performance.\u201d (p. 2) Resource-based view, dynamic capabilities view Akhtar et al. (2019), Akter et al. (2016), Wang et al. (2019) Zhang et al. (2020) BDAC refers to \u201ca firm's ability to assemble, integrate, and deploy its big data-specific resources.\u201d (p. 3) Source-position-performance theoretical framework Sustainability development projects Gupta and George (2016), Akter et al. (2016), Ferraris et al. (2019), Wamba et al. (2017) Ashaari et al. (2021) \u201cBDAC is termed as an organization's capacity to efficiently and strategically arrange, assemble, and apply BDA resources so that effective decision-making can be made to enhance overall organization's performance.\u201d (p. 1) Resource-based view, organizational information processing theory Higher education institutions Mikalef et al. (2020b), Shamim et al. (2020), Janssen et al. (2017), Cao et al. (2015), Awan et al. (2021) BDACs refer to a \u201cholistic process that involves the collection, analysis, use and interpretation of data for various functional divisions to gain actionable insights, create business value and establish competitive advantage.\u201d (p. 86) Resource-based view, dynamic capabilities view, institution-based view Manufacturing sector Akter et al. (2016) Bertello et al. (2021) BDAC is defined \u201cas the ability to acquire, store, process, and analyze large amounts of data in various forms and deliver meaningful information to users, allowing them to discover business values and insights in a timely fashion.\u201d (p.1040) Resource-based view Wang et al. (2018a) Ciampi et al. (2021) BDAC \u201crefer to the company's abilities to leverage on technology and talent to exploit BD towards the generation of the insights that are necessary to overperform rivals.\u201d (p. 2) Knowledge-based view, IT capability Mikalef et al. (2017), Wamba et al. (2017), Gupta and George (2016) Contreras Pinochet et al. (2021) BDAC is recognized as \u201cthe competence to provide business insights using the capacity of data management, infrastructure (technology) and talent (personnel) to transform a business into a competitive force.\u201d (p. 1410) Process-oriented dynamic capabilities, business value Gupta and George (2016) Gu et al. (2021) BDAC in supply chain management is described as \u201cthe ability of organizations to collect and organize supply chain data from heterogeneous systems distributed across organizational boundaries, analyze it either batch-wise, or real-time, or near real-time, and visualize it intuitively to create proactive supply chain system and support decision making,\u201d (p. 155) Resource-based view, dynamic capabilities view, contingency theory Supply chain management Arunachalam et al. (..",
                "url": "https://openalex.org/W4387418465",
                "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
                "publication_date": "2023-10-06"
              }
            ]
          },
          {
            "id": "S0274819899",
            "text": "Big data analytics capabilities (BDACs) significantly enhance the resilience of healthcare supply chains by improving organizational agility and responsiveness during disruptions such as the COVID-19 pandemic.",
            "children": [
              {
                "id": "E0909104077",
                "text": "..? J. Bus. Res. 133, 337\u2013340. Pigni, F., Piccoli, G., Watson, R., 2016. Digital data streams: creating value from the realtime flow of big data. Calif. Manag. Rev. 58 (3), 5\u201325. **Popovi\u02c7c, A., Hackney, R., Tassabehji, R., Castelli, M., 2018. The impact of big data analytics on firms\u2019 high value business performance. Inf. Syst. Front. 20 (2), 209\u2013222. Premukar, G., Ramamurthy, K., Saunders, C.S., 2005. Information processing view of organizations: an exploratory examination of fit in the context of interorganizational relationships. J. Manag. Inf. Syst. 22 (1), 257\u2013294. Prescott, M.E., 2014. Big data and competitive advantage at Nielsen. Manag. Decis. 52 (3), 573\u2013601. **Qaffas, A.A., Ilmudeen, A., Almazmomi, N.K., Alharbi, I.M., 2022. The impact of big data analytics talent capability on business intelligence infrastructure to achieve firm performance. Foresight 25 (3), 448\u2013464. **Ramadan, M., Shuqqo, H., Qtaishat, L., Asmar, H., Salah, B., 2020. Sustainable competitive advantage driven by big data analytics and innovation. Appl. Sci. 10 (19), 6784. Ransbotham, S., Kiron, D., Prentice, P.K., 2015. Minding the analytics gap. MIT Sloan Manag. Rev. 56 (3), 63. Ravichandran, T., Lertwongsatien, C., 2005. Effect of information systems resources and capabilities on firm performance: a resource-based perspective. J. Manag. Inf. Syst. 21 (4), 237\u2013276. **Razaghi, S., Shokouhyar, S., 2021. Impacts of big data analytics management capabilities and supply chain integration on global sourcing: a survey on firm performance. The Bottom Line 34 (2), 198\u2013223. Regulation, E. G. D. P, 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/EC (General Data Protection Regulation) 2016. OJ L 119 (1). **Rialti, R., Zollo, L., Ferraris, A., Alon, I., 2019. Big data analytics capabilities and performance: evidence from a moderated multi-mediation model. Technol. Forecast. Soc. Chang. 149, 119781. **Rialti, R., Marzi, G., Caputo, A., Mayah, K.A., 2020. Achieving strategic flexibility in the era of big data. Manag. Decis. 58 (8), 1585\u20131600. Richey, R.G., Morgan, T.R., Lindsey-Hall, K., Adams, F.G., 2016. A global exploration of big data in the supply chain. Int. J. Phys. Distrib. Logist. Manag. 46 (8), 710\u2013739. Ross, J.W., Beath, C.M., Quaadgras, A., 2013. You may not need big data after all. Harv. Bus. Rev. 91 (12), 90\u201398. Russom, P., 2011. Big data analytics. TDWI Best Pract. Rep. Fourth Q. 19 (4), 1\u201334. **Sabharwal, R., Miah, S.J., 2021. A new theoretical understanding of big data analytics capabilities in organizations: a thematic analysis. J. Big Data 8 (1), 1\u201317. Sahut, J.M., Schweizer, D., Peris-Ortiz, M., 2022. Technological forecasting and social change introduction to the VSI technological innovations to ensure confidence in the digital world. Technol. Forecast. Soc. Chang. 179, 121680. Schilke, O., Hu, S., Helfat, C.E., 2018. Quo vadis, dynamic capabilities? A contentanalytic review of the current state of knowledge and recommendations for future research. Acad. Manag. Ann. 12 (1), 390\u2013439. **Schlegel, A., Birkel, H.S., Hartmann, E., 2021. Enabling integrated business planning through big data analytics: a case study on sales and operations planning. Int. J. Phys. Distrib. Logist. Manag. 51 (6), 607\u2013633. **Shamim, S., Zeng, J., Khan, Z., Zia, N.U., 2020. Big data analytics capability and decision making performance in emerging market firms: the role of contractual and relational governance mechanisms. Technol. Forecast. Soc. Chang. 161, 120315. **Shamim, S., Yang, Y., Zia, N. Ul, Shah, M.H., 2021. Big data management capabilities in the hospitality sector: service innovation and customer generated online quality ratings. Comput. Hum. Behav. 121, 106777. Sharif, A.M., Irani, Z., 2006. Exploring fuzzy cognitive mapping for IS evaluation. Eur. J. Oper. Res. 173 (3), 1175\u20131187. Sheng, J., Amankwah-Amoah, J., Wang, X., 2017. A multidisciplinary perspective of big data in management research. Int. J. Prod. Econ. 191, 97\u2013112. **Sheng, H., Feng, T., Chen, L., Chu, D., 2021. Responding to market turbulence by big data analytics and mass customization capability. Ind. Manag. Data Syst. 121 (12), 2614\u20132636. Shuradze, G., Wagner, H.T., 2016. Towards a conceptualization of data analytics capabilities. In: 49th Hawaii International Conference on System Sciences (HICSS), pp. 5052\u20135064. Simon, P., 2013. Too Big to Ignore: The Business Case for Big Data, vol. 72. John Wiley & Sons Inc. **Singh, N.P., Singh, S., 2019. Building supply chain risk resilience. BIJ 26 (7), 2318\u20132342. **Song, M., Zhang, H., Heng, J., 2020. Creating sustainable innovativeness through big data and big data analytics capability: from the perspective of the information processing theory. Sustainability 12 (5), 1984. **Song, J., Xia, S., Vrontis, D., Sukumar, A., Liao, B., Li, Q., et al., 2022. The source of SMEs\u2019 competitive performance in COVID-19: matching big data analytics capability to business models. Inf. Systems Front. J. Res. Innov. 24 (4), 1167\u20131187. **Srimarut, T., Mekhum, W., 2020. From supply chain connectivity (SCC) to supply chain agility (SCA), adaptability and alignment: mediating role of big data analytics capability. Int. J. Supply Chain Manag. 9 (1), 183\u2013189. **Srinivasan, Ravi, Swink, Morgan, 2018. An investigation of visibility and flexibility as complements to supply chain analytics: an organizational information processing theory perspective. Prod. Oper. Manag. 27 (10), 1849\u20131867. **Su, X., Zeng, W., Zheng, M., Jiang, X., Lin, W., Xu, A., 2022. Big data analytics capabilities and organizational performance: the mediating effect of dual innovations. Eur. J. Innov. Manag. 25 (4), 1142\u20131160. **Sun, B., Liu, Y., 2021. Business model designs, big data analytics capabilities and new product development performance: evidence from China. Eur. J. Innov. Manag. 24 (4), 162\u20131183. Trkman, P., McCormack, K., de Oliveira, M.P.V., Ladeira, M.B., 2010. The impact of business analytics on supply chain performance. Decis. Support. Syst. 49 (3), 318\u2013327. **Uddin Murad, M.A., Cetindamar, D., Chakraborty, S., 2022. Identifying the key big data analytics capabilities in bangladesh\u2019s healthcare sector. Sustainability 14 (12), 7077. **Upadhyay, P., Kumar, A., 2020. The intermediating role of organizational culture and internal analytical knowledge between the capability of big data analytics and a firm\u2019s performance. Int. J. Inf. Manag. 52, 102100. Wamba, S.F., Akter, S., Edwards, A., Chopin, G., Gnanzou, D., 2015. How \u2018big data\u2019 can make big impact: findings from a systematic review and a longitudinal case study. Int. J. Prod. Econ. 165, 234\u2013246. **Wamba, S.F..",
                "url": "https://openalex.org/W4387418465",
                "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
                "publication_date": "2023-10-06"
              },
              {
                "id": "E5397305730",
                "text": "..2022) Innovation leadership BDACs and healthcare supply chain innovation/ responsiveness Aljumah et al. (2021) Business value of big data BDACs and organizational performance Shamim et al. (2020) Data driven culture BDACs and decision making performance Wang et al. (2020) BDACs as a moderator Corporate social responsibility and green supply chain management Mikalef et al. (2020a) Information governance, environmental uncertainty BDACs and incremental/radical innovative capability Sun and Liu (2021) BDACs as a moderator Business model novelty/efficiency design and new product development performance Xiao et al. (2020) Digital platform capabilities BDACs and dynamic capabilities Hao et al. (2019) BDACs as a moderator Big data and sustainability of innovation and organizational development Rialti et al. (2019) Organization information management system fit, and organizational resistance to information management BDACs-ambidexterity, BDACs-agility, BDACs-organizational performance Lin and Kunnathur (2019) Developmental culture Technological, customer, entrepreneurial orientation and BDACs Dubey et al. (2019a), Srinivasan and Swink (2018) Organization flexibility BDACs and supply chain agility, BDACs and competitive advantage Mikalef et al. (2019b) Environment dynamism, heterogeneity, and hostility BDACs and dynamic capabilities Dubey et al. (2019b) Control orientation and flexible orientation BDACs and collaborative performance Appendix G. Outcome variables of BDACs in the literature Outcomes References Competitive advantage, sustainable competitive advantage Behl et al. (2022), Horng et al. (2022), Zhang et al. (2022b), Al-Khatib (2022b), Behl (2022), Jha et al. (2020), Corte-Real \u02c6 et al. (2020), Dubey et al. (2019a), Anwar et al. (2018), Ramadan et al. (2020) Business value/strategic business value Contreras Pinochet et al. (2021), Wamba et al. (2020) Organizational performance (general) Bhatti et al. (2022a), Horng et al. (2022), Elia et al. (2022), Ashaari et al. (2021), Gu et al. (2021), Bahrami and Shokouhyar (2022), Razaghi and Shokouhyar (2021), Aljumah et al. (2021), Su et al. (2022), Upadhyay and Kumar (2020), Wamba et al. (2020), Rialti et al. (2019), Ferraris et al. (2019), Mikalef et al. (2019a), Anwar et al. (2018), Wamba et al. (2017), Akter et al. (2016) Operational performance and operational flexibility, strategic sales performance Zhu et al. (2022), Chatterjee et al. (2022), Ilmudeen (2021), Yu et al. (2022), Yasmin et al. (2020), Srinivasan and Swink (2018), Gupta and George (2016), Yu et al. (2021b) Market performance, marketing performance, growth and financial performance Qaffas et al. (2022), Song et al. (2022), Olabode et al. (2022), Ilmudeen (2021), Yasmin et al. (2020), Gupta and George (2016) Decision making quality and performance, decision speed and quality Awan et al. (2021), Shamim et al. (2020), Chen et al. (2022) Manufacturing performance, smart service performance, environmental performance Zhu et al. (2022), Awan et al. (2021), Popovi\u02c7c et al. (2018), Zhang and Lv (2021), AlNuaimi et al. (2021), Belhadi et al. (2020) Explorative and exploitative, incremental and radical innovation, innovation performance, innovative organizational performance, innovation and learning performance Rialti et al. (2020), Mikalef et al. (2020a), Mikalef et al. (2019b), Muhammad et al. (2021), Demir et al. (2022), Khan and Tao (2022), Nisar et al. (2022), *Munir et al. (2023) Product innovation performance, service innovation, business model innovation, Contreras Pinochet et al. (2021), Shamim et al. (2021), Ciampi et al. (2021), Elia et al. (2022) Dual innovations, co-innovation, process Eco-innovation Su et al. (2022), Lozada et al. (2019), Munodawafa and Johl (2019) Sustainable innovativeness, innovative green product development Song et al. (2020), Hao et al. (2019), Bag et al. (2020) Dynamic capabilities Mikalef et al. (2020b), Xiao et al. (2020), Mikalef et al. (2019b), Wamba et al. (2017) Innovation capabilities Bahrami and Shokouhyar (2022), Ramadan et al. (2020) Marketing capabilities and technological capabilities, digital platform and network capabilities Bhatti et al. (2022a), Mikalef et al. (2020b) Knowledge management capabilities and information processing capabilities Rialti et al. (2020), Schlegel et al. (2021) (Firm's) agility, adaptability, alignment, ambidexterity, entrepreneurial orientation, organizational resilience Srimarut and Mekhum (2020), Wamba et al. (2020), Rialti et al. (2019), Popovi\u02c7c et al. (2018), Aljumah et al. (2021), Rialti et al. (2019), Ciampi et al. (2021), Khan and Tao (2022), Ciasullo et al. (2022), Al-Darras and Tanova (2022) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 18 (continued ) Outcomes References Supply chain innovation and resilience, supply chain innovativeness Bhatti et al. (2022b), Bhatti et al. (2022a), Jaouadi (2022), Bag et al. (2022), Singh and Singh (2019) External and internal integration, internal finance integration, global sourcing, green supplier and customer integration Razaghi and Shokouhyar (2021), Yu et al. (2021a), Chen and Chen (2022), Liu et al. (2022) Sustainable supply chain performance and flexibility, supply chain development, green supply chain performance, humanitarian supply chain performance Edwin Cheng et al. (2022), Gu et al. (2021), Al-Khatib (2022a), Al-Khatib (2023), Dubey et al. (2022), Nisar et al. (2022) Swift trust and collaborative performance, strategic and tactical reverse logistics decisions Dubey et al. (2019b), Bag et al. (2021) Supply chain preparedness, alertness, agility, adaptability, responsiveness, automated supply chain disruption risk alert tool Park and Singh (2022), Bhatti et al. (2022b), Bag et al. (2022), Dubey et al. (2019a), Mandal (2019) Circular economy practices, green manufacturing practices/Lean Six Sigma efforts Awan et al. (2021), Edwin Cheng et al. (2022), Belhadi et al. (2020) Sustainable design and commercialization, mass customization, and internationalization Zhang et al. (2020), Sheng et al. (2021), Bertello et al. (2021) Information quality, knowledge creation Bahrami and Shokouhyar (2022), Shamim et al. (2021), Awan et al. (2021) Organization, employee, new product development Dubey et al. (2021), Bag et al. (2020), Hao et al. (2019), Business processes improvement, consumer experience and market enhancement Elia et al. (2022) Quality of (health) care services and potential benefits Wang et al. (2019), Wang et al. (2018a), Wang and Hajli (2017) References2 Akhtar, P., Khan, Z., Frynas, J.G., Tse, Y.K., Rao-Nicholson, 2018. Essential microfoundations for contemporary business operations: top management tangible competencies, relationship-based business networks and environmental sustainability. Br. J. Manag. 29 (1), 43\u201362. Akhtar, P., Frynas, J.G., Mellahi, K., Ullah, S., 2019. Big data-savvy teams\u2019 skills, big data-driven actions and business performance. Br. J. Manag. 30 (2), 252\u2013271. **Akter, S., Wamba, S.F., Gunasekaran, A., Dubey, R., Childe, S.J., 2016. How to improve firm performance using big data analytics capability and business strategy alignment? Int. J. Prod. Econ. 182, 113\u2013131. **Al-Darras, O.M.A., Tanova, C., 2022. From big data analytics to organizational agility: what is the mechanism? SAGE Open 12 (2), 215824402211061. **Aljumah, A.I., Nuseir, M.T., Alam, M.M., 2021. Organizational performance and capabilities to analyze big data: do the ambidexterity and business value of big data analytics matter? Bus. Process. Manag. J. 27 (4)..",
                "url": "https://openalex.org/W4387418465",
                "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
                "publication_date": "2023-10-06"
              },
              {
                "id": "E3716506598",
                "text": "..Akter et al. (2016), Ferraris et al. (2019), Wamba et al. (2017) Ashaari et al. (2021) \u201cBDAC is termed as an organization's capacity to efficiently and strategically arrange, assemble, and apply BDA resources so that effective decision-making can be made to enhance overall organization's performance.\u201d (p. 1) Resource-based view, organizational information processing theory Higher education institutions Mikalef et al. (2020b), Shamim et al. (2020), Janssen et al. (2017), Cao et al. (2015), Awan et al. (2021) BDACs refer to a \u201cholistic process that involves the collection, analysis, use and interpretation of data for various functional divisions to gain actionable insights, create business value and establish competitive advantage.\u201d (p. 86) Resource-based view, dynamic capabilities view, institution-based view Manufacturing sector Akter et al. (2016) Bertello et al. (2021) BDAC is defined \u201cas the ability to acquire, store, process, and analyze large amounts of data in various forms and deliver meaningful information to users, allowing them to discover business values and insights in a timely fashion.\u201d (p.1040) Resource-based view Wang et al. (2018a) Ciampi et al. (2021) BDAC \u201crefer to the company's abilities to leverage on technology and talent to exploit BD towards the generation of the insights that are necessary to overperform rivals.\u201d (p. 2) Knowledge-based view, IT capability Mikalef et al. (2017), Wamba et al. (2017), Gupta and George (2016) Contreras Pinochet et al. (2021) BDAC is recognized as \u201cthe competence to provide business insights using the capacity of data management, infrastructure (technology) and talent (personnel) to transform a business into a competitive force.\u201d (p. 1410) Process-oriented dynamic capabilities, business value Gupta and George (2016) Gu et al. (2021) BDAC in supply chain management is described as \u201cthe ability of organizations to collect and organize supply chain data from heterogeneous systems distributed across organizational boundaries, analyze it either batch-wise, or real-time, or near real-time, and visualize it intuitively to create proactive supply chain system and support decision making,\u201d (p. 155) Resource-based view, dynamic capabilities view, contingency theory Supply chain management Arunachalam et al. (2018) Henao-Garc\u00eda et al. (2021) \u201cBDAC is defined as the ability of a firm to capture and analyze data for the generation of insights by effectively orchestrating and deploying its data, technology, and talent.\u201d (p. 28) Resource-based view, dynamic capabilities view Mikalef et al. (2018) Schlegel et al. (2021) BDAC is defined \u201cas a firm's ability to assemble, integrate and deploy its big data-based resources.\u201d (p. 609) Organizational information processing theory Gupta and George (2016), Akter et al. (2016) Sheng et al. (2021) BDA capability refers to \u201can enterprise's ability to realize data-driven operation plan and decisionmaking through processing, organizing and analyzing data.\u201d (p. 2618) Organizational information processing theory Supply chain management Dubey et al. (2019b), Gupta and George (2016) Sun and Liu (2021) \u201cBDA capabilities comprise a firm's techniques, processes and talents that enable the organization to process, visualize and analyze big data, thereby producing insights that enable data-driven operational planning, decision-making and execution.\u201d (p. 1163) Kiron et al. (2014), Akter et al. (2016), Dubey et al. (2019a) for definition; Akter et al. (2016), Ferraris et al. (2019) for measures Sabharwal and Miah (2021) BDAC is defined as \u201cthe combined ability to store, process, and analyze large amounts of data to provide meaningful information to users.\u201d (p. 9) Organization development theory Gupta and George (2016), Wang et al. (2019), Mikalef et al. (2020b), Shuradze and Wagner (2016) Yu et al. (2021a) BDAC is defined as an \u201corganizational facility with tools, techniques, and processes that enable a firm to process, organize, visualize, and analyse data thereby Organizational information processing theory Supply chain management Srinivasan and Swink (2018) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 14 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies producing insights that enable data-driven operational planning, decision-making, and execution.\u201d (p. 2) Yu et al. (2021b) BDAC refers to \u201corganizational facility with tools, techniques, and processes that enable a firm to process, organize, visualize, and analyse data, thereby producing insights that enable data-driven operational planning, decision-making, and execution.\u201d (p. 4) Organizational information processing theory Health care Srinivasan and Swink (2018) Yu et al. (2022) BDAC is defined as \u201cthe ability to acquire, store, process, and analyse large amount of health data in various forms, and deliver meaningful information to users that allows them to discover business values and insights in a timely fashion.\u201d (p. 3) Resource orchestration theory Health care Wang et al. (2018a) Zhang and Lv (2021) BDACs are proposed with three dimensions of tangible resources, human resources and intangible resources, which are analyzed \u201cfrom three dimensions of management capabilities, infrastructure capabilities and human capabilities.\u201d (p. 50525) Resource-based view, IT capabilities Smart cities, public sectors Gupta and George (2016) Behl et al. (2022) \u201cBig data analytics capabilities (BDAC) are broadly defined as the competence to provide business insights using data management, infrastructure (technology) and talent (personnel) capability to transform business into a competitive force.\u201d (p. 380) Organizational information processing theory, institutional theory Micro, small and medium enterprises Akter et al. (2016) Elia et al. (2022) \u201cBDAC are defined as the knowledge, skills, and abilities that combine technology and management issues to explore data potential through sophisticated statistical, computational, and visualization tools.\u201d (p. 2) Resource-based view, dynamic capability view, and absorptive capacity view Wamba et al. (2020b) Liu et al. (2022) \u201cBDAC refers to an organizational ability that enable firms to capture, consolidate, and analyze data thus generating new insights to implement data-driven programming, decision-making, and operation.\u201d (p. 2561) Organizational information processing theories Supply chain management Gupta and George (2016), Srinivasan and Swink (2018) Uddin Murad et al. (2022) \u201cBDACs are an organizational ability with the necessary tools and techniques to process big data to produce internal associations, patterns, and insights.\u201d (p. 3) Srinivasan and Swink (2018) Jaouadi (2022) \u201cThe term big data analytics capability is the extent wherein firm has distinctive capability to identify quality problems, competency to set optimal pricing, trace profitable customers and manage lowest inventory using big data tools.\u201d (p. 2) Akter et al. (2016) Horng et al. (2022) \u201cBig data analytics capabilities refer to obtaining knowledge from internal or external partners and gaining market insight through big data tools.\u201d (p. 24) Knowledge-based dynamic capability view Germann et al. (2013) Ciasullo et al. (2022) \u201cBDA capability refers to a company's management ability, that is, the ongoing deployment of big data resources at the strategic aims to create value and develop a competitive advantage for the firm.\u201d (p. 205) Wamba et al. (2017) Zhu et al. (2022) \u201c...the ability of an organization to integrate, build, and reconfigure the information resources, as well as business processes, to address rapidly changing environments.\u201d (p. 5) Dynamic capabilities view Supply chain management Lee and Kang (2015) Arias-P \u0301erez et al. (2022) \u201cBDAC is defined as the ability of a firm to capture and analyze big data toward the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 2) Knowledge-based view Henao-Garc\u00eda et al. (2021) Olabode et al. (2022) \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 1219) Knowledge-based view and contingency theory Mikalef et al. (2020b) Bhatti et al. (2022a) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions to gain actionable insights, create business value, and establish competitive advantage\u201d (p. 4) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Bhatti et al. (2022b) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions with a view to gaining actionable insights, creating business value, and establishing competitive advantage.\u201d (p. 5) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Song et al. (2022) \u201cBDAC is defined as the ability to develop business insight by using data management, technical foundations and talents.\u201d (p. 1168) Kiron et al. (2014) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (..",
                "url": "https://openalex.org/W4387418465",
                "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
                "publication_date": "2023-10-06"
              }
            ]
          }
        ]
      },
      "S5927119467": {
        "id": "S5927119467",
        "text": "Concerns regarding privacy violations and security breaches associated with big data analytics in healthcare supply chains have been amplified during the COVID-19 pandemic, highlighting the need for organizations to implement robust data protection measures.",
        "children": [
          {
            "id": "E3394960366",
            "text": "..to SCM. They present a range of practical applications, and identify potential further research areas with the key message being that more research is required at the nexus of big data and SCM. The main difference between this paper and other survey papers is that we provided a more comprehensive and rigorous review of Big Data Analytics from 2005-2020, drawn from mainstream Journals. More so, we critically identified the main challenges of this disruptive technology in operations and supply chain management, particularly the key ethical, security and privacy issues posed to businesses, society and industrial community. Furthermore, our work acknowledge the growth of Industrial Internet of Things (IIoT) has increased the applicability and transformation of big data analytics across industries such as healthcare sector, science, engineering, finance, business, and eventually the society (Musa and Dabo, 2016). A good example is in pharmaceutical sector, where big data and mobile health are starting to transform the healthcare and diagnostics in a significant way, with new players such as Apple and Google acting as increasingly disruptive catalysts (Gautam and Pan, 2016; cited by Festa et al., 2018). Vaccine manufacturing firm, MERCK implemented Hadoop to utilise huge amount of data in producing vaccines faster and reduce the discard rates which were quite steep (Henschen, 2014; cited by Lamba and Singh, 2017). Further example for big data application in the healthcare sector is the utilisation of enormous volume of information from patient data by Scientist to detect drug interactions and design to implement optimal drug therapies (healthworkscollective.com, 2014; cited by Festa et al., 2018). ). In fact, big data can help fight the spread of communicable diseases. For example, a retrospective analysis of the 2010 cholera outbreak in Haiti indicated that mining data from Twitter and online news reports could have given the country\u2019s health officials an accurate indication of the disease\u2019s spread with lead time of two weeks (Kshetri, 2014). In banking sectors and retail industries, and in government, big data is applied extensively for analysing spending behaviour of customers, security and fraud detection, sentiment and social network analysis (Kshetri, 2014). In addition, big data can help law enforcement agencies to deploy resources more efficiently, respond quickly and increase presence in crime prone areas (Kang, 1998). Big data also enhances retail supply chain, which heavily depends on the availability of accurate and up-to-date information for demand forecast and other business execution (Gunasekaran et al., 2016). According to Schoenherr and Speier-Pero (2015; cited by Bumblauskas et al., 2017), nearly 50 percent of supply chain professionals were using BDA or had plans to use BDA in the near future. These professionals saw BDA as an enabler for better decision making, as well as a predictive analytical tool to achieve performance advantages (Bumblauskas et al., 2017). Raman et al. (2018) have also noted that the concept of big data helps to improve visibility by providing an integrated framework for monitoring performance and customer interaction through real-time data analysis and critical decision-making scenarios. Thus militating risk and supply chain disruption and failures. According to IBM\u2019s Big data and Analytics Hub (2014), \u201c10 billion mobile phones will be in use by 2020, 294 billion emails are sent every day, and trillion of sensors monitor, track with each other, populating the Internet of Things with real time data\u201d. Several supply chain literature (Baihaqi and Sohal, 2013; Li et al., 2009; Prajogo and Olhager, 2012; Vickery et al., 2003; cited by Raman et al., 2018) have acknowledged that the use of various information technologies such as Enterprise Resource Planning (ERP), Global Positioning System (GPS), Radio Frequency Identification (RFID), and Electronic Data Interchange (EDI) in operations and supply chain management enabled firms to effectively maintain control over their information, material and financial flows. Additional example of companies that adopt big data analytics and customer profiling are Amazon and Wal-Mart (Accenture, 2014). They utilise technologies to monitor, track, and secure millions of items in their inventories and rely on forecasting analytics for their \u2018\u2019anticipatory shipping\u2019\u2019, thereby predicting when customers will purchase a product and preship it to a depot close to the final destination (Rozados and Tjahjono, 2014;cited by Niebel et al., 2018). Similarly, Ford Motor Company, employed big data in capturing consumer data from vehicles through sensors and remote app-management software (Niebel et al., 2018). Based on the analysis of data from their cars\u2019 voice recognition systems, the company found out that the surrounding noise affected the performance of the software, which led to the improvement of the system through noise reduction technology and repositioning of microphone. These examples indicate the applicability of big data analytics, particularly in operations and supply chain, especially in its decision-making capacity, which ensure process improvement, management of logistics activities, enhancement of inventory management, and cost optimisation. However, despite the impacts of these disruptive technologies, it required guidance and interpretation by people who knew the tools as well as the business, including the supply chain operators. According to Lamba and Singh (2017), big data provided limited real-time applications in the context of functional areas of operations and supply chain management such as procurement, production, logistics and forecasting. In addition, the author argued that there are certain barriers to implementing predictive analytics, such as the lack of skilled professionals, lack of awareness and dearth of tools for training the next generation of data scientists in the supply chain industry. Alter et al. (2013; cited by Yang et al., 2017) went further to highlight three kinds of expertise needed in supply chain data science, namely: Technological knowledge about the tools and technology capability, functional knowledge about supply chain management, and mathematical knowledge of algorithms. The authors maintained that the blend of these three skillsets will enable companies combine data into new insights for the enterprise-decision making. Accordingly, these skillsets are in short supply now and for the foreseeable future, across all areas of business, including supply chain sector. Yang et al., (2017) echoed that the major difficulties lie in data capture, storage, searching, sharing, analysis, and visualisation. The authors warned that if organisations failed in surmounting these major skillsets gap, Big Data will become a gold mine that firms lacked the capabilities to explore. Against this background therefore, this study offers useful contribution by addressing the research question: RQ1. How is Big Data Analytics being applied and in which ways it has improved supply chain? 3. Ethical, Privacy and Security challenges of big data business analytics Tremendous progress has been made in computing power, storage capacity, and software, resulting in an increased surge in big data technologies (Pauleen et al., 2017; cited by Maple, 2017). This upsurge, has brought several privacy concerns (Biajian et al., 2018). Similarly, it has also led to the growth in the number of smart devices and the speed of that growth present challenges, particularly security and privacy issues to society and individuals\u2019 freedom, as government battle to develop policies, standards and laws that shape this development (Maple, 2017). The question is whether such laws and regulations offer adequate protections for individuals\u2019 data, especially when it relates to Internet of Things. Of course, there is no known new laws that address this emerging phenomenon. Instead, they are governed by existing legal framework. In the UK for example, personal data remains subject to the Data Protection Act of 1998. Although, there is a new proposed regulatory framework by UK government that will improve citizens\u2019 online safety (Online Hams Whitepaper, 2019). Moreover, the characteristics of big data are tightly linked to privacy, ethical and security effects to customer welfare and these fallouts have attracted the attention of scholars, businesses, society, industries and policy makers (Kshetri, 2014). According to ISACA 9 (2014) and Kshetri (2014) huge amount of data means that security breaches and privacy violation are likely, which can cause severe consequences and losses, leading to reputational damage, legal liability, ethical harms and other amplified technical issues. Further, Laney (2001) outlined some factors that can necessitate these challenges, namely: (1) the enormous amount of data (volume), (2) a wide variety of data coming from highly diverse sources (variety), and (3) the pace of data processing (velocity). Therefore, the greatest challenge now is to find new ways of storing and processing these different types of data, since the old databases are mainly structured for only one type of data (Chen et al., 2014 and Hofmann, 2017). The analysis of these data, extraction of insights and appropriation of its values, without any infringement to ethical, security, and personal privacy data protection, presents a key challenge to not only business but also society. Therefore, against this background, this paper will attempt to answer the following research questions: RQ2. What are the ethical, security and privacy challenges associated with Big Data Analytics? RQ3. What are the likely societal, business and governance roles in addressing these emerging technologies? 3.1. Ethical issues The importance of ethics is very significant in big data analytics, especially when it comes to profiling of users through social media analytics. At the same time, people are increasingly becoming aware of how their personal data are being used, even as some individuals and firms are very keen to trade personal data for commercial gains. Therefore, specific laws and regulations must be established to preserve the personal and sensitive information of customers, as no specific laws address these invasive and intrusive technologies. Some data will need more protection than others. For example, a sensor tracking pallets, being shipped overseas is unlikely to transmit such sensitive personal data by comparison with a wearable fitness device, which for instance, might be transmitting medical information. It is also clear that connected devices are often located in intimate spaces such as the home and car or the case of smart pills that..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          },
          {
            "id": "E3902845575",
            "text": "..shown in Table 4. Moreover, the matrix level was explored based on broad theme and sub-themes. This was done in a way that shows a progressing expansion of conceptual framework, from bottom-up to topdown approach. The bottom-up approach is the beginning of the extraction of sub-themes, as individual research articles in the review were determined inductively. The coding was designed in a manner that allowed the capturing of the most dominant conceptual standpoints in the articles towards the main issues of privacy and security, as well as on how big data analytics can enhance economic efficiency, improve access to social services, as well as increase innovative platform for information and communications. To ensure validity and reliability of the coding, the research team read the articles in chronological order starting from 2005 to 2020. Separate lists were generated and crossvalidated. There were discussions among the research team about the classification of the themes and sub-themes and consensus were reached. In general, the reconciliation of the useful insights arising from the top-bottom and bottom-up approaches were achieved within the research team and the team adopted the four principal facets of BDSCA development, namely: big data analytics, applications, ethics and privacy \u2013 for representing the conceptual matrix at the top-most level. The next section explained the results in details. Table 4: Thematic landscape in terms of broad themes and sub \u2013themes (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (Number): showing the conceptual underpinning of the sub-them focus Conceptual standpoints of selective articles (full list of the authors) Big data analytics (1): conceptualisation of big data analytics represents a paradigm shift, enabling better exploitation of the growing amount of data that originates within or outside the focal business environment. Article Stream (1): this conceptualisation is helpful as big data platforms can influence business process management in productivity and flexibility for an enterprise. Article (stream 2): focus increases in the volume and detail of data captured by organisations, such as rise in social media, Internet of Things, Cloud computing, cyberphysical systems, Machine learning, 3D printing, and multimedia has produced overwhelming flow of data in either structured or unstructured format. These Industry 4.0 impacts revenue growth, competitiveness, productivity gains to organisations. Article (stream 3): the evolutionary trend of these technologies has shifted focus from the traditional supply chain data to an accelerating data sources characterised by volume, variety, veracity and value. Chae (2015) proposed a new analytical framework designed for the assessment of social use in supply chain setup. Based on Twitter hashtags, the author investigates how Twitter posts can help companies to better shape demand whilst at the same time delivering valuable customer insights, which are useful for new products developments. Although the presented approach is an interesting application of Big data in supply chain context, it lacks a wider conceptual approach. Musa and Dabo. (2016) investigated the status of Big data in the environments of cloud computing. The relationship between big data and cloud computing were addressed, with particular focus on scalability, data integrity, availability, data transformation and regulatory and legal issues. However, the study has limited managerial inputs in terms of supply chain and operations applications. Combining RFID and cloud computing, Guo et al. (2015) present an intelligent decision support system architecture for production monitoring and scheduling in labour-intensive distributed manufacturing setups. This research addresses main benefits of linking Big Data Analytics and SCM, as the novel architecture approach highlights how the systematic integration of data-driven decision making in production and logistics operations increases information visibility and transparency across supply chain. Supply chain shocks and adaptations amid the covid-19 pandemic and post-pandemic recoveries provide indisputable evidence for the urgent needs of digital techniques for mapping supply chain networks and ensure visibility (Ivanov and Dolgui, 2020). Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number): broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Big data analytics (2): Being able to make sense of a subjective, disparate data, such as audio files, in the system for analysis remains a challenge. Article stream (4): a focus on Machinelearning allows a software to respond to data such as customer shopping trends, weather and geography, and can sense patterns in shopping behaviour. Article (stream 5): there are more focus in employing machine learning through uploading of information and making predictions about the future behaviour based on behavioural residues and interpretation of individuals\u2019 footprints. Article (stream 6): Similar focus on other related advanced technology such as cloud computing to mitigate the time-demanding and challenging task, which required computational infrastructure to ensure successful data processing and analysis. Kenda and Dunja (2018) proposed a model that uses an algorithm for data cleaning stream mining with low noise, which intends to perform better than the current state-of-the-art batch processing method. Although this proposed model sounds very plausible, it shows limited applications since data preparation is crucial for real-time data mining applications. The article by Lopez-Cuadrado et al. (2019; cited by Din et al, 2019) investigates big and small datasets in a non-big data environment, analysing big datasets by applying the first stage machinelearning model. The paper proposes a framework that focused on data preparation by finding an optimal machine \u2013learning model. The study established layers of big data architectures that focused on getting an outcome of data. However, the framework fails to address how this approach enhances supply chain operations. Din et al. (2019) investigates how Machinelearning used behavioural residue in digital footprints to assess and drive personality predictions, particularly in examining Anxiety in micro-blogs from social media tweets. This particular technique is new, and it is limited to healthcare sector. In addition, the research by Zhaodan et al. (2016; cited by Gruda and Souleiman. 2018) provided a link between machine-learning and cyber-physical systems performance. However, the article showed limited managerial inputs in SCM in terms of the applicability of this technology. Presenting findings on how Supply Chain (SC) 4.0 can gain competitive advantage, the research by Makris et al. (2019) focus on three key emerging developmental trends: Big data, Cloud Computing and 3D printing. Although the article presents graphical framework but show limitations in understanding the issues around adaptability, strength and challenges. Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Applications (1): Big data application complements decision support systems (DSS), foster complex decision-making and help problem solving in supply chain management. Article stream (7): the focus is on the emphasis of the properties of big data in providing useful business insights to companies in decision making towards optimising operations and supply chain. Article stream (8): Big data is also used for positioning of goods on the shop shelves as well as employed to get a better handle on company account, manage regulatory requirements and cost savings. Tan et al. (2015), for example, highlighted the potential of Big data as a source for supply chain innovation. Being an interesting contribution to research at the nexus of Big data Analytics and SCM, the proposed analytical structured approach enables companies to integrate their supply chain innovation competencies with supply chain partners systematically, leveraging Big data Analytics insights. In an increasingly competitive and fast-paced business environment, the article\u2019s promoted extension of the innovation capability into the supply chain seems a promising strategy, considering that crucial knowledge may reside outside a single company. The article presented by Sabashini and Kavitha (2011) contributes to the theory development in SCM through investigation of the potential impact of Big Data Analytics on information usage in a corporate and supply chain context. The paper provided insights into the opportunities and challenges emerging form the adoption of Big Data Analytics in SCM. Drawing from resource-based view, Chavez et al. (2017) investigated the linkage between data-driven supply chains, manufacturing capability and customer satisfaction. The research outcomes suggest that data-driven supply chain are positively associated with manufacturing competitiveness dimensions such as quality, delivery, flexibility, and cost, which in turn leads to customer satisfaction. Addressing the challenges linked to the growing amount of data, a major problem in modern manufacturing, Zhang et al. (2015) present an integrated approach for optimised logistics management on the shop floor, based on the smart manufacturing concept, using production resources equipped with RFID facilities. Table 4: Thematic landscape in terms of broad themes and sub \u2013theme (article streams) BDSCA(the number):broad theme reflecting the main focus Article stream (number): showing the conceptual underpinning of the sub-theme focus Conceptual standpoints of selective articles (full list of the authors) Applications (2): Business intelligence and analytics are enhanced with Big data for decision on strategy, managing optimisation and competition. Article stream (9): the focus is centred on the prediction that Big data would improve 60 per cent of existing business and will foster new businesses in the next decade. Article stream (10): the focus in real-time information processing technology based on RFID and NFC in IoT will be widely used in supply chain. As result, the demand forecast would be more accurate and extra buffers would be unnecessary. Opresnik and Taisha (2015; cited by Wemba et al., 2015) linked the advantages of Big Data to the business strategy of servitisation. Their proposed strategic framework outlines how companies can extend their value portfolio by adding information as the pillar alongside the traditional aspects of product and service. Barbaray et. (2018; cited by Fatorachian and Kazemi, 2018) reviewed research on Industry 4.0 concerning issues of SMEs and suggested that SMEs adopt Industry 4.0 concepts only for monitoring industrial processes. Presenting findings from a systematic review and longitudinal case..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          },
          {
            "id": "E4940110996",
            "text": ".., Cuomo, M.T. and Solima, L. (2018). \"Big data for big pharma: Harmonizing business process management to enhance ambidexterity\u201d, Business Process Management Journal, 24(5), pp. 1110-1123. https://doi.org/10.1108/BPMJ-10-2017-0272 Financial Times Archive (2015). Gartner, M., and Brocca. R. 2015. \u2018\u2019Deconstructing supply chain analytics.\u2019\u2019 Journal of Supply Chain Management, 25(2015): 200-326. Gunasekaran, A., Papadopoulos, T., and Dubey, R., and Childe, S.J.Stephen J. 2016. \u2018\u2019Big data and predictive analytics for supply chain and organizational performance.\u2019 Journal of Business Research, 70(2016): pp.308-31. https://doi.org/10.1016/j.jbusres.2016.08.004 Gruda, D. and Hasan, S., 2019. \u201cFeeling anxious? Perceiving anxiety in tweets using machine learning\u201d, Computers in Human Behaviour, 98(2019), pp.245-255. https://doi.org/10.1016/j.chb.2019.04.020 Guo, Z., Ngai, E., Yang, C. and Liang, X., 2015. \u201cAn RFID-based intelligent decision support system architecture for production monitoring and scheduling in a distributed manufacturing environment\u201d, International journal of production economics, 159, pp.16-28. Harris, I., Wang, Y. and Wang, H., 2015. \u201cICT in multimodal transport and technological trends: Unleashing potential for the future\u201d, International Journal of Production Economics, 159(2015), pp.88-103. https://doi.org/10.1016/j.ijpe.2014.09.005 Hashem, I.A.T., Yaqoob, I., Anuar, N.B., Mokhtar, S., Gani, A. and Khan, S.U., 2015. \u201cThe rise of \u201cbig data\u201d on cloud computing: Review and open research issues\u201d, Information systems, 47(2015), pp.98-115. https://doi.org/10.1016/j.is.2014.07.006 Hofmann, E., 2017. \u201cBig data and supply chain decisions: the impact of volume, variety and velocity properties on the bullwhip effect\u201d, International Journal of Production Research, 55(17), pp.5108-5126. https://doi.org/10.1080/00207543.2015.1061222 Ivanov, D. and Dolgui, A., 2020. \u201cA digital supply chain twin for managing the disruption risks and resilience in the era of Industry 4.0\u201d, Production Planning & Control, pp.1-14. https://doi.org/10.1080/09537287.2020.1768450 Kenda, K. and Mladeni\u0107, D., 2018. \u201cAutonomous sensor data cleaning in stream mining setting\u201d, Business Systems Research Journal, 9(2), pp.69-79. DOI: https://doi.org/10.2478/bsrj-2018-0020 Kshetri, N., 2014. \u201cBig data\u05f3 s impact on privacy, security and consumer welfare\u201d, Telecommunications Policy, 38(11), pp.1134-1145. https://doi.org/10.1016/j.telpol.2014.10.002 Kusiak, A., 2018. \u201cSmart manufacturing\u201d, International Journal of Production Research, 56(1- 2), pp.508-517. https://doi.org/10.1080/00207543.2017.1351644 Lamba, K. and Singh, S.P., 2017. \u201cBig data in operations and supply chain management: current trends and future perspectives\u201d, Production Planning & Control, 28(11-12), pp.877- 890. https://doi.org/10.1080/09537287.2017.1336787 Li, J., Tao, F., Cheng, Y. and Zhao, L., 2015. \u201cBig data in product lifecycle management\u201d, The International Journal of Advanced Manufacturing Technology, 81(1-4), pp.667-684. Luppicini, R. and So, A., 2016. \u201cA technoethical review of commercial drone use in the context of governance, ethics, and privacy\u201d, Technology in Society, 46, pp.109-119. https://doi.org/10.1016/j.techsoc.2016.03.003 Mahrt, M. and Scharkow, M., 2013. \u201cThe value of big data in digital media research\u201d, Journal of Broadcasting and Electronic Media, 57(1), pp.20-33. https://doi.org/10.1080/08838151.2012.761700 Makris, D., Hansen, Z.N.L. and Khan, O., 2019. \u201cAdapting to supply chain 4.0: an explorative study of multinational companies\u201d, In Supply Chain Forum: An International Journal, 20(2), pp. 116-131. Maple, C., 2017. \u201cSecurity and privacy in the internet of things\u201d, Journal of Cyber Policy, 2(2), pp.155-184. https://doi.org/10.1080/23738871.2017.1366536 Mendez Mena, D., Papapanagiotou, I. and Yang, B., 2018. \u201cInternet of things: Survey on security\u201d, Information Security Journal: A Global Perspective, 27(3), pp.162-182. Mikalef, P., Boura, M., Lekakos, G. and Krogstie, J., 2019. \u201cBig data analytics and firm performance: Findings from a mixed-method approach\u201d, Journal of Business Research, 98(2019), pp.261-276. https://doi.org/10.1016/j.jbusres.2019.01.044 Moeuf, A., Pellerin, R., Lamouri, S., Tamayo-Giraldo, S. and Barbaray, R., 2018. \u201cThe industrial management of SMEs in the era of Industry 4.0\u201d, International Journal of Production Research, 56(3), pp.1118-1136. https://doi.org/10.1080/00207543.2017.1372647 Mueller, S.D. and Jensen, P., 2017. \u201cBig data in the Danish industry: application and value creation\u201d, Business Process Management Journal. 23(3), pp. 645-670. https://doi.org/10.1108/BPMJ-01-2016-0017 Musa, A. and Dabo, A.A.A., 2016. \u201cA review of RFID in supply chain management: 2000\u2013 2015\u201d, Global Journal of Flexible Systems Management, 17(2), pp.189-228. https://doi.org/10.1007/s40171-016-0136-2 Niebel, T., Rasel, F. and Viete, S., 2019. \u201cBIG data\u2013BIG gains? Understanding the link between big data analytics and innovation\u201d, Economics of Innovation and New Technology, 28(3), pp.296-316. https://doi.org/10.1080/10438599.2018.1493075 Oberl\u00e4nder, A.M., R\u00f6glinger, M., Rosemann, M. and Kees, A., 2018. \u201cConceptualizing business-to-thing interactions\u2013A sociomaterial perspective on the Internet of Things\u201d, European Journal of Information Systems, 27(4), pp.486-502. https://doi.org/10.1080/0960085X.2017.1387714 Opresnik, D. and Taisch, M., 2015. \u201cThe value of big data in servitization\u201d, International journal of production economics, 165, pp.174-184. https://doi.org/10.1016/j.ijpe.2014.12.036 Phase, A. and Mhetre, N., 2018. \u201cUsing IoT in Supply Chain Management\u201d International Journal of Engineering and Techniques, 4 (2), pp. 973-979. Psannis, K.E., Xinogalos, S. and Sifaleras, A., 2014. \u201cConvergence of Internet of things and mobile cloud computing\u201d, Systems Science & Control Engineering: An Open Access Journal, 2(1), pp.476-483. https://doi.org/10.1080/21642583.2014.913213 Queiroz, M.M. and Telles, R., 2018. \u201cBig data analytics in supply chain and logistics: An empirical approach\u201d, The International Journal of Logistics Management, 29(2), pp. 767-783. https://doi.org/10.1108/IJLM-05-2017-0116 Raman, S., Patwa, N., Niranjan, I., Ranjan, U., Moorthy, K. and Mehta, A., 2018. \u201cImpact of big data on supply chain management\u201d, International Journal of Logistics Research and Applications, 21(6..",
            "url": "https://clok.uclan.ac.uk/34756/1/34756%20Big%20Data%20Analytics%20-%20Manuscripts%20Final%20Version%20%281a%29.pdf"
          }
        ]
      },
      "S9043179572": {
        "id": "S9043179572",
        "text": "The COVID-19 pandemic prompted healthcare supply chains to adopt big data analytics for real-time decision-making, enhancing their responsiveness to sudden demand shifts and supply disruptions.",
        "children": [
          {
            "id": "E3155705893",
            "text": "..against damage or loss, as well as the ability to rapidly recover from a disruption (Hora & Klassen, 2013). Thus, resilience is dynamic instead of static, which is considered as a fundamental attribute that supply chains need to adopt for maintaining stable growth in the face of external disruptions (Essuman et al., 2020) such as the COVID-19 pandemic. Supply chain resilience consists of multiple constituent elements, including stability, agility, robustness, collaboration, redundancy, centralization, visibility, and information sharing (Hosseini et al., 2019). Tukamuhabwa et al. (2015) emphasized the importance of building collaborative relationships in improving supply chain resilience. Other measures include maintaining slack resources, adopting a flexible production strategy, and building a risk-management infrastructure (Ambulkar et al., 2015; Modi & Mishra, 2011). Although 2Rs are the key attributes for enterprises to improve supply chain performance, there is limited research on how to develop responsive and resilient strategies to deal with long-lasting, structural, and global impacts. Previous studies mainly focused on normal situations, in which supply chain integration and big data analytics are the bases of building 2Rs supply chains. Supply chain integration is the ability to integrate all activities among a company's internal functions and external partners (supplier, distributor, retailer, etc.), until the finished product arrives at the end customer (Zhao et al., 2013). From the perspective of collaborative partners, supply chain integration could be divided by horizontal strategies and vertical strategies (Mesquita & Lazzarini, 2008), while the vertical integration is further divided by integration with suppliers (also known as upstream integration) and integration with customers (also known as downstream integration). In the era of Industry 4.0, supply chain integration consists of three dimensions: process and activity integration, technology and system integration, and organizational relationship linkages (Tiwari, 2020). A recent editorial in JOM (Browning, 2020) discussed related aspects of organizational and process integration. In recent years, under the influence of economic globalization, supply chains have been transformed. Since globalization has become pervasive, suppliers have pursued global markets, and most companies source extensively from global suppliers (Cohen & Lee, 2020). This has led to an increase in outsourcing activities and a corresponding decline in vertical integration of supply chains. As a result, supply chain networks have become flatter and more complex, composed of different organizations dispersed across multiple tiers and different geographies, and extended beyond a single country's boundaries (Choi & Hong, 2002). Global supply chains are characterized by focal firms that distribute across multiple countries, locate production facilities abroad, or source from offshore suppliers. Munir et al. (2020) showed that integration in the global supply chain could increase companies' resilience in making flexible deliveries and the number of products. Big data refers to data that arrive at a high volume and velocity with considerable variation, while analytics refers to the ability to gain insights from data via statistics, learning, optimization, or other techniques. The applications of big data and analytics are closely interlinked to enable firms to make better decisions. Hence, prior literature has typically discussed them together as big data analytics, which allows the use of advanced computing techniques, strategies and architectures to store, extract, and analyze multi-source, heterogeneous data to support decisions, and has been commonly used in operations management (Wamba et al., 2015). In the era of economic globalization, supply chain management has become extremely complex, with large-scale and online decision-making challenges emerging (Yang et al., 2021), for example, the joint decision-making between proactive planning and reactive operations in the forms of demand forecasting, production planning, inventory management, supply allocation, transportation, and distribution. It is no longer efficient to rely on traditional analytics methods. Many firms have been exploring how to take big data analytics to promote lean and agile activities in supply chain management (Baruffaldi et al., 2019). Existing studies have shown that applications of digital technologies can help improve supply chain performance by enhancing visibility and reducing supply chain risks (Govindan et al., 2018). The digitalization of supply chains produces large volumes of data, which is regarded as a new kind of resource and has the potential to create value and enhance competitiveness. Singh and El-Kassar (2019) proposed that digital technologies have transformed traditional supply chain management into a more data-driven approach, which requires a much higher level of big data analytics capabilities compared to traditional supply chain management. Following the call for papers, the submission of 114 manuscripts, and the review and revision process, seven articles were selected for this special issue that contribute to our understanding of the impact of COVID-19 on supply chains and its effect on addressing the 2Rs. In \"Strengthening supply chain resilience during COVID-19: A case study of JD.com\" (Shen & Sun, 2023), the authors used quantitative operational data obtained from JD.com4 to analyze the impact of the pandemic on supply chain resilience. They described the challenging scenarios that retailing supply chains experienced in China and the practical response of JD.com over the course of the pandemic the pandemic. JD.com was observed to respond well to the exceptional demand and severe logistical disruptions caused by COVID-19 in China based on its highly integrated supply chain structure (including both process and activity integration and technology and system integration) and comprehensive digital technologies. In particular, the existing, intelligent platforms and delivery procedures were modified slightly but promptly to deal with specific disruptions. The joint efforts of multiple firms, the government, and the entire Chinese society contributed to surmounting the challenges. The experience of JD.com contributes to understanding of the value of investing in operational flexibility and beyond-supply-chain collaboration given the possibility of large-scale supply chain disruptions such as the COVID-19 outbreak. In \"Breaking out of the pandemic: How can firms match internal competence with external resources to shape operational resilience?\" (Li et al., 2023), the authors explored how firms sought to effectively combine internal competence with external resources from the supply chain network to improve operational flexibility and stability during the COVID-19 pandemic. The internal flexibility refers to product diversity, the internal stability refers to operational efficiency, the external flexibility refers to structural holes, and the external stability refers to network centrality. Drawing upon matching theory, the authors provided an internal-external combinative perspective to explain operational mechanisms underlying different matchings. Based on the empirical results of 2994 unique firms and 5293 observations, they found that more heterogeneous combinations between internal (external) flexibility and external (internal) stability may result in a complementary effect that enhances operational resilience, whereas more homogeneous combinations between internal flexibility (or stability) and external flexibility (or stability) may have a substitutive effect that reduces operational resilience. With the COVID-19 pandemic having had a significant impact on supply chains, government initiatives have played a central role in managing the crisis. In \"The impact of governmental COVID-19 measures on manufacturers' stock market valuations: The role of labor intensity and operational slack\" (Chen et al., 2023), the authors investigated the impact of the Chinese government's Level I emergency-response policy (Ge et al., 2020) on manufacturers' stock-market values, and the role of manufacturers' operational slack on adding resilience. Specifically, through an event study of 1357 Chinese manufacturing companies listed on the Shenzhen Stock Exchange, the authors found that the government's emergency-response policy triggered a statistically significant positive reaction from the stock market for manufacturers. However, the authors also found negative impacts on stock market values for manufacturers in labor-intensive industries because of the labor immobility triggered by the Level I measures. In addition, this article identified the positive role of operational slack in the form of financial slack and excess inventory in helping to maintain operations and business continuity, mitigate risks caused by the labor mobility restrictions, and improve supply chain resilience, which identifies operational slack as a supply chain resilience strategy to mitigate pandemic-related risks. When the COVID-19 pandemic broke out, the medical-product industry faced unprecedented demand shocks for personal protective equipment, including face masks, face shields, disinfectants, and gowns. Companies from various industries responded to the urgent need for these potentially life-saving products by adopting ad hoc supply chains in an exceptionally short time. In \"Realizing supply chain agility under time pressure: Ad hoc supply chains during the COVID-19 pandemic\" (M\u00fcller et al., 2023), the authors explored the use by 34 German companies of ad hoc supply chains to produce personal protective equipment. From these cases, the authors developed an emergent theoretical model of ad hoc supply chains around enablers of supply chain agility such as dynamic capabilities (the ability to integrate, build, and reconfigure internal and external competences to address rapidly scenario changes), entrepreneurial orientation (proactiveness, risk-taking, innovativeness, autonomy, and competitive aggressiveness), and temporary orientation (speedy action in a limited time). To cope with the COVID-19 crisis, many firms allowed their employees to work from home (WFH). In \"Working from home and firm resilience to the COVID-19 pandemic\" (Ge et al., 2023), the authors examined whether a firm's WFH capacity increased its resilience. The authors put forward and tested a unique data set that combines listed firms' financial data, epidemiological data, and online job postings data from China. They found that imposing COVID-19 anti-contagion policies on firms and their suppliers or customers significantly increased their operating revenue volatility, slowed their recovery, and had repercussions on their supply chains. WFH enhanced firms' resistance capacity by reducing the effect of COVID-19 on their operating revenue volatility and disruptions to their supply chain partners; however, it also decreased their recovery capacity by extending the time taken to return to normal. Firm attributes, along with workers' occupations, education, and experience, impacted the effect of WFH on firm resilience. This article enhances our understanding of shock transmission across supply..",
            "url": "https://openalex.org/W4365518480",
            "title": "Building responsive and resilient supply chains: Lessons from the <scp>COVID</scp>\u201019 disruption",
            "publication_date": "2023-04-01"
          },
          {
            "id": "E6357881701",
            "text": "..of giant techs is needed because researchers, doctors and scientists can be effectively supported to expedite the research and development of COVID-19 virus. 130822 VOLUME 8, 2020 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts Recently, IBM announced that they are now providing a cloud-based research resource that has been trained on a COVID-19 open dataset (CORD-19) [16], which is a collection of research articles related to COVID-19. Moreover, IBM has adopted their proposed AI technology for drug discovery, from which 3000 novel COVID-19 molecules have been obtained, officially reported in [17]. Another support is from the White House Office of Science and Technology Policy, the U.S. Department of Energy and IBM with the development of COVID-19 HPC Consortium (https://covid19- hpc-consortium.org/), which is open for research proposals concerning COVID-19. Another example is the Coronavirus International Research Team (COV-IRT) (https://www.covirt.org/), a group of scientists who are developing vaccines and therapeutic solutions against COVID-19. C. BIG DATA 1) DEFINITION AND CHARACTERISTICS The rapid development of the Internet of Things (IoT) results in a massive explosion of data generated from ubiquitous wearable devices and sensors [18]. The unprecedented increase of data volumes associated with advances of analytic techniques empowered from AI has led to the emergence of a big data era [18], [19]. Big data has been employed in a wide range of industrial application domains, including healthcare where electronic healthcare records (EHRs) are exploited by using intelligent analytics for facilitating medical services. For example, health big data potentially supports patient health analysis, diagnosis assistance, and drug manufacturing [20]. Big data can be generated from a number of sources which may include online social graphs, mobile devices (i.e. smartphones), IoT devices (i.e. sensors), and public data [21] in various formats such as text or video. In the context of COVID-19, big data refers to the patient care data such as physician notes, X-Ray reports, case history, list of doctors and nurses, and information of outbreak areas. In general, big data is the information asset characterized by such a high volume, velocity and variety to acquire specific technology and analytical methods for its transformation into useful information to serve the end users, i.e, big data in digital twin technologies [22]\u2013[25]. The three characteristics of big data are summarized as follows. \u2022 Volume: This feature shows the huge amount of data that can range from terabytes to exabytes. According to a Cisco\u2019s forecast, the data traffic is expected to reach 930 exabytes by 2020, a seven-fold growth from 2017 [26]. \u2022 Variety: It refers to the diversity and heterogeneity of big data. For example, big data in healthcare can be produced from healthcare users (i.e. doctors, patients), medical IoT devices, and healthcare organizations. Data can be formatted in text, images, videos with structured or un-structured dataset types [27]. \u2022 Velocity: It expresses the data generation rate that can be calculated in time or frequency domain. In fact, in industrial applications like healthcare, data generated from devices is always updated in real-time, which is of significant importance for time-sensitive applications such as health monitoring or diagnosis [28]. 2) BIG DATA FOR COVID-19 FIGHTING Big data has been proved its capability to support fighting infectious diseases like COVID-19 [29], [30]. Big data potentially provide a number of promising solutions to help combat COVID-19 epidemic. By combining with AI analytics, big data helps us to understand the COVID-19 in terms of outbreak tracking, virus structure, disease treatment, and vaccine manufacturing [31]. For example, big data associated with intelligent AI-based tools can build complex simulation models using coronavirus data streams for outbreak estimation. This would aid health agencies in monitoring the coronavirus spread and preparing better preventive measurements [32]. Models from big data also supports future prediction of COVID-19 epidemic by its data aggregation ability to leverage large amounts of data for early detection. Moreover, the big data analytics from a variety of real-world sources including infected patients can help implement large-scale COVID-19 investigations to develop comprehensive treatment solutions with high reliability [33], [34]. This would also help healthcare providers to understand the virus development for better response to the various treatment and diagnoses. Based on the above analysis, we want to highlight that big data analytics is the process of collecting and analyzing the large volume of data sets to discover useful hidden patterns and other information, e.g., COVID-19 data discovering. Moreover, AI (and explainable AI [35]) aims to apply logic and reasoning to build human intelligence that can mimic the function of a machine for learning, classifying, and estimating possible outcomes, e.g., COVID-19 symptom classifications. The potential applications of each technology in fighting COVID-19 pandemic will be explained and discussed in the following sections through a number of practical use cases. III. APPLICATIONS OF AI FOR FIGHTING COVID-19 This section presents representative applications of AI in fighting the COVID-19 outbreak. A. AI FOR COVID-19 DETECTION AND DIAGNOSIS As one of the most effective solutions to combat the COVID-19 pandemic, early treatment and prediction are of importance. Currently, the standard method for classifying respiratory viruses is the reverse transcription polymerase chain reaction (RT-PCR) detection technique. In response to the COVID-19 virus, some efforts have been dedicated to improve this technique [36] and for other alternatives [37]. These techniques are, however, usually costly and time-consuming, have low true positive rate, and require specific materials, equipment and instruments. Moreover, most countries are suffering from a lack of testing kits due to the limitation on budget and techniques. Thus, VOLUME 8, 2020 130823 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts FIGURE 2. An illustration of DL-based frameworks for the COVID-19 detection and diagnosis. the standard method is not suitable to meet the requirements of fast detection and tracking during the COVID-19 pandemic. A simple and low-cost solution for COVID-19 identification is using smart devices together with AI frameworks [38], [39]. This is referred to as mobile health or mHealth in the literature [40]. These works are advantageous since smart devices are daily used for multi-purposes. Moreover, the emergence of cloud and edge computing can effectively overcome the limitation of batter, storage, and computing capabilities [41]. Another directive for COVID-19 detection is to use AI techniques for medical image processing, which recently appeared in many research works on coronavirus [42]\u2013[51]. As we limit this paper to the COVID-19 virus, the interested readers are invited to read the surveys in [52], [53] for other applications of DL in medical image analysis. It is noted from these works that X-ray images and computed tomography (CT) scans are widely used as the input of DL models so as to automatically detect the infected COVID-19 case. Motivated by an important finding that infected COVID-19 patients normally reveal abnormalities in chest radiography images, the authors in [54] designed a deep convolutional neural network (CNN) model for the detection of COVID-19 cases. The three-class classification (normal, COVID-19 infected, and non-COVID-19 infected) in this work is helpful if the medical staff needs to decide which cases should be tested with standard methods (between normal and COVID-19 infected cases), and which treatment strategies should be taken (between non- and COVID-19 infected cases). By training on an open source dataset with 13, 975 images of 13, 870 patients, the proposed CNN model can achieve an accuracy of 93.3%. The use of ML and DL techniques with chest CT scans for COVID-19 detection were considered in [42], [43], [55]\u2013[58], respectively. These works show a high performance as they can achieve a high classification accuracy, e.g., 99.68% in [42], an area under curve (AUC) score of 0.994 in [43], AUC of 0.996 in [55], and accuracy of 82.9% (98.27%) with specificity of 80.5% (97.60%) and sensitivity of 84% (98.93%) in [56] and [57]. As the cost of X-ray scans is usually cheaper than CT images, a large portion of research works for COVID-19 detection utilize DL models with X-ray images. For example, the work in [46] leveraged a deep CNN model, called Decompose, Transfer, and Compose (DeTraC), to process chest X-ray images for the classification of COVID-19. The main purpose of the decomposition layer is to reduce the feature space, thus yielding more sub-classes but improving the training efficiency, whereas the composition layer is to combine sub-classes from the previous layer so as to produce the final classification result. Besides the decomposition and composition layers, a transfer layer is positioned in the middle to speed up the training time, reduce the computation costs, and make the DL model trainable with small datasets. The importance of transfer learning made it a wide consideration in many works, e.g., [45], [48], [49]..",
            "url": "https://openalex.org/W3019283372",
            "title": "Artificial Intelligence (AI) and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts",
            "publication_date": "2020-01-01"
          }
        ]
      },
      "S9715092690": {
        "id": "S9715092690",
        "text": "The integration of big data analytics with innovation leadership fosters supply chain innovation, which is crucial for maintaining operational effectiveness in healthcare supply chains during crises.",
        "children": [
          {
            "id": "E3581931727",
            "text": "..), 1621\u20131651. Barlette, Y., Baillette, P., 2022. Big data analytics in turbulent contexts: towards organizational change for enhanced agility. Prod. Plan. Control 33 (2\u20133), 1\u201318. Barney, J., 1991. Firm resources and sustained competitive advantage. J. Manag. 17 (1), 99\u2013120. Barton, D., Court, D., 2012. Making advanced analytics work for you. Harv. Bus. Rev. 90 (10), 78\u201383. **Behl, A., 2022. Antecedents to firm performance and competitiveness using the lens of big data analytics: a cross-cultural study. Manag. Decis. 60 (2), 368\u2013398. **Behl, A., Gaur, J., Pereira, V., Yadav, R., Laker, B., 2022. Role of big data analytics capabilities to improve sustainable competitive advantage of MSME service firms during COVID-19 \u2013 a multi-theoretical approach. J. Bus. Res. 148, 378\u2013389. **Belhadi, A., Kamble, S.S., Zkik, K., Cherrafi, A., Touriki, F.E., 2020. The integrated effect of Big Data Analytics, Lean Six Sigma and Green Manufacturing on the environmental performance of manufacturing companies: the case of North Africa. J. Clean. Prod. 252, 119903. **Bertello, A., Ferraris, A., Bresciani, S., de Bernardi, P., 2021. Big data analytics (BDA) and degree of internationalization: the interplay between governance of BDA infrastructure and BDA capabilities. J. Manag. Gov. 25 (4), 1035\u20131055. Beyer, M.A., Laney, D., 2012. The importance of \u2018Big Data\u2019: a definition. Available online at, Gartner. https://www.gartner.com/en/documents/2057415. (Accessed 8 March 2021). Bharadwaj, A.S., 2000. A resource-based perspective on information technology capability and firm performance: an empirical investigation. MIS Q. 24 (1), 169\u2013196. Bhatt, G.D., Grover, V., 2005. Types of information technology capabilities and their role in competitive advantage: an empirical study. J. Manag. Inf. Syst. 22 (2), 253\u2013277. **Bhatti, S.H., Ahmed, A., Ferraris, A., Hirwani Wan Hussain, W.M., Wamba, S.F., 2022a. Big data analytics capabilities and MSME innovation and performance: a double mediation model of digital platform and network capabilities. Ann. Oper. Res. 1\u201324. **Bhatti, S.H., Hussain, W.M.H.W., Khan, J., Sultan, S., Ferraris, A., 2022b. Exploring data-driven innovation: what\u2019s missing in the relationship between big data analytics capabilities and supply chain innovation? Ann. Oper. Res. 1\u201326. Bo\u02c7zi\u02c7c, K., Dimovski, V., 2019. Business intelligence and analytics for value creation: the role of absorptive capacity. Int. J. Inf. Manag. 46, 93\u2013103. Brynjolfsson, E., 1993. The productivity paradox of information technology. Commun. ACM 36 (12), 66\u201377. Brynjolfsson, E., Hitt, L.M., 1998. Beyond the productivity paradox. Commun. ACM 41 (8), 49\u201355. Cao, G., Duan, Y., Li, G., 2015. Linking business analytics to decision making effectiveness: a path model analysis. IEEE Trans. Eng. Manag. 62 (3), 384\u2013395. **Cetindamar, D., Shdifat, B., Erfani, E., 2022. Understanding big data analytics capability and sustainable supply chains. Inf. Syst. Manag. 39 (1), 19\u201333. **Chatterjee, S., Chaudhuri, R., Vrontis, D., 2022. Big data analytics in strategic sales performance: mediating role of CRM capability and moderating role of leadership support. EuroMed J. Bus. 17 (3), 295\u2013311. Chatterjee, S., Chaudhuri, R., Gupta, S., Sivarajah, U., Bag, S., 2023. Assessing the impact of big data analytics on decision-making processes, forecasting, and performance of a firm. Technol. Forecast. Soc. Chang. 196, 122824. 2 * indicates papers resulting from the literature review. M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 19 **Chen, C.H.V., Chen, Y.C., 2022. Influence of intellectual capital and integration on operational performance: big data analytical capability perspectives. Chin. Manag. Stud. 16 (3), 551\u2013570. Chen, C.L.P., Zhang, C.Y., 2014. Data-intensive applications, challenges, techniques, and technologies: a survey on Big Data. Inf. Sci. 275, 314\u2013347. Chen, D.Q., Preston, D.S., Swink, M., 2015. How the use of big data analytics affects value creation in supply chain management. J. Manag. Inf. Syst. 32 (4), 4\u201339. **Chen, L., Liu, H., Zhou, Z., Chen, M., Chen, Y., 2022. IT-business alignment, big data analytics capability, and strategic decision-making: moderating roles of event criticality and disruption of COVID-19. Decis. Support. Syst. 161, 113745. **Ciampi, F., Demi, S., Magrini, A., Marzi, G., Papa, A., 2021. Exploring the impact of big data analytics capabilities on business model innovation: the mediating role of entrepreneurial orientation. J. Bus. Res. 123, 1\u201313. **Ciasullo, M.V., Montera, R., Douglas, A., 2022. Building SMEs\u2019 resilience in times of uncertainty: the role of big data analytics capability and co-innovation. Transform. Gov. People Process Policy 16 (2), 203\u2013217. **Contreras Pinochet, L.H., Amorim, G.D.C.B., Lucas J\u00fanior, D., de Souza, C.A., 2021. Consequential factors of Big Data\u2019s Analytics Capability: how firms use data in the competitive scenario. J. Enterp. Inf. Manag. 34 (5), 1406\u20131428. **Corte-Real, \u02c6 N., Ruivo, P., Oliveira, T., 2020. Leveraging internet of things and big data analytics initiatives in European and American firms: is data quality a way to extract business value? Inf. Manag. 57 (1), 103141. Cosic, R., Shanks, G., Maynard, S., 2012. Towards a business analytics capability maturity model. In: ACIS 2012: 23rd Australasian Conference on Information Systems Proceedings, p. 14 (Geelong, 3-5 December). Davenport, T.H., Harris, J.G., 2007. Competing on analytics: the new science of winning. In: Language, vol. 15. Harvard Business School Press, Boston (217p). Davenport, T.H., Barth, P., Bean, R., 2012. How \u2018big data\u2019 is different. MIT Sloan Manag. Rev. 54 (1), 22\u201324. **Demir, M., Demir, S \u0327 .S \u0327 ., Yas \u0327ar, E., 2022. Big data and innovative organizational performance: evidence from a moderated-mediated model. Creat. Innov. Manag. 31 (4), 696\u2013709. **Dubey, R., Gunasekaran, A., Childe, S.J., 2019a. Big data analytics capability in supply chain agility: the moderating effect of organizational flexibility. Manag. Decis. 57 (8), 2092\u20132112. **Dubey, R., Gunasekaran, A., Childe, S.J., Roubaud, D., Wamba, S.F., Giannakis, M., Foropon, C., 2019b. Big data analytics and organizational culture as complements to swift trust and collaborative performance in the humanitarian supply chain. Int. J. Prod. Econ. 210, 120\u2013136. **Dubey, R., Bryde, D.J., Graham, G., Foropon, C., Kumari, S., Gupta, O., 2021. The role of alliance management, big data analytics and information visibility on newproduct development capability. Ann. Oper. Res. 1\u201325. **Dubey, R., Bryde, D.J., Dwivedi, Y.K., Graham, G., Foropon, C., 2022. Impact of artificial intelligence-driven big data analytics culture on agility and resilience in humanitarian supply chain: a practice-based view. Int. J. Prod. Econ. 250, 108618. **Edwin Cheng, T.C., K..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          },
          {
            "id": "E0223838797",
            "text": ".., and assume society will be mobilised to engender positive, transformative change in the short and medium term. We recognize of course this optimism may be misplaced. In the case of business-as-usual, in the medium and long term, there will also be transformative, systemic change affecting whole sectors, governance, value networks etc. in dramatic ways, perhaps to the advantage of the few, but definitely to the detriment of wider society. References Backstrand, \u0308 J., Halldorsson, \u0301 A., \u0301 2019. Engaged scholar(ship) in purchasing and supply management (PSM): creative tension or squeezed in the middle? J. Purch. Supply Manag. 25 (4), 100557. Banerjee, S.B., 2003. Who sustains whose development? Sustainable development and the reinvention of nature. Organ. Stud. 24 (1), 143\u2013180. Banerjee, S.B., 2008. Corporate social responsibility: the good, the bad and the ugly. Crit. Sociol. 34 (1), 51\u201379. Banerjee, S.B., 2010. Governing the global corporation: a critical perspective. Bus. Ethics Q. 20 (2), 265\u2013274. Bansal, P., Roth, K., 2000. Why companies go green: a model of ecological responsiveness. Acad. Manag. J. 43 (4), 717\u2013736. Bapuji, H., Husted, B.W., Lu, J., Mir, R., 2018. Value creation, appropriation, and distribution: how firms contribute to societal economic inequality. Bus. Soc. 57 (6), 983\u20131009. Baron, R.A., 2006. Opportunity recognition as pattern recognition: how entrepreneurs \u201cconnect the dots\u201d to identify new business opportunities. Acad. Manag. Perspect. 20 (1), 104\u2013119. Boer, H., Holweg, M., Kilduff, M., Pagell, M., Schmenner, R., Voss, C.A., 2015. Making a meaningful contribution to theory. Int. J. Oper. Prod. Manag. 35 (9), 1231\u20131252. Bruce, J., de Figueiredo, J., Silverman, B., 2019. Public contracting for private innovation: government capabilities, decision rights, and performance outcomes. Strat. Manag. J. 40 (4), 533\u2013555. Bryce, E., 2021. Are clothes made from recycled materials really more sustainable? The Guardian. Available at. https://www.theguardian.com/environment/2021/nov/06/ clothes-made-from-recycled-materials-sustainable-plastic-climate. Carney, M., 2021. Value(s): Building a Better World for All. William Collins, London. Carnovale, S., DuHadway, S., 2021. Continuity in the face of disruptions: purchasing and supply management research\u2019s persistence amidst COVID-19. J. Purch. Supply Manag. 27, 100720. Carter, C.R., Kaufmann, L., Ketchen, D.J., 2020. Expect the unexpected: toward a theory of the unintended consequences of sustainable supply chain management. Int. J. Oper. Prod. Manag. 40 (12), 1857\u20131871. Choi, T.Y., Rogers, D., Vakil, B., 2020. Coronavirus is a wake-up call for supply chain management. Harv. Bus. Rev. 27, 364, 0017-8012. Choi, T.Y., Krause, D.R., 2006. The supply base and its complexity: implications for transaction costs, risks, responsiveness, and innovation. J. Oper. Manag. 24 (5), 637\u2013652. L. Knight et al. Journal of Purchasing and Supply Management 28 (2022) 100753 11 Circular Flanders, 2019. The circular ambition chart. Accessed date: November 2nd, 2021. Available at: https://aankopen.vlaanderen-circulair.be/en/getting-started/th e-ambition-map. Ciulli, F., Kolk, A., Boe-Lillegraven, S., 2019. Circularity brokers: digital platform organizations and waste recovery in food supply chains. J. Bus. Ethics 1\u201333. Cook, J.A., 2010. Supply chain versus the volcano, supply chain quarterly, perspectives. Available at: https://www.supplychainquarterly.com/articles/348-supply-chain -versus-the-volcano. Cornelissen, J., Hollerer, \u0308 M.A., Seidl, D., 2021. What theory is and can Be: forms of theorizing in organizational scholarship. Organ. Theor. 2 (3), 26317877211020328. Craighead, C.W., Blackhurst, J., Rungtusanatham, M.J., Handfield, R.B., 2007. The severity of supply chain disruptions: design characteristics and mitigation capabilities. Decis. Sci. J. 38 (1), 131\u2013156. Cutler, A.C., Lark, D., 2020. The Hidden Costs of Law in the Governance of Global Supply Chains: the Turn to Arbitration, Review of International Political Economy. https:// doi.org/10.1080/09692290.2020.1821748. De Angelis, R., Howard, M., Miemczyk, J., 2018. Supply chain management and the circular economy: towards the circular supply chain. Prod. Plann. Control 29 (6), 425\u2013437. de Sousa Santos, B., 2018. Decolonising the University. Cambridge Scholars Publishing Cambridge. Dhanorkar, S., Kim, Y., Linderman, K., 2019. An empirical investigation of transaction dynamics in online surplus networks: a complex adaptive system perspective. J. Oper. Manag. 65 (2), 160\u2013189. Di Mauro, C., Giannakis, M., 2019. Special issue of the 27th annual IPSERA conference 2018-purchasing & supply management: fostering innovation. J. Purch. Supply Manag. 25 (4), 100573. Diprose, R., Kurniawan, N., Macdonald, K., Winanti, P., 2020. Regulating Sustainable Minerals in Electronics Supply Chains: Local Power Struggles and the \u2018hidden Costs\u2019 of Global Tin Supply Chain Governance, Review of International Political Economy. https://doi.org/10.1080/09692290.2020.1814844. du Plessis, C., 2012. Towards a regenerative paradigm for the built environment. Build. Res. Inf. 40 (1), 7\u201322. Ehrenfeld, J.R., Hoffman, A.J., 2013. Flourishing: A Frank Conversation about Sustainability. Stanford Business Books, Stanford, US. European Commission, 2017. Public procurement for a circular economy: good practice and guidance. Accessed on Oct 18th, 2021. Available at: https://ec.europa.eu/enviro nment/gpp/circular_procurement_en.htm. Figueres, C., Rivett-Carnac, T., 2020. The Future We Choose: Surviving the Climate Crisis. Manila Press, London. Finkenstadt, D.J., Handfield, R., 2021. Blurry vision: supply chain visibility for personal protective equipment during COVID-19. J. Purch. Supply Manag., 100689 General Motors, 2020. Ventec Life Systems and GM partner to mass produce critical care ventilators in response to COVID-19 pandemic. GM press release. March 27, 2020. https://media.gm.com/media/us/en/gm/news.detail.html/content/Pages/ne ws/us/en/2020/mar/0327-coronavirus-update-6-kokomo.html. George, G., Howard-Grenville, J., Joshi, A., Tihanyi, L., 2016. Understanding and tackling societal grand challenges through management research. Acad. Manag. J. 59 (6), 1880\u20131895. Gereffi, G., Lee, J., 2012. Why the world suddenly cares about global supply chains. J. Supply Chain Manag. 48 (3), 24\u201332. Ghosh, A., 2016. The Great Derangement: Climate Change and the Unthinkable. University of Chicago Press, Chicago and London. Goranova, M., Verstegen Ryan, L., 2021. The corporate objective revisited: the shareholder perspective. J. Manag. Stud. 59, 2. https://doi.org/10.1111/ joms.12714. Gray, J.V., Helper, S., Osborn, B., 2020. Value first, cost later: total value contribution as a new approach to sourcing decisions. J. Oper. Manag. 66 (6), 735\u2013750. Gualandris, J., Lee, D., Beattie, C., 2021a. HP Canada Co.: a circular supply chain for recycled plastic. Ivey Publishing. Available at: https://www.iveypublishing.ca/s/p roduct/hp-canada-co-a-circular-supply-chain-for-recycled-plastic/01t5c00000 CwqXiAAJ. Gualandris, J., Lee, D., Lin, R., 2021b...",
            "url": "https://openalex.org/W4220751036",
            "title": "Future business and the role of purchasing and supply management: Opportunities for \u2018business-not-as-usual\u2019 PSM research",
            "publication_date": "2022-01-01"
          },
          {
            "id": "E3703744204",
            "text": "..broadly defined \u201cas the ability of a firm to provide insights using data management, infrastructure, and talent to transform business into a competitive force.\u201d (p. 557) Resource-based view, dynamic capabilities view Akter et al. (2016), Kiron et al. (2014) Dubey et al. (2019a) \u201cBDAC is an organizational facility with tools, techniques, and processes that enable the organization to process, visualize, and analyze data, thereby producing insights that enable data-driven operational planning, decision making and execution.\u201d (p. 2095) Dynamic capabilities view, contingency theory Supply chain management Srinivasan and Swink (2018) Lin and Kunnathur (2019) \u201cBig Data Capability as a firm's capability of identifying sources, where large volumes of various kinds of data flow out at high speed, and collecting, storing, and analyzing such Big Data for the purpose of accomplishing the firm's strategic as well as operational goals.\u201d (p. 51) Dynamic capabilities view Pigni et al. (2016), Chen et al. (2015) Lozada et al. (2019) \u201c... BDA capability refers to a company's management ability, that is, the continuous use and deployment of big data resources with the strategic goal of creating value and developing a competitive advantage for the firm.\u201d (p. 2) Wamba et al. (2017), Garmaki et al. (2016), Gupta and George (2016), Kiron et al. (2014) Mandal (2019) BDA capabilities are conceptualized as \u201ca thirdorder formative construct of BDA management capability, BDA personnel expertise capability and BDA infrastructure flexibility capability\u201d (p. 298). \u201cBDA management capabilities comprise of essential first-order capabilities of planning, investment decision making, coordination and control.\u201d (p. 298). Resource-based view, dynamic capabilities view Supply chain management Wamba et al. (2017) Mikalef et al. (2019b) BDAC is \u201cdefined as the ability of a firm to capture and analyse data towards the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 273) Resource-based view, dynamic capabilities view Gupta and George (2016), Mikalef et al. (2018), Wamba et al. (2017) Rialti et al. (2019) \u201cOrganizational BDA capabilities are an ensemble of capabilities that include infrastructure flexibility, management capabilities and personnel capabilities.\u201d (p. 1) Dynamic capabilities view Wamba et al. (2017), Gunasekaran et al. (2018), Mikalef and Pateli (2017) Definition based on Wamba et al. (2017), measures of constructs based on the 3 papers. Wang et al. (2019) \u201cBig data analytics capability is defined as the ability to acquire, store, process and analyse large amounts of health data in various forms, and deliver meaningful information to users, which allows them to discover business values and insights in a timely fashion.\u201d (p. 368) Configuration theory Health care Wang and Hajli (2017) Belhadi et al. (2020) \u201cBDA capability is defined as the ability of the organizations in developing competency to generate business insights based on organizational (i.e., BDA management), physical (i.e., IT infrastructure), and human (e.g., analytics skill or knowledge) capabilities for increased business performance\u201d (p. 2) Resource-based view, dynamic capabilities view Manufacturing firms Mikalef et al. (2020b), Akter et al. (2016) Corte-Real \u02c6 et al. (2020) BDACs refer to \u201cthe extent to which BDA has been used to provide business insights into primary activities (e.g., production, distribution, and customer service).\u201d (p. 6) Resource-based view, dynamic capabilities view, knowledge management Chen et al. (2015) Mikalef et al. (2020a) BDAC is \u201cthe ability of a firm to capture and analyze data toward the generation of insights by effectively orchestrating and deploying its data, technology, and talent.\u201d (p. 2) Resource-based view, information governance theory Akter et al. (2016), Gupta and George (2016), Wamba et al. (2017), Kiron et al. (2014) Mikalef et al. (2020b) BDAC is defined as \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 7) Resource-based view, dynamic capabilities view, IT capability Gupta and George (2016), Mikalef et al. (2018) Ramadan et al. (2020) \u201cBig data analytics capabilities refer to the firm's ability to recognize and analyze different data sources to provide valuable insights.\u201d (p. 2) Manufacturing firms Hu et al. (2018) Shamim et al. (2020) BDAC is referred to as \u201ca holistic approach of analysing and processing big data for value creation.\u201d (p. 4) Knowledge based dynamic capabilities view, social capital theory Wamba et al. (2017), Akhtar et al. (2019), Song et al. (2020) BDAC is defined \u201cas the capability of firms to combine, integrate, and deploy specific big data resources.\u201d (p. 5) Information processing theory Gupta and George (2016) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 13 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies Upadhyay and Kumar (2020) \u201cBDAC is broadly defined as the competence to provide business insights using data management, infrastructure (technology), and talent (personal) capabilities to transform the business into a competitive force.\u201d (p. 2) Resource-based view, dynamic capabilities view, socio-materialism theory Wamba et al. (2017), Gupta and George (2016), Kiron et al. (2013) Kim et al. (2011), Kim et al. (2012) for measurement of constructs Wamba et al. (2020) BDAC is defined as \u201ca firm's ability to assemble, integrate, and deploy its big data-specific resource.\u201d (p. 10) IT capability, resource-based view Gupta and George (2016) Xiao et al. (2020) \u201cBDAC refers to the ability to provide business insights in the big data environment by using big data analytics personnel, big data analytics technical, and big data analytics management capabilities.\u201d (p. 18780) Dynamic capabilities view Service sector Akter et al. (2016) Yasmin et al. (2020) BDACs are referred to \u201cas a balanced combination of requisite human resource, big-data skills, advanced technologies supported by large datasets to generate analytical reports and actionable insights utilized, produced, and processed by mathematical, statistical techniques, and machine learning tools for enhanced performance.\u201d (p. 2) Resource-based view, dynamic capabilities view Akhtar et al. (2019), Akter et al. (2016), Wang et al. (2019) Zhang et al. (2020) BDAC refers to \u201ca firm's ability to assemble, integrate, and deploy its big data-specific resources.\u201d (p. 3) Source-position-performance theoretical framework Sustainability development projects Gupta and George (2016), Akter et al. (2016), Ferraris et al. (2019), Wamba et al. (2017) Ashaari et al. (2021) \u201cBDAC is termed as an organization's capacity to efficiently and strategically arrange, assemble, and apply BDA resources so that effective decision-making can be made to enhance overall organization's performance.\u201d (p. 1) Resource-based view, organizational information processing theory Higher education institutions Mikalef et al. (2020b), Shamim et al. (2020), Janssen et al. (2017), Cao et al. (2015), Awan et al. (2021) BDACs refer to a \u201cholistic process that involves the collection, analysis, use and interpretation of data for various functional divisions to gain actionable insights, create business value and establish competitive advantage.\u201d (p. 86) Resource-based view, dynamic capabilities view, institution-based view Manufacturing sector Akter et al. (2016) Bertello et al. (2021) BDAC is defined \u201cas the ability to acquire, store, process, and analyze large amounts of data in various forms and deliver meaningful information to users, allowing them to discover business values and insights in a timely fashion.\u201d (p.1040) Resource-based view Wang et al. (2018a) Ciampi et al. (2021) BDAC \u201crefer to the company's abilities to leverage on technology and talent to exploit BD towards the generation of the insights that are necessary to overperform rivals.\u201d (p. 2) Knowledge-based view, IT capability Mikalef et al. (2017), Wamba et al. (2017), Gupta and George (2016) Contreras Pinochet et al. (2021) BDAC is recognized as \u201cthe competence to provide business insights using the capacity of data management, infrastructure (technology) and talent (personnel) to transform a business into a competitive force.\u201d (p. 1410) Process-oriented dynamic capabilities, business value Gupta and George (2016) Gu et al. (2021) BDAC in supply chain management is described as \u201cthe ability of organizations to collect and organize supply chain data from heterogeneous systems distributed across organizational boundaries, analyze it either batch-wise, or real-time, or near real-time, and visualize it intuitively to create proactive supply chain system and support decision making,\u201d (p. 155) Resource-based view, dynamic capabilities view, contingency theory Supply chain management Arunachalam et al. (..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          }
        ]
      },
      "S4459050185": {
        "id": "S4459050185",
        "text": "The integration of big data analytics in healthcare supply chains during COVID-19 is supported by theoretical models that highlight the need for enhanced visibility and risk management capabilities, which are essential for navigating disruptions.",
        "children": [
          {
            "id": "S0323885390",
            "text": "The application of big data analytics in healthcare supply chains during COVID-19 has been linked to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions.",
            "children": [
              {
                "id": "E3155705893",
                "text": "..against damage or loss, as well as the ability to rapidly recover from a disruption (Hora & Klassen, 2013). Thus, resilience is dynamic instead of static, which is considered as a fundamental attribute that supply chains need to adopt for maintaining stable growth in the face of external disruptions (Essuman et al., 2020) such as the COVID-19 pandemic. Supply chain resilience consists of multiple constituent elements, including stability, agility, robustness, collaboration, redundancy, centralization, visibility, and information sharing (Hosseini et al., 2019). Tukamuhabwa et al. (2015) emphasized the importance of building collaborative relationships in improving supply chain resilience. Other measures include maintaining slack resources, adopting a flexible production strategy, and building a risk-management infrastructure (Ambulkar et al., 2015; Modi & Mishra, 2011). Although 2Rs are the key attributes for enterprises to improve supply chain performance, there is limited research on how to develop responsive and resilient strategies to deal with long-lasting, structural, and global impacts. Previous studies mainly focused on normal situations, in which supply chain integration and big data analytics are the bases of building 2Rs supply chains. Supply chain integration is the ability to integrate all activities among a company's internal functions and external partners (supplier, distributor, retailer, etc.), until the finished product arrives at the end customer (Zhao et al., 2013). From the perspective of collaborative partners, supply chain integration could be divided by horizontal strategies and vertical strategies (Mesquita & Lazzarini, 2008), while the vertical integration is further divided by integration with suppliers (also known as upstream integration) and integration with customers (also known as downstream integration). In the era of Industry 4.0, supply chain integration consists of three dimensions: process and activity integration, technology and system integration, and organizational relationship linkages (Tiwari, 2020). A recent editorial in JOM (Browning, 2020) discussed related aspects of organizational and process integration. In recent years, under the influence of economic globalization, supply chains have been transformed. Since globalization has become pervasive, suppliers have pursued global markets, and most companies source extensively from global suppliers (Cohen & Lee, 2020). This has led to an increase in outsourcing activities and a corresponding decline in vertical integration of supply chains. As a result, supply chain networks have become flatter and more complex, composed of different organizations dispersed across multiple tiers and different geographies, and extended beyond a single country's boundaries (Choi & Hong, 2002). Global supply chains are characterized by focal firms that distribute across multiple countries, locate production facilities abroad, or source from offshore suppliers. Munir et al. (2020) showed that integration in the global supply chain could increase companies' resilience in making flexible deliveries and the number of products. Big data refers to data that arrive at a high volume and velocity with considerable variation, while analytics refers to the ability to gain insights from data via statistics, learning, optimization, or other techniques. The applications of big data and analytics are closely interlinked to enable firms to make better decisions. Hence, prior literature has typically discussed them together as big data analytics, which allows the use of advanced computing techniques, strategies and architectures to store, extract, and analyze multi-source, heterogeneous data to support decisions, and has been commonly used in operations management (Wamba et al., 2015). In the era of economic globalization, supply chain management has become extremely complex, with large-scale and online decision-making challenges emerging (Yang et al., 2021), for example, the joint decision-making between proactive planning and reactive operations in the forms of demand forecasting, production planning, inventory management, supply allocation, transportation, and distribution. It is no longer efficient to rely on traditional analytics methods. Many firms have been exploring how to take big data analytics to promote lean and agile activities in supply chain management (Baruffaldi et al., 2019). Existing studies have shown that applications of digital technologies can help improve supply chain performance by enhancing visibility and reducing supply chain risks (Govindan et al., 2018). The digitalization of supply chains produces large volumes of data, which is regarded as a new kind of resource and has the potential to create value and enhance competitiveness. Singh and El-Kassar (2019) proposed that digital technologies have transformed traditional supply chain management into a more data-driven approach, which requires a much higher level of big data analytics capabilities compared to traditional supply chain management. Following the call for papers, the submission of 114 manuscripts, and the review and revision process, seven articles were selected for this special issue that contribute to our understanding of the impact of COVID-19 on supply chains and its effect on addressing the 2Rs. In \"Strengthening supply chain resilience during COVID-19: A case study of JD.com\" (Shen & Sun, 2023), the authors used quantitative operational data obtained from JD.com4 to analyze the impact of the pandemic on supply chain resilience. They described the challenging scenarios that retailing supply chains experienced in China and the practical response of JD.com over the course of the pandemic the pandemic. JD.com was observed to respond well to the exceptional demand and severe logistical disruptions caused by COVID-19 in China based on its highly integrated supply chain structure (including both process and activity integration and technology and system integration) and comprehensive digital technologies. In particular, the existing, intelligent platforms and delivery procedures were modified slightly but promptly to deal with specific disruptions. The joint efforts of multiple firms, the government, and the entire Chinese society contributed to surmounting the challenges. The experience of JD.com contributes to understanding of the value of investing in operational flexibility and beyond-supply-chain collaboration given the possibility of large-scale supply chain disruptions such as the COVID-19 outbreak. In \"Breaking out of the pandemic: How can firms match internal competence with external resources to shape operational resilience?\" (Li et al., 2023), the authors explored how firms sought to effectively combine internal competence with external resources from the supply chain network to improve operational flexibility and stability during the COVID-19 pandemic. The internal flexibility refers to product diversity, the internal stability refers to operational efficiency, the external flexibility refers to structural holes, and the external stability refers to network centrality. Drawing upon matching theory, the authors provided an internal-external combinative perspective to explain operational mechanisms underlying different matchings. Based on the empirical results of 2994 unique firms and 5293 observations, they found that more heterogeneous combinations between internal (external) flexibility and external (internal) stability may result in a complementary effect that enhances operational resilience, whereas more homogeneous combinations between internal flexibility (or stability) and external flexibility (or stability) may have a substitutive effect that reduces operational resilience. With the COVID-19 pandemic having had a significant impact on supply chains, government initiatives have played a central role in managing the crisis. In \"The impact of governmental COVID-19 measures on manufacturers' stock market valuations: The role of labor intensity and operational slack\" (Chen et al., 2023), the authors investigated the impact of the Chinese government's Level I emergency-response policy (Ge et al., 2020) on manufacturers' stock-market values, and the role of manufacturers' operational slack on adding resilience. Specifically, through an event study of 1357 Chinese manufacturing companies listed on the Shenzhen Stock Exchange, the authors found that the government's emergency-response policy triggered a statistically significant positive reaction from the stock market for manufacturers. However, the authors also found negative impacts on stock market values for manufacturers in labor-intensive industries because of the labor immobility triggered by the Level I measures. In addition, this article identified the positive role of operational slack in the form of financial slack and excess inventory in helping to maintain operations and business continuity, mitigate risks caused by the labor mobility restrictions, and improve supply chain resilience, which identifies operational slack as a supply chain resilience strategy to mitigate pandemic-related risks. When the COVID-19 pandemic broke out, the medical-product industry faced unprecedented demand shocks for personal protective equipment, including face masks, face shields, disinfectants, and gowns. Companies from various industries responded to the urgent need for these potentially life-saving products by adopting ad hoc supply chains in an exceptionally short time. In \"Realizing supply chain agility under time pressure: Ad hoc supply chains during the COVID-19 pandemic\" (M\u00fcller et al., 2023), the authors explored the use by 34 German companies of ad hoc supply chains to produce personal protective equipment. From these cases, the authors developed an emergent theoretical model of ad hoc supply chains around enablers of supply chain agility such as dynamic capabilities (the ability to integrate, build, and reconfigure internal and external competences to address rapidly scenario changes), entrepreneurial orientation (proactiveness, risk-taking, innovativeness, autonomy, and competitive aggressiveness), and temporary orientation (speedy action in a limited time). To cope with the COVID-19 crisis, many firms allowed their employees to work from home (WFH). In \"Working from home and firm resilience to the COVID-19 pandemic\" (Ge et al., 2023), the authors examined whether a firm's WFH capacity increased its resilience. The authors put forward and tested a unique data set that combines listed firms' financial data, epidemiological data, and online job postings data from China. They found that imposing COVID-19 anti-contagion policies on firms and their suppliers or customers significantly increased their operating revenue volatility, slowed their recovery, and had repercussions on their supply chains. WFH enhanced firms' resistance capacity by reducing the effect of COVID-19 on their operating revenue volatility and disruptions to their supply chain partners; however, it also decreased their recovery capacity by extending the time taken to return to normal. Firm attributes, along with workers' occupations, education, and experience, impacted the effect of WFH on firm resilience. This article enhances our understanding of shock transmission across supply..",
                "url": "https://openalex.org/W4365518480",
                "title": "Building responsive and resilient supply chains: Lessons from the <scp>COVID</scp>\u201019 disruption",
                "publication_date": "2023-04-01"
              },
              {
                "id": "E3704507684",
                "text": "..Kamble, S., Gunasekaran, A., Dhone, N.C., 2020b. Industry 4.0 and lean manufacturing practices for sustainable organisational performance in Indian manufacturing companies. Int J Production Research 58 (5), 1319\u20131337. Karkaria, U., 2020. Factories stay on alert for COVID-19; BMW, Toyota, others cope with outbreaks. Automotive News. http://search.ebscohost.com/login.aspx?direct=tru e&db=edsgbe&AN=edsgcl.627622912&site=eds-live (accessed 08.08.2020). Katafuchi, Y., Kurita, K., Managi, S., 2020. COVID-19 with Stigma: theory and Evidence from Mobility Data. Economics of Disasters and Climate Change. https://doi.org/ 10.1007/s41885-020-00077-w. Kochan, C.G., Nowicki, D.R., 2019. Supply chain resilience: a systematic literature review and typological framework. Int J Physical Distribution Logistics Management 48 (8), 842\u2013865. Kotoky, A., Modi, M., Turner, M., 2020. Jobs Are Being Wiped Out at Airlines, And There\u2019s Worse to Come. Bloomberg. http://search.ebscohost.com/login.aspx? direct=true&db=bth&AN=144748168&site=eds-live (accessed 08.08.2020). KPMG, 2020. COVID-19 impact on the automotive sector. https://home.kpmg/xx/en/b logs/home/posts/2020/03/covid-19-impact-on-the-automotive-sector.html (accessed 04.12.2020). Kumar, S., Managi, S., 2020. Economics of Disasters and Climate Change. Process Integration and Optimization for Sustainability 4, 481\u2013502. Lai, C.-C., Shih, T.-P., Ko, W.-C., Tang, H.-J., Hsueh, P.-R., 2020. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and coronavirus disease-2019 (COVID-19): the epidemic and the challenges. Int. J. Antimicrob. Agents 55 (3). LMC, 2020. Impact of COVID-19 on Global Auto Industry: expect deeper decline than during the Great Recession. https://lmc-auto.com/wp-content/uploads/202 0/03/LMCA-Global-LV-Sales-COVID-19-Impact-26-March-2020.pdf (accessed 04.11.2020). L\u00fccker, F., Seifert, R.W., Bi\u00e7er, I., 2019. Roles of inventory and reserve capacity in mitigating supply chain disruption risk. Int J Production Research 57 (4), 1238\u20131249. Martin, A., Markhvida, M., Hallegatte, S., Walsh, B., 2020. Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty. Economics of Disasters and Climate Change 4, 453\u2013479. Martinez, M., 2020. COVID-19 crisis is spurring long-term changes, execs say. Automotive News. http://search.ebscohost.com/login.aspx?direct=true&db=eds gao&AN=edsgcl.621032845&site=eds-live (accessed 07.22.2020). McKinsey, 2020. Covid-19 Briefing Materials. https://www.mckinsey.com /~/media/mckinsey/business%20functions/risk/our%20insights/covid%2019%20 implications%20for%20business/covid%2019%20march%2030/covid-19-facts-an d-insights-april-3.ashx (accessed 04.15.2020). Nakamura, H., Managi, S., 2020a. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. Nakamura, H., Managi, S., 2020b. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. NAM, 2020. Manufacturers\u2019 Survey Reveals Current Industry Impact of COVID-19. https://www.nam.org/manufacturers-survey-reveals-current-industry-impact-of-c ovid-19-7411 (accessed 04.08.2020). OCED, 2020. Coronavirus: the world economy at risk. https://www.oecd.org/berlin/ publikationen/Interim-Economic-Assessment-2-March-2020.pdf (accessed 04.14.2020). Ojha, R., Ghadge, A., Tiwari, M.K., Bititci, U.S., 2018. Bayesian network modelling for supply chain risk propagation. Int J Production Research 56 (17), 5795\u20135819. Patel, T., Horobin, W., 2020. France to Unveil Aid Packages for Auto, Aviation Industries. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=edb&AN=14 3227953&site=eds-live (accessed 08.08.2020). Philip, S., 2020. U.K. Car Industry Joins European Jobs Wipeout to Battle Slump. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=14 3595812&site=eds-live (accessed 08.08.2020). PwC, 2020. PwC\u2019s COVID-19 CFO Pulse Survey. https://www.pwc.com/us/en/li brary/covid-19/pwc-covid-19-cfo-pulse-survey.html (accessed 04.08.2020). Ralston, P., Blackhurst, J., 2020. Industry 4.0 and resilience in the supply chain: a driver of capability enhancement or capability loss? Int J Production Research 58 (16), 5006\u20135019. Roland Berger., 2020. THREE SCENARIOS FOR HOW CORONAVIRUS MAY AFFECT ECONOMIES AND INDUSTRIES. https://www.rolandberger.com/fr/Point-of-Vie w/Three-scenarios-for-how-Coronavirus-may-affect-economies-and-industries.html (accessed 04.14.2020). Rose, A., 2004. In: Springer (Ed.). Modeling Spatial and Economic Impacts of Disasters, pp. 13\u201336. Rose, A., Avetisyan, M., Rosoff, H., Burns, W.J., Slovic, P., Chan, O., 2017. The role of behavioral responses in the total economic consequences of terrorist attacks on US air travel targets. Risk analysis 37 (7), 1403\u20131418. Ro\u00dfmann, B., Canzaniello, A., Gracht, H.v., Hartmann, E., 2018. The future and social impact of Big Data Analytics in Supply Chain Management: results from a Delphi study. Technol Forecast Soc Change 130, 135\u2013149. Santos, J.R., May, L., ElHaimar, A., 2012. Risk-Based input-output analysis of influenza epidemic consequences on interdependent workforce sectors. Risk Analysis 33 (9), 1620\u20131635. Sawik, T., 2019. Disruption mitigation and recovery in supply chains using portfolio approach. Omega (Westport) 84, 232\u2013248. Schmitt, A.J., Singh, M., 2012. A quantitative analysis of disruption risk in a multiechelon supply chain. International Journal of Production Economics 139 (1), 22\u201332. Sharifi, H., Kehoe, D., Hopkins, J., 2006. A classification and selection model of emarketplaces for better alignment of supply chains. Journal of Enterprise Information Management 19 (5), 483\u2013503. Sharma, R., Shishodia, A., Kamble, S., Gunasekaran, A., Belhadi, A., 2020. Agriculture supply chain risks and COVID-19: mitigation strategies and implications for the practitioners. Int J Logistics Research and Applications 1\u201327. https://doi.org/ 10.1080/13675567.2020.1830049. Simchi-Levi, D., Schmidt, W., Wei, Y., Zhang, P.Y., Combs, K., Ge, Y., Gusikhin, O., Sanders, M., Zhang, D., 2015. Identifying risks and mitigating disruptions in the automotive supply chain. Interfaces (Providence) 45 (5), 375\u2013390. Sobieralski, J.B., 2020. COVID-19 and airline employment: insights from historical uncertainty shocks to the industry. Transportation Research Interdisciplinary Perspectives 5. Strauss, A., Corbin, J., 1990. Basics of Qualitative Research. Sage publications. Tan, W.J., Cai, W., Zhang, A.N., 2019. Structural-..",
                "url": "https://openalex.org/W3095984860",
                "title": "Manufacturing and service supply chain resilience to the COVID-19 outbreak: Lessons learned from the automobile and airline industries",
                "publication_date": "2021-02-01"
              }
            ]
          },
          {
            "id": "S7852842407",
            "text": "The application of big data analytics in healthcare supply chains during COVID-19 has led to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions.",
            "children": [
              {
                "id": "E6357881701",
                "text": "..of giant techs is needed because researchers, doctors and scientists can be effectively supported to expedite the research and development of COVID-19 virus. 130822 VOLUME 8, 2020 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts Recently, IBM announced that they are now providing a cloud-based research resource that has been trained on a COVID-19 open dataset (CORD-19) [16], which is a collection of research articles related to COVID-19. Moreover, IBM has adopted their proposed AI technology for drug discovery, from which 3000 novel COVID-19 molecules have been obtained, officially reported in [17]. Another support is from the White House Office of Science and Technology Policy, the U.S. Department of Energy and IBM with the development of COVID-19 HPC Consortium (https://covid19- hpc-consortium.org/), which is open for research proposals concerning COVID-19. Another example is the Coronavirus International Research Team (COV-IRT) (https://www.covirt.org/), a group of scientists who are developing vaccines and therapeutic solutions against COVID-19. C. BIG DATA 1) DEFINITION AND CHARACTERISTICS The rapid development of the Internet of Things (IoT) results in a massive explosion of data generated from ubiquitous wearable devices and sensors [18]. The unprecedented increase of data volumes associated with advances of analytic techniques empowered from AI has led to the emergence of a big data era [18], [19]. Big data has been employed in a wide range of industrial application domains, including healthcare where electronic healthcare records (EHRs) are exploited by using intelligent analytics for facilitating medical services. For example, health big data potentially supports patient health analysis, diagnosis assistance, and drug manufacturing [20]. Big data can be generated from a number of sources which may include online social graphs, mobile devices (i.e. smartphones), IoT devices (i.e. sensors), and public data [21] in various formats such as text or video. In the context of COVID-19, big data refers to the patient care data such as physician notes, X-Ray reports, case history, list of doctors and nurses, and information of outbreak areas. In general, big data is the information asset characterized by such a high volume, velocity and variety to acquire specific technology and analytical methods for its transformation into useful information to serve the end users, i.e, big data in digital twin technologies [22]\u2013[25]. The three characteristics of big data are summarized as follows. \u2022 Volume: This feature shows the huge amount of data that can range from terabytes to exabytes. According to a Cisco\u2019s forecast, the data traffic is expected to reach 930 exabytes by 2020, a seven-fold growth from 2017 [26]. \u2022 Variety: It refers to the diversity and heterogeneity of big data. For example, big data in healthcare can be produced from healthcare users (i.e. doctors, patients), medical IoT devices, and healthcare organizations. Data can be formatted in text, images, videos with structured or un-structured dataset types [27]. \u2022 Velocity: It expresses the data generation rate that can be calculated in time or frequency domain. In fact, in industrial applications like healthcare, data generated from devices is always updated in real-time, which is of significant importance for time-sensitive applications such as health monitoring or diagnosis [28]. 2) BIG DATA FOR COVID-19 FIGHTING Big data has been proved its capability to support fighting infectious diseases like COVID-19 [29], [30]. Big data potentially provide a number of promising solutions to help combat COVID-19 epidemic. By combining with AI analytics, big data helps us to understand the COVID-19 in terms of outbreak tracking, virus structure, disease treatment, and vaccine manufacturing [31]. For example, big data associated with intelligent AI-based tools can build complex simulation models using coronavirus data streams for outbreak estimation. This would aid health agencies in monitoring the coronavirus spread and preparing better preventive measurements [32]. Models from big data also supports future prediction of COVID-19 epidemic by its data aggregation ability to leverage large amounts of data for early detection. Moreover, the big data analytics from a variety of real-world sources including infected patients can help implement large-scale COVID-19 investigations to develop comprehensive treatment solutions with high reliability [33], [34]. This would also help healthcare providers to understand the virus development for better response to the various treatment and diagnoses. Based on the above analysis, we want to highlight that big data analytics is the process of collecting and analyzing the large volume of data sets to discover useful hidden patterns and other information, e.g., COVID-19 data discovering. Moreover, AI (and explainable AI [35]) aims to apply logic and reasoning to build human intelligence that can mimic the function of a machine for learning, classifying, and estimating possible outcomes, e.g., COVID-19 symptom classifications. The potential applications of each technology in fighting COVID-19 pandemic will be explained and discussed in the following sections through a number of practical use cases. III. APPLICATIONS OF AI FOR FIGHTING COVID-19 This section presents representative applications of AI in fighting the COVID-19 outbreak. A. AI FOR COVID-19 DETECTION AND DIAGNOSIS As one of the most effective solutions to combat the COVID-19 pandemic, early treatment and prediction are of importance. Currently, the standard method for classifying respiratory viruses is the reverse transcription polymerase chain reaction (RT-PCR) detection technique. In response to the COVID-19 virus, some efforts have been dedicated to improve this technique [36] and for other alternatives [37]. These techniques are, however, usually costly and time-consuming, have low true positive rate, and require specific materials, equipment and instruments. Moreover, most countries are suffering from a lack of testing kits due to the limitation on budget and techniques. Thus, VOLUME 8, 2020 130823 Q.-V. Pham et al.: AI and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts FIGURE 2. An illustration of DL-based frameworks for the COVID-19 detection and diagnosis. the standard method is not suitable to meet the requirements of fast detection and tracking during the COVID-19 pandemic. A simple and low-cost solution for COVID-19 identification is using smart devices together with AI frameworks [38], [39]. This is referred to as mobile health or mHealth in the literature [40]. These works are advantageous since smart devices are daily used for multi-purposes. Moreover, the emergence of cloud and edge computing can effectively overcome the limitation of batter, storage, and computing capabilities [41]. Another directive for COVID-19 detection is to use AI techniques for medical image processing, which recently appeared in many research works on coronavirus [42]\u2013[51]. As we limit this paper to the COVID-19 virus, the interested readers are invited to read the surveys in [52], [53] for other applications of DL in medical image analysis. It is noted from these works that X-ray images and computed tomography (CT) scans are widely used as the input of DL models so as to automatically detect the infected COVID-19 case. Motivated by an important finding that infected COVID-19 patients normally reveal abnormalities in chest radiography images, the authors in [54] designed a deep convolutional neural network (CNN) model for the detection of COVID-19 cases. The three-class classification (normal, COVID-19 infected, and non-COVID-19 infected) in this work is helpful if the medical staff needs to decide which cases should be tested with standard methods (between normal and COVID-19 infected cases), and which treatment strategies should be taken (between non- and COVID-19 infected cases). By training on an open source dataset with 13, 975 images of 13, 870 patients, the proposed CNN model can achieve an accuracy of 93.3%. The use of ML and DL techniques with chest CT scans for COVID-19 detection were considered in [42], [43], [55]\u2013[58], respectively. These works show a high performance as they can achieve a high classification accuracy, e.g., 99.68% in [42], an area under curve (AUC) score of 0.994 in [43], AUC of 0.996 in [55], and accuracy of 82.9% (98.27%) with specificity of 80.5% (97.60%) and sensitivity of 84% (98.93%) in [56] and [57]. As the cost of X-ray scans is usually cheaper than CT images, a large portion of research works for COVID-19 detection utilize DL models with X-ray images. For example, the work in [46] leveraged a deep CNN model, called Decompose, Transfer, and Compose (DeTraC), to process chest X-ray images for the classification of COVID-19. The main purpose of the decomposition layer is to reduce the feature space, thus yielding more sub-classes but improving the training efficiency, whereas the composition layer is to combine sub-classes from the previous layer so as to produce the final classification result. Besides the decomposition and composition layers, a transfer layer is positioned in the middle to speed up the training time, reduce the computation costs, and make the DL model trainable with small datasets. The importance of transfer learning made it a wide consideration in many works, e.g., [45], [48], [49]..",
                "url": "https://openalex.org/W3019283372",
                "title": "Artificial Intelligence (AI) and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts",
                "publication_date": "2020-01-01"
              },
              {
                "id": "S0323885390",
                "text": "The application of big data analytics in healthcare supply chains during COVID-19 has been linked to improved visibility and risk management, allowing organizations to better anticipate and mitigate disruptions.",
                "children": [
                  {
                    "id": "E3155705893",
                    "text": "..against damage or loss, as well as the ability to rapidly recover from a disruption (Hora & Klassen, 2013). Thus, resilience is dynamic instead of static, which is considered as a fundamental attribute that supply chains need to adopt for maintaining stable growth in the face of external disruptions (Essuman et al., 2020) such as the COVID-19 pandemic. Supply chain resilience consists of multiple constituent elements, including stability, agility, robustness, collaboration, redundancy, centralization, visibility, and information sharing (Hosseini et al., 2019). Tukamuhabwa et al. (2015) emphasized the importance of building collaborative relationships in improving supply chain resilience. Other measures include maintaining slack resources, adopting a flexible production strategy, and building a risk-management infrastructure (Ambulkar et al., 2015; Modi & Mishra, 2011). Although 2Rs are the key attributes for enterprises to improve supply chain performance, there is limited research on how to develop responsive and resilient strategies to deal with long-lasting, structural, and global impacts. Previous studies mainly focused on normal situations, in which supply chain integration and big data analytics are the bases of building 2Rs supply chains. Supply chain integration is the ability to integrate all activities among a company's internal functions and external partners (supplier, distributor, retailer, etc.), until the finished product arrives at the end customer (Zhao et al., 2013). From the perspective of collaborative partners, supply chain integration could be divided by horizontal strategies and vertical strategies (Mesquita & Lazzarini, 2008), while the vertical integration is further divided by integration with suppliers (also known as upstream integration) and integration with customers (also known as downstream integration). In the era of Industry 4.0, supply chain integration consists of three dimensions: process and activity integration, technology and system integration, and organizational relationship linkages (Tiwari, 2020). A recent editorial in JOM (Browning, 2020) discussed related aspects of organizational and process integration. In recent years, under the influence of economic globalization, supply chains have been transformed. Since globalization has become pervasive, suppliers have pursued global markets, and most companies source extensively from global suppliers (Cohen & Lee, 2020). This has led to an increase in outsourcing activities and a corresponding decline in vertical integration of supply chains. As a result, supply chain networks have become flatter and more complex, composed of different organizations dispersed across multiple tiers and different geographies, and extended beyond a single country's boundaries (Choi & Hong, 2002). Global supply chains are characterized by focal firms that distribute across multiple countries, locate production facilities abroad, or source from offshore suppliers. Munir et al. (2020) showed that integration in the global supply chain could increase companies' resilience in making flexible deliveries and the number of products. Big data refers to data that arrive at a high volume and velocity with considerable variation, while analytics refers to the ability to gain insights from data via statistics, learning, optimization, or other techniques. The applications of big data and analytics are closely interlinked to enable firms to make better decisions. Hence, prior literature has typically discussed them together as big data analytics, which allows the use of advanced computing techniques, strategies and architectures to store, extract, and analyze multi-source, heterogeneous data to support decisions, and has been commonly used in operations management (Wamba et al., 2015). In the era of economic globalization, supply chain management has become extremely complex, with large-scale and online decision-making challenges emerging (Yang et al., 2021), for example, the joint decision-making between proactive planning and reactive operations in the forms of demand forecasting, production planning, inventory management, supply allocation, transportation, and distribution. It is no longer efficient to rely on traditional analytics methods. Many firms have been exploring how to take big data analytics to promote lean and agile activities in supply chain management (Baruffaldi et al., 2019). Existing studies have shown that applications of digital technologies can help improve supply chain performance by enhancing visibility and reducing supply chain risks (Govindan et al., 2018). The digitalization of supply chains produces large volumes of data, which is regarded as a new kind of resource and has the potential to create value and enhance competitiveness. Singh and El-Kassar (2019) proposed that digital technologies have transformed traditional supply chain management into a more data-driven approach, which requires a much higher level of big data analytics capabilities compared to traditional supply chain management. Following the call for papers, the submission of 114 manuscripts, and the review and revision process, seven articles were selected for this special issue that contribute to our understanding of the impact of COVID-19 on supply chains and its effect on addressing the 2Rs. In \"Strengthening supply chain resilience during COVID-19: A case study of JD.com\" (Shen & Sun, 2023), the authors used quantitative operational data obtained from JD.com4 to analyze the impact of the pandemic on supply chain resilience. They described the challenging scenarios that retailing supply chains experienced in China and the practical response of JD.com over the course of the pandemic the pandemic. JD.com was observed to respond well to the exceptional demand and severe logistical disruptions caused by COVID-19 in China based on its highly integrated supply chain structure (including both process and activity integration and technology and system integration) and comprehensive digital technologies. In particular, the existing, intelligent platforms and delivery procedures were modified slightly but promptly to deal with specific disruptions. The joint efforts of multiple firms, the government, and the entire Chinese society contributed to surmounting the challenges. The experience of JD.com contributes to understanding of the value of investing in operational flexibility and beyond-supply-chain collaboration given the possibility of large-scale supply chain disruptions such as the COVID-19 outbreak. In \"Breaking out of the pandemic: How can firms match internal competence with external resources to shape operational resilience?\" (Li et al., 2023), the authors explored how firms sought to effectively combine internal competence with external resources from the supply chain network to improve operational flexibility and stability during the COVID-19 pandemic. The internal flexibility refers to product diversity, the internal stability refers to operational efficiency, the external flexibility refers to structural holes, and the external stability refers to network centrality. Drawing upon matching theory, the authors provided an internal-external combinative perspective to explain operational mechanisms underlying different matchings. Based on the empirical results of 2994 unique firms and 5293 observations, they found that more heterogeneous combinations between internal (external) flexibility and external (internal) stability may result in a complementary effect that enhances operational resilience, whereas more homogeneous combinations between internal flexibility (or stability) and external flexibility (or stability) may have a substitutive effect that reduces operational resilience. With the COVID-19 pandemic having had a significant impact on supply chains, government initiatives have played a central role in managing the crisis. In \"The impact of governmental COVID-19 measures on manufacturers' stock market valuations: The role of labor intensity and operational slack\" (Chen et al., 2023), the authors investigated the impact of the Chinese government's Level I emergency-response policy (Ge et al., 2020) on manufacturers' stock-market values, and the role of manufacturers' operational slack on adding resilience. Specifically, through an event study of 1357 Chinese manufacturing companies listed on the Shenzhen Stock Exchange, the authors found that the government's emergency-response policy triggered a statistically significant positive reaction from the stock market for manufacturers. However, the authors also found negative impacts on stock market values for manufacturers in labor-intensive industries because of the labor immobility triggered by the Level I measures. In addition, this article identified the positive role of operational slack in the form of financial slack and excess inventory in helping to maintain operations and business continuity, mitigate risks caused by the labor mobility restrictions, and improve supply chain resilience, which identifies operational slack as a supply chain resilience strategy to mitigate pandemic-related risks. When the COVID-19 pandemic broke out, the medical-product industry faced unprecedented demand shocks for personal protective equipment, including face masks, face shields, disinfectants, and gowns. Companies from various industries responded to the urgent need for these potentially life-saving products by adopting ad hoc supply chains in an exceptionally short time. In \"Realizing supply chain agility under time pressure: Ad hoc supply chains during the COVID-19 pandemic\" (M\u00fcller et al., 2023), the authors explored the use by 34 German companies of ad hoc supply chains to produce personal protective equipment. From these cases, the authors developed an emergent theoretical model of ad hoc supply chains around enablers of supply chain agility such as dynamic capabilities (the ability to integrate, build, and reconfigure internal and external competences to address rapidly scenario changes), entrepreneurial orientation (proactiveness, risk-taking, innovativeness, autonomy, and competitive aggressiveness), and temporary orientation (speedy action in a limited time). To cope with the COVID-19 crisis, many firms allowed their employees to work from home (WFH). In \"Working from home and firm resilience to the COVID-19 pandemic\" (Ge et al., 2023), the authors examined whether a firm's WFH capacity increased its resilience. The authors put forward and tested a unique data set that combines listed firms' financial data, epidemiological data, and online job postings data from China. They found that imposing COVID-19 anti-contagion policies on firms and their suppliers or customers significantly increased their operating revenue volatility, slowed their recovery, and had repercussions on their supply chains. WFH enhanced firms' resistance capacity by reducing the effect of COVID-19 on their operating revenue volatility and disruptions to their supply chain partners; however, it also decreased their recovery capacity by extending the time taken to return to normal. Firm attributes, along with workers' occupations, education, and experience, impacted the effect of WFH on firm resilience. This article enhances our understanding of shock transmission across supply..",
                    "url": "https://openalex.org/W4365518480",
                    "title": "Building responsive and resilient supply chains: Lessons from the <scp>COVID</scp>\u201019 disruption",
                    "publication_date": "2023-04-01"
                  },
                  {
                    "id": "E3704507684",
                    "text": "..Kamble, S., Gunasekaran, A., Dhone, N.C., 2020b. Industry 4.0 and lean manufacturing practices for sustainable organisational performance in Indian manufacturing companies. Int J Production Research 58 (5), 1319\u20131337. Karkaria, U., 2020. Factories stay on alert for COVID-19; BMW, Toyota, others cope with outbreaks. Automotive News. http://search.ebscohost.com/login.aspx?direct=tru e&db=edsgbe&AN=edsgcl.627622912&site=eds-live (accessed 08.08.2020). Katafuchi, Y., Kurita, K., Managi, S., 2020. COVID-19 with Stigma: theory and Evidence from Mobility Data. Economics of Disasters and Climate Change. https://doi.org/ 10.1007/s41885-020-00077-w. Kochan, C.G., Nowicki, D.R., 2019. Supply chain resilience: a systematic literature review and typological framework. Int J Physical Distribution Logistics Management 48 (8), 842\u2013865. Kotoky, A., Modi, M., Turner, M., 2020. Jobs Are Being Wiped Out at Airlines, And There\u2019s Worse to Come. Bloomberg. http://search.ebscohost.com/login.aspx? direct=true&db=bth&AN=144748168&site=eds-live (accessed 08.08.2020). KPMG, 2020. COVID-19 impact on the automotive sector. https://home.kpmg/xx/en/b logs/home/posts/2020/03/covid-19-impact-on-the-automotive-sector.html (accessed 04.12.2020). Kumar, S., Managi, S., 2020. Economics of Disasters and Climate Change. Process Integration and Optimization for Sustainability 4, 481\u2013502. Lai, C.-C., Shih, T.-P., Ko, W.-C., Tang, H.-J., Hsueh, P.-R., 2020. Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and coronavirus disease-2019 (COVID-19): the epidemic and the challenges. Int. J. Antimicrob. Agents 55 (3). LMC, 2020. Impact of COVID-19 on Global Auto Industry: expect deeper decline than during the Great Recession. https://lmc-auto.com/wp-content/uploads/202 0/03/LMCA-Global-LV-Sales-COVID-19-Impact-26-March-2020.pdf (accessed 04.11.2020). L\u00fccker, F., Seifert, R.W., Bi\u00e7er, I., 2019. Roles of inventory and reserve capacity in mitigating supply chain disruption risk. Int J Production Research 57 (4), 1238\u20131249. Martin, A., Markhvida, M., Hallegatte, S., Walsh, B., 2020. Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty. Economics of Disasters and Climate Change 4, 453\u2013479. Martinez, M., 2020. COVID-19 crisis is spurring long-term changes, execs say. Automotive News. http://search.ebscohost.com/login.aspx?direct=true&db=eds gao&AN=edsgcl.621032845&site=eds-live (accessed 07.22.2020). McKinsey, 2020. Covid-19 Briefing Materials. https://www.mckinsey.com /~/media/mckinsey/business%20functions/risk/our%20insights/covid%2019%20 implications%20for%20business/covid%2019%20march%2030/covid-19-facts-an d-insights-april-3.ashx (accessed 04.15.2020). Nakamura, H., Managi, S., 2020a. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. Nakamura, H., Managi, S., 2020b. Airport risk of importation and exportation of the COVID-19 pandemic. Transp Policy (Oxf) 96, 40\u201347. NAM, 2020. Manufacturers\u2019 Survey Reveals Current Industry Impact of COVID-19. https://www.nam.org/manufacturers-survey-reveals-current-industry-impact-of-c ovid-19-7411 (accessed 04.08.2020). OCED, 2020. Coronavirus: the world economy at risk. https://www.oecd.org/berlin/ publikationen/Interim-Economic-Assessment-2-March-2020.pdf (accessed 04.14.2020). Ojha, R., Ghadge, A., Tiwari, M.K., Bititci, U.S., 2018. Bayesian network modelling for supply chain risk propagation. Int J Production Research 56 (17), 5795\u20135819. Patel, T., Horobin, W., 2020. France to Unveil Aid Packages for Auto, Aviation Industries. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=edb&AN=14 3227953&site=eds-live (accessed 08.08.2020). Philip, S., 2020. U.K. Car Industry Joins European Jobs Wipeout to Battle Slump. Bloomberg. http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=14 3595812&site=eds-live (accessed 08.08.2020). PwC, 2020. PwC\u2019s COVID-19 CFO Pulse Survey. https://www.pwc.com/us/en/li brary/covid-19/pwc-covid-19-cfo-pulse-survey.html (accessed 04.08.2020). Ralston, P., Blackhurst, J., 2020. Industry 4.0 and resilience in the supply chain: a driver of capability enhancement or capability loss? Int J Production Research 58 (16), 5006\u20135019. Roland Berger., 2020. THREE SCENARIOS FOR HOW CORONAVIRUS MAY AFFECT ECONOMIES AND INDUSTRIES. https://www.rolandberger.com/fr/Point-of-Vie w/Three-scenarios-for-how-Coronavirus-may-affect-economies-and-industries.html (accessed 04.14.2020). Rose, A., 2004. In: Springer (Ed.). Modeling Spatial and Economic Impacts of Disasters, pp. 13\u201336. Rose, A., Avetisyan, M., Rosoff, H., Burns, W.J., Slovic, P., Chan, O., 2017. The role of behavioral responses in the total economic consequences of terrorist attacks on US air travel targets. Risk analysis 37 (7), 1403\u20131418. Ro\u00dfmann, B., Canzaniello, A., Gracht, H.v., Hartmann, E., 2018. The future and social impact of Big Data Analytics in Supply Chain Management: results from a Delphi study. Technol Forecast Soc Change 130, 135\u2013149. Santos, J.R., May, L., ElHaimar, A., 2012. Risk-Based input-output analysis of influenza epidemic consequences on interdependent workforce sectors. Risk Analysis 33 (9), 1620\u20131635. Sawik, T., 2019. Disruption mitigation and recovery in supply chains using portfolio approach. Omega (Westport) 84, 232\u2013248. Schmitt, A.J., Singh, M., 2012. A quantitative analysis of disruption risk in a multiechelon supply chain. International Journal of Production Economics 139 (1), 22\u201332. Sharifi, H., Kehoe, D., Hopkins, J., 2006. A classification and selection model of emarketplaces for better alignment of supply chains. Journal of Enterprise Information Management 19 (5), 483\u2013503. Sharma, R., Shishodia, A., Kamble, S., Gunasekaran, A., Belhadi, A., 2020. Agriculture supply chain risks and COVID-19: mitigation strategies and implications for the practitioners. Int J Logistics Research and Applications 1\u201327. https://doi.org/ 10.1080/13675567.2020.1830049. Simchi-Levi, D., Schmidt, W., Wei, Y., Zhang, P.Y., Combs, K., Ge, Y., Gusikhin, O., Sanders, M., Zhang, D., 2015. Identifying risks and mitigating disruptions in the automotive supply chain. Interfaces (Providence) 45 (5), 375\u2013390. Sobieralski, J.B., 2020. COVID-19 and airline employment: insights from historical uncertainty shocks to the industry. Transportation Research Interdisciplinary Perspectives 5. Strauss, A., Corbin, J., 1990. Basics of Qualitative Research. Sage publications. Tan, W.J., Cai, W., Zhang, A.N., 2019. Structural-..",
                    "url": "https://openalex.org/W3095984860",
                    "title": "Manufacturing and service supply chain resilience to the COVID-19 outbreak: Lessons learned from the automobile and airline industries",
                    "publication_date": "2021-02-01"
                  }
                ]
              }
            ]
          }
        ]
      },
      "S6842133324": {
        "id": "S6842133324",
        "text": "Theoretical frameworks such as the resource-based view and dynamic capabilities view support the integration of big data analytics and innovation leadership in enhancing healthcare supply chain resilience.",
        "children": [
          {
            "id": "E4154459855",
            "text": "..) Akter et al. (2016) Horng et al. (2022) \u201cBig data analytics capabilities refer to obtaining knowledge from internal or external partners and gaining market insight through big data tools.\u201d (p. 24) Knowledge-based dynamic capability view Germann et al. (2013) Ciasullo et al. (2022) \u201cBDA capability refers to a company's management ability, that is, the ongoing deployment of big data resources at the strategic aims to create value and develop a competitive advantage for the firm.\u201d (p. 205) Wamba et al. (2017) Zhu et al. (2022) \u201c...the ability of an organization to integrate, build, and reconfigure the information resources, as well as business processes, to address rapidly changing environments.\u201d (p. 5) Dynamic capabilities view Supply chain management Lee and Kang (2015) Arias-P \u0301erez et al. (2022) \u201cBDAC is defined as the ability of a firm to capture and analyze big data toward the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 2) Knowledge-based view Henao-Garc\u00eda et al. (2021) Olabode et al. (2022) \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 1219) Knowledge-based view and contingency theory Mikalef et al. (2020b) Bhatti et al. (2022a) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions to gain actionable insights, create business value, and establish competitive advantage\u201d (p. 4) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Bhatti et al. (2022b) \u201ca holistic process that involves the collection, analysis, use, and interpretation of data for various functional divisions with a view to gaining actionable insights, creating business value, and establishing competitive advantage.\u201d (p. 5) Resource-based view, dynamic capability view Supply chain management Wamba et al. (2020b) Song et al. (2022) \u201cBDAC is defined as the ability to develop business insight by using data management, technical foundations and talents.\u201d (p. 1168) Kiron et al. (2014) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 15 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies *Munir et al. (2023) \u201cit can be defined as the organization's capacity to provide insight into the use of data management, infrastructure and human capabilities to increase the competitiveness of the business.\u201d (p. 5) Resource-based view, process-oriented dynamic capability view, sociomateriality theory Kiron et al. (2014), Akter et al. (2016) Cetindamar et al. (2022) \u201cthe ability of an organization to integrate, build, and reconfigure the information resources, as well as business processes, to address rapidly changing environments.\u201d (p. 4) Supply chain management Olszak (2014) Appendix C. Typologies of BDACs proposed and studied in existing literature Documents Typologies of BDACs Cetindamar et al. (2022) BDA human, non-human and infrastructure capabilities Qaffas et al. (2022) Big data analytics management capability Park and Singh (2022) Infrastructure, human capital, knowledge management capability Chatterjee et al. (2022) Personalization and real-time analytics Liu et al. (2022) Big data technical capability and big data managerial capability Demir et al. (2022) Big data collection, processing, analysis and processing, transformation capability Ashaari et al. (2021) BDA technological, organizational, and people capabilities AlNuaimi et al. (2021) Technological and human capabilities Shamim et al. (2021) BDA management capabilities Nisar et al. (2022), Muhammad et al. (2021) BDA management capabilities, technical capabilities, and talent capabilities Uddin Murad et al. (2022), Razaghi and Shokouhyar (2021), Rialti et al. (2020), *Edwin Cheng et al. (2022), Rialti et al. (2019) BDA infrastructure flexibility, BDA management capabilities, BDA personnel expertise capability Bag et al. (2021) Data creation capabilities, data integration and management capabilities, advanced analytics capabilities, data visualization capabilities, and a data driven culture Zhang and Lv (2021) Big data system capabilities, big data human capabilities, and big data management capabilities. Bag et al. (2020) BDA management and talent capability Yasmin et al. (2020) Infrastructure, human resource, management capabilities Belhadi et al. (2020) Organizational (i.e., BDA management), physical (i.e., IT infrastructure), and human (e.g., analytics skill or knowledge) capabilities Wamba et al. (2020), Wamba et al. (2017) BDA management, infrastructure and personnel capability Xiao et al. (2020) BDA technology capabilities and BDA personnel capabilities Ferraris et al. (2019), Song et al. (2022) BDA technology and BDA management Munodawafa and Johl (2019) IT capability, personnel expertise capability, and management capability Wang et al. (2019) Data integration capability, analytical capability, data interpretation capability, predictive analytics, analytical personnel's technical and business skills Mandal (2019) Only focus on BDA management capabilities, which further refers to BDA planning, BDA investment decision making, BDA coordination, BDA control Popovi\u02c7c et al. (2018) Data provisioning, analytical capabilities and people skills Appendix D. Antecedents of BDACs identified from the literature Documents Antecedents Zhu et al. (2022) Sustainable supply chain management practices Horng et al. (2022) Offensive and defensive data strategies, big data knowledge management Khan and Tao (2022) Knowledge absorption capacity Behl (2022) Managerial and technical skills Demir et al. (2022) Big data utilization, knowledge sharing of big data Elia et al. (2022), Lozada et al. (2019), Mikalef et al. (2018), Mikalef et al. (2019b), Gupta and George (2016) Tangible resources, human skills, intangible resources AlNuaimi et al. (2021) Managerial experience and employee skills (BDA human capabilities); data availability and technological infrastructures (BDA technology capabilities); e-procurements Shamim et al. (2021) Strategic and operational big data management capabilities Henao-Garc\u00eda et al. (2021) Tangibles, intangibles, human resources, management innovation Yu et al. (2022) Data driven culture, digital technology orientation and their interaction Dubey et al. (2021) Alliance management capability Chen and Chen (2022) Intellectual capital Shamim et al. (2020) Big data contractual governance, big data relational governance Jha et al. (2020) Data management, advanced software, human resource and training, organizational politics, global integration, environmental determinism Ramadan et al. (2020) Data availability Yasmin et al. (2020) Management, human resources and infrastructure capability Upadhyay and Kumar (2020) Internal analytics knowledge, organizational culture Srimarut and Mekhum (2020) Supply chain connectivity Corte-Real \u02c6 et al. (2020) Data quality Lin and Kunnathur (2019) Developmental culture, customer orientation, technology orientation, entrepreneurial orientation Ferraris et al. (2019) BDA technological and management (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 16 (continued ) Documents Antecedents Singh and Singh (2019) Institutional response to supply chain disruption, IT infrastructure Mikalef et al. (2019a) Data, process, technology, organization, people, context Anwar et al. (2018) Technical capabilities: compatibility, modularity, connectivity; Personnel capabilities: technical knowledge, business knowledge, technical management knowledge Popovi\u02c7c et al. (2018) Data provisioning, analytical capabilities, people skills Wang et al. (2018b) Traceability, analytical capability, decision support capability, predictive capability Srinivasan and Swink (2018) Supply chain visibility Wamba et al. (2017) BDA infrastructure flexibility, management capabilities, personnel expertise capabilities Wang and Hajli (2017) Data aggregation, data processing, data visualization, big data architectural components Akter et al. (2016) BDA management capability, technology, and talent capability Appendix E. Mediating factors in BDACs research Documents Mediators Link Behl et al. (2022) Supply chain coordination and swift trust BDACs and supply chain risk *Munir et al. (2023) Process-oriented dynamic capabilities BDACs and organizational innovation performance Qaffas et al. (2022) Business intelligence infrastructure BDACs and financial/marketing performance Song et al. (2022) Infrastructure and value attribute of business model BDACs and growth/financial performance Park and Singh (2022) Upstream supply chain management IT infrastructure, downstream supply chain inventory management IT utilization BDACs and automated supply chain disruption risk alert tool Bhatti et al. (2022b) Supply chain agility and adaptability BDACs and supply chain innovation Nisar et al. (2022) Innovative green product development and supply chain risk management BDACs and innovation & learning performance Al-Khatib and Valeri (2022) BDACs as a mediator Service innovation and competitive advantage Dubey et al. (2022) Humanitarian supply chain agility and resilience BDACs and humanitarian supply chain performance Bhatti et al. (2022a) Digital platform and network capabilities BDACs and supply chain innovation, BDACs and firm performance Al-Khatib (2023) Green incremental and radical supply chain innovation BDACs and green supply chain performance Olabode et al. (2022)..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          },
          {
            "id": "E0223838797",
            "text": ".., and assume society will be mobilised to engender positive, transformative change in the short and medium term. We recognize of course this optimism may be misplaced. In the case of business-as-usual, in the medium and long term, there will also be transformative, systemic change affecting whole sectors, governance, value networks etc. in dramatic ways, perhaps to the advantage of the few, but definitely to the detriment of wider society. References Backstrand, \u0308 J., Halldorsson, \u0301 A., \u0301 2019. Engaged scholar(ship) in purchasing and supply management (PSM): creative tension or squeezed in the middle? J. Purch. Supply Manag. 25 (4), 100557. Banerjee, S.B., 2003. Who sustains whose development? Sustainable development and the reinvention of nature. Organ. Stud. 24 (1), 143\u2013180. Banerjee, S.B., 2008. Corporate social responsibility: the good, the bad and the ugly. Crit. Sociol. 34 (1), 51\u201379. Banerjee, S.B., 2010. Governing the global corporation: a critical perspective. Bus. Ethics Q. 20 (2), 265\u2013274. Bansal, P., Roth, K., 2000. Why companies go green: a model of ecological responsiveness. Acad. Manag. J. 43 (4), 717\u2013736. Bapuji, H., Husted, B.W., Lu, J., Mir, R., 2018. Value creation, appropriation, and distribution: how firms contribute to societal economic inequality. Bus. Soc. 57 (6), 983\u20131009. Baron, R.A., 2006. Opportunity recognition as pattern recognition: how entrepreneurs \u201cconnect the dots\u201d to identify new business opportunities. Acad. Manag. Perspect. 20 (1), 104\u2013119. Boer, H., Holweg, M., Kilduff, M., Pagell, M., Schmenner, R., Voss, C.A., 2015. Making a meaningful contribution to theory. Int. J. Oper. Prod. Manag. 35 (9), 1231\u20131252. Bruce, J., de Figueiredo, J., Silverman, B., 2019. Public contracting for private innovation: government capabilities, decision rights, and performance outcomes. Strat. Manag. J. 40 (4), 533\u2013555. Bryce, E., 2021. Are clothes made from recycled materials really more sustainable? The Guardian. Available at. https://www.theguardian.com/environment/2021/nov/06/ clothes-made-from-recycled-materials-sustainable-plastic-climate. Carney, M., 2021. Value(s): Building a Better World for All. William Collins, London. Carnovale, S., DuHadway, S., 2021. Continuity in the face of disruptions: purchasing and supply management research\u2019s persistence amidst COVID-19. J. Purch. Supply Manag. 27, 100720. Carter, C.R., Kaufmann, L., Ketchen, D.J., 2020. Expect the unexpected: toward a theory of the unintended consequences of sustainable supply chain management. Int. J. Oper. Prod. Manag. 40 (12), 1857\u20131871. Choi, T.Y., Rogers, D., Vakil, B., 2020. Coronavirus is a wake-up call for supply chain management. Harv. Bus. Rev. 27, 364, 0017-8012. Choi, T.Y., Krause, D.R., 2006. The supply base and its complexity: implications for transaction costs, risks, responsiveness, and innovation. J. Oper. Manag. 24 (5), 637\u2013652. L. Knight et al. Journal of Purchasing and Supply Management 28 (2022) 100753 11 Circular Flanders, 2019. The circular ambition chart. Accessed date: November 2nd, 2021. Available at: https://aankopen.vlaanderen-circulair.be/en/getting-started/th e-ambition-map. Ciulli, F., Kolk, A., Boe-Lillegraven, S., 2019. Circularity brokers: digital platform organizations and waste recovery in food supply chains. J. Bus. Ethics 1\u201333. Cook, J.A., 2010. Supply chain versus the volcano, supply chain quarterly, perspectives. Available at: https://www.supplychainquarterly.com/articles/348-supply-chain -versus-the-volcano. Cornelissen, J., Hollerer, \u0308 M.A., Seidl, D., 2021. What theory is and can Be: forms of theorizing in organizational scholarship. Organ. Theor. 2 (3), 26317877211020328. Craighead, C.W., Blackhurst, J., Rungtusanatham, M.J., Handfield, R.B., 2007. The severity of supply chain disruptions: design characteristics and mitigation capabilities. Decis. Sci. J. 38 (1), 131\u2013156. Cutler, A.C., Lark, D., 2020. The Hidden Costs of Law in the Governance of Global Supply Chains: the Turn to Arbitration, Review of International Political Economy. https:// doi.org/10.1080/09692290.2020.1821748. De Angelis, R., Howard, M., Miemczyk, J., 2018. Supply chain management and the circular economy: towards the circular supply chain. Prod. Plann. Control 29 (6), 425\u2013437. de Sousa Santos, B., 2018. Decolonising the University. Cambridge Scholars Publishing Cambridge. Dhanorkar, S., Kim, Y., Linderman, K., 2019. An empirical investigation of transaction dynamics in online surplus networks: a complex adaptive system perspective. J. Oper. Manag. 65 (2), 160\u2013189. Di Mauro, C., Giannakis, M., 2019. Special issue of the 27th annual IPSERA conference 2018-purchasing & supply management: fostering innovation. J. Purch. Supply Manag. 25 (4), 100573. Diprose, R., Kurniawan, N., Macdonald, K., Winanti, P., 2020. Regulating Sustainable Minerals in Electronics Supply Chains: Local Power Struggles and the \u2018hidden Costs\u2019 of Global Tin Supply Chain Governance, Review of International Political Economy. https://doi.org/10.1080/09692290.2020.1814844. du Plessis, C., 2012. Towards a regenerative paradigm for the built environment. Build. Res. Inf. 40 (1), 7\u201322. Ehrenfeld, J.R., Hoffman, A.J., 2013. Flourishing: A Frank Conversation about Sustainability. Stanford Business Books, Stanford, US. European Commission, 2017. Public procurement for a circular economy: good practice and guidance. Accessed on Oct 18th, 2021. Available at: https://ec.europa.eu/enviro nment/gpp/circular_procurement_en.htm. Figueres, C., Rivett-Carnac, T., 2020. The Future We Choose: Surviving the Climate Crisis. Manila Press, London. Finkenstadt, D.J., Handfield, R., 2021. Blurry vision: supply chain visibility for personal protective equipment during COVID-19. J. Purch. Supply Manag., 100689 General Motors, 2020. Ventec Life Systems and GM partner to mass produce critical care ventilators in response to COVID-19 pandemic. GM press release. March 27, 2020. https://media.gm.com/media/us/en/gm/news.detail.html/content/Pages/ne ws/us/en/2020/mar/0327-coronavirus-update-6-kokomo.html. George, G., Howard-Grenville, J., Joshi, A., Tihanyi, L., 2016. Understanding and tackling societal grand challenges through management research. Acad. Manag. J. 59 (6), 1880\u20131895. Gereffi, G., Lee, J., 2012. Why the world suddenly cares about global supply chains. J. Supply Chain Manag. 48 (3), 24\u201332. Ghosh, A., 2016. The Great Derangement: Climate Change and the Unthinkable. University of Chicago Press, Chicago and London. Goranova, M., Verstegen Ryan, L., 2021. The corporate objective revisited: the shareholder perspective. J. Manag. Stud. 59, 2. https://doi.org/10.1111/ joms.12714. Gray, J.V., Helper, S., Osborn, B., 2020. Value first, cost later: total value contribution as a new approach to sourcing decisions. J. Oper. Manag. 66 (6), 735\u2013750. Gualandris, J., Lee, D., Beattie, C., 2021a. HP Canada Co.: a circular supply chain for recycled plastic. Ivey Publishing. Available at: https://www.iveypublishing.ca/s/p roduct/hp-canada-co-a-circular-supply-chain-for-recycled-plastic/01t5c00000 CwqXiAAJ. Gualandris, J., Lee, D., Lin, R., 2021b...",
            "url": "https://openalex.org/W4220751036",
            "title": "Future business and the role of purchasing and supply management: Opportunities for \u2018business-not-as-usual\u2019 PSM research",
            "publication_date": "2022-01-01"
          },
          {
            "id": "E3703744204",
            "text": "..broadly defined \u201cas the ability of a firm to provide insights using data management, infrastructure, and talent to transform business into a competitive force.\u201d (p. 557) Resource-based view, dynamic capabilities view Akter et al. (2016), Kiron et al. (2014) Dubey et al. (2019a) \u201cBDAC is an organizational facility with tools, techniques, and processes that enable the organization to process, visualize, and analyze data, thereby producing insights that enable data-driven operational planning, decision making and execution.\u201d (p. 2095) Dynamic capabilities view, contingency theory Supply chain management Srinivasan and Swink (2018) Lin and Kunnathur (2019) \u201cBig Data Capability as a firm's capability of identifying sources, where large volumes of various kinds of data flow out at high speed, and collecting, storing, and analyzing such Big Data for the purpose of accomplishing the firm's strategic as well as operational goals.\u201d (p. 51) Dynamic capabilities view Pigni et al. (2016), Chen et al. (2015) Lozada et al. (2019) \u201c... BDA capability refers to a company's management ability, that is, the continuous use and deployment of big data resources with the strategic goal of creating value and developing a competitive advantage for the firm.\u201d (p. 2) Wamba et al. (2017), Garmaki et al. (2016), Gupta and George (2016), Kiron et al. (2014) Mandal (2019) BDA capabilities are conceptualized as \u201ca thirdorder formative construct of BDA management capability, BDA personnel expertise capability and BDA infrastructure flexibility capability\u201d (p. 298). \u201cBDA management capabilities comprise of essential first-order capabilities of planning, investment decision making, coordination and control.\u201d (p. 298). Resource-based view, dynamic capabilities view Supply chain management Wamba et al. (2017) Mikalef et al. (2019b) BDAC is \u201cdefined as the ability of a firm to capture and analyse data towards the generation of insights by effectively orchestrating and deploying its data, technology and talent.\u201d (p. 273) Resource-based view, dynamic capabilities view Gupta and George (2016), Mikalef et al. (2018), Wamba et al. (2017) Rialti et al. (2019) \u201cOrganizational BDA capabilities are an ensemble of capabilities that include infrastructure flexibility, management capabilities and personnel capabilities.\u201d (p. 1) Dynamic capabilities view Wamba et al. (2017), Gunasekaran et al. (2018), Mikalef and Pateli (2017) Definition based on Wamba et al. (2017), measures of constructs based on the 3 papers. Wang et al. (2019) \u201cBig data analytics capability is defined as the ability to acquire, store, process and analyse large amounts of health data in various forms, and deliver meaningful information to users, which allows them to discover business values and insights in a timely fashion.\u201d (p. 368) Configuration theory Health care Wang and Hajli (2017) Belhadi et al. (2020) \u201cBDA capability is defined as the ability of the organizations in developing competency to generate business insights based on organizational (i.e., BDA management), physical (i.e., IT infrastructure), and human (e.g., analytics skill or knowledge) capabilities for increased business performance\u201d (p. 2) Resource-based view, dynamic capabilities view Manufacturing firms Mikalef et al. (2020b), Akter et al. (2016) Corte-Real \u02c6 et al. (2020) BDACs refer to \u201cthe extent to which BDA has been used to provide business insights into primary activities (e.g., production, distribution, and customer service).\u201d (p. 6) Resource-based view, dynamic capabilities view, knowledge management Chen et al. (2015) Mikalef et al. (2020a) BDAC is \u201cthe ability of a firm to capture and analyze data toward the generation of insights by effectively orchestrating and deploying its data, technology, and talent.\u201d (p. 2) Resource-based view, information governance theory Akter et al. (2016), Gupta and George (2016), Wamba et al. (2017), Kiron et al. (2014) Mikalef et al. (2020b) BDAC is defined as \u201cthe ability of a firm to effectively deploy technology and talent to capture, store and analyze data, toward the generation of insight.\u201d (p. 7) Resource-based view, dynamic capabilities view, IT capability Gupta and George (2016), Mikalef et al. (2018) Ramadan et al. (2020) \u201cBig data analytics capabilities refer to the firm's ability to recognize and analyze different data sources to provide valuable insights.\u201d (p. 2) Manufacturing firms Hu et al. (2018) Shamim et al. (2020) BDAC is referred to as \u201ca holistic approach of analysing and processing big data for value creation.\u201d (p. 4) Knowledge based dynamic capabilities view, social capital theory Wamba et al. (2017), Akhtar et al. (2019), Song et al. (2020) BDAC is defined \u201cas the capability of firms to combine, integrate, and deploy specific big data resources.\u201d (p. 5) Information processing theory Gupta and George (2016) (continued on next page) M.-T. Huynh et al. Technological Forecasting & Social Change 197 (2023) 122884 13 (continued ) Author(s) Definition/conceptualization Theories/views Context Prior studies Upadhyay and Kumar (2020) \u201cBDAC is broadly defined as the competence to provide business insights using data management, infrastructure (technology), and talent (personal) capabilities to transform the business into a competitive force.\u201d (p. 2) Resource-based view, dynamic capabilities view, socio-materialism theory Wamba et al. (2017), Gupta and George (2016), Kiron et al. (2013) Kim et al. (2011), Kim et al. (2012) for measurement of constructs Wamba et al. (2020) BDAC is defined as \u201ca firm's ability to assemble, integrate, and deploy its big data-specific resource.\u201d (p. 10) IT capability, resource-based view Gupta and George (2016) Xiao et al. (2020) \u201cBDAC refers to the ability to provide business insights in the big data environment by using big data analytics personnel, big data analytics technical, and big data analytics management capabilities.\u201d (p. 18780) Dynamic capabilities view Service sector Akter et al. (2016) Yasmin et al. (2020) BDACs are referred to \u201cas a balanced combination of requisite human resource, big-data skills, advanced technologies supported by large datasets to generate analytical reports and actionable insights utilized, produced, and processed by mathematical, statistical techniques, and machine learning tools for enhanced performance.\u201d (p. 2) Resource-based view, dynamic capabilities view Akhtar et al. (2019), Akter et al. (2016), Wang et al. (2019) Zhang et al. (2020) BDAC refers to \u201ca firm's ability to assemble, integrate, and deploy its big data-specific resources.\u201d (p. 3) Source-position-performance theoretical framework Sustainability development projects Gupta and George (2016), Akter et al. (2016), Ferraris et al. (2019), Wamba et al. (2017) Ashaari et al. (2021) \u201cBDAC is termed as an organization's capacity to efficiently and strategically arrange, assemble, and apply BDA resources so that effective decision-making can be made to enhance overall organization's performance.\u201d (p. 1) Resource-based view, organizational information processing theory Higher education institutions Mikalef et al. (2020b), Shamim et al. (2020), Janssen et al. (2017), Cao et al. (2015), Awan et al. (2021) BDACs refer to a \u201cholistic process that involves the collection, analysis, use and interpretation of data for various functional divisions to gain actionable insights, create business value and establish competitive advantage.\u201d (p. 86) Resource-based view, dynamic capabilities view, institution-based view Manufacturing sector Akter et al. (2016) Bertello et al. (2021) BDAC is defined \u201cas the ability to acquire, store, process, and analyze large amounts of data in various forms and deliver meaningful information to users, allowing them to discover business values and insights in a timely fashion.\u201d (p.1040) Resource-based view Wang et al. (2018a) Ciampi et al. (2021) BDAC \u201crefer to the company's abilities to leverage on technology and talent to exploit BD towards the generation of the insights that are necessary to overperform rivals.\u201d (p. 2) Knowledge-based view, IT capability Mikalef et al. (2017), Wamba et al. (2017), Gupta and George (2016) Contreras Pinochet et al. (2021) BDAC is recognized as \u201cthe competence to provide business insights using the capacity of data management, infrastructure (technology) and talent (personnel) to transform a business into a competitive force.\u201d (p. 1410) Process-oriented dynamic capabilities, business value Gupta and George (2016) Gu et al. (2021) BDAC in supply chain management is described as \u201cthe ability of organizations to collect and organize supply chain data from heterogeneous systems distributed across organizational boundaries, analyze it either batch-wise, or real-time, or near real-time, and visualize it intuitively to create proactive supply chain system and support decision making,\u201d (p. 155) Resource-based view, dynamic capabilities view, contingency theory Supply chain management Arunachalam et al. (..",
            "url": "https://openalex.org/W4387418465",
            "title": "Big data analytics capabilities: Patchwork or progress? A systematic review of the status quo and implications for future research",
            "publication_date": "2023-10-06"
          }
        ]
      }
    },
    "evaluation": {
      "scores": {
        "accuracy": {
          "reasoning": "The given answer accurately reflects the role of big data analytics in enhancing healthcare supply chain resilience during the COVID-19 pandemic, similar to the golden answer. It discusses the integration of big data analytics capabilities (BDACs) and their impact on organizational agility and responsiveness, which aligns with the golden answer's emphasis on these aspects. However, while it captures the essence, it lacks some specific theoretical frameworks mentioned in the golden answer, such as the Organization Information Processing Theory (OIPT) and the Scanning Interpretation-Action-Performance (SIAP) model. Therefore, it is mostly accurate but not entirely comprehensive in its theoretical backing.",
          "score": 8
        },
        "completeness": {
          "reasoning": "The given answer covers several key aspects of the golden answer, including the integration of big data analytics and innovation leadership, and the importance of these elements for supply chain resilience. However, it does not fully address the theoretical frameworks that support this integration, which are crucial for a complete understanding as presented in the golden answer. The inclusion of ethical considerations is a positive addition, but it diverges from the primary focus of the golden answer. Thus, while it is thorough, it misses some critical components, leading to a slightly lower completeness score.",
          "score": 7
        },
        "relevance": {
          "reasoning": "The given answer is highly relevant to the complex question, addressing the integration of big data analytics and innovation leadership in healthcare supply chains during the COVID-19 pandemic. It discusses how these integrations enhance resilience and responsiveness, which are central themes of the question. However, the answer could have been more focused on the specific theoretical frameworks mentioned in the golden answer, which would have strengthened its relevance. Overall, it effectively addresses the main points but could improve in specificity.",
          "score": 8
        },
        "evidence_quality": {
          "reasoning": "The given answer cites various sources to support its claims about big data analytics capabilities and their impact on healthcare supply chains. However, the quality of the evidence is mixed; while some references are authoritative, others are less well-known or lack sufficient context. In contrast, the golden answer references well-established theories and empirical studies, which enhances its credibility. Therefore, while the evidence in the given answer is generally reliable, it does not match the authoritative quality of the golden answer's references.",
          "score": 6
        },
        "clarity": {
          "reasoning": "The given answer is generally clear and well-structured, making it easy to follow the main arguments regarding big data analytics and supply chain resilience. However, some sections could benefit from more concise language and clearer transitions between ideas, particularly when introducing new concepts or evidence. The golden answer is more polished in its presentation, which contributes to its overall clarity. Thus, while the given answer is understandable, it lacks the same level of refinement.",
          "score": 7
        },
        "logical_structure": {
          "reasoning": "The logical structure of the given answer is coherent, with a clear progression from the introduction of big data analytics to its implications for healthcare supply chains. However, it could benefit from a more explicit outline of how each section relates to the overall argument, particularly in connecting theoretical frameworks to practical applications. The golden answer presents a more systematic structure, which enhances its logical flow. Therefore, while the given answer is organized, it does not achieve the same level of clarity in its structure.",
          "score": 7
        },
        "evidence_support": {
          "reasoning": "The given answer provides evidence to support its claims about the benefits of big data analytics in healthcare supply chains, referencing various studies and reports. However, the integration of this evidence could be stronger; some claims are made without sufficient backing or context, which weakens the overall argument. The golden answer effectively uses evidence to substantiate its claims, particularly in relation to theoretical frameworks. Thus, while the given answer does use evidence, it does not do so as effectively as the golden answer.",
          "score": 6
        },
        "depth_of_analysis": {
          "reasoning": "The depth of analysis in the given answer is commendable, as it explores the implications of big data analytics for supply chain resilience and discusses the role of innovation leadership. However, it lacks a thorough exploration of the theoretical frameworks that underpin these concepts, which are critical for a comprehensive analysis. The golden answer provides a more in-depth examination of these frameworks, enhancing its analytical depth. Therefore, while the given answer is insightful, it does not reach the same level of depth as the golden answer.",
          "score": 6
        },
        "objectivity": {
          "reasoning": "The given answer maintains a largely objective tone, presenting information about big data analytics and its impact on healthcare supply chains without overt bias. It acknowledges the ethical considerations associated with data usage, which adds to its objectivity. However, the golden answer presents a more balanced view by integrating multiple theoretical perspectives, which enhances its objectivity. Thus, while the given answer is generally objective, it could benefit from a broader range of viewpoints.",
          "score": 7
        },
        "synthesis": {
          "reasoning": "The given answer synthesizes information from various sources regarding big data analytics and its role in healthcare supply chains. However, it does not effectively integrate the theoretical frameworks that support its claims, which limits the overall synthesis of ideas. The golden answer excels in synthesizing literature and theoretical perspectives, providing a more cohesive understanding of the topic. Therefore, while the given answer demonstrates some synthesis, it does not achieve the same level of integration as the golden answer.",
          "score": 6
        }
      }
    },
    "average_score": 6.8
  },
  {
    "id": "https://openalex.org/W4225512839",
    "meta": {
      "id": "https://openalex.org/W4225512839",
      "title": "Region-Object Relation-Aware Dense Captioning via Transformer",
      "publication_date": "2024-01-01",
      "cited_by_count": 73,
      "topics": "Visual Question Answering in Images and Videos, Image Feature Retrieval and Recognition Techniques, Human Action Recognition and Pose Estimation",
      "keywords": "Closed captioning, Image Captioning, Object Recognition, Action Recognition, Interest Point Detectors, Feature Matching",
      "concepts": "Closed captioning, Computer science, Transformer, Encoder, Artificial intelligence, Natural language processing, Computer vision, Image (mathematics), Engineering, Voltage, Electrical engineering, Operating system",
      "pdf_urls_by_priority": [
        "https://wrap.warwick.ac.uk/163920/1/WRAP-Region-object-relation-aware-dense-captioning-transformer-2022.pdf",
        "http://pure.aber.ac.uk/ws/files/50445086/TNNLS_Zhuang.pdf",
        "https://pure.aber.ac.uk/portal/files/50445086/TNNLS_Zhuang.pdf"
      ],
      "text_type": "full_text",
      "successful_pdf_url": "https://wrap.warwick.ac.uk/163920/1/WRAP-Region-object-relation-aware-dense-captioning-transformer-2022.pdf",
      "openalex_rank": 23,
      "num_tokens": 15084,
      "url": "https://wrap.warwick.ac.uk/163920/1/WRAP-Region-object-relation-aware-dense-captioning-transformer-2022.pdf"
    },
    "text": "warwick.ac.uk/lib-publications\nManuscript version: Author\u2019s Accepted Manuscript\nThe version presented in WRAP is the author\u2019s accepted manuscript and may differ from the\npublished version or Version of Record.\nPersistent WRAP URL:\nhttp://wrap.warwick.ac.uk/163920\nHow to cite:\nPlease refer to published version for the most recent bibliographic citation information.\nIf a published version is known of, the repository item page linked to above, will contain\ndetails on accessing it.\nCopyright and reuse:\nThe Warwick Research Archive Portal (WRAP) makes this work by researchers of the\nUniversity of Warwick available open access under the following conditions.\nCopyright \u00a9 and all moral rights to the version of the paper presented here belong to the\nindividual author(s) and/or other copyright owners. To the extent reasonable and\npracticable the material made available in WRAP has been checked for eligibility before\nbeing made available.\nCopies of full items can be used for personal research or study, educational, or not-for-profit\npurposes without prior permission or charge. Provided that the authors, title and full\nbibliographic details are credited, a hyperlink and/or URL is given for the original metadata\npage and the content is not changed in any way.\nPublisher\u2019s statement:\nPlease refer to the repository item page, publisher\u2019s statement section, for further\ninformation.\nFor more information, please contact the WRAP Team at: wrap@warwick.ac.uk.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1\nRegion-object Relation-aware Dense Captioning via\nTransformer\nZhuang Shao, Jungong Han, Demetris Marnerides, Kurt Debattista\nAbstract\u2014Dense captioning provides detailed captions of com\u0002plex visual scenes. While a number of successes have been\nachieved in recent years, there are still two broad limitations:\n1) Most existing methods adopt an encoder-decoder framework,\nwhere the contextual information is sequentially encoded using\nLong Short-Term Memory (LSTM). However, the forget gate\nmechanism of LSTM makes it vulnerable when dealing with a\nlong sequence; 2)The vast majority of prior arts consider Regions\nof Interests (RoIs) equally important, thus failing to focus on\nmore informative regions. The consequence is that the generated\ncaptions cannot highlight important contents of the image, which\ndoes not seem natural. To overcome these limitations, in this\npaper, we propose a novel end-to-end transformer-based dense\nimage captioning architecture, termed Transformer-based Dense\nCaptioner (TDC). TDC learns the mapping between images\nand their dense captions via a Transformer, prioritising more\ninformative regions. To this end, we present a novel unit, named\nRegion-Object Correlation Score Unit (ROCSU), to measure\nthe importance of each region, where the relationships between\ndetected objects and the region, alongside the confidence scores\nof detected objects within the region, are taken into account.\nExtensive experimental results and ablation studies on the stan\u0002dard dense-captioning datasets demonstrate the superiority of\nthe proposed method to the state-of-the-art methods.\nIndex Terms\u2014Dense Image Captioning, Transformer-based\nDense Image Captioner, Region-Object correlation score unit\nI. INTRODUCTION\nD\nEnse captioning has gained significant attention from\nboth the engineering and research communities recently.\nOn the one hand, it facilitates important practical applications\n[1], such as human-robot interaction [2], navigation for the\nblind, object detection [3] [4] or segmentation [5] and image\u0002text retrieval [6] [7]. On the other hand, it poses substantial\nchallenges to both computer vision and natural language\nprocessing research communities. Its complexity in generat\u0002ing richer and more detailed descriptions for local regions,\ncompared to image captioning, hastens the emergence of more\nadvanced captioning techniques.\nDense captioning stems from image captioning, and recent\nyears have witnessed a rapid development of image captioning\nManuscript received xxx, xxx; revised xxx, xxx and xxx, xxx; accepted\nxxx, xxx. (Corresponding author: Jungong Han). This research was supported\nby the funds of China Scholarship Council under Grant No. 201909120012.\nZhuang Shao is with Warwick Manufacturing Group, University of War\u0002wick, CV4 7AL, UK (e-mail: ZhuangShao@warwick.ac.uk).\nJungong Han is with the Department of Computer Science, Aberystwyth\nUniversity, SY23 3DB, UK (e-mail: jungonghan77@gmail.com).\nDemetris Marnerides is with Warwick Manufacturing Group, University of\nWarwick, CV4 7AL, UK (e-mail: dmarnerides@gmail.com).\nKurt Debattista is with Warwick Manufacturing Group, University of\nWarwick, CV4 7AL, UK (e-mail: K.Debattista@warwick.ac.uk).\na green plastic chair and three pegions\na bird on a building\n(a)\nbird 0.983\nbird 0.948\nbird 0.998\nchair 0.726\n(b)\nFig. 1. (a) An example of the RoI description created by the LSTM method\nCOCG [14]. (b) The corresponding object detection results as context to guide\nthe dense captioning.\ntechniques. Many of these methods are based on encoder\u0002decoder frameworks and inspired by the successful transfer of\nsequence to sequence training used for machine translation [8].\nBroadly, image features are first extracted by a Convolutional\nNeural Network (CNN) as an encoder, and then fed into an\nRNN-based decoder that outputs the corresponding captions.\nHowever, such a captioning mechanism based on encoder\u0002decoder frameworks fails to focus on areas that may be worthy\nof more attention at the training stage. To address this issue,\nmany updated methods have been proposed. For example, [9]\nproposed aligned high-level information while [10]\u2013[13] re\u0002sorted to different forms of attention to aid guidance during\ntraining.\nDense captioning is beyond image captioning due to the\nneed to provide richer and more detailed descriptions for\na given image. [15] took the initiative to develop a Fully\nConvolutional Localization Network (FCLN) for the dense\ncaptioning task, in which Regions of Interests (RoIs) are\nlocalized before being described. Afterwards, many follow-ups\nappeared, which can be generally categorized into two classes\ndepending on whether the contextual information encoded in\nthe model is used. At the early stage, the architecture was\ncomposed of a Faster Region-based Convolutional Network\n(R-CNN) [16] module to detect RoIs and describe them\nwith a Long Short-Term Memory (LSTM) [17], which was\nan advanced variant of Recurrent Neural Network (RNN).\nUnfortunately, this kind of framework only considered the\nRoIs but ignored possible contextual information that can be\nleveraged to improve training. To address this problem, [18]\nproposed to integrate the RoI features with image features as a\nglobal context to build up a joint and contextual fusion before\ncaptioning via an LSTM. However, the proposed global con\u0002text seems too coarse, and there have been several methods that\nexplored fine-grained contexts. For instance, [19] proposed a\nnon-local similarity graph for the feature interaction between\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 2\nthe target RoI and its neighboring RoIs. Also, supported by\ndata statistics, [14] revealed the close relationship between\nRoIs and detected objects via object detection, thus resulting\nin an architecture with contextual information considered.\nDespite the preliminary success of the aforementioned\nmethods, dense image captioning can, arguably, be considered\nstill in its infancy. We believe a number of limitations still ex\u0002ist, two of which are critical. Firstly, LSTMs, as the dominant\nstructures for the methods mentioned above, suffer from the\nnature of the forget gate mechanism: forgotten information\nafter a sequence cannot be avoided, especially when the\ninputted sequence is long. In state-of-the-art methods, if the\ncontextual information is encoded by an LSTM, and with\ntime rolling, the initial object would be \u201cforgotten\u201d and thus\nit weakens the guidance function of context especially when\nthere are interactions of multiple people and multiple objects.\nHence, the training model may fail to \u201coversee\u201d the objects\nso that it cannot guide the captioning process properly. As\na result, this kind of gap often gives rise to the missing of\ndescriptive objects, as illustrated in Fig. 1. Obviously, the\nobject detection results as guided context for dense captioning\nare in good conditions, with accurate localizations and high\nconfidence on the right. However, on the left, due to the\naforementioned deficit of LSTM, the output caption does not\ninclude all three birds and a chair in its answer. Instead, it\ngenerates only a bird on a building, but forgets the other two\nbirds and the chair.\nSecondly, in the previous methods, e.g. [14], all the RoIs\nare treated with equal weights during training. However, in\nthe real world, the useful information carried by each RoI can\nbe hugely different. Also, the detection confidence scores of\nobjects within and around the region may vary considerably\nfrom region to region. These all imply that the regions should\nbe treated differently during model training. As shown in\nFig. 2, it may make more sense if assigning larger weights\nto the RoIs with more information at the training stage.\nConcretely, in this example, on the left are two RoIs detected,\nbut apparently, they have different IoUs with the overall object\nbounding boxes illustrated on the right. According to the\ndescriptive languages of these RoIs, it is obvious that the\ncaption of the one in red with a higher IoU with the objects\non the right. Also, it contains much more information in its\nground truth since its description reveals the theme of the\nimage. In contrast, the RoI in yellow contains too detailed\ninformation and this kind of information is even far difficult\nfor the human being to observe, not to mention attain it by\nmachine learning. Inspired by the common exam strategy that\na student should focus more on the basic questions accounting\nfor a large proportion of marks, rather than concentrating on\ndifficult ones, we hold a view that the informative regions\ndeserve more priorities.\nTo alleviate the first issue, we propose a novel end-to-end\ndense captioning framework based on Transformer [20], which\nis currently popular in a great variety of computer vision\ntasks, termed Transformer-based Dense Captioner (TDC), to\novercome the limitations of the forget mechanism of LSTM\nwhen encoding and decoding visual and language information.\nFig. 3 gives an overview of TDC. Particularly, inspired by [14],\nwe compose both object detection information and holistic\nimage features as context. Along with the detected RoIs from\nFaster R-CNN Region Proposal Network (RPN) and contex\u0002tual information, the visual information is projected into a\nvisual representation by applying a dot product between them.\nThe same operation is implemented on language information\nas well. At the decoding phase, a probability distribution for\ncaptions of detected RoIs is learnt by cross-modality attention\nof both visual and language encoding results. During encoding\nand decoding, all of the input vectors are aligned and computed\ntogether, hence it can overcome the forget problem.\nIn order to address the second limitation, we propose a\nmodule, which allocates weights for the language loss of each\nregion at each step of training. The underlying assumption\nis that the regions comprising more objects with high detec\u0002tion confidence scores are more important, and thus, deserve\npriority. To this end, we propose a novel unit, which makes\nuse of both the object detection score and the intersection\nof union [21] (IoU), named Region-Object Correlation Score\nUnit (ROCSU).\nThe major contributions of this work are summarized as:\n\u2022 A novel end-to-end dense captioning framework based\non the Transformer, dubbed TDC, is proposed. A distinct\nproperty of TDC is the advocate of a Transformer to\ncapture the long-range contextual information among\nobjects. It is clearly advantageous over LSTM that is\nimpotent in capturing long-range dependencies among\nobjects. To the best of our knowledge, this is the first work\nthat builds up a Transformer-based architecture rather\nthan an LSTM for the dense captioning topic.\n\u2022 An RoI importance unit, named Region-Object Corre\u0002lation Score Unit (ROCSU), drives the loss function to\nfocus more on RoIs with more information. In doing so,\nour work, for the first time, weighs RoIs by jointly con\u0002sidering object-region relationships and object detection\nconfidence scores during model training. It differs from\ntreating each RoI equally at the training stage.\n\u2022 Extensive experimental results on different challenging\ndatasets show the superiority of the proposed method\nagainst the state-of-the-art methods.\nThe rest of this paper is organized as follows: We discuss re\u0002lated work in Section II. In Section III, the proposed method is\nintroduced in detail with a comprehensive analysis. Extensive\nexperimental results are demonstrated in Section IV with both\nqualitative and quantitative analysis. Finally, we summarize\nthis paper with a conclusion in Section V.\nII. RELATED WORK\nIn this section, we will review the related works from two\naspects: image captioning and dense captioning.\nA. Image Captioning\nEarlier neural network models for image captioning [11],\n[22]\u2013[24] encoded visual information using a single feature\nrepresentation of the image [25] with very limited additional\ninformation. However, with the development of deep learning,\nmore auxiliary information can be added up into a model\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3\npeople skiing on the snow\nA ski hat\n(a)\nPerson\n0.999\nSkis 0.984\nPerson\n0.998\nPerson\n0.998\nPerson\n0.998\n(b)\nFig. 2. An example shows the RoIs with different IoUs should be weighted differently. (a) Two RoIs and their descriptions; (b) Object detection results.\n\u2022\u2022\u2022\nObject Detection\nImage feature\nObject Context\nFeatures\n\u2022\u2022\u2022\nObject\nFeatures\n\u2022\u2022\u2022\nGeometry\nFeatures\nDense captions\nA boy throwing baseball\nTwo men are watching\nRoI detector\n(RPN)\nPre-trained\nResnet152\nRoI\n\u2022\u2022\u2022\nFeatures\nOff-line\nPre-trained\nContext\nModule\nVisual\n(Transformer)\nEncoder\nNRoI\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\n\u2022\u2022\u2022\nA\nboy\nbaseball\nTwo\nmen\nwatching\nCaptioning\n(Transformer)\nDecoder\nWord\nEmbedding\nA boy throwing baseball\nTwo men are watching\n\u2022\u2022\u2022\nFig. 3. The proposed TDC framework is made up of an RoI detector, context module, visual encoder and captioning decoder. Given an image, the RoI\ndetector detects RoIs and the context module prepares contextual information generated via the pre-trained object detector for further use. After this, the visual\nencoder encodes visual information by attention, which gains a visual representation. Finally, after the word embeddings are conducted, visual representation\nand sentence information are decoded by the captioning decoder to generate dense captions for each RoI.\nstructure. [9] extracted region features from images with an R\u0002CNN object detector [26] and generated separate captions for\nthe regions as the captions of the given image. [10] proposed a\nmethod to generate image descriptions by first detecting words\nassociated with different regions within the image [25]. In\naddition, [27] proposed an efficient concept learning module\nto get pseudo pairs.\nTo better focus on important parts of images and model\ntheir correspondent relations with words in captions, a series of\nvariants of attention models have been incorporated. [12] pro\u0002posed a semantic attention module, which combines the top\u0002down and bottom-up attention together. Also, [28] involved ge\u0002ometric attention, which inspired [29] to develop a framework\nwith two Graph Convolutional Networks to explore visual\nrelationships. In recent years, with the advance of Natural\nLanguage Processing (NLP), the Transformer architecture [20]\nhas led to significant performance improvements for various\ntasks. [30] proposed a Transformer-based model by extracting\na single global image feature from the image as well as\nuniformly sampling features by dividing the image into 8x8\npartitions. In the latter case, the feature vectors were fed in a\nsequence to the Transformer encoder [25].\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 4\nB. Dense Captioning\nLater on, dense captioning [15] emerged as a new task\nthat requires an intelligent vision system to both localize and\ndescribe salient regions within an image in natural language.\nExisting dense captioning algorithms can be roughly catego\u0002rized into two types: captioning with the guidance of con\u0002textual information and captioning without using contextual\ninformation.\n1) Dense Captioning Without Context: In [15], Johnson et\nal. proposed a bilinear interpolation with a prototype of an\nRPN in Faster R-CNN. All the RoIs are represented by the\nsame-size features, denoted as region features. Subsequently,\nthey are passed through a fully-connected layer to determine\nif they are foreground (the descriptive region) or background.\nThe locations of these regions are also amended at this stage\nvia regression. At a later stage, region features are described\nby an LSTM language model, which is trained in an end-to\u0002end manner.\n2) Dense Captioning With Context: The work in [18] is\nconceptually similar to [15]. But the difference lies in that the\nimage feature acted as the contextual information, which was\nfed into the captioning module together with RoIs. Despite\nan improved performance, the contextual information is just\na kind of global and coarse information, thus leading to the\nfailure to encode more detailed context information.\nSubsequent works attempted to incorporate fine-grained\ncontext into the framework. For instance, [19] established a\nnon-local similarity graph for the feature interaction between\nthe target RoIs and its neighboring RoIs. Furthermore, it is\nnoted that in [14], the authors argued that objects provide\nvaluable cues to help locate captioning regions and generate\ndescriptions for them via the use of data statistics. Inspired\nby this, the authors proposed to bring in local contextual\ninformation to guide the training of the model. To capture\nuseful object information in an image, a novel framework\nfor learning a complementary object context for each RoI\nwas proposed using an LSTM. This context is derived from\na concatenation of extracted object features and geometry\ninformation. The LSTM cell progressively accepts each object\nas input and decides whether to keep it or discard it. In the\nend, the context is also used as guidance information to help\ngenerate the descriptions and predict the bounding box offsets.\nA close look at the method in [14] reveals that the entire\nalgorithm carries out an encoding-decoding procedure. In the\nencoding procedure, the representations of each contextual\nobject fused with its CNN feature and geometry features\n(relative coordinates) are encoded step by step with a guid\u0002ance LSTM, where the guidance information is composed of\nregion features. The output of this procedure is the contextual\ninformation denoted as ci. For the decoding procedure, the\nauthors tried two kinds of caption decoder frameworks, namely\ncontext as guidance (COCG) and context is decoded with\nan LSTM (COCD), respectively. Although they both have a\ncaption LSTM for captioning as well as a location LSTM for\nlocalization, the main difference between these two decoders is\ntheir context decoding architectures. Concretely, COCD adds\nanother LSTM to decode context ci while COCG removes\nthis LSTM and turns the caption LSTM into a guidance\nLSTM to decode ci. In conclusion, as shown in the section\nof experiments in [14], the COCG framework outperforms\nthe COCD framework and other methods, thus obtaining the\nstate-of-the-art results due to the alleviation of the vanishing\ngradient problem by the guidance LSTM unit inside.\nIII. METHODOLOGY\nIn this section, we first briefly describe the popular Trans\u0002former architecture, which is a fundamental component of our\nmethod. Then, we present the framework of our proposed\nTDC. Finally, we elaborate on the proposed ROCSU loss\nadaptation.\nA. Preliminary Review of Transformer\n1) Scaled dot-product attention: The scaled dot-product\nattention is a basic component of the Transformer [20] archi\u0002tecture. Given a query qi \u2208 Rd\nin all T queries, a group of keys\nkt \u2208 Rdand values vt \u2208 Rd, where t = 1, 2, ..., T, the output\nof dot-product attention is the weighted sum of the vt values.\nThe weights are determined by the dot-products of query qi\nand keys kt. Specifically, kt and vt are placed into respective\nmatrices K = (k1, ..., kT ) and V = (v1, ..., vT ) [31]. The\noutput from a query qiis as follows:\nA(qi, K, V ) = V\nexp (KTqi/\n\u221a\nd)\nPT\nt=1 exp (k\nT\nt\nqi/\n\u221a\nd)\n, (1)\nwhere d is the dimension of qi and \u221ad is to normalize the dot\u0002product value. To capture detailed features of the input, an ad\u0002ditional component called multi-head attention is introduced.\nThe multi-head attention is composed of H parallel partial\ndot-product attention components, {hj |j \u2208 [1, H]} refer to\nheads, with each head being independent. The realization of\nthe attention resulting from the multi-head attention (MA) is\ngiven by:\nMA(qi, K, V ) = concat(h1, h2, ..., hH)WO,\nhj = A(W\nq\nj\nqi, W K\nj K, WV\nj V ),\n(2)\nwhere W\nq\nj\n, W K\nj\n, WV\nj\ndenote the transfer weight matrices q,\nK, V for hj . WO is the weight matrix for each head. All\nof these weights are learned during training. This formula\nof attention is generic so that it can represent two kinds of\nattention according to where its input comes from. Specifically,\nwhen the query is from the decoder layer, and meanwhile, both\nthe keys and values come from the encoder layer, it represents\nthe mutual attention due to its cross-module attribute. The\nsecond multi-head attention is called self-attention, where the\nqueries, keys, and values keep unchanged in both encoder and\ndecoder.\n2) Transformer: We now present the use of the Transformer\non top of scaled dot-product attention. The basic unit of the\nTransformer is multi-head attention with feed-forward layers\nfollowed by layer normalization [32]. The feed-forward layers\nmap the output of the multi-head attention layer by two linear\nprojections and an Rectified Linear Unit (ReLU) as the activate\nfunction. The encoder and decoder of the Transformer are\ncomposed of multiple basic structures, and usually, their layer\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 5\nSelf-Attention\nLayer\nV (Q) V(K) V(V)\nFeed-Forward Visual\nEncoder\n(Layer=2)\nV\u2019\nSelf-Attention\nLayer\nS(Q) S(K) S(V)\nCross module\nattention\nS\u2019\nFeed-Forward\nV\u2019 V\u2019\nFC layer and\nsoftmax\nCaption\nDecoder\n(Layer=2)\npositional\nencoding\n+\nVisual Input\npositional\nencoding\n+ Sentence Input\n+\nFig. 4. Transformer structure in our dense captioning scenario, where the\nlayer normalization is omitted.\nnumbers are the same. The decoder of each layer takes the\noutput of the corresponding encoder along with the output\nof the lower layer decoder output. Self-attention exists in\nboth encoder and decoder. Cross-module attention between\nencoder and decoder is also applied in the decoder. Residual\nconnection [33] and layer normalization [32] are implemented\nto all layers. Furthermore, because there is no recurrence\nmodule in a Transformer, to indicate positions for each vector,\npositional encoding (PE) of the input is used. PE occurs at\nthe bottom of the multi-layer Transformer-based encoder and\ndecoder stacks. The dimension of PE is the same as the input,\nso PE embedding can be added directly to the input. The\nrealization of PE is as follows:\nP E(pos, 2i) = sin(pos/100002i/d),\nP E(pos, 2i + 1) = cos(pos/100002i/d),\n(3)\nwhere pos is the position of the embedded vector inside the\ninput matrix, and i is the dimension of the encoded element in\nthe input matrix, d is the total dimension of the input matrix.\nB. Transformer in Dense Captioning Scenario\nFig. 4 shows the structure of the Transformer in this dense\ncaptioning scenario. To be specific, in the visual encoder, the\ninput is encoded into visual features plus positional encodings,\ndenoted as V . The self-attention layer takes three V s at the\npositions of Q, K, V . After the output of the feed-forward\nlayer denoted as V\n0\n, on the other side, the embedded words\nplus positional encodings defined as S undergo the same\nself-attention. At the cross-module attention unit, these two\nmodalities of data interact with each other to gain the output\nof cross-module attention, which proceeds to feed-forward to\nlearn a captioning probability distribution by fully connected\nlayers and a softmax.\nC. Transformer-based Dense Captioner\nIn this section, we introduce our novel Transformer-based\nDense Captioner. Given an image from an image set I =\n{I1, I2, ...IN }, our target is to detect an RoI set, denoted as\nR = {r1, r2, ...rM} and then describe each of them with\ncorresponding sentence set defined as S = {s1, s2, ...sM}.\nTo achieve this goal, our proposed TDC consists of four parts\nwith different functions, namely RoI detector, context module,\nvisual encoder, and captioning decoder, each being elaborated\nin the following subsections. For ease of explanation, we omit\nthe positional encodings in the following sections.\n1) RoI Detector: Inspired by the success of the Faster R\u0002CNN framework in the area of object detection [34], we adopt\nits Region Proposal Network (RPN) as our RoI detector. This\nRPN-based RoI detector is trained in an end-to-end manner to\u0002gether with the captioning downstream task to identify whether\na region proposal is an RoI to be described. However, our\nframework not only uses RoI features from RPN; we integrate\nRoI features with contextual information as introduced in the\nnext sections. Specifically, we use almost the same config\u0002uration as [14], however, we replace its backbone structure\nVGG16 [35] with a ResNet-101 due to its superiority of\nshortcut structure [33]. In addition, we leverage RoI Align [36]\nrather than RoI Pooling due to its better performance for small\nobject detection. Via the RoI detector, given an image in I, we\nget the RoI set R = {r1, r2, ...rM} and its corresponding RoI\nfeature set, denoted as RF = {rf1, rf2, ...rfM}.\n2) Context Module: According to the data statistics in [14],\nthe description of RoIs has a very close relationship with the\nobjects detected in the image, and therefore, the prior knowl\u0002edge of object detection can provide useful aids as contextual\ninformation for dense captioning. Inspired by this and to obtain\nsuch prior knowledge, we pre-trained a Faster R-CNN object\ndetection network on the MS COCO dataset [37] with the same\noperation as [14]. This is used to create contextual information.\nIn this way, we can gain a set of bounding box coordi\u0002nates of detected objects Bobj = {b1, b2, ...bobjN } with their\nconfidence scores confobj = {conf1, conf2, ...confobjN }.\nAdditionally, to get features of each bounding box, we extract\nbounding box and image features with a pre-trained ResNet\u0002152 network because the deeper neural network can capture\nmore local features and it is more suitable for local bounding\nboxes. We denote corresponding bounding box features as\nB = {bf1, bf2, ...bfobjN }. The image features are defined\nas Imgf = {Imgf1, Imgf2, ...ImgfN }. We also get the\ngeometry information of each object bounding box, namely\nG = {g1, g2, ...gobjN }. Same as [14], gi, i \u2208 [1, objN ] is\nthe corresponding coordinate and size ratios of bi. We only\nadd up class information ahead. Finally, the information is\nmerged together with image features extracted by a pre-trained\nResNet-152 network as contextual information for each RoI\ndetected.\n3) Visual Encoder: Given the aforementioned visual fea\u0002tures consisting of prepared context and RoI information, there\nis a visual encoder to learn a combined feature representation.\nWe use both visual features (object features) and geometry\ninformation (relative bounding box coordinates in an image,\nand object class label) as context. These two kinds of features\nare firstly concatenated together as context encoding. Then, for\nthe feature of each RoI detected, the object context encoding\nfrom the object detection is concatenated with image features\nas the final context information. For the context dimension, we\nfirst concatenate visual features and geometry features, then\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 6\nwe use a linear layer to align the context with the size of RoI\nfeatures and image features. For a fair comparison with the\nstate-of-the-art methods, we follow the configuration of [14].\nWe detect 10 objects for each image. For each RoI detected,\nwe assign the features of these 10 objects as the encoding\nfeatures of this RoI. First of all, we concatenate B with G\nto get the potential context for each RoI as BG. Then it is\nallocated to each RoI and thus we get a context matrix denoted\nas C \u2208 RM\u00d7objN \u00d7(dF +dG), where dF is the dimension of\nfeatures and dG is the dimension of geometry information.\nBecause of the different dimensions of object features and\nRoI features, to align with the image and RoI features and\neventually fuse the context information, a linear mapping from\nRdF +dG to Rdis formulated into:\nCalign = WcC + b, (4)\nwhere Wc and b are weight and bias, which can be learned\nin the linear layer for alignment. After we attain Calign, we\nincorporate it with expanded image feature of given image Ii,\nwhose image feature is Imgfi and RoI feature is Rfi. Finally,\nwe get the visual features F\n0 = (f0\n1\n, ...f 0\nT\n) \u2208 RM\u00d7T \u00d7d, T =\n2 + objN as the input of our visual encoder.\nThe encoding process is as follows:\nV\nF\nl\n\u0001\n= \u03d5(P F(\u03c9(F\nl\n)), \u03c9(F\nl\n));\n\u03c9(F\nl\n) =\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u03d5(MA(f\nl\n1\n, Fl, Fl), fl\n1\n...\n\u03d5(MA(f\nl\nT\n, Fl, Fl), fl\nT\n\uf8f6\n\uf8f7\uf8f7\uf8f8\n;\n\u03d5(\u03b1, \u03b2) = LayerNorm(\u03b1 + \u03b2);\nP F(\u03b3) = Ml\n2 max(0, Ml\n1\u03b3 + b\nl\n1\n) + b\nl\n2\n,\n(5)\nwhere \u03d5 is layer normalization on residual output, P F rep\u0002resents the feed-forward unit, which is composed of two\nlinear layers with a nonlinear transformation by an activation\nfunction. MA is the multi-head attention that is composed\nof H parallel partial dot-product attention components. \u03c9 is\nthe output of assembled multi-head attention with a layer\nnormalization by \u03d5. Ml\n1\nand Ml\n2\nare the weights trained for the\nfeed-forward layers, and b\nl\n1\nand b\nl\n2\nare corresponding biases.\nFor the t\n(\nth) feature vector encoded inside the representation\nof an RoI, f\nl\nt\nis given as the query to the attention layer\nand the result is the weighted sum of each f\nl\nt\n, t \u2208 [1, T],\nwhich processed all the encoded features for an RoI, from\nglobal image feature to local RoI feature. Therefore, the output\nvector can gather the encoded the information from all kinds\nof features by rating their relationships one by one. In other\nwords, it makes the encoder with a broad horizon so that it can\navoid forgetting information with the bigger picture observed.\n4) Captioning Decoder: With visual features encoded, the\ncaptioning process is as follows:\nY\nl+1\n\u2264t = \u03d5(P F(\u03c9(Y\nl\n\u2264t\n)), \u03c9(Y\nl\n\u2264t\n));\n\u03c9(Y\nl\n\u2264t\n) =\n\uf8eb\n\uf8ed\n\u03d5(MA((\u03b4(Y\nl\n\u2264t\n)1), Fl, Fl), \u03b4(Y\nl\n\u2264t\n)1\n...\n\u03d5(MA((\u03b4(Y\nl\n\u2264t\n)t), Fl, Fl), \u03b4(Y\nl\n\u2264t\n)t\n\uf8f6\n\uf8f8 ;\n\u03b4(Y\nl\n\u2264t\n) =\n\uf8eb\n\uf8ed\n\u03d5(MA(y\nl\n1\n, Y l, Y l), yl\n1\n...\n\u03d5(MA(y\nl\nt\n, Y l, Y l), yl\nt\n\uf8f6\n\uf8f8 ;\np(wt+1|F\n0\n, Y L\n\u2264t\n) = sof t max(WV Y\nL\nt+1),\n(6)\nwhere y\n0\ni\ndenotes a word token with an embedding dimension\nWV , and Y\nl\n\u2264t = (y\nl\n1\n, ..., yl\nt\n) ,wt+1 is the probability of vocabu\u0002lary bank at time step t+1. \u03b4 is the cross-module attention that\nuses the current representation of word embedding to attend\nto the visual representation from the corresponding layer of\nthe encoder. \u03d5 represents the self-attention part in the decoder.\nHowever, different from the encoder, its inputs are words. It is\nnoted that the restriction of time step means that the attention\nis only on the already generated words.\nD. Training and Optimization\nIn this section, we introduce the training and optimization\ndetails. First, we show the loss function during training. Then\nin the second subsection, we explain our novel ROCSU.\n1) Loss Function: In order to enforce both of the localiza\u0002tion of detected RoIs and descriptive captions to be as close\nas training examples in an end-to-end manner, multiple loss\nfunction terms are leveraged during the Stochastic Gradient\nDescent [38] (SGD) at each training step in a training batch\nas follows:\nL = Lcls + Lreg + rgscore \u00d7 Lcaption\nT\n, (7)\nwhere Lcls is the classification binary cross entropy loss\nfunction of Faster R-CNN RPN [16] for RoI detection, Lreg\nis the smooth l1 loss [39] for coordinate regression of the\nlocation of detected RoIs. It is notable that Lcaption is the\ncross entropy loss of P = {p(wi|F\n0\n; \u03b8), i \u2208 [1, max], which is\nthe probability distribution of descriptive sentence for RoIs in\nthe RoI batch, and their ground truth sentences word by word.\nTo allocate different weights for each detected RoI according\nto its importance, we design a module ROCSU, its output is\ndenoted as rgscore. We will introduce ROCSU in detail in the\nnext subsection.\n2) ROCSU: In this section, we introduce our novel unit\nROCSU to measure the region score for each RoI according\nto its overlap with detected object bounding boxes as follow:\nGiven an RoI riin R = {r1, r2, ...rM} and detected object\nat a training step, the corresponding rgscorei\nis computed as\nfollows:\nrgscorei = BW + IoU(ri\n, Bobj)confobj\nT\n, (8)\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 7\nwhere BW is the basic weight preset for each RoI, IoU is\nthe Intersection of Union between ri and Bobj. To assemble\nall the rgscoreiinto vector, rgscore can be achieved.\nIV. EXPERIMENT\nIn this section, we report and discuss the experiments\nconducted on three public datasets in order to evaluate the\ndense captioning performance of our proposed method.\nWe use the Visual Genome dataset (VG) [40] and the VG\u0002COCO dataset, which is the intersection of VG V1.2 and\nMS COCO [37], as the evaluation benchmarks. The choice\nof datasets is the same as the state-of-the-art methods [14],\n[19] for a fair comparison. The details of each dataset as well\nas the adopted evaluate metrics are elaborated below:\n1) VG: Visual Genome currently has three versions: VG\nV1.0, VG V1.2, VG V1.4. As the state-of-the-art methods\nhave always used VG V1.0 and VG V1.2, we also conduct\nour experiments on VG V1.0 and VG V1.2. The training,\nvalidation and test splits are chosen similarly as [14], [15],\n[19]. There are 77,398 images for training and 5,000 images\nfor validation and testing [14].\n2) VG-COCO: As demonstrated in [14], the target bound\u0002ing boxes of VG V1.0 and VG V1.2 are much denser than the\nbounding boxes in other object detection benchmark datasets\nsuch as MS COCO and ImageNet [41]. For example, each\nimage in the training set of VG V1.2 contains an average of\n35.4 objects, whilst the average value for MS COCO is only\n7.1. To get proper object bounding boxes and caption region\nbounding boxes for each image, following the configuration\nin [14], the intersection of VG V1.2 and MS COCO is used\nin our paper, which is denoted as VG-COCO in which there\nare 38,080 images for training, 2,489 images for validation\nand 2,476 for testing.\n3) Evaluation Metrics: For evaluation, to comply with eval\u0002uation metrics of the state-of-the-art methods, we use the same\nmetric as in [14], [15], [19] called mean Average Precision\n(mAP). It measures the precision for both localization and\ndescription of RoIs. Following the threshold setting in [15],\naverage precision is computed with combinations of different\nIoU thresholds (0.3, 0.4, 0.5, 0.6, 0.7) for the evaluation of\nRoI locations and different Meteor [42] thresholds (0, 0.05,\n0.10, 0.15, 0.20, 0.25) for the evaluation of language similarity\nwith the ground truth. In the end, the mean value of these\nAPs is the mAP score. For each test image, top boxes with\nhigh confidence after non-maximum suppression [43] (NMS)\nwith an IoU threshold of 0.7 are generated. The final results\nare generated by the second round of NMS under the IoU\nthreshold of 0.5.\nA. Implementation Details\nThe experiments are carried out on Linux Ubuntu Server\nwith an Intel i7-5960X CPU@3.0GHz, 64GB RAM and\nNVIDIA GTX 2080 Ti GPU. Specifically, in the proposed\nmethod, all the image features, RoI features, and object bound\u0002ing box features consist of 2048 dimensions. The image batch\nsize is set to 1, the detected RoI batch size in a training step is\n32, and the maximum iteration is 1, 000, 000 for VG-COCO,\nTABLE I\nThe mAP (%) performance of dense captioning algorithms on VG-COCO\ndataset\nMethod mAP(%)\nFCLN [15] 4.23\nJIVC [18] 7.85\nMax Pooling [14] 7.86\nCOCD [14] 7.92\nCOCG [14] 8.90\nImgG [14] 7.81\nCOCG-LocSiz [14] 8.76\nCOCG&GT [14] 9.79\nTDC+ROCSU 11.58\n\u0003\u0002\u0003\u0003 \u0003\u0002\u0003\b \u0003\u0002\u0004\u0003 \u0003\u0002\u0004\b \u0003\u0002\u0005\u0003 \u0003\u0002\u0005\b\n\u000f\u0019 \u0019\n\u0003\u0002\u0003\b\n\u0003\u0002\u0004\u0003\n\u0003\u0002\u0004\b\n\u0003\u0002\u0005\u0003\n\u0003\u0002\u0005\b\n\u0003\u0002\u0006\u0003\n!\u0019 \u0017\u001a\u0019\u0011 \u0019\u0018\u001b \u001b\n\u0014\n\u0001\u0012\u0010 \u0013\u0015\u0016\u0003\u0002\u0006\n\u0010 \u000e\u0016\u0003\u0002\u0006\n\u0014\n\u0001\u0012\u0010 \u0013\u0015\u0016\u0003\u0002\u0007\n\u0010 \u000e\u0016\u0003\u0002\u0007\n\u0014\n\u0001\u0012\u0010 \u0013\u0015\u0016\u0003\u0002\b\n\u0010 \u000e\u0016\u0003\u0002\b\n\u0014\n\u0001\u0012\u0010 \u0013\u0015\u0016\u0003\u0002\n\u0010 \u000e\u0016\u0003\u0002\n\u0014\n\u0001\u0012\u0010 \u0013\u0015\u0016\u0003\u0002\n\u0010 \u000e\u0016\u0003\u0002\nFig. 5. Average precision with different Meteor scores and different IoU\nthresholds on the VG-COCO dataset.\nTABLE II\nThe mAP (%) performance of dense captioning algorithms on VG V1.0\ndataset\nMethod mAP(%)\nFCLN [15] 5.39\nJIVC [18] 9.31\nImgG [14] 9.25\nCOCD [14] 9.36\nCOCG [14] 9.82\nCAG-Net [19] 10.51\nTDC 10.64\nTDC+ROCSU 11.49\nTABLE III\nThe mAP (%) performance of dense captioning algorithms on VG V1.2\ndataset\nMethod mAP(%)\nFCLN [15] 5.16\nJIVC [18] 9.96\nImgG [14] 9.68\nCOCD [14] 9.75\nCOCG [14] 10.39\nTDC 10.33\nTDC+ROCSU 11.90\nand 2, 000, 000 for VG V1.0 and VG V1.2. The learning rate\ndecrease factor is 0.1 at step 480, 000, 640, 000, 800, 000 for\nVG-COCO, and 1, 200, 000, 1, 500, 000, 1, 800, 000 for VG\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 8\nV1.0 and VG V1.2. The basic learning rate is set to 0.001,\nmomentum is 0.9, and weight decay is 0.0005. The BW is\nset as a matrix with all values 0.75.\nIt is noted that the RoI detector and object detector are\ntrained separately. The RPN based RoI detector is trained\nonline as a part of the entire architecture, while the object\ndetection framework is pre-trained offline. They cannot be\ntrained together because they are designed for different tasks.\nRPN is trained for selecting potential RoIs. It is a binary\nclassification and regression problem while the object detector\nis used to create more comprehensive object information. In\naddition, this kind of training settings keeps the same with [14]\nfor a fair comparison.\nB. Quantitative Results and Analysis\n1) Results on VG-COCO Dataset: On the VG-COCO\ndataset, we conduct extensive experiments to compare our\napproach and other baseline methods. These baselines are\ncategorized into two groups: state-of-the-art methods includ\u0002ing Max Pooling, COCD, COCG, ImgG, COCG-LocSiz and\nCOCG&GT in [14] and earlier methods including FCLN [15]\nand joint inference and visual context fusion (JIVC) [18].\nmAP values are provided in Table I. In the following section,\nwe denote our proposed method as TDC+ROCSU, and the\nmethod treats each RoI equally without ROCSU as TDC.\nTable I shows significant improvement in mAP. First of all,\ncompared with the state-of-the-art LSTM method, i.e. COCG,\nthe mAP increases dramatically by about 30%. The gap\nbetween TDC+ROCSU is even larger, reaching almost three\ntimes the mAP of the FCLN method. The results demonstrate\nthe superiority of TDC+ROCSU, which comes from the broad\nhorizon gained of TDC in encoding and decoding and the\nfocus on informative RoIs from ROCSU. It should be noted\nthat even against ground truth localization of each RoI plus\nthe state-of-the-art method COCG denoted as COCG&GT,\nTDC+ROCSU still outperforms it by an 18.28% mAP in\u0002crease.\n2) Results on VG V1.0 Dataset: TDC+ROCSU is also eval\u0002uated on the VG V1.0 dataset. In order to have a fair compari\u0002son with state-of-the-art methods, we adopted the same setting\nas used in [14], [19]. The mAP results are shown in Table II. It\ncan be seen that TDC+ROCSU outperforms the state-of-the-art\nmethods by a significant margin on this dataset also. Overall,\nour method achieves a 17% mAP increase against the COCG\nmethod [14]. Furthermore, the comparison with CAG-Net in\n[12] also shows the superiority of TDC+ROCSU, with 9.32%\nmAP improvements. The improvement is, to a large extent, due\nto the Transformer in TDC+ROCSU that can provide a broad\nvision for RoI captioning. In addition, ROCSU can capture\nmore important information. It is also noted that the TDC\nmethod by itself also achieves 10.64, which surpasses the state\u0002of-the-art methods. This clearly demonstrates the suitability of\nthe Transformer-based model. On top of that, TDC+ROCSU\noutperforms TDC by a 0.85 mAP increase, which shows the\nimportance of ROCSU.\n3) Results on VG V1.2 Dataset: We also evaluate our\nproposed TDC+ROCSU method on the VG V1.2 dataset. As\nTABLE IV\nThe mAP (%) performance of ablation studies on VG-COCO Dataset\nMethod mAP(%)\nTDC 11.47\nTDC+img+RoI 9.50\nTDC+RoI 10.24\nTABLE V\nThe mAP (%) performance of different ROCSU weighting schemes on VG\nV1.0 dataset\nMethod mAP(%)\nROCSUN orm 9.25\nROCSUOnes 9.82\nwith the VG V1.0 experiments, we adopted the same settings\nas [14], [19]. The mAP results are shown in Table III. It can\nbe observed that the TDC+ROCSU method obtains a relative\ngain of 14.5% on VG V1.2 with an mAP of 11.90, compared\nwith the state-of-the-art COCG (10.39). It is worth noting that\nthe mAP achieved by our method is more than twice the mAP\nof the FCLN method. Furthermore, the TDC method without\nour contributed ROCSU achieves 10.33, which is very close\nto COCG. However, it is still far (around 15%) from the\nTDC+ROCSU method, which again shows the effectiveness\nof ROCSU.\n4) AP Values Comparison with Different Threshold Combi\u0002nations: Fig. 5 shows quantitative comparisons between the\nbaseline (COCG) and TDC+ROCSU. With the Meteor thresh\u0002old of 0, our TDC+ROCSU method achieves a significant\nimprovement. This is mainly because ROCSU can make the\nmodel focus on RoIs with more information. Furthermore,\nTDC+ROCSU performs better than COCG at nearly all pa\u0002rameters. This shows both the encoding and decoding powers\nof our TDC and the capability of ROCSU to help the model\nto grab the important regions.\n5) Ablation Studies: To validate the effectiveness of our\nROCSU component, we remove it and only leave TDC with\nthe same feature encoding method as TDC+ROCSU, which\nis denoted as TDC. We can see the value drops by 0.11 due\nto the equal weights of each RoI allocated during the training\nstage as the regions that deserve higher priorities are not used.\nTo validate the function of comprehensive feature encoding,\nwe also propose a wide range of experiment settings as shown\nin Table IV. We maintain TDC and adopt different ways of\nfeature encoding. For example, the configuration of image and\nRoI features with TDC is defined as TDC+image+RoI. It is\nobvious that with object guidance, the performance improves\nsharply by 1.97 whilst TDC+Img+RoI achieves even worse\nresults than TDC+RoI possibly because the image features\nmay be too compact to understand, and thus, weaken its own\nfunction to guide dense captioning. To better clarify why\nTDC+ROCSU can achieve better dense captioning ability, we\nalso illustrate an example and analyze the reason in depth in\nthe next section.\nFurthermore, to validate the effectiveness of ROCSU setting\nin Eq. 8. We have also adopted two more kinds of ROCSU\nweighting schemes as shown in Table V. The first one is as\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 9\nfollows:\nROCSUNorm =\nBW + IoU(ri, Bobj)confobj\nT\nPNrg\ni=1 IoU(ri\n, Bobj)\n, (9)\nwhere Nrg is the total number of RoIs in the RoI batch, other\nvectors and actors are the same with Eq. 8. This weighting\nscheme is denoted as ROCSUNorm. It is observed that it\nonly achieves an mAP of 9.25, only 80% of ROCSU when\nusing Eq. 8. This is mainly due to the weakened value by\nthe normalization term, which undermines the function of\nROCSU.\nAnother weighting scheme we adopted is denoted as\nROCSUOnes. It differs from Eq. 8 in the value of BW.\nFor ROCSUOnes, we adopted a matrix of all ones as the\nbasic weight of each RoI. The performance of ROCSUOnes\nis better than ROCSUNorm with an mAP of 9.82. It is still\nlower than ROCSU setting using Eq. 8, which demonstrates\nthe superiority of the chosen ROCSU score function.\nC. Qualitative Results and Analysis\nIn this section, we show qualitative results and analysis to\nhelp evaluate the experimental results in a more subjective\nway. In the first subsection, we present four examples from\nVG-COCO, VG V1.0 and VG V1.2 dataset respectively with\nthe visualisation of all RoIs and the descriptions of them. In\nthe second subsection, we will display results, in comparison\nwith the COCG method and also the provided ground truth.\n1) Examples of RoIs and Captions by TDC+ROCSU:\nFour complete examples of dense captioning results by\nTDC+ROCSU targeted on an image are shown in Fig. 6. From\nthis visualization, we can clearly see the decent quality of both\nlocalizations and captions of RoIs achieved by TDC+ROCSU.\nTo begin with, the model is able to capture the grammar of\nnatural languages fairly well. A majority of the generated\nsentences comply with plain English grammar recognised by\nhumans and are completely readable and understandable. We\nshould owe this to the powerful encoding and decoding ca\u0002pability to learn representative features in order to correspond\nwith visual and language clues as well as be aware of intra\u0002modality connections with each other. Furthermore, it is easy\nto see the proposed model has a very good command of\ncommonly used ways of description (e.g., in the first example,\n\u2019with structure\u2019 is used three times correctly. This attributes\nto the function of ROCSU. Its aim is to attend more on\nRoIs overlapping more with objects. As we all know, \u2019with\nstructure\u2019 can easily bridge multiple entities together so it is\nmore likely to occur in the RoIs with more attention. Hence, a\ngood command of \u2019with structure\u2019 complies with the doctrine\nof ROCSU.\n2) Ablation Studies: To have a discussion about the exper\u0002imental results of TDC+ROCSU and TDC in depth, in this\nsection, we will analyze the importance of each part of our\ncontributions, TDC and ROCSU separately. To be specific,\nwe provide the top-5 visualization results according to region\nconfidence of both TDC+ROCSU and TDC methods with the\nobject detection results in the same image from VG-COCO as\nshown in Fig. 7 although we have given quantitative analysis\nin the last section.\nIn Fig. 7, it is clear that due to the power of TDC to process\nsequential data, both methods can generate decent captions for\ncommon regions in the dataset that only describe the action of\na person (\u2018a man skiing on the mountain in the middle\u2019 and\n\u2018trees covered in snow\u2019 at the left top of the image). The only\ndifference is TDC+ROCSU provides the \u2018pine trees\u2019, which\nis more detailed. The region almost has no overlap with the\nobjects detected in (c) and according to Eq. 8, there are no\nextra weights on this region while training by TDC+ROCSU.\nHowever, these good results, to a large extent, come from\nplenty of training samples from images with similar scenes in\nthe dataset.\nFurthermore, there are two examples showing that ROCSU\nworks better if the given region has more overlaps with objects,\nthus enabling the ROCSU to give more priority to this region\neven though it is focusing on more detailed information.\nSpecifically, ROCSU helps the machine to recognise the red\nhat for the orange region instead of the helmet in the results\nof the TDC method due to more weights allocated to (a)\nduring the training stage than (b), which derives from more\noverlaps with objects (specifically the IoU with the person\nwith a score of 1.000) in (c) than in (b). Also, based on the\nsame explanation, with the aid of ROCSU, it can benefit from\nthe bigger weight so that it is relatively easier to recognise the\ncolour of the jacket (yellow not brown and yellow in (b)) on\nthe man.\nFinally, from the red box in (a), it is easy to observe that for\na given region that corresponds with different semantics in the\nimage, ROCSU can show its superiority due to a high weight\nin the training from the summation of overlap with different\ndetected objects. Because of this, ROCSU can encourage the\ngeneration of captions that link different semantics in the\nimage in order to create more comprehensive descriptions that\nare likely to reveal the theme of the whole image rather than\ndetailed descriptions.\n3) Results with COCG and the Ground Truth: Fig. 8 shows\nthe comparison results of our TDC+ROCSU method and the\nground truth as a reference to measure their performance in\nrandomly sampled RoIs. From these results, it is also visible\nthat TDC+ROCSU performs better in both localisation and\ndescription of RoIs. This can be reflected by higher IoUs and\nMeteor displayed in the graph. It is noted that TDC+ROCSU is\nlikely to accurately find the salient semantic in ground truth.\nIt might be due to the joint ability of captioning modelling\nby both TDCs that learns better feature representation and\ntheir relationships and ROCSU, which focuses on RoIs that\nhave more overlaps with semantic objects. We argue that it\nis not proper to owe this superiority to a unique module.\nFor instance, in the first subfigure, without TDC, the close\nrelationship between object surfboard and woman cannot be\nperfectly built up. Instead, it may suffer from the forget\nshortcoming like LSTM methods, losing the guidance from\nthe word surfboard. Without ROCSU, this kind of informative\nRoI may not gain a priority, therefore causing a decrease in\nperformance.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 10\na large green tree\nwoman\nwearing a\nlight blue\nshirt\ngreen grass\non the field\nwomen\nwearing\nblack shorts\ngreen grass\non the\nground\na grassy\nfield\nwoman\nwith\nlong hair\nwoman with\nblonde hair\na woman\nwearing red\nshirt\nThe girl is wearing black shoes\nwoman\nwith\nred hair\npeople\nsitting in\nthe grass\npeople\nwatching\nthe game\ntwo women playing frisbee\na green\nfence\nthe arm of\na man\nwoman\nwearing\nblack\nshirt\na black\nshirt on a\nwoman\na white\nfrisbee\na long\nwooden\nfence\na\nmountain\nrange\npeople\nskiing\ndown a\nmountain\na clear blue sky\nmountains\nin the\ndistance\na\nmountain\nin the\ndistance\norange pole on the\nground\nsnow covered ground\nperson wearing\norange jacket\nperson skiing\ndown a hill\nperson\nskiing\ndown a\nmountain\npeople\nskiing\ndown a\nhill\na large\nbuilding\nwindows of a building\nwhite clouds in the sky\nthe sky is cloudy\na tall\nbuilding\na building\nwith many\nwindows\na group of\ntall\nbuildings\na tall tower\nthe water is calm\na tall\nclock\ntower\nclock on\nthe tower\na bridge\nover water\nclock\non the\ntower\na toy of orange\na\nwooden\nfloor\na baby a head of baby\na baby wearing a\npajamas\ntoy on\nthe floor\nA purple cell phone\nwooden floor\na baby sitting on a\nbed\nFig. 6. Detected RoIs with their corresponding captions by TDC+ROCSU of three different datasets: VG-COCO, VG V1.0 and VG V1.2. Specifically, two\nexamples at the top are from VG-COCO, whilst the left bottom one from is VG V1.0 and the right bottom is from VG V1.2.\n(a) (b)\nperson\n1.000 person\n0.966\nskis 0.744\nperson\n0.999\nperson\n0.936\nskis 0.744\nred and\nblack ski\nhelmet\nskier in\nred jacket\ntrees\ncovered in\nsnow brown\nand black\njacket\na man\nskiing on\nthe\nmountain\nred hat on\nthe head\na group of people\non ski slope\npine trees\ncovered in\nsnow\na man\nwearing a\nyellow\njacket a man\nskiing on\nthe\nmountain\n(c)\nFig. 7. Dense captioning results of TDC+ROCSU and TDC method on VG-COCO dataset along with their object detection results. (a). Dense captioning\nresults of TDC+ROCSU (Top-5 results according to confidence). (b). Dense captioning results of TDC (Top-5 results according to confidence). (c). Object\ndetection results of the same image.\nV. CONCLUSION\nIn this paper, a novel end-to-end trainable Transformer\u0002based Dense Captioning Captioner (TDC) was proposed to\nfacilitate the encoding and decoding of both visual and lan\u0002guage features. This TDC can encode and decode both visual\nfeatures and language features effectively with the guidance\nof object detection information. To make the model pay more\nattention to the detected RoIs with more information, particu\u0002larly, we proposed another innovative unit, named ROCSU, to\nmeasure the importance of an RoI. Doing so allows the model\nto give higher priority to them, thus learning more useful\nknowledge. Experiments on several public datasets show that\nthe TDC+ROCSU method outperforms the state-of-the-art\nsignificantly. This framework is easily to be transplanted to\nsimilar applications due to its flexibility. In our future work,\nwe will apply the proposed TDC+ROCSU to the application\nof image captioning, dense video captioning [44] etc. though\nthere might be some changes for ROCSU module according\nto the specific task.\nREFERENCES\n[1] Y. Miao, Z. Lin, X. Ma, G. Ding, and J. Han, \u201cLearning transformation\u0002invariant local descriptors with low-coupling binary codes,\u201d IEEE Trans\u0002actions on Image Processing, vol. 30, pp. 7554\u20137566, 2021.\n[2] Z. R. Khavas, S. R. Ahmadzadeh, and P. Robinette, \u201cModeling trust\nin human-robot interaction: A survey,\u201d in International Conference on\nSocial Robotics (ICSR). Springer, 2020, pp. 529\u2013541.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 11\nthe woman is holding a surfboard\na woman wearing a wetsuit\n(IoU: 0.77, Meteor: 0.19)\nwoman holding a surfboard\n(IoU: 0.86, Meteor: 0.41)\nman wearing black knee pads\nblack shorts on a man\n(IoU: 0.75, Meteor: 0.16)\nblack knee pads\n(IoU: 0.78, Meteor: 0.31)\nFence posts in sand\na wooden beach\n(IoU: 0.76, Meteor: 0.04)\na wooden fence\n(IoU: 0.87, Meteor: 0.13)\nflower patten on the man\u2019s shorts\nman wearing shorts\n(IoU: 0.85, Meteor: 0.18)\nshorts on the man\n(IoU: 0.87, Meteor: 0.27)\nFig. 8. Qualitative comparisons between baseline (COCG) and our method (TDC+ROCSU). The green box refers to the ground truth, the red box and the\nblue box are the prediction results of COCG and TDC+ROCSU respectively (Best viewed in color).\n[3] J. Cao, Y. Pang, J. Han, and X. Li, \u201cHierarchical regression and clas\u0002sification for accurate object detection,\u201d IEEE Transactions on Neural\nNetworks and Learning Systems, 2021.\n[4] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, \u201cObject detection with\ndeep learning: A review,\u201d IEEE Transactions on Neural Networks and\nLearning Systems, vol. 30, no. 11, pp. 3212\u20133232, 2019.\n[5] Y. Liu, D. Zhang, Q. Zhang, and J. Han, \u201cPart-object relational visual\nsaliency,\u201d IEEE Transactions on Pattern Analysis and Machine Intelli\u0002gence, 2021.\n[6] H. Chen, G. Ding, X. Liu, Z. Lin, J. Liu, and J. Han, \u201cImram: Iterative\nmatching with recurrent attention memory for cross-modal image-text\nretrieval,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020, pp. 12 655\u201312 663.\n[7] X. Xu, T. Wang, Y. Yang, L. Zuo, F. Shen, and H. T. Shen, \u201cCross\u0002modal attention with semantic consistence for image\u2013text matching,\u201d\nIEEE Transactions on Neural Networks and Learning Systems, vol. 31,\nno. 12, pp. 5412\u20135425, 2020.\n[8] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, \u00a8\nH. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn\nencoder-decoder for statistical machine translation,\u201d in Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing\n(EMNLP), 2014, pp. 1724\u20131734.\n[9] A. Karpathy and L. Fei-Fei, \u201cDeep visual-semantic alignments for\ngenerating image descriptions,\u201d in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3128\u2013\n3137.\n[10] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, \u00b4\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al., \u201cFrom captions to\nvisual concepts and back,\u201d in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2015, pp. 1473\u2013\n1482.\n[11] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 12\nand Y. Bengio, \u201cShow, attend and tell: Neural image caption generation\nwith visual attention,\u201d in International Conference on Machine Learning\n(ICML), 2015, pp. 2048\u20132057.\n[12] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, \u201cImage captioning\nwith semantic attention,\u201d in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016, pp. 4651\u2013\n4659.\n[13] J. Lu, C. Xiong, D. Parikh, and R. Socher, \u201cKnowing when to look:\nAdaptive attention via a visual sentinel for image captioning,\u201d in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017, pp. 375\u2013383.\n[14] X. Li, S. Jiang, and J. Han, \u201cLearning object context for dense caption\u0002ing,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence\n(AAAI), vol. 33, 2019, pp. 8650\u20138657.\n[15] J. Johnson, A. Karpathy, and L. Fei-Fei, \u201cDensecap: Fully convolutional\nlocalization networks for dense captioning,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition (CVPR), 2016,\npp. 4565\u20134574.\n[16] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time\nobject detection with region proposal networks,\u201d in Advances in Neural\nInformation Processing Systems (NeurIPS), 2015, pp. 91\u201399.\n[17] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\ncomputation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[18] L. Yang, K. Tang, J. Yang, and L.-J. Li, \u201cDense captioning with joint\ninference and visual context,\u201d in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2193\u2013\n2202.\n[19] G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, and J. Shao, \u201cContext\nand attribute grounded dense captioning,\u201d in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2019,\npp. 6241\u20136250.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances\nin Neural Information Processing Systems (NeurIPS), 2017, pp. 5998\u2013\n6008.\n[21] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese,\n\u201cGeneralized intersection over union: A metric and a loss for bounding\nbox regression,\u201d in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019, pp. 658\u2013666.\n[22] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille, \u201cDeep captioning\nwith multimodal recurrent neural networks (m-rnn),\u201d in International\nConference on Learning Representations (ICLR), 2015.\n[23] R. Kiros, R. Salakhutdinov, and R. Zemel, \u201cMultimodal neural language\nmodels,\u201d in International Conference on Machine Learning (ICML),\n2014, pp. 595\u2013603.\n[24] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venu\u0002gopalan, K. Saenko, and T. Darrell, \u201cLong-term recurrent convolutional\nnetworks for visual recognition and description,\u201d in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2015, pp. 2625\u20132634.\n[25] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, \u201cImage captioning:\nTransforming objects into words,\u201d in Advances in Neural Information\nProcessing Systems (NeurIPS), 2019, pp. 11 137\u201311 147.\n[26] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature\nhierarchies for accurate object detection and semantic segmentation,\u201d in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2014, pp. 580\u2013587.\n[27] K. Fu, J. Li, J. Jin, and C. Zhang, \u201cImage-text surgery: Efficient\nconcept learning in image captioning by generating pseudopairs,\u201d IEEE\nTransactions on Neural Networks and Learning Systems, vol. 29, pp.\n5910\u20135921, 2018.\n[28] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, \u201cRelation networks for\nobject detection,\u201d in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2018, pp. 3588\u20133597.\n[29] T. Yao, Y. Pan, Y. Li, and T. Mei, \u201cExploring visual relationship\nfor image captioning,\u201d in Proceedings of the European conference on\ncomputer vision (ECCV), 2018, pp. 684\u2013699.\n[30] P. Sharma, N. Ding, S. Goodman, and R. Soricut, \u201cConceptual captions:\nA cleaned, hypernymed, image alt-text dataset for automatic image\ncaptioning,\u201d in Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL), vol. 1, 2018, pp. 2556\u20132565.\n[31] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, \u201cEnd-to-end\ndense video captioning with masked transformer,\u201d in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2018, pp. 8739\u20138748.\n[32] J. L. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d arXiv\npreprint arXiv:1607.06450, 2016.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016, pp. 770\u2013778.\n[34] J. Cao, Y. Pang, S. Zhao, and X. Li, \u201cHigh-level semantic networks\nfor multi-scale object detection,\u201d IEEE Transactions on Circuits and\nSystems for Video Technology, 2019.\n[35] X. Zhang, J. Zou, K. He, and J. Sun, \u201cAccelerating very deep convolu\u0002tional networks for classification and detection,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 38, no. 10, pp. 1943\u2013\n1955, 2015.\n[36] K. He, G. Gkioxari, P. Dollar, and R. Girshick, \u201cMask r-cnn,\u201d in \u00b4\nProceedings of the IEEE international conference on computer vision\n(ICCV), 2017, pp. 2961\u20132969.\n[37] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Dollar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in \u00b4\ncontext,\u201d in Proceedings of the European conference on computer vision\n(ECCV), 2014, pp. 740\u2013755.\n[38] S. Ruder, \u201cAn overview of gradient descent optimization algorithms,\u201d\narXiv preprint arXiv:1609.04747, 2016.\n[39] K. Miyaguchi and K. Yamanishi, \u201cAdaptive minimax regret against\nsmooth logarithmic losses over high-dimensional l1-balls via envelope\ncomplexity,\u201d in International Conference on Artificial Intelligence and\nStatistics AISTATS, 2019, pp. 3440\u20133448.\n[40] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY. Kalantidis, L.-J. Li, D. A. Shamma et al., \u201cVisual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations,\u201d\nInternational journal of computer vision, vol. 123, no. 1, pp. 32\u201373,\n2017.\n[41] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \u201cImagenet large\nscale visual recognition challenge,\u201d International journal of computer\nvision, vol. 115, no. 3, pp. 211\u2013252, 2015.\n[42] A. Lavie and A. Agarwal, \u201cMeteor: An automatic metric for mt\nevaluation with high levels of correlation with human judgments,\u201d in\nProceedings of the second workshop on statistical machine translation,\n2007, pp. 228\u2013231.\n[43] A. Neubeck and L. Van Gool, \u201cEfficient non-maximum suppression,\u201d in\nInternational Conference on Pattern Recognition (ICPR), vol. 3, 2006,\npp. 850\u2013855.\n[44] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles, \u201cDense\u0002captioning events in videos,\u201d in Proceedings of the International Con\u0002ference on Computer Vision (ICCV), 2017, pp. 706\u2013715.\nZhuang Shao is currently a Ph.D candidate with Warwick Manufacturing\nGroup at University of Warwick, Coventry, UK. He holds a BEng in Electronic\n& Information Engineering (Northwestern Poly-technical University, 2015),\nan MSc in Information & Communication Engineering (Tianjin University,\n2018). His research interests include image captioning, video captioning and\nmachine learning.\nJungong Han is currently a Chair Professor and the Director of the Re\u0002search of Computer Science, Aberystwyth University, U.K. He also holds an\nHonorary Professorship with the University of Warwick, U.K. His research\ninterests include computer vision, artificial intelligence, and machine learning.\nDemetris Marnerides has previously worked as a Research Fellow at the\nWarwick Manufacturing Group (WMG), University of Warwick. He holds a\nBA in Physics (University of Cambridge, 2013), an MSc in Scientific Com\u0002puting (University of Warwick, 2015), and a PhD in Engineering (University\nof Warwick, 2019). His research topics include Machine Learning, Computer\nVision, Image Processing and HDR Imaging.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 13\nKurt Debattista is Professor at WMG, University of Warwick. He holds a\nPhD from the University of Bristol. His research has focused on high-fidelity\nrendering, high-dynamic range imaging, applications of vision, and applied\nperception.",
    "title": "Region-Object Relation-Aware Dense Captioning via Transformer",
    "publication_date": "2024-01-01",
    "cited_by_count": 73,
    "topics": "Visual Question Answering in Images and Videos, Image Feature Retrieval and Recognition Techniques, Human Action Recognition and Pose Estimation",
    "keywords": "Closed captioning, Image Captioning, Object Recognition, Action Recognition, Interest Point Detectors, Feature Matching",
    "concepts": "Closed captioning, Computer science, Transformer, Encoder, Artificial intelligence, Natural language processing, Computer vision, Image (mathematics), Engineering, Voltage, Electrical engineering, Operating system",
    "text_type": "full_text",
    "openalex_rank": 23,
    "num_tokens": 15084,
    "best_oa_location_pdf_url": NaN,
    "pdf_urls_by_priority": [
      "https://wrap.warwick.ac.uk/163920/1/WRAP-Region-object-relation-aware-dense-captioning-transformer-2022.pdf",
      "http://pure.aber.ac.uk/ws/files/50445086/TNNLS_Zhuang.pdf",
      "https://pure.aber.ac.uk/portal/files/50445086/TNNLS_Zhuang.pdf"
    ],
    "successful_pdf_url": "https://wrap.warwick.ac.uk/163920/1/WRAP-Region-object-relation-aware-dense-captioning-transformer-2022.pdf",
    "url": "https://wrap.warwick.ac.uk/163920/1/WRAP-Region-object-relation-aware-dense-captioning-transformer-2022.pdf",
    "snippets": {
      "s1": "warwick.ac.uk/lib-publications Manuscript version: Author\u2019s Accepted Manuscript The version presented in WRAP is the author\u2019s accepted manuscript and may differ from the published version or Version of Record. Persistent WRAP URL: http://wrap.warwick.ac.uk/163920 How to cite: Please refer to published version for the most recent bibliographic citation information. If a published version is known of, the repository item page linked to above, will contain details on accessing it. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. Copyright \u00a9 and all moral rights to the version of the paper presented here belong to the individual author(s) and/or other copyright owners. To the extent reasonable and practicable the material made available in WRAP has been checked for eligibility before being made available. Copies of full items can be used for personal research or study, educational, or not-for-profit purposes without prior permission or charge. Provided that the authors, title and full bibliographic details are credited, a hyperlink and/or URL is given for the original metadata page and the content is not changed in any way. Publisher\u2019s statement: Please refer to..",
      "s2": "..the repository item page, publisher\u2019s statement section, for further information. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Region-object Relation-aware Dense Captioning via Transformer Zhuang Shao, Jungong Han, Demetris Marnerides, Kurt Debattista Abstract\u2014Dense captioning provides detailed captions of complex visual scenes. While a number of successes have been achieved in recent years, there are still two broad limitations: 1) Most existing methods adopt an encoder-decoder framework, where the contextual information is sequentially encoded using Long Short-Term Memory (LSTM). However, the forget gate mechanism of LSTM makes it vulnerable when dealing with a long sequence; 2)The vast majority of prior arts consider Regions of Interests (RoIs) equally important, thus failing to focus on more informative regions. The consequence is that the generated captions cannot highlight important contents of the image, which does not seem natural. To overcome these limitations, in this paper, we propose a novel end-to-end transformer-based dense image captioning architecture, termed Transformer-based Dense Captioner (TDC). TDC learns the mapping between images and their dense captions via a Transformer,..",
      "s3": "..prioritising more informative regions. To this end, we present a novel unit, named Region-Object Correlation Score Unit (ROCSU), to measure the importance of each region, where the relationships between detected objects and the region, alongside the confidence scores of detected objects within the region, are taken into account. Extensive experimental results and ablation studies on the standard dense-captioning datasets demonstrate the superiority of the proposed method to the state-of-the-art methods. Index Terms\u2014Dense Image Captioning, Transformer-based Dense Image Captioner, Region-Object correlation score unit I. INTRODUCTION D Ense captioning has gained significant attention from both the engineering and research communities recently. On the one hand, it facilitates important practical applications [1], such as human-robot interaction [2], navigation for the blind, object detection [3] [4] or segmentation [5] and imagetext retrieval [6] [7]. On the other hand, it poses substantial challenges to both computer vision and natural language processing research communities. Its complexity in generating richer and more detailed descriptions for local regions, compared to image captioning, hastens the emergence of more advanced captioning techniques. Dense captioning stems from image captioning, and recent years have witnessed a rapid development of image captioning Manuscript..",
      "s4": "..received xxx, xxx; revised xxx, xxx and xxx, xxx; accepted xxx, xxx. (Corresponding author: Jungong Han). This research was supported by the funds of China Scholarship Council under Grant No. 201909120012. Zhuang Shao is with Warwick Manufacturing Group, University of Warwick, CV4 7AL, UK (e-mail: ZhuangShao@warwick.ac.uk). Jungong Han is with the Department of Computer Science, Aberystwyth University, SY23 3DB, UK (e-mail: jungonghan77@gmail.com). Demetris Marnerides is with Warwick Manufacturing Group, University of Warwick, CV4 7AL, UK (e-mail: dmarnerides@gmail.com). Kurt Debattista is with Warwick Manufacturing Group, University of Warwick, CV4 7AL, UK (e-mail: K.Debattista@warwick.ac.uk). a green plastic chair and three pegions a bird on a building (a) bird 0.983 bird 0.948 bird 0.998 chair 0.726 (b) Fig. 1. (a) An example of the RoI description created by the LSTM method COCG [14]. (b..",
      "s5": "..) The corresponding object detection results as context to guide the dense captioning. techniques. Many of these methods are based on encoderdecoder frameworks and inspired by the successful transfer of sequence to sequence training used for machine translation [8]. Broadly, image features are first extracted by a Convolutional Neural Network (CNN) as an encoder, and then fed into an RNN-based decoder that outputs the corresponding captions. However, such a captioning mechanism based on encoderdecoder frameworks fails to focus on areas that may be worthy of more attention at the training stage. To address this issue, many updated methods have been proposed. For example, [9] proposed aligned high-level information while [10]\u2013[13] resorted to different forms of attention to aid guidance during training. Dense captioning is beyond image captioning due to the need to provide richer and more detailed descriptions for a given image. [15] took the initiative to develop a Fully Convolutional Localization Network (FCLN) for the dense captioning task, in which Regions of Interests (RoIs) are localized before being described. Afterwards, many follow-ups appeared, which can be generally categorized into two classes depending on whether the contextual information encoded in the model is used. At the early stage, the architecture was composed..",
      "s6": "..of a Faster Region-based Convolutional Network (R-CNN) [16] module to detect RoIs and describe them with a Long Short-Term Memory (LSTM) [17], which was an advanced variant of Recurrent Neural Network (RNN). Unfortunately, this kind of framework only considered the RoIs but ignored possible contextual information that can be leveraged to improve training. To address this problem, [18] proposed to integrate the RoI features with image features as a global context to build up a joint and contextual fusion before captioning via an LSTM. However, the proposed global context seems too coarse, and there have been several methods that explored fine-grained contexts. For instance, [19] proposed a non-local similarity graph for the feature interaction between IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 2 the target RoI and its neighboring RoIs. Also, supported by data statistics, [14] revealed the close relationship between RoIs and detected objects via object detection, thus resulting in an architecture with contextual information considered. Despite the preliminary success of the aforementioned methods, dense image captioning can, arguably, be considered still in its infancy. We believe a number of limitations still exist, two of which are critical. Firstly, LSTMs,..",
      "s7": "..as the dominant structures for the methods mentioned above, suffer from the nature of the forget gate mechanism: forgotten information after a sequence cannot be avoided, especially when the inputted sequence is long. In state-of-the-art methods, if the contextual information is encoded by an LSTM, and with time rolling, the initial object would be \u201cforgotten\u201d and thus it weakens the guidance function of context especially when there are interactions of multiple people and multiple objects. Hence, the training model may fail to \u201coversee\u201d the objects so that it cannot guide the captioning process properly. As a result, this kind of gap often gives rise to the missing of descriptive objects, as illustrated in Fig. 1. Obviously, the object detection results as guided context for dense captioning are in good conditions, with accurate localizations and high confidence on the right. However, on the left, due to the aforementioned deficit of LSTM, the output caption does not include all three birds and a chair in its answer. Instead, it generates only a bird on a building, but forgets the other two birds and the chair. Secondly, in the previous methods, e.g. [14], all the RoIs are treated with equal weights during training. However, in the real world,..",
      "s8": "..the useful information carried by each RoI can be hugely different. Also, the detection confidence scores of objects within and around the region may vary considerably from region to region. These all imply that the regions should be treated differently during model training. As shown in Fig. 2, it may make more sense if assigning larger weights to the RoIs with more information at the training stage. Concretely, in this example, on the left are two RoIs detected, but apparently, they have different IoUs with the overall object bounding boxes illustrated on the right. According to the descriptive languages of these RoIs, it is obvious that the caption of the one in red with a higher IoU with the objects on the right. Also, it contains much more information in its ground truth since its description reveals the theme of the image. In contrast, the RoI in yellow contains too detailed information and this kind of information is even far difficult for the human being to observe, not to mention attain it by machine learning. Inspired by the common exam strategy that a student should focus more on the basic questions accounting for a large proportion of marks, rather than concentrating on difficult ones, we hold a view that the informative regions deserve more priorities. To alleviate the first issue, we propose a..",
      "s9": "..novel end-to-end dense captioning framework based on Transformer [20], which is currently popular in a great variety of computer vision tasks, termed Transformer-based Dense Captioner (TDC), to overcome the limitations of the forget mechanism of LSTM when encoding and decoding visual and language information. Fig. 3 gives an overview of TDC. Particularly, inspired by [14], we compose both object detection information and holistic image features as context. Along with the detected RoIs from Faster R-CNN Region Proposal Network (RPN) and contextual information, the visual information is projected into a visual representation by applying a dot product between them. The same operation is implemented on language information as well. At the decoding phase, a probability distribution for captions of detected RoIs is learnt by cross-modality attention of both visual and language encoding results. During encoding and decoding, all of the input vectors are aligned and computed together, hence it can overcome the forget problem. In order to address the second limitation, we propose a module, which allocates weights for the language loss of each region at each step of training. The underlying assumption is that the regions comprising more objects with high detection confidence scores are more important, and thus, deserve priority. To this end, we propose a..",
      "s10": "..novel unit, which makes use of both the object detection score and the intersection of union [21] (IoU), named Region-Object Correlation Score Unit (ROCSU). The major contributions of this work are summarized as: \u2022 A novel end-to-end dense captioning framework based on the Transformer, dubbed TDC, is proposed. A distinct property of TDC is the advocate of a Transformer to capture the long-range contextual information among objects. It is clearly advantageous over LSTM that is impotent in capturing long-range dependencies among objects. To the best of our knowledge, this is the first work that builds up a Transformer-based architecture rather than an LSTM for the dense captioning topic. \u2022 An RoI importance unit, named Region-Object Correlation Score Unit (ROCSU), drives the loss function to focus more on RoIs with more information. In doing so, our work, for the first time, weighs RoIs by jointly considering object-region relationships and object detection confidence scores during model training. It differs from treating each RoI equally at the training stage. \u2022 Extensive experimental results on different challenging datasets show the superiority of the proposed method against the state-of-the-art methods. The rest of this paper is organized as follows: We..",
      "s11": "..discuss related work in Section II. In Section III, the proposed method is introduced in detail with a comprehensive analysis. Extensive experimental results are demonstrated in Section IV with both qualitative and quantitative analysis. Finally, we summarize this paper with a conclusion in Section V. II. RELATED WORK In this section, we will review the related works from two aspects: image captioning and dense captioning. A. Image Captioning Earlier neural network models for image captioning [11], [22]\u2013[24] encoded visual information using a single feature representation of the image [25] with very limited additional information. However, with the development of deep learning, more auxiliary information can be added up into a model IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3 people skiing on the snow A ski hat (a) Person 0.999 Skis 0.984 Person 0.998 Person 0.998 Person 0.998 (b) Fig. 2. An example shows the RoIs with different IoUs should be weighted differently. (a) Two RoIs and their descriptions; (b) Object detection results. \u2022\u2022\u2022 Object Detection Image feature Object Context Features \u2022\u2022\u2022 Object Features \u2022\u2022\u2022 Geometry Features Dense captions A boy throwing baseball Two men are watching RoI detector (RP..",
      "s12": "..N) Pre-trained Resnet152 RoI \u2022\u2022\u2022 Features Off-line Pre-trained Context Module Visual (Transformer) Encoder NRoI \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 A boy baseball Two men watching Captioning (Transformer) Decoder Word Embedding A boy throwing baseball Two men are watching \u2022\u2022\u2022 Fig. 3. The proposed TDC framework is made up of an RoI detector, context module, visual encoder and captioning decoder. Given an image, the RoI detector detects RoIs and the context module prepares contextual information generated via the pre-trained object detector for further use. After this, the visual encoder encodes visual information by attention, which gains a visual representation. Finally, after the word embeddings are conducted, visual representation and sentence information are decoded by the captioning decoder to generate dense captions for each RoI. structure. [9] extracted region features from images with an RCNN object detector [26] and generated separate captions for the regions as the captions of the given image. [10] proposed a method to generate image descriptions by first detecting words associated with different regions within the image [25]. In addition, [27] proposed an efficient concept learning module to get pseudo pairs. To..",
      "s13": "..better focus on important parts of images and model their correspondent relations with words in captions, a series of variants of attention models have been incorporated. [12] proposed a semantic attention module, which combines the topdown and bottom-up attention together. Also, [28] involved geometric attention, which inspired [29] to develop a framework with two Graph Convolutional Networks to explore visual relationships. In recent years, with the advance of Natural Language Processing (NLP), the Transformer architecture [20] has led to significant performance improvements for various tasks. [30] proposed a Transformer-based model by extracting a single global image feature from the image as well as uniformly sampling features by dividing the image into 8x8 partitions. In the latter case, the feature vectors were fed in a sequence to the Transformer encoder [25]. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 4 B. Dense Captioning Later on, dense captioning [15] emerged as a new task that requires an intelligent vision system to both localize and describe salient regions within an image in natural language. Existing dense captioning algorithms can be roughly categorized into two types: captioning with the guidance of contextual information and captioning without using contextual information. 1) Dense Captioning Without Context: In [15], Johnson..",
      "s14": "..et al. proposed a bilinear interpolation with a prototype of an RPN in Faster R-CNN. All the RoIs are represented by the same-size features, denoted as region features. Subsequently, they are passed through a fully-connected layer to determine if they are foreground (the descriptive region) or background. The locations of these regions are also amended at this stage via regression. At a later stage, region features are described by an LSTM language model, which is trained in an end-toend manner. 2) Dense Captioning With Context: The work in [18] is conceptually similar to [15]. But the difference lies in that the image feature acted as the contextual information, which was fed into the captioning module together with RoIs. Despite an improved performance, the contextual information is just a kind of global and coarse information, thus leading to the failure to encode more detailed context information. Subsequent works attempted to incorporate fine-grained context into the framework. For instance, [19] established a non-local similarity graph for the feature interaction between the target RoIs and its neighboring RoIs. Furthermore, it is noted that in [14], the authors argued that objects provide valuable cues to help locate captioning regions and generate descriptions for them via the..",
      "s15": "..use of data statistics. Inspired by this, the authors proposed to bring in local contextual information to guide the training of the model. To capture useful object information in an image, a novel framework for learning a complementary object context for each RoI was proposed using an LSTM. This context is derived from a concatenation of extracted object features and geometry information. The LSTM cell progressively accepts each object as input and decides whether to keep it or discard it. In the end, the context is also used as guidance information to help generate the descriptions and predict the bounding box offsets. A close look at the method in [14] reveals that the entire algorithm carries out an encoding-decoding procedure. In the encoding procedure, the representations of each contextual object fused with its CNN feature and geometry features (relative coordinates) are encoded step by step with a guidance LSTM, where the guidance information is composed of region features. The output of this procedure is the contextual information denoted as ci. For the decoding procedure, the authors tried two kinds of caption decoder frameworks, namely context as guidance (COCG) and context is decoded with an LSTM (COCD), respectively. Although they both have a caption LSTM for captioning as well as a location LSTM for localization, the main difference..",
      "s16": "..between these two decoders is their context decoding architectures. Concretely, COCD adds another LSTM to decode context ci while COCG removes this LSTM and turns the caption LSTM into a guidance LSTM to decode ci. In conclusion, as shown in the section of experiments in [14], the COCG framework outperforms the COCD framework and other methods, thus obtaining the state-of-the-art results due to the alleviation of the vanishing gradient problem by the guidance LSTM unit inside. III. METHODOLOGY In this section, we first briefly describe the popular Transformer architecture, which is a fundamental component of our method. Then, we present the framework of our proposed TDC. Finally, we elaborate on the proposed ROCSU loss adaptation. A. Preliminary Review of Transformer 1) Scaled dot-product attention: The scaled dot-product attention is a basic component of the Transformer [20] architecture. Given a query qi \u2208 Rd in all T queries, a group of keys kt \u2208 Rdand values vt \u2208 Rd, where t = 1, 2, ..., T, the output of dot-product attention is the weighted sum of the vt values. The weights are determined by the dot-products of query qi and..",
      "s17": "..keys kt. Specifically, kt and vt are placed into respective matrices K = (k1, ..., kT ) and V = (v1, ..., vT ) [31]. The output from a query qiis as follows: A(qi, K, V ) = V exp (KTqi/ \u221a d) PT t=1 exp (k T t qi/ \u221a d) , (1) where d is the dimension of qi and \u221ad is to normalize the dotproduct value. To capture detailed features of the input, an additional component called multi-head attention is introduced. The multi-head attention is composed of H parallel partial dot-product attention components, {hj |j \u2208 [1, H]} refer to heads, with each head being independent. The realization of the attention resulting from the multi-head attention (MA) is given by: MA(qi, K, V ) = concat(h1, h2, ..., hH)WO, hj = A(W q j qi, W K j K, WV j V ), (2) where W q j , W K j , WV j denote the transfer weight matrices q,..",
      "s18": "..K, V for hj . WO is the weight matrix for each head. All of these weights are learned during training. This formula of attention is generic so that it can represent two kinds of attention according to where its input comes from. Specifically, when the query is from the decoder layer, and meanwhile, both the keys and values come from the encoder layer, it represents the mutual attention due to its cross-module attribute. The second multi-head attention is called self-attention, where the queries, keys, and values keep unchanged in both encoder and decoder. 2) Transformer: We now present the use of the Transformer on top of scaled dot-product attention. The basic unit of the Transformer is multi-head attention with feed-forward layers followed by layer normalization [32]. The feed-forward layers map the output of the multi-head attention layer by two linear projections and an Rectified Linear Unit (ReLU) as the activate function. The encoder and decoder of the Transformer are composed of multiple basic structures, and usually, their layer IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 5 Self-Attention Layer V (Q) V(K) V(V) Feed-Forward Visual Encoder (Layer=2) V\u2019 Self-..",
      "s19": "..Attention Layer S(Q) S(K) S(V) Cross module attention S\u2019 Feed-Forward V\u2019 V\u2019 FC layer and softmax Caption Decoder (Layer=2) positional encoding + Visual Input positional encoding + Sentence Input + Fig. 4. Transformer structure in our dense captioning scenario, where the layer normalization is omitted. numbers are the same. The decoder of each layer takes the output of the corresponding encoder along with the output of the lower layer decoder output. Self-attention exists in both encoder and decoder. Cross-module attention between encoder and decoder is also applied in the decoder. Residual connection [33] and layer normalization [32] are implemented to all layers. Furthermore, because there is no recurrence module in a Transformer, to indicate positions for each vector, positional encoding (PE) of the input is used. PE occurs at the bottom of the multi-layer Transformer-based encoder and decoder stacks. The dimension of PE is the same as the input, so PE embedding can be added directly to the input. The realization of PE is as follows: P E(pos, 2i) = sin(pos/100002i/d), P E(pos, 2i + 1) = cos(pos/100002i/d..",
      "s20": "..), (3) where pos is the position of the embedded vector inside the input matrix, and i is the dimension of the encoded element in the input matrix, d is the total dimension of the input matrix. B. Transformer in Dense Captioning Scenario Fig. 4 shows the structure of the Transformer in this dense captioning scenario. To be specific, in the visual encoder, the input is encoded into visual features plus positional encodings, denoted as V . The self-attention layer takes three V s at the positions of Q, K, V . After the output of the feed-forward layer denoted as V 0 , on the other side, the embedded words plus positional encodings defined as S undergo the same self-attention. At the cross-module attention unit, these two modalities of data interact with each other to gain the output of cross-module attention, which proceeds to feed-forward to learn a captioning probability distribution by fully connected layers and a softmax. C. Transformer-based Dense Captioner In this section, we introduce our novel Transformer-based Dense Captioner. Given an image from an image set I = {I1, I2, ...IN }, our target is to detect an RoI set, denoted as R = {r1..",
      "s21": ".., r2, ...rM} and then describe each of them with corresponding sentence set defined as S = {s1, s2, ...sM}. To achieve this goal, our proposed TDC consists of four parts with different functions, namely RoI detector, context module, visual encoder, and captioning decoder, each being elaborated in the following subsections. For ease of explanation, we omit the positional encodings in the following sections. 1) RoI Detector: Inspired by the success of the Faster RCNN framework in the area of object detection [34], we adopt its Region Proposal Network (RPN) as our RoI detector. This RPN-based RoI detector is trained in an end-to-end manner together with the captioning downstream task to identify whether a region proposal is an RoI to be described. However, our framework not only uses RoI features from RPN; we integrate RoI features with contextual information as introduced in the next sections. Specifically, we use almost the same configuration as [14], however, we replace its backbone structure VGG16 [35] with a ResNet-101 due to its superiority of shortcut structure [33]. In addition, we leverage RoI Align [36..",
      "s22": "..] rather than RoI Pooling due to its better performance for small object detection. Via the RoI detector, given an image in I, we get the RoI set R = {r1, r2, ...rM} and its corresponding RoI feature set, denoted as RF = {rf1, rf2, ...rfM}. 2) Context Module: According to the data statistics in [14], the description of RoIs has a very close relationship with the objects detected in the image, and therefore, the prior knowledge of object detection can provide useful aids as contextual information for dense captioning. Inspired by this and to obtain such prior knowledge, we pre-trained a Faster R-CNN object detection network on the MS COCO dataset [37] with the same operation as [14]. This is used to create contextual information. In this way, we can gain a set of bounding box coordinates of detected objects Bobj = {b1, b2, ...bobjN } with their confidence scores confobj = {conf1, conf2, ...confobjN }. Additionally, to get features of each bounding box, we extract bounding box and image features with a pre-trained ResNet152..",
      "s23": "..network because the deeper neural network can capture more local features and it is more suitable for local bounding boxes. We denote corresponding bounding box features as B = {bf1, bf2, ...bfobjN }. The image features are defined as Imgf = {Imgf1, Imgf2, ...ImgfN }. We also get the geometry information of each object bounding box, namely G = {g1, g2, ...gobjN }. Same as [14], gi, i \u2208 [1, objN ] is the corresponding coordinate and size ratios of bi. We only add up class information ahead. Finally, the information is merged together with image features extracted by a pre-trained ResNet-152 network as contextual information for each RoI detected. 3) Visual Encoder: Given the aforementioned visual features consisting of prepared context and RoI information, there is a visual encoder to learn a combined feature representation. We use both visual features (object features) and geometry information (relative bounding box coordinates in an image, and object class label) as context. These two kinds of features are firstly concatenated together as context encoding. Then, for the feature of each RoI detected, the object context encoding from..",
      "s24": "..the object detection is concatenated with image features as the final context information. For the context dimension, we first concatenate visual features and geometry features, then IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 6 we use a linear layer to align the context with the size of RoI features and image features. For a fair comparison with the state-of-the-art methods, we follow the configuration of [14]. We detect 10 objects for each image. For each RoI detected, we assign the features of these 10 objects as the encoding features of this RoI. First of all, we concatenate B with G to get the potential context for each RoI as BG. Then it is allocated to each RoI and thus we get a context matrix denoted as C \u2208 RM\u00d7objN \u00d7(dF +dG), where dF is the dimension of features and dG is the dimension of geometry information. Because of the different dimensions of object features and RoI features, to align with the image and RoI features and eventually fuse the context information, a linear mapping from RdF +dG to Rdis formulated into: Calign = WcC + b, (4) where Wc and b are weight and bias..",
      "s25": ".., which can be learned in the linear layer for alignment. After we attain Calign, we incorporate it with expanded image feature of given image Ii, whose image feature is Imgfi and RoI feature is Rfi. Finally, we get the visual features F 0 = (f0 1 , ...f 0 T ) \u2208 RM\u00d7T \u00d7d, T = 2 + objN as the input of our visual encoder. The encoding process is as follows: V F l = \u03c6(P F(\u03c9(F l )), \u03c9(F l )); \u03c9(F l ) =   \u03c6(MA(f l 1 , Fl, Fl), fl 1 ... \u03c6(MA(f l T , Fl, Fl), fl T   ; \u03c6(\u03b1, \u03b2) = LayerNorm(\u03b1 + \u03b2); P F(\u03b3) = Ml 2 max(0, Ml 1\u03b3 + b l 1 ) + b l 2 , (5) where \u03c6 is layer normalization on residual output, P F represents the feed-forward unit, which is composed of two linear layers with a nonlinear transformation by an activation function. MA is the multi-head attention that is composed of..",
      "s26": "..H parallel partial dot-product attention components. \u03c9 is the output of assembled multi-head attention with a layer normalization by \u03c6. Ml 1 and Ml 2 are the weights trained for the feed-forward layers, and b l 1 and b l 2 are corresponding biases. For the t ( th) feature vector encoded inside the representation of an RoI, f l t is given as the query to the attention layer and the result is the weighted sum of each f l t , t \u2208 [1, T], which processed all the encoded features for an RoI, from global image feature to local RoI feature. Therefore, the output vector can gather the encoded the information from all kinds of features by rating their relationships one by one. In other words, it makes the encoder with a broad horizon so that it can avoid forgetting information with the bigger picture observed. 4) Captioning Decoder: With visual features encoded, the captioning process is as follows: Y l+1 \u2264t = \u03c6(P F(\u03c9(Y l \u2264t )), \u03c9(Y l \u2264t )); \u03c9(Y l \u2264t ) =   \u03c6(MA((\u03b4(Y l \u2264t )1), Fl, Fl), \u03b4..",
      "s27": "..(Y l \u2264t )1 ... \u03c6(MA((\u03b4(Y l \u2264t )t), Fl, Fl), \u03b4(Y l \u2264t )t   ; \u03b4(Y l \u2264t ) =   \u03c6(MA(y l 1 , Y l, Y l), yl 1 ... \u03c6(MA(y l t , Y l, Y l), yl t   ; p(wt+1|F 0 , Y L \u2264t ) = sof t max(WV Y L t+1), (6) where y 0 i denotes a word token with an embedding dimension WV , and Y l \u2264t = (y l 1 , ..., yl t ) ,wt+1 is the probability of vocabulary bank at time step t+1. \u03b4 is the cross-module attention that uses the current representation of word embedding to attend to the visual representation from the corresponding layer of the encoder. \u03c6 represents the self-attention part in the decoder. However, different from the encoder, its inputs are words. It is noted that the restriction of time step means that the attention is only on the already generated words. D. Training..",
      "s28": "..and Optimization In this section, we introduce the training and optimization details. First, we show the loss function during training. Then in the second subsection, we explain our novel ROCSU. 1) Loss Function: In order to enforce both of the localization of detected RoIs and descriptive captions to be as close as training examples in an end-to-end manner, multiple loss function terms are leveraged during the Stochastic Gradient Descent [38] (SGD) at each training step in a training batch as follows: L = Lcls + Lreg + rgscore \u00d7 Lcaption T , (7) where Lcls is the classification binary cross entropy loss function of Faster R-CNN RPN [16] for RoI detection, Lreg is the smooth l1 loss [39] for coordinate regression of the location of detected RoIs. It is notable that Lcaption is the cross entropy loss of P = {p(wi|F 0 ; \u03b8), i \u2208 [1, max], which is the probability distribution of descriptive sentence for RoIs in the RoI batch, and their ground truth sentences word by word. To allocate different weights for each detected RoI according to its importance, we design a module ROCSU, its output is denoted as..",
      "s29": "..rgscore. We will introduce ROCSU in detail in the next subsection. 2) ROCSU: In this section, we introduce our novel unit ROCSU to measure the region score for each RoI according to its overlap with detected object bounding boxes as follow: Given an RoI riin R = {r1, r2, ...rM} and detected object at a training step, the corresponding rgscorei is computed as follows: rgscorei = BW + IoU(ri , Bobj)confobj T , (8) IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 7 where BW is the basic weight preset for each RoI, IoU is the Intersection of Union between ri and Bobj. To assemble all the rgscoreiinto vector, rgscore can be achieved. IV. EXPERIMENT In this section, we report and discuss the experiments conducted on three public datasets in order to evaluate the dense captioning performance of our proposed method. We use the Visual Genome dataset (VG) [40] and the VGCOCO dataset, which is the intersection of VG V1.2 and MS COCO [37], as the evaluation benchmarks. The choice of datasets is the same as the state..",
      "s30": "..-of-the-art methods [14], [19] for a fair comparison. The details of each dataset as well as the adopted evaluate metrics are elaborated below: 1) VG: Visual Genome currently has three versions: VG V1.0, VG V1.2, VG V1.4. As the state-of-the-art methods have always used VG V1.0 and VG V1.2, we also conduct our experiments on VG V1.0 and VG V1.2. The training, validation and test splits are chosen similarly as [14], [15], [19]. There are 77,398 images for training and 5,000 images for validation and testing [14]. 2) VG-COCO: As demonstrated in [14], the target bounding boxes of VG V1.0 and VG V1.2 are much denser than the bounding boxes in other object detection benchmark datasets such as MS COCO and ImageNet [41]. For example, each image in the training set of VG V1.2 contains an average of 35.4 objects, whilst the average value for MS COCO is only 7.1. To get proper object bounding boxes and caption region bounding boxes for each image..",
      "s31": ".., following the configuration in [14], the intersection of VG V1.2 and MS COCO is used in our paper, which is denoted as VG-COCO in which there are 38,080 images for training, 2,489 images for validation and 2,476 for testing. 3) Evaluation Metrics: For evaluation, to comply with evaluation metrics of the state-of-the-art methods, we use the same metric as in [14], [15], [19] called mean Average Precision (mAP). It measures the precision for both localization and description of RoIs. Following the threshold setting in [15], average precision is computed with combinations of different IoU thresholds (0.3, 0.4, 0.5, 0.6, 0.7) for the evaluation of RoI locations and different Meteor [42] thresholds (0, 0.05, 0.10, 0.15, 0.20, 0.25) for the evaluation of language similarity with the ground truth. In the end, the mean value of these APs is the mAP score. For each test image, top boxes with high confidence after non-maximum suppression [43] (NMS) with an IoU..",
      "s32": "..threshold of 0.7 are generated. The final results are generated by the second round of NMS under the IoU threshold of 0.5. A. Implementation Details The experiments are carried out on Linux Ubuntu Server with an Intel i7-5960X CPU@3.0GHz, 64GB RAM and NVIDIA GTX 2080 Ti GPU. Specifically, in the proposed method, all the image features, RoI features, and object bounding box features consist of 2048 dimensions. The image batch size is set to 1, the detected RoI batch size in a training step is 32, and the maximum iteration is 1, 000, 000 for VG-COCO, TABLE I The mAP (%) performance of dense captioning algorithms on VG-COCO dataset Method mAP(%) FCLN [15] 4.23 JIVC [18] 7.85 Max Pooling [14] 7.86 COCD [14] 7.92 COCG [14] 8.90 ImgG [14] 7.81 COCG-LocSiz [14] 8.76 COCG&GT [14] 9.79 TDC+ROCSU 11.58 ! Fig. 5. Average precision with different Meteor scores and different IoU thresholds on the..",
      "s33": "..VG-COCO dataset. TABLE II The mAP (%) performance of dense captioning algorithms on VG V1.0 dataset Method mAP(%) FCLN [15] 5.39 JIVC [18] 9.31 ImgG [14] 9.25 COCD [14] 9.36 COCG [14] 9.82 CAG-Net [19] 10.51 TDC 10.64 TDC+ROCSU 11.49 TABLE III The mAP (%) performance of dense captioning algorithms on VG V1.2 dataset Method mAP(%) FCLN [15] 5.16 JIVC [18] 9.96 ImgG [14] 9.68 COCD [14] 9.75 COCG [14] 10.39 TDC 10.33 TDC+ROCSU 11.90 and 2, 000, 000 for VG V1.0 and VG V1.2. The learning rate decrease factor is 0.1 at step 480, 000, 640, 000, 800, 000 for VG-COCO, and 1, 200, 000, 1, 500, 000, 1, 800, 000 for VG IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 8 V1.0 and VG..",
      "s34": "..V1.2. The basic learning rate is set to 0.001, momentum is 0.9, and weight decay is 0.0005. The BW is set as a matrix with all values 0.75. It is noted that the RoI detector and object detector are trained separately. The RPN based RoI detector is trained online as a part of the entire architecture, while the object detection framework is pre-trained offline. They cannot be trained together because they are designed for different tasks. RPN is trained for selecting potential RoIs. It is a binary classification and regression problem while the object detector is used to create more comprehensive object information. In addition, this kind of training settings keeps the same with [14] for a fair comparison. B. Quantitative Results and Analysis 1) Results on VG-COCO Dataset: On the VG-COCO dataset, we conduct extensive experiments to compare our approach and other baseline methods. These baselines are categorized into two groups: state-of-the-art methods including Max Pooling, COCD, COCG, ImgG, COCG-LocSiz and COCG&GT in [14] and earlier methods including FCLN [15] and joint inference and visual context fusion (JIVC..",
      "s35": "..) [18]. mAP values are provided in Table I. In the following section, we denote our proposed method as TDC+ROCSU, and the method treats each RoI equally without ROCSU as TDC. Table I shows significant improvement in mAP. First of all, compared with the state-of-the-art LSTM method, i.e. COCG, the mAP increases dramatically by about 30%. The gap between TDC+ROCSU is even larger, reaching almost three times the mAP of the FCLN method. The results demonstrate the superiority of TDC+ROCSU, which comes from the broad horizon gained of TDC in encoding and decoding and the focus on informative RoIs from ROCSU. It should be noted that even against ground truth localization of each RoI plus the state-of-the-art method COCG denoted as COCG&GT, TDC+ROCSU still outperforms it by an 18.28% mAP increase. 2) Results on VG V1.0 Dataset: TDC+ROCSU is also evaluated on the VG V1.0 dataset. In order to have a fair comparison with state-of-the-art methods, we adopted the same setting as used in [14], [19]..",
      "s36": "... The mAP results are shown in Table II. It can be seen that TDC+ROCSU outperforms the state-of-the-art methods by a significant margin on this dataset also. Overall, our method achieves a 17% mAP increase against the COCG method [14]. Furthermore, the comparison with CAG-Net in [12] also shows the superiority of TDC+ROCSU, with 9.32% mAP improvements. The improvement is, to a large extent, due to the Transformer in TDC+ROCSU that can provide a broad vision for RoI captioning. In addition, ROCSU can capture more important information. It is also noted that the TDC method by itself also achieves 10.64, which surpasses the stateof-the-art methods. This clearly demonstrates the suitability of the Transformer-based model. On top of that, TDC+ROCSU outperforms TDC by a 0.85 mAP increase, which shows the importance of ROCSU. 3) Results on VG V1.2 Dataset: We also evaluate our proposed TDC+ROCSU method on the VG V1.2 dataset. As TABLE IV The mAP (%) performance of ablation studies on VG-COCO Dataset Method mAP(%) TDC 11.47..",
      "s37": "..TDC+img+RoI 9.50 TDC+RoI 10.24 TABLE V The mAP (%) performance of different ROCSU weighting schemes on VG V1.0 dataset Method mAP(%) ROCSUN orm 9.25 ROCSUOnes 9.82 with the VG V1.0 experiments, we adopted the same settings as [14], [19]. The mAP results are shown in Table III. It can be observed that the TDC+ROCSU method obtains a relative gain of 14.5% on VG V1.2 with an mAP of 11.90, compared with the state-of-the-art COCG (10.39). It is worth noting that the mAP achieved by our method is more than twice the mAP of the FCLN method. Furthermore, the TDC method without our contributed ROCSU achieves 10.33, which is very close to COCG. However, it is still far (around 15%) from the TDC+ROCSU method, which again shows the effectiveness of ROCSU. 4) AP Values Comparison with Different Threshold Combinations: Fig. 5 shows quantitative comparisons between the baseline (COCG) and TDC+ROCSU. With the Meteor threshold of 0..",
      "s38": ".., our TDC+ROCSU method achieves a significant improvement. This is mainly because ROCSU can make the model focus on RoIs with more information. Furthermore, TDC+ROCSU performs better than COCG at nearly all parameters. This shows both the encoding and decoding powers of our TDC and the capability of ROCSU to help the model to grab the important regions. 5) Ablation Studies: To validate the effectiveness of our ROCSU component, we remove it and only leave TDC with the same feature encoding method as TDC+ROCSU, which is denoted as TDC. We can see the value drops by 0.11 due to the equal weights of each RoI allocated during the training stage as the regions that deserve higher priorities are not used. To validate the function of comprehensive feature encoding, we also propose a wide range of experiment settings as shown in Table IV. We maintain TDC and adopt different ways of feature encoding. For example, the configuration of image and RoI features with TDC is defined as TDC+image+RoI. It is obvious that with object guidance, the performance improves sharply by 1.97 whilst TDC+Img+RoI achieves even worse results than TDC+RoI possibly because the image features may be too compact to understand, and thus, weaken..",
      "s39": "..its own function to guide dense captioning. To better clarify why TDC+ROCSU can achieve better dense captioning ability, we also illustrate an example and analyze the reason in depth in the next section. Furthermore, to validate the effectiveness of ROCSU setting in Eq. 8. We have also adopted two more kinds of ROCSU weighting schemes as shown in Table V. The first one is as IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 9 follows: ROCSUNorm = BW + IoU(ri, Bobj)confobj T PNrg i=1 IoU(ri , Bobj) , (9) where Nrg is the total number of RoIs in the RoI batch, other vectors and actors are the same with Eq. 8. This weighting scheme is denoted as ROCSUNorm. It is observed that it only achieves an mAP of 9.25, only 80% of ROCSU when using Eq. 8. This is mainly due to the weakened value by the normalization term, which undermines the function of ROCSU. Another weighting scheme we adopted is denoted as ROCSUOnes. It differs from Eq. 8 in the value of BW. For ROCSUOnes, we adopted a matrix of all ones as the basic..",
      "s40": "..weight of each RoI. The performance of ROCSUOnes is better than ROCSUNorm with an mAP of 9.82. It is still lower than ROCSU setting using Eq. 8, which demonstrates the superiority of the chosen ROCSU score function. C. Qualitative Results and Analysis In this section, we show qualitative results and analysis to help evaluate the experimental results in a more subjective way. In the first subsection, we present four examples from VG-COCO, VG V1.0 and VG V1.2 dataset respectively with the visualisation of all RoIs and the descriptions of them. In the second subsection, we will display results, in comparison with the COCG method and also the provided ground truth. 1) Examples of RoIs and Captions by TDC+ROCSU: Four complete examples of dense captioning results by TDC+ROCSU targeted on an image are shown in Fig. 6. From this visualization, we can clearly see the decent quality of both localizations and captions of RoIs achieved by TDC+ROCSU. To begin with, the model is able to capture the grammar of natural languages fairly well. A majority of the generated sentences comply with plain English grammar recognised by humans and are completely readable and understandable. We should owe this to..",
      "s41": "..the powerful encoding and decoding capability to learn representative features in order to correspond with visual and language clues as well as be aware of intramodality connections with each other. Furthermore, it is easy to see the proposed model has a very good command of commonly used ways of description (e.g., in the first example, \u2019with structure\u2019 is used three times correctly. This attributes to the function of ROCSU. Its aim is to attend more on RoIs overlapping more with objects. As we all know, \u2019with structure\u2019 can easily bridge multiple entities together so it is more likely to occur in the RoIs with more attention. Hence, a good command of \u2019with structure\u2019 complies with the doctrine of ROCSU. 2) Ablation Studies: To have a discussion about the experimental results of TDC+ROCSU and TDC in depth, in this section, we will analyze the importance of each part of our contributions, TDC and ROCSU separately. To be specific, we provide the top-5 visualization results according to region confidence of both TDC+ROCSU and TDC methods with the object detection results in the same image from VG-COCO as shown in Fig. 7 although we have given quantitative analysis in the last section. In Fig. 7,..",
      "s42": "..it is clear that due to the power of TDC to process sequential data, both methods can generate decent captions for common regions in the dataset that only describe the action of a person (\u2018a man skiing on the mountain in the middle\u2019 and \u2018trees covered in snow\u2019 at the left top of the image). The only difference is TDC+ROCSU provides the \u2018pine trees\u2019, which is more detailed. The region almost has no overlap with the objects detected in (c) and according to Eq. 8, there are no extra weights on this region while training by TDC+ROCSU. However, these good results, to a large extent, come from plenty of training samples from images with similar scenes in the dataset. Furthermore, there are two examples showing that ROCSU works better if the given region has more overlaps with objects, thus enabling the ROCSU to give more priority to this region even though it is focusing on more detailed information. Specifically, ROCSU helps the machine to recognise the red hat for the orange region instead of the helmet in the results of the TDC method due to more weights allocated to (a) during the training stage than (b), which derives from more overlaps with objects (specifically the IoU with the person with a score of 1..",
      "s43": "...000) in (c) than in (b). Also, based on the same explanation, with the aid of ROCSU, it can benefit from the bigger weight so that it is relatively easier to recognise the colour of the jacket (yellow not brown and yellow in (b)) on the man. Finally, from the red box in (a), it is easy to observe that for a given region that corresponds with different semantics in the image, ROCSU can show its superiority due to a high weight in the training from the summation of overlap with different detected objects. Because of this, ROCSU can encourage the generation of captions that link different semantics in the image in order to create more comprehensive descriptions that are likely to reveal the theme of the whole image rather than detailed descriptions. 3) Results with COCG and the Ground Truth: Fig. 8 shows the comparison results of our TDC+ROCSU method and the ground truth as a reference to measure their performance in randomly sampled RoIs. From these results, it is also visible that TDC+ROCSU performs better in both localisation and description of RoIs. This can be reflected by higher IoUs and Meteor displayed in the graph. It is noted that TDC+ROCSU is likely to accurately find the salient..",
      "s44": "..semantic in ground truth. It might be due to the joint ability of captioning modelling by both TDCs that learns better feature representation and their relationships and ROCSU, which focuses on RoIs that have more overlaps with semantic objects. We argue that it is not proper to owe this superiority to a unique module. For instance, in the first subfigure, without TDC, the close relationship between object surfboard and woman cannot be perfectly built up. Instead, it may suffer from the forget shortcoming like LSTM methods, losing the guidance from the word surfboard. Without ROCSU, this kind of informative RoI may not gain a priority, therefore causing a decrease in performance. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 10 a large green tree woman wearing a light blue shirt green grass on the field women wearing black shorts green grass on the ground a grassy field woman with long hair woman with blonde hair a woman wearing red shirt The girl is wearing black shoes woman with red hair people sitting in the grass people watching the game two women playing frisbee a green fence the arm of a man woman wearing black shirt a black shirt on a woman a white frisbee a long wooden fence a mountain range people skiing down a mountain a clear blue sky mountains in the distance a mountain in the distance orange pole on..",
      "s45": "..the ground snow covered ground person wearing orange jacket person skiing down a hill person skiing down a mountain people skiing down a hill a large building windows of a building white clouds in the sky the sky is cloudy a tall building a building with many windows a group of tall buildings a tall tower the water is calm a tall clock tower clock on the tower a bridge over water clock on the tower a toy of orange a wooden floor a baby a head of baby a baby wearing a pajamas toy on the floor A purple cell phone wooden floor a baby sitting on a bed Fig. 6. Detected RoIs with their corresponding captions by TDC+ROCSU of three different datasets: VG-COCO, VG V1.0 and VG V1.2. Specifically, two examples at the top are from VG-COCO, whilst the left bottom one from is VG V1.0 and the right bottom is from VG V1.2. (a) (b) person 1.000 person 0.966 skis 0.744 person 0.999 person 0.936 skis 0.744 red and black ski helmet skier in red jacket trees covered in snow brown and black jacket a man skiing on the mountain red hat on the head a group of people on ski slope pine trees covered in..",
      "s46": "..snow a man wearing a yellow jacket a man skiing on the mountain (c) Fig. 7. Dense captioning results of TDC+ROCSU and TDC method on VG-COCO dataset along with their object detection results. (a). Dense captioning results of TDC+ROCSU (Top-5 results according to confidence). (b). Dense captioning results of TDC (Top-5 results according to confidence). (c). Object detection results of the same image. V. CONCLUSION In this paper, a novel end-to-end trainable Transformerbased Dense Captioning Captioner (TDC) was proposed to facilitate the encoding and decoding of both visual and language features. This TDC can encode and decode both visual features and language features effectively with the guidance of object detection information. To make the model pay more attention to the detected RoIs with more information, particularly, we proposed another innovative unit, named ROCSU, to measure the importance of an RoI. Doing so allows the model to give higher priority to them, thus learning more useful knowledge. Experiments on several public datasets show that the TDC+ROCSU method outperforms the state-of-the-art significantly. This framework is easily to be transplanted to similar applications due to its flexibility. In our future..",
      "s47": "..work, we will apply the proposed TDC+ROCSU to the application of image captioning, dense video captioning [44] etc. though there might be some changes for ROCSU module according to the specific task. REFERENCES [1] Y. Miao, Z. Lin, X. Ma, G. Ding, and J. Han, \u201cLearning transformationinvariant local descriptors with low-coupling binary codes,\u201d IEEE Transactions on Image Processing, vol. 30, pp. 7554\u20137566, 2021. [2] Z. R. Khavas, S. R. Ahmadzadeh, and P. Robinette, \u201cModeling trust in human-robot interaction: A survey,\u201d in International Conference on Social Robotics (ICSR). Springer, 2020, pp. 529\u2013541. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 11 the woman is holding a surfboard a woman wearing a wetsuit (IoU: 0.77, Meteor: 0.19) woman holding a surfboard (IoU: 0.86, Meteor: 0.41) man wearing black knee pads black shorts on a man (IoU: 0.75, Meteor: 0.16) black knee pads (IoU: 0.78, Meteor..",
      "s48": "..: 0.31) Fence posts in sand a wooden beach (IoU: 0.76, Meteor: 0.04) a wooden fence (IoU: 0.87, Meteor: 0.13) flower patten on the man\u2019s shorts man wearing shorts (IoU: 0.85, Meteor: 0.18) shorts on the man (IoU: 0.87, Meteor: 0.27) Fig. 8. Qualitative comparisons between baseline (COCG) and our method (TDC+ROCSU). The green box refers to the ground truth, the red box and the blue box are the prediction results of COCG and TDC+ROCSU respectively (Best viewed in color). [3] J. Cao, Y. Pang, J. Han, and X. Li, \u201cHierarchical regression and classification for accurate object detection,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2021. [4] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, \u201cObject detection with deep learning: A review,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 11, pp. 3212\u20133232,..",
      "s49": "..2019. [5] Y. Liu, D. Zhang, Q. Zhang, and J. Han, \u201cPart-object relational visual saliency,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [6] H. Chen, G. Ding, X. Liu, Z. Lin, J. Liu, and J. Han, \u201cImram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12 655\u201312 663. [7] X. Xu, T. Wang, Y. Yang, L. Zuo, F. Shen, and H. T. Shen, \u201cCrossmodal attention with semantic consistence for image\u2013text matching,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 12, pp. 5412\u20135425, 2020. [8] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, \u0308 H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d in..",
      "s50": "..Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1724\u20131734. [9] A. Karpathy and L. Fei-Fei, \u201cDeep visual-semantic alignments for generating image descriptions,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3128\u2013 3137. [10] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, \u0301 J. Gao, X. He, M. Mitchell, J. C. Platt et al., \u201cFrom captions to visual concepts and back,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1473\u2013 1482. [11] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 12 and Y. Bengio, \u201cShow, attend and tell: Neural image caption generation with visual attention,\u201d in International Conference on Machine Learning (ICML), 2015, pp...",
      "s51": "..2048\u20132057. [12] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, \u201cImage captioning with semantic attention,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 4651\u2013 4659. [13] J. Lu, C. Xiong, D. Parikh, and R. Socher, \u201cKnowing when to look: Adaptive attention via a visual sentinel for image captioning,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 375\u2013383. [14] X. Li, S. Jiang, and J. Han, \u201cLearning object context for dense captioning,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), vol. 33, 2019, pp. 8650\u20138657. [15] J. Johnson, A. Karpathy, and L. Fei-Fei, \u201cDensecap: Fully convolutional localization networks for dense captioning,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2016, pp. 4565\u20134574. [16] S. Ren, K. He, R...",
      "s52": "..Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time object detection with region proposal networks,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2015, pp. 91\u201399. [17] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. [18] L. Yang, K. Tang, J. Yang, and L.-J. Li, \u201cDense captioning with joint inference and visual context,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2193\u2013 2202. [19] G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, and J. Shao, \u201cContext and attribute grounded dense captioning,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 6241\u20136250. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L..",
      "s53": "... Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2017, pp. 5998\u2013 6008. [21] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, \u201cGeneralized intersection over union: A metric and a loss for bounding box regression,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 658\u2013666. [22] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille, \u201cDeep captioning with multimodal recurrent neural networks (m-rnn),\u201d in International Conference on Learning Representations (ICLR), 2015. [23] R. Kiros, R. Salakhutdinov, and R. Zemel, \u201cMultimodal neural language models,\u201d in International Conference on Machine Learning (ICML), 2014, pp. 595\u2013603. [24] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan..",
      "s54": ".., K. Saenko, and T. Darrell, \u201cLong-term recurrent convolutional networks for visual recognition and description,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 2625\u20132634. [25] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, \u201cImage captioning: Transforming objects into words,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 11 137\u201311 147. [26] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature hierarchies for accurate object detection and semantic segmentation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 580\u2013587. [27] K. Fu, J. Li, J. Jin, and C. Zhang, \u201cImage-text surgery: Efficient concept learning in image captioning by generating pseudopairs,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 29, pp. 5910\u20135921, 2018. [28] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, \u201c..",
      "s55": "..Relation networks for object detection,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3588\u20133597. [29] T. Yao, Y. Pan, Y. Li, and T. Mei, \u201cExploring visual relationship for image captioning,\u201d in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 684\u2013699. [30] P. Sharma, N. Ding, S. Goodman, and R. Soricut, \u201cConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning,\u201d in Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), vol. 1, 2018, pp. 2556\u20132565. [31] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, \u201cEnd-to-end dense video captioning with masked transformer,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8739\u20138748. [32] J. L. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d..",
      "s56": "..arXiv preprint arXiv:1607.06450, 2016. [33] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778. [34] J. Cao, Y. Pang, S. Zhao, and X. Li, \u201cHigh-level semantic networks for multi-scale object detection,\u201d IEEE Transactions on Circuits and Systems for Video Technology, 2019. [35] X. Zhang, J. Zou, K. He, and J. Sun, \u201cAccelerating very deep convolutional networks for classification and detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 10, pp. 1943\u2013 1955, 2015. [36] K. He, G. Gkioxari, P. Dollar, and R. Girshick, \u201cMask r-cnn,\u201d in \u0301 Proceedings of the IEEE international conference on computer vision (ICCV), 2017, pp. 2961\u20132969. [37] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Per..",
      "s57": "..ona, D. Ramanan, P. Dollar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in \u0301 context,\u201d in Proceedings of the European conference on computer vision (ECCV), 2014, pp. 740\u2013755. [38] S. Ruder, \u201cAn overview of gradient descent optimization algorithms,\u201d arXiv preprint arXiv:1609.04747, 2016. [39] K. Miyaguchi and K. Yamanishi, \u201cAdaptive minimax regret against smooth logarithmic losses over high-dimensional l1-balls via envelope complexity,\u201d in International Conference on Artificial Intelligence and Statistics AISTATS, 2019, pp. 3440\u20133448. [40] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al., \u201cVisual genome: Connecting language and vision using crowdsourced dense image annotations,\u201d International journal of computer vision, vol. 123, no. 1, pp. 32\u201373, 2017. [41] O. Russakovsky, J. Deng, H. Su, J. Krause, S..",
      "s58": "... Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \u201cImagenet large scale visual recognition challenge,\u201d International journal of computer vision, vol. 115, no. 3, pp. 211\u2013252, 2015. [42] A. Lavie and A. Agarwal, \u201cMeteor: An automatic metric for mt evaluation with high levels of correlation with human judgments,\u201d in Proceedings of the second workshop on statistical machine translation, 2007, pp. 228\u2013231. [43] A. Neubeck and L. Van Gool, \u201cEfficient non-maximum suppression,\u201d in International Conference on Pattern Recognition (ICPR), vol. 3, 2006, pp. 850\u2013855. [44] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles, \u201cDensecaptioning events in videos,\u201d in Proceedings of the International Conference on Computer Vision (ICCV), 2017, pp. 706\u2013715. Zhuang Shao is currently a Ph.D candidate with Warwick Manufacturing Group at University of Warwick, Coventry, UK. He holds a BEng in Electronic & Information Engineering (North..",
      "s59": "..western Poly-technical University, 2015), an MSc in Information & Communication Engineering (Tianjin University, 2018). His research interests include image captioning, video captioning and machine learning. Jungong Han is currently a Chair Professor and the Director of the Research of Computer Science, Aberystwyth University, U.K. He also holds an Honorary Professorship with the University of Warwick, U.K. His research interests include computer vision, artificial intelligence, and machine learning. Demetris Marnerides has previously worked as a Research Fellow at the Warwick Manufacturing Group (WMG), University of Warwick. He holds a BA in Physics (University of Cambridge, 2013), an MSc in Scientific Computing (University of Warwick, 2015), and a PhD in Engineering (University of Warwick, 2019). His research topics include Machine Learning, Computer Vision, Image Processing and HDR Imaging. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 13 Kurt Debattista is Professor at WMG, University of Warwick. He holds a PhD from the University of Bristol. His research has focused on high-fidelity rendering, high-dynamic range imaging, applications of vision, and applied perception."
    },
    "question_generated": "In the context of dense image captioning, how can the limitations of traditional encoder-decoder frameworks, particularly those using LSTMs, be addressed by leveraging transformer architectures to enhance the integration of contextual information and attention mechanisms, ultimately leading to more accurate and natural descriptions?",
    "golden_answer_generated": "# Report on Dense Image Captioning: A Literature Synthesis\n\n## Introduction to Dense Captioning\nDense image captioning is a complex task that extends beyond traditional image captioning by requiring detailed descriptions for multiple regions within an image\u3010s3\u3011\u3010s5\u3011. This task is crucial for applications such as human-robot interaction and navigation for the visually impaired\u3010s3\u3011.\n\n## Challenges in Existing Methods\nExisting methods predominantly utilize encoder-decoder frameworks, often relying on LSTM networks\u3010s2\u3011\u3010s5\u3011. However, these approaches face significant challenges:\n- **Forget Gate Mechanism:** LSTMs struggle with long sequences, leading to the loss of contextual information\u3010s2\u3011\u3010s7\u3011.\n- **Equal Weighting of Regions:** Prior methods treat all Regions of Interest (RoIs) equally, failing to prioritize more informative regions\u3010s2\u3011\u3010s8\u3011.\n\n## Role of Transformers in Image Captioning\nTransformers have emerged as a powerful alternative to LSTMs, offering superior capabilities in capturing long-range dependencies\u3010s9\u3011\u3010s10\u3011. Their architecture, based on self-attention mechanisms, allows for more effective encoding and decoding of visual and language information\u3010s17\u3011\u3010s18\u3011.\n\n## Synthesis of Prior Work\nRecent advancements in dense captioning have focused on integrating contextual information to improve caption quality\u3010s14\u3011\u3010s15\u3011. Techniques such as non-local similarity graphs and object context learning have been explored to enhance the interaction between RoIs and their surroundings\u3010s14\u3011\u3010s19\u3011.\n\n### Key Insights from Literature\n- **Contextual Fusion:** Combining RoI features with global image context has shown promise in improving caption relevance\u3010s6\u3011\u3010s14\u3011.\n- **Attention Mechanisms:** Attention models, including semantic and geometric attention, have been pivotal in aligning visual features with language descriptions\u3010s13\u3011\u3010s28\u3011.\n\n## Hypothesis Formation\nBuilding on the synthesis of existing literature, a plausible hypothesis emerges: By leveraging transformer architectures to prioritize informative regions based on their contextual importance, dense image captioning can achieve more accurate and natural descriptions\u3010s10\u3011\u3010s29\u3011.\n\nThis hypothesis aligns with the broader trend of utilizing transformers to overcome the limitations of LSTMs, suggesting a promising direction for future research in dense captioning\u3010s9\u3011\u3010s10\u3011.",
    "used_snippets_with_context": {
      "s1": "warwick.ac.uk/lib-publications Manuscript version: Author\u2019s Accepted Manuscript The version presented in WRAP is the author\u2019s accepted manuscript and may differ from the published version or Version of Record. Persistent WRAP URL: http://wrap.warwick.ac.uk/163920 How to cite: Please refer to published version for the most recent bibliographic citation information. If a published version is known of, the repository item page linked to above, will contain details on accessing it. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. Copyright \u00a9 and all moral rights to the version of the paper presented here belong to the individual author(s) and/or other copyright owners. To the extent reasonable and practicable the material made available in WRAP has been checked for eligibility before being made available. Copies of full items can be used for personal research or study, educational, or not-for-profit purposes without prior permission or charge. Provided that the authors, title and full bibliographic details are credited, a hyperlink and/or URL is given for the original metadata page and the content is not changed in any way. Publisher\u2019s statement: Please refer to..",
      "s2": "..the repository item page, publisher\u2019s statement section, for further information. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Region-object Relation-aware Dense Captioning via Transformer Zhuang Shao, Jungong Han, Demetris Marnerides, Kurt Debattista Abstract\u2014Dense captioning provides detailed captions of complex visual scenes. While a number of successes have been achieved in recent years, there are still two broad limitations: 1) Most existing methods adopt an encoder-decoder framework, where the contextual information is sequentially encoded using Long Short-Term Memory (LSTM). However, the forget gate mechanism of LSTM makes it vulnerable when dealing with a long sequence; 2)The vast majority of prior arts consider Regions of Interests (RoIs) equally important, thus failing to focus on more informative regions. The consequence is that the generated captions cannot highlight important contents of the image, which does not seem natural. To overcome these limitations, in this paper, we propose a novel end-to-end transformer-based dense image captioning architecture, termed Transformer-based Dense Captioner (TDC). TDC learns the mapping between images and their dense captions via a Transformer,..",
      "s3": "..prioritising more informative regions. To this end, we present a novel unit, named Region-Object Correlation Score Unit (ROCSU), to measure the importance of each region, where the relationships between detected objects and the region, alongside the confidence scores of detected objects within the region, are taken into account. Extensive experimental results and ablation studies on the standard dense-captioning datasets demonstrate the superiority of the proposed method to the state-of-the-art methods. Index Terms\u2014Dense Image Captioning, Transformer-based Dense Image Captioner, Region-Object correlation score unit I. INTRODUCTION D Ense captioning has gained significant attention from both the engineering and research communities recently. On the one hand, it facilitates important practical applications [1], such as human-robot interaction [2], navigation for the blind, object detection [3] [4] or segmentation [5] and imagetext retrieval [6] [7]. On the other hand, it poses substantial challenges to both computer vision and natural language processing research communities. Its complexity in generating richer and more detailed descriptions for local regions, compared to image captioning, hastens the emergence of more advanced captioning techniques. Dense captioning stems from image captioning, and recent years have witnessed a rapid development of image captioning Manuscript..",
      "s4": "..received xxx, xxx; revised xxx, xxx and xxx, xxx; accepted xxx, xxx. (Corresponding author: Jungong Han). This research was supported by the funds of China Scholarship Council under Grant No. 201909120012. Zhuang Shao is with Warwick Manufacturing Group, University of Warwick, CV4 7AL, UK (e-mail: ZhuangShao@warwick.ac.uk). Jungong Han is with the Department of Computer Science, Aberystwyth University, SY23 3DB, UK (e-mail: jungonghan77@gmail.com). Demetris Marnerides is with Warwick Manufacturing Group, University of Warwick, CV4 7AL, UK (e-mail: dmarnerides@gmail.com). Kurt Debattista is with Warwick Manufacturing Group, University of Warwick, CV4 7AL, UK (e-mail: K.Debattista@warwick.ac.uk). a green plastic chair and three pegions a bird on a building (a) bird 0.983 bird 0.948 bird 0.998 chair 0.726 (b) Fig. 1. (a) An example of the RoI description created by the LSTM method COCG [14]. (b..",
      "s5": "..) The corresponding object detection results as context to guide the dense captioning. techniques. Many of these methods are based on encoderdecoder frameworks and inspired by the successful transfer of sequence to sequence training used for machine translation [8]. Broadly, image features are first extracted by a Convolutional Neural Network (CNN) as an encoder, and then fed into an RNN-based decoder that outputs the corresponding captions. However, such a captioning mechanism based on encoderdecoder frameworks fails to focus on areas that may be worthy of more attention at the training stage. To address this issue, many updated methods have been proposed. For example, [9] proposed aligned high-level information while [10]\u2013[13] resorted to different forms of attention to aid guidance during training. Dense captioning is beyond image captioning due to the need to provide richer and more detailed descriptions for a given image. [15] took the initiative to develop a Fully Convolutional Localization Network (FCLN) for the dense captioning task, in which Regions of Interests (RoIs) are localized before being described. Afterwards, many follow-ups appeared, which can be generally categorized into two classes depending on whether the contextual information encoded in the model is used. At the early stage, the architecture was composed..",
      "s6": "..of a Faster Region-based Convolutional Network (R-CNN) [16] module to detect RoIs and describe them with a Long Short-Term Memory (LSTM) [17], which was an advanced variant of Recurrent Neural Network (RNN). Unfortunately, this kind of framework only considered the RoIs but ignored possible contextual information that can be leveraged to improve training. To address this problem, [18] proposed to integrate the RoI features with image features as a global context to build up a joint and contextual fusion before captioning via an LSTM. However, the proposed global context seems too coarse, and there have been several methods that explored fine-grained contexts. For instance, [19] proposed a non-local similarity graph for the feature interaction between IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 2 the target RoI and its neighboring RoIs. Also, supported by data statistics, [14] revealed the close relationship between RoIs and detected objects via object detection, thus resulting in an architecture with contextual information considered. Despite the preliminary success of the aforementioned methods, dense image captioning can, arguably, be considered still in its infancy. We believe a number of limitations still exist, two of which are critical. Firstly, LSTMs,..",
      "s7": "..as the dominant structures for the methods mentioned above, suffer from the nature of the forget gate mechanism: forgotten information after a sequence cannot be avoided, especially when the inputted sequence is long. In state-of-the-art methods, if the contextual information is encoded by an LSTM, and with time rolling, the initial object would be \u201cforgotten\u201d and thus it weakens the guidance function of context especially when there are interactions of multiple people and multiple objects. Hence, the training model may fail to \u201coversee\u201d the objects so that it cannot guide the captioning process properly. As a result, this kind of gap often gives rise to the missing of descriptive objects, as illustrated in Fig. 1. Obviously, the object detection results as guided context for dense captioning are in good conditions, with accurate localizations and high confidence on the right. However, on the left, due to the aforementioned deficit of LSTM, the output caption does not include all three birds and a chair in its answer. Instead, it generates only a bird on a building, but forgets the other two birds and the chair. Secondly, in the previous methods, e.g. [14], all the RoIs are treated with equal weights during training. However, in the real world,..",
      "s8": "..the useful information carried by each RoI can be hugely different. Also, the detection confidence scores of objects within and around the region may vary considerably from region to region. These all imply that the regions should be treated differently during model training. As shown in Fig. 2, it may make more sense if assigning larger weights to the RoIs with more information at the training stage. Concretely, in this example, on the left are two RoIs detected, but apparently, they have different IoUs with the overall object bounding boxes illustrated on the right. According to the descriptive languages of these RoIs, it is obvious that the caption of the one in red with a higher IoU with the objects on the right. Also, it contains much more information in its ground truth since its description reveals the theme of the image. In contrast, the RoI in yellow contains too detailed information and this kind of information is even far difficult for the human being to observe, not to mention attain it by machine learning. Inspired by the common exam strategy that a student should focus more on the basic questions accounting for a large proportion of marks, rather than concentrating on difficult ones, we hold a view that the informative regions deserve more priorities. To alleviate the first issue, we propose a..",
      "s9": "..novel end-to-end dense captioning framework based on Transformer [20], which is currently popular in a great variety of computer vision tasks, termed Transformer-based Dense Captioner (TDC), to overcome the limitations of the forget mechanism of LSTM when encoding and decoding visual and language information. Fig. 3 gives an overview of TDC. Particularly, inspired by [14], we compose both object detection information and holistic image features as context. Along with the detected RoIs from Faster R-CNN Region Proposal Network (RPN) and contextual information, the visual information is projected into a visual representation by applying a dot product between them. The same operation is implemented on language information as well. At the decoding phase, a probability distribution for captions of detected RoIs is learnt by cross-modality attention of both visual and language encoding results. During encoding and decoding, all of the input vectors are aligned and computed together, hence it can overcome the forget problem. In order to address the second limitation, we propose a module, which allocates weights for the language loss of each region at each step of training. The underlying assumption is that the regions comprising more objects with high detection confidence scores are more important, and thus, deserve priority. To this end, we propose a..",
      "s10": "..novel unit, which makes use of both the object detection score and the intersection of union [21] (IoU), named Region-Object Correlation Score Unit (ROCSU). The major contributions of this work are summarized as: \u2022 A novel end-to-end dense captioning framework based on the Transformer, dubbed TDC, is proposed. A distinct property of TDC is the advocate of a Transformer to capture the long-range contextual information among objects. It is clearly advantageous over LSTM that is impotent in capturing long-range dependencies among objects. To the best of our knowledge, this is the first work that builds up a Transformer-based architecture rather than an LSTM for the dense captioning topic. \u2022 An RoI importance unit, named Region-Object Correlation Score Unit (ROCSU), drives the loss function to focus more on RoIs with more information. In doing so, our work, for the first time, weighs RoIs by jointly considering object-region relationships and object detection confidence scores during model training. It differs from treating each RoI equally at the training stage. \u2022 Extensive experimental results on different challenging datasets show the superiority of the proposed method against the state-of-the-art methods. The rest of this paper is organized as follows: We..",
      "s11": "..discuss related work in Section II. In Section III, the proposed method is introduced in detail with a comprehensive analysis. Extensive experimental results are demonstrated in Section IV with both qualitative and quantitative analysis. Finally, we summarize this paper with a conclusion in Section V. II. RELATED WORK In this section, we will review the related works from two aspects: image captioning and dense captioning. A. Image Captioning Earlier neural network models for image captioning [11], [22]\u2013[24] encoded visual information using a single feature representation of the image [25] with very limited additional information. However, with the development of deep learning, more auxiliary information can be added up into a model IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3 people skiing on the snow A ski hat (a) Person 0.999 Skis 0.984 Person 0.998 Person 0.998 Person 0.998 (b) Fig. 2. An example shows the RoIs with different IoUs should be weighted differently. (a) Two RoIs and their descriptions; (b) Object detection results. \u2022\u2022\u2022 Object Detection Image feature Object Context Features \u2022\u2022\u2022 Object Features \u2022\u2022\u2022 Geometry Features Dense captions A boy throwing baseball Two men are watching RoI detector (RP..",
      "s12": "..N) Pre-trained Resnet152 RoI \u2022\u2022\u2022 Features Off-line Pre-trained Context Module Visual (Transformer) Encoder NRoI \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 \u2022\u2022\u2022 A boy baseball Two men watching Captioning (Transformer) Decoder Word Embedding A boy throwing baseball Two men are watching \u2022\u2022\u2022 Fig. 3. The proposed TDC framework is made up of an RoI detector, context module, visual encoder and captioning decoder. Given an image, the RoI detector detects RoIs and the context module prepares contextual information generated via the pre-trained object detector for further use. After this, the visual encoder encodes visual information by attention, which gains a visual representation. Finally, after the word embeddings are conducted, visual representation and sentence information are decoded by the captioning decoder to generate dense captions for each RoI. structure. [9] extracted region features from images with an RCNN object detector [26] and generated separate captions for the regions as the captions of the given image. [10] proposed a method to generate image descriptions by first detecting words associated with different regions within the image [25]. In addition, [27] proposed an efficient concept learning module to get pseudo pairs. To..",
      "s13": "..better focus on important parts of images and model their correspondent relations with words in captions, a series of variants of attention models have been incorporated. [12] proposed a semantic attention module, which combines the topdown and bottom-up attention together. Also, [28] involved geometric attention, which inspired [29] to develop a framework with two Graph Convolutional Networks to explore visual relationships. In recent years, with the advance of Natural Language Processing (NLP), the Transformer architecture [20] has led to significant performance improvements for various tasks. [30] proposed a Transformer-based model by extracting a single global image feature from the image as well as uniformly sampling features by dividing the image into 8x8 partitions. In the latter case, the feature vectors were fed in a sequence to the Transformer encoder [25]. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 4 B. Dense Captioning Later on, dense captioning [15] emerged as a new task that requires an intelligent vision system to both localize and describe salient regions within an image in natural language. Existing dense captioning algorithms can be roughly categorized into two types: captioning with the guidance of contextual information and captioning without using contextual information. 1) Dense Captioning Without Context: In [15], Johnson..",
      "s14": "..et al. proposed a bilinear interpolation with a prototype of an RPN in Faster R-CNN. All the RoIs are represented by the same-size features, denoted as region features. Subsequently, they are passed through a fully-connected layer to determine if they are foreground (the descriptive region) or background. The locations of these regions are also amended at this stage via regression. At a later stage, region features are described by an LSTM language model, which is trained in an end-toend manner. 2) Dense Captioning With Context: The work in [18] is conceptually similar to [15]. But the difference lies in that the image feature acted as the contextual information, which was fed into the captioning module together with RoIs. Despite an improved performance, the contextual information is just a kind of global and coarse information, thus leading to the failure to encode more detailed context information. Subsequent works attempted to incorporate fine-grained context into the framework. For instance, [19] established a non-local similarity graph for the feature interaction between the target RoIs and its neighboring RoIs. Furthermore, it is noted that in [14], the authors argued that objects provide valuable cues to help locate captioning regions and generate descriptions for them via the..",
      "s15": "..use of data statistics. Inspired by this, the authors proposed to bring in local contextual information to guide the training of the model. To capture useful object information in an image, a novel framework for learning a complementary object context for each RoI was proposed using an LSTM. This context is derived from a concatenation of extracted object features and geometry information. The LSTM cell progressively accepts each object as input and decides whether to keep it or discard it. In the end, the context is also used as guidance information to help generate the descriptions and predict the bounding box offsets. A close look at the method in [14] reveals that the entire algorithm carries out an encoding-decoding procedure. In the encoding procedure, the representations of each contextual object fused with its CNN feature and geometry features (relative coordinates) are encoded step by step with a guidance LSTM, where the guidance information is composed of region features. The output of this procedure is the contextual information denoted as ci. For the decoding procedure, the authors tried two kinds of caption decoder frameworks, namely context as guidance (COCG) and context is decoded with an LSTM (COCD), respectively. Although they both have a caption LSTM for captioning as well as a location LSTM for localization, the main difference..",
      "s16": "..between these two decoders is their context decoding architectures. Concretely, COCD adds another LSTM to decode context ci while COCG removes this LSTM and turns the caption LSTM into a guidance LSTM to decode ci. In conclusion, as shown in the section of experiments in [14], the COCG framework outperforms the COCD framework and other methods, thus obtaining the state-of-the-art results due to the alleviation of the vanishing gradient problem by the guidance LSTM unit inside. III. METHODOLOGY In this section, we first briefly describe the popular Transformer architecture, which is a fundamental component of our method. Then, we present the framework of our proposed TDC. Finally, we elaborate on the proposed ROCSU loss adaptation. A. Preliminary Review of Transformer 1) Scaled dot-product attention: The scaled dot-product attention is a basic component of the Transformer [20] architecture. Given a query qi \u2208 Rd in all T queries, a group of keys kt \u2208 Rdand values vt \u2208 Rd, where t = 1, 2, ..., T, the output of dot-product attention is the weighted sum of the vt values. The weights are determined by the dot-products of query qi and..",
      "s17": "..keys kt. Specifically, kt and vt are placed into respective matrices K = (k1, ..., kT ) and V = (v1, ..., vT ) [31]. The output from a query qiis as follows: A(qi, K, V ) = V exp (KTqi/ \u221a d) PT t=1 exp (k T t qi/ \u221a d) , (1) where d is the dimension of qi and \u221ad is to normalize the dotproduct value. To capture detailed features of the input, an additional component called multi-head attention is introduced. The multi-head attention is composed of H parallel partial dot-product attention components, {hj |j \u2208 [1, H]} refer to heads, with each head being independent. The realization of the attention resulting from the multi-head attention (MA) is given by: MA(qi, K, V ) = concat(h1, h2, ..., hH)WO, hj = A(W q j qi, W K j K, WV j V ), (2) where W q j , W K j , WV j denote the transfer weight matrices q,..",
      "s18": "..K, V for hj . WO is the weight matrix for each head. All of these weights are learned during training. This formula of attention is generic so that it can represent two kinds of attention according to where its input comes from. Specifically, when the query is from the decoder layer, and meanwhile, both the keys and values come from the encoder layer, it represents the mutual attention due to its cross-module attribute. The second multi-head attention is called self-attention, where the queries, keys, and values keep unchanged in both encoder and decoder. 2) Transformer: We now present the use of the Transformer on top of scaled dot-product attention. The basic unit of the Transformer is multi-head attention with feed-forward layers followed by layer normalization [32]. The feed-forward layers map the output of the multi-head attention layer by two linear projections and an Rectified Linear Unit (ReLU) as the activate function. The encoder and decoder of the Transformer are composed of multiple basic structures, and usually, their layer IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 5 Self-Attention Layer V (Q) V(K) V(V) Feed-Forward Visual Encoder (Layer=2) V\u2019 Self-..",
      "s19": "..Attention Layer S(Q) S(K) S(V) Cross module attention S\u2019 Feed-Forward V\u2019 V\u2019 FC layer and softmax Caption Decoder (Layer=2) positional encoding + Visual Input positional encoding + Sentence Input + Fig. 4. Transformer structure in our dense captioning scenario, where the layer normalization is omitted. numbers are the same. The decoder of each layer takes the output of the corresponding encoder along with the output of the lower layer decoder output. Self-attention exists in both encoder and decoder. Cross-module attention between encoder and decoder is also applied in the decoder. Residual connection [33] and layer normalization [32] are implemented to all layers. Furthermore, because there is no recurrence module in a Transformer, to indicate positions for each vector, positional encoding (PE) of the input is used. PE occurs at the bottom of the multi-layer Transformer-based encoder and decoder stacks. The dimension of PE is the same as the input, so PE embedding can be added directly to the input. The realization of PE is as follows: P E(pos, 2i) = sin(pos/100002i/d), P E(pos, 2i + 1) = cos(pos/100002i/d..",
      "s20": "..), (3) where pos is the position of the embedded vector inside the input matrix, and i is the dimension of the encoded element in the input matrix, d is the total dimension of the input matrix. B. Transformer in Dense Captioning Scenario Fig. 4 shows the structure of the Transformer in this dense captioning scenario. To be specific, in the visual encoder, the input is encoded into visual features plus positional encodings, denoted as V . The self-attention layer takes three V s at the positions of Q, K, V . After the output of the feed-forward layer denoted as V 0 , on the other side, the embedded words plus positional encodings defined as S undergo the same self-attention. At the cross-module attention unit, these two modalities of data interact with each other to gain the output of cross-module attention, which proceeds to feed-forward to learn a captioning probability distribution by fully connected layers and a softmax. C. Transformer-based Dense Captioner In this section, we introduce our novel Transformer-based Dense Captioner. Given an image from an image set I = {I1, I2, ...IN }, our target is to detect an RoI set, denoted as R = {r1..",
      "s27": "..(Y l \u2264t )1 ... \u03c6(MA((\u03b4(Y l \u2264t )t), Fl, Fl), \u03b4(Y l \u2264t )t   ; \u03b4(Y l \u2264t ) =   \u03c6(MA(y l 1 , Y l, Y l), yl 1 ... \u03c6(MA(y l t , Y l, Y l), yl t   ; p(wt+1|F 0 , Y L \u2264t ) = sof t max(WV Y L t+1), (6) where y 0 i denotes a word token with an embedding dimension WV , and Y l \u2264t = (y l 1 , ..., yl t ) ,wt+1 is the probability of vocabulary bank at time step t+1. \u03b4 is the cross-module attention that uses the current representation of word embedding to attend to the visual representation from the corresponding layer of the encoder. \u03c6 represents the self-attention part in the decoder. However, different from the encoder, its inputs are words. It is noted that the restriction of time step means that the attention is only on the already generated words. D. Training..",
      "s28": "..and Optimization In this section, we introduce the training and optimization details. First, we show the loss function during training. Then in the second subsection, we explain our novel ROCSU. 1) Loss Function: In order to enforce both of the localization of detected RoIs and descriptive captions to be as close as training examples in an end-to-end manner, multiple loss function terms are leveraged during the Stochastic Gradient Descent [38] (SGD) at each training step in a training batch as follows: L = Lcls + Lreg + rgscore \u00d7 Lcaption T , (7) where Lcls is the classification binary cross entropy loss function of Faster R-CNN RPN [16] for RoI detection, Lreg is the smooth l1 loss [39] for coordinate regression of the location of detected RoIs. It is notable that Lcaption is the cross entropy loss of P = {p(wi|F 0 ; \u03b8), i \u2208 [1, max], which is the probability distribution of descriptive sentence for RoIs in the RoI batch, and their ground truth sentences word by word. To allocate different weights for each detected RoI according to its importance, we design a module ROCSU, its output is denoted as..",
      "s29": "..rgscore. We will introduce ROCSU in detail in the next subsection. 2) ROCSU: In this section, we introduce our novel unit ROCSU to measure the region score for each RoI according to its overlap with detected object bounding boxes as follow: Given an RoI riin R = {r1, r2, ...rM} and detected object at a training step, the corresponding rgscorei is computed as follows: rgscorei = BW + IoU(ri , Bobj)confobj T , (8) IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 7 where BW is the basic weight preset for each RoI, IoU is the Intersection of Union between ri and Bobj. To assemble all the rgscoreiinto vector, rgscore can be achieved. IV. EXPERIMENT In this section, we report and discuss the experiments conducted on three public datasets in order to evaluate the dense captioning performance of our proposed method. We use the Visual Genome dataset (VG) [40] and the VGCOCO dataset, which is the intersection of VG V1.2 and MS COCO [37], as the evaluation benchmarks. The choice of datasets is the same as the state..",
      "s30": "..-of-the-art methods [14], [19] for a fair comparison. The details of each dataset as well as the adopted evaluate metrics are elaborated below: 1) VG: Visual Genome currently has three versions: VG V1.0, VG V1.2, VG V1.4. As the state-of-the-art methods have always used VG V1.0 and VG V1.2, we also conduct our experiments on VG V1.0 and VG V1.2. The training, validation and test splits are chosen similarly as [14], [15], [19]. There are 77,398 images for training and 5,000 images for validation and testing [14]. 2) VG-COCO: As demonstrated in [14], the target bounding boxes of VG V1.0 and VG V1.2 are much denser than the bounding boxes in other object detection benchmark datasets such as MS COCO and ImageNet [41]. For example, each image in the training set of VG V1.2 contains an average of 35.4 objects, whilst the average value for MS COCO is only 7.1. To get proper object bounding boxes and caption region bounding boxes for each image.."
    },
    "eval_answer": "**Addressing Limitations of LSTM Frameworks in Dense Image Captioning with Transformers**\n\n**Key Advantages of Transformer Architectures:**  \n- **Long-range Dependency Modeling:** Transformers utilize a self-attention mechanism that effectively captures long-range dependencies in image data, overcoming the limitations of LSTMs, which struggle with such tasks [S7255739676].  \n- **Enhanced Spatial Understanding:** The integration of attention mechanisms in transformers provides a more nuanced understanding of spatial relationships among objects in images, leading to improved quality in dense image captions compared to traditional LSTM-based methods [S2844395874].  \n\n**Notable Transformer Models:**  \n- **EnTangled Attention (ETA) Model:** This model has shown significant improvements in dense image captioning by analyzing the relationships between detected entities, addressing key limitations of LSTM frameworks [S5001938868] [S3713005270].  \n- **Attention on Attention (AoA) Module:** This module enhances the relevance of attended vectors in image captioning tasks, offering a more sophisticated approach than LSTM-based attention mechanisms [S5760155098].  \n- **Vision Transformers (ViTs):** ViTs effectively capture both local and global features in images, which is crucial for generating detailed and contextually relevant captions in dense image captioning tasks [S2548369752].  \n\n**Conclusion:**  \nBy leveraging transformer architectures, researchers can significantly enhance the integration of contextual information and attention mechanisms in dense image captioning, leading to more accurate and natural descriptions. The advancements in models like ETA and AoA further illustrate the potential of transformers over traditional LSTM frameworks.",
    "eval_references": {
      "S2548369752": {
        "id": "S2548369752",
        "text": "Vision transformers (ViTs) have been shown to effectively capture both local and global features in images, which is crucial for generating detailed and contextually relevant captions in dense image captioning tasks.",
        "children": [
          {
            "id": "E9032834918",
            "text": "..discriminative. Or, in other words, after the progressive extraction of information, layer after layer, raw input data is refined into more condensed and abstract representations that are imbued with semantic meaning, encapsulating essential characteristics of the input. They are highly discriminative and have lower dimensionality than the raw input data, which not only conserves computational resources but also simplifies subsequent processing, making it especially beneficial in the analysis of high-dimensional data, such as images. This process also eliminates the tedious and error-prone process of handcrafted feature selection, leading to optimized feature sets and to the possibility of building the so-called \u201cend-to-end\u201d systems. Deep features can also help mitigate overfitting, a common challenge in machine learning, since by learning relevant representations, they prevent models from memorizing the training data and encourage more robust generalization. Another great advantage of deep feature extraction pipelines is the possibility of using transfer learning techniques. In this case, a deep feature extraction network previously successfully developed on one task or dataset can be transferred and fine-tuned to another related task, significantly reducing the need for large, labeled datasets and speeding up model training. This versatility is a game changer in many applications. After this extraction front end, continuing with the processing pipeline and moving towards the end of the network, fully connected layers are introduced. These layers come after the convolutional and pooling layers and play a pivotal role in feature aggregation and classification. The deep features extracted by the previous layers are flattened and processed through one or more fully connected layers. Finally, the output layer emerges as the last layer of the network. The number of neurons in this layer corresponds to the number of classes in a classification task or the number of output units in a regression task. For classification tasks, a sigmoid or a softmax activation function is typically used to calculate class probabilities, providing the final output of the CNN [25,26]. A sigmoid function is commonly employed in binary classification, producing a single probability score indicating the likelihood of belonging to the positive class. The softmax function is favored for its ability to transform raw output scores into probability distributions across multiple classes. This conversion ensures that the computed probabilities represent the likelihood of the input belonging to each class, with the sum of probabilities equating to one, thereby constituting a valid probability distribution. Beyond this interpretability, both functions are differentiable, a critical attribute for the application of gradient-based optimization algorithms like backpropagation during training. The described structured sequence of layers, from the input layer to the output layer, captures the hierarchical feature learning process in a CNN, allowing it to excel in image classification tasks (among others). Specific CNN architectures may introduce varia- Bioengineering 2023, 10, 1435 5 of 21 tions, additional components, or specialized layers based on the network\u2019s design goals and requirements. 3.1. Transformers CNNs are well suited for grid-like data, such as images, where local patterns can be captured efficiently. However, they struggle with sequential data because they lack a mechanism for modeling dependencies between distant elements (for example, in distinct time instants or far in the image). Also, CNNs do not inherently model the position or order of elements within the data. They rely on shared weight filters, which makes them translation invariant but can be problematic when absolute spatial relationships are important [27]. To overcome these limitations (handling sequential data, modeling long-range dependencies, incorporating positional information, and addressing tasks involving multimodal data, among others), transformers were introduced [28]. In the context of machine learning applied to images, transformers are a type of neural network architecture that extends the transformer model, originally designed for natural language processing [28], to handle computer vision tasks. These models are often referred to as vision transformers (ViTs) or image transformers [29] and come to introduce performance benefits, especially in noisy conditions [30,31]. In clinical settings, applications cover diagnosis and prognosis [32], encompassing classification, segmentation, and reconstruction tasks in distinct stages [31,33]. In vision transformers (ViT), the initial image undergoes a transformation process, wherein it is divided into a sequence of patches, as can be observed in Figure 2. Each of these patches is associated with a positional encoding technique, which captures and encodes the spatial positions of the patches, thus preserving spatial information. These patches, together with a class token, are then input into a transformer model to perform multi-head self-attention (MHSA) and generate embeddings that represent the learned characteristics of the patches. The class token\u2019s state in the ViT\u2019s output underscores a pivotal aspect of the model\u2019s architecture since it acts as a global aggregator of information from all patches, offering a comprehensive representation of the entire image. The token\u2019s state is dynamically updated during processing, reflecting a holistic understanding that encapsulates both local details and also the broader context of the image. Finally, a multilayer perceptron (MLP) is employed for the purpose of classifying the learned image representation. Notably, in addition to using raw images, it is also possible to supply feature maps generated by convolutional neural networks (CNNs) as input into a vision transformer for the purpose of establishing relational mappings [34]. It is also possible to use the transformer\u2019s encoding technique to explore the model\u2019s explainability [35]. Bioengineering 2023, 10, x FOR PEER REVIEW 6 of 22 Figure 2. Pipeline for applying the transformer\u2019s technique to images. The attention mechanism is a fundamental component in transformers. It plays a pivotal role in enabling the model to selectively focus on different parts of the input data with varying degrees of attention. At its core, the attention mechanism allows the model to assign varying levels of importance to different elements within the input data. This means the model can \u201cpay attention\u201d to specific elements while processing the data, prioritizing those that are most relevant to the task at hand. This selective attention enhances the model\u2019s ability to capture essential information and relationships within the input. The mechanism operates as follows: First, the input data is organized into a sequence of elements, such as tokens in a sentence for NLP or patches in an image for computer vision. Then, the mechanism introduces three sets of learnable parameters: query (Q), key (K), and value (V). The query represents the element of interest, while the key and value pairs are associated with each element in the input sequence. For each element in the in Figure 2. Pipeline for applying the transformer\u2019s technique to images. The attention mechanism is a fundamental component in transformers. It plays a pivotal role in enabling the model to selectively focus on different parts of the input data with varying degrees of attention. At its core, the attention mechanism allows the model to assign varying levels of importance to different elements within the input data. This means the model can \u201cpay attention\u201d to specific elements while processing the data, prioritizing those that are most relevant to the task at hand. This selective attention enhances the model\u2019s ability to capture essential information and relationships within the input. The Bioengineering 2023, 10, 1435 6 of 21 mechanism operates as follows: First, the input data is organized into a sequence of elements, such as tokens in a sentence for NLP or patches in an image for computer vision. Then, the mechanism introduces three sets of learnable parameters: query (Q), key (K), and value (V). The query represents the element of interest, while the key and value pairs are associated with each element in the input sequence. For each element in the input sequence, the attention mechanism calculates an attention score, reflecting the similarity between the query and the key for that element. The method used to measure this similarity can vary, with techniques like dot product and scaled dot product being common choices. These attention scores represent how relevant each element is to the query. The next step involves applying the softmax function to the attention scores. This converts them into weights that sum to one, effectively determining the importance of each input element concerning the query. The higher the weight, the more attention the model allocates to that specific element in the input data. Finally, the attention mechanism computes a weighted sum of the values, using the attention weights. The resulting output is a combination of information from all input elements, with elements more relevant to the query receiving higher weight in the final representation [36,37]. The attention mechanism can be used in various ways (attention gate [38], mixed attention [39], among others in the medical field), with one prominent variant being selfattention. In self-attention, the query, key, and value all originate from the same input sequence. This allows the architecture to model relationships and dependencies between elements within the same sequence, making it particularly useful for tasks that involve capturing long-range dependencies and context [7,40,41]. The original ViT architecture, as in Figure 3a, was enhanced with the hierarchical vision transformer using shifted windows (SWIN transformer) [42] where a hierarchical partitioning of the image into patches is used. This means that the image is first divided into smaller patches, and then these patches are merged together as the network goes deeper, as in Figure 3b. This hierarchical approach allows SWIN to capture both local and global features in the image, which can improve its performance on a variety of tasks. In the SWIN transformer, images of different resolutions belonging to outputs of different stages can be used to facilitate segmentation tasks. ioengineering 2023, 10, x FOR PEER REVIEW 7 of 22 (a) ViT (b) SWIN Figure 3. Comparison of architecture operation when going deep in the network. Another key difference between SWIN and ViT is that SWIN uses a shifted window self-attention mechanism, as depicted in Figure 4. This means that the self-attention operation is only applied to a local window of patches, or in other words, to a limited number of neighbor patches (as represented in green in Figure 4) rather than the entire image. Then, in a second stage, the attention..",
            "url": "https://www.mdpi.com/2306-5354/10/12/1435/pdf?version=1702863805"
          },
          {
            "id": "E9366560628",
            "text": "..814. 19. Agarap, A.F. Deep Learning Using Rectified Linear Units (ReLU). arXiv 2018, arXiv:1803.08375. 20. Deng, L. The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]. IEEE Signal Process. Mag. 2012, 29, 141\u2013142. [CrossRef] 21. LeCun, Y.; Bengio, Y.; Hinton, G. Deep Learning. Nature 2015, 521, 436\u2013444. [CrossRef] 22. Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent Advances in Convolutional Neural Networks. Pattern Recognit. 2018, 77, 354\u2013377. [CrossRef] 23. O\u2019Shea, K.; Nash, R. An Introduction to Convolutional Neural Networks. arXiv 2015, arXiv:1511.08458. 24. Yandex, A.B.; Lempitsky, V. Aggregating Local Deep Features for Image Retrieval. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 7 December 2015; pp. 1269\u20131277. 25. Nwankpa, C.; Ijomah, W.; Gachagan, A.; Marshall, S. Activation Functions: Comparison of Trends in Practice and Research for Deep Learning. arXiv 2018, arXiv:1811.03378. Bioengineering 2023, 10, 1435 18 of 21 26. Szanda\u0142a, T. Review and Comparison of Commonly Used Activation Functions for Deep Neural Networks. In Bio-Inspired Neurocomputing; Bhoi, A.K., Mallick, P.K., Liu, C.-M., Balas, V.E., Eds.; Studies in Computational Intelligence; Springer: Singapore, 2021; pp. 203\u2013224, ISBN 9789811554957. 27. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet Classification with Deep Convolutional Neural Networks. Commun. ACM 2017, 60, 84\u201390. [CrossRef] 28. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, \u0141.; Polosukhin, I. Attention Is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, CA, USA, 4\u20139 December 2017; Curran Associates Inc.: Red Hook, NY, USA, 2017; pp. 6000\u20136010. 29. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. An Image Is Worth 16 \u00d7 16 Words: Transformers for Image Recognition at Scale. arXiv 2020, arXiv:2010.11929. 30. Maur\u00edcio, J.; Domingues, I.; Bernardino, J. Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review. Appl. Sci. 2023, 13, 5521. [CrossRef] 31. Shamshad, F.; Khan, S.; Zamir, S.W.; Khan, M.H.; Hayat, M.; Khan, F.S.; Fu, H. Transformers in Medical Imaging: A Survey. Med Image Anal. 2022, 88, 102802. [CrossRef] [PubMed] 32. Ali, H.; Mohsen, F.; Shah, Z. Improving Diagnosis and Prognosis of Lung Cancer Using Vision Transformers: A Scoping Review. BMC Med. Imaging 2023, 23, 129. [CrossRef] [PubMed] 33. Al-hammuri, K.; Gebali, F.; Kanan, A.; Chelvan, I.T. Vision Transformer Architecture and Applications in Digital Health: A Tutorial and Survey. Vis. Comput. Ind. Biomed. Art 2023, 6, 14. [CrossRef] 34. He, K.; Gan, C.; Li, Z.; Rekik, I.; Yin, Z.; Ji, W.; Gao, Y.; Wang, Q.; Zhang, J.; Shen, D. Transformers in Medical Image Analysis. Intell. Med. 2023, 3, 59\u201378. [CrossRef] 35. Katar, O.; Yildirim, O. An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization. Diagnostics 2023, 13, 2459. [CrossRef] 36. Hu, J.; Shen, L.; Albanie, S.; Sun, G.; Wu, E. Squeeze-and-Excitation Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18\u201323 June 2019. 37. Touvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; J\u00e9gou, H. Going Deeper with Image Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 11\u201317 October 2021. 38. Oktay, O.; Schlemper, J.; Folgoc, L.L.; Lee, M.; Heinrich, M.; Misawa, K.; Mori, K.; McDonagh, S.; Hammerla, N.Y.; Kainz, B.; et al. Attention U-Net: Learning Where to Look for the Pancreas. arXiv 2018, arXiv:1804.03999. 39. Nwoye, C.I.; Yu, T.; Gonzalez, C.; Seeliger, B.; Mascagni, P.; Mutter, D.; Marescaux, J.; Padoy, N. Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. Med. Image Anal. 2022, 78, 102433. [CrossRef] [PubMed] 40. Rao, A.; Park, J.; Woo, S.; Lee, J.-Y.; Aalami, O. Studying the Effects of Self-Attention for Medical Image Analysis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 11\u201317 October 2021. 41. You, H.; Wang, J.; Ma, R.; Chen, Y.; Li, L.; Song, C.; Dong, Z.; Feng, S.; Zhou, X. Clinical Interpretability of Deep Learning for Predicting Microvascular Invasion in Hepatocellular Carcinoma by Using Attention Mechanism. Bioengineering 2023, 10, 948. [CrossRef] [PubMed] 42. Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; Guo, B. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, QC, Canada, 10\u201317 October 2021. 43. Liu, Z.; Hu, H.; Lin, Y.; Yao, Z.; Xie, Z.; Wei, Y.; Ning, J.; Cao, Y.; Zhang, Z.; Dong, L.; et al. Swin Transformer V2: Scaling Up Capacity and Resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 18\u201324 June 2022. 44. He, B.; Hofmann, T. Simplifying Transformer Blocks. arXiv 2023, arXiv:2311.01906. 45. Henry, E.U.; Emebob, O.; Omonhinmin, C.A. Vision Transformers in Medical Imaging: A Review. arXiv 2022, arXiv:2211.10043. 46. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016. 47. Tayebi, R.M.; Mu, Y.; Dehkharghanian, T.; Ross, C.; Sur, M.; Foley, R.; Tizhoosh, H.R.; Campbell, C.J.V. Automated Bone Marrow Cytology Using Deep Learning to Generate a Histogram of Cell Types. Commun. Med. 2022, 2, 45. [CrossRef] [PubMed] 48. Mammeri, S.; Amroune, M.; Haouam, M.-Y.; Bendib, I.; Corr\u00eaa Silva, A. Early Detection and Diagnosis of Lung Cancer Using YOLO v7, and Transfer Learning. Multimed. Tools Appl. 2023, 1\u201316. [CrossRef] 49. Mortada, M.J.; Tomassini, S.; Anbar, H.; Morettini, M.; Burattini, L.; Sbrollini, A. Segmentation of Anatomical Structures of the Left Heart from Echocardiographic Images Using Deep Learning. Diagnostics 2023, 13, 1683. [CrossRef] 50. Su, Y.; Liu, Q.; Xie, W...",
            "url": "https://www.mdpi.com/2306-5354/10/12/1435/pdf?version=1702863805"
          }
        ]
      },
      "S5001938868": {
        "id": "S5001938868",
        "text": "Recent transformer-based models, such as the EnTangled Attention (ETA) model, have been shown to improve image captioning by examining the relationships between detected entities, addressing a key limitation of LSTM frameworks.",
        "children": [
          {
            "id": "E3874012582",
            "text": "..Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 779\u2013788. 51. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. Ssd: Single shot multibox detector. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11\u201314 October 2016; Springer: Berlin/Heidelberg, Germany, 2016; pp. 21\u201337. 52. Lin, T.Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22\u201329 October 2017; pp. 2980\u20132988. 53. Dash, S.K.; Acharya, S.; Pakray, P.; Das, R.; Gelbukh, A. Topic-based image caption generation. Arab. J. Sci. Eng. 2020, 45, 3025\u20133034. [CrossRef] 54. Hu, H.; Gu, J.; Zhang, Z.; Dai, J.; Wei, Y. Relation networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 3588\u20133597. 55. Guo, H.; Zheng, K.; Fan, X.; Yu, H.; Wang, S. Visual attention consistency under image transforms for multi-label image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 729\u2013739. 56. Ghiasi, G.; Lin, T.Y.; Le, Q.V. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 7036\u20137045. 57. Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; Zagoruyko, S. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision, Glasgow, UK, 23\u201328 August 2020; Springer: Berlin/Heidelberg, Germany, 2020; pp. 213\u2013229. 58. Tan, M.; Pang, R.; Le, Q.V. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 10781\u201310790. 59. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation using Tag Relations and Graph Convolutional Networks. In Proceedings of the 2021 5th International Conference on Pattern Recognition and Image Analysis (IPRIA), Kashan, Iran, 28\u201329 April 2021; IEEE: Piscataway, NJ, USA, 2021; pp. 1\u20136. Algorithms 2023, 16, 135 36 of 40 60. Joseph, K.; Khan, S.; Khan, F.S.; Balasubramanian, V.N. Towards open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 19\u201325 June 2021; pp. 5830\u20135840. 61. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation Using Quantization Reweighting Function and Graph Neural Networks. In Proceedings of the International Conference on Service-Oriented Computing, Seville, Spain, 29 November\u20132 December 2022; Springer: Berlin/Heidelberg, Germany, 2022; pp. 46\u201360. 62. Yang, P.; Luo, F.; Chen, P.; Li, L.; Yin, Z.; He, X.; Sun, X. Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling. In Proceedings of the IJCAI, Macao, China, 11\u201312 August 2019; Volume 3, p. 7. 63. Speer, R.; Havasi, C. Representing general relational knowledge in conceptnet 5. In Proceedings of the LREC, Istanbul, Turkey, 23\u201325 May 2012; Volume 2012, pp. 3679\u20133786. 64. Chen, H.; Huang, Y.; Takamura, H.; Nakayama, H. Commonsense knowledge aware concept selection for diverse and informative visual storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, Virtual, 2\u20139 February 2021; Volume 35, pp. 999\u20131008. 65. Li, J.; Shi, H.; Tang, S.; Wu, F.; Zhuang, Y. Informative visual storytelling with cross-modal rules. In Proceedings of the 27th ACM International Conference on Multimedia, Nice, France, 21\u201325 October 2019; pp. 2314\u20132322. 66. Aggarwal, C.C. Data classification. In Data Mining; Springer: Berlin/Heidelberg, Germany, 2015; pp. 285\u2013344. 67. Aggarwal, C.C.; Reddy, C.K. Data clustering. In Algorithms and Applications; Chapman & Hall/CRC Data Mining and Knowledge Discovery Series; Chapman & Hall: London, UK, 2014. 68. Ahmed, M. Data summarization: A survey. Knowl. Inf. Syst. 2019, 58, 249\u2013273. [CrossRef] 69. Mao, J.; Xu, W.; Yang, Y.; Wang, J.; Huang, Z.; Yuille, A. Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv 2014, arXiv:1412.6632. 70. Karpathy, A.; Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3128\u20133137. 71. Vinyals, O.; Toshev, A.; Bengio, S.; Erhan, D. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3156\u20133164. 72. Rennie, S.J.; Marcheret, E.; Mroueh, Y.; Ross, J.; Goel, V. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 7008\u20137024. 73. Ren, Z.; Wang, X.; Zhang, N.; Lv, X.; Li, L.J. Deep reinforcement learning-based image captioning with embedding reward. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 290\u2013298. 74. Gordon, D.; Kembhavi, A.; Rastegari, M.; Redmon, J.; Fox, D.; Farhadi, A. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 4089\u20134098. 75. Patro, B.; Patel, S.; Namboodiri, V. Robust explanations for visual question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Snowmass Village, CO, USA, 1\u20135 March 2020; pp. 1577\u20131586. 76. Wu, Q.; Wang, P.; Shen, C.; Reid, I.; Van Den Hengel, A. Are you talking to me? reasoned visual dialog generation through adversarial learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 6106\u20136115. 77. Chen, C.; Mu, S.; Xiao, W.; Ye, Z.; Wu, L.; Ju, Q. Improving image captioning with conditional generative adversarial nets. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27 January\u20131 February 2019; Volume 33, pp. 8142\u20138150. 78. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning, Lille, France, 7\u20139 July 2015; 2015; pp. 2048\u20132057. 79. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014, arXiv:1409.0473. 80. Ba, J.; Mnih, V.; Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv 2014, arXiv:1412.7755. 81. Mnih, V.; Heess, N.; Graves,..",
            "url": "https://openalex.org/W4323044469",
            "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
            "publication_date": "2023-03-02"
          },
          {
            "id": "E1765349546",
            "text": "..the concept of a narrative, then investigate its generation process from different perspectives. Narrative construction can be studied and categorized into learning-based, feature-map-based, language-based, and narrative length-based methods. Definition 1. A narrative N is a specific subgraph, path, or walk in a Story Model starting from a random node i and ending in another random node j while traversing the Knowledge Graph G with a sequence of vertices in-between them. Graph G is the Knowledge Graph containing the contextualized data information, extracted concepts, objects, topics, captions, metadata, and rules from the image data. Learning-Based Methods. Narrative construction has utilized a variety of learningbased approaches. We classify them into three main categories. A supervised learning method, which is the first category, refers to training the model under the supervision of a teacher. Labels are used to supervise the learning process. In the second category, reinforcement learning (RL) aims to maximize the long-term reward of agents by taking the optimal action (through trial and error) in a particular state. Finally, a generative adversarial network (GAN) trains a generative model by defining the problem as a supervised learning problem consisting of two submodels: a generator and a discriminator. The generator generates fake new samples and the discriminator tries to ascertain whether the input samples are real or fake. Finally, a zero-sum adversarial game is used to train these two models together until the discriminator is fooled half the time. \u2022 Supervised Learning Methods. Multimodal Recurrent Neural Network (m-RNN) architecture has been proposed in [69] to handle these two tasks: (1) image-to-sentence description generation and (2) image and sentence retrieval. Additionally, this architecture is trained by utilizing a log-likelihood cost function. The model parameters are learned by differentiating from the cost function given the input and the backpropagation algorithm. Karlpathy et al. [70] introduce a multimodal RNN architecture that learns to generate narratives for image regions based on inferred alignments (multimodal embeddings of convolutions over images and bi-RNNs over sentences). They map every image and sentence into a shared space of h-dimensional vectors. The supervision is carried out at the entire image and sentence level, so they formulate an image\u2013sentence score based on the individual region scores. Vinyals et al. [71] present an end-to-end solution to the caption generation problem, which is fully trainable by employing stochastic gradient descent, and the loss is the sum of each step\u2019s negative log-likelihood of the correct word. \u2022 Reinforcement Learning (RL). SCST [72] is a reinforcement learning (RL) optimization technique that normalizes rewards by exploiting the output of its test-time inference algorithm. This approach baselines the REINFORCE approach more efficaciously, Algorithms 2023, 16, 135 19 of 40 leading to better results with nondifferentiable evaluation metrics. Ren et al. [73] take a collaborative approach and propose a decision-making framework. They use a policy and value network to predict the word for each step of narrative generation. Specifically, an actor\u2013critic RL algorithm is introduced to learn these two networks. Gordon et al. [74] introduce the Interactive Question Answering (IQA) task (answering questions that demand the agent\u2019s interaction with a dynamic environment) and propose the Hierarchical Interactive Memory Network (HIMN). HIMN is factorized into a Planner, a set of controllers, and a semantic spatial memory. The Planner, formulated as an RL problem, invokes the controllers to explore the environment and answer the question in the IQA task. \u2022 Generative Adversarial Networks (GANs). Patro et al. [75] present a Correlated Collaborative Model (CCM) that guarantees the coherence of the generated textual explanations and answers. CCM collaborates with the answer and explanation features and employs generative adversarial strategies for training. In addition, the results are robust to noise in images and text (even if the model is not trained for noise-based attacks). \u2022 Combined Methods (RL + GAN). To generate more human-like answers to questions, Wu et al. [76] combine RL with GANs and introduce a novel framework. This framework updates the generator by employing the reward of the generator policy at each training step. After sampling data from the dialog history (a sequence of textual data), maximum likelihood estimation (MLE) is used to update the generator. Chen et al. [77] introduce a conditional generative adversarial captioning technique to extend RL-based architectures. Furthermore, CNN- and RNN-based architectures are presented for the discriminator module. The discriminator judges whether a human described the resulting caption or if it is machine-generated. Feature-Map-Based Methods. One of the critical challenges in narrative construction based on an image is extracting visual features which best represent the image content. Therefore, we categorize these methods into three main groups based on various strategies proposed for extracting visual features: CNN-based, attention-based, graph-based, and vision-transformer-based methods, which are discussed in detail in the following paragraphs. \u2022 CNN-Based Methods. Vinyals et al. [71] employ a vision CNN model to extract image features, since CNNs can embed images in fixed-length vectors. They utilize the result feature vector of GoogleNet [141] for the visual feature extraction step. Moreover, Karpathy et al. [70] used the visual features extracted from AlexNet [142]. Many studies employ the CNN modules to extract features such as [72,143,144]. \u2022 Attention-Based Methods. Simple CNN-based methods have the main advantage of being compact and straightforward. At the same time, the extreme compactness and lack of granularity are crucial issues that need to be addressed. An attention module handles this issue by computing attention weights and attending to specific parts of the visual features. Many approaches fit into attention-based methods that can be divided into three main categories: grid-based, region-based, and self-attention. In the following items, these three categories are further discussed. \u2013 Grid-Based Attention Methods. A significant proportion of image captioning methods use the attention mechanism to make captioning more flexible and provide better granularity. Xu et al. [78] introduced an attention-based image captioning model inspired by a recent study in machine translation [79] and object detection [80,81]. The authors proposed two attention-based image caption generators: a \u201csoft\u201d deterministic attention mechanism and a \u201chard\u201d stochastic attention mechanism. An adaptive encoder\u2013decoder model [82] also automatically determines when to rely on the language model and when to look at the image (the spatial CNN features at each of the k grid locations of the image). \u2013 Region-Based Attention Methods. Anderson et al. [83] employ a novel bottomup and top-down attention module (based on Faster R-CNN [49] and task-specific Algorithms 2023, 16, 135 20 of 40 context). First, the bottom-up visual attention module extracts salient regions of an image and represents them using convolutional feature vectors. Then, the top-down module estimates the distribution of attention over image regions (specifies weights of the features). As a result, a weighted average of all image features is the final attended feature vector. The Look-Back and Predict-Forward (LBPF) approach [84] presents two main modules: Look-Back (LB) module and Predict-Forward (PF) module. As the input of the attention module, the LB module concatenates the previous attention vector and the current hidden state. In contrast, the PF module sequentially predicts the two following hidden states based on the current hidden state. For constructing high-quality image captions, the Reflective Decoding Network (RDN) [85] improves the capability of the standard caption generator to handle long sequential modeling by examining the caption\u2019s word consistency. Li et al. [145] also addresses both the tasks of VQA and VQG using the attention mechanism after extracting the visual features from the input image. \u2013 Self-Attention Methods. Despite the widespread use of attention mechanisms in image captioning, we still do not know how closely related attended vectors and given attention queries are. Therefore, Huang et al. [86] proposed the \u201cAttention on Attention\u201d (AoA) module, which specifies the attention results and queries relevancy. Applying AoA to both the encoder and decoder in this research, the authors introduced AoANet for image captioning. Guo et al. [87] first proposed a normalized self-attention (NSA), which indicated that conducting this normalization on the hidden activations inside self-attention is advantageous. Then, a geometric-aware self-attention (GSA) module was proposed to compute the objects\u2019 geometric bias to assist with image comprehension. Furthermore, the self-attention module in the encoder facilitated the EnTangled Attention (ETA) model [88] to examine the detected entities\u2019 relationships. \u2022 Graph-Based Methods. This image encoder incorporates two kinds of visual relationships (semantic and spatial object relationships) in the proposed GCN-LSTM framework [89], which attempts to explore the relations between objects. Based on these visual relationships, they developed graphs over the detected objects in an image and used GCNs proposed in [146]. The Scene Graph Auto-Encoder (SGAE) [90] embeds inductive bias into a dictionary unsupervised. Subsequently, it is shared as a re-encoder for text generation, enhancing the encoder\u2013decoder performance. For captioning, Yao et al. [91] utilize the hierarchical structure in images at the instance, region, and whole image level. This hierarchical structure is analyzed with a tree-structured LSTM model, and each instance-, region-, and image-level feature is improved. Image encoding is also based on a hierarchical scene parsing architecture. \u2022 Vision-Transformer-Based Methods. In this category, the methods [147..",
            "url": "https://openalex.org/W4323044469",
            "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
            "publication_date": "2023-03-02"
          }
        ]
      },
      "S5760155098": {
        "id": "S5760155098",
        "text": "The Attention on Attention (AoA) module enhances the relevance of attended vectors in image captioning tasks, providing a more sophisticated approach than LSTM-based attention mechanisms.",
        "children": [
          {
            "id": "E9499381511",
            "text": "... It computes the topic smoothness value of the current constructed paragraph description at each recurrent step, assessing the coherence of topics across the generated sentences. Future Internet 2023, 15, 260 23 of 60 With these multi-level assessments, the model can generate long yet realistic descriptions, maintaining both sentence-level plausibility and topic coherence. In the CNN-GAN model, a convolutional encoder\u2013decoder network is utilized for generating new content by jointly training it with adversarial networks. This training setup aims to ensure coherence between the generated pixels and the existing ones. These CNN-based methods have demonstrated the ability to generate realistic and plausible content in highly structured images, such as faces, objects, and scenes [127]. Generative Adversarial Networks (GAN) with Denoising Diffusion Probabilistic Models (DDPM) and Transformers: Combination DDPMs, GANs, and transformers can create a hybrid generative AI model with enhanced capabilities. This combination allows for the generation of diverse and high-quality samples while leveraging the strengths of each component. DiffGAN-TTS [128] and ProDiff [53] implement this combination of models. DiffGAN-TTS is a novel text-to-speech (TTS) model that achieves high-fidelity and efficient speech synthesis. It takes inspiration from the denoising diffusion GAN model and models the denoising distribution using an expressive acoustic generator. This generator is trained adversarially to match the true denoising distribution, ensuring high-quality output spectrograms. DiffGAN-TTS ability to allow large denoising steps during inference. This reduces the number of denoising steps required and accelerates the sampling process. To further enhance sampling efficiency, DiffGAN-TTS incorporates an active shallow diffusion mechanism. ProDiff utilizes generator-based parameterization, where the denoising model directly predicts clean data using a neural network [53]. This approach has shown advantages in accelerating sampling from complex distributions. By directly predicting clean data, ProDiff avoids the need to estimate gradients and achieves faster synthesis. Transformer with Recurrent Neural Network (RNN): The combination of transformers and RNNs can leverage the strengths of both architectures, allowing for improved modeling of sequential data with long-term dependencies and global context understanding. This combination is useful for tasks such as speech recognition, time series forecasting, and video processing, where both local temporal dependencies and global context are crucial for accurate predictions. MolT5 [58] implements three baseline models for the tasks of molecule captioning and molecule generation. The first baseline is a four-layer GRU recurrent neural network with a bidirectional encoder. This model leverages the sequential nature of the data and captures contextual information from both past and future. The second baseline is based on the transformer architecture, consisting of six encoder and decoder layers. Transformers utilize self-attention mechanisms to capture global dependencies and have been successful in various sequence-to-sequence tasks. The third baseline is based on the T5 model, a pre-trained sequence-to-sequence model. Three T5 checkpoints, namely small, base, and large, are fine-tuned for molecule captioning and molecule generation. T5 models have shown strong performance in natural language processing tasks. Transformer with Graph Convolutional Network (GCN): For tasks that require graphstructured data, this hybrid model combines the strength of transformers and GCNs. Transformers excel at sequence-to-sequence tasks and have demonstrated success in natural language processing and image processing. GCNs, on the other hand, are especially intended to handle graph-structured data and capture node relationships. This hybrid model can effectively capture both the sequential dependencies of the data and the graphbased relationships by combining transformers and GCNs, enabling enhanced modeling and representation learning in graph-based tasks such as node classification, link prediction, graph generation and molecule structure generation [60]. Transformer with Long Short-Term Memory (LSTM): The transformer architecture with long short-term memory (LSTM) is a type of recurrent neural network known for its ability to capture long-term dependencies in sequential data. Transformers are powerful models for sequence processing, leveraging self-attention mechanisms to capture dependencies across the sequence. The GTR-LSTM [129] encoder provides a graph-based approach to encoding triples, considering the structural relationships between entities in a knowledge Future Internet 2023, 15, 260 24 of 60 graph. By incorporating attention mechanisms and entity masking, the model aims to generate coherent and meaningful output sequences based on the input graph. Vision Transformers with Residual Neural Networks (ResNet): Vision transformers leverage the self-attention mechanism of transformers to capture long-range dependencies and enable effective modeling of image data. The combination of ResNet and vision transformers can benefit from both the local feature extraction capabilities of ResNet and the global context understanding of vision transformers, resulting in improved image understanding and representation [130]. Diffusion probabilistic models with Contrastive Language-Image Pretraining (CLIP): Diffusion modeling is a powerful technique for modeling complex data distributions and generating high-quality samples. CLIP, on the other hand, is a state-of-the-art method for learning visual representations from images and corresponding textual descriptions. DiffusionCLIP [131] combines the power of diffusion modeling and the guidance of CLIP to enable precise and controlled image manipulation. It leverages pretrained diffusion models and the CLIP loss to fine-tune the diffusion model and generate samples that align with a target textual description, which opens new possibilities for image generation and manipulation tasks. Convolutional Neural Network (CNN) with Bidirectional Encoder Representations from Transformers (BERT): CLAP (contrastive learning for audio and text pairing) [47] is a model that jointly trains an audio encoder and a text encoder to learn the similarity or dissimilarity between audio and text pairs. The goal is to enable zero-shot classification by computing embeddings for audio and text and using cosine similarity to measure their similarity. The model takes audio and text pairs as input, which are separately processed by the audio encoder and text encoder. The encoders extract meaningful representations from the audio and text inputs. These representations are then projected into a joint multimodal space using linear projections. Convolutional Sequence-to-Sequence Learning (ConvS2S): This is a neural network architecture that was introduced for sequence-to-sequence tasks, such as machine translation or speech recognition. It leverages convolutional neural networks (CNNs) to process input sequences and generate output sequences, providing an alternative to the commonly used recurrent neural networks (RNNs) Unlike RNN-based models that rely on sequential processing, ConvS2S [132] applies parallel convolutions across the input sequence. This enables more efficient computation and allows for better utilization of parallel processing capabilities, leading to faster training and inference times. The use of convolutions also helps capture local dependencies in the input sequence, which can be beneficial for tasks where context is primarily determined by nearby elements. The architecture of ConvS2S typically consists of an encoder and a decoder. The encoder is composed of several layers of 1D convolutional filters followed by non-linear activation functions. These filters capture different patterns and features in the input sequence, allowing for effective representation learning. The decoder, on the other hand, employs similar convolutional layers but with additional techniques like attention mechanisms to generate the output sequence. Unlike RNN-based models that rely on sequential processing, ConvS2S applies parallel convolutions across the input sequence. This enables more efficient computation and allows for better utilization of parallel processing capabilities, leading to faster training and inference times. The use of convolutions also helps capture local dependencies in the input sequence, which can be beneficial for tasks where context is primarily determined by nearby elements. The architecture of ConvS2S typically consists of an encoder and a decoder. The encoder is composed of several layers of 1D convolutional filters followed by non-linear activation functions. These filters capture different patterns and features in the input sequence, allowing for effective representation learning. The decoder, on the other hand, employs similar convolutional layers but with additional techniques like attention mechanisms to generate the output sequence. Future Internet 2023, 15, 260 25 of 60 3.3. AIGC Input\u2013Output Classification The field of AI content generation encompasses a wide array of aspects, leading to the development of numerous methods. These methods can be categorized into different groups based on the nature of the input and output involved. In this section, we will explore various techniques that enable the generation of AI-driven content by transforming different types of input into desired output. The classification of the input\u2013output types of AIGC is given in Table 5. Table 5. Generative AI Models for Various Input\u2013Output Transformations. Input Output Prescribed Task Technique/ Technology/ Model Ref. Image 3D Image Text-guided 3D object generation: generating 3D objects based on textual descriptions DREAMFUSION [133] Image Blind motion deblurring of a single photograph DeblurGAN [61] DeblurGAN-v2 [56] Image Generate highly realistic and diverse synthetic images StyleGAN [4] Image Blending two Images Gaussian-Poisson Generative Adversarial Network (GP-GAN) [134] Image Image compositing or image blending Geometrically and Color Consistent GANs (GCC-GANs) [64] Image Filling absent pixels in an image or image inpainting Exemplar GANs (ExGANs) [99] Contextual Attention Generative Adversarial Networks (CAGenerative Adversarial Networks (GAN)) [127] PGGAN [126] Image Face aging: generating images that depict a hypothetical future appearance of a person. Age-cGAN [135] Conditional Adversarial Autoencoder (CAAE) [122] Identity-Preserved Conditional Generative Adversarial Networks (IPCGANs) [136] Image Image editing Introspective Adversarial Network (IAN) [124] Image Medical image analysis: segmenting objects or regions in an image SegAN [100] Image Converting low-resolution images to high resolution Multi-Level Densely Connected Super-Resolution Network (mDCSRN) [137] Image Synthesizing a photorealistic frontal view..",
            "url": "https://www.mdpi.com/1999-5903/15/8/260/pdf?version=1690812126"
          }
        ]
      },
      "S3713005270": {
        "id": "S3713005270",
        "text": "The EnTangled Attention (ETA) model has demonstrated improved performance in dense image captioning by analyzing relationships between detected entities, showcasing its effectiveness over traditional LSTM frameworks.",
        "children": [
          {
            "id": "E9499381511",
            "text": "... It computes the topic smoothness value of the current constructed paragraph description at each recurrent step, assessing the coherence of topics across the generated sentences. Future Internet 2023, 15, 260 23 of 60 With these multi-level assessments, the model can generate long yet realistic descriptions, maintaining both sentence-level plausibility and topic coherence. In the CNN-GAN model, a convolutional encoder\u2013decoder network is utilized for generating new content by jointly training it with adversarial networks. This training setup aims to ensure coherence between the generated pixels and the existing ones. These CNN-based methods have demonstrated the ability to generate realistic and plausible content in highly structured images, such as faces, objects, and scenes [127]. Generative Adversarial Networks (GAN) with Denoising Diffusion Probabilistic Models (DDPM) and Transformers: Combination DDPMs, GANs, and transformers can create a hybrid generative AI model with enhanced capabilities. This combination allows for the generation of diverse and high-quality samples while leveraging the strengths of each component. DiffGAN-TTS [128] and ProDiff [53] implement this combination of models. DiffGAN-TTS is a novel text-to-speech (TTS) model that achieves high-fidelity and efficient speech synthesis. It takes inspiration from the denoising diffusion GAN model and models the denoising distribution using an expressive acoustic generator. This generator is trained adversarially to match the true denoising distribution, ensuring high-quality output spectrograms. DiffGAN-TTS ability to allow large denoising steps during inference. This reduces the number of denoising steps required and accelerates the sampling process. To further enhance sampling efficiency, DiffGAN-TTS incorporates an active shallow diffusion mechanism. ProDiff utilizes generator-based parameterization, where the denoising model directly predicts clean data using a neural network [53]. This approach has shown advantages in accelerating sampling from complex distributions. By directly predicting clean data, ProDiff avoids the need to estimate gradients and achieves faster synthesis. Transformer with Recurrent Neural Network (RNN): The combination of transformers and RNNs can leverage the strengths of both architectures, allowing for improved modeling of sequential data with long-term dependencies and global context understanding. This combination is useful for tasks such as speech recognition, time series forecasting, and video processing, where both local temporal dependencies and global context are crucial for accurate predictions. MolT5 [58] implements three baseline models for the tasks of molecule captioning and molecule generation. The first baseline is a four-layer GRU recurrent neural network with a bidirectional encoder. This model leverages the sequential nature of the data and captures contextual information from both past and future. The second baseline is based on the transformer architecture, consisting of six encoder and decoder layers. Transformers utilize self-attention mechanisms to capture global dependencies and have been successful in various sequence-to-sequence tasks. The third baseline is based on the T5 model, a pre-trained sequence-to-sequence model. Three T5 checkpoints, namely small, base, and large, are fine-tuned for molecule captioning and molecule generation. T5 models have shown strong performance in natural language processing tasks. Transformer with Graph Convolutional Network (GCN): For tasks that require graphstructured data, this hybrid model combines the strength of transformers and GCNs. Transformers excel at sequence-to-sequence tasks and have demonstrated success in natural language processing and image processing. GCNs, on the other hand, are especially intended to handle graph-structured data and capture node relationships. This hybrid model can effectively capture both the sequential dependencies of the data and the graphbased relationships by combining transformers and GCNs, enabling enhanced modeling and representation learning in graph-based tasks such as node classification, link prediction, graph generation and molecule structure generation [60]. Transformer with Long Short-Term Memory (LSTM): The transformer architecture with long short-term memory (LSTM) is a type of recurrent neural network known for its ability to capture long-term dependencies in sequential data. Transformers are powerful models for sequence processing, leveraging self-attention mechanisms to capture dependencies across the sequence. The GTR-LSTM [129] encoder provides a graph-based approach to encoding triples, considering the structural relationships between entities in a knowledge Future Internet 2023, 15, 260 24 of 60 graph. By incorporating attention mechanisms and entity masking, the model aims to generate coherent and meaningful output sequences based on the input graph. Vision Transformers with Residual Neural Networks (ResNet): Vision transformers leverage the self-attention mechanism of transformers to capture long-range dependencies and enable effective modeling of image data. The combination of ResNet and vision transformers can benefit from both the local feature extraction capabilities of ResNet and the global context understanding of vision transformers, resulting in improved image understanding and representation [130]. Diffusion probabilistic models with Contrastive Language-Image Pretraining (CLIP): Diffusion modeling is a powerful technique for modeling complex data distributions and generating high-quality samples. CLIP, on the other hand, is a state-of-the-art method for learning visual representations from images and corresponding textual descriptions. DiffusionCLIP [131] combines the power of diffusion modeling and the guidance of CLIP to enable precise and controlled image manipulation. It leverages pretrained diffusion models and the CLIP loss to fine-tune the diffusion model and generate samples that align with a target textual description, which opens new possibilities for image generation and manipulation tasks. Convolutional Neural Network (CNN) with Bidirectional Encoder Representations from Transformers (BERT): CLAP (contrastive learning for audio and text pairing) [47] is a model that jointly trains an audio encoder and a text encoder to learn the similarity or dissimilarity between audio and text pairs. The goal is to enable zero-shot classification by computing embeddings for audio and text and using cosine similarity to measure their similarity. The model takes audio and text pairs as input, which are separately processed by the audio encoder and text encoder. The encoders extract meaningful representations from the audio and text inputs. These representations are then projected into a joint multimodal space using linear projections. Convolutional Sequence-to-Sequence Learning (ConvS2S): This is a neural network architecture that was introduced for sequence-to-sequence tasks, such as machine translation or speech recognition. It leverages convolutional neural networks (CNNs) to process input sequences and generate output sequences, providing an alternative to the commonly used recurrent neural networks (RNNs) Unlike RNN-based models that rely on sequential processing, ConvS2S [132] applies parallel convolutions across the input sequence. This enables more efficient computation and allows for better utilization of parallel processing capabilities, leading to faster training and inference times. The use of convolutions also helps capture local dependencies in the input sequence, which can be beneficial for tasks where context is primarily determined by nearby elements. The architecture of ConvS2S typically consists of an encoder and a decoder. The encoder is composed of several layers of 1D convolutional filters followed by non-linear activation functions. These filters capture different patterns and features in the input sequence, allowing for effective representation learning. The decoder, on the other hand, employs similar convolutional layers but with additional techniques like attention mechanisms to generate the output sequence. Unlike RNN-based models that rely on sequential processing, ConvS2S applies parallel convolutions across the input sequence. This enables more efficient computation and allows for better utilization of parallel processing capabilities, leading to faster training and inference times. The use of convolutions also helps capture local dependencies in the input sequence, which can be beneficial for tasks where context is primarily determined by nearby elements. The architecture of ConvS2S typically consists of an encoder and a decoder. The encoder is composed of several layers of 1D convolutional filters followed by non-linear activation functions. These filters capture different patterns and features in the input sequence, allowing for effective representation learning. The decoder, on the other hand, employs similar convolutional layers but with additional techniques like attention mechanisms to generate the output sequence. Future Internet 2023, 15, 260 25 of 60 3.3. AIGC Input\u2013Output Classification The field of AI content generation encompasses a wide array of aspects, leading to the development of numerous methods. These methods can be categorized into different groups based on the nature of the input and output involved. In this section, we will explore various techniques that enable the generation of AI-driven content by transforming different types of input into desired output. The classification of the input\u2013output types of AIGC is given in Table 5. Table 5. Generative AI Models for Various Input\u2013Output Transformations. Input Output Prescribed Task Technique/ Technology/ Model Ref. Image 3D Image Text-guided 3D object generation: generating 3D objects based on textual descriptions DREAMFUSION [133] Image Blind motion deblurring of a single photograph DeblurGAN [61] DeblurGAN-v2 [56] Image Generate highly realistic and diverse synthetic images StyleGAN [4] Image Blending two Images Gaussian-Poisson Generative Adversarial Network (GP-GAN) [134] Image Image compositing or image blending Geometrically and Color Consistent GANs (GCC-GANs) [64] Image Filling absent pixels in an image or image inpainting Exemplar GANs (ExGANs) [99] Contextual Attention Generative Adversarial Networks (CAGenerative Adversarial Networks (GAN)) [127] PGGAN [126] Image Face aging: generating images that depict a hypothetical future appearance of a person. Age-cGAN [135] Conditional Adversarial Autoencoder (CAAE) [122] Identity-Preserved Conditional Generative Adversarial Networks (IPCGANs) [136] Image Image editing Introspective Adversarial Network (IAN) [124] Image Medical image analysis: segmenting objects or regions in an image SegAN [100] Image Converting low-resolution images to high resolution Multi-Level Densely Connected Super-Resolution Network (mDCSRN) [137] Image Synthesizing a photorealistic frontal view..",
            "url": "https://www.mdpi.com/1999-5903/15/8/260/pdf?version=1690812126"
          },
          {
            "id": "S5001938868",
            "text": "Recent transformer-based models, such as the EnTangled Attention (ETA) model, have been shown to improve image captioning by examining the relationships between detected entities, addressing a key limitation of LSTM frameworks.",
            "children": [
              {
                "id": "E3874012582",
                "text": "..Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 779\u2013788. 51. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. Ssd: Single shot multibox detector. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11\u201314 October 2016; Springer: Berlin/Heidelberg, Germany, 2016; pp. 21\u201337. 52. Lin, T.Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22\u201329 October 2017; pp. 2980\u20132988. 53. Dash, S.K.; Acharya, S.; Pakray, P.; Das, R.; Gelbukh, A. Topic-based image caption generation. Arab. J. Sci. Eng. 2020, 45, 3025\u20133034. [CrossRef] 54. Hu, H.; Gu, J.; Zhang, Z.; Dai, J.; Wei, Y. Relation networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 3588\u20133597. 55. Guo, H.; Zheng, K.; Fan, X.; Yu, H.; Wang, S. Visual attention consistency under image transforms for multi-label image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 729\u2013739. 56. Ghiasi, G.; Lin, T.Y.; Le, Q.V. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 7036\u20137045. 57. Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; Zagoruyko, S. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision, Glasgow, UK, 23\u201328 August 2020; Springer: Berlin/Heidelberg, Germany, 2020; pp. 213\u2013229. 58. Tan, M.; Pang, R.; Le, Q.V. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 10781\u201310790. 59. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation using Tag Relations and Graph Convolutional Networks. In Proceedings of the 2021 5th International Conference on Pattern Recognition and Image Analysis (IPRIA), Kashan, Iran, 28\u201329 April 2021; IEEE: Piscataway, NJ, USA, 2021; pp. 1\u20136. Algorithms 2023, 16, 135 36 of 40 60. Joseph, K.; Khan, S.; Khan, F.S.; Balasubramanian, V.N. Towards open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 19\u201325 June 2021; pp. 5830\u20135840. 61. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation Using Quantization Reweighting Function and Graph Neural Networks. In Proceedings of the International Conference on Service-Oriented Computing, Seville, Spain, 29 November\u20132 December 2022; Springer: Berlin/Heidelberg, Germany, 2022; pp. 46\u201360. 62. Yang, P.; Luo, F.; Chen, P.; Li, L.; Yin, Z.; He, X.; Sun, X. Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling. In Proceedings of the IJCAI, Macao, China, 11\u201312 August 2019; Volume 3, p. 7. 63. Speer, R.; Havasi, C. Representing general relational knowledge in conceptnet 5. In Proceedings of the LREC, Istanbul, Turkey, 23\u201325 May 2012; Volume 2012, pp. 3679\u20133786. 64. Chen, H.; Huang, Y.; Takamura, H.; Nakayama, H. Commonsense knowledge aware concept selection for diverse and informative visual storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, Virtual, 2\u20139 February 2021; Volume 35, pp. 999\u20131008. 65. Li, J.; Shi, H.; Tang, S.; Wu, F.; Zhuang, Y. Informative visual storytelling with cross-modal rules. In Proceedings of the 27th ACM International Conference on Multimedia, Nice, France, 21\u201325 October 2019; pp. 2314\u20132322. 66. Aggarwal, C.C. Data classification. In Data Mining; Springer: Berlin/Heidelberg, Germany, 2015; pp. 285\u2013344. 67. Aggarwal, C.C.; Reddy, C.K. Data clustering. In Algorithms and Applications; Chapman & Hall/CRC Data Mining and Knowledge Discovery Series; Chapman & Hall: London, UK, 2014. 68. Ahmed, M. Data summarization: A survey. Knowl. Inf. Syst. 2019, 58, 249\u2013273. [CrossRef] 69. Mao, J.; Xu, W.; Yang, Y.; Wang, J.; Huang, Z.; Yuille, A. Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv 2014, arXiv:1412.6632. 70. Karpathy, A.; Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3128\u20133137. 71. Vinyals, O.; Toshev, A.; Bengio, S.; Erhan, D. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3156\u20133164. 72. Rennie, S.J.; Marcheret, E.; Mroueh, Y.; Ross, J.; Goel, V. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 7008\u20137024. 73. Ren, Z.; Wang, X.; Zhang, N.; Lv, X.; Li, L.J. Deep reinforcement learning-based image captioning with embedding reward. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 290\u2013298. 74. Gordon, D.; Kembhavi, A.; Rastegari, M.; Redmon, J.; Fox, D.; Farhadi, A. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 4089\u20134098. 75. Patro, B.; Patel, S.; Namboodiri, V. Robust explanations for visual question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Snowmass Village, CO, USA, 1\u20135 March 2020; pp. 1577\u20131586. 76. Wu, Q.; Wang, P.; Shen, C.; Reid, I.; Van Den Hengel, A. Are you talking to me? reasoned visual dialog generation through adversarial learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 6106\u20136115. 77. Chen, C.; Mu, S.; Xiao, W.; Ye, Z.; Wu, L.; Ju, Q. Improving image captioning with conditional generative adversarial nets. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27 January\u20131 February 2019; Volume 33, pp. 8142\u20138150. 78. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning, Lille, France, 7\u20139 July 2015; 2015; pp. 2048\u20132057. 79. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014, arXiv:1409.0473. 80. Ba, J.; Mnih, V.; Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv 2014, arXiv:1412.7755. 81. Mnih, V.; Heess, N.; Graves,..",
                "url": "https://openalex.org/W4323044469",
                "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
                "publication_date": "2023-03-02"
              },
              {
                "id": "E1765349546",
                "text": "..the concept of a narrative, then investigate its generation process from different perspectives. Narrative construction can be studied and categorized into learning-based, feature-map-based, language-based, and narrative length-based methods. Definition 1. A narrative N is a specific subgraph, path, or walk in a Story Model starting from a random node i and ending in another random node j while traversing the Knowledge Graph G with a sequence of vertices in-between them. Graph G is the Knowledge Graph containing the contextualized data information, extracted concepts, objects, topics, captions, metadata, and rules from the image data. Learning-Based Methods. Narrative construction has utilized a variety of learningbased approaches. We classify them into three main categories. A supervised learning method, which is the first category, refers to training the model under the supervision of a teacher. Labels are used to supervise the learning process. In the second category, reinforcement learning (RL) aims to maximize the long-term reward of agents by taking the optimal action (through trial and error) in a particular state. Finally, a generative adversarial network (GAN) trains a generative model by defining the problem as a supervised learning problem consisting of two submodels: a generator and a discriminator. The generator generates fake new samples and the discriminator tries to ascertain whether the input samples are real or fake. Finally, a zero-sum adversarial game is used to train these two models together until the discriminator is fooled half the time. \u2022 Supervised Learning Methods. Multimodal Recurrent Neural Network (m-RNN) architecture has been proposed in [69] to handle these two tasks: (1) image-to-sentence description generation and (2) image and sentence retrieval. Additionally, this architecture is trained by utilizing a log-likelihood cost function. The model parameters are learned by differentiating from the cost function given the input and the backpropagation algorithm. Karlpathy et al. [70] introduce a multimodal RNN architecture that learns to generate narratives for image regions based on inferred alignments (multimodal embeddings of convolutions over images and bi-RNNs over sentences). They map every image and sentence into a shared space of h-dimensional vectors. The supervision is carried out at the entire image and sentence level, so they formulate an image\u2013sentence score based on the individual region scores. Vinyals et al. [71] present an end-to-end solution to the caption generation problem, which is fully trainable by employing stochastic gradient descent, and the loss is the sum of each step\u2019s negative log-likelihood of the correct word. \u2022 Reinforcement Learning (RL). SCST [72] is a reinforcement learning (RL) optimization technique that normalizes rewards by exploiting the output of its test-time inference algorithm. This approach baselines the REINFORCE approach more efficaciously, Algorithms 2023, 16, 135 19 of 40 leading to better results with nondifferentiable evaluation metrics. Ren et al. [73] take a collaborative approach and propose a decision-making framework. They use a policy and value network to predict the word for each step of narrative generation. Specifically, an actor\u2013critic RL algorithm is introduced to learn these two networks. Gordon et al. [74] introduce the Interactive Question Answering (IQA) task (answering questions that demand the agent\u2019s interaction with a dynamic environment) and propose the Hierarchical Interactive Memory Network (HIMN). HIMN is factorized into a Planner, a set of controllers, and a semantic spatial memory. The Planner, formulated as an RL problem, invokes the controllers to explore the environment and answer the question in the IQA task. \u2022 Generative Adversarial Networks (GANs). Patro et al. [75] present a Correlated Collaborative Model (CCM) that guarantees the coherence of the generated textual explanations and answers. CCM collaborates with the answer and explanation features and employs generative adversarial strategies for training. In addition, the results are robust to noise in images and text (even if the model is not trained for noise-based attacks). \u2022 Combined Methods (RL + GAN). To generate more human-like answers to questions, Wu et al. [76] combine RL with GANs and introduce a novel framework. This framework updates the generator by employing the reward of the generator policy at each training step. After sampling data from the dialog history (a sequence of textual data), maximum likelihood estimation (MLE) is used to update the generator. Chen et al. [77] introduce a conditional generative adversarial captioning technique to extend RL-based architectures. Furthermore, CNN- and RNN-based architectures are presented for the discriminator module. The discriminator judges whether a human described the resulting caption or if it is machine-generated. Feature-Map-Based Methods. One of the critical challenges in narrative construction based on an image is extracting visual features which best represent the image content. Therefore, we categorize these methods into three main groups based on various strategies proposed for extracting visual features: CNN-based, attention-based, graph-based, and vision-transformer-based methods, which are discussed in detail in the following paragraphs. \u2022 CNN-Based Methods. Vinyals et al. [71] employ a vision CNN model to extract image features, since CNNs can embed images in fixed-length vectors. They utilize the result feature vector of GoogleNet [141] for the visual feature extraction step. Moreover, Karpathy et al. [70] used the visual features extracted from AlexNet [142]. Many studies employ the CNN modules to extract features such as [72,143,144]. \u2022 Attention-Based Methods. Simple CNN-based methods have the main advantage of being compact and straightforward. At the same time, the extreme compactness and lack of granularity are crucial issues that need to be addressed. An attention module handles this issue by computing attention weights and attending to specific parts of the visual features. Many approaches fit into attention-based methods that can be divided into three main categories: grid-based, region-based, and self-attention. In the following items, these three categories are further discussed. \u2013 Grid-Based Attention Methods. A significant proportion of image captioning methods use the attention mechanism to make captioning more flexible and provide better granularity. Xu et al. [78] introduced an attention-based image captioning model inspired by a recent study in machine translation [79] and object detection [80,81]. The authors proposed two attention-based image caption generators: a \u201csoft\u201d deterministic attention mechanism and a \u201chard\u201d stochastic attention mechanism. An adaptive encoder\u2013decoder model [82] also automatically determines when to rely on the language model and when to look at the image (the spatial CNN features at each of the k grid locations of the image). \u2013 Region-Based Attention Methods. Anderson et al. [83] employ a novel bottomup and top-down attention module (based on Faster R-CNN [49] and task-specific Algorithms 2023, 16, 135 20 of 40 context). First, the bottom-up visual attention module extracts salient regions of an image and represents them using convolutional feature vectors. Then, the top-down module estimates the distribution of attention over image regions (specifies weights of the features). As a result, a weighted average of all image features is the final attended feature vector. The Look-Back and Predict-Forward (LBPF) approach [84] presents two main modules: Look-Back (LB) module and Predict-Forward (PF) module. As the input of the attention module, the LB module concatenates the previous attention vector and the current hidden state. In contrast, the PF module sequentially predicts the two following hidden states based on the current hidden state. For constructing high-quality image captions, the Reflective Decoding Network (RDN) [85] improves the capability of the standard caption generator to handle long sequential modeling by examining the caption\u2019s word consistency. Li et al. [145] also addresses both the tasks of VQA and VQG using the attention mechanism after extracting the visual features from the input image. \u2013 Self-Attention Methods. Despite the widespread use of attention mechanisms in image captioning, we still do not know how closely related attended vectors and given attention queries are. Therefore, Huang et al. [86] proposed the \u201cAttention on Attention\u201d (AoA) module, which specifies the attention results and queries relevancy. Applying AoA to both the encoder and decoder in this research, the authors introduced AoANet for image captioning. Guo et al. [87] first proposed a normalized self-attention (NSA), which indicated that conducting this normalization on the hidden activations inside self-attention is advantageous. Then, a geometric-aware self-attention (GSA) module was proposed to compute the objects\u2019 geometric bias to assist with image comprehension. Furthermore, the self-attention module in the encoder facilitated the EnTangled Attention (ETA) model [88] to examine the detected entities\u2019 relationships. \u2022 Graph-Based Methods. This image encoder incorporates two kinds of visual relationships (semantic and spatial object relationships) in the proposed GCN-LSTM framework [89], which attempts to explore the relations between objects. Based on these visual relationships, they developed graphs over the detected objects in an image and used GCNs proposed in [146]. The Scene Graph Auto-Encoder (SGAE) [90] embeds inductive bias into a dictionary unsupervised. Subsequently, it is shared as a re-encoder for text generation, enhancing the encoder\u2013decoder performance. For captioning, Yao et al. [91] utilize the hierarchical structure in images at the instance, region, and whole image level. This hierarchical structure is analyzed with a tree-structured LSTM model, and each instance-, region-, and image-level feature is improved. Image encoding is also based on a hierarchical scene parsing architecture. \u2022 Vision-Transformer-Based Methods. In this category, the methods [147..",
                "url": "https://openalex.org/W4323044469",
                "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
                "publication_date": "2023-03-02"
              }
            ]
          }
        ]
      },
      "S2844395874": {
        "id": "S2844395874",
        "text": "The integration of attention mechanisms in transformer architectures allows for a more nuanced understanding of spatial relationships among objects in images, enhancing the quality of dense image captions compared to traditional LSTM-based methods.",
        "children": [
          {
            "id": "E4633555150",
            "text": "..IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 290\u2013298. 74. Gordon, D.; Kembhavi, A.; Rastegari, M.; Redmon, J.; Fox, D.; Farhadi, A. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 4089\u20134098. 75. Patro, B.; Patel, S.; Namboodiri, V. Robust explanations for visual question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Snowmass Village, CO, USA, 1\u20135 March 2020; pp. 1577\u20131586. 76. Wu, Q.; Wang, P.; Shen, C.; Reid, I.; Van Den Hengel, A. Are you talking to me? reasoned visual dialog generation through adversarial learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 6106\u20136115. 77. Chen, C.; Mu, S.; Xiao, W.; Ye, Z.; Wu, L.; Ju, Q. Improving image captioning with conditional generative adversarial nets. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27 January\u20131 February 2019; Volume 33, pp. 8142\u20138150. 78. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning, Lille, France, 7\u20139 July 2015; 2015; pp. 2048\u20132057. 79. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014, arXiv:1409.0473. 80. Ba, J.; Mnih, V.; Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv 2014, arXiv:1412.7755. 81. Mnih, V.; Heess, N.; Graves, A. Recurrent models of visual attention. Adv. Neural Inf. Process. Syst. 2014, 27, 2204\u20132212. 82. Lu, J.; Xiong, C.; Parikh, D.; Socher, R. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 375\u2013383. 83. Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; Zhang, L. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 6077\u20136086. 84. Qin, Y.; Du, J.; Zhang, Y.; Lu, H. Look back and predict forward in image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201317 June 2019; pp. 8367\u20138375. 85. Ke, L.; Pei, W.; Li, R.; Shen, X.; Tai, Y.W. Reflective decoding network for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Repulic of Korea, 27 October\u20132 November 2019; pp. 8888\u20138897. 86. Huang, L.; Wang, W.; Chen, J.; Wei, X.Y. Attention on attention for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Repulic of Korea, 27 October\u20132 November 2019; pp. 4634\u20134643. Algorithms 2023, 16, 135 37 of 40 87. Guo, L.; Liu, J.; Zhu, X.; Yao, P.; Lu, S.; Lu, H. Normalized and geometry-aware self-attention network for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 10327\u201310336. 88. Li, G.; Zhu, L.; Liu, P.; Yang, Y. Entangled transformer for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Repulic of Korea, 27 October\u20132 November 2019; pp. 8928\u20138937. 89. Yao, T.; Pan, Y.; Li, Y.; Mei, T. Exploring visual relationship for image captioning. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 684\u2013699. 90. Yang, X.; Tang, K.; Zhang, H.; Cai, J. Auto-encoding scene graphs for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201317 June 2019; pp. 10685\u201310694. 91. Yao, T.; Pan, Y.; Li, Y.; Mei, T. Hierarchy parsing for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Repulic of Korea, 27 October\u20132 November 2019; pp. 2621\u20132629. 92. Aneja, J.; Deshpande, A.; Schwing, A.G. Convolutional image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 5561\u20135570. 93. Wang, Q.; Chan, A.B. Cnn+ cnn: Convolutional decoders for image captioning. arXiv 2018, arXiv:1805.09019. 94. Herdade, S.; Kappeler, A.; Boakye, K.; Soares, J. Image captioning: Transforming objects into words. Adv. Neural Inf. Process. Syst. 2019, 32, 11135\u201311145. 95. Luo, Y.; Ji, J.; Sun, X.; Cao, L.; Wu, Y.; Huang, F.; Lin, C.W.; Ji, R. Dual-level collaborative transformer for image captioning. arXiv 2021, arXiv:2101.06462. 96. Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; Gao, J. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, New York, NY, USA, 7\u201312 February 2020; Volume 34, pp. 13041\u201313049. 97. Johnson, J.; Karpathy, A.; Fei-Fei, L. Densecap: Fully convolutional localization networks for dense captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 4565\u20134574. 98. Yang, L.; Tang, K.; Yang, J.; Li, L.J. Dense captioning with joint inference and visual context. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 2193\u20132202. 99. Kim, D.J.; Choi, J.; Oh, T.H.; Kweon, I.S. Dense relational captioning: Triple-stream networks for relationship-based captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201317 June 2019; pp. 6271\u20136280. 100. Uehara, K.; Mori, Y.; Mukuta, Y.; Harada, T. ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer. In Proceedings of the Companion Proceedings of the Web Conference 2022, Virtual Event/Lyon, France, 25\u201329 April 2022; pp. 716\u2013725. 101. Su, J.; Dai, Q.; Guerin, F.; Zhou, M. BERT-hLSTMs: BERT and hierarchical LSTMs for visual storytelling. Comput. Speech Lang. 2021, 67, 101169. [CrossRef] 102. Riahi Samani, Z.; Ebrahimi Moghaddam, M. Image Collection Summarization Method Based on Semantic Hierarchies. AI 2020, 1, 209\u2013228. [CrossRef] 103. Sharma, V.; Kumar, A.; Agrawal, N.; Singh, P.; Kulshreshtha, R. Image summarization using topic modelling. In Proceedings of the 2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), Kuala Lumpur, Malaysia, 19\u201321 October 2015; IEEE: Piscataway, NJ, USA, 2015; pp. 226\u2013231. 104. Camargo, J.E.; Gonz\u00e1lez, F.A. A multi-class kernel alignment method for image collection summarization. In Proceedings of the Iberoamerican Congress on Pattern Recognition, Guadalajara, Mexico, 15\u201318 November 2009; Springer: Berlin..",
            "url": "https://openalex.org/W4323044469",
            "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
            "publication_date": "2023-03-02"
          },
          {
            "id": "E3874012582",
            "text": "..Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 779\u2013788. 51. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. Ssd: Single shot multibox detector. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11\u201314 October 2016; Springer: Berlin/Heidelberg, Germany, 2016; pp. 21\u201337. 52. Lin, T.Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22\u201329 October 2017; pp. 2980\u20132988. 53. Dash, S.K.; Acharya, S.; Pakray, P.; Das, R.; Gelbukh, A. Topic-based image caption generation. Arab. J. Sci. Eng. 2020, 45, 3025\u20133034. [CrossRef] 54. Hu, H.; Gu, J.; Zhang, Z.; Dai, J.; Wei, Y. Relation networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 3588\u20133597. 55. Guo, H.; Zheng, K.; Fan, X.; Yu, H.; Wang, S. Visual attention consistency under image transforms for multi-label image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 729\u2013739. 56. Ghiasi, G.; Lin, T.Y.; Le, Q.V. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 7036\u20137045. 57. Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; Zagoruyko, S. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision, Glasgow, UK, 23\u201328 August 2020; Springer: Berlin/Heidelberg, Germany, 2020; pp. 213\u2013229. 58. Tan, M.; Pang, R.; Le, Q.V. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 10781\u201310790. 59. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation using Tag Relations and Graph Convolutional Networks. In Proceedings of the 2021 5th International Conference on Pattern Recognition and Image Analysis (IPRIA), Kashan, Iran, 28\u201329 April 2021; IEEE: Piscataway, NJ, USA, 2021; pp. 1\u20136. Algorithms 2023, 16, 135 36 of 40 60. Joseph, K.; Khan, S.; Khan, F.S.; Balasubramanian, V.N. Towards open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 19\u201325 June 2021; pp. 5830\u20135840. 61. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation Using Quantization Reweighting Function and Graph Neural Networks. In Proceedings of the International Conference on Service-Oriented Computing, Seville, Spain, 29 November\u20132 December 2022; Springer: Berlin/Heidelberg, Germany, 2022; pp. 46\u201360. 62. Yang, P.; Luo, F.; Chen, P.; Li, L.; Yin, Z.; He, X.; Sun, X. Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling. In Proceedings of the IJCAI, Macao, China, 11\u201312 August 2019; Volume 3, p. 7. 63. Speer, R.; Havasi, C. Representing general relational knowledge in conceptnet 5. In Proceedings of the LREC, Istanbul, Turkey, 23\u201325 May 2012; Volume 2012, pp. 3679\u20133786. 64. Chen, H.; Huang, Y.; Takamura, H.; Nakayama, H. Commonsense knowledge aware concept selection for diverse and informative visual storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, Virtual, 2\u20139 February 2021; Volume 35, pp. 999\u20131008. 65. Li, J.; Shi, H.; Tang, S.; Wu, F.; Zhuang, Y. Informative visual storytelling with cross-modal rules. In Proceedings of the 27th ACM International Conference on Multimedia, Nice, France, 21\u201325 October 2019; pp. 2314\u20132322. 66. Aggarwal, C.C. Data classification. In Data Mining; Springer: Berlin/Heidelberg, Germany, 2015; pp. 285\u2013344. 67. Aggarwal, C.C.; Reddy, C.K. Data clustering. In Algorithms and Applications; Chapman & Hall/CRC Data Mining and Knowledge Discovery Series; Chapman & Hall: London, UK, 2014. 68. Ahmed, M. Data summarization: A survey. Knowl. Inf. Syst. 2019, 58, 249\u2013273. [CrossRef] 69. Mao, J.; Xu, W.; Yang, Y.; Wang, J.; Huang, Z.; Yuille, A. Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv 2014, arXiv:1412.6632. 70. Karpathy, A.; Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3128\u20133137. 71. Vinyals, O.; Toshev, A.; Bengio, S.; Erhan, D. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3156\u20133164. 72. Rennie, S.J.; Marcheret, E.; Mroueh, Y.; Ross, J.; Goel, V. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 7008\u20137024. 73. Ren, Z.; Wang, X.; Zhang, N.; Lv, X.; Li, L.J. Deep reinforcement learning-based image captioning with embedding reward. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 290\u2013298. 74. Gordon, D.; Kembhavi, A.; Rastegari, M.; Redmon, J.; Fox, D.; Farhadi, A. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 4089\u20134098. 75. Patro, B.; Patel, S.; Namboodiri, V. Robust explanations for visual question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Snowmass Village, CO, USA, 1\u20135 March 2020; pp. 1577\u20131586. 76. Wu, Q.; Wang, P.; Shen, C.; Reid, I.; Van Den Hengel, A. Are you talking to me? reasoned visual dialog generation through adversarial learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 6106\u20136115. 77. Chen, C.; Mu, S.; Xiao, W.; Ye, Z.; Wu, L.; Ju, Q. Improving image captioning with conditional generative adversarial nets. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27 January\u20131 February 2019; Volume 33, pp. 8142\u20138150. 78. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning, Lille, France, 7\u20139 July 2015; 2015; pp. 2048\u20132057. 79. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014, arXiv:1409.0473. 80. Ba, J.; Mnih, V.; Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv 2014, arXiv:1412.7755. 81. Mnih, V.; Heess, N.; Graves,..",
            "url": "https://openalex.org/W4323044469",
            "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
            "publication_date": "2023-03-02"
          }
        ]
      },
      "S7255739676": {
        "id": "S7255739676",
        "text": "Transformers, through their self-attention mechanism, can effectively model long-range dependencies in image data, which is a significant improvement over LSTMs that struggle with such tasks.",
        "children": [
          {
            "id": "E0134451776",
            "text": "Authors:\nSalman Khan\nMuzammal Naseer\nMunawar Hayat\nWaqas Syed\nZamir\nShahbaz Fahad\nMubarak Khan\nShah\n\u2022 S Khan\nF S Khan\nS W Zamir\nM Shah\nAbstract\nAstounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges towards the application of transformer models in computer vision.\n1 INTRODUCTION\nT RANSFORMER models [1] have recently demonstrated exemplary performance on a broad range of language tasks e.g., text classification, machine translation [2] and question answering. Among these models, the most popular ones include BERT (Bidirectional Encoder Representations from Transformers) [3] , GPT (Generative Pre-trained Transformer) v1-3 [4] - [6] , RoBERTa (Robustly Optimized BERT Pre-training) [7] and T5 (Text-to-Text Transfer Transformer) [8] . The profound impact of Transformer models has become more clear with their scalability to very large capacity models [9] , [10] . For example, the BERT-large [3] model with 340 million parameters was significantly outperformed by the GPT-3 [6] model with 175 billion parameters while the latest mixture-of-experts Switch transformer [10] scales up to a whopping 1.6 trillion parameters!\nThe breakthroughs from Transformer networks in Natural Language Processing (NLP) domain has sparked great interest in the computer vision community to adapt these models for vision and multi-modal learning tasks (Fig. 1 ).\nManuscript received March, 2021.\nHowever, visual data follows a typical structure (e.g., spatial and temporal coherence), thus demanding novel network designs and training schemes. As a result, Transformer models and their variants have been successfully used for image recognition [11] , [12] , object detection [13] , [14] , segmentation [15] , image super-resolution [16] , video understanding [17] , [18] , image generation [19] , text-image synthesis [20] and visual question answering [21] , [22] , among several other use cases [23] - [26] . This survey aims to cover such recent and exciting efforts in the computer vision domain, providing a comprehensive reference to interested readers.\nTransformer architectures are based on a self-attention mechanism that learns the relationships between elements of a sequence. As opposed to recurrent networks that process sequence elements recursively and can only attend to short-term context, Transformers can attend to complete sequences thereby learning long-range relationships. Although attention models have been extensively used in both feed-forward and recurrent networks [27] , [28] , Transformers are based solely on the attention mechanism and have a unique implementation (i.e., multi-head attention) optimized for parallelization. An important feature of these models is their scalability to high-complexity models and large-scale datasets e.g., in comparison to some of the other alternatives such as hard attention [29] which is stochastic in nature and requires Monte Carlo sampling for sampling attention locations. Since Transformers assume minimal prior knowledge about the structure of the problem as compared to their convolutional and recurrent counterparts [30] - [32] , they are typically pre-trained using pretext tasks on largescale (unlabelled) datasets [1] , [3] . Such a pre-training avoids costly manual annotations, thereby encoding highly expres- sive and generalizable representations that model rich relationships between the entities present in a given dataset. The learned representations are then fine-tuned on the downstream tasks in a supervised manner to obtain favorable results.\nThis paper provides a holistic overview of the transformer models developed for computer vision applications. We develop a taxonomy of the network design space and highlight the major strengths and shortcomings of the existing methods. Other literature reviews mainly focus on the NLP domain [33] , [34] or cover generic attention-based approaches [27] , [33] . By focusing on the newly emerging area of visual transformers, we comprehensively organize the recent approaches according to the intrinsic features of self-attention and the investigated task. We first provide an introduction to the salient concepts underlying Transformer networks and then elaborate on the specifics of recent vision transformers. Where ever possible, we draw parallels between the Transformers used in the NLP domain [1] and the ones developed for vision problems to flash major novelties and interesting domain-specific insights. Recent approaches show that convolution operations can be fully replaced with attention-based transformer modules and have also been used jointly in a single design to encourage symbiosis between the two complementary set of operations. This survey finally details open research questions with an outlook towards the possible future work.\n2 FOUNDATIONS\nThere exist two key ideas that have contributed towards the development of conventional transformer models. (a) The first one is self-attention, which allows capturing 'longterm' dependencies between sequence elements as compared to conventional recurrent models that find it challenging to encode such relationships. (b) The second key idea is that of pre-training 1 on a large (un)labelled corpus in a (self)supervised manner, and subsequently fine-tuning to the target task with a small labeled dataset [3] , [7] , [38] . Below, we provide a brief tutorial on these two ideas (Sec. 2.2 and 2.1), along with a summary of seminal Transformer 1. Several recent Vision Transformers demonstrate that the model can be learned end-to-end on ImageNet-1K without any dedicated pretraining phase [35] - [37] . However, the performance generally remains lower than the pre-trained counter-parts. Fig. 2 : An example self-attention block used in the vision domain [39] . Given the input sequence of image features, the triplet of (key, query, value) is calculated followed by attention calculation and applying it to reweight the values. A single head is shown here and an output projection (W) is finally applied to obtain output features with the same dimension as the input. Figure adapted from [39] . networks (Sec. 2.3 and 2.4) where these ideas have been applied. This background will help us better understand the forthcoming Transformer based models used in the computer vision domain (Sec. 3).\n2.1 Self-Attention in Transformers\nGiven a sequence of items, self-attention estimates the relevance of one item to other items (e.g., which words are likely to come together in a sentence). The self-attention mechanism is an integral component of Transformers, which explicitly models the interactions between all entities of a sequence for structured prediction tasks. Basically, a selfattention layer updates each component of a sequence by aggregating global information from the complete input sequence. Lets denote a sequence of n entities (x 1 , x 2 , \u2022 \u2022 \u2022 x n ) by X \u2208 R n\u00d7d , where d is the embedding dimension to represent each entity. The goal of self-attention is to capture the interaction amongst all n entities by encoding each entity in terms of the global contextual information. This is done by defining three learnable weight matrices to transform Queries (W Q \u2208 R d\u00d7dq ), Keys (W K \u2208 R d\u00d7d k ) and Values (W V \u2208 R d\u00d7dv ), where d q = d k . The input sequence X is first projected onto these weight matrices to get Q = XW Q , K = XW K and V = XW V . The output Z \u2208 R n\u00d7dv of the self attention layer is,\nZ = softmax QK T d q V.\nFig. 3 : Architecture of the Transformer Model [1] . The model was first developed for the language translation task where an input sequence in one language is required to be converted to the output sequence in another language. The Transformer encoder (middle row) operates on the input language sequence and converts it to an embedding before passing it on to the encoder blocks. The Transformer decoder (bottom row) operates on the previously generated outputs in the translated language and the encoded input sequence from the middle branch to output the next word in the outpu",
            "url": "https://openalex.org/W4206706211",
            "title": "Transformers in Vision: A Survey",
            "publication_date": "2022-01-06"
          },
          {
            "id": "E3874012582",
            "text": "..Girshick, R.; Farhadi, A. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 779\u2013788. 51. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. Ssd: Single shot multibox detector. In Proceedings of the European Conference on Computer Vision, Amsterdam, The Netherlands, 11\u201314 October 2016; Springer: Berlin/Heidelberg, Germany, 2016; pp. 21\u201337. 52. Lin, T.Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22\u201329 October 2017; pp. 2980\u20132988. 53. Dash, S.K.; Acharya, S.; Pakray, P.; Das, R.; Gelbukh, A. Topic-based image caption generation. Arab. J. Sci. Eng. 2020, 45, 3025\u20133034. [CrossRef] 54. Hu, H.; Gu, J.; Zhang, Z.; Dai, J.; Wei, Y. Relation networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 3588\u20133597. 55. Guo, H.; Zheng, K.; Fan, X.; Yu, H.; Wang, S. Visual attention consistency under image transforms for multi-label image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 729\u2013739. 56. Ghiasi, G.; Lin, T.Y.; Le, Q.V. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 7036\u20137045. 57. Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; Zagoruyko, S. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision, Glasgow, UK, 23\u201328 August 2020; Springer: Berlin/Heidelberg, Germany, 2020; pp. 213\u2013229. 58. Tan, M.; Pang, R.; Le, Q.V. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 10781\u201310790. 59. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation using Tag Relations and Graph Convolutional Networks. In Proceedings of the 2021 5th International Conference on Pattern Recognition and Image Analysis (IPRIA), Kashan, Iran, 28\u201329 April 2021; IEEE: Piscataway, NJ, USA, 2021; pp. 1\u20136. Algorithms 2023, 16, 135 36 of 40 60. Joseph, K.; Khan, S.; Khan, F.S.; Balasubramanian, V.N. Towards open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 19\u201325 June 2021; pp. 5830\u20135840. 61. Lotfi, F.; Jamzad, M.; Beigy, H. Automatic Image Annotation Using Quantization Reweighting Function and Graph Neural Networks. In Proceedings of the International Conference on Service-Oriented Computing, Seville, Spain, 29 November\u20132 December 2022; Springer: Berlin/Heidelberg, Germany, 2022; pp. 46\u201360. 62. Yang, P.; Luo, F.; Chen, P.; Li, L.; Yin, Z.; He, X.; Sun, X. Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling. In Proceedings of the IJCAI, Macao, China, 11\u201312 August 2019; Volume 3, p. 7. 63. Speer, R.; Havasi, C. Representing general relational knowledge in conceptnet 5. In Proceedings of the LREC, Istanbul, Turkey, 23\u201325 May 2012; Volume 2012, pp. 3679\u20133786. 64. Chen, H.; Huang, Y.; Takamura, H.; Nakayama, H. Commonsense knowledge aware concept selection for diverse and informative visual storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, Virtual, 2\u20139 February 2021; Volume 35, pp. 999\u20131008. 65. Li, J.; Shi, H.; Tang, S.; Wu, F.; Zhuang, Y. Informative visual storytelling with cross-modal rules. In Proceedings of the 27th ACM International Conference on Multimedia, Nice, France, 21\u201325 October 2019; pp. 2314\u20132322. 66. Aggarwal, C.C. Data classification. In Data Mining; Springer: Berlin/Heidelberg, Germany, 2015; pp. 285\u2013344. 67. Aggarwal, C.C.; Reddy, C.K. Data clustering. In Algorithms and Applications; Chapman & Hall/CRC Data Mining and Knowledge Discovery Series; Chapman & Hall: London, UK, 2014. 68. Ahmed, M. Data summarization: A survey. Knowl. Inf. Syst. 2019, 58, 249\u2013273. [CrossRef] 69. Mao, J.; Xu, W.; Yang, Y.; Wang, J.; Huang, Z.; Yuille, A. Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv 2014, arXiv:1412.6632. 70. Karpathy, A.; Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3128\u20133137. 71. Vinyals, O.; Toshev, A.; Bengio, S.; Erhan, D. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7\u201312 June 2015; pp. 3156\u20133164. 72. Rennie, S.J.; Marcheret, E.; Mroueh, Y.; Ross, J.; Goel, V. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 7008\u20137024. 73. Ren, Z.; Wang, X.; Zhang, N.; Lv, X.; Li, L.J. Deep reinforcement learning-based image captioning with embedding reward. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 290\u2013298. 74. Gordon, D.; Kembhavi, A.; Rastegari, M.; Redmon, J.; Fox, D.; Farhadi, A. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 4089\u20134098. 75. Patro, B.; Patel, S.; Namboodiri, V. Robust explanations for visual question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Snowmass Village, CO, USA, 1\u20135 March 2020; pp. 1577\u20131586. 76. Wu, Q.; Wang, P.; Shen, C.; Reid, I.; Van Den Hengel, A. Are you talking to me? reasoned visual dialog generation through adversarial learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 6106\u20136115. 77. Chen, C.; Mu, S.; Xiao, W.; Ye, Z.; Wu, L.; Ju, Q. Improving image captioning with conditional generative adversarial nets. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 27 January\u20131 February 2019; Volume 33, pp. 8142\u20138150. 78. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning, Lille, France, 7\u20139 July 2015; 2015; pp. 2048\u20132057. 79. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014, arXiv:1409.0473. 80. Ba, J.; Mnih, V.; Kavukcuoglu, K. Multiple object recognition with visual attention. arXiv 2014, arXiv:1412.7755. 81. Mnih, V.; Heess, N.; Graves,..",
            "url": "https://openalex.org/W4323044469",
            "title": "Storytelling with Image Data: A Systematic Review and Comparative Analysis of Methods and Tools",
            "publication_date": "2023-03-02"
          }
        ]
      }
    },
    "evaluation": {
      "scores": {
        "accuracy": {
          "reasoning": "The given answer accurately identifies the advantages of transformer architectures over LSTMs in dense image captioning, such as long-range dependency modeling and enhanced spatial understanding. It also mentions specific transformer models like the EnTangled Attention (ETA) model and Attention on Attention (AoA) module, which are relevant to the context. However, it lacks some details present in the golden answer, such as the challenges faced by LSTMs and the broader implications of using transformers. Overall, the answer is factually correct but could be more comprehensive in addressing all aspects of the question.",
          "score": 8
        },
        "completeness": {
          "reasoning": "The given answer covers several key points regarding the advantages of transformers in dense image captioning, including specific models and their functionalities. However, it does not fully address the challenges of traditional LSTM frameworks as outlined in the golden answer, such as the forget gate mechanism and equal weighting of regions. This omission affects the completeness of the response, as it does not provide a holistic view of the topic. Therefore, while it covers many important aspects, it lacks some critical details.",
          "score": 6
        },
        "relevance": {
          "reasoning": "The given answer is highly relevant to the complex question, as it directly addresses the limitations of LSTMs and how transformers can enhance dense image captioning. It discusses specific transformer models and their advantages, which aligns well with the question's focus. However, the lack of discussion on the challenges faced by LSTMs slightly detracts from its relevance to the full scope of the question. Nonetheless, it remains focused on the core topic.",
          "score": 8
        },
        "evidence_quality": {
          "reasoning": "The given answer cites several transformer models and their contributions to dense image captioning, which are backed by references. However, the references provided are less authoritative compared to those in the golden answer, which includes a broader range of literature and foundational studies. The quality of evidence is good but not as robust as that found in the golden answer, which affects the overall reliability of the claims made.",
          "score": 7
        },
        "clarity": {
          "reasoning": "The given answer is generally clear and well-articulated, making it easy to understand the advantages of transformer architectures. The use of bullet points helps in organizing the information effectively. However, some technical jargon may be challenging for readers unfamiliar with the subject, which slightly impacts clarity. Overall, it communicates its points well but could benefit from simpler explanations in some areas.",
          "score": 7
        },
        "logical_structure": {
          "reasoning": "The answer is logically structured, with clear headings and bullet points that guide the reader through the key advantages of transformer architectures. Each point builds on the previous one, creating a coherent flow of information. However, it could improve by including a brief introduction or conclusion to frame the discussion more effectively. Overall, the organization is good, but there is room for enhancement.",
          "score": 8
        },
        "evidence_support": {
          "reasoning": "The given answer uses evidence to support its claims about the advantages of transformers, mentioning specific models and their functionalities. However, the depth of evidence is not as extensive as in the golden answer, which provides a more comprehensive synthesis of literature. While the evidence presented is relevant, it could be strengthened by including more diverse sources and examples to bolster the claims made.",
          "score": 6
        },
        "depth_of_analysis": {
          "reasoning": "The analysis in the given answer is solid, discussing the advantages of transformers in detail and mentioning specific models. However, it lacks a deeper exploration of the challenges faced by LSTMs and how transformers specifically address these challenges, which is a significant aspect of the golden answer. The depth of analysis is good but not exhaustive, as it does not fully engage with the complexities of the topic.",
          "score": 6
        },
        "objectivity": {
          "reasoning": "The given answer maintains an objective tone, focusing on the technical aspects of transformer architectures without bias. It presents the advantages of transformers over LSTMs without disparaging the latter, which is commendable. However, it could enhance objectivity by acknowledging any potential limitations or challenges associated with transformer models, which would provide a more balanced perspective.",
          "score": 7
        },
        "synthesis": {
          "reasoning": "The given answer synthesizes information from various transformer models and their applications in dense image captioning. However, it does not integrate information from multiple sources or viewpoints as effectively as the golden answer, which provides a broader synthesis of the literature. The integration of ideas is present but could be more comprehensive, particularly in discussing the interplay between LSTMs and transformers.",
          "score": 6
        }
      }
    },
    "average_score": 6.9
  },
  {
    "id": "https://openalex.org/W4391232091",
    "meta": {
      "id": "https://openalex.org/W4391232091",
      "title": "Roadmapping the next generation of silicon photonics",
      "publication_date": "2024-01-25",
      "cited_by_count": 51,
      "topics": "Silicon Photonics Technology, Photonic Reservoir Computing for Neural Computation, Optical Fiber Communication Technologies",
      "keywords": "Silicon Photonics, Transceiver",
      "concepts": "Photonics, Silicon photonics, Computer science, CMOS, Telecommunications, Transceiver, Electronic engineering, Engineering, Materials science, Wireless, Optoelectronics",
      "best_oa_location_pdf_url": "https://www.nature.com/articles/s41467-024-44750-0.pdf",
      "pdf_urls_by_priority": [
        "https://www.nature.com/articles/s41467-024-44750-0.pdf"
      ],
      "text_type": "full_text",
      "successful_pdf_url": "https://www.nature.com/articles/s41467-024-44750-0.pdf",
      "openalex_rank": 47,
      "num_tokens": 15551,
      "url": "https://www.nature.com/articles/s41467-024-44750-0.pdf"
    },
    "text": "Perspective https://doi.org/10.1038/s41467-024-44750-0\nRoadmapping the next generation of silicon\nphotonics\nSudip Shekhar 1 , Wim Bogaerts 2, Lukas Chrostowski1, John E. Bowers3,\nMichael Hochberg4\n, Richard Soref5 & Bhavin J. Shastri 6\nSilicon photonics has developed into a mainstream technology driven by\nadvances in optical communications. The current generation has led to a\nproliferation of integrated photonic devices from thousands to millions\u0002mainly in the form of communication transceivers for data centers. Products in\nmany exciting applications, such as sensing and computing, are around the\ncorner. What will it take to increase the proliferation of silicon photonics from\nmillions to billions of units shipped? What will the next generation of silicon\nphotonics look like? What are the common threads in the integration and\nfabrication bottlenecks that silicon photonic applications face, and which\nemerging technologies can solve them? This perspective article is an attempt\nto answer such questions. We chart the generational trends in silicon photo\u0002nics technology, drawing parallels from the generational definitions of CMOS\ntechnology. We identify the crucial challenges that must be solved to make\ngiant strides in CMOS-foundry-compatible devices, circuits, integration, and\npackaging. We identify challenges critical to the next generation of systems\nand applications\u2014in communication, signal processing, and sensing. By iden\u0002tifying and summarizing such challenges and opportunities, we aim to sti\u0002mulate further research on devices, circuits, and systems for the silicon\nphotonics ecosystem.\nThe generational roadmap\nFigure 1 maps the evolution of silicon photonics1,2. Silicon-based pho\u0002tonic integrated circuits (PICs) were introduced in 19853 and low-loss\nwaveguides in a thick silicon on insulator (SOI) process demonstrated\nin 1991\u2013924,5\n. Various optical devices were next demonstrated6, and\nsoon, silicon photonics was in the small-scale integration (SSI) era\u2014\nwith 1-to-10 components on a PIC. They included demonstrations of\nhigh-speed pn junction modulators7\u20139 and photodetectors (PDs)10\u201313, as\nwell as heterogeneous integration of a III-V laser to a silicon PIC14. The\nnext era ushered in the commercial success of silicon photonics. With\n10-to-500 components on a PIC, this medium-scale integration (MSI)\nera saw successful demonstration and adoption of Mach-Zehnder\nmodulator (MZM) in intensity-modulated direct-detect (IMDD) trans\u0002ceivers within data centers\u2014both single-wavelength15 and multi\u0002wavelength16\u201319. Microring-modulator (MRM)-based IMDD transcei\u0002vers (see Fig. 2a) demonstrated the multiplexing and energy-efficiency\nbenefits of PIC technology20\u201322. Coherent transceivers in silicon pho\u0002tonics/electronics platforms proved that the technology could\nReceived: 4 May 2023\nAccepted: 3 January 2024\nCheck for updates\n1\nDepartment of Electrical & Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver V6T1Z4 BC, Canada. 2Department of Infor\u0002mation Technology, Ghent University - IMEC, Technologiepark-Zwijnaarde 126, Ghent 9052, Belgium. 3\nDepartment of Electrical & Computer Engineering,\nUniversity of California Santa Barbara, Santa Barbara 93106 CA, USA. 4\nLuminous Computing, 4750 Patrick Henry Drive, Santa Clara 95054 CA, USA. 5College\nof Science and Mathematics, University of Massachusetts Boston, 100 William T. Morrissey Blvd., Boston 02125 MA, USA. 6\nDepartment of Physics, Engineering\nPhysics & Astronomy, Queen\u2019s University, 64 Bader Lane, Kingston K7L3N6 ON, Canada. e-mail: sudip@ece.ubc.ca; bhavin.shastri@queensu.ca\nNature Communications | (2024) 15:751 1\n1234567890():,;1234567890():,;\ncompete in performance with their LiNbO3 photonic and III-V elec\u0002tronic counterparts23\u201325. Besides communications, silicon photonics\nalso found new applications such as evanescent-field biosensors26.\nSilicon photonics is now embarking on the next era of large-scale\nintegration (LSI)\u2014towards 500-to-10,000 components on the same\nchip. Applications for LSI include LIDAR (see Fig. 2b)27\u201332, image\nprojection33, photonic switching34, photonic computing35\u201339, pro\u0002grammable circuits40, and multiplexed biosensors41. Even VLSI\n(>10,000 components) prototypes have now been\ndemonstrated30,32,34. In the field of communication, which has been the\nessential market driver for silicon photonics, silicon photonics has\ntransformed from a challenger technology in the SSI era to arguably a\ndominant technology in the MSI era for intra-, and inter-datacenter\ninterconnects, and it is poised to become the incumbent technology in\nthe LSI era. For co-packaged optics (CPO) to succeed, high\u0002performance computing to scale22, and disaggregated computing to\nbecome a reality42, silicon photonics will be pivotal.\nSilicon photonics: technology perspective\nThrough the generations of CMOS process development, many\nmaterials were added to silicon to reduce the Power, improve the\nPerformance, and shrink the Area\u2014often called the PPA metrics. The\nadditions include Al and Cu for metal traces, Ge for inducing strain and\nenabling heterojunction BJTs, and silicon nitride (SiN) for passivation\nand diffusion barriers. The CMOS R&D budgets and commercial mar\u0002kets are orders of magnitude larger than for silicon photonics, so it is\nnatural for silicon photonics foundries to learn from and adopt the\ninnovations from CMOS processes. Hence, we have seen a similar\ntrend in silicon photonics process development. Besides p/n dopants\nfor high-speed modulation, two materials that are now natively sup\u0002ported by several foundries are (1) Ge high-speed photodetectors43,\nand (2) SiN to expand the wavelength range, enable higher optical\npower, and support waveguides with lower loss and better phase\ncontrol in interferometric devices44.\n102\n101\n103\n104\n105\n106\n100\n1985 1990 1995 2000 2005 2010 2015 2020 2025\nVLSI\nSSI\nMSI\nLSI\nCI P/ st nenop moCf or eb muN\nYear\nInP/Si or GaAs/Si Heterogeneous\nSi Monolithic\nInP Monolithic\nFig. 1 | Timeline for the number of components on a silicon photonic integrated\ncircuit (PIC) over generations of small-scale, medium-scale, large-scale, and\nvery-large-scale integration (SSI, MSI, LSI, VLSI, respectively). A component is a\nunit cell that is combined with other unit cells to build a circuit, such as a wave\u0002guide, directional coupler, heater, grating coupler, etc. Heterogeneous silicon\nphotonics lags hybrid by approximately two years. For comparison, data for InP\u0002based integrated photonics is also shown. In general, the higher the number of\nhigh-speed modulators, the more challenging the scaling. The figure is adapted\nfrom refs. 1,2.\nTunable Laser\nSMLL\nFrequency Discrimination\nCoherent Detection\nBeam Steering\nFig. 2 | Illustrative renditions of LSI silicon photonic systems capturing current\nand future technologies. a WDM Transceiver: A semiconductor mode-locked laser\n(SMLL) provides multi-wavelength continuous-wave (CW) light to an array of\ncompact, WDM-capable modulators and filters. Reflection control circuits limit\nback reflections into the laser. High-speed photodetectors (PDs) carry out the O/E\nconversion. b The electrical current is then amplified by transimpedance amplifiers\n(TIAs) and limiting amplifiers. Analog-to-digital converters (ADCs) are used to\ndigitize the signal for further digital signal processing (DSP). Monitoring PDs are\nused for control and stabilization of wavelength, phase shift, and temperature.\nDigital-to-analog converters (DACs) and drivers are used for E/O modulation of the\ndigital signal. Dynamic random-access memory (DRAM) provides large memory\naccess. Micro-controllers (\u03bcC) may be used to offload some of the digital proces\u0002sing as well. c LIDAR: A tunable laser provides frequency chirped light to a network\nof phase shifters, circulators/duplexers and coherent frontend for homodyne/\nheterodyne frequency-modulated CW (FMCW) ranging and detection. Beam\nsteering is done using optical phase arrays (OPAs) or focal-plane arrays (FPAs).\nDelay line interferometers aid in calibrating the received beat frequencies and\nsupport chirp linearization by directly controlling the tunable laser or a modulator\nand various forms of error correction through DSP.\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 2\nShrinking the area will be a key focus for the next decade of silicon\nphotonics process development for the LSI and VLSI era. In reality, the\nbiggest density limitations rarely come from device size; the spacing\nbetween waveguides to eliminate crosstalk is much larger than the size\nof the actual waveguides. For radio-frequency (RF) devices, spacings\nbetween active elements\u2014which are microns in critical dimension\u2014are\noften in the hundreds of microns, to eliminate RF crosstalk. Shrinking\nthese \u2018blank spaces\u2019 requires very detailed systems-level simulation\nand aggressive multi-physics modeling, and will be at the heart of\nmaking chips smaller, cheaper, and higher density. The passives\nthemselves are generally limited in size reduction by the index contrast\nand the operating wavelength of 1\u20132 \u03bcm. There is still some headroom\nwith the use of inverse design techniques to shrink passive building\nblocks, but the waveguide itself cannot really shrink much below\ntoday\u2019s 400\u2013500 nm width for silicon platforms. However, significant\nscaling is still possible in the optical I/O couplers and high-speed\nmodulators. For coupling to optical fibers, V-grooves with edge cou\u0002plers provide low-loss, easy-to-package connectivity at the cost of a\nconsiderable chip area. Edge couplers without V-grooves are smaller\nbut require more precise active alignment and surface treatment\n(polishing, dicing), thereby increasing cost. Multicore fibers are an\nattractive solution for efficiently using limited photonic beachfront\naround the edges of a chip45. The main alternative coupling approach is\nthrough grating couplers, which are compact, provide the flexibility of\npositioning on the chip surface, enable wafer level testing, and can also\nbe realized with low insertion loss (IL), but suffer from polarization and\ntemperature sensitivity and lower optical bandwidth46. Passive align\u0002ment packaging techniques, such as photonic wire bonding (PWB)47,\noffer an attractive potential alternative. Using computer vision and\nautomation, PWBs can be fabricated in polymer photoresist through\ntwo-photon absorption between two coupling sites allowing up to\n30 \u03bcm of offset. Simple alignment markers are used to locate the\ncoupling sites, and the sites do not require strict pitch or large foot\u0002prints, thereby providing a passive-aligned, low-loss, scalable port\ncount. In another passive alignment technique for pluggable connec\u0002tion, the complexity and accuracy requirement can be moved from\nfiber assembly to wafer-level manufacturing, where a fiber-receptor die\ncan be flip-chip integrated to the silicon photonic die with a glass\nspacer48. Using a combination of V-grooves and mirrors in the fiber\u0002receptor die, and mirrors and surface couplers on the silicon photonic\ndie, a confocal imaging assembly tolerant to >10 \u03bcm relative dis\u0002placements of the two dies can be realized, providing a passive\u0002aligned, low-loss, scalable port-count and pluggable connector48. More\nreliability studies for these passive alignment-based assemblies will be\nhelpful for broad adoption.\nE/O modulation\nThe central quest for the next decade in shrinking photonic chips and\nthus increasing density is to find the elusive \u2018ideal\u2019 modulator in silicon\nphotonics\u2014small in length (L), requiring a small drive voltage to incur a\n\u03c0 phase shift (V\u03c0), offering low propagation loss (\u03b1) and IL, and for\nseveral applications, highly linear and with large \u22123 dB E/O bandwidth\n(BW)49. Also, this modulator is preferably a phase shifter, as this\nenables higher-order coherent modulation formats.\nHigh-speed modulators. The often-used efficiency figure-of-merit\n(FoMefficiency) of waveguide-based modulators (Table 1) is \u03b1V\u03c0L. For\nMRMs, which are very compact, the loss due to \u03b1 becomes less\ncritical50, and a better FoMefficiency inculcates the IL and Vpp (the peak\u0002to-peak voltage swing for a specific optical modulation amplitude or\nOMA). All modulators suffer from a tradeoff between FoMefficiency and\nE/O BW49\u201351. Finally, the power consumed in the driver depends on the\nmodulator impedance as seen by the driver. A resistive impedance (a\nterminated traveling-wave modulator) consumes static (DC) and\ndynamic (AC) power, whereas a high impedance (capacitive)\nconsumes primarily dynamic power. A high IL also is a proxy for higher\npower consumption since the laser power needs to be increased to\ncompensate for the losses.\nBesides the topology, the PPA metrics for a modulator depend on\nthe material and mechanism used for modulation. Table 2 shows the\ndifferent materials used for modulators in various silicon photonic\nprocesses. p-n dopants utilizing the free-carrier plasma dispersion are\nnatively available in all commercial silicon photonic foundries today,\nsupporting 60 GHz or even higher E/O BW. Currently, the commercial\nmarket is dominated by such devices, in the form of traveling-wave\nMZM modulators52. Plasma dispersion in Si leads to mediocre\nFoMefficiency, with high IL for average OMA. Carrier accumulation allows\nfor shorter MZMs, but with BW limitations53,54. When implemented as\nMRMs, the devices are much smaller, but IL and OMA remain sub\u0002optimal to support LSI/VLSI ICs.\nWith Ge PDs already supported by most commercial silicon\nphotonics foundries, various teams have attempted to use GeSi, a\nrelated but not identical technology, to implement a better modulator.\nGeSi electro-absorption modulators (EAM) based on the Franz\u0002Kelydysh effect can operate in the C/L band at high E/O BW. How\u0002ever, they are generally not optically broadband since they use band\u0002edge modulation for absorption. For O-band operations, modulators\nutilizing the quantum-confined Stark effect (QCSE) still suffer from a\nlarge IL55. While there have been multiple academic and commercial\nefforts in this space, it is unclear whether these modulators will find\ntheir way into future generations of commercial devices.\nHeterogeneous integration of modulator technologies\u2014InP, thin\u0002film LiNbO3 on insulator (LNOI), or thin-film BaTiO3 (BTO)\u2014with Si can\nbe done using die-to-die, die-to-wafer, or wafer-to-wafer direct\n(molecular) bonding or adhesive-assisted bonding. Die-to-wafer\nbonding provides the flexibility of using known-good dies, increasing\nyield. Wafer-to-wafer bonding remains expensive because the size\nmismatches between the SOI acceptor wafers (200 mm or 300 mm)\nand the modulator donor wafers (150 mm or smaller) lead to wastage.\nIntegration proximity of the (dissimilar) materials in direct\nbonding facilitates superior optical coupling and heat transportation\nbetween them56. However, very smooth and clean surfaces are\nrequired. Chemical mechanical polishing (CMP) procedures, already\nused in high-volume manufacturing (HVM) for heterogeneous direct\nbonding of InP to Si for lasers, must be optimized for a scalable\nmodulator integration pathway. Annealing is needed for strong\nmolecular bonding and outgassing, but the pre-processed SOI wafer\nsignificantly restricts the annealing temperature. Therefore, \u201clow\u0002temperature\u201d annealing at  100 GHz E/O BW modulators are attractive\nfor both telecom and data center applications, they require electronics\ncapable of driving them at such speeds. Unless V\u03c0 (or Vpp) is reduced\nsignificantly, such electronics will consume a lot of power, regardless\nof CMOS/BiCMOS/III-V implementation.\nPhase-Shifters for tuning and switching. Many photonic applications\nrequire phase shifters that consume little or no power and have a low\nTable 2 | Comparing different modulation materials and mechanisms in their readiness to be integrated into a commercial\nCMOS foundry, and in power, performance (as of 2023) and area (PPA) metrics\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 4\n\u03b1V\u03c0L for configuration, tuning and switching. For certain applications,\nthese phase shifters should be fast as well, but 10s of GHz E/O BW is not\nneeded. While in many circuits light only passes through one high\u0002speed modulator, it will have to traverse many low-speed phase shif\u0002ters for tuning and switching, thereby compounding the penalty of\npower consumption and \u03b1V\u03c0L. Metal heaters (or doped waveguides)\nutilizing the thermo-optic effect are available in all foundry platforms\ntoday. They have 1\u201310 \u03bcs response time, and consume considerable\npower, generating thermal crosstalk, and thus limiting LSI/VLSI scal\u0002ing. But they do not introduce optical loss, a significant advantage over\nother alternatives. Improving thermal insulation reduces their power\nconsumption by > 10 \u00d7 at the expense of an even higher response\ntime76. Even > 100 \u00d7 improvement is possible by folding the wave\u0002guides to increase interaction with the metal heaters, but that results in\nan IL77.\nThe final set of materials and techniques listed in Table 2 are\nattractive alternatives to heaters. They include liquid crystals (LC),\nMEMS/NOEMS, and phase change materials (PCMs). LC on silicon\n(LCOS) tuning for display applications has been demonstrated at a\nlarge scale, and LC has also been the technology of choice for free\u0002space wavelength-selective switches. As phase shifters, they leverage\nbirefringence to demonstrate a strong electro-optic effect. The align\u0002ment of the LC molecules can be controlled by applying electrical\nvoltage ( 8\u03bb WDM LSI PICs needing multiple lasers,\ngain elements, etc., remain to be thoroughly demonstrated.\nAnother commercially successful technique in HVM (>million/\nyear)14 has been heterogeneous integration, where multiple materials\nor epitaxial stacks are processed together into one silicon chip at wafer\nscale. Again, various strategies have been adopted105,106. They include\nbonding III-V chips to Si with coarse alignment followed by post\u0002processing the Si wafer to make quantum-well (QW) lasers\n(Fig. 3e)14,18,19,22. Thermal isolation of the gain medium by the buried\noxide (BOX) and the mismatched CTE must be carefully addressed for\nhigh-temperature operation, efficiency, and reliability. Placing redun\u0002dant lasers helps improve the failure-in-time (FIT) rates22. Benefits of\nthe heterogeneous approach include sub-dB coupling loss and a\nmechanism to leverage the low-loss external cavity in silicon to sig\u0002nificantly reduce the laser linewidth using self-injection locking107.\nAnother longer term approach, desirable for quantum-dot (QD)\nlasers, is to directly grow epitaxial gain material on the Si wafer108. Due\nto its lower linewidth enhancement factor, \u03b1H, QD lasers enable lower\nlinewidth and reduced sensitivity to reflections94,109. They also have\nlower threshold current density. Monolithic integration using hetero\u0002epitaxial growth (Fig. 3f), where the III-V substrate is not even needed,\nremains the end goal, with several recent progress and more to come1\n.\nMultiple silicon photonic foundries are developing hybrid or\nheterogeneous laser solutions. For scalability purposes, the foundries\nwill likely favor a technology that lends itself to multi-wavelength\nTable 3 | Comparing different techniques to attach a laser to a silicon PIC in PPA metrics (as of 2023), cost, testing, packaging\nstyle, and suitability for applications\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 6\nsupport, which is crucial for several LSI applications. It is likely that\nscale-out will be supported first by bonding multiple single-wavelength\nlasers22. Comb lasers110 such as passive semiconductor mode-locked\nlasers (SMLLs)111 are being actively pursued by various research groups.\nDFB arrays ensure large output optical power in each wavelength,\nwhereas, in the SMLLs, the power is split between the wavelengths. The\npresence of the saturable absorber further lowers the total (and hence\nper wavelength) output power of SMLLs. However, an SMLL is sig\u0002nificantly smaller than a DFB array. The linewidth of passive SMLLs111 is\nusually lower than DFB arrays22. More R&D is expected for SMLLs to\ndemonstrate higher power, reliability, and lifetime in the next decade.\nSuch requirements for DWDM applications are even more stringent,\nand any temperature drift creates inter-channel crosstalk112,113.\nAvalanche photodetectors\nMost of the silicon photonic applications are constrained by limited\noutput power and WPE of the laser, and the high IL in the circuits. An\nalternative is to improve the SNR at the detection stage (Fig. 2). Low\u0002voltage APDs which have large \u22123 dB O/E BW, high overall\nresponsivity114, and simultaneously low noise will be beneficial for\nreceiver signal-to-noise ratio (SNR) improvement115,116. It is important to\nnote that the overall responsivity (in A/W) and low noise is crucial. A\nlarge multiplication gain for an APD which has a poor intrinsic\nresponsivity does not lead to a superior performance. Although rela\u0002tively easier to achieve in Si APDs at 850 nm117, simultaneous (at the\nsame bias voltage) optimization of gain-BW-noise has remained chal\u0002lenging for low-voltage Si/Ge APDs118 or Si resonant APDs119,120 in C/L/O\nbands. In comparison to Ge PDs, APDs generally have inferior BW,\nlinearity and power handling, which limits their use in various appli\u0002cations. APDs also need to be biased optimally and stabilized for\ntemperature and voltage drift, but that is less challenging117 than what\nhas already been demonstrated for microring circuits50.\nDelay\nSeveral silicon photonic applications require hundreds of picoseconds\nto nanoseconds of delay. Examples include microwave photonics,\noptical phase-locked loops (OPLLs), frequency discriminators (Fig. 2),\nlaser linewidth reduction circuits, OPAs, optical coherence tomo\u0002graphy (OCT), and gyroscopes. Many of these applications also require\ntunability in 10 s of picoseconds and broadband operation121. Realizing\nsuch a delay in silicon photonics with low-loss and low-area has been\nvery challenging122. Resonant devices provide a narrowband delay. Si\nor SiN delay lines are difficult to tune and require narrow bends leading\nto significant scattering and radiation losses. Shallow etched ridge\nwaveguides or ultrathin waveguides break compatibility with the 220-\nnm processes. Modifying the fabrication process without sacrificing\nthe performance of other photonic components remains\nchallenging123.\nSilicon photonics: systems perspective\nPhotonics & electronics interplay\nSilicon PICs almost always exist in conjunction with electronic ICs\n(EICs). When we look at systems based on photonic chips, the land\u0002scape today is almost 100% dominated by data communication, and\nwe expect this to continue for the near future. In this context, EICs\nserve two purposes (Fig. 2): (1) Enable E/O and O/E conversions of the\nend-to-end data. (2) Bias, control and compensate for temperature and\nfabrication variations. Thus, photonics serve electronics by providing\nthe data links, and electronics serve photonics by providing control\nand readout and digital signal processing (DSP). A major difference\nbetween photonics and electronics is that photons don\u2019t interact and\nthus are excellent for transmission of information, whereas electrons\ninteract and repel each other and thus make good switches and\ncomputing elements. Each silicon photonic switch therefore requires a\ncorresponding electronic switch. On the whole, the number of\ntransistors in the EIC that must accompany an LSI PIC are orders of\nmagnitude larger than the number of components in the PIC. Here lies\na natural interplay, since transistors consume much lower power in (1)\nswitching, (2) providing gain (both linear and limiting), and (3) offering\nhigh precision, while being orders of magnitude smaller than the\nphotonic components124. On the other hand, the photonic components\n(1) enable lower frequency-dependent loss when moving data over a\nlonger distance compared to copper, (2) may provide lower latency\nthrough asynchronous and repeaterless data movement, and (3) ease\nparallelism of very high-speed data on an optical waveguide (through\nWDM). When the data is already in the optical domain, photonic signal\nswitching or processing can become attractive. The former is a widely\ndeployed technology, while the latter has yet to make the leap from\nresearch to product to replace DSP functionality. Thus, it is good to be\ncognizant of the respective virtues of the PIC and EIC technologies. For\nexample, the E/O and O/E overhead of processing electronic data in the\nphotonic domain must be carefully analyzed. Conversely, silicon\nphotonics provides opportunities to shrink large optical systems, and\nbring new applications (such as in sensing and imaging) to reality,\nwhich electronics cannot enable by itself. Finally, silicon photonics\noperates on a carrier wave of hundreds of THz, while silicon electronics\nis limited to sub-THz. Such differing attributes open attractive co\u0002design opportunities, such as designing electronic clocks with ultra\u0002low phase noise125.\nPhotonics & electronics ecosystem\nIt is insightful to look at the electronics industry ecosystem briefly.\nMoore\u2019s law demonstrates that the cost per component goes down\nwith every generation of CMOS technology reducing the critical\ndimensions of the transistors. This scaling is enabled by an exponential\nincrease over time in the economic scale of the semiconductor\nindustry, which allows the industry to pay for ever more expensive\nfoundries and process development. Foundries enable many users to\naccess these advanced processes, without each needing to pay to\ndevelop the process on their own. At the most extreme, the MPW\n(multi-project wafer) runs that the foundries host allow multiple\nusers to share the costs of a single wafer run to develop products\ncost-efficiently.\nAs processes mature, yields go up, and costs come down. The\nfoundries and third-party intellectual property (IP) providers enable a\nprocess design kit (PDK) and design IP libraries, allowing the custo\u0002mers to build incredibly complex electronic circuits and get them right\nthe first time. By relying on both proven devices and proven circuit\u0002level IP, the designers can focus on system-on-chip (SoC) integration\nwithout ever touching the transistor level in several cases.\nOnce the chips are fabricated, there is a rich ecosystem of test\nhouses, packaging service providers, and so forth. Electrical wire\u0002bonding (Fig. 4a) and flip-chip bonding (with C4 bumps and micro\u0002bumps, Fig. 4b) are reliable and popular means of packaging, with the\nlatter providing more bumps instead of just peripheral connections.\nMore advanced packaging techniques (see Fig. 4) such as through\u0002silicon via (TSV), TSV-less interposers, and heterogeneous integration\nare used to improve signal integrity, power and thermal distribution,\nand die yield by breaking complex and large SoCs into smaller\nchiplets126. Because the FPGAs, GPUs, and CPUs are produced in HVM,\nthe overall cost still goes down despite the complex packaging tech\u0002niques. Nevertheless, judicious packaging decisions are made to avoid\nunnecessary complexity; generally, the simplest package is best, and\nadvanced packaging techniques (chip on wafer, chip stacking, etc.)\ntend to be introduced only when no other alternative is feasible.\nThe photonics industry has several similarities but also many stark\ndifferences. Just like in the electronics industry, increasing the number\nof photonic components is not always about reducing cost, but is often\nabout providing new functionality, improved performance, or reduced\narea per component. MPW runs are now available at many foundries,\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 7\nalthough mature PDKs, and abstraction languages are still in very early\nstages. Third-party IP support is mostly non-existent thus far. Com\u0002panies wall off the most advanced PIC processes to protect their\ninvestment and IP (reminiscent of the early decades of the CMOS\nindustry, acting as virtual integrated device manufacturers (IDMs),\nmaintaining differentiation at the process and PDK level. Meanwhile,\nacademic research mainly focuses on improving the devices.\nPhotonic foundries face a significant dilemma: Their customers\noften demand that they customize their processes, which involves a\ngreat deal of R&D expense, and endangers the reliability and yield of\nthe final wafers. Driving customers into a standard process is the\nsolution for this, but in order to do that, the customers need to see\nsignificant value in stability and in a settled PDK and IP ecosystem; only\na few designers see the world this way, because so many of the\nmembers of the design community today were trained as device\npeople, rather than SoC designers. Changing process parameters often\nseems to such designers to be the easiest way to generate performance\ndifferentiation, but the downstream costs for such changes can be very\nhigh from a reliability and process maintenance perspective. As more\ndesigners who are used to the idea of settled PDKs graduate and come\ninto the field, disruptive process changes will slowly become less and\nless common; the foundries will also likely grow ever more resistant to\nprocess changes from customers that are not justified by substantial\npurchase commitments.\nThe overall yield for silicon photonics products is still lower than\ntheir CMOS electronic counterparts. Additional factors at the process,\ndesign, and packaging level account for the difference: fabrication127,128\nand thermal sensitivity, lack of robust PDK components and variation\u0002and-mismatch aware models127,128, design flow methodologies still\nmissing hierarchical simulations, schematic driven layout and layout\u0002versus-schematic verification127, custom process modifications for\nspecific components, challenges with epitaxial growth, Ge integration\nfor photodetection, integration of laser (whether at the die or package\nlevel), laser FIT, and fiber connectivity. Only a handful of HVM silicon\nphotonics products are shipping today, requiring the fab to timeshare\nthe production with other processes, and adding another source of\nyield impact.\nPhotonics & electronics co-integration\nThe option to integrate the PIC with the EIC has been around since the\nfirst commercially successful silicon photonic product1,127,129.\nDeveloping a monolithic EPIC process (Fig. 4e), starting with a CMOS\n(or BiCMOS) SOI process and optimizing it for photonic applications,\nhas been demonstrated several times130\u2013132 successfully. From the\nperspective of commercialization and time-to-market, a monolithic\nEPIC often \u2018seems to\u2019 be the superior technology of choice (Table 4).\nHigh-speed circuits such as drivers and TIAs can be colocated next to\nmodulators and PDs, reducing parasitics and power consumption133.\nControllers (thermal, wavelength) can be designed and placed\nnext to the photonic components without needing dedicated pads.\nFor LSI applications, a monolithic EPIC can simplify packaging\ncomplexity significantly. However, when the die area is dominated\nby photonics, photonic components being orders of magnitude lar\u0002ger than their electronic counterparts124, the overall die cost can\nincrease significantly without arguably making full use of CMOS\ndevices. This analysis has to be done case-by-case for individual\nproducts.\nIn principle, microring-based circuits appear to be very appealing\nfor monolithic EPIC processes until the next-generation modulator\nwith a superior FoMefficiency is developed (see the \u201cHigh-speed mod\u0002ulators\u201d section). But to conclude whether they make sense in a given,\nspecific application, a complete systems analysis is necessary;\nmicrorings come with considerable control overhead and perfor\u0002mance tradeoffs, especially at very high speeds. If the application\nrequires high-speed ADC/DAC and especially DSP (Fig. 2), another\nfinFET EIC must also be added to save power consumption, as the\nfastest monolithic EPIC process today in 45-nm CMOS SOI is still sev\u0002eral generations slower (in fanout delay) than finFET processes. Inte\u0002grating photonics directly onto CMOS wafers below the 45 nm node is\nunlikely to occur in the next few years; doing so does not make eco\u0002nomic or technical sense in a world where chip-on-wafer bonding\nbetween PICs and scaled microelectronics is comparatively\nstraightforward.\nOther possibilities for EPICs have also been explored. Adding\nphotonics to an older generation CMOS process leads to high-power\nand slower drivers and TIAs, leading to worse transceiver designs and\nrendering them unattractive to the biggest customers of silicon pho\u0002tonics\u2014datacom and telecom. Nevertheless, such a process is appeal\u0002ing to university researchers as it opens up opportunities to co-design\nand innovate new EPIC circuits134,135 at low cost and packaging effort.\nOn the other hand, multiple efforts are underway to integrate tran\u0002sistors onto the same wafers as silicon photonic devices136. However,\nPIC electronic chip\nEWB\nEIC\nbump\nPIC\nEIC\nInterposer\nPIC EIC\nEPIC\nelectronic chip\nEIC\nPIC TOV\na\nb\ne\nc\nd\nelectronic chip\nEIC\nPIC TSV\nFig. 4 | Comparing different techniques to attach a PIC to an electronic IC (EIC).\na Electrical wire bonding (EWB) side-by-side. b 2.5D flip-chipped side-by-side or\nstacked. c Hybrid 3D TSV (Through-Silicon Via). d Heterogenous 3D with TOV\n(Through-Oxide Via). e Monolithic electronic photonic IC (EPIC).\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 8\ndoing so has thus far involved unacceptable compromises to the\nperformance of the bipolar electronics.\nMost of the silicon photonic transceivers in HVM today are based\non a 2.5D integration approach, where the PIC and EIC(s) are designed,\nsized, optimized, tested in their best respective processes, and then\nflip-chipped to an interposer substrate17,19,24,25,137 (Fig. 4b, Table 4). The\nEIC process can be chosen from one of the many CMOS/SiGe foun\u0002dries. Multiple EIC chips can also be flip-chipped, such as (1) a SiGe chip\nor scaled-CMOS chip with a reasonably large breakdown voltage to\npermit high-swing drivers and a reasonable switching speed to support\nthe RF speed requirements, and (2) an advanced FinFET chip for DSP/\nADC/DAC138. An EIC process with faster transistors may even com\u0002pensate for the parasitic capacitance due to additional pad, ESD, and\nrouting (compared to a monolithic EPIC solution). For LSI applications\nwhere most PIC components require electronics at a relatively low\nspeed (such as LIDAR), flip-chip solutions seem reasonable32. However,\nfor LSI applications that need many high-speed drive/readout lines, a\nflip-chip solution means many RF traces on the interposer, leading to\ncomplexity and crosstalk considerations. In either case, the size of the\nPIC is increased due to the necessity of many I/O bumps, though with\nmicrobumping and copper pillar technologies to realize a stacked flip\u0002chipped 2.5D package139,140 (Fig. 4b, bottom), these increases are often\ncommercially negligible. The parasitics and interconnects are also\nreduced compared to their side-by-side counterparts. A hybrid 3D\nintegration can be considered in some cases, where the EIC is flip\u0002chipped on the (larger) PIC chip and uses advanced techniques such as\nTSVs or through-oxide vias (TOVs) (Fig. 4c, Table 4). The RF lines still\nneed to be routed from the small EIC to several places on the PIC,\nwhich remains challenging. A WoW heterogeneous 3D integration is\nalso being researched where the photonics wafer is flipped and verti\u0002cally attached with the CMOS wafer through oxide-bonding, the silicon\nhandle on the photonics wafer is removed, and TOVs are formed at the\nwafers\u2019 interface141,142; further improvements are expected for the\nperformance of photonic components in such an integration tech\u0002nology (Fig. 4d, Table 4). One possibility is to use multiple EICs 3D\nintegrated on the PIC.\nOverall, the application, performance specifications and the\nvolume of shipments (affecting the cost) will decide whether a more\nexpensive monolithic EPIC with simpler packaging, a multi-chip 2.5D\nintegration with more complex packaging, or a 3D integration with\nmore complex processing/packaging is the right choice (Table 4). We\nexpect that all of these scenarios will co-exist, just like in the electro\u0002nics ecosystem.\nSilicon photonics: applications perspective\nIn this section, we describe the top technical impediments to the\nsuccess of various silicon photonics applications (Table 5), connecting\nthem to some of the challenges and opportunities discussed in pre\u0002vious sections. We limit the impediments to PIC/EIC technology only,\nexcluding economic, regulatory, market, and other factors such as\nchemistry, biomarkers, quantum advantage, etc. We also do not delve\ninto the benefits of silicon photonics for these applications since most\nof the previous works describe them in detail.\nFor IMDD transceivers (XVRs) to further improve their energy\nefficiency (pJ/b) and scale to higher data rates, the modulator\nFoMefficiency needs further reduction, and the \u22123 dB E/O BW needs to be\nimproved towards 100 GHz. Improving the WPE of lasers is essential\nfor most applications but especially crucial for communication and\ncomputing applications. Efficient multi-wavelength light sources are\nalso needed with adequately large power in each wavelength. Low\u0002noise, large gain-bandwidth APDs in O/L/C bands could provide an SNR\nimprovement without significant power consumption penalty, but\nhistorically their bandwidth, linearity, noise, and power handling\ncharacteristics have prevented their use at the highest bandwidths.\nFinally, amplifying PD signals using high-gain, low-noise TIAs remains a\ncrucial challenge. Several equalization-based techniques have been\nrecently demonstrated to limit the noise using low-BW TIAs143, but\nmost operate on the assumption that the receiver clock is available.\nFor coherent transceivers to be competitive inside data centers,\nadditional challenges (vs. IMDD) must be solved. Linearity require\u0002ments for the TIAs and drivers are more stringent137,144, and the reliance\non power-hungry DSP needs to be reduced as much as possible. One\nTable 4 | Comparing different techniques to attach a PIC to an electronic IC (EIC) in PPA metrics (as of 2023), cost, test\npossibilities, packaging style, and suitability for applications\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 9\nstrategy being explored by researchers is moving some signal pro\u0002cessing tasks into the optical domain145,146 leveraging integrated pho\u0002tonics and analog electronic circuits. The latter requires significant\nelectronic-photonic co-design effort, opening up several opportunities\nfor CMOS designers to leverage the expertise from mixed-signal and\nRF ICs.\nHigh throughput network switches for short-reach to long-haul\nmarkets require the phase shifters to have excellent FoMefficiency to\nenable large fabrics. The switching must incur low power consump\u0002tion, low loss and demonstrate a large extinction ratio. For applications\nthat permit slower switching speeds, insulated metal heaters in inter\u0002ferometric switches are currently the popular implementation\nchoice147, but technologies such as MEMS/NOEMS look promising34.\nLong-term reliability and demonstration in large-scale fabrics co\u0002integrated with electronics and packaged with optical I/Os are needed.\nPolarization diversity and wavelength considerations further compli\u0002cate the scaling and packaging considerations. Applications requiring\nfast switching are even more challenging since high-speed modulators\nwith comparatively inferior FoMefficiency further deteriorate IL and\nextinction ratio. Regardless of the switching speed requirements, the\ninherent losses in large switch fabrics require optical amplification,\nnecessitating the integration of SOAs, ideally uncooled, for energy\nefficiency considerations.\nPractical quantum communication and computing applications\nrequire LSI-VLSI photonic components with advanced CMOS con\u0002trollers. For chip-scale discrete-variable quantum key distribution\n(QKD), the foremost requirements are the cryo-compatible photonic/\nelectronic readout and control of superconducting nanowire single\u0002photon detector (SPD) arrays; developing low-loss, low-power cryo\u0002modulators and cryo-compatible WDM mux/demux; and integrating\nsingle-photon source (SPS) arrays atthe transmitter in a low-noise, low\u0002crosstalk chip-scale photonic-electronic solution. Superconducting\nnanowire SPDs operate at telecom wavelengths, facilitating the use of\nexisting optical fibers as a quantum channel. Besides massive paralle\u0002lization, reducing the loss in the receiver and improving the SPD per\u0002formance will help increase the transmission rate148. For quantum\ncomputing applications, the challenges are similar, but require much\nlarger scalability of qubit control/readout, including the photonics and\nlow-latency control electronics149. The quality of qubits is, of course,\nparamount. Scalability of control/readout degrades with IL\u2014every\nphoton lost degrades the capability of the quantum system in an\nexponential way. Ultra-low-loss couplers are therefore needed to\nconnect to the PIC.\nPhotonic computing involves analog computation and processing\nof information within the photonic domain37,38. This requires handling\nmulti-level signaling150 and increasing the precision of weight control151\nto ensure a high SNR. Such improvements are crucial to achieve\naccuracy comparable to the incumbent CMOS EIC compute engines76.\nAnother challenge is access to high-speed memory to prevent a\nmemory bottleneck, especially for activations and tasks that are not\nweight-stationary. Photonic computing uses high parallelism, so it is\nessential to reduce the IL of passive and active devices (modulators,\nphase shifters) and boost the output power of multi-wavelength lasers\nto accommodate larger network sizes. In addition, for neural networks,\nefficiently implementing programmable nonlinearities stands out as a\nsignificant hurdle37.\nFor automobile driving, silicon photonics LIDARs are positioning\nthemselves as a solid-state challenger to Time-of-Flight (ToF) LIDARs\nutilizing mechanical or MEMS-based scanning. LIDARs consist of two\nsubsystems\u2014ranging and beam steering, both of which can use silicon\nphotonics. ToF and frequency-modulated CW (FMCW) are ranging\ntechniques. FMCW provides the benefits of (1) coherently detecting\nsignals down to a few photons, (2) robustness to interference from\nambient sources, and (3) simultaneous distance and velocity mea\u0002surement. All of the necessary components for coherent detection can\nbe integrated on a single chip. For beam steering, two integrated\npossibilities exist: (1) Optical phase arrays (OPAs), based on continuous\ntunable phase shifters and gratings32. Bulk optics solutions, such as\nspinning mirrors and oscillating mirrors, have the advantage of being\ncheap, mature, and simple; displacing such solutions with an on-chip\nOPA will be a significant challenge. For an OPA to emit a single beam,\nthe grating antennas need to be spaced less than half a wavelength (in\nfree space)\u2014a challenging proposition for 2D beam steering on a sili\u0002con chip. Therefore, silicon photonic OPAs typically have gratings\narranged for beam steering in 1D and the wavelength of the laser is\nswept to steer the beam in the other direction. (2) Focal plane arrays\n(FPAs) based on on-chip switch networks and grating couplers31. These\ninclude 2D FPAs, utilizing MEMS switches29,30, or 1D FPA with wave\u0002length steering. Regardless of the solution, low-power (10 s of nW) and\nimproved FoMefficiency phase shifters are important, and necessary for\nbeam steering. Improved lasers are the next challenge. For 1D OPAs or\nFPAs, multi-wavelength lasers can relax wavelength tuning28. For\nFMCW demodulation, narrow linewidth ( 40 GHz driver with\n4.5 \u00d7 bandwidth extension for a 272 Gb/s dual-polarization 16-\nQAM silicon photonic transmitter. In 2019 IEEE International Solid\u0002State Circuits Conference - (ISSCC), 484\u2013486 (2019).\n26. Iqbal, M. et al. Label-free biosensor arrays based on silicon ring\nresonators and high-speed optical scanning instrumentation. IEEE\nJ. Sel. Top. Quantum Electron. 16, 654\u2013661 (2010).\n27. Poulton, C. V. et al. 8192-element optical phased array with 100\u2218\nsteering range and flip-chip CMOS. In Conference on Lasers and\nElectro-Optics, 4\u20133 (Optica Publishing Group, 2020).\n28. Riemensberger, J. et al. Massively parallel coherent laser ranging\nusing a soliton microcomb. Nature 581, 164\u2013170 (2020).\n29. Zhang, X., Kwon, K., Henriksson, J., Luo, J. & Wu, M. C. Large-scale\nsilicon photonics focal plane switch array for optical beam steer\u0002ing. In Optical Fiber Communication Conference (OFC) 2021, 4\u20132\n(Optica Publishing Group, 2021).\n30. Zhang, X., Kwon, K., Henriksson, J., Luo, J. & Wu, M. C. A large\u0002scale microelectromechanical-systems-based silicon photonics\nlidar. Nature 603, 253\u2013258 (2022).\n31. Rogers, C. et al. A universal 3D imaging sensor on a silicon pho\u0002tonics platform. Nature 590, 256\u2013261 (2021).\n32. Poulton, C. V. et al. Coherent lidar with an 8,192-element optical\nphased array and driving laser. IEEE J. Sel. Top. quantum Electron.\n28, 1\u20138 (2022).\n33. Raval, M., Yaacobi, A. & Watts, M. R. Integrated visible light phased\narray system for autostereoscopic image projection. Opt. Lett. 43\n15, 3678\u20133681 (2018).\n34. Seok, T. J., Kwon, K., Henriksson, J., Luo, J. & Wu, M. C. Wafer-scale\nsilicon photonic switches beyond die size limit. Optica 6,\n490\u2013494 (2019).\n35. Ramey, C. Silicon photonics for artificial intelligence acceleration:\nHotchips 32. In 2020 IEEE Hot Chips 32 Symposium (HCS),\n1\u201326 (2020).\n36. Huang, C. et al. A silicon photonic\u2013electronic neural network for\nfibre nonlinearity compensation. Nat. Electron. 4, 837\u2013844 (2021).\n37. Shastri, B. J. et al. Photonics for artificial intelligence and neuro\u0002morphic computing. Nat. Photonics 15, 102\u2013114 (2021).\n38. Bandyopadhyay, S. et al. Single chip photonic deep neural net\u0002work with accelerated training. Preprint at https://arxiv.org/abs/\n2208.01623 (2022).\n39. Ashtiani, F., Geers, A. J. & Aflatouni, F. An on-chip photonic deep\nneural network for image classification. Nature 606,\n501\u2013506 (2022).\n40. Bogaerts, W. et al. Programmable photonic circuits. Nature 586,\n207\u2013216 (2020).\n41. Reed, B. D. et al. Real-time dynamic single-molecule protein\nsequencing on an integrated semiconductor device. Science 378,\n186\u2013192 (2022).\n42. Michelogiannakis, G. et al. Efficient intra-rack resource dis\u0002aggregation for HPC using co-packaged DWDM photonics. In IEEE\nInternational Conference on Cluster Computing (CLUSTER),\n158\u2013172 (2023).\n43. Pinguet, T. et al. High-volume manufacturing platform for silicon\nphotonics. Proc. IEEE 106, 2281\u20132290 (2018).\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 12\n44. Bauters, J. F. et al. Silicon on ultra-low-loss waveguide photonic\nintegration platform. Opt. Express 14, 544\u2013555 (2013).\n45. Lindenmann, N. et al. Connecting silicon photonic circuits to\nmulticore fibers by photonic wire bonding. J. Light. Technol. 33,\n755\u2013760 (2015).\n46. Cheng, L., Mao, S., Li, Z., Han, Y. & Fu, H. Y. Grating couplers on\nsilicon photonics: design principles, emerging trends and prac\u0002tical issues. Micromachines 11, 666 (2020).\n47. Blaicher, M. et al. Hybrid multi-chip assembly of optical commu\u0002nication engines by in situ 3D nano-lithography. Light Sci. Appl. 9,\n71 (2020).\n48. Israel, A. et al. Photonic plug for scalable silicon photonics\npackaging. in Optical Interconnects XX, Vol. 11286 (eds. Schr\u00f6der,\nH. & Chen, R. T.) 1128607 (SPIE, 2020).\n49. Taghavi, I. et al. Polymer modulators in silicon photonics: review\nand projections. Nanophotonics 11, 3855\u20133871 (2022).\n50. Sun, J. et al. A 128 Gb/s PAM4 silicon microring modulator with\nintegrated thermo-optic resonance tuning. J. Light. Technol. 37,\n110\u2013115 (2019).\n51. Yu, H. et al. Trade-off between optical modulation amplitude and\nmodulation bandwidth of silicon micro-ring modulators. Opt.\nExpress 22, 15178\u201315189 (2014).\n52. Murray, B., Antony, C., Talli, G. & Townsend, P. D. Predistortion for\nhigh-speed lumped silicon photonic mach-zehnder modulators.\nIEEE Photonics J. 14, 1\u201311 (2022).\n53. Wu, X. et al. A 20Gb/s NRZ/PAM-4 1V transmitter in 40 nm CMOS\ndriving a Si-photonic modulator in 0.13 \u03bcm CMOS. In IEEE Inter\u0002national Solid-State Circuits Conference Digest of Technical\nPapers. 128\u2013129 (2013).\n54. Talkhooncheh, A. H. et al. A 2.4 pJ/b 100 Gb/s 3D-integrated PAM\u00024 optical transmitter with segmented SiP MOSCAP modulators\nand a 2-channel 28 nm CMOS driver. In IEEE International Solid\u0002State Circuits Conference (ISSCC), Vol. 65, 284\u2013286 (2022).\n55. Srinivasan, S. A. et al. 60Gb/s waveguide-coupled O-band GeSi\nquantum-confined Stark effect electro-absorption modulator. In\nOptical Fiber Communication Conference (OFC) 2021, 1\u20133 (Optica\nPublishing Group, 2021).\n56. Liang, D., Roelkens, G., Baets, R. & Bowers, J. E. Hybrid integrated\nplatforms for silicon photonics. Materials 3, 1782\u20131802 (2010).\n57. Weigel, P. O. et al. Bonded thin film lithium niobate modulator\non a silicon photonics platform exceeding 100 GHz 3-dB elec\u0002trical modulation bandwidth. Opt. Express 26, 23728\u201323739\n(2018).\n58. Wang, Z. et al. Silicon\u2013lithium niobate hybrid intensity and\ncoherent modulators using a periodic capacitively loaded\ntraveling-wave electrode. ACS Photonics 9, 2668\u20132675 (2022).\n59. Roelkens, G. et al. Adhesive bonding of InP/InGaAsP dies to pro\u0002cessed silicon-on-insulator wafers using DVS-bis\u0002Benzocyclobutene. J. Electrochem. Soc. 153, 1015 (2006).\n60. Mookherjea, S., Mere, V. & Valdez, F. Thin-film lithium niobate\nelectro-optic modulators: to etch or not to etch. Appl. Phys. Lett.\n122, 120501 (2023).\n61. Royter, Y. et al. Dense heterogeneous integration for InP Bi-CMOS\ntechnology. In 2009 IEEE International Conference on Indium\nPhosphide & Related Materials, 105\u2013110 (2009).\n62. Tang, Y., Peters, J. D. & Bowers, J. E. Over 67 GHz bandwidth hybrid\nsilicon electroabsorption modulator with asymmetric segmented\nelectrode for 1.3 \u03bcm transmission. Opt. Express 20,\n11529\u201311535 (2012).\n63. Han, J.-H. et al. Efficient low-loss InGaAsP/Si hybrid MOS optical\nmodulator. Nat. Photonics 11, 486\u2013490 (2017).\n64. Hiraki, T. et al. Integration of a high-efficiency Mach-Zehnder\nmodulator with a DFB laser using membrane InP-based devices on\na Si photonics platform. Opt. Express 29, 2431\u20132441 (2021).\n65. Eltes, F. et al. A BaTiO3-based electro-optic pockels modulator\nmonolithically integrated on an advanced silicon photonics plat\u0002form. J. Light. Technol. 37, 1456\u20131462 (2019).\n66. Doerr, C. et al. Silicon photonics coherent transceiver in a ball-grid\narray package. In 2017 Optical Fiber Communications Conference\nand Exhibition (OFC), 1\u20133 (2017).\n67. Alloatti, L. et al. 100 GHz silicon\u2013organic hybrid modulator. Light\nSci. Appl. 3, 173\u2013173 (2014).\n68. Wang, C. et al. Integrated lithium niobate electro-optic mod\u0002ulators operating at CMOS-compatible voltages. Nature 562,\n101\u2013104 (2018).\n69. Burla, M. et al. 500 GHz plasmonic Mach-Zehnder modulator\nenabling sub-THz microwave photonics. APL Photonics 4,\n056106 (2019).\n70. Li, M. et al. Integrated pockels laser. Nat. Commun. 13,\n5344 (2022).\n71. Wang, M. et al. Eight-channel laser array with 100 GHz channel\nspacing based on surface-slotted structures fabricated by stan\u0002dard lithography. Opt. Lett. 43, 4867\u20134870 (2018).\n72. Eschenbaum, C. et al. Thermally stable Silicon-Organic Hybrid\n(SOH) Mach-Zehnder Modulator for 140 GBd PAM4 transmission\nwith sub-1 V drive signals. In 2022 European Conference on Optical\nCommunication (ECOC), 1\u20134 (2022).\n73. Czornomaz, L. & Abel, S. BTO-enhanced silicon photonics\u2014a\nscalable PIC platform with ultra-efficient electro-optical modula\u0002tion. In 2022 Optical Fiber Communications Conference and\nExhibition (FC), 1\u20133 (2022).\n74. Xu, H. et al. Design and synthesis of chromophores with enhanced\nelectro-optic activities in both bulk and plasmonic-organic hybrid\ndevices. Mater. Horiz. 9, 261\u2013270 (2022).\n75. Eltes, F. et al. Thin-film BTO-based modulators enabling 200 Gb/s\ndata rates with sub 1 Vpp drive signal. In Optical Fiber Commu\u0002nication Conference (OFC) 2023, 4\u20132 (Optica Publishing\nGroup, 2023).\n76. Al-Qadasi, M. A., Chrostowski, L., Shastri, B. J. & Shekhar, S.\nScaling up silicon photonic-based accelerators: challenges and\nopportunities. APL Photonics 7, 020902 (2022).\n77. Lu, Z., Murray, K., Jayatilleka, H. & Chrostowski, L. Michelson\ninterferometer thermo-optic switch on SOI with a 50 \u03bcW power\nconsumption. IEEE Photonics Technol. Lett. 27, 2319\u20132322 (2015).\n78. Iseghem, L. V. et al. Low power optical phase shifter using liquid\ncrystal actuation on a silicon photonics platform. Opt. Mater.\nExpress 12, 2181\u20132198 (2022).\n79. Notaros, M. et al. Integrated visible-light liquid-crystal-based\nphase modulators. Opt. Express 30, 13790\u201313801 (2022).\n80. Izraelevitz, J. et al. Basic performance measurements of the Intel\nOptane DC persistent memory module. Preprint at https://arxiv.\norg/abs/1903.05714 (2019).\n81. Mukherjee, A., Saurav, K., Nair, P., Shekhar, S. & Lis, M. A case for\nemerging memories in DNN accelerators. In 2021 Design, Auto\u0002mation & Test in Europe Conference & Exhibition (DATE)\n938\u2013941 (2021).\n82. R\u00edos, C. et al. Ultra-compact nonvolatile phase shifter based on\nelectrically reprogrammable transparent phase change materials.\nPhotoniX 3, 26 (2022).\n83. Yang, X. et al. Non-volatile optical switch element enabled by low\u0002loss phase change material. Adv. Funct. Mater. n/a,\n2304601 (2023).\n84. Feng, Y., Thomson, D. J., Mashanovich, G. Z. & Yan, J. Performance\nanalysis of a silicon NOEMS device applied as an optical mod\u0002ulator based on a slot waveguide. Opt. Express 28,\n38206\u201338222 (2020).\n85. Pruessner, M. W. et al. Foundry-processed optomechanical pho\u0002tonic integrated circuits. OSA Contin. 4, 1215\u20131222 (2021).\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 13\n86. Edinger, P. et al. Silicon photonic microelectromechanical phase\nshifters for scalable programmable photonics. Opt. Lett. 46,\n5671\u20135674 (2021).\n87. Baghdadi, R. et al. Dual slot-mode NOEM phase shifter. Opt.\nExpress 29, 19113\u201319119 (2021).\n88. Midolo, L., Schliesser, A. & Fiore, A. Nano-opto-electro\u0002mechanical systems. Nat. Nanotechnol. 13, 11\u201318 (2018).\n89. Jo, G. et al. Wafer-level hermetically sealed silicon photonic\nMEMS. Photon. Res. 10, 14\u201321 (2022).\n90. Ortmann, J. E. et al. Ultra-low-power tuning in hybrid Barium\nTitanate-Silicon Nitride electro-optic devices on silicon. ACS\nPhotonics 6, 2677\u20132684 (2019).\n91. Sorianello, V., Contestabile, G. & Romagnoli, M. Graphene on\nsilicon modulators. J. Light. Technol. 38, 2782\u20132789 (2020).\n92. Gui, Y. et al. Monolithic PIC integrated compact GHz ITO\u0002modulators. In CLEO 2023, 1\u20136 (Optica Publishing Group, 2023).\n93. Nezami, M. S. et al. Packaging and interconnect considerations in\nneuromorphic photonic accelerators. IEEE J. Sel. Top. Quantum\nElectron. 29, 1\u201311 (2023).\n94. Duan, J. et al. Dynamic and nonlinear properties of epitaxial\nquantum dot lasers on silicon for isolator-free integration. Photo\u0002nics Res. 7, 1222\u20131228 (2019).\n95. Zhang, Y. et al. Monolithic integration of broadband optical iso\u0002lators for polarization-diverse silicon photonics. Optica 6,\n473\u2013478 (2019).\n96. Doerr, C. R., Chen, L. & Vermeulen, D. Silicon photonics broad\u0002band modulation-based isolator. Opt. Express 22,\n4493\u20134498 (2014).\n97. Shoman, H. et al. Stable and reduced-linewidth laser through\nactive cancellation of reflections without a magneto-optic iso\u0002lator. J. Light. Technol. 39, 6215\u20136230 (2021).\n98. Jin, W. et al. Hertz-linewidth semiconductor lasers using CMOS\u0002ready ultra-high-Q microresonators. Nat. Photonics 15,\n346\u2013353 (2021).\n99. Billah, M. R. et al. Hybrid integration of silicon photonics circuits\nand InP lasers by photonic wire bonding. Optica 5,\n876\u2013883 (2018).\n100. Song, B., Stagarescu, C., Ristic, S., Behfar, A. & Klamkin, J. 3D\nintegrated hybrid silicon laser. Opt. Express 24,\n10435\u201310444 (2016).\n101. Guan, H. et al. Widely-tunable, narrow-linewidth III-V/silicon\nhybrid external-cavity laser for coherent communication. Opt.\nExpress 26, 7920\u20137933 (2018).\n102. Zhang, J. et al. Transfer-printing-based integration of a III-V-on\u0002silicon distributed feedback laser. Opt. Express 26,\n8821\u20138830 (2018).\n103. Li, B. et al. Reaching fiber-laser coherence in integrated photonics.\nOpt. Lett. 46, 5201\u20135204 (2021).\n104. Guo, J. et al. Chip-based laser with 1-Hertz integrated linewidth.\nSci. Adv. 8, 9006 (2022).\n105. Koch, B.R. et al. Integrated silicon photonic laser sources for tel\u0002ecom and datacom. In 2013 Optical Fiber Communication Con\u0002ference and Exposition and the National Fiber Optic Engineers\nConference (OFC/NFOEC), 1\u20133 (2013).\n106. Liang, D., Huang, X., Kurczveil, G., Fiorentino, M. & Beausoleil, R. G.\nIntegrated finely tunable microring laser on silicon. Nat. Photonics\n10, 719\u2013722 (2016).\n107. Kondratiev, N. M. et al. Recent advances in laser self-injection\nlocking to high-Q microresonators. Front. Phys. 18, 21305 (2023).\n108. Liu, A. Y. et al. Reliability of InAs/GaAs quantum dot lasers epi\u0002taxially grown on silicon. IEEE J. Sel. Top. Quantum Electron. 21,\n690\u2013697 (2015).\n109. Norman, J. C., Jung, D., Wan, Y. & Bowers, J. E. Perspective: The\nfuture of quantum dot photonic integrated circuits. APL Photonics\n3, 030901 (2018).\n110. Chang, L., Liu, S. & Bowers, J. E. Integrated optical frequency\ncomb technologies. Nat. Photonics 16, 95\u2013108 (2022).\n111. Liu, S. et al. High-channel-count 20 GHz passively mode-locked\nquantum dot laser directly grown on Si with 4.1 Tbit/s transmission\ncapacity. Optica 6, 128\u2013134 (2019).\n112. Chen, C.-H. et al. A comb laser-driven DWDM silicon photonic\ntransmitter based on microring modulators. Opt. Express 23,\n21541\u201321548 (2015).\n113. Jayatilleka, H. et al. Crosstalk in SOI microring resonator-based\nfilters. J. Light. Technol. 34, 2886\u20132896 (2016).\n114. Chowdhury, A. et al. High performance avalanche photodiode in a\nmonolithic silicon photonics technology. In 2022 Optical Fiber\nCommunications Conference and Exhibition (OFC), 1\u20133 (2022).\n115. Benedikovic, D. et al. Silicon-Germanium avalanche receivers with\nfJ/bit energy consumption. IEEE J. Sel. Top. Quantum Electron. 28,\n1\u20138 (2022).\n116. Kang, Y. et al. Monolithic germanium/silicon avalanche photo\u0002diodes with 340 GHz gain-bandwidth product. Nat. Photonics 3,\n59\u201363 (2009).\n117. Nayak, S. et al. A 10-Gb/s \u221218.8 dBm sensitivity 5.7 mW fully\u0002integrated optoelectronic receiver with avalanche photodetector\nin 0.13-\u03bcm CMOS. IEEE Trans. Circuits Syst. I: Regul. Pap. 66,\n3162\u20133173 (2019).\n118. Wang, B. & Mu, J. High-speed Si-Ge avalanche photodiodes.\nPhotoniX 3, 8 (2022).\n119. Sakib, M. et al. A 112 Gb/s all-silicon micro-ring photodetector for\ndatacom applications. In 2020 Optical Fiber Communications\nConference and Exhibition (OFC), 1\u20133 (2020).\n120. Peng, Y. et al. All-silicon microring avalanche photodiodes with a\n>65 A/W response. Opt. Lett. 48, 1315\u20131318 (2023).\n121. Ji, X. et al. On-chip tunable photonic delay line. APL Photonics 4,\n090803 (2019).\n122. Hong, S. et al. Ultralow-loss compact silicon photonic waveguide\nspirals and delay lines. Photon. Res. 10, 1\u20137 (2022).\n123. Xiang, C. et al. 3D integration enables ultralow-noise isolator-free\nlasers in silicon photonics. Nature 620, 78\u201385 (2023).\n124. Shekhar, S. Silicon photonics: a brief tutorial. IEEE Solid-State\nCircuits Mag. 13, 22\u201332 (2021).\n125. Li, J., Lee, H. & Vahala, K. J. Microwave synthesizer using an on\u0002chip Brillouin oscillator. Nat. Commun. 4, 2097 (2013).\n126. Lau, J. H. Recent advances and trends in advanced packaging. In\nIEEE Transactions on Components, Packaging and Manufacturing\nTechnology, Vol. 12, 228\u2013252 (2022).\n127. Bogaerts, W. & Chrostowski, L. Silicon photonics circuit design:\nmethods, tools and challenges. Laser Photonics Rev. 12,\n1700237 (2018).\n128. Xing, Y., Dong, J., Khan, U. & Bogaerts, W. Capturing the effects of\nspatial process variations in silicon photonic circuits. ACS Photo\u0002nics 10, 928\u2013944 (2023).\n129. Stojanovi\u0107, V. et al. Monolithic silicon-photonic platforms in state\u0002of-the-art CMOS SOI processes. Opt. Express 26,\n13106\u201313121 (2018).\n130. Gunn, C. CMOS photonics for high-speed interconnects. IEEE\nMicro 26, 58\u201366 (2006).\n131. Zimmermann, L. et al. BiCMOS silicon photonics platform. In 2015\nOptical Fiber Communications Conference and Exhibition (OFC),\n1\u20133 (2015).\n132. Rakowski, M. et al. 45 nm CMOS - silicon photonics monolithic\ntechnology (45CLO) for next-generation, low power and high\nspeed optical interconnects. In 2020 Optical Fiber Communica\u0002tions Conference and Exhibition (OFC), 1\u20133 (2020).\n133. Giewont, K. et al. 300-mm monolithic silicon photonics foundry\ntechnology. IEEE J. Sel. Top. Quantum Electron. 25, 1\u201311 (2019).\n134. Idjadi, M. H. & Aflatouni, F. Integrated Pound-Drever-Hall laser\nstabilization system in silicon. Nat. Commun. 8, 1209 (2017).\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 14\n135. Moazeni, S. et al. A 40-Gb/s PAM-4 transmitter based on a ring\u0002resonator optical DAC in 45-nm SOI CMOS. IEEE J. Solid-State\nCircuits 52, 3503\u20133516 (2017).\n136. Zanetto, F. et al. Time-multiplexed control of programmable sili\u0002con photonic circuits enabled by monolithic CMOS electronics.\nLaser Photonics Rev. 17, 2300124 (2023).\n137. Ahmed, A. H. et al. A dual-polarization silicon-photonic coherent\nreceiver front-end supporting 528 Gb/s/wavelength. IEEE J. Solid\u0002State Circuits, 1\u201312 (2023).\n138. Rakowski, M. et al. Hybrid 14 nm FinFET - Silicon photonics tech\u0002nology for low-power Tb/s/mm2 optical I/O. In 2018 IEEE Sympo\u0002sium on VLSI Technology, 221\u2013222 (2018).\n139. Boeuf, F. et al. A multi-wavelength 3D-compatible silicon photo\u0002nics platform on 300 mm SOI wafers for 25 Gb/s applications. In\n2013 IEEE International Electron Devices Meeting,\n13\u2013311334 (2013).\n140. De Dobbelaere, P. et al. Advanced silicon photonics technology\nplatform leveraging a semiconductor supply chain. In 2017 IEEE\nInternational Electron Devices Meeting (IEDM), 34\u2013113414 (2017).\n141. Uzoh, C.E. Through-dielectric-vias (TDVs) for 3D integrated cir\u0002cuits in silicon. In United States Patent Application,\n2016\u201303436131 (2016).\n142. Kim, T. et al. A single-chip optical phased array in a wafer-scale\nsilicon photonics/CMOS 3D-integration platform. IEEE J. Solid\u0002State Circuits 54, 3061\u20133074 (2019).\n143. Ahmed, M. G. et al. A 16-Gb/s -11.6-dBm OMA sensitivity 0.7-pJ/bit\noptical receiver in 65-nm CMOS enabled by duobinary sampling.\nIEEE J. Solid-State Circuits 56, 2795\u20132803 (2021).\n144. Ahmed, A. H. et al. A dual-polarization silicon-photonic coherent\ntransmitter supporting 552 Gb/s/wavelength. IEEE J. Solid-State\nCircuits 55, 2597\u20132608 (2020).\n145. Morsy-Osman, M. et al. DSP-free coherent-lite transceiver for next\ngeneration single wavelength optical intra-datacenter inter\u0002connects. Opt. Express 26, 8890\u20138903 (2018).\n146. Hirokawa, T. et al. Analog coherent detection for energy efficient\nintra-data center links at 200 Gbps per wavelength. J. Light.\nTechnol. 39, 520\u2013531 (2021).\n147. Lee, B. G. & Dupuis, N. Silicon photonic switch fabrics: Technology\nand architecture. J. Light. Technol. 37, 6\u201320 (2019).\n148. Beutel, F. et al. Fully integrated four-channel wavelength-division\nmultiplexed QKD receiver. Optica 9, 1121\u20131130 (2022).\n149. Vigliar, C. et al. Error-protected qubits in a silicon photonic chip.\nNat. Phys. 17, 1137\u20131143 (2021).\n150. Guo, Z. et al. Multi-level encoding and decoding in a scalable\nphotonic tensor processor with a photonic general matrix multiply\n(GeMM) compiler. IEEE J. Sel. Top. Quantum Electron. 28,\n1\u201314 (2022).\n151. Tait, A. N. et al. Feedback control for microring weight banks. Opt.\nExpress 26, 26422\u201326443 (2018).\n152. Liu, Y., Marpaung, D., Choudhary, A., Hotten, J. & Eggleton, B. J.\nLink performance optimization of chip-based Si3N4 microwave\nphotonic filters. J. Light. Technol. 36, 4361\u20134370 (2018).\n153. Liang, W. et al. Resonant microphotonic gyroscope. Optica 4,\n114\u2013117 (2017).\n154. Li, A. et al. Advances in cost-effective integrated spectrometers.\nLight Sci. Appl. 11, 174 (2022).\n155. Zilkie, A. J. et al. Multi-micron silicon photonics platform for highly\nmanufacturable and versatile photonic integrated circuits. IEEE J.\nSel. Top. Quantum Electron. 25, 1\u201313 (2019).\n156. Puumala, L. S. et al. Biofunctionalization of multiplexed silicon\nphotonic biosensors. Biosensors 13, 53 (2023).\n157. Adamopoulos, C. et al. Fully integrated electronic-photonic sen\u0002sor for label-free refractive index sensing in advanced zero\u0002change CMOS-SOI process. In 2021 IEEE Custom Integrated Cir\u0002cuits Conference (CICC), 1\u20132 (2021).\n158. Chrostowski, L. et al. A silicon photonic evanescent-field sensor\narchitecture using a fixed-wavelength laser. In Optical Inter\u0002connects XXI, Vol. 11692, 116920 (SPIE, 2021).\n159. Rank, E. A. et al. Toward optical coherence tomography on a chip:\nin vivo three-dimensional human retinal imaging using photonic\nintegrated circuit-based arrayed waveguide gratings. Light Sci.\nAppl. 10, 6 (2021).\nAcknowledgements\nWe acknowledge Abdelrahman Afifi, Rod Augur, Jonathan Doylend,\nFelix Eltes, Ken Giewont, Samantha Grist, Hasitha Jayatilleka, Gordon\nKeeler, Matthew Mitchell, Volker Sorger, Iman Taghavi, Ming Wu, Mark\nWebster, and Gunay Yurtsever for technical discussions. S.S. is sup\u0002ported by Schmidt Science Polymath Award. S.S., L.C., and B.J.S.\nacknowledge support from the Natural Sciences and Engineering\nResearch Council of Canada (NSERC). J.E.B. is supported by\nDARPA MTO.\nAuthor contributions\nS.S. led the manuscript writing and figure creation; S.S., W.B., L.C.,\nJ.E.B., M.H., R.S., and B.J.S. critically discussed the content and con\u0002tributed to editing and revising the manuscript.\nCompeting interests\nS.S. and L.C. cofounded Dream Photonics. J.E.B. cofounded Nexus\nPhotonics and Quintessent. B.J.S. cofounded Milkshake Technology.\nThe remaining authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to\nSudip Shekhar or Bhavin J. Shastri.\nPeer review information Nature Communications thanks Fr\u00e9d\u00e9ric\nBoeuf, Linjie Zhou and the other, anonymous, reviewer(s) for their con\u0002tribution to the peer review of this work.\nReprints and permissions information is available at\nhttp://www.nature.com/reprints\nPublisher\u2019s note Springer Nature remains neutral with regard to jur\u0002isdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article\u2019s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article\u2019s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\n\u00a9 The Author(s) 2024\nPerspective https://doi.org/10.1038/s41467-024-44750-0\nNature Communications | (2024) 15:751 15",
    "title": "Roadmapping the next generation of silicon photonics",
    "publication_date": "2024-01-25",
    "cited_by_count": 51,
    "topics": "Silicon Photonics Technology, Photonic Reservoir Computing for Neural Computation, Optical Fiber Communication Technologies",
    "keywords": "Silicon Photonics, Transceiver",
    "concepts": "Photonics, Silicon photonics, Computer science, CMOS, Telecommunications, Transceiver, Electronic engineering, Engineering, Materials science, Wireless, Optoelectronics",
    "text_type": "full_text",
    "openalex_rank": 47,
    "num_tokens": 15551,
    "best_oa_location_pdf_url": "https://www.nature.com/articles/s41467-024-44750-0.pdf",
    "pdf_urls_by_priority": [
      "https://www.nature.com/articles/s41467-024-44750-0.pdf"
    ],
    "successful_pdf_url": "https://www.nature.com/articles/s41467-024-44750-0.pdf",
    "url": "https://www.nature.com/articles/s41467-024-44750-0.pdf",
    "snippets": {
      "s1": "Perspective https://doi.org/10.1038/s41467-024-44750-0 Roadmapping the next generation of silicon photonics Sudip Shekhar 1 , Wim Bogaerts 2, Lukas Chrostowski1, John E. Bowers3, Michael Hochberg4 , Richard Soref5 & Bhavin J. Shastri 6 Silicon photonics has developed into a mainstream technology driven by advances in optical communications. The current generation has led to a proliferation of integrated photonic devices from thousands to millionsmainly in the form of communication transceivers for data centers. Products in many exciting applications, such as sensing and computing, are around the corner. What will it take to increase the proliferation of silicon photonics from millions to billions of units shipped? What will the next generation of silicon photonics look like? What are the common threads in the integration and fabrication bottlenecks that silicon photonic applications face, and which emerging technologies can solve them? This perspective article is an attempt to answer such questions. We chart the generational trends in silicon photonics technology, drawing parallels from the generational definitions of CMOS technology. We identify the crucial challenges that must be solved to make giant strides in CMOS-foundry-compatible devices, circuits, integration, and packaging. We identify challenges critical to the next..",
      "s2": "..generation of systems and applications\u2014in communication, signal processing, and sensing. By identifying and summarizing such challenges and opportunities, we aim to stimulate further research on devices, circuits, and systems for the silicon photonics ecosystem. The generational roadmap Figure 1 maps the evolution of silicon photonics1,2. Silicon-based photonic integrated circuits (PICs) were introduced in 19853 and low-loss waveguides in a thick silicon on insulator (SOI) process demonstrated in 1991\u2013924,5 . Various optical devices were next demonstrated6, and soon, silicon photonics was in the small-scale integration (SSI) era\u2014 with 1-to-10 components on a PIC. They included demonstrations of high-speed pn junction modulators7\u20139 and photodetectors (PDs)10\u201313, as well as heterogeneous integration of a III-V laser to a silicon PIC14. The next era ushered in the commercial success of silicon photonics. With 10-to-500 components on a PIC, this medium-scale integration (MSI) era saw successful demonstration and adoption of Mach-Zehnder modulator (MZM) in intensity-modulated direct-detect (IMDD) transceivers within data centers\u2014both single-wavelength15 and multiwavelength16..",
      "s3": "..\u201319. Microring-modulator (MRM)-based IMDD transceivers (see Fig. 2a) demonstrated the multiplexing and energy-efficiency benefits of PIC technology20\u201322. Coherent transceivers in silicon photonics/electronics platforms proved that the technology could Received: 4 May 2023 Accepted: 3 January 2024 Check for updates 1 Department of Electrical & Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver V6T1Z4 BC, Canada. 2Department of Information Technology, Ghent University - IMEC, Technologiepark-Zwijnaarde 126, Ghent 9052, Belgium. 3 Department of Electrical & Computer Engineering, University of California Santa Barbara, Santa Barbara 93106 CA, USA. 4 Luminous Computing, 4750 Patrick Henry Drive, Santa Clara 95054 CA, USA. 5College of Science and Mathematics, University of Massachusetts Boston, 100 William T. Morrissey Blvd., Boston 02125 MA, USA. 6 Department of Physics, Engineering Physics & Astronomy, Queen\u2019s University, 64 Bader Lane, Kingston K7L3N6 ON, Canada. e-mail: sudip@ece.ubc.ca; bhavin.shastri@queensu.ca Nature Communications | (2024) 15:751 1 ..",
      "s4": "..1234567890():,;1234567890():,; compete in performance with their LiNbO3 photonic and III-V electronic counterparts23\u201325. Besides communications, silicon photonics also found new applications such as evanescent-field biosensors26. Silicon photonics is now embarking on the next era of large-scale integration (LSI)\u2014towards 500-to-10,000 components on the same chip. Applications for LSI include LIDAR (see Fig. 2b)27\u201332, image projection33, photonic switching34, photonic computing35\u201339, programmable circuits40, and multiplexed biosensors41. Even VLSI (>10,000 components) prototypes have now been demonstrated30,32,34. In the field of communication, which has been the essential market driver for silicon photonics, silicon photonics has transformed from a challenger technology in the SSI era to arguably a dominant technology in the MSI era for intra-, and inter-datacenter interconnects, and it is poised to become the incumbent technology in the LSI era. For co-packaged optics (CPO) to succeed, highperformance computing to scale22, and disaggregated computing to become a reality42, silicon photonics will be pivotal. Silicon photonics:..",
      "s5": "..technology perspective Through the generations of CMOS process development, many materials were added to silicon to reduce the Power, improve the Performance, and shrink the Area\u2014often called the PPA metrics. The additions include Al and Cu for metal traces, Ge for inducing strain and enabling heterojunction BJTs, and silicon nitride (SiN) for passivation and diffusion barriers. The CMOS R&D budgets and commercial markets are orders of magnitude larger than for silicon photonics, so it is natural for silicon photonics foundries to learn from and adopt the innovations from CMOS processes. Hence, we have seen a similar trend in silicon photonics process development. Besides p/n dopants for high-speed modulation, two materials that are now natively supported by several foundries are (1) Ge high-speed photodetectors43, and (2) SiN to expand the wavelength range, enable higher optical power, and support waveguides with lower loss and better phase control in interferometric devices44. 102 101 103 104 105 106 100 1985 1990 1995 2000 2005 2010 2015 2020 2025 VLSI SSI MSI LSI CI P/ st nenop moCf or eb muN Year InP/Si or GaAs/Si Heterogeneous Si Monolithic InP Monolithic Fig. 1 | Timeline for..",
      "s6": "..the number of components on a silicon photonic integrated circuit (PIC) over generations of small-scale, medium-scale, large-scale, and very-large-scale integration (SSI, MSI, LSI, VLSI, respectively). A component is a unit cell that is combined with other unit cells to build a circuit, such as a waveguide, directional coupler, heater, grating coupler, etc. Heterogeneous silicon photonics lags hybrid by approximately two years. For comparison, data for InPbased integrated photonics is also shown. In general, the higher the number of high-speed modulators, the more challenging the scaling. The figure is adapted from refs. 1,2. Tunable Laser SMLL Frequency Discrimination Coherent Detection Beam Steering Fig. 2 | Illustrative renditions of LSI silicon photonic systems capturing current and future technologies. a WDM Transceiver: A semiconductor mode-locked laser (SMLL) provides multi-wavelength continuous-wave (CW) light to an array of compact, WDM-capable modulators and filters. Reflection control circuits limit back reflections into the laser. High-speed photodetectors (PDs) carry out the O/E conversion. b The electrical current is then amplified by transimpedance amplifiers (TIAs) and limiting amplifiers..",
      "s7": "... Analog-to-digital converters (ADCs) are used to digitize the signal for further digital signal processing (DSP). Monitoring PDs are used for control and stabilization of wavelength, phase shift, and temperature. Digital-to-analog converters (DACs) and drivers are used for E/O modulation of the digital signal. Dynamic random-access memory (DRAM) provides large memory access. Micro-controllers (\u03bcC) may be used to offload some of the digital processing as well. c LIDAR: A tunable laser provides frequency chirped light to a network of phase shifters, circulators/duplexers and coherent frontend for homodyne/ heterodyne frequency-modulated CW (FMCW) ranging and detection. Beam steering is done using optical phase arrays (OPAs) or focal-plane arrays (FPAs). Delay line interferometers aid in calibrating the received beat frequencies and support chirp linearization by directly controlling the tunable laser or a modulator and various forms of error correction through DSP. Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 2 Shrinking the area will be a key focus for the next decade of silicon photonics process development for..",
      "s8": "..the LSI and VLSI era. In reality, the biggest density limitations rarely come from device size; the spacing between waveguides to eliminate crosstalk is much larger than the size of the actual waveguides. For radio-frequency (RF) devices, spacings between active elements\u2014which are microns in critical dimension\u2014are often in the hundreds of microns, to eliminate RF crosstalk. Shrinking these \u2018blank spaces\u2019 requires very detailed systems-level simulation and aggressive multi-physics modeling, and will be at the heart of making chips smaller, cheaper, and higher density. The passives themselves are generally limited in size reduction by the index contrast and the operating wavelength of 1\u20132 \u03bcm. There is still some headroom with the use of inverse design techniques to shrink passive building blocks, but the waveguide itself cannot really shrink much below today\u2019s 400\u2013500 nm width for silicon platforms. However, significant scaling is still possible in the optical I/O couplers and high-speed modulators. For coupling to optical fibers, V-grooves with edge couplers provide low-loss, easy-to-package connectivity at the cost of a considerable chip area. Edge couplers without V-grooves are smaller but require more precise active alignment and surface treatment (polishing, dicing..",
      "s9": "..), thereby increasing cost. Multicore fibers are an attractive solution for efficiently using limited photonic beachfront around the edges of a chip45. The main alternative coupling approach is through grating couplers, which are compact, provide the flexibility of positioning on the chip surface, enable wafer level testing, and can also be realized with low insertion loss (IL), but suffer from polarization and temperature sensitivity and lower optical bandwidth46. Passive alignment packaging techniques, such as photonic wire bonding (PWB)47, offer an attractive potential alternative. Using computer vision and automation, PWBs can be fabricated in polymer photoresist through two-photon absorption between two coupling sites allowing up to 30 \u03bcm of offset. Simple alignment markers are used to locate the coupling sites, and the sites do not require strict pitch or large footprints, thereby providing a passive-aligned, low-loss, scalable port count. In another passive alignment technique for pluggable connection, the complexity and accuracy requirement can be moved from fiber assembly to wafer-level manufacturing, where a fiber-receptor die can be flip-chip integrated to the silicon photonic die with a glass spacer48. Using a combination of V-grooves and mirrors in the fiberreceptor die, and mirrors and surface couplers on the silicon photonic die, a confocal..",
      "s10": "..imaging assembly tolerant to >10 \u03bcm relative displacements of the two dies can be realized, providing a passivealigned, low-loss, scalable port-count and pluggable connector48. More reliability studies for these passive alignment-based assemblies will be helpful for broad adoption. E/O modulation The central quest for the next decade in shrinking photonic chips and thus increasing density is to find the elusive \u2018ideal\u2019 modulator in silicon photonics\u2014small in length (L), requiring a small drive voltage to incur a \u03c0 phase shift (V\u03c0), offering low propagation loss (\u03b1) and IL, and for several applications, highly linear and with large \u22123 dB E/O bandwidth (BW)49. Also, this modulator is preferably a phase shifter, as this enables higher-order coherent modulation formats. High-speed modulators. The often-used efficiency figure-of-merit (FoMefficiency) of waveguide-based modulators (Table 1) is \u03b1V\u03c0L. For MRMs, which are very compact, the loss due to \u03b1 becomes less critical50, and a better FoMefficiency inculcates the IL and Vpp (the peakto-peak voltage swing for a specific optical modulation amplitude or OMA). All modulators suffer from..",
      "s11": "..a tradeoff between FoMefficiency and E/O BW49\u201351. Finally, the power consumed in the driver depends on the modulator impedance as seen by the driver. A resistive impedance (a terminated traveling-wave modulator) consumes static (DC) and dynamic (AC) power, whereas a high impedance (capacitive) consumes primarily dynamic power. A high IL also is a proxy for higher power consumption since the laser power needs to be increased to compensate for the losses. Besides the topology, the PPA metrics for a modulator depend on the material and mechanism used for modulation. Table 2 shows the different materials used for modulators in various silicon photonic processes. p-n dopants utilizing the free-carrier plasma dispersion are natively available in all commercial silicon photonic foundries today, supporting 60 GHz or even higher E/O BW. Currently, the commercial market is dominated by such devices, in the form of traveling-wave MZM modulators52. Plasma dispersion in Si leads to mediocre FoMefficiency, with high IL for average OMA. Carrier accumulation allows for shorter MZMs, but with BW limitations53,54. When implemented as MRMs, the devices are much smaller, but IL and OMA remain suboptimal to support LSI/VLSI ICs. With Ge..",
      "s12": "..PDs already supported by most commercial silicon photonics foundries, various teams have attempted to use GeSi, a related but not identical technology, to implement a better modulator. GeSi electro-absorption modulators (EAM) based on the FranzKelydysh effect can operate in the C/L band at high E/O BW. However, they are generally not optically broadband since they use bandedge modulation for absorption. For O-band operations, modulators utilizing the quantum-confined Stark effect (QCSE) still suffer from a large IL55. While there have been multiple academic and commercial efforts in this space, it is unclear whether these modulators will find their way into future generations of commercial devices. Heterogeneous integration of modulator technologies\u2014InP, thinfilm LiNbO3 on insulator (LNOI), or thin-film BaTiO3 (BTO)\u2014with Si can be done using die-to-die, die-to-wafer, or wafer-to-wafer direct (molecular) bonding or adhesive-assisted bonding. Die-to-wafer bonding provides the flexibility of using known-good dies, increasing yield. Wafer-to-wafer bonding remains expensive because the size mismatches between the SOI acceptor..",
      "s13": "..wafers (200 mm or 300 mm) and the modulator donor wafers (150 mm or smaller) lead to wastage. Integration proximity of the (dissimilar) materials in direct bonding facilitates superior optical coupling and heat transportation between them56. However, very smooth and clean surfaces are required. Chemical mechanical polishing (CMP) procedures, already used in high-volume manufacturing (HVM) for heterogeneous direct bonding of InP to Si for lasers, must be optimized for a scalable modulator integration pathway. Annealing is needed for strong molecular bonding and outgassing, but the pre-processed SOI wafer significantly restricts the annealing temperature. Therefore, \u201clowtemperature\u201d annealing at 100 GHz E/O BW modulators are attractive for both telecom and data center applications, they require electronics capable of driving them at such speeds. Unless V\u03c0 (or Vpp) is reduced significantly, such electronics will consume a lot of power, regardless of CMOS/BiCMOS/III-V implementation. Phase-Shifters for tuning and switching. Many photonic applications require phase shifters that consume little or no power and have a low Table 2 | Comparing different modulation materials and mechanisms in their readiness to be integrated into a commercial CMOS foundry, and in power, performance (as of 2023) and area (PPA) metrics..",
      "s14": "..Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 4 \u03b1V\u03c0L for configuration, tuning and switching. For certain applications, these phase shifters should be fast as well, but 10s of GHz E/O BW is not needed. While in many circuits light only passes through one highspeed modulator, it will have to traverse many low-speed phase shifters for tuning and switching, thereby compounding the penalty of power consumption and \u03b1V\u03c0L. Metal heaters (or doped waveguides) utilizing the thermo-optic effect are available in all foundry platforms today. They have 1\u201310 \u03bcs response time, and consume considerable power, generating thermal crosstalk, and thus limiting LSI/VLSI scaling. But they do not introduce optical loss, a significant advantage over other alternatives. Improving thermal insulation reduces their power consumption by > 10 \u00d7 at the expense of an even higher response time76. Even > 100 \u00d7 improvement is possible by folding the waveguides to increase interaction with the metal heaters, but that results in an IL77. The final set of materials and techniques listed in Table 2 are attractive alternatives to heaters. They include liquid crystals (LC), MEMS/NOEMS..",
      "s15": ".., and phase change materials (PCMs). LC on silicon (LCOS) tuning for display applications has been demonstrated at a large scale, and LC has also been the technology of choice for freespace wavelength-selective switches. As phase shifters, they leverage birefringence to demonstrate a strong electro-optic effect. The alignment of the LC molecules can be controlled by applying electrical voltage ( 8\u03bb WDM LSI PICs needing multiple lasers, gain elements, etc., remain to be thoroughly demonstrated. Another commercially successful technique in HVM (>million/ year)14 has been heterogeneous integration, where multiple materials or epitaxial stacks are processed together into one silicon chip at wafer scale. Again, various strategies have been adopted105,106. They include bonding III-V chips to Si with coarse alignment followed by postprocessing the Si wafer to make quantum-well (QW) lasers (Fig. 3e)14,18,19,22. Thermal isolation of the gain medium by the buried oxide (BOX) and the mismatched CTE must be carefully addressed for high-temperature operation, efficiency, and reliability. Placing redundant lasers helps improve the failure-in-time (FIT) rates22. Benefits of the heterogeneous approach include sub-dB coupling loss and a mechanism to..",
      "s16": "..leverage the low-loss external cavity in silicon to significantly reduce the laser linewidth using self-injection locking107. Another longer term approach, desirable for quantum-dot (QD) lasers, is to directly grow epitaxial gain material on the Si wafer108. Due to its lower linewidth enhancement factor, \u03b1H, QD lasers enable lower linewidth and reduced sensitivity to reflections94,109. They also have lower threshold current density. Monolithic integration using heteroepitaxial growth (Fig. 3f), where the III-V substrate is not even needed, remains the end goal, with several recent progress and more to come1 . Multiple silicon photonic foundries are developing hybrid or heterogeneous laser solutions. For scalability purposes, the foundries will likely favor a technology that lends itself to multi-wavelength Table 3 | Comparing different techniques to attach a laser to a silicon PIC in PPA metrics (as of 2023), cost, testing, packaging style, and suitability for applications Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 6 support, which is crucial for several LSI applications. It is likely that scale-out will be supported first by bonding multiple single-wavelength lasers22..",
      "s17": "... Comb lasers110 such as passive semiconductor mode-locked lasers (SMLLs)111 are being actively pursued by various research groups. DFB arrays ensure large output optical power in each wavelength, whereas, in the SMLLs, the power is split between the wavelengths. The presence of the saturable absorber further lowers the total (and hence per wavelength) output power of SMLLs. However, an SMLL is significantly smaller than a DFB array. The linewidth of passive SMLLs111 is usually lower than DFB arrays22. More R&D is expected for SMLLs to demonstrate higher power, reliability, and lifetime in the next decade. Such requirements for DWDM applications are even more stringent, and any temperature drift creates inter-channel crosstalk112,113. Avalanche photodetectors Most of the silicon photonic applications are constrained by limited output power and WPE of the laser, and the high IL in the circuits. An alternative is to improve the SNR at the detection stage (Fig. 2). Lowvoltage APDs which have large \u22123 dB O/E BW, high overall responsivity114, and simultaneously low noise will be beneficial for receiver signal-to-noise ratio (SNR) improvement115,116. It is important..",
      "s18": "..to note that the overall responsivity (in A/W) and low noise is crucial. A large multiplication gain for an APD which has a poor intrinsic responsivity does not lead to a superior performance. Although relatively easier to achieve in Si APDs at 850 nm117, simultaneous (at the same bias voltage) optimization of gain-BW-noise has remained challenging for low-voltage Si/Ge APDs118 or Si resonant APDs119,120 in C/L/O bands. In comparison to Ge PDs, APDs generally have inferior BW, linearity and power handling, which limits their use in various applications. APDs also need to be biased optimally and stabilized for temperature and voltage drift, but that is less challenging117 than what has already been demonstrated for microring circuits50. Delay Several silicon photonic applications require hundreds of picoseconds to nanoseconds of delay. Examples include microwave photonics, optical phase-locked loops (OPLLs), frequency discriminators (Fig. 2), laser linewidth reduction circuits, OPAs, optical coherence tomography (OCT), and gyroscopes. Many of these applications also require tunability in 10 s of picoseconds and broadband operation121. Realizing such a delay in silicon photonics with low-loss and low..",
      "s19": "..-area has been very challenging122. Resonant devices provide a narrowband delay. Si or SiN delay lines are difficult to tune and require narrow bends leading to significant scattering and radiation losses. Shallow etched ridge waveguides or ultrathin waveguides break compatibility with the 220- nm processes. Modifying the fabrication process without sacrificing the performance of other photonic components remains challenging123. Silicon photonics: systems perspective Photonics & electronics interplay Silicon PICs almost always exist in conjunction with electronic ICs (EICs). When we look at systems based on photonic chips, the landscape today is almost 100% dominated by data communication, and we expect this to continue for the near future. In this context, EICs serve two purposes (Fig. 2): (1) Enable E/O and O/E conversions of the end-to-end data. (2) Bias, control and compensate for temperature and fabrication variations. Thus, photonics serve electronics by providing the data links, and electronics serve photonics by providing control and readout and digital signal processing (DSP). A major difference between photonics and electronics is that photons don\u2019t interact and thus are excellent for transmission of information, whereas electrons interact and repel each other and thus make good switches and computing elements...",
      "s20": "..Each silicon photonic switch therefore requires a corresponding electronic switch. On the whole, the number of transistors in the EIC that must accompany an LSI PIC are orders of magnitude larger than the number of components in the PIC. Here lies a natural interplay, since transistors consume much lower power in (1) switching, (2) providing gain (both linear and limiting), and (3) offering high precision, while being orders of magnitude smaller than the photonic components124. On the other hand, the photonic components (1) enable lower frequency-dependent loss when moving data over a longer distance compared to copper, (2) may provide lower latency through asynchronous and repeaterless data movement, and (3) ease parallelism of very high-speed data on an optical waveguide (through WDM). When the data is already in the optical domain, photonic signal switching or processing can become attractive. The former is a widely deployed technology, while the latter has yet to make the leap from research to product to replace DSP functionality. Thus, it is good to be cognizant of the respective virtues of the PIC and EIC technologies. For example, the E/O and O/E overhead of processing electronic data in the photonic domain must be carefully analyzed. Conversely, silicon photonics provides opportunities to shrink large..",
      "s21": "..optical systems, and bring new applications (such as in sensing and imaging) to reality, which electronics cannot enable by itself. Finally, silicon photonics operates on a carrier wave of hundreds of THz, while silicon electronics is limited to sub-THz. Such differing attributes open attractive codesign opportunities, such as designing electronic clocks with ultralow phase noise125. Photonics & electronics ecosystem It is insightful to look at the electronics industry ecosystem briefly. Moore\u2019s law demonstrates that the cost per component goes down with every generation of CMOS technology reducing the critical dimensions of the transistors. This scaling is enabled by an exponential increase over time in the economic scale of the semiconductor industry, which allows the industry to pay for ever more expensive foundries and process development. Foundries enable many users to access these advanced processes, without each needing to pay to develop the process on their own. At the most extreme, the MPW (multi-project wafer) runs that the foundries host allow multiple users to share the costs of a single wafer run to develop products cost-efficiently. As processes mature, yields go up, and costs come down. The foundries and third-party intellectual property (IP) providers enable a process design kit (PDK) and design IP libraries, allowing the..",
      "s22": "..customers to build incredibly complex electronic circuits and get them right the first time. By relying on both proven devices and proven circuitlevel IP, the designers can focus on system-on-chip (SoC) integration without ever touching the transistor level in several cases. Once the chips are fabricated, there is a rich ecosystem of test houses, packaging service providers, and so forth. Electrical wirebonding (Fig. 4a) and flip-chip bonding (with C4 bumps and microbumps, Fig. 4b) are reliable and popular means of packaging, with the latter providing more bumps instead of just peripheral connections. More advanced packaging techniques (see Fig. 4) such as throughsilicon via (TSV), TSV-less interposers, and heterogeneous integration are used to improve signal integrity, power and thermal distribution, and die yield by breaking complex and large SoCs into smaller chiplets126. Because the FPGAs, GPUs, and CPUs are produced in HVM, the overall cost still goes down despite the complex packaging techniques. Nevertheless, judicious packaging decisions are made to avoid unnecessary complexity; generally, the simplest package is best, and advanced packaging techniques (chip on wafer, chip stacking, etc.) tend to be introduced only when no other alternative is feasible..",
      "s23": "... The photonics industry has several similarities but also many stark differences. Just like in the electronics industry, increasing the number of photonic components is not always about reducing cost, but is often about providing new functionality, improved performance, or reduced area per component. MPW runs are now available at many foundries, Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 7 although mature PDKs, and abstraction languages are still in very early stages. Third-party IP support is mostly non-existent thus far. Companies wall off the most advanced PIC processes to protect their investment and IP (reminiscent of the early decades of the CMOS industry, acting as virtual integrated device manufacturers (IDMs), maintaining differentiation at the process and PDK level. Meanwhile, academic research mainly focuses on improving the devices. Photonic foundries face a significant dilemma: Their customers often demand that they customize their processes, which involves a great deal of R&D expense, and endangers the reliability and yield of the final wafers. Driving customers into a standard process is the solution for this, but in order to do that, the customers need to see significant value in stability and..",
      "s24": "..in a settled PDK and IP ecosystem; only a few designers see the world this way, because so many of the members of the design community today were trained as device people, rather than SoC designers. Changing process parameters often seems to such designers to be the easiest way to generate performance differentiation, but the downstream costs for such changes can be very high from a reliability and process maintenance perspective. As more designers who are used to the idea of settled PDKs graduate and come into the field, disruptive process changes will slowly become less and less common; the foundries will also likely grow ever more resistant to process changes from customers that are not justified by substantial purchase commitments. The overall yield for silicon photonics products is still lower than their CMOS electronic counterparts. Additional factors at the process, design, and packaging level account for the difference: fabrication127,128 and thermal sensitivity, lack of robust PDK components and variationand-mismatch aware models127,128, design flow methodologies still missing hierarchical simulations, schematic driven layout and layoutversus-schematic verification127, custom process modifications for specific components, challenges with epitaxial growth, Ge integration for photodetection, integration of laser (whether at the die or package level), laser FIT, and fiber connectivity. Only a handful of H..",
      "s25": "..VM silicon photonics products are shipping today, requiring the fab to timeshare the production with other processes, and adding another source of yield impact. Photonics & electronics co-integration The option to integrate the PIC with the EIC has been around since the first commercially successful silicon photonic product1,127,129. Developing a monolithic EPIC process (Fig. 4e), starting with a CMOS (or BiCMOS) SOI process and optimizing it for photonic applications, has been demonstrated several times130\u2013132 successfully. From the perspective of commercialization and time-to-market, a monolithic EPIC often \u2018seems to\u2019 be the superior technology of choice (Table 4). High-speed circuits such as drivers and TIAs can be colocated next to modulators and PDs, reducing parasitics and power consumption133. Controllers (thermal, wavelength) can be designed and placed next to the photonic components without needing dedicated pads. For LSI applications, a monolithic EPIC can simplify packaging complexity significantly. However, when the die area is dominated by photonics, photonic components being orders of magnitude larger than their electronic counterparts124, the overall die cost can increase significantly without arguably making full use of CMOS devices. This analysis has to be done case-by-case for individual products. In principle,..",
      "s26": "..microring-based circuits appear to be very appealing for monolithic EPIC processes until the next-generation modulator with a superior FoMefficiency is developed (see the \u201cHigh-speed modulators\u201d section). But to conclude whether they make sense in a given, specific application, a complete systems analysis is necessary; microrings come with considerable control overhead and performance tradeoffs, especially at very high speeds. If the application requires high-speed ADC/DAC and especially DSP (Fig. 2), another finFET EIC must also be added to save power consumption, as the fastest monolithic EPIC process today in 45-nm CMOS SOI is still several generations slower (in fanout delay) than finFET processes. Integrating photonics directly onto CMOS wafers below the 45 nm node is unlikely to occur in the next few years; doing so does not make economic or technical sense in a world where chip-on-wafer bonding between PICs and scaled microelectronics is comparatively straightforward. Other possibilities for EPICs have also been explored. Adding photonics to an older generation CMOS process leads to high-power and slower drivers and TIAs, leading to worse transceiver designs and rendering them unattractive to the biggest customers of silicon photonics\u2014datacom and telecom. Nevertheless, such a process is appealing to..",
      "s27": "..university researchers as it opens up opportunities to co-design and innovate new EPIC circuits134,135 at low cost and packaging effort. On the other hand, multiple efforts are underway to integrate transistors onto the same wafers as silicon photonic devices136. However, PIC electronic chip EWB EIC bump PIC EIC Interposer PIC EIC EPIC electronic chip EIC PIC TOV a b e c d electronic chip EIC PIC TSV Fig. 4 | Comparing different techniques to attach a PIC to an electronic IC (EIC). a Electrical wire bonding (EWB) side-by-side. b 2.5D flip-chipped side-by-side or stacked. c Hybrid 3D TSV (Through-Silicon Via). d Heterogenous 3D with TOV (Through-Oxide Via). e Monolithic electronic photonic IC (EPIC). Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 8 doing so has thus far involved unacceptable compromises to the performance of the bipolar electronics. Most of the silicon photonic transceivers in HVM today are based on a 2.5D integration approach, where the PIC and EIC(s) are designed, sized..",
      "s28": ".., optimized, tested in their best respective processes, and then flip-chipped to an interposer substrate17,19,24,25,137 (Fig. 4b, Table 4). The EIC process can be chosen from one of the many CMOS/SiGe foundries. Multiple EIC chips can also be flip-chipped, such as (1) a SiGe chip or scaled-CMOS chip with a reasonably large breakdown voltage to permit high-swing drivers and a reasonable switching speed to support the RF speed requirements, and (2) an advanced FinFET chip for DSP/ ADC/DAC138. An EIC process with faster transistors may even compensate for the parasitic capacitance due to additional pad, ESD, and routing (compared to a monolithic EPIC solution). For LSI applications where most PIC components require electronics at a relatively low speed (such as LIDAR), flip-chip solutions seem reasonable32. However, for LSI applications that need many high-speed drive/readout lines, a flip-chip solution means many RF traces on the interposer, leading to complexity and crosstalk considerations. In either case, the size of the PIC is increased due to the necessity of many I/O bumps, though with microbumping and copper pillar technologies..",
      "s29": "..to realize a stacked flipchipped 2.5D package139,140 (Fig. 4b, bottom), these increases are often commercially negligible. The parasitics and interconnects are also reduced compared to their side-by-side counterparts. A hybrid 3D integration can be considered in some cases, where the EIC is flipchipped on the (larger) PIC chip and uses advanced techniques such as TSVs or through-oxide vias (TOVs) (Fig. 4c, Table 4). The RF lines still need to be routed from the small EIC to several places on the PIC, which remains challenging. A WoW heterogeneous 3D integration is also being researched where the photonics wafer is flipped and vertically attached with the CMOS wafer through oxide-bonding, the silicon handle on the photonics wafer is removed, and TOVs are formed at the wafers\u2019 interface141,142; further improvements are expected for the performance of photonic components in such an integration technology (Fig. 4d, Table 4). One possibility is to use multiple EICs 3D integrated on the PIC. Overall, the application, performance specifications and the volume of shipments (affecting the cost) will decide whether a more expensive monolithic EPIC with simpler packaging, a multi..",
      "s30": "..-chip 2.5D integration with more complex packaging, or a 3D integration with more complex processing/packaging is the right choice (Table 4). We expect that all of these scenarios will co-exist, just like in the electronics ecosystem. Silicon photonics: applications perspective In this section, we describe the top technical impediments to the success of various silicon photonics applications (Table 5), connecting them to some of the challenges and opportunities discussed in previous sections. We limit the impediments to PIC/EIC technology only, excluding economic, regulatory, market, and other factors such as chemistry, biomarkers, quantum advantage, etc. We also do not delve into the benefits of silicon photonics for these applications since most of the previous works describe them in detail. For IMDD transceivers (XVRs) to further improve their energy efficiency (pJ/b) and scale to higher data rates, the modulator FoMefficiency needs further reduction, and the \u22123 dB E/O BW needs to be improved towards 100 GHz. Improving the WPE of lasers is essential for most applications but especially crucial for communication and computing applications. Efficient multi-wavelength light sources are also needed with adequately large power in each wavelength. Lownoise, large gain-bandwidth APDs in..",
      "s31": "..O/L/C bands could provide an SNR improvement without significant power consumption penalty, but historically their bandwidth, linearity, noise, and power handling characteristics have prevented their use at the highest bandwidths. Finally, amplifying PD signals using high-gain, low-noise TIAs remains a crucial challenge. Several equalization-based techniques have been recently demonstrated to limit the noise using low-BW TIAs143, but most operate on the assumption that the receiver clock is available. For coherent transceivers to be competitive inside data centers, additional challenges (vs. IMDD) must be solved. Linearity requirements for the TIAs and drivers are more stringent137,144, and the reliance on power-hungry DSP needs to be reduced as much as possible. One Table 4 | Comparing different techniques to attach a PIC to an electronic IC (EIC) in PPA metrics (as of 2023), cost, test possibilities, packaging style, and suitability for applications Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 9 strategy being explored by researchers is moving some signal processing tasks into the optical domain145,146 leveraging integrated photonics and analog electronic circuits. The latter requires significant electronic-..",
      "s32": "..photonic co-design effort, opening up several opportunities for CMOS designers to leverage the expertise from mixed-signal and RF ICs. High throughput network switches for short-reach to long-haul markets require the phase shifters to have excellent FoMefficiency to enable large fabrics. The switching must incur low power consumption, low loss and demonstrate a large extinction ratio. For applications that permit slower switching speeds, insulated metal heaters in interferometric switches are currently the popular implementation choice147, but technologies such as MEMS/NOEMS look promising34. Long-term reliability and demonstration in large-scale fabrics cointegrated with electronics and packaged with optical I/Os are needed. Polarization diversity and wavelength considerations further complicate the scaling and packaging considerations. Applications requiring fast switching are even more challenging since high-speed modulators with comparatively inferior FoMefficiency further deteriorate IL and extinction ratio. Regardless of the switching speed requirements, the inherent losses in large switch fabrics require optical amplification, necessitating the integration of SOAs, ideally uncooled, for energy efficiency considerations. Practical quantum communication and computing applications require LSI-VLSI photonic components with advanced CMOS controllers. For chip-scale discrete-variable quantum key distribution (QKD), the foremost requirements are the cryo-compatible photonic/ electronic readout and control of..",
      "s33": "..superconducting nanowire singlephoton detector (SPD) arrays; developing low-loss, low-power cryomodulators and cryo-compatible WDM mux/demux; and integrating single-photon source (SPS) arrays atthe transmitter in a low-noise, lowcrosstalk chip-scale photonic-electronic solution. Superconducting nanowire SPDs operate at telecom wavelengths, facilitating the use of existing optical fibers as a quantum channel. Besides massive parallelization, reducing the loss in the receiver and improving the SPD performance will help increase the transmission rate148. For quantum computing applications, the challenges are similar, but require much larger scalability of qubit control/readout, including the photonics and low-latency control electronics149. The quality of qubits is, of course, paramount. Scalability of control/readout degrades with IL\u2014every photon lost degrades the capability of the quantum system in an exponential way. Ultra-low-loss couplers are therefore needed to connect to the PIC. Photonic computing involves analog computation and processing of information within the photonic domain37,38. This requires handling multi-level signaling150 and increasing the precision of weight control151 to ensure a high SNR. Such improvements are crucial to achieve accuracy comparable to the incumbent CMOS EIC compute engines76. Another challenge..",
      "s34": "..is access to high-speed memory to prevent a memory bottleneck, especially for activations and tasks that are not weight-stationary. Photonic computing uses high parallelism, so it is essential to reduce the IL of passive and active devices (modulators, phase shifters) and boost the output power of multi-wavelength lasers to accommodate larger network sizes. In addition, for neural networks, efficiently implementing programmable nonlinearities stands out as a significant hurdle37. For automobile driving, silicon photonics LIDARs are positioning themselves as a solid-state challenger to Time-of-Flight (ToF) LIDARs utilizing mechanical or MEMS-based scanning. LIDARs consist of two subsystems\u2014ranging and beam steering, both of which can use silicon photonics. ToF and frequency-modulated CW (FMCW) are ranging techniques. FMCW provides the benefits of (1) coherently detecting signals down to a few photons, (2) robustness to interference from ambient sources, and (3) simultaneous distance and velocity measurement. All of the necessary components for coherent detection can be integrated on a single chip. For beam steering, two integrated possibilities exist: (1) Optical phase arrays (OPAs), based on continuous tunable phase shifters and gratings32. Bulk optics solutions, such as spinning mirrors..",
      "s35": "..and oscillating mirrors, have the advantage of being cheap, mature, and simple; displacing such solutions with an on-chip OPA will be a significant challenge. For an OPA to emit a single beam, the grating antennas need to be spaced less than half a wavelength (in free space)\u2014a challenging proposition for 2D beam steering on a silicon chip. Therefore, silicon photonic OPAs typically have gratings arranged for beam steering in 1D and the wavelength of the laser is swept to steer the beam in the other direction. (2) Focal plane arrays (FPAs) based on on-chip switch networks and grating couplers31. These include 2D FPAs, utilizing MEMS switches29,30, or 1D FPA with wavelength steering. Regardless of the solution, low-power (10 s of nW) and improved FoMefficiency phase shifters are important, and necessary for beam steering. Improved lasers are the next challenge. For 1D OPAs or FPAs, multi-wavelength lasers can relax wavelength tuning28. For FMCW demodulation, narrow linewidth ( 40 GHz driver with 4.5 \u00d7 bandwidth extension for a 272 Gb/s dual-polarization 16- QAM silicon photonic transmitter. In 2019 IEEE International SolidState Circuits Conference - (ISSCC..",
      "s36": "..), 484\u2013486 (2019). 26. Iqbal, M. et al. Label-free biosensor arrays based on silicon ring resonators and high-speed optical scanning instrumentation. IEEE J. Sel. Top. Quantum Electron. 16, 654\u2013661 (2010). 27. Poulton, C. V. et al. 8192-element optical phased array with 100\u2218 steering range and flip-chip CMOS. In Conference on Lasers and Electro-Optics, 4\u20133 (Optica Publishing Group, 2020). 28. Riemensberger, J. et al. Massively parallel coherent laser ranging using a soliton microcomb. Nature 581, 164\u2013170 (2020). 29. Zhang, X., Kwon, K., Henriksson, J., Luo, J. & Wu, M. C. Large-scale silicon photonics focal plane switch array for optical beam steering. In Optical Fiber Communication Conference (OFC) 2021, 4\u20132 (Optica Publishing Group, 2021). 30. Zhang, X., Kwon, K., Henriksson, J., Luo, J. & Wu, M. C. A largescale microelectromechanical-systems-based silicon photonics lidar. Nature..",
      "s37": "..603, 253\u2013258 (2022). 31. Rogers, C. et al. A universal 3D imaging sensor on a silicon photonics platform. Nature 590, 256\u2013261 (2021). 32. Poulton, C. V. et al. Coherent lidar with an 8,192-element optical phased array and driving laser. IEEE J. Sel. Top. quantum Electron. 28, 1\u20138 (2022). 33. Raval, M., Yaacobi, A. & Watts, M. R. Integrated visible light phased array system for autostereoscopic image projection. Opt. Lett. 43 15, 3678\u20133681 (2018). 34. Seok, T. J., Kwon, K., Henriksson, J., Luo, J. & Wu, M. C. Wafer-scale silicon photonic switches beyond die size limit. Optica 6, 490\u2013494 (2019). 35. Ramey, C. Silicon photonics for artificial intelligence acceleration: Hotchips 32. In 2020 IEEE Hot Chips 32 Symposium (HCS), 1\u201326 (2020). 36. Huang, C. et al. A silicon photonic\u2013electronic neural network for fibre nonlinearity compensation. Nat. Electron. 4..",
      "s38": ".., 837\u2013844 (2021). 37. Shastri, B. J. et al. Photonics for artificial intelligence and neuromorphic computing. Nat. Photonics 15, 102\u2013114 (2021). 38. Bandyopadhyay, S. et al. Single chip photonic deep neural network with accelerated training. Preprint at https://arxiv.org/abs/ 2208.01623 (2022). 39. Ashtiani, F., Geers, A. J. & Aflatouni, F. An on-chip photonic deep neural network for image classification. Nature 606, 501\u2013506 (2022). 40. Bogaerts, W. et al. Programmable photonic circuits. Nature 586, 207\u2013216 (2020). 41. Reed, B. D. et al. Real-time dynamic single-molecule protein sequencing on an integrated semiconductor device. Science 378, 186\u2013192 (2022). 42. Michelogiannakis, G. et al. Efficient intra-rack resource disaggregation for HPC using co-packaged DWDM photonics. In IEEE International Conference on Cluster Computing (CLUSTER), 158\u2013172 (2023). 43. Pinguet, T. et al. High-volume manufacturing platform for silicon..",
      "s39": "..photonics. Proc. IEEE 106, 2281\u20132290 (2018). Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 12 44. Bauters, J. F. et al. Silicon on ultra-low-loss waveguide photonic integration platform. Opt. Express 14, 544\u2013555 (2013). 45. Lindenmann, N. et al. Connecting silicon photonic circuits to multicore fibers by photonic wire bonding. J. Light. Technol. 33, 755\u2013760 (2015). 46. Cheng, L., Mao, S., Li, Z., Han, Y. & Fu, H. Y. Grating couplers on silicon photonics: design principles, emerging trends and practical issues. Micromachines 11, 666 (2020). 47. Blaicher, M. et al. Hybrid multi-chip assembly of optical communication engines by in situ 3D nano-lithography. Light Sci. Appl. 9, 71 (2020). 48. Israel, A. et al. Photonic plug for scalable silicon photonics packaging. in Optical Interconnects XX, Vol. 11286 (eds. Schr\u00f6der,..",
      "s40": "..H. & Chen, R. T.) 1128607 (SPIE, 2020). 49. Taghavi, I. et al. Polymer modulators in silicon photonics: review and projections. Nanophotonics 11, 3855\u20133871 (2022). 50. Sun, J. et al. A 128 Gb/s PAM4 silicon microring modulator with integrated thermo-optic resonance tuning. J. Light. Technol. 37, 110\u2013115 (2019). 51. Yu, H. et al. Trade-off between optical modulation amplitude and modulation bandwidth of silicon micro-ring modulators. Opt. Express 22, 15178\u201315189 (2014). 52. Murray, B., Antony, C., Talli, G. & Townsend, P. D. Predistortion for high-speed lumped silicon photonic mach-zehnder modulators. IEEE Photonics J. 14, 1\u201311 (2022). 53. Wu, X. et al. A 20Gb/s NRZ/PAM-4 1V transmitter in 40 nm CMOS driving a Si-photonic modulator in 0.13 \u03bcm CMOS. In IEEE International Solid-State Circuits Conference Digest of Technical Papers. 128\u2013129 (2013..",
      "s41": "..). 54. Talkhooncheh, A. H. et al. A 2.4 pJ/b 100 Gb/s 3D-integrated PAM4 optical transmitter with segmented SiP MOSCAP modulators and a 2-channel 28 nm CMOS driver. In IEEE International SolidState Circuits Conference (ISSCC), Vol. 65, 284\u2013286 (2022). 55. Srinivasan, S. A. et al. 60Gb/s waveguide-coupled O-band GeSi quantum-confined Stark effect electro-absorption modulator. In Optical Fiber Communication Conference (OFC) 2021, 1\u20133 (Optica Publishing Group, 2021). 56. Liang, D., Roelkens, G., Baets, R. & Bowers, J. E. Hybrid integrated platforms for silicon photonics. Materials 3, 1782\u20131802 (2010). 57. Weigel, P. O. et al. Bonded thin film lithium niobate modulator on a silicon photonics platform exceeding 100 GHz 3-dB electrical modulation bandwidth. Opt. Express 26, 23728\u201323739 (2018). 58. Wang, Z. et al. Silicon\u2013lithium niobate hybrid intensity and coherent modulators using a periodic capacitively loaded traveling..",
      "s42": "..-wave electrode. ACS Photonics 9, 2668\u20132675 (2022). 59. Roelkens, G. et al. Adhesive bonding of InP/InGaAsP dies to processed silicon-on-insulator wafers using DVS-bisBenzocyclobutene. J. Electrochem. Soc. 153, 1015 (2006). 60. Mookherjea, S., Mere, V. & Valdez, F. Thin-film lithium niobate electro-optic modulators: to etch or not to etch. Appl. Phys. Lett. 122, 120501 (2023). 61. Royter, Y. et al. Dense heterogeneous integration for InP Bi-CMOS technology. In 2009 IEEE International Conference on Indium Phosphide & Related Materials, 105\u2013110 (2009). 62. Tang, Y., Peters, J. D. & Bowers, J. E. Over 67 GHz bandwidth hybrid silicon electroabsorption modulator with asymmetric segmented electrode for 1.3 \u03bcm transmission. Opt. Express 20, 11529\u201311535 (2012). 63. Han, J.-H. et al. Efficient low-loss InGaAsP/Si hybrid MOS optical modulator. Nat. Photonics 11..",
      "s43": ".., 486\u2013490 (2017). 64. Hiraki, T. et al. Integration of a high-efficiency Mach-Zehnder modulator with a DFB laser using membrane InP-based devices on a Si photonics platform. Opt. Express 29, 2431\u20132441 (2021). 65. Eltes, F. et al. A BaTiO3-based electro-optic pockels modulator monolithically integrated on an advanced silicon photonics platform. J. Light. Technol. 37, 1456\u20131462 (2019). 66. Doerr, C. et al. Silicon photonics coherent transceiver in a ball-grid array package. In 2017 Optical Fiber Communications Conference and Exhibition (OFC), 1\u20133 (2017). 67. Alloatti, L. et al. 100 GHz silicon\u2013organic hybrid modulator. Light Sci. Appl. 3, 173\u2013173 (2014). 68. Wang, C. et al. Integrated lithium niobate electro-optic modulators operating at CMOS-compatible voltages. Nature 562, 101\u2013104 (2018). 69. Burla, M. et al. 500 GHz plasmonic Mach-Zehnder modulator enabling sub-THz microwave photonics. APL Photonics 4..",
      "s44": ".., 056106 (2019). 70. Li, M. et al. Integrated pockels laser. Nat. Commun. 13, 5344 (2022). 71. Wang, M. et al. Eight-channel laser array with 100 GHz channel spacing based on surface-slotted structures fabricated by standard lithography. Opt. Lett. 43, 4867\u20134870 (2018). 72. Eschenbaum, C. et al. Thermally stable Silicon-Organic Hybrid (SOH) Mach-Zehnder Modulator for 140 GBd PAM4 transmission with sub-1 V drive signals. In 2022 European Conference on Optical Communication (ECOC), 1\u20134 (2022). 73. Czornomaz, L. & Abel, S. BTO-enhanced silicon photonics\u2014a scalable PIC platform with ultra-efficient electro-optical modulation. In 2022 Optical Fiber Communications Conference and Exhibition (FC), 1\u20133 (2022). 74. Xu, H. et al. Design and synthesis of chromophores with enhanced electro-optic activities in both bulk and plasmonic-organic hybrid devices. Mater. Horiz. 9, 261\u2013270 (2022). 75. Eltes, F. et al. Thin-film B..",
      "s45": "..TO-based modulators enabling 200 Gb/s data rates with sub 1 Vpp drive signal. In Optical Fiber Communication Conference (OFC) 2023, 4\u20132 (Optica Publishing Group, 2023). 76. Al-Qadasi, M. A., Chrostowski, L., Shastri, B. J. & Shekhar, S. Scaling up silicon photonic-based accelerators: challenges and opportunities. APL Photonics 7, 020902 (2022). 77. Lu, Z., Murray, K., Jayatilleka, H. & Chrostowski, L. Michelson interferometer thermo-optic switch on SOI with a 50 \u03bcW power consumption. IEEE Photonics Technol. Lett. 27, 2319\u20132322 (2015). 78. Iseghem, L. V. et al. Low power optical phase shifter using liquid crystal actuation on a silicon photonics platform. Opt. Mater. Express 12, 2181\u20132198 (2022). 79. Notaros, M. et al. Integrated visible-light liquid-crystal-based phase modulators. Opt. Express 30, 13790\u201313801 (2022). 80. Izraelevitz, J. et al. Basic performance measurements of..",
      "s46": "..the Intel Optane DC persistent memory module. Preprint at https://arxiv. org/abs/1903.05714 (2019). 81. Mukherjee, A., Saurav, K., Nair, P., Shekhar, S. & Lis, M. A case for emerging memories in DNN accelerators. In 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE) 938\u2013941 (2021). 82. R\u00edos, C. et al. Ultra-compact nonvolatile phase shifter based on electrically reprogrammable transparent phase change materials. PhotoniX 3, 26 (2022). 83. Yang, X. et al. Non-volatile optical switch element enabled by lowloss phase change material. Adv. Funct. Mater. n/a, 2304601 (2023). 84. Feng, Y., Thomson, D. J., Mashanovich, G. Z. & Yan, J. Performance analysis of a silicon NOEMS device applied as an optical modulator based on a slot waveguide. Opt. Express 28, 38206\u201338222 (2020). 85. Pruessner, M. W. et al. Foundry-processed optomechanical photonic integrated circuits. OSA Contin. 4, 1215\u2013..",
      "s47": "..1222 (2021). Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 13 86. Edinger, P. et al. Silicon photonic microelectromechanical phase shifters for scalable programmable photonics. Opt. Lett. 46, 5671\u20135674 (2021). 87. Baghdadi, R. et al. Dual slot-mode NOEM phase shifter. Opt. Express 29, 19113\u201319119 (2021). 88. Midolo, L., Schliesser, A. & Fiore, A. Nano-opto-electromechanical systems. Nat. Nanotechnol. 13, 11\u201318 (2018). 89. Jo, G. et al. Wafer-level hermetically sealed silicon photonic MEMS. Photon. Res. 10, 14\u201321 (2022). 90. Ortmann, J. E. et al. Ultra-low-power tuning in hybrid Barium Titanate-Silicon Nitride electro-optic devices on silicon. ACS Photonics 6, 2677\u20132684 (2019). 91. Sorianello, V., Contestabile, G. & Romagnoli, M. Graphene on silicon modulators..",
      "s48": "... J. Light. Technol. 38, 2782\u20132789 (2020). 92. Gui, Y. et al. Monolithic PIC integrated compact GHz ITOmodulators. In CLEO 2023, 1\u20136 (Optica Publishing Group, 2023). 93. Nezami, M. S. et al. Packaging and interconnect considerations in neuromorphic photonic accelerators. IEEE J. Sel. Top. Quantum Electron. 29, 1\u201311 (2023). 94. Duan, J. et al. Dynamic and nonlinear properties of epitaxial quantum dot lasers on silicon for isolator-free integration. Photonics Res. 7, 1222\u20131228 (2019). 95. Zhang, Y. et al. Monolithic integration of broadband optical isolators for polarization-diverse silicon photonics. Optica 6, 473\u2013478 (2019). 96. Doerr, C. R., Chen, L. & Vermeulen, D. Silicon photonics broadband modulation-based isolator. Opt. Express 22, 4493\u20134498 (2014). 97. Shoman, H. et al. Stable and reduced-linewidth laser through active cancellation of reflections without a magneto-optic isolator. J. Light. Technol. 39, 6215\u2013..",
      "s49": "..6230 (2021). 98. Jin, W. et al. Hertz-linewidth semiconductor lasers using CMOSready ultra-high-Q microresonators. Nat. Photonics 15, 346\u2013353 (2021). 99. Billah, M. R. et al. Hybrid integration of silicon photonics circuits and InP lasers by photonic wire bonding. Optica 5, 876\u2013883 (2018). 100. Song, B., Stagarescu, C., Ristic, S., Behfar, A. & Klamkin, J. 3D integrated hybrid silicon laser. Opt. Express 24, 10435\u201310444 (2016). 101. Guan, H. et al. Widely-tunable, narrow-linewidth III-V/silicon hybrid external-cavity laser for coherent communication. Opt. Express 26, 7920\u20137933 (2018). 102. Zhang, J. et al. Transfer-printing-based integration of a III-V-onsilicon distributed feedback laser. Opt. Express 26, 8821\u20138830 (2018). 103. Li, B. et al. Reaching fiber-laser coherence in integrated photonics. Opt. Lett. 46, 5201\u20135204 (..",
      "s50": "..2021). 104. Guo, J. et al. Chip-based laser with 1-Hertz integrated linewidth. Sci. Adv. 8, 9006 (2022). 105. Koch, B.R. et al. Integrated silicon photonic laser sources for telecom and datacom. In 2013 Optical Fiber Communication Conference and Exposition and the National Fiber Optic Engineers Conference (OFC/NFOEC), 1\u20133 (2013). 106. Liang, D., Huang, X., Kurczveil, G., Fiorentino, M. & Beausoleil, R. G. Integrated finely tunable microring laser on silicon. Nat. Photonics 10, 719\u2013722 (2016). 107. Kondratiev, N. M. et al. Recent advances in laser self-injection locking to high-Q microresonators. Front. Phys. 18, 21305 (2023). 108. Liu, A. Y. et al. Reliability of InAs/GaAs quantum dot lasers epitaxially grown on silicon. IEEE J. Sel. Top. Quantum Electron. 21, 690\u2013697 (2015). 109. Norman, J. C., Jung, D., Wan, Y. & Bowers, J..",
      "s51": "... E. Perspective: The future of quantum dot photonic integrated circuits. APL Photonics 3, 030901 (2018). 110. Chang, L., Liu, S. & Bowers, J. E. Integrated optical frequency comb technologies. Nat. Photonics 16, 95\u2013108 (2022). 111. Liu, S. et al. High-channel-count 20 GHz passively mode-locked quantum dot laser directly grown on Si with 4.1 Tbit/s transmission capacity. Optica 6, 128\u2013134 (2019). 112. Chen, C.-H. et al. A comb laser-driven DWDM silicon photonic transmitter based on microring modulators. Opt. Express 23, 21541\u201321548 (2015). 113. Jayatilleka, H. et al. Crosstalk in SOI microring resonator-based filters. J. Light. Technol. 34, 2886\u20132896 (2016). 114. Chowdhury, A. et al. High performance avalanche photodiode in a monolithic silicon photonics technology. In 2022 Optical Fiber Communications Conference and Exhibition (OFC), 1\u20133 (2022). 115. Benedikovic, D. et al. Silicon-Germanium avalanche receivers with fJ..",
      "s52": "../bit energy consumption. IEEE J. Sel. Top. Quantum Electron. 28, 1\u20138 (2022). 116. Kang, Y. et al. Monolithic germanium/silicon avalanche photodiodes with 340 GHz gain-bandwidth product. Nat. Photonics 3, 59\u201363 (2009). 117. Nayak, S. et al. A 10-Gb/s \u221218.8 dBm sensitivity 5.7 mW fullyintegrated optoelectronic receiver with avalanche photodetector in 0.13-\u03bcm CMOS. IEEE Trans. Circuits Syst. I: Regul. Pap. 66, 3162\u20133173 (2019). 118. Wang, B. & Mu, J. High-speed Si-Ge avalanche photodiodes. PhotoniX 3, 8 (2022). 119. Sakib, M. et al. A 112 Gb/s all-silicon micro-ring photodetector for datacom applications. In 2020 Optical Fiber Communications Conference and Exhibition (OFC), 1\u20133 (2020). 120. Peng, Y. et al. All-silicon microring avalanche photodiodes with a >65 A/W response. Opt. Lett. 48, 1315\u20131318 (2023). 121. Ji, X. et al..",
      "s53": "... On-chip tunable photonic delay line. APL Photonics 4, 090803 (2019). 122. Hong, S. et al. Ultralow-loss compact silicon photonic waveguide spirals and delay lines. Photon. Res. 10, 1\u20137 (2022). 123. Xiang, C. et al. 3D integration enables ultralow-noise isolator-free lasers in silicon photonics. Nature 620, 78\u201385 (2023). 124. Shekhar, S. Silicon photonics: a brief tutorial. IEEE Solid-State Circuits Mag. 13, 22\u201332 (2021). 125. Li, J., Lee, H. & Vahala, K. J. Microwave synthesizer using an onchip Brillouin oscillator. Nat. Commun. 4, 2097 (2013). 126. Lau, J. H. Recent advances and trends in advanced packaging. In IEEE Transactions on Components, Packaging and Manufacturing Technology, Vol. 12, 228\u2013252 (2022). 127. Bogaerts, W. & Chrostowski, L. Silicon photonics circuit design: methods, tools and challenges. Laser Photonics Rev. 12, 1700237 (2018). 128. Xing, Y., Dong, J., Khan, U. & Bogaerts..",
      "s54": ".., W. Capturing the effects of spatial process variations in silicon photonic circuits. ACS Photonics 10, 928\u2013944 (2023). 129. Stojanovi\u0107, V. et al. Monolithic silicon-photonic platforms in stateof-the-art CMOS SOI processes. Opt. Express 26, 13106\u201313121 (2018). 130. Gunn, C. CMOS photonics for high-speed interconnects. IEEE Micro 26, 58\u201366 (2006). 131. Zimmermann, L. et al. BiCMOS silicon photonics platform. In 2015 Optical Fiber Communications Conference and Exhibition (OFC), 1\u20133 (2015). 132. Rakowski, M. et al. 45 nm CMOS - silicon photonics monolithic technology (45CLO) for next-generation, low power and high speed optical interconnects. In 2020 Optical Fiber Communications Conference and Exhibition (OFC), 1\u20133 (2020). 133. Giewont, K. et al. 300-mm monolithic silicon photonics foundry technology. IEEE J. Sel. Top. Quantum Electron. 25, 1\u201311 (2019). 134. Idjadi, M. H. & Aflatouni, F. Integrated Pound-Drever-Hall laser..",
      "s55": "..stabilization system in silicon. Nat. Commun. 8, 1209 (2017). Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 14 135. Moazeni, S. et al. A 40-Gb/s PAM-4 transmitter based on a ringresonator optical DAC in 45-nm SOI CMOS. IEEE J. Solid-State Circuits 52, 3503\u20133516 (2017). 136. Zanetto, F. et al. Time-multiplexed control of programmable silicon photonic circuits enabled by monolithic CMOS electronics. Laser Photonics Rev. 17, 2300124 (2023). 137. Ahmed, A. H. et al. A dual-polarization silicon-photonic coherent receiver front-end supporting 528 Gb/s/wavelength. IEEE J. SolidState Circuits, 1\u201312 (2023). 138. Rakowski, M. et al. Hybrid 14 nm FinFET - Silicon photonics technology for low-power Tb/s/mm2 optical I/O. In 2018 IEEE Symposium on VLSI Technology, 221\u2013222 (2018). 139. Boeuf, F. et al. A multi-wavelength 3D-..",
      "s56": "..compatible silicon photonics platform on 300 mm SOI wafers for 25 Gb/s applications. In 2013 IEEE International Electron Devices Meeting, 13\u2013311334 (2013). 140. De Dobbelaere, P. et al. Advanced silicon photonics technology platform leveraging a semiconductor supply chain. In 2017 IEEE International Electron Devices Meeting (IEDM), 34\u2013113414 (2017). 141. Uzoh, C.E. Through-dielectric-vias (TDVs) for 3D integrated circuits in silicon. In United States Patent Application, 2016\u201303436131 (2016). 142. Kim, T. et al. A single-chip optical phased array in a wafer-scale silicon photonics/CMOS 3D-integration platform. IEEE J. SolidState Circuits 54, 3061\u20133074 (2019). 143. Ahmed, M. G. et al. A 16-Gb/s -11.6-dBm OMA sensitivity 0.7-pJ/bit optical receiver in 65-nm CMOS enabled by duobinary sampling. IEEE J. Solid-State Circuits 56, 2795\u20132803 (2021). 144. Ahmed, A. H. et al. A dual-polarization silicon-..",
      "s57": "..photonic coherent transmitter supporting 552 Gb/s/wavelength. IEEE J. Solid-State Circuits 55, 2597\u20132608 (2020). 145. Morsy-Osman, M. et al. DSP-free coherent-lite transceiver for next generation single wavelength optical intra-datacenter interconnects. Opt. Express 26, 8890\u20138903 (2018). 146. Hirokawa, T. et al. Analog coherent detection for energy efficient intra-data center links at 200 Gbps per wavelength. J. Light. Technol. 39, 520\u2013531 (2021). 147. Lee, B. G. & Dupuis, N. Silicon photonic switch fabrics: Technology and architecture. J. Light. Technol. 37, 6\u201320 (2019). 148. Beutel, F. et al. Fully integrated four-channel wavelength-division multiplexed QKD receiver. Optica 9, 1121\u20131130 (2022). 149. Vigliar, C. et al. Error-protected qubits in a silicon photonic chip. Nat. Phys. 17, 1137\u20131143 (2021). 150. Guo, Z. et al. Multi-level encoding and decoding in a scalable photonic tensor processor with a photonic general matrix..",
      "s58": "..multiply (GeMM) compiler. IEEE J. Sel. Top. Quantum Electron. 28, 1\u201314 (2022). 151. Tait, A. N. et al. Feedback control for microring weight banks. Opt. Express 26, 26422\u201326443 (2018). 152. Liu, Y., Marpaung, D., Choudhary, A., Hotten, J. & Eggleton, B. J. Link performance optimization of chip-based Si3N4 microwave photonic filters. J. Light. Technol. 36, 4361\u20134370 (2018). 153. Liang, W. et al. Resonant microphotonic gyroscope. Optica 4, 114\u2013117 (2017). 154. Li, A. et al. Advances in cost-effective integrated spectrometers. Light Sci. Appl. 11, 174 (2022). 155. Zilkie, A. J. et al. Multi-micron silicon photonics platform for highly manufacturable and versatile photonic integrated circuits. IEEE J. Sel. Top. Quantum Electron. 25, 1\u201313 (2019). 156. Puumala, L. S. et al. Biofunctionalization of multiplexed silicon photonic biosensors. Biosensors 13..",
      "s59": ".., 53 (2023). 157. Adamopoulos, C. et al. Fully integrated electronic-photonic sensor for label-free refractive index sensing in advanced zerochange CMOS-SOI process. In 2021 IEEE Custom Integrated Circuits Conference (CICC), 1\u20132 (2021). 158. Chrostowski, L. et al. A silicon photonic evanescent-field sensor architecture using a fixed-wavelength laser. In Optical Interconnects XXI, Vol. 11692, 116920 (SPIE, 2021). 159. Rank, E. A. et al. Toward optical coherence tomography on a chip: in vivo three-dimensional human retinal imaging using photonic integrated circuit-based arrayed waveguide gratings. Light Sci. Appl. 10, 6 (2021). Acknowledgements We acknowledge Abdelrahman Afifi, Rod Augur, Jonathan Doylend, Felix Eltes, Ken Giewont, Samantha Grist, Hasitha Jayatilleka, Gordon Keeler, Matthew Mitchell, Volker Sorger, Iman Taghavi, Ming Wu, Mark Webster, and Gunay Yurtsever for technical discussions. S.S. is supported by Schmidt Science Polymath Award. S.S., L.C., and B.J.S. acknowledge support..",
      "s60": "..from the Natural Sciences and Engineering Research Council of Canada (NSERC). J.E.B. is supported by DARPA MTO. Author contributions S.S. led the manuscript writing and figure creation; S.S., W.B., L.C., J.E.B., M.H., R.S., and B.J.S. critically discussed the content and contributed to editing and revising the manuscript. Competing interests S.S. and L.C. cofounded Dream Photonics. J.E.B. cofounded Nexus Photonics and Quintessent. B.J.S. cofounded Milkshake Technology. The remaining authors declare no competing interests. Additional information Correspondence and requests for materials should be addressed to Sudip Shekhar or Bhavin J. Shastri. Peer review information Nature Communications thanks Fr\u00e9d\u00e9ric Boeuf, Linjie Zhou and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,..",
      "s61": "..which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/ licenses/by/4.0/. \u00a9 The Author(s) 2024 Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 15"
    },
    "question_generated": "What are the historical developments and current challenges in silicon photonics, and how can emerging technologies and innovative packaging solutions address these challenges to enable large-scale integration and unlock the full potential of this transformative technology?",
    "golden_answer_generated": "# Roadmapping the Next Generation of Silicon Photonics\n\n## Introduction\nSilicon photonics has emerged as a transformative technology, driven by advancements in optical communications. It has transitioned from a niche technology to a mainstream solution, primarily in the form of communication transceivers for data centers\u3010s1\u3011. The proliferation of integrated photonic devices has reached millions, and the next challenge is scaling to billions\u3010s1\u3011.\n\n## Background\nThe development of silicon photonics can be traced back to the introduction of silicon-based photonic integrated circuits (PICs) in 1985\u3010s2\u3011. The technology has evolved through various integration scales, from small-scale integration (SSI) to medium-scale integration (MSI), and is now entering the large-scale integration (LSI) era\u3010s2\u3011\u3010s4\u3011. This evolution mirrors the generational trends observed in CMOS technology\u3010s5\u3011.\n\n## Challenges and Opportunities\n### Integration and Fabrication Bottlenecks\nSilicon photonics faces several integration and fabrication challenges that must be addressed to achieve LSI and VLSI\u3010s4\u3011. These include:\n- **Material Integration:** The addition of materials such as Ge and SiN to silicon to enhance performance metrics like power, performance, and area (PPA)\u3010s5\u3011.\n- **Waveguide Spacing:** The need to reduce spacing between waveguides to eliminate crosstalk, which requires detailed systems-level simulation\u3010s8\u3011.\n- **Coupling Techniques:** The development of efficient coupling techniques, such as grating couplers and photonic wire bonding, to improve connectivity and reduce losses\u3010s9\u3011.\n\n### Emerging Technologies\nEmerging technologies offer potential solutions to these challenges. For instance, heterogeneous integration of modulator technologies and the use of advanced materials like thin-film LiNbO3 can enhance modulator performance\u3010s12\u3011\u3010s13\u3011. Additionally, innovations in packaging techniques, such as passive alignment packaging, can improve scalability\u3010s9\u3011.\n\n## Hypothesis\nBased on the synthesis of existing literature, it is hypothesized that addressing the integration and fabrication bottlenecks through the adoption of emerging technologies and innovative packaging solutions will enable the next generation of silicon photonics to achieve large-scale integration\u3010s4\u3011\u3010s5\u3011\u3010s9\u3011. This hypothesis aligns with the roadmap proposed in the paper, which emphasizes the importance of overcoming these challenges to unlock the full potential of silicon photonics\u3010s1\u3011\u3010s4\u3011.",
    "used_snippets_with_context": {
      "s1": "Perspective https://doi.org/10.1038/s41467-024-44750-0 Roadmapping the next generation of silicon photonics Sudip Shekhar 1 , Wim Bogaerts 2, Lukas Chrostowski1, John E. Bowers3, Michael Hochberg4 , Richard Soref5 & Bhavin J. Shastri 6 Silicon photonics has developed into a mainstream technology driven by advances in optical communications. The current generation has led to a proliferation of integrated photonic devices from thousands to millionsmainly in the form of communication transceivers for data centers. Products in many exciting applications, such as sensing and computing, are around the corner. What will it take to increase the proliferation of silicon photonics from millions to billions of units shipped? What will the next generation of silicon photonics look like? What are the common threads in the integration and fabrication bottlenecks that silicon photonic applications face, and which emerging technologies can solve them? This perspective article is an attempt to answer such questions. We chart the generational trends in silicon photonics technology, drawing parallels from the generational definitions of CMOS technology. We identify the crucial challenges that must be solved to make giant strides in CMOS-foundry-compatible devices, circuits, integration, and packaging. We identify challenges critical to the next..",
      "s2": "..generation of systems and applications\u2014in communication, signal processing, and sensing. By identifying and summarizing such challenges and opportunities, we aim to stimulate further research on devices, circuits, and systems for the silicon photonics ecosystem. The generational roadmap Figure 1 maps the evolution of silicon photonics1,2. Silicon-based photonic integrated circuits (PICs) were introduced in 19853 and low-loss waveguides in a thick silicon on insulator (SOI) process demonstrated in 1991\u2013924,5 . Various optical devices were next demonstrated6, and soon, silicon photonics was in the small-scale integration (SSI) era\u2014 with 1-to-10 components on a PIC. They included demonstrations of high-speed pn junction modulators7\u20139 and photodetectors (PDs)10\u201313, as well as heterogeneous integration of a III-V laser to a silicon PIC14. The next era ushered in the commercial success of silicon photonics. With 10-to-500 components on a PIC, this medium-scale integration (MSI) era saw successful demonstration and adoption of Mach-Zehnder modulator (MZM) in intensity-modulated direct-detect (IMDD) transceivers within data centers\u2014both single-wavelength15 and multiwavelength16..",
      "s3": "..\u201319. Microring-modulator (MRM)-based IMDD transceivers (see Fig. 2a) demonstrated the multiplexing and energy-efficiency benefits of PIC technology20\u201322. Coherent transceivers in silicon photonics/electronics platforms proved that the technology could Received: 4 May 2023 Accepted: 3 January 2024 Check for updates 1 Department of Electrical & Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver V6T1Z4 BC, Canada. 2Department of Information Technology, Ghent University - IMEC, Technologiepark-Zwijnaarde 126, Ghent 9052, Belgium. 3 Department of Electrical & Computer Engineering, University of California Santa Barbara, Santa Barbara 93106 CA, USA. 4 Luminous Computing, 4750 Patrick Henry Drive, Santa Clara 95054 CA, USA. 5College of Science and Mathematics, University of Massachusetts Boston, 100 William T. Morrissey Blvd., Boston 02125 MA, USA. 6 Department of Physics, Engineering Physics & Astronomy, Queen\u2019s University, 64 Bader Lane, Kingston K7L3N6 ON, Canada. e-mail: sudip@ece.ubc.ca; bhavin.shastri@queensu.ca Nature Communications | (2024) 15:751 1 ..",
      "s4": "..1234567890():,;1234567890():,; compete in performance with their LiNbO3 photonic and III-V electronic counterparts23\u201325. Besides communications, silicon photonics also found new applications such as evanescent-field biosensors26. Silicon photonics is now embarking on the next era of large-scale integration (LSI)\u2014towards 500-to-10,000 components on the same chip. Applications for LSI include LIDAR (see Fig. 2b)27\u201332, image projection33, photonic switching34, photonic computing35\u201339, programmable circuits40, and multiplexed biosensors41. Even VLSI (>10,000 components) prototypes have now been demonstrated30,32,34. In the field of communication, which has been the essential market driver for silicon photonics, silicon photonics has transformed from a challenger technology in the SSI era to arguably a dominant technology in the MSI era for intra-, and inter-datacenter interconnects, and it is poised to become the incumbent technology in the LSI era. For co-packaged optics (CPO) to succeed, highperformance computing to scale22, and disaggregated computing to become a reality42, silicon photonics will be pivotal. Silicon photonics:..",
      "s5": "..technology perspective Through the generations of CMOS process development, many materials were added to silicon to reduce the Power, improve the Performance, and shrink the Area\u2014often called the PPA metrics. The additions include Al and Cu for metal traces, Ge for inducing strain and enabling heterojunction BJTs, and silicon nitride (SiN) for passivation and diffusion barriers. The CMOS R&D budgets and commercial markets are orders of magnitude larger than for silicon photonics, so it is natural for silicon photonics foundries to learn from and adopt the innovations from CMOS processes. Hence, we have seen a similar trend in silicon photonics process development. Besides p/n dopants for high-speed modulation, two materials that are now natively supported by several foundries are (1) Ge high-speed photodetectors43, and (2) SiN to expand the wavelength range, enable higher optical power, and support waveguides with lower loss and better phase control in interferometric devices44. 102 101 103 104 105 106 100 1985 1990 1995 2000 2005 2010 2015 2020 2025 VLSI SSI MSI LSI CI P/ st nenop moCf or eb muN Year InP/Si or GaAs/Si Heterogeneous Si Monolithic InP Monolithic Fig. 1 | Timeline for..",
      "s6": "..the number of components on a silicon photonic integrated circuit (PIC) over generations of small-scale, medium-scale, large-scale, and very-large-scale integration (SSI, MSI, LSI, VLSI, respectively). A component is a unit cell that is combined with other unit cells to build a circuit, such as a waveguide, directional coupler, heater, grating coupler, etc. Heterogeneous silicon photonics lags hybrid by approximately two years. For comparison, data for InPbased integrated photonics is also shown. In general, the higher the number of high-speed modulators, the more challenging the scaling. The figure is adapted from refs. 1,2. Tunable Laser SMLL Frequency Discrimination Coherent Detection Beam Steering Fig. 2 | Illustrative renditions of LSI silicon photonic systems capturing current and future technologies. a WDM Transceiver: A semiconductor mode-locked laser (SMLL) provides multi-wavelength continuous-wave (CW) light to an array of compact, WDM-capable modulators and filters. Reflection control circuits limit back reflections into the laser. High-speed photodetectors (PDs) carry out the O/E conversion. b The electrical current is then amplified by transimpedance amplifiers (TIAs) and limiting amplifiers..",
      "s7": "... Analog-to-digital converters (ADCs) are used to digitize the signal for further digital signal processing (DSP). Monitoring PDs are used for control and stabilization of wavelength, phase shift, and temperature. Digital-to-analog converters (DACs) and drivers are used for E/O modulation of the digital signal. Dynamic random-access memory (DRAM) provides large memory access. Micro-controllers (\u03bcC) may be used to offload some of the digital processing as well. c LIDAR: A tunable laser provides frequency chirped light to a network of phase shifters, circulators/duplexers and coherent frontend for homodyne/ heterodyne frequency-modulated CW (FMCW) ranging and detection. Beam steering is done using optical phase arrays (OPAs) or focal-plane arrays (FPAs). Delay line interferometers aid in calibrating the received beat frequencies and support chirp linearization by directly controlling the tunable laser or a modulator and various forms of error correction through DSP. Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 2 Shrinking the area will be a key focus for the next decade of silicon photonics process development for..",
      "s8": "..the LSI and VLSI era. In reality, the biggest density limitations rarely come from device size; the spacing between waveguides to eliminate crosstalk is much larger than the size of the actual waveguides. For radio-frequency (RF) devices, spacings between active elements\u2014which are microns in critical dimension\u2014are often in the hundreds of microns, to eliminate RF crosstalk. Shrinking these \u2018blank spaces\u2019 requires very detailed systems-level simulation and aggressive multi-physics modeling, and will be at the heart of making chips smaller, cheaper, and higher density. The passives themselves are generally limited in size reduction by the index contrast and the operating wavelength of 1\u20132 \u03bcm. There is still some headroom with the use of inverse design techniques to shrink passive building blocks, but the waveguide itself cannot really shrink much below today\u2019s 400\u2013500 nm width for silicon platforms. However, significant scaling is still possible in the optical I/O couplers and high-speed modulators. For coupling to optical fibers, V-grooves with edge couplers provide low-loss, easy-to-package connectivity at the cost of a considerable chip area. Edge couplers without V-grooves are smaller but require more precise active alignment and surface treatment (polishing, dicing..",
      "s9": "..), thereby increasing cost. Multicore fibers are an attractive solution for efficiently using limited photonic beachfront around the edges of a chip45. The main alternative coupling approach is through grating couplers, which are compact, provide the flexibility of positioning on the chip surface, enable wafer level testing, and can also be realized with low insertion loss (IL), but suffer from polarization and temperature sensitivity and lower optical bandwidth46. Passive alignment packaging techniques, such as photonic wire bonding (PWB)47, offer an attractive potential alternative. Using computer vision and automation, PWBs can be fabricated in polymer photoresist through two-photon absorption between two coupling sites allowing up to 30 \u03bcm of offset. Simple alignment markers are used to locate the coupling sites, and the sites do not require strict pitch or large footprints, thereby providing a passive-aligned, low-loss, scalable port count. In another passive alignment technique for pluggable connection, the complexity and accuracy requirement can be moved from fiber assembly to wafer-level manufacturing, where a fiber-receptor die can be flip-chip integrated to the silicon photonic die with a glass spacer48. Using a combination of V-grooves and mirrors in the fiberreceptor die, and mirrors and surface couplers on the silicon photonic die, a confocal..",
      "s10": "..imaging assembly tolerant to >10 \u03bcm relative displacements of the two dies can be realized, providing a passivealigned, low-loss, scalable port-count and pluggable connector48. More reliability studies for these passive alignment-based assemblies will be helpful for broad adoption. E/O modulation The central quest for the next decade in shrinking photonic chips and thus increasing density is to find the elusive \u2018ideal\u2019 modulator in silicon photonics\u2014small in length (L), requiring a small drive voltage to incur a \u03c0 phase shift (V\u03c0), offering low propagation loss (\u03b1) and IL, and for several applications, highly linear and with large \u22123 dB E/O bandwidth (BW)49. Also, this modulator is preferably a phase shifter, as this enables higher-order coherent modulation formats. High-speed modulators. The often-used efficiency figure-of-merit (FoMefficiency) of waveguide-based modulators (Table 1) is \u03b1V\u03c0L. For MRMs, which are very compact, the loss due to \u03b1 becomes less critical50, and a better FoMefficiency inculcates the IL and Vpp (the peakto-peak voltage swing for a specific optical modulation amplitude or OMA). All modulators suffer from..",
      "s11": "..a tradeoff between FoMefficiency and E/O BW49\u201351. Finally, the power consumed in the driver depends on the modulator impedance as seen by the driver. A resistive impedance (a terminated traveling-wave modulator) consumes static (DC) and dynamic (AC) power, whereas a high impedance (capacitive) consumes primarily dynamic power. A high IL also is a proxy for higher power consumption since the laser power needs to be increased to compensate for the losses. Besides the topology, the PPA metrics for a modulator depend on the material and mechanism used for modulation. Table 2 shows the different materials used for modulators in various silicon photonic processes. p-n dopants utilizing the free-carrier plasma dispersion are natively available in all commercial silicon photonic foundries today, supporting 60 GHz or even higher E/O BW. Currently, the commercial market is dominated by such devices, in the form of traveling-wave MZM modulators52. Plasma dispersion in Si leads to mediocre FoMefficiency, with high IL for average OMA. Carrier accumulation allows for shorter MZMs, but with BW limitations53,54. When implemented as MRMs, the devices are much smaller, but IL and OMA remain suboptimal to support LSI/VLSI ICs. With Ge..",
      "s12": "..PDs already supported by most commercial silicon photonics foundries, various teams have attempted to use GeSi, a related but not identical technology, to implement a better modulator. GeSi electro-absorption modulators (EAM) based on the FranzKelydysh effect can operate in the C/L band at high E/O BW. However, they are generally not optically broadband since they use bandedge modulation for absorption. For O-band operations, modulators utilizing the quantum-confined Stark effect (QCSE) still suffer from a large IL55. While there have been multiple academic and commercial efforts in this space, it is unclear whether these modulators will find their way into future generations of commercial devices. Heterogeneous integration of modulator technologies\u2014InP, thinfilm LiNbO3 on insulator (LNOI), or thin-film BaTiO3 (BTO)\u2014with Si can be done using die-to-die, die-to-wafer, or wafer-to-wafer direct (molecular) bonding or adhesive-assisted bonding. Die-to-wafer bonding provides the flexibility of using known-good dies, increasing yield. Wafer-to-wafer bonding remains expensive because the size mismatches between the SOI acceptor..",
      "s13": "..wafers (200 mm or 300 mm) and the modulator donor wafers (150 mm or smaller) lead to wastage. Integration proximity of the (dissimilar) materials in direct bonding facilitates superior optical coupling and heat transportation between them56. However, very smooth and clean surfaces are required. Chemical mechanical polishing (CMP) procedures, already used in high-volume manufacturing (HVM) for heterogeneous direct bonding of InP to Si for lasers, must be optimized for a scalable modulator integration pathway. Annealing is needed for strong molecular bonding and outgassing, but the pre-processed SOI wafer significantly restricts the annealing temperature. Therefore, \u201clowtemperature\u201d annealing at 100 GHz E/O BW modulators are attractive for both telecom and data center applications, they require electronics capable of driving them at such speeds. Unless V\u03c0 (or Vpp) is reduced significantly, such electronics will consume a lot of power, regardless of CMOS/BiCMOS/III-V implementation. Phase-Shifters for tuning and switching. Many photonic applications require phase shifters that consume little or no power and have a low Table 2 | Comparing different modulation materials and mechanisms in their readiness to be integrated into a commercial CMOS foundry, and in power, performance (as of 2023) and area (PPA) metrics..",
      "s14": "..Perspective https://doi.org/10.1038/s41467-024-44750-0 Nature Communications | (2024) 15:751 4 \u03b1V\u03c0L for configuration, tuning and switching. For certain applications, these phase shifters should be fast as well, but 10s of GHz E/O BW is not needed. While in many circuits light only passes through one highspeed modulator, it will have to traverse many low-speed phase shifters for tuning and switching, thereby compounding the penalty of power consumption and \u03b1V\u03c0L. Metal heaters (or doped waveguides) utilizing the thermo-optic effect are available in all foundry platforms today. They have 1\u201310 \u03bcs response time, and consume considerable power, generating thermal crosstalk, and thus limiting LSI/VLSI scaling. But they do not introduce optical loss, a significant advantage over other alternatives. Improving thermal insulation reduces their power consumption by > 10 \u00d7 at the expense of an even higher response time76. Even > 100 \u00d7 improvement is possible by folding the waveguides to increase interaction with the metal heaters, but that results in an IL77. The final set of materials and techniques listed in Table 2 are attractive alternatives to heaters. They include liquid crystals (LC), MEMS/NOEMS.."
    },
    "eval_answer": "**Historical Developments in Silicon Photonics:**  \n- Silicon photonics has evolved significantly, particularly with the emergence of photonic integrated circuits (PICs), which are essential for creating scalable and reconfigurable systems capable of executing complex operations [S8674834158] [S6377695058].  \n- Neuromorphic photonics, a branch of silicon photonics, combines neural networks' efficiency with photonics' speed, enabling advancements in artificial intelligence and machine learning [S3220163013].  \n\n**Current Challenges:**  \n- Key challenges in silicon photonics include energy consumption and the limitations of traditional microelectronic architectures [S6595692251].  \n- The integration of photonic interconnects is crucial for addressing these challenges, as it significantly enhances energy efficiency compared to conventional electronic systems [S5005986864] [S0472210420].  \n\n**Emerging Technologies and Innovative Packaging Solutions:**  \n- Advanced packaging techniques are revolutionizing silicon photonics integration by improving the performance and scalability of systems [S4621417116].  \n- The compatibility of neuromorphic photonics with existing CMOS technology allows for enhanced performance while utilizing established manufacturing processes [S5577362794] [S6201274203].  \n- The ongoing convergence of photonic and electronic design automation (EDA) tools facilitates the integration of complex photonic circuits with existing electronic systems, which is vital for enhancing overall performance [S6369263537].  \n- The development of programmable photonic circuits that can self-configure and self-correct represents a transformative trend, enabling more versatile and efficient circuit designs [S8508160691].  \n\n**Future Trends:**  \n- A significant trend is the shift from component-oriented to circuit-oriented design methodologies, which enhances the scalability and complexity of PICs [S4416117706].  \n- Establishing standardized circuit models and design kits for silicon photonics will streamline the design process and improve collaboration between designers and fabrication facilities [S1618801126].  \n\n**Conclusion:**  \nSilicon photonics has made substantial progress through innovations in PICs and neuromorphic photonics, yet it faces challenges like energy efficiency and integration with existing technologies. Emerging packaging solutions and design methodologies are critical to overcoming these challenges, enabling large-scale integration and unlocking the full potential of silicon photonics.",
    "eval_references": {
      "S4416117706": {
        "id": "S4416117706",
        "text": "The transition from component-oriented design to circuit-oriented design methodologies in silicon photonics is a significant trend that enhances the scalability and complexity of photonic integrated circuits (PICs).",
        "children": [
          {
            "id": "E6462589905",
            "text": "January 8, 2018 Abstract Silicon Photonics technology is rapidly maturing as a platform for larger-scale photonic circuits. As a result, the associated design methodologies are also evolving from componentoriented design to a more circuit-oriented design flow, that makes abstraction from the very detailed geometry and enables design on a larger scale. In this paper, we review the state of this emerging photonic circuit design flow and its synergies with electronic design automation (EDA). We cover the design flow from schematic capture, circuit simulation, layout and verification. We discuss the similarities and the differences between photonic and electronic design, and the challenges and opportunities that present themselves in the new photonic design landscape, such as variability analysis, photonic-electronic co-simulation and compact model definition. Silicon Photonics Circuit Design: Methods, Tools and Challenges Wim Bogaerts1,2,* and Lukas Chrostowski3 1. Introduction Silicon photonics is the technology to integrate a large number of optical functions on a chip using the fabrication technology of the CMOS industry, thereby enabling low cost, large volume, manufacturing [1\u20133]. The field has rapidly evolved from a \u2018scientific hot topic\u2019 to an industrially viable platform, largely driven by telecom and datacom applications, and enabled by the growing number of manufacturing and prototyping facilities (\u2018fabs\u2019) [4]. Today there coexist a wide diversity of technology platforms to build photonic integrated circuits (PIC) [5], using different material systems such as III-V semiconductors, Lithium Niobate, high-index glasses and nitrides, polymers, and of course silicon. What makes silicon photonics a unique technology is exactly its compatibility with the manufacturing processes and tools used in the CMOS industry: this offers a route towards high volume manufacturing at potentially low cost per device. The second unique feature of silicon photonics is its high refractive index contrast, which allows for sub-micrometer waveguide dimensions, tight bends and close spacing, and in turn, this allows for dense packing of optical functions on the surface of a chip [6]. This combination makes silicon photonics the only viable technology platform for high complexity, large-scale photonic integrated circuits. However, the high refractive index contrast comes with a weakness: it imposes very stringent requirements on the dimensions of the silicon photonic circuits, as nanometer-scale variations in waveguide core width or thickness can have non-negligible effects on the performance of the photonic circuits [7]. This implies that variability introduced by the fabrication process can have a significant impact on the overall performance of a circuit. Large complex circuits will automatically suffer more from variability than simple circuits. In the end, it is the overall yield of a circuit that determines whether it is commercially viable. As CMOS manufacturing technologies continue to advance, higher precision lithography is required to fabricate ever smaller devices. While transistors continue shrinking in size, photonic devices are fundamentally limited and remain approximately constant as a function of the technology node (e.g., a ring modulator has a fixed size determined by the design target free spectral range); the benefit of improved manufacturing is that it reduces the manufacturing variability and improves yield [8]. Still, silicon photonics processes are now considered to be sufficiently good for a number of applications, as is demonstrated by products released on the market. The various fabs provide processes for silicon waveguides with acceptable propagation losses around 1-2 dB/cm [9], thermal tuners with phase shifter efficiencies ranging from 100 \u03bcW/\u03c0 to 100 mW \u03c0 [10], carrier-based electro-optic modulators working in both travelling wave and resonant modes [11, 12], and Germanium photodetectors with effi1 Ghent University - IMEC, Photonics Research Group, Department of Information Technology, Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 2 Center for Nano and Biophotonics (NB-Photonics), Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 3 University of British Columbia, Department of Electrical and Computer Engineering, 2332 Main Mall, Vancouver, British Columbia, V6T 0A7, Canada * Corresponding author: e-mail: wim.bogaerts@ugent.be Copyright line will be provided by the publisher 2 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design ciencies of \u2248 1A/W [13\u201315], with both modulators and detectors operating at high-speeds of many tens of gigahertz. Spectral filters can be implemented using combinations of waveguides and coupling structures [16\u201319]. Only the integration of the laser source, optical amplifier, and optical isolator is somewhat lagging, but solutions are becoming available based either on external sources [20] or heterogeneous integration [21\u201324]. While the majority of silicon photonics technologies operate around wavelengths in the traditional telecommunication bands between 1.2 - 1.6 \u03bcm, the wavelength range can be extended to the visible domain using silicon nitride [25]. SOI wafers (silicon on insulator, with silicon as the waveguide core and silicon dioxide as the cladding) can be used up to 3.6 \u03bcm (limited by silicon dioxide absorption), and even longer wavelengths in the mid-infrared can be accessed using germanium waveguides on a silicon substrate [26, 27]. These technologies don\u2019t lose their compatibility with CMOS manufacturing technologies and dense integration, and therefore fall under the same definition of silicon photonics used at the start of this article. Even when silicon photonics enables high complexity and large circuits, today\u2019s circuit demonstrations are generally quite small and/or simple. For datacom applications, optical transceivers usually consist of a single light path between 3-10 optical elements. Larger optical circuits usually consist of simple repetitive scaling, such as switch matrices [28, 29] or phased arrays for beam steering [30]. While these circuits demonstrate the integration potential of silicon photonics, they are not very complex, and their functionality is limited. Other applications may leverage the potential of added complexity in photonic circuits. Silicon photonics is seen as an enabling technology for biosensing and diagnostics [31\u201333], spectroscopy [25], structural monitoring [34, 35], quantum information / quantum computing [36\u201338], microwave photonics [39\u201342], and can be applied for various sensor functions (accelerometers, gyroscopes, magnetic fields), etc. Such applications will require custom chip designs with very different requirements than transceivers for datacenter and telecom applications. Fabrication processes for silicon photonics have become good enough to make large, complex circuits, with waveguide losses smaller than 1dB/cm, low-loss crossings, splitters, couplers, as well as good modulators and excellent photodetectors, all integrated into technology platforms that are subject to statistical process control (SPC) [43\u201345]. Even though there is still ample headroom for technological improvements, the complexity of the optical circuits is now largely limited by the capability to design them, while taking into account the limitations of the fabrication process such as variability and parasitics. A reliable design flow, transforming a circuit concept into a working chip, should accurately predict the yield of a complex circuit. Today, many photonic circuit designers employ manual techniques to compose their photonic circuits, with a focus on the physical geometry. This is reminiscent of the first electronic circuit design in the 1960s and early 1970s. Photonic integrated circuits share many characteristics of electronic integrated circuits. They are defined by planar processes on semiconductor wafers. The functionality can be described and modelled as a circuit, with signals propagating between the functional building blocks. As with electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          },
          {
            "id": "E2210057765",
            "text": "..electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic circuit design. While today there exist several powerful circuit simulation tools for photonics, they all have their own compact model implementation. There is no common definition of the models for even the simplest components (e.g., Copyright line will be provided by the publisher 3 waveguide, directional coupler), and the implementation of models in each tool is very different. This raises a significant barrier for fabs to invest in a compact model library for their PDK. A standard model implementation language (like Verilog-A for analog electronics [50]), or even an agreement on standard model definitions for the most common building blocks (like the BSIM transistor models [51]) would present a strong incentive to invest in compact model libraries for circuit-driven photonic design. Without reliable models, the added value of a photonic circuit design flow as in electronics is limited. Still, the parallels between electronic and photonic design automation are driving a convergence in design flows, as design tools for photonic circuits are now being coupled to established electronic design tools [49, 52\u201355]. This convergence is driven by necessity, and among all the different PIC technologies, this necessity is most acute in silicon photonics, because silicon photonics is both the most sensitive and most scalable of PIC technologies. First, silicon photonic circuits need electronic interfaces such as for the processing of high-speed signals and for electronic control loops that govern and stabilize the behavior of the photonic circuit. Second, electronics is also looking in the direction of silicon photonics to solve the interconnect bottlenecks [2, 56]. Photonic-electronic co-integration and codesign will make it possible to create integrated photonicelectronic-software systems with control and monitoring. These can compensate the process variability and enable larger, more complex circuits, and create opportunities to implement functionality that cannot be achieved with photonics or electronics separately. Photonics is in many ways very different from electronics, and these differences are also reflected in the design flows. Photonic layouts are usually not based on rectangular patterns, and this can create difficulties for design verification, and control of pattern density. Photonic signals are also different from electrical signals, and cannot be expressed as voltages and currents. Rather, the signal propagation bears a stronger resemblance to radio-frequency (RF) signals. True electronic-photonic co-design will therefore require a new mixed-signal model for co-simulation. In this paper, we present a review of the landscape of silicon photonics design methodologies, from the perspective of the circuit designer (as opposed to the device/component designer). First, we give a brief introduction about what constitutes a circuit design flow in section 2. In section 3 we start with an analysis of today\u2019s historically grown design processes, which are an evolution of component/device design. The requirements for component design, with a focus on geometrical optimization, are very different from those of circuit design, where circuit functionality is governed by the connectivity of functional building blocks. Section 4 then discusses the emerging trend towards an EDA-like design flow, with a focus on a schematic-based circuit design. Design tools are evolving at a rapid pace in this domain, but the necessary shift in mindset in the actual design community is experiencing some inertia, especially where designers have built custom tools for their specific needs, and where foundries do not yet supply PDKs compatible with schematic-driven design. In section 5 we discuss a number of significant challenges that will need to be addressed in the near future to give photonics circuit designers similar firsttime-right capabilities as electronics designers have today. Finally, section 6 presents a number of opportunities for the research community and the important actors in photonic design automation (PDA) to provide a dramatic boost to the photonic design community. 2. Design Flows The purpose of a design flow is to translate a functional idea into a working chip (i.e., the design), using a reproducible method (the flow). The final objective, i.e., a working chip, is important. While the design of simple photonic components can be done intuitively, a reproducible flow, backed up by efficient software tools, is important to guarantee that more complex chips and circuits are fabricated with sufficient yield. When implementing functionality on a photonic chip, the first step is to articulate the needed functionality. This system-level consideration is usually expressed as a relation between inputs and outputs: what behaviour or output signal is expected for a given input signal? From this abstract level, this functionality should be translated into a gradually more refined description (a circuit) until it can be implemented as a photonic integrated circuit (PIC). In a PIC, light is manipulated on the surface of a chip. At the basic level, this manipulation is done by the geometric distribution of material (or by locally changing material properties). At this detailed level, the exact behavior of the electromagnetic waves in the structure can be engineered. However, when the dimensions of the circuit become larger, this level of detail can no longer be captured efficiently, and a more abstract design approach is needed. The different levels of abstraction in a circuit design flow are illustrated in Fig. 1. We can roughly break down the design flow into the following steps: \u2013 Design Capture: the functional idea is converted into a logical circuit of functional building blocks or hierarchical subcircuits. There can be an exploration of different circuit architectures or topologies, with different choices of building blocks. \u2013 Circuit simulation: The logical circuit is simulated and its parameters are optimized so it will perform as intended. This can also include a yield analysis by introducing variability in the circuit parameters. \u2013 Circuit Layout: The logical circuit is converted into a mask layout representation that can be used for fabrication. This results eventually in a large number of polygons on different mask layers. \u2013 Global Chip Design: The logical circuits put together, and connected to a power supply distribution network, electrical I/Os, and generation of dummy tiling patterns to maintain uniform pattern density. \u2013 Verification: The layout is checked against errors, making sure it is compatible with the fabrication process and Copyright line will be provided by the publisher 4 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design Figure 1 Different levels of abstraction in a circuit design flow. The horizontal axis indicates the sequence of design steps, while the vertical axis indicates the level of abstraction. In a circuit design flow, the physical modelling of components is preferably avoided, and circuit simulations are based on compact models. post-layout simulations are performed to ensure that the layout will perform the intended function. \u2013 Tape-out and fabrication: The layout file undergoes a number of post-processing steps to convert it into the actual write patterns, and the chip is fabricated. \u2013 Testing and Packaging: The fabricated chip is packaged and tested, and the results are compared with the original design. If needed, the design information will be updated to improve the next generation of designs. Clearly identifying and separating these steps and levels of abstraction in the design is essential to the scaling of circuits. This is a lesson that has been learned in electronics [57]. Electronic circuits are not designed at the geometry of the individual transistors. Rather, known transistor devices, or known subcircuits consisting of many transistors, diodes and other electrical elements, are reused to compose larger circuits. The circuit designers trust that the building blocks have been properly designed and qualified by the fabs and device designers, and that the relevant geometries and models are supplied in a process design kit (PDK) and external libraries. A process design kit (PDK), in general, is an information package that contains sufficient information for a designer to create a chip design that can be fabricated..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          }
        ]
      },
      "S0472210420": {
        "id": "S0472210420",
        "text": "The integration of photonic interconnects in neuromorphic computing systems significantly enhances the energy efficiency of silicon photonics, addressing key challenges in the field.",
        "children": [
          {
            "id": "E1341997583",
            "text": "&lt;p&gt;&lt;a href=\"http://www.osa-jon.org/virtual_issue.cfm?vid=28\"&gt;Feature Issue on Nanoscale Integrated Photonics for Optical Networks&lt;/a&gt;&lt;/p&gt;Fiber optic communication is well established today in long-haul, metro, and some data communication segments. Optical technologies continue to penetrate more into the network owing to the increase in bandwidth demands; however, they still suffer from too expensive solutions. Silicon photonics is a new technology developing integrated photonic devices and circuits based on the unique silicon material that has already revolutionized the face of our planet through the microelectronics industry. This paper reviews silicon photonics technology at Intel, showing how using the same mature, low-cost silicon CMOS technology we develop many of the building blocks required in current and future optical networks. After introducing the silicon photonics motivation for networks, we discuss the various devices--waveguides, modulators, Raman amplifiers and lasers, photodetectors, optical interconnects, and photonic crystals--from the points of view of applications, principle of operation, process development, and performance results.",
            "url": "https://openalex.org/W2025067302",
            "title": "Integrated silicon photonics for optical networks [Invited]",
            "publication_date": "2007-01-01"
          },
          {
            "id": "S6595692251",
            "text": "The integration of photonic interconnects in neuromorphic computing systems can significantly reduce energy consumption compared to traditional microelectronic architectures, addressing one of the key challenges in silicon photonics.",
            "children": [
              {
                "id": "E3069199739",
                "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              },
              {
                "id": "E7997048239",
                "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              }
            ]
          }
        ]
      },
      "S6377695058": {
        "id": "S6377695058",
        "text": "Recent advancements in photonic integrated circuits (PICs) are crucial for enabling scalable and reconfigurable systems in silicon photonics, which can overcome traditional limitations.",
        "children": [
          {
            "id": "E4511217456",
            "text": "This paper surveys technical challenges involved in designing and manufacturing integrated optoelectronic devices in a high-volume complementary metal-oxide-semiconductor (CMOS) microelectronic fabrication facility. The paper begins by introducing the motivations for building these devices in silicon. We discuss the advantages and challenges of both hybrid and monolithic strategies for optoelectronic integration. We then discuss the issues involved in building the devices in a standard CMOS facility, including specific technical examples. These include low-loss waveguides (WGs) for Raman lasers, fast silicon modulators, SiGe heterostructures for infrared photodetection, silicon-oxynitride (SiON) devices on silicon-on-insulator (SOI), silicon optical bench (SiOB) technology, and waveguide tapers. We conclude with a discussion and recommendations for future work in silicon photonics",
            "url": "https://openalex.org/W2118390587",
            "title": "Development of CMOS-Compatible Integrated Silicon Photonics Devices",
            "publication_date": "2006-11-01"
          },
          {
            "id": "S8674834158",
            "text": "Recent developments in photonic integrated circuits (PICs) are paving the way for scalable and reconfigurable systems that can execute complex operations, which is crucial for overcoming the limitations of traditional silicon photonics.",
            "children": [
              {
                "id": "E3069199739",
                "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              },
              {
                "id": "E7997048239",
                "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              }
            ]
          }
        ]
      },
      "S5005986864": {
        "id": "S5005986864",
        "text": "Silicon photonics enables high-speed data transmission in telecommunications by integrating photonic interconnects, which significantly enhance energy efficiency compared to traditional electronic systems.",
        "children": [
          {
            "id": "E6302234436",
            "text": "Volker J Sorger, George Washington University, USA\nDaniel J Blumenthal, University of California, Santa Barbara, USA\nPaul W Juodawlkis, MIT Lincoln Laboratory, USA\nWilliam Loh, MIT Lincoln Laboratory, USA\nCheryl Sorace-Agaskar, MIT Lincoln Laboratory, USA\nAlex E Jones, University of Bristol, UK\nKrishna C Balram, University of Bristol, UK\nJonathan C F Matthews, University of Bristol, UK\nAnthony Laing, University of Bristol, UK\nMarcelo Davanco, National Institute of Standards and Technology, USA\nLin Chang, University of California, Santa Barbara, USA\nJohn E Bowers, University of California, Santa Barbara, USA\nNiels Quack, \u00c9cole Polytechnique D\u00e9d\u00e9rale de Lausanne, Switzerland\nChristophe Galland, \u00c9cole Polytechnique D\u00e9d\u00e9rale de Lausanne, Switzerland\nIgor Aharonovich, University of Technology Sydney, Australia\nMartin A Wolff, University of M\u00fcnster, Germany\nCarsten Schuck, University of M\u00fcnster, Germany\nNeil Sinclair, Harvard University, USA\nMarko Lon\u010dar, Harvard University, USA\nTin Komljenovic, Nexus Photonics, Goleta, USA\nDavid Weld, University of California, Santa Barbara, USA\nShayan Mookherjea, University of California, San Diego, USA\nSonia Buckley, National Institute of Standards and Technology, USA\nMarina Radulaski, University of California, Davis, USA\nStephan Reitzenstein, Technische Universit\u00e4t Berlin, Germany\nBenjamin Pingault, Harvard University, USA\nBartholomeus Machielse, Harvard University, USA\nDebsuvra Mukhopadhyay, Texas A&M University, USA\nAlexey Akimov, Texas A&M University, USA\nAleksei Zheltikov, PN Lebedev Physical Institute, 17 -Russian Quantum Center, Moscow, Moscow Region, Russia, Russia\nGirish S Agarwal, Texas A&M University, USA\nKartik Srinivasan, National Institute of Standards and Technology, USA\nJuanjuan Lu, Yale University, USA\nHong X Tang, Yale University, USA\nWentao Jiang, Stanford University, USA\nTimothy P Mckenna, Stanford University, USA\nAmir H Safavi-Naeini, Stanford University, USA\nStephan Steinhauer, KTH Royal Institute of Technology, Sweden\nAli W Elshaari, KTH Royal Institute of Technology, Sweden\nVal Zwiller, KTH Royal Institute of Technology, Sweden\nPaul S Davids, -Sandia National Labs, USA\nNicholas Martinez, -Sandia National Labs, USA\nMichael Gehl, -Sandia National Labs, USA\nJohn Chiaverini, MIT Lincoln Laboratory, USA\nKaran K Mehta\nJacquiline Romero, Australian Research Council Centre of Excellence for Engineered Quantum Systems (EQUS), Australia\nNavin B Lingaraju, -Purdue University, USA\nAndrew M Weiner, -Purdue University, USA\nDaniel Peace, Griffith University, Australia\nRobert Cernansky, Griffith University, Australia\nMirko Lobino, Griffith University, Australia\nEleni Diamanti, -Centre National de la Recherche Scientifique, France\nLuis Trigo Vidarte, -Centre National de la Recherche Scientifique, France\nRyan M Camacho, Brigham Young University, USA Roadmap\nBenjamin Pingault1, University of California, Santa Barbara, USA\nBartholomeus Machielse1, University of California, Santa Barbara, USA\nMarko Lon\u010dar1, University of California, Santa Barbara, USA\nAbstract\nRoadmap complementary resource to the recent OIDA Roadmap on Quantum Photonics [12]. With advances in materials, PIC-based platforms, devices and circuits, fabrication and integration processes, packaging, and testing and benchmarking, we can expect a transition from single-and few-function prototypes to largescale integration of multi-functional and reconfigurable QPICs. These circuits will play a key role in how quantum information is processed, stored, transmitted, and utilized for quantum computing, communications, metrology, and sensing. This roadmap highlights the current progress in the field of integrated quantum photonics, future challenges, and advances in science and technology needed to meet these challenges. Key areas of research and technology addressed include: 1. PIC Platforms: While silicon-based photonics has been the workhorse for QPICs owing to the low waveguide loss and the existing foundry infrastructure, other material platforms have been developed to further extend the capabilities beyond what silicon can offer [13], including III-V semiconductors, lithium niobate, silicon carbide, nitrides, diamond, and tantalum pentoxide. Future advances in quantum photonics will likely require a heterogeneous approach [14] that combines multiple PIC materials to achieve high-level functionality with integrated lasers and amplifiers, passive components, modulators, quantum frequency converters, efficient detectors or chip-to-fiber couplers, and control logic. 2. Quantum and Classical Light Sources and Qubits: Quantum light sources generating single photons, entangled-photon pairs, squeezed light, and other non-classical states are fundamental resources for quantum information science. Quantum sources typically fall into two categories: Quantum emitters that can produce photons on-demand, and sources based on \uf063 (2) or \uf063 (3)\nnonlinear optics that produce photons probabilistically. Rapid progress with quantum emitters based on InAs quantum dots, defects in diamond and silicon carbide, emissive centers in silicon, and probabilistic sources in several nonlinear PICs have already been used successfully for numerous applications in communications, computing, and networking. Further technological developments that improve PIC integration must follow to increase the photon generation quality, efficiency, and rates. Likewise, as these sources currently rely on off-chip optical pumping, improvement in their efficiency will facilitate the integration of precision and tunable pump lasers, filters, and control electronics on chip for packaged, turn-key systems. Such heterogeneous integration may demand co-design approaches beyond current single platform design rules.\nQuantum frequency conversion is essential to establish interconnections between quantum systems operating in different wavelength regimes, for example, to connect a trapped ion quantum processor to a neutral atom ensemble quantum memory or to an optical network featuring microwave-frequency superconducting qubits. Numerous approaches to frequency conversion exist, including optomechanics and nonlinear photonics and optoelectronics, providing exciting prospects for achieving high conversion efficiency spanning ultraviolet to telecommunications wavelengths with low added noise.\nTo take full advantage of the low loss and high throughput afforded by integrated photonics, effective schemes for on-chip coupling of high-performance photodetectors with single-and photon-number resolving capabilities are required. Detectors based on Figure 2. The Roadmap on Integrated Quantum Photonics covers topics spanning classical and quantum light sources, detectors, frequency conversion and transduction, photonic material platforms, methods for large-scale integration, and applications in computing, communications, and sensing.\n1 -Introduction\nGalan Moody 1 , Volker J. Sorger 2 , and Daniel J. Blumenthal 1 1 University of California Santa Barbara, Santa Barbara, CA, USA 2 George Washington University, Washington D.C., USA Integrated photonics is at the heart of many classical technologies, from optical communications to biosensors, LIDAR, and data center fiber interconnects. There is strong evidence that these integrated technologies will play a key role in quantum systems as they grow from few-qubit prototypes to tens of thousands of qubits [1] . The underlying laser and optical quantum technologies, with the required functionality and performance, can only be realized through the integration of these components onto quantum photonic integrated circuits (QPICs) with accompanying electronics. In the last decade, remarkable advances in quantum photonic integration and a dramatic reduction in optical losses [2] have enabled benchtop experiments to be scaled down to prototype chips with improvements in efficiency, robustness, and key performance metrics [3, 4] . The reduction in size, weight, power, and improvement in stability that will be enabled by QPICs will play a key role in increasing the degree of complexity and scale in quantum demonstrations. As an example, the timeline in Figure 1 illustrates this rapid progression from few-component circuits enabling two-photon quantum interference in 2008 [5] to a decade later with devices combining more than 650 components capable of arbitrary and programmable two-qubit operations [6] , enabling advances in foundational quantum mechanics, computing, communications, and metrology [7] . Today, experiments that until recently occupied an optical table, such as Boson sampling [8] , diamond color-center emitter arrays [9] , and multi-ion quantum logic [10, 11] , have moved on-chip. Despite these early advancements, the level of QPIC complexity lags that of conventional PICs, which today comprise \uf07e5x10 3 components on chip. Interestingly, this progression mirrors the development of digital electronics that occurred through the 1960s and 1970s, leading to potential quantum integration scaling laws and roadmaps. Today's state-of-the-art integration, with current materials, fabrication, and packaging technologies, gives a snapshot of on-chip complexity currently achievable.\nRoadmap Organization and Goals\nIn this roadmap article, we highlight the status, current and future challenges, and emerging technologies in several key research areas in integrated quantum photonics, which serves as a\nPHOTONIC CIRCUIT INTEGRATED PLATFORMS 2 -Heterogeneous Integrated Photonics for Quantum Information Science and Engineering\nPaul W. Juodawlkis, William Loh, Cheryl Sorace-Agaskar MIT Lincoln Laboratory Status Integrated photonic technologies will play a central role in advancing the frontiers of quantum information science and in the full realization of quantum processing, sensing, and communication applications. The required scaling (i.e., thousands to millions of quantum processing or sensing elements) and functi",
            "url": "https://arxiv.org/pdf/2102.03323"
          },
          {
            "id": "S6595692251",
            "text": "The integration of photonic interconnects in neuromorphic computing systems can significantly reduce energy consumption compared to traditional microelectronic architectures, addressing one of the key challenges in silicon photonics.",
            "children": [
              {
                "id": "E3069199739",
                "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              },
              {
                "id": "E7997048239",
                "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              }
            ]
          }
        ]
      },
      "S4621417116": {
        "id": "S4621417116",
        "text": "The compatibility of neuromorphic photonics with existing CMOS technology allows for improved performance in silicon photonics systems while utilizing established manufacturing processes.",
        "children": [
          {
            "id": "E1815322147",
            "text": "..century and supplied the internet infrastructure. Although created before the 1980s, the word photonics was used commonly for the first time in the 1980s as network operators of Figure 1. The electromagnetic spectrum. Photonics is essentially the science that involves generation of a photon (light), its detection, as well as manipulation via transmission, emission, signal processing, modulation, switching, amplification, and sensing. Most importantly, photonics involves the proper use of light as a tool for the benefit of humans [2,3]. Most photonics applications, even though they cover all technical applications over the entire electromagnetic spectrum, range from near-infrared light to visible region. The term \u201cphotonics\u201d was derived from the root word \u201cphoton\u201d, which connotes the tiniest entity of light analogous to an electron in electricity. Just as the electronics revolutionized the 20th century, photonics is doing the same in the 21st century. Photonics is made up of many different technologies including optical fibers, lasers, detectors, quantum electronics, fibers, and materials [4]. The term photonics was first used to designate a field of research area responsible for utilizing light to perform tasks that are conventionally related to the traditional sphere of electronics, like telecommunications, information processing, and so on. Studies in the field of potonics began in 1960 after the discovery of lasers. Other progress followed including optical fibers for transmitting information, the laser diode in the 1970s, as well as erbium fiber amplifiers. These developments made the foundation for the industrial revolution in the telecommunications sector during the late 20th Micromachines 2018, 9, 452 3 of 25 century and supplied the internet infrastructure. Although created before the 1980s, the word photonics was used commonly for the first time in the 1980s as network operators of telecommunications embraced fiber-optic data transmission. Photonics came into being when the \u201cIEEE Lasers and Electro-Optics Society\u201d came up with a journal called \u201cPhotonics Technology Letters\u201d towards the end of the 18th century. Through the years, until 2001 with the dot-com crash, research was primarily focused on optical fiber telecommunication. Nevertheless, the field of photonics has a broad range of scientific and technological applications. These include chemical and biological sensors, laser manufacturing, medical therapy and medical diagnosis, optical computing and displaying technology. Advancement of photonics is possible due to the current success recorded concerning the development of silicon photonics. Photonics is related to opto-mechanics, electro-optics, quantum electronics and quantum optics. Nevertheless, these fields mean different things to both the scientific as well as the business community. Quantum optics is often concerned with fundamental theoretical research areas. Photonics, on the other hand, deals with applied research and progress. Optoelectronics is used to refer to the circuits or devices consisting of both electrical and optical components. The word \u201celectro-optics\u201d was utilized in the past to specifically relate to nonlinear interactions between electrical and optical devices. These devices include bulk crystal modulators and later include advanced imaging sensors that are typically employed by both government and private individuals in surveillance activities [5,6]. Photonics is said to be an \u201cAll-Pervasive\u201d technology because it allows unlimited light to travel faster than the electrons that are used in electronic computer chips, which means that optical computers will compute thousands of times faster than any electronic computers because of the physical limitations of electronic conduction. More wavelengths can be packed into an optical fiber to allow an increase in the transmission bandwidth that can be in conventional copper wires. There is no electromagnetic interference in light compared to electrons in copper wires [7,8]. 2. Applications of Photonics Photonics have uses in almost every aspect of our life, ranging from daily life to highly innovative science. For instance, information processing, telecommunications, light detection, metrology, lighting, spectroscopy, photonic computing, holography, medical field (surgery, vision correction, health monitoring and endoscopy), fighting machinery, visual art, agriculture, laser material processing, robotics, and biophotonics. Similar to the way electronics have been used extensively since the creation of earlier transistors of 1948, the exceptional use of photonics continuously increases. Economically significant uses of photonic devices include fiber optic telecommunications, optical data storage, displays, optical pumping of high-power lasers and laser printing. Prospective applications of photonics are practically limitless and include medical diagnostics, organic synthesis, information, and communication, as well as fusion energy [9,10]: \u2022 Telecommunication: optical down-converter to microwave, and optical fiber communications. \u2022 Medical applications: laser surgery, poor eyesight correction, tattoo removal and surgical endoscopy. \u2022 Manufacturing processes in industries: involves the use of laser in welding, cutting, drilling, and many surface modification techniques. \u2022 Building and construction: smart structures, laser range finding, and laser leveling. \u2022 Space exploration and aviation: including astronomical telescopes. \u2022 Military operations: command and control, IR sensors, navigation, mine laying, hunt and salvage, and discovery. \u2022 Metrology: range finding, frequency and time measurements. \u2022 Photonic computing: printed circuit boards, and quantum computing. \u2022 Micro-photonics and nanophotonics. These typically include solid-state devices and photonic crystals [11]. In simple terms, photonics is currently solving and addressing the challenges of a modern world. Photonics enhances the quality of life; it safeguards our health, security, and safety, it drives our economic growth, and it creates jobs Micromachines 2018, 9, 452 4 of 25 as well as global effectiveness. Photonics technology enhances the quality of life in many areas. Specific areas are communications and information technology, advanced manufacturing, defense, health and medicine, and energy [12,13]. Photodetectors are used to detect light. They can be very slow, as in the case of solar cells that are used in harvesting sunlight energy, or very fast like photodiodes that are very fast and are employed in communications in conjunction with digital cameras. Numerous others centered on quantum, thermal, photoelectric and chemical areas also exist. Photonics likewise involves research on photonic systems. The term photonics system has found its application in optical communication systems [14]. 3. Advances in Photonics There has been an exponential growth in the research activities in the field of photonics and optics over the years, as illustrated by the publication and citation trends from the Thomson Reuters web of science database (Figure 2). Micromachines 2018, 9, x FOR PEER REVIEW 4 of 25 health and medicine, and energy [12,13]. Photodetectors are used to detect light. They can be very slow, as in the case of solar cells that are used in harvesting sunlight energy, or very fast like photodiodes that are very fast and are employed in communications in conjunction with digital cameras. Numerous others centered on quantum, thermal, photoelectric and chemical areas also exist. Photonics likewise involves research on photonic systems. The term photonics system has found its application in optical communication systems [14]. 3. Advances in Photonics There has been an exponential growth in the research activities in the field of photonics and optics over the years, as illustrated by the publication and citation trends from the Thomson Reuters web of science database (Figure 2). Figure 2. Publications and citation trends in Photonics (Source: Thomson Reuters Web of Science). Photonic networks are the backbone of data dissemination, specifically in the modern and upcoming wireless communication systems. Photonic networks continue to gain interest for distribution of data from, say, central location to a remote antenna unit at base stations. While the demand for wireless photonic systems continues to rise, there is a need for implementation of lowcost systems [15]. Two of the most popular data transmission methods in wireless photonic systems are digital baseband and RoF (Radio-over-Fiber) optical communication. In addition, further emerging fields are opto-atomics, in which there is an integration of both atomic and photonic devices. Opto-atomics applications include precise time-keeping. Opto-mechanics, metrology, and navigation, as well as polaritonics, are different from photonics due to the presence of polarization as the primary carrier of information. Microwave photonics is considered to be an emerging research field. Microwave photonics is an enabling technology for the generation, control, distribution, measurement, and detection of microwave signals. It also deals with the operation of new systems and devices [16\u201319]. Part of the various functionalities facilitated by photonics, microwave measurements centered on photonics can offer greater performance regarding broad frequency coverage, significant direct bandwidth, high immunity to electromagnetic interference (EMI) and low frequency-dependent loss. Photonic microwave measurements therefore have been widely investigated in recent times. Moreover, several new methodologies have been offered to address the challenges confronting electronic solutions [20]. Plasmon lasers are among the categories of optical frequency amplifiers that send strong, penetrating, and guiding superficial plasmons underneath the diffraction walls. The interactions between light energy and matter can be intensely improved by the tightly held electric fields in plasmon lasers. This will also bring substantial innovative possibilities to data storage, bio-sensing, optical communications and photolithography [21]. Because they can generate high-intensity nano-scale electromagnetic radiation in a fraction of a second, the modern development of plasmon lasers today has sparked the investigation of nanoscience and technology. This would enable more feature sizes than the conventional lasers [22,23]. They could also be used to Figure 2. Publications and citation trends in Photonics (Source: Thomson Reuters Web of Science). Photonic networks are the backbone of data dissemination, specifically in the modern and upcoming wireless communication systems. Photonic networks continue to gain interest for distribution of data from, say, central location to a remote antenna unit at base stations. While the demand for wireless photonic systems continues to rise, there is a need for implementation of low-cost systems [15]. Two of the most popular data transmission methods in wireless photonic systems are digital baseband and RoF (Radio-over-Fiber) optical communication. In addition, further emerging fields are opto-atomics, in which there is an integration of both atomic and photonic devices. Opto-atomics applications include precise time-keeping...",
            "url": "https://www.mdpi.com/2072-666X/9/9/452/pdf?version=1536714669"
          },
          {
            "id": "S5577362794",
            "text": "The compatibility of neuromorphic photonics with existing CMOS technology presents a unique opportunity to enhance the performance of silicon photonics systems while leveraging established manufacturing processes.",
            "children": [
              {
                "id": "E3069199739",
                "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              },
              {
                "id": "E7997048239",
                "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              }
            ]
          }
        ]
      },
      "S6201274203": {
        "id": "S6201274203",
        "text": "Recent advancements in photonic integrated circuits (PICs) are crucial for the scalability of silicon photonics in telecommunications, allowing for complex operations that traditional systems cannot achieve.",
        "children": [
          {
            "id": "E6302234436",
            "text": "Volker J Sorger, George Washington University, USA\nDaniel J Blumenthal, University of California, Santa Barbara, USA\nPaul W Juodawlkis, MIT Lincoln Laboratory, USA\nWilliam Loh, MIT Lincoln Laboratory, USA\nCheryl Sorace-Agaskar, MIT Lincoln Laboratory, USA\nAlex E Jones, University of Bristol, UK\nKrishna C Balram, University of Bristol, UK\nJonathan C F Matthews, University of Bristol, UK\nAnthony Laing, University of Bristol, UK\nMarcelo Davanco, National Institute of Standards and Technology, USA\nLin Chang, University of California, Santa Barbara, USA\nJohn E Bowers, University of California, Santa Barbara, USA\nNiels Quack, \u00c9cole Polytechnique D\u00e9d\u00e9rale de Lausanne, Switzerland\nChristophe Galland, \u00c9cole Polytechnique D\u00e9d\u00e9rale de Lausanne, Switzerland\nIgor Aharonovich, University of Technology Sydney, Australia\nMartin A Wolff, University of M\u00fcnster, Germany\nCarsten Schuck, University of M\u00fcnster, Germany\nNeil Sinclair, Harvard University, USA\nMarko Lon\u010dar, Harvard University, USA\nTin Komljenovic, Nexus Photonics, Goleta, USA\nDavid Weld, University of California, Santa Barbara, USA\nShayan Mookherjea, University of California, San Diego, USA\nSonia Buckley, National Institute of Standards and Technology, USA\nMarina Radulaski, University of California, Davis, USA\nStephan Reitzenstein, Technische Universit\u00e4t Berlin, Germany\nBenjamin Pingault, Harvard University, USA\nBartholomeus Machielse, Harvard University, USA\nDebsuvra Mukhopadhyay, Texas A&M University, USA\nAlexey Akimov, Texas A&M University, USA\nAleksei Zheltikov, PN Lebedev Physical Institute, 17 -Russian Quantum Center, Moscow, Moscow Region, Russia, Russia\nGirish S Agarwal, Texas A&M University, USA\nKartik Srinivasan, National Institute of Standards and Technology, USA\nJuanjuan Lu, Yale University, USA\nHong X Tang, Yale University, USA\nWentao Jiang, Stanford University, USA\nTimothy P Mckenna, Stanford University, USA\nAmir H Safavi-Naeini, Stanford University, USA\nStephan Steinhauer, KTH Royal Institute of Technology, Sweden\nAli W Elshaari, KTH Royal Institute of Technology, Sweden\nVal Zwiller, KTH Royal Institute of Technology, Sweden\nPaul S Davids, -Sandia National Labs, USA\nNicholas Martinez, -Sandia National Labs, USA\nMichael Gehl, -Sandia National Labs, USA\nJohn Chiaverini, MIT Lincoln Laboratory, USA\nKaran K Mehta\nJacquiline Romero, Australian Research Council Centre of Excellence for Engineered Quantum Systems (EQUS), Australia\nNavin B Lingaraju, -Purdue University, USA\nAndrew M Weiner, -Purdue University, USA\nDaniel Peace, Griffith University, Australia\nRobert Cernansky, Griffith University, Australia\nMirko Lobino, Griffith University, Australia\nEleni Diamanti, -Centre National de la Recherche Scientifique, France\nLuis Trigo Vidarte, -Centre National de la Recherche Scientifique, France\nRyan M Camacho, Brigham Young University, USA Roadmap\nBenjamin Pingault1, University of California, Santa Barbara, USA\nBartholomeus Machielse1, University of California, Santa Barbara, USA\nMarko Lon\u010dar1, University of California, Santa Barbara, USA\nAbstract\nRoadmap complementary resource to the recent OIDA Roadmap on Quantum Photonics [12]. With advances in materials, PIC-based platforms, devices and circuits, fabrication and integration processes, packaging, and testing and benchmarking, we can expect a transition from single-and few-function prototypes to largescale integration of multi-functional and reconfigurable QPICs. These circuits will play a key role in how quantum information is processed, stored, transmitted, and utilized for quantum computing, communications, metrology, and sensing. This roadmap highlights the current progress in the field of integrated quantum photonics, future challenges, and advances in science and technology needed to meet these challenges. Key areas of research and technology addressed include: 1. PIC Platforms: While silicon-based photonics has been the workhorse for QPICs owing to the low waveguide loss and the existing foundry infrastructure, other material platforms have been developed to further extend the capabilities beyond what silicon can offer [13], including III-V semiconductors, lithium niobate, silicon carbide, nitrides, diamond, and tantalum pentoxide. Future advances in quantum photonics will likely require a heterogeneous approach [14] that combines multiple PIC materials to achieve high-level functionality with integrated lasers and amplifiers, passive components, modulators, quantum frequency converters, efficient detectors or chip-to-fiber couplers, and control logic. 2. Quantum and Classical Light Sources and Qubits: Quantum light sources generating single photons, entangled-photon pairs, squeezed light, and other non-classical states are fundamental resources for quantum information science. Quantum sources typically fall into two categories: Quantum emitters that can produce photons on-demand, and sources based on \uf063 (2) or \uf063 (3)\nnonlinear optics that produce photons probabilistically. Rapid progress with quantum emitters based on InAs quantum dots, defects in diamond and silicon carbide, emissive centers in silicon, and probabilistic sources in several nonlinear PICs have already been used successfully for numerous applications in communications, computing, and networking. Further technological developments that improve PIC integration must follow to increase the photon generation quality, efficiency, and rates. Likewise, as these sources currently rely on off-chip optical pumping, improvement in their efficiency will facilitate the integration of precision and tunable pump lasers, filters, and control electronics on chip for packaged, turn-key systems. Such heterogeneous integration may demand co-design approaches beyond current single platform design rules.\nQuantum frequency conversion is essential to establish interconnections between quantum systems operating in different wavelength regimes, for example, to connect a trapped ion quantum processor to a neutral atom ensemble quantum memory or to an optical network featuring microwave-frequency superconducting qubits. Numerous approaches to frequency conversion exist, including optomechanics and nonlinear photonics and optoelectronics, providing exciting prospects for achieving high conversion efficiency spanning ultraviolet to telecommunications wavelengths with low added noise.\nTo take full advantage of the low loss and high throughput afforded by integrated photonics, effective schemes for on-chip coupling of high-performance photodetectors with single-and photon-number resolving capabilities are required. Detectors based on Figure 2. The Roadmap on Integrated Quantum Photonics covers topics spanning classical and quantum light sources, detectors, frequency conversion and transduction, photonic material platforms, methods for large-scale integration, and applications in computing, communications, and sensing.\n1 -Introduction\nGalan Moody 1 , Volker J. Sorger 2 , and Daniel J. Blumenthal 1 1 University of California Santa Barbara, Santa Barbara, CA, USA 2 George Washington University, Washington D.C., USA Integrated photonics is at the heart of many classical technologies, from optical communications to biosensors, LIDAR, and data center fiber interconnects. There is strong evidence that these integrated technologies will play a key role in quantum systems as they grow from few-qubit prototypes to tens of thousands of qubits [1] . The underlying laser and optical quantum technologies, with the required functionality and performance, can only be realized through the integration of these components onto quantum photonic integrated circuits (QPICs) with accompanying electronics. In the last decade, remarkable advances in quantum photonic integration and a dramatic reduction in optical losses [2] have enabled benchtop experiments to be scaled down to prototype chips with improvements in efficiency, robustness, and key performance metrics [3, 4] . The reduction in size, weight, power, and improvement in stability that will be enabled by QPICs will play a key role in increasing the degree of complexity and scale in quantum demonstrations. As an example, the timeline in Figure 1 illustrates this rapid progression from few-component circuits enabling two-photon quantum interference in 2008 [5] to a decade later with devices combining more than 650 components capable of arbitrary and programmable two-qubit operations [6] , enabling advances in foundational quantum mechanics, computing, communications, and metrology [7] . Today, experiments that until recently occupied an optical table, such as Boson sampling [8] , diamond color-center emitter arrays [9] , and multi-ion quantum logic [10, 11] , have moved on-chip. Despite these early advancements, the level of QPIC complexity lags that of conventional PICs, which today comprise \uf07e5x10 3 components on chip. Interestingly, this progression mirrors the development of digital electronics that occurred through the 1960s and 1970s, leading to potential quantum integration scaling laws and roadmaps. Today's state-of-the-art integration, with current materials, fabrication, and packaging technologies, gives a snapshot of on-chip complexity currently achievable.\nRoadmap Organization and Goals\nIn this roadmap article, we highlight the status, current and future challenges, and emerging technologies in several key research areas in integrated quantum photonics, which serves as a\nPHOTONIC CIRCUIT INTEGRATED PLATFORMS 2 -Heterogeneous Integrated Photonics for Quantum Information Science and Engineering\nPaul W. Juodawlkis, William Loh, Cheryl Sorace-Agaskar MIT Lincoln Laboratory Status Integrated photonic technologies will play a central role in advancing the frontiers of quantum information science and in the full realization of quantum processing, sensing, and communication applications. The required scaling (i.e., thousands to millions of quantum processing or sensing elements) and functi",
            "url": "https://arxiv.org/pdf/2102.03323"
          },
          {
            "id": "S8674834158",
            "text": "Recent developments in photonic integrated circuits (PICs) are paving the way for scalable and reconfigurable systems that can execute complex operations, which is crucial for overcoming the limitations of traditional silicon photonics.",
            "children": [
              {
                "id": "E3069199739",
                "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              },
              {
                "id": "E7997048239",
                "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
                "url": "https://openalex.org/W4389817732",
                "title": "Neuromorphic Photonics Circuits: Contemporary Review",
                "publication_date": "2023-12-14"
              }
            ]
          }
        ]
      },
      "S6369263537": {
        "id": "S6369263537",
        "text": "The ongoing convergence of photonic and electronic design automation (EDA) tools is a critical trend that facilitates the integration of complex photonic circuits with existing electronic systems, enhancing overall performance.",
        "children": [
          {
            "id": "E6462589905",
            "text": "January 8, 2018 Abstract Silicon Photonics technology is rapidly maturing as a platform for larger-scale photonic circuits. As a result, the associated design methodologies are also evolving from componentoriented design to a more circuit-oriented design flow, that makes abstraction from the very detailed geometry and enables design on a larger scale. In this paper, we review the state of this emerging photonic circuit design flow and its synergies with electronic design automation (EDA). We cover the design flow from schematic capture, circuit simulation, layout and verification. We discuss the similarities and the differences between photonic and electronic design, and the challenges and opportunities that present themselves in the new photonic design landscape, such as variability analysis, photonic-electronic co-simulation and compact model definition. Silicon Photonics Circuit Design: Methods, Tools and Challenges Wim Bogaerts1,2,* and Lukas Chrostowski3 1. Introduction Silicon photonics is the technology to integrate a large number of optical functions on a chip using the fabrication technology of the CMOS industry, thereby enabling low cost, large volume, manufacturing [1\u20133]. The field has rapidly evolved from a \u2018scientific hot topic\u2019 to an industrially viable platform, largely driven by telecom and datacom applications, and enabled by the growing number of manufacturing and prototyping facilities (\u2018fabs\u2019) [4]. Today there coexist a wide diversity of technology platforms to build photonic integrated circuits (PIC) [5], using different material systems such as III-V semiconductors, Lithium Niobate, high-index glasses and nitrides, polymers, and of course silicon. What makes silicon photonics a unique technology is exactly its compatibility with the manufacturing processes and tools used in the CMOS industry: this offers a route towards high volume manufacturing at potentially low cost per device. The second unique feature of silicon photonics is its high refractive index contrast, which allows for sub-micrometer waveguide dimensions, tight bends and close spacing, and in turn, this allows for dense packing of optical functions on the surface of a chip [6]. This combination makes silicon photonics the only viable technology platform for high complexity, large-scale photonic integrated circuits. However, the high refractive index contrast comes with a weakness: it imposes very stringent requirements on the dimensions of the silicon photonic circuits, as nanometer-scale variations in waveguide core width or thickness can have non-negligible effects on the performance of the photonic circuits [7]. This implies that variability introduced by the fabrication process can have a significant impact on the overall performance of a circuit. Large complex circuits will automatically suffer more from variability than simple circuits. In the end, it is the overall yield of a circuit that determines whether it is commercially viable. As CMOS manufacturing technologies continue to advance, higher precision lithography is required to fabricate ever smaller devices. While transistors continue shrinking in size, photonic devices are fundamentally limited and remain approximately constant as a function of the technology node (e.g., a ring modulator has a fixed size determined by the design target free spectral range); the benefit of improved manufacturing is that it reduces the manufacturing variability and improves yield [8]. Still, silicon photonics processes are now considered to be sufficiently good for a number of applications, as is demonstrated by products released on the market. The various fabs provide processes for silicon waveguides with acceptable propagation losses around 1-2 dB/cm [9], thermal tuners with phase shifter efficiencies ranging from 100 \u03bcW/\u03c0 to 100 mW \u03c0 [10], carrier-based electro-optic modulators working in both travelling wave and resonant modes [11, 12], and Germanium photodetectors with effi1 Ghent University - IMEC, Photonics Research Group, Department of Information Technology, Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 2 Center for Nano and Biophotonics (NB-Photonics), Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 3 University of British Columbia, Department of Electrical and Computer Engineering, 2332 Main Mall, Vancouver, British Columbia, V6T 0A7, Canada * Corresponding author: e-mail: wim.bogaerts@ugent.be Copyright line will be provided by the publisher 2 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design ciencies of \u2248 1A/W [13\u201315], with both modulators and detectors operating at high-speeds of many tens of gigahertz. Spectral filters can be implemented using combinations of waveguides and coupling structures [16\u201319]. Only the integration of the laser source, optical amplifier, and optical isolator is somewhat lagging, but solutions are becoming available based either on external sources [20] or heterogeneous integration [21\u201324]. While the majority of silicon photonics technologies operate around wavelengths in the traditional telecommunication bands between 1.2 - 1.6 \u03bcm, the wavelength range can be extended to the visible domain using silicon nitride [25]. SOI wafers (silicon on insulator, with silicon as the waveguide core and silicon dioxide as the cladding) can be used up to 3.6 \u03bcm (limited by silicon dioxide absorption), and even longer wavelengths in the mid-infrared can be accessed using germanium waveguides on a silicon substrate [26, 27]. These technologies don\u2019t lose their compatibility with CMOS manufacturing technologies and dense integration, and therefore fall under the same definition of silicon photonics used at the start of this article. Even when silicon photonics enables high complexity and large circuits, today\u2019s circuit demonstrations are generally quite small and/or simple. For datacom applications, optical transceivers usually consist of a single light path between 3-10 optical elements. Larger optical circuits usually consist of simple repetitive scaling, such as switch matrices [28, 29] or phased arrays for beam steering [30]. While these circuits demonstrate the integration potential of silicon photonics, they are not very complex, and their functionality is limited. Other applications may leverage the potential of added complexity in photonic circuits. Silicon photonics is seen as an enabling technology for biosensing and diagnostics [31\u201333], spectroscopy [25], structural monitoring [34, 35], quantum information / quantum computing [36\u201338], microwave photonics [39\u201342], and can be applied for various sensor functions (accelerometers, gyroscopes, magnetic fields), etc. Such applications will require custom chip designs with very different requirements than transceivers for datacenter and telecom applications. Fabrication processes for silicon photonics have become good enough to make large, complex circuits, with waveguide losses smaller than 1dB/cm, low-loss crossings, splitters, couplers, as well as good modulators and excellent photodetectors, all integrated into technology platforms that are subject to statistical process control (SPC) [43\u201345]. Even though there is still ample headroom for technological improvements, the complexity of the optical circuits is now largely limited by the capability to design them, while taking into account the limitations of the fabrication process such as variability and parasitics. A reliable design flow, transforming a circuit concept into a working chip, should accurately predict the yield of a complex circuit. Today, many photonic circuit designers employ manual techniques to compose their photonic circuits, with a focus on the physical geometry. This is reminiscent of the first electronic circuit design in the 1960s and early 1970s. Photonic integrated circuits share many characteristics of electronic integrated circuits. They are defined by planar processes on semiconductor wafers. The functionality can be described and modelled as a circuit, with signals propagating between the functional building blocks. As with electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          },
          {
            "id": "E2210057765",
            "text": "..electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic circuit design. While today there exist several powerful circuit simulation tools for photonics, they all have their own compact model implementation. There is no common definition of the models for even the simplest components (e.g., Copyright line will be provided by the publisher 3 waveguide, directional coupler), and the implementation of models in each tool is very different. This raises a significant barrier for fabs to invest in a compact model library for their PDK. A standard model implementation language (like Verilog-A for analog electronics [50]), or even an agreement on standard model definitions for the most common building blocks (like the BSIM transistor models [51]) would present a strong incentive to invest in compact model libraries for circuit-driven photonic design. Without reliable models, the added value of a photonic circuit design flow as in electronics is limited. Still, the parallels between electronic and photonic design automation are driving a convergence in design flows, as design tools for photonic circuits are now being coupled to established electronic design tools [49, 52\u201355]. This convergence is driven by necessity, and among all the different PIC technologies, this necessity is most acute in silicon photonics, because silicon photonics is both the most sensitive and most scalable of PIC technologies. First, silicon photonic circuits need electronic interfaces such as for the processing of high-speed signals and for electronic control loops that govern and stabilize the behavior of the photonic circuit. Second, electronics is also looking in the direction of silicon photonics to solve the interconnect bottlenecks [2, 56]. Photonic-electronic co-integration and codesign will make it possible to create integrated photonicelectronic-software systems with control and monitoring. These can compensate the process variability and enable larger, more complex circuits, and create opportunities to implement functionality that cannot be achieved with photonics or electronics separately. Photonics is in many ways very different from electronics, and these differences are also reflected in the design flows. Photonic layouts are usually not based on rectangular patterns, and this can create difficulties for design verification, and control of pattern density. Photonic signals are also different from electrical signals, and cannot be expressed as voltages and currents. Rather, the signal propagation bears a stronger resemblance to radio-frequency (RF) signals. True electronic-photonic co-design will therefore require a new mixed-signal model for co-simulation. In this paper, we present a review of the landscape of silicon photonics design methodologies, from the perspective of the circuit designer (as opposed to the device/component designer). First, we give a brief introduction about what constitutes a circuit design flow in section 2. In section 3 we start with an analysis of today\u2019s historically grown design processes, which are an evolution of component/device design. The requirements for component design, with a focus on geometrical optimization, are very different from those of circuit design, where circuit functionality is governed by the connectivity of functional building blocks. Section 4 then discusses the emerging trend towards an EDA-like design flow, with a focus on a schematic-based circuit design. Design tools are evolving at a rapid pace in this domain, but the necessary shift in mindset in the actual design community is experiencing some inertia, especially where designers have built custom tools for their specific needs, and where foundries do not yet supply PDKs compatible with schematic-driven design. In section 5 we discuss a number of significant challenges that will need to be addressed in the near future to give photonics circuit designers similar firsttime-right capabilities as electronics designers have today. Finally, section 6 presents a number of opportunities for the research community and the important actors in photonic design automation (PDA) to provide a dramatic boost to the photonic design community. 2. Design Flows The purpose of a design flow is to translate a functional idea into a working chip (i.e., the design), using a reproducible method (the flow). The final objective, i.e., a working chip, is important. While the design of simple photonic components can be done intuitively, a reproducible flow, backed up by efficient software tools, is important to guarantee that more complex chips and circuits are fabricated with sufficient yield. When implementing functionality on a photonic chip, the first step is to articulate the needed functionality. This system-level consideration is usually expressed as a relation between inputs and outputs: what behaviour or output signal is expected for a given input signal? From this abstract level, this functionality should be translated into a gradually more refined description (a circuit) until it can be implemented as a photonic integrated circuit (PIC). In a PIC, light is manipulated on the surface of a chip. At the basic level, this manipulation is done by the geometric distribution of material (or by locally changing material properties). At this detailed level, the exact behavior of the electromagnetic waves in the structure can be engineered. However, when the dimensions of the circuit become larger, this level of detail can no longer be captured efficiently, and a more abstract design approach is needed. The different levels of abstraction in a circuit design flow are illustrated in Fig. 1. We can roughly break down the design flow into the following steps: \u2013 Design Capture: the functional idea is converted into a logical circuit of functional building blocks or hierarchical subcircuits. There can be an exploration of different circuit architectures or topologies, with different choices of building blocks. \u2013 Circuit simulation: The logical circuit is simulated and its parameters are optimized so it will perform as intended. This can also include a yield analysis by introducing variability in the circuit parameters. \u2013 Circuit Layout: The logical circuit is converted into a mask layout representation that can be used for fabrication. This results eventually in a large number of polygons on different mask layers. \u2013 Global Chip Design: The logical circuits put together, and connected to a power supply distribution network, electrical I/Os, and generation of dummy tiling patterns to maintain uniform pattern density. \u2013 Verification: The layout is checked against errors, making sure it is compatible with the fabrication process and Copyright line will be provided by the publisher 4 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design Figure 1 Different levels of abstraction in a circuit design flow. The horizontal axis indicates the sequence of design steps, while the vertical axis indicates the level of abstraction. In a circuit design flow, the physical modelling of components is preferably avoided, and circuit simulations are based on compact models. post-layout simulations are performed to ensure that the layout will perform the intended function. \u2013 Tape-out and fabrication: The layout file undergoes a number of post-processing steps to convert it into the actual write patterns, and the chip is fabricated. \u2013 Testing and Packaging: The fabricated chip is packaged and tested, and the results are compared with the original design. If needed, the design information will be updated to improve the next generation of designs. Clearly identifying and separating these steps and levels of abstraction in the design is essential to the scaling of circuits. This is a lesson that has been learned in electronics [57]. Electronic circuits are not designed at the geometry of the individual transistors. Rather, known transistor devices, or known subcircuits consisting of many transistors, diodes and other electrical elements, are reused to compose larger circuits. The circuit designers trust that the building blocks have been properly designed and qualified by the fabs and device designers, and that the relevant geometries and models are supplied in a process design kit (PDK) and external libraries. A process design kit (PDK), in general, is an information package that contains sufficient information for a designer to create a chip design that can be fabricated..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          }
        ]
      },
      "S8674834158": {
        "id": "S8674834158",
        "text": "Recent developments in photonic integrated circuits (PICs) are paving the way for scalable and reconfigurable systems that can execute complex operations, which is crucial for overcoming the limitations of traditional silicon photonics.",
        "children": [
          {
            "id": "E3069199739",
            "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          },
          {
            "id": "E7997048239",
            "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          }
        ]
      },
      "S5577362794": {
        "id": "S5577362794",
        "text": "The compatibility of neuromorphic photonics with existing CMOS technology presents a unique opportunity to enhance the performance of silicon photonics systems while leveraging established manufacturing processes.",
        "children": [
          {
            "id": "E3069199739",
            "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          },
          {
            "id": "E7997048239",
            "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          }
        ]
      },
      "S6595692251": {
        "id": "S6595692251",
        "text": "The integration of photonic interconnects in neuromorphic computing systems can significantly reduce energy consumption compared to traditional microelectronic architectures, addressing one of the key challenges in silicon photonics.",
        "children": [
          {
            "id": "E3069199739",
            "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          },
          {
            "id": "E7997048239",
            "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          }
        ]
      },
      "S3220163013": {
        "id": "S3220163013",
        "text": "Neuromorphic photonics, as a branch of silicon photonics, is emerging as a transformative technology that combines the efficiency of neural networks with the speed of photonics, enabling significant advancements in artificial intelligence and machine learning applications.",
        "children": [
          {
            "id": "E3069199739",
            "text": "Citation: Kutluyarov, R.V.; Zakoyan, A.G.; Voronkov, G.S.; Grakhova, E.P.; Butt, M.A. Neuromorphic Photonics Circuits: Contemporary Review. Nanomaterials 2023, 13, 3139. https:// doi.org/10.3390/nano13243139 Academic Editor: Yia-Chung Chang Received: 13 November 2023 Revised: 8 December 2023 Accepted: 10 December 2023 Published: 14 December 2023 Copyright: \u00a9 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). nanomaterials Review Neuromorphic Photonics Circuits: Contemporary Review Ruslan V. Kutluyarov 1, Aida G. Zakoyan 1, Grigory S. Voronkov 1, Elizaveta P. Grakhova 1 and Muhammad A. Butt 2,* 1 School of Photonics Engineering and Research Advances (SPhERA), Ufa University of Science and Technology, 32, Z. Validi St., 450076 Ufa, Russia 2 Samara National Research University, 443086 Samara, Russia * Correspondence: butt.m@ssau.ru Abstract: Neuromorphic photonics is a cutting-edge fusion of neuroscience-inspired computing and photonics technology to overcome the constraints of conventional computing architectures. Its significance lies in the potential to transform information processing by mimicking the parallelism and efficiency of the human brain. Using optics and photonics principles, neuromorphic devices can execute intricate computations swiftly and with impressive energy efficiency. This innovation holds promise for advancing artificial intelligence and machine learning while addressing the limitations of traditional silicon-based computing. Neuromorphic photonics could herald a new era of computing that is more potent and draws inspiration from cognitive processes, leading to advancements in robotics, pattern recognition, and advanced data processing. This paper reviews the recent developments in neuromorphic photonic integrated circuits, applications, and current challenges. Keywords: neuromorphic computing; photonic integrated circuit; imaging; artificial intelligence; machine learning 1. Introduction Neuromorphic photonics represents a cutting-edge, multidisciplinary realm at the confluence of artificial intelligence (AI), photonics, and neuroscience [1]. Its overarching goal is nothing short of a transformative evolution in computing, seamlessly uniting the foundational principles of neuromorphic computing with the swiftness and efficiency inherent in photonics [2]. This inventive paradigm employs light-based neurons and optical synapses to emulate the intricate behaviors of human brain cells closely, resulting in specialized hardware uniquely tailored for the domains of AI and machine learning [3]. The standout feature of this field is its remarkable energy efficiency, enabling lightning-fast, parallel data processing while conserving power resources. By harnessing the velocity of light and mirroring the intricate neural networks (NNs) of the human brain, neuromorphic photonics has the potential to unlock entirely novel horizons in high-performance computing, poised to dramatically elevate applications in pattern recognition, data manipulation, and intricate problem-solving [4,5]. While still in its infancy, this field holds promise of more capable and efficient AI systems, with the potential to fundamentally reshape the computing landscape [6]. AI technologies, encompassing facial recognition, machine learning, and autonomous driving, are reshaping our daily lives [7,8]. Deploying of task-specific AI systems demands training NNs with extensive datasets on conventional computers. However, limitations in throughput and efficiency due to prevailing computer architectures currently hinder this process [9]. Drawing inspiration from the intricate architecture of the human brain, researchers are pioneering the development of next-generation intelligent computing systems designed to emulate synapses and neurons. These systems encode information using spatiotemporal pulse patterns generated by presynaptic neurons, with postsynaptic neurons accumulating and generating new neuronal pulses upon reaching stimulation Nanomaterials 2023, 13, 3139. https://doi.org/10.3390/nano13243139 https://www.mdpi.com/journal/nanomaterials Nanomaterials 2023, 13, 3139 2 of 36 thresholds. By integrating myriad neurons, these systems give rise to nonlinear spiking NNs, enabling information processing through spatiotemporally encoded neuron pulses. Intel\u2019s TrueNorth chips, for instance, have achieved a remarkable level of energy efficiency, surpassing conventional microelectronic chips for specific AI tasks and rivaling the computational capabilities of the human brain [10]. Nevertheless, the scalability of integrated neurons remains hampered by challenges such as electrical interconnect bandwidth, pulse loss, and communication delays. Optical interconnects, offering substantial bandwidth, minimal loss, and negligible latency, have the potential to address these electrical interconnect limitations [11]. The demands of real-time, data-intensive, intelligent information processing tasks underscore the need for innovative and smart optimization hardware. Convolutional neural networks (CNNs) excel at extracting hierarchical feature maps to enhance recognition accuracy, and there is a growing interest in employing photonics for their implementation. In this context, a large-scale and adaptable photonic convolutional neural network (PCNN) that leverages a hardware-friendly distributed feedback laser diode (DFB-LD) is proposed [12]. This approach involves applying a biological time-to-first-spike coding method to a DFB-LD neuron to execute temporal convolutional operations (TCO) for image processing. In practical experiments, PCNN successfully employs TCO to extract image features using 11 \u00d7 11 convolutional kernels. Additionally, the temporal pulse shaping of a DFB-LD neuron is explored to construct a densely connected and fully connected layer, enabling rapid adjustments of synaptic weights at a remarkable rate of 5 GHz and providing high classification accuracy in benchmark image classification tasks, with 98.56% for MNIST and 87.48% for Fashion-MNIST. These findings underscore the potential of optical analog computing platforms resembling neurons for real-time and intricate intelligent processing networks [13]. This paper is meticulously organized as follows: Section 2 provides a comprehensive overview of the current market size of neuromorphic computing. Section 3 delves into the intricacies of neuromorphic photonic integrated circuits, encompassing topics such as deep neural networks, neural networks involving complex arithmetic calculations, spike neural networks, convolutional neural networks, methodologies for implementing activation functions in optical neural networks, and programmable photonic neural networks. Moving on to Section 4, we spotlight the most significant applications of neuromorphic photonics, embracing areas like neuromorphic computing-based photonic integrated circuits, neuromorphic imaging, and image processing via neuromorphic structures. Section 5 is dedicated to thoroughly discussing the prevailing challenges of developing neuromorphic photonics. Finally, in Section 6, the paper concludes with insightful remarks summarizing key findings and contributions. This deliberate structure ensures a systematic exploration of the diverse facets of neuromorphic photonics, from market dimensions to cutting-edge applications and challenges, providing a comprehensive understanding for readers. 2. Neuromorphic Computing Market Size The global neuromorphic computing market reached an impressive value of USD 4237.7 million in 2022, and its trajectory is nothing short of remarkable. Projections indicate that this market is set to undergo a staggering expansion, with a projected compound annual growth rate (CAGR) of 21.2% from 2023 to 2030 [14], as shown in Figure 1. What fuels this extraordinary growth? It is the ever-increasing adoption of neuromorphic technology across a broad spectrum of applications. Notably, integrating neuromorphic technology in deep learning (DL) applications, transistors, accelerators, next-generation semiconductors, and autonomous systems, such as robotics, drones, self-driving cars, and artificial intelligence, are key drivers behind this surge. For instance, in August 2022, a trailblazing multidisciplinary research team achieved a breakthrough with NeuRRAM, a cutting-edge neuromorphic chip. This innovative development promises to revolutionize AI applications by delivering superior accuracy while consuming significantly less energy than other Nanomaterials 2023, 13, 3139 3 of 36 platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Nanomaterials 2023, 13, x FOR PEER REVIEW 3 of 37 applications by delivering superior accuracy while consuming significantly less energy than other platforms. As we move forward, the global neuromorphic computing market is poised to play a pivotal role in shaping the future of technology. Figure 1. Neuromorphic computing market. Inspired by [14]. In the realm of neuromorphic computing, the power of intricate algorithms lies in their ability to efficiently execute within robotic systems, offering an impressive blend of superior performance and minimized energy consumption. This capability is a cornerstone for creating cutting-edge robotic systems that operate with remarkable efficiency and precision. Illustrating the potential of this technology, in September 2022, Intel Corporation embarked on a groundbreaking collaboration with the Italian Institute of Technology and the Technical University of Munich. Their joint endeavor introduced a novel object-learning method deeply rooted in neural networks (NNs). This collaborative partnership is dedicated to harnessing the prowess of neuromorphic computing using an interactive online object-learning approach. The goal is to empower robots to learn about new objects swiftly and accurately post-deployment, enhancing their adaptability and capabilities. Furthermore, leading companies in the market are actively investing in continuous research and development initiatives while introducing innovative products that drive the frontiers of research technology. An illustrative example comes from December 2022 when Polyn Technology, an Israel-based Fabless semiconductor company, made a significant announcement [15]. They unveiled the availability of neuromorphic analog signal processing models designed for Edge Impulse, a machine learning development platform geared towards edge devices [16]. These solutions specifically target ultra-low power sensor solutions for wearables and the Industrial Internet of Things, demonstrating the relentless commitment to pushing the boundaries of technological innovation. In a rapidly evolving landscape, neuromorphic computing is at the forefront of shaping the future of robotics and advanced technological solutions. 3. Neuromorphic Photonic Integrated Circuits With the recent emergence of Photonic Integrated Circuit (PIC) technology platforms, the timing is perfect for developing scalable, fully reconfigurable systems capable of executing vastly more complex operations than ever before [17..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          },
          {
            "id": "E7997048239",
            "text": "...; Xu, B.; Dong, J.; Zhang, X. A small microring array that performs large complex-valued matrix-vector multiplication. Front. Optoelectron. 2022, 15, 15. [CrossRef] 52. Goel, A.; Goel, A.K.; Kumar, A. The Role of Artificial Neural Network and Machine Learning in Utilizing Spatial Information. Spat. Inf. Res. 2023, 31, 275\u2013285. Available online: https://link.springer.com/article/10.1007/s41324-022-00494-x (accessed on 25 October 2023). [CrossRef] 53. Yamazaki, K.; Vo-Ho, V.-K.; Bulsara, D.; Le, N. Spiking Neural Networks and Their Applications: A Review. Brain Sci. 2022, 12, 863. [CrossRef] 54. Zhang, D.; Yang, J.; Ye, D.; Hua, G. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 365\u2013382. Available online: https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ ECCV_2018_paper.html (accessed on 25 October 2023). Nanomaterials 2023, 13, 3139 32 of 36 55. Li, G.; Qian, C.; Jiang, C.; Lu, X.; Tang, K. Optimization based Layer-wise Magnitude-based Pruning for DNN Compression. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, Stockholm, Sweden, 13 July 2018; International Joint Conferences on Artificial Intelligence Organization: San Francisco, CA, USA, 2018; pp. 2383\u20132389. [CrossRef] 56. Jin, X.; Peng, B.; Wu, Y.; Liu, Y.; Liu, J.; Liang, D.; Yan, J.; Hu, X. Knowledge Distillation via Route Constrained Optimization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea, October 27\u2013November 2 2019; pp. 1345\u20131354. Available online: https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_ via_Route_Constrained_Optimization_ICCV_2019_paper.html (accessed on 25 October 2023). 57. Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668\u2013673. [CrossRef] [PubMed] 58. Davies, M.; Srinivasa, N.; Lin, T.-H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou, G.; Joshi, P.; Imam, N.; Jain, S.; et al. Loihi: A Neuromorphic Manycore Processor with On-Chip Learning. IEEE Micro 2018, 38, 82\u201399. [CrossRef] 59. Furber, S.B.; Galluppi, F.; Temple, S.; Plana, L.A. The SpiNNaker Project. Proc. IEEE 2014, 102, 652\u2013665. [CrossRef] 60. Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.; Bussat, J.-M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K. Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations. Proc. IEEE 2014, 102, 699\u2013716. [CrossRef] 61. Kasabov, N.K. Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence; Springer: Berlin/Heidelberg, Germany, 2019; Available online: https://link.springer.com/book/10.1007/978-3-662-57715-8 (accessed on 25 October 2023). 62. Hubel, D.H.; Wiesel, T.N. Receptive fields of single neurones in the cat\u2019s striate cortex. J. Physiol. 1959, 148, 574\u2013591. [CrossRef] [PubMed] 63. Hubel, D.H.; Wiesel, T.N. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. J. Physiol. 1962, 160, 106\u2013154.2. [CrossRef] [PubMed] 64. Mo, W.; Luo, X.; Zhong, Y.; Jiang, W. Image recognition using convolutional neural network combined with ensemble learning algorithm. J. Phys. Conf. Ser. 2019, 1237, 022026. [CrossRef] 65. Rawat, W.; Wang, Z. Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review. Neural Comput. 2017, 29, 2352\u20132449. Available online: https://pubmed.ncbi.nlm.nih.gov/28599112/ (accessed on 25 October 2023). [CrossRef] 66. He, Y.; Zhao, C.; Zhou, X.; Shen, W. MJAR: A novel joint generalization-based diagnosis method for industrial robots with compound faults. Robot. Comput. Manuf. 2024, 86, 102668. [CrossRef] 67. Daidone, M.; Ferrantelli, S.; Tuttolomondo, A. Machine learning applications in stroke medicine: Advancements, challenges, and future prospectives. Neural Regen. Res. 2024, 19, 769\u2013773. [CrossRef] 68. Pacal, I. Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model. Expert Syst. Appl. 2024, 238, 122099. [CrossRef] 69. Stoica, I.; Song, D.; Popa, R.A.; Patterson, D.; Mahoney, M.W.; Katz, R.; Joseph, A.D.; Jordan, M.; Hellerstein, J.M.; Gonzalez, J.E.; et al. A Berkeley View of Systems Challenges for AI. arXiv 2017, arXiv:1712.05855. Available online: https://www2.eecs.berkeley. edu/Pubs/TechRpts/2017/EECS-2017-159.html (accessed on 24 October 2023). 70. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE J. Mag. 2016, 52, 127\u2013138. Available online: https://ieeexplore.ieee.org/document/7738524 (accessed on 24 October 2023). 71. De Lima, T.F.; Peng, H.T.; Tait, A.N.; Nahmias, M.A.; Miller, H.B.; Shastri, B.J.; Prucnal, P.R. Machine Learning with Neuromorphic Photonics. IEEE J. Mag. 2019, 37, 1515\u20131534. Available online: https://ieeexplore.ieee.org/document/8662590 (accessed on 21 October 2023). [CrossRef] 72. Bai, B.; Shu, H.; Wang, X.; Zou, W. Towards Silicon Photonic Neural Networks for Artificial Intelligence. Sci. China Inf. Sci. 2020, 63, 1\u201314. Available online: https://link.springer.com/article/10.1007/s11432-020-2872-3 (accessed on 24 October 2023). [CrossRef] 73. Sacher, W.D.; Poon, J.K. Dynamics of Microring Resonator Modulators. Opt. Express 2008, 16, 15741\u201315753. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-16-20-15741&id=172148 (accessed on 24 October 2023). [CrossRef] [PubMed] 74. Hassanien, A.E.; Ghoname, A.O.; Chow, E.; Goddard, L.L.; Gong, S. Compact MZI Modulators on Thin Film Z-Cut Lithium Niobate. Opt. Express 2022, 30, 4543\u20134552. Available online: https://opg.optica.org/oe/fulltext.cfm?uri=oe-30-3-4543&id=468841 (accessed on 24 October 2023). [CrossRef] 75. Hughes, T.W.; Minkov, M.; Shi, Y.; Fan, S. Training of Photonic Neural Networks through in Situ Backpropagation and Gradient Measurement. Optica 2018, 5, 864\u2013871. Available online: https://opg.optica.org/optica/fulltext.cfm?..",
            "url": "https://openalex.org/W4389817732",
            "title": "Neuromorphic Photonics Circuits: Contemporary Review",
            "publication_date": "2023-12-14"
          }
        ]
      },
      "S8508160691": {
        "id": "S8508160691",
        "text": "The development of programmable photonic circuits, which can self-configure and self-correct, represents a transformative trend in silicon photonics, enabling more versatile and efficient circuit designs.",
        "children": [
          {
            "id": "E6462589905",
            "text": "January 8, 2018 Abstract Silicon Photonics technology is rapidly maturing as a platform for larger-scale photonic circuits. As a result, the associated design methodologies are also evolving from componentoriented design to a more circuit-oriented design flow, that makes abstraction from the very detailed geometry and enables design on a larger scale. In this paper, we review the state of this emerging photonic circuit design flow and its synergies with electronic design automation (EDA). We cover the design flow from schematic capture, circuit simulation, layout and verification. We discuss the similarities and the differences between photonic and electronic design, and the challenges and opportunities that present themselves in the new photonic design landscape, such as variability analysis, photonic-electronic co-simulation and compact model definition. Silicon Photonics Circuit Design: Methods, Tools and Challenges Wim Bogaerts1,2,* and Lukas Chrostowski3 1. Introduction Silicon photonics is the technology to integrate a large number of optical functions on a chip using the fabrication technology of the CMOS industry, thereby enabling low cost, large volume, manufacturing [1\u20133]. The field has rapidly evolved from a \u2018scientific hot topic\u2019 to an industrially viable platform, largely driven by telecom and datacom applications, and enabled by the growing number of manufacturing and prototyping facilities (\u2018fabs\u2019) [4]. Today there coexist a wide diversity of technology platforms to build photonic integrated circuits (PIC) [5], using different material systems such as III-V semiconductors, Lithium Niobate, high-index glasses and nitrides, polymers, and of course silicon. What makes silicon photonics a unique technology is exactly its compatibility with the manufacturing processes and tools used in the CMOS industry: this offers a route towards high volume manufacturing at potentially low cost per device. The second unique feature of silicon photonics is its high refractive index contrast, which allows for sub-micrometer waveguide dimensions, tight bends and close spacing, and in turn, this allows for dense packing of optical functions on the surface of a chip [6]. This combination makes silicon photonics the only viable technology platform for high complexity, large-scale photonic integrated circuits. However, the high refractive index contrast comes with a weakness: it imposes very stringent requirements on the dimensions of the silicon photonic circuits, as nanometer-scale variations in waveguide core width or thickness can have non-negligible effects on the performance of the photonic circuits [7]. This implies that variability introduced by the fabrication process can have a significant impact on the overall performance of a circuit. Large complex circuits will automatically suffer more from variability than simple circuits. In the end, it is the overall yield of a circuit that determines whether it is commercially viable. As CMOS manufacturing technologies continue to advance, higher precision lithography is required to fabricate ever smaller devices. While transistors continue shrinking in size, photonic devices are fundamentally limited and remain approximately constant as a function of the technology node (e.g., a ring modulator has a fixed size determined by the design target free spectral range); the benefit of improved manufacturing is that it reduces the manufacturing variability and improves yield [8]. Still, silicon photonics processes are now considered to be sufficiently good for a number of applications, as is demonstrated by products released on the market. The various fabs provide processes for silicon waveguides with acceptable propagation losses around 1-2 dB/cm [9], thermal tuners with phase shifter efficiencies ranging from 100 \u03bcW/\u03c0 to 100 mW \u03c0 [10], carrier-based electro-optic modulators working in both travelling wave and resonant modes [11, 12], and Germanium photodetectors with effi1 Ghent University - IMEC, Photonics Research Group, Department of Information Technology, Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 2 Center for Nano and Biophotonics (NB-Photonics), Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 3 University of British Columbia, Department of Electrical and Computer Engineering, 2332 Main Mall, Vancouver, British Columbia, V6T 0A7, Canada * Corresponding author: e-mail: wim.bogaerts@ugent.be Copyright line will be provided by the publisher 2 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design ciencies of \u2248 1A/W [13\u201315], with both modulators and detectors operating at high-speeds of many tens of gigahertz. Spectral filters can be implemented using combinations of waveguides and coupling structures [16\u201319]. Only the integration of the laser source, optical amplifier, and optical isolator is somewhat lagging, but solutions are becoming available based either on external sources [20] or heterogeneous integration [21\u201324]. While the majority of silicon photonics technologies operate around wavelengths in the traditional telecommunication bands between 1.2 - 1.6 \u03bcm, the wavelength range can be extended to the visible domain using silicon nitride [25]. SOI wafers (silicon on insulator, with silicon as the waveguide core and silicon dioxide as the cladding) can be used up to 3.6 \u03bcm (limited by silicon dioxide absorption), and even longer wavelengths in the mid-infrared can be accessed using germanium waveguides on a silicon substrate [26, 27]. These technologies don\u2019t lose their compatibility with CMOS manufacturing technologies and dense integration, and therefore fall under the same definition of silicon photonics used at the start of this article. Even when silicon photonics enables high complexity and large circuits, today\u2019s circuit demonstrations are generally quite small and/or simple. For datacom applications, optical transceivers usually consist of a single light path between 3-10 optical elements. Larger optical circuits usually consist of simple repetitive scaling, such as switch matrices [28, 29] or phased arrays for beam steering [30]. While these circuits demonstrate the integration potential of silicon photonics, they are not very complex, and their functionality is limited. Other applications may leverage the potential of added complexity in photonic circuits. Silicon photonics is seen as an enabling technology for biosensing and diagnostics [31\u201333], spectroscopy [25], structural monitoring [34, 35], quantum information / quantum computing [36\u201338], microwave photonics [39\u201342], and can be applied for various sensor functions (accelerometers, gyroscopes, magnetic fields), etc. Such applications will require custom chip designs with very different requirements than transceivers for datacenter and telecom applications. Fabrication processes for silicon photonics have become good enough to make large, complex circuits, with waveguide losses smaller than 1dB/cm, low-loss crossings, splitters, couplers, as well as good modulators and excellent photodetectors, all integrated into technology platforms that are subject to statistical process control (SPC) [43\u201345]. Even though there is still ample headroom for technological improvements, the complexity of the optical circuits is now largely limited by the capability to design them, while taking into account the limitations of the fabrication process such as variability and parasitics. A reliable design flow, transforming a circuit concept into a working chip, should accurately predict the yield of a complex circuit. Today, many photonic circuit designers employ manual techniques to compose their photonic circuits, with a focus on the physical geometry. This is reminiscent of the first electronic circuit design in the 1960s and early 1970s. Photonic integrated circuits share many characteristics of electronic integrated circuits. They are defined by planar processes on semiconductor wafers. The functionality can be described and modelled as a circuit, with signals propagating between the functional building blocks. As with electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          },
          {
            "id": "E2210057765",
            "text": "..electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic circuit design. While today there exist several powerful circuit simulation tools for photonics, they all have their own compact model implementation. There is no common definition of the models for even the simplest components (e.g., Copyright line will be provided by the publisher 3 waveguide, directional coupler), and the implementation of models in each tool is very different. This raises a significant barrier for fabs to invest in a compact model library for their PDK. A standard model implementation language (like Verilog-A for analog electronics [50]), or even an agreement on standard model definitions for the most common building blocks (like the BSIM transistor models [51]) would present a strong incentive to invest in compact model libraries for circuit-driven photonic design. Without reliable models, the added value of a photonic circuit design flow as in electronics is limited. Still, the parallels between electronic and photonic design automation are driving a convergence in design flows, as design tools for photonic circuits are now being coupled to established electronic design tools [49, 52\u201355]. This convergence is driven by necessity, and among all the different PIC technologies, this necessity is most acute in silicon photonics, because silicon photonics is both the most sensitive and most scalable of PIC technologies. First, silicon photonic circuits need electronic interfaces such as for the processing of high-speed signals and for electronic control loops that govern and stabilize the behavior of the photonic circuit. Second, electronics is also looking in the direction of silicon photonics to solve the interconnect bottlenecks [2, 56]. Photonic-electronic co-integration and codesign will make it possible to create integrated photonicelectronic-software systems with control and monitoring. These can compensate the process variability and enable larger, more complex circuits, and create opportunities to implement functionality that cannot be achieved with photonics or electronics separately. Photonics is in many ways very different from electronics, and these differences are also reflected in the design flows. Photonic layouts are usually not based on rectangular patterns, and this can create difficulties for design verification, and control of pattern density. Photonic signals are also different from electrical signals, and cannot be expressed as voltages and currents. Rather, the signal propagation bears a stronger resemblance to radio-frequency (RF) signals. True electronic-photonic co-design will therefore require a new mixed-signal model for co-simulation. In this paper, we present a review of the landscape of silicon photonics design methodologies, from the perspective of the circuit designer (as opposed to the device/component designer). First, we give a brief introduction about what constitutes a circuit design flow in section 2. In section 3 we start with an analysis of today\u2019s historically grown design processes, which are an evolution of component/device design. The requirements for component design, with a focus on geometrical optimization, are very different from those of circuit design, where circuit functionality is governed by the connectivity of functional building blocks. Section 4 then discusses the emerging trend towards an EDA-like design flow, with a focus on a schematic-based circuit design. Design tools are evolving at a rapid pace in this domain, but the necessary shift in mindset in the actual design community is experiencing some inertia, especially where designers have built custom tools for their specific needs, and where foundries do not yet supply PDKs compatible with schematic-driven design. In section 5 we discuss a number of significant challenges that will need to be addressed in the near future to give photonics circuit designers similar firsttime-right capabilities as electronics designers have today. Finally, section 6 presents a number of opportunities for the research community and the important actors in photonic design automation (PDA) to provide a dramatic boost to the photonic design community. 2. Design Flows The purpose of a design flow is to translate a functional idea into a working chip (i.e., the design), using a reproducible method (the flow). The final objective, i.e., a working chip, is important. While the design of simple photonic components can be done intuitively, a reproducible flow, backed up by efficient software tools, is important to guarantee that more complex chips and circuits are fabricated with sufficient yield. When implementing functionality on a photonic chip, the first step is to articulate the needed functionality. This system-level consideration is usually expressed as a relation between inputs and outputs: what behaviour or output signal is expected for a given input signal? From this abstract level, this functionality should be translated into a gradually more refined description (a circuit) until it can be implemented as a photonic integrated circuit (PIC). In a PIC, light is manipulated on the surface of a chip. At the basic level, this manipulation is done by the geometric distribution of material (or by locally changing material properties). At this detailed level, the exact behavior of the electromagnetic waves in the structure can be engineered. However, when the dimensions of the circuit become larger, this level of detail can no longer be captured efficiently, and a more abstract design approach is needed. The different levels of abstraction in a circuit design flow are illustrated in Fig. 1. We can roughly break down the design flow into the following steps: \u2013 Design Capture: the functional idea is converted into a logical circuit of functional building blocks or hierarchical subcircuits. There can be an exploration of different circuit architectures or topologies, with different choices of building blocks. \u2013 Circuit simulation: The logical circuit is simulated and its parameters are optimized so it will perform as intended. This can also include a yield analysis by introducing variability in the circuit parameters. \u2013 Circuit Layout: The logical circuit is converted into a mask layout representation that can be used for fabrication. This results eventually in a large number of polygons on different mask layers. \u2013 Global Chip Design: The logical circuits put together, and connected to a power supply distribution network, electrical I/Os, and generation of dummy tiling patterns to maintain uniform pattern density. \u2013 Verification: The layout is checked against errors, making sure it is compatible with the fabrication process and Copyright line will be provided by the publisher 4 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design Figure 1 Different levels of abstraction in a circuit design flow. The horizontal axis indicates the sequence of design steps, while the vertical axis indicates the level of abstraction. In a circuit design flow, the physical modelling of components is preferably avoided, and circuit simulations are based on compact models. post-layout simulations are performed to ensure that the layout will perform the intended function. \u2013 Tape-out and fabrication: The layout file undergoes a number of post-processing steps to convert it into the actual write patterns, and the chip is fabricated. \u2013 Testing and Packaging: The fabricated chip is packaged and tested, and the results are compared with the original design. If needed, the design information will be updated to improve the next generation of designs. Clearly identifying and separating these steps and levels of abstraction in the design is essential to the scaling of circuits. This is a lesson that has been learned in electronics [57]. Electronic circuits are not designed at the geometry of the individual transistors. Rather, known transistor devices, or known subcircuits consisting of many transistors, diodes and other electrical elements, are reused to compose larger circuits. The circuit designers trust that the building blocks have been properly designed and qualified by the fabs and device designers, and that the relevant geometries and models are supplied in a process design kit (PDK) and external libraries. A process design kit (PDK), in general, is an information package that contains sufficient information for a designer to create a chip design that can be fabricated..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          }
        ]
      },
      "S1618801126": {
        "id": "S1618801126",
        "text": "The establishment of standardized circuit models and design kits for silicon photonics is an emerging trend that will streamline the design process and improve collaboration between designers and fabrication facilities.",
        "children": [
          {
            "id": "E6462589905",
            "text": "January 8, 2018 Abstract Silicon Photonics technology is rapidly maturing as a platform for larger-scale photonic circuits. As a result, the associated design methodologies are also evolving from componentoriented design to a more circuit-oriented design flow, that makes abstraction from the very detailed geometry and enables design on a larger scale. In this paper, we review the state of this emerging photonic circuit design flow and its synergies with electronic design automation (EDA). We cover the design flow from schematic capture, circuit simulation, layout and verification. We discuss the similarities and the differences between photonic and electronic design, and the challenges and opportunities that present themselves in the new photonic design landscape, such as variability analysis, photonic-electronic co-simulation and compact model definition. Silicon Photonics Circuit Design: Methods, Tools and Challenges Wim Bogaerts1,2,* and Lukas Chrostowski3 1. Introduction Silicon photonics is the technology to integrate a large number of optical functions on a chip using the fabrication technology of the CMOS industry, thereby enabling low cost, large volume, manufacturing [1\u20133]. The field has rapidly evolved from a \u2018scientific hot topic\u2019 to an industrially viable platform, largely driven by telecom and datacom applications, and enabled by the growing number of manufacturing and prototyping facilities (\u2018fabs\u2019) [4]. Today there coexist a wide diversity of technology platforms to build photonic integrated circuits (PIC) [5], using different material systems such as III-V semiconductors, Lithium Niobate, high-index glasses and nitrides, polymers, and of course silicon. What makes silicon photonics a unique technology is exactly its compatibility with the manufacturing processes and tools used in the CMOS industry: this offers a route towards high volume manufacturing at potentially low cost per device. The second unique feature of silicon photonics is its high refractive index contrast, which allows for sub-micrometer waveguide dimensions, tight bends and close spacing, and in turn, this allows for dense packing of optical functions on the surface of a chip [6]. This combination makes silicon photonics the only viable technology platform for high complexity, large-scale photonic integrated circuits. However, the high refractive index contrast comes with a weakness: it imposes very stringent requirements on the dimensions of the silicon photonic circuits, as nanometer-scale variations in waveguide core width or thickness can have non-negligible effects on the performance of the photonic circuits [7]. This implies that variability introduced by the fabrication process can have a significant impact on the overall performance of a circuit. Large complex circuits will automatically suffer more from variability than simple circuits. In the end, it is the overall yield of a circuit that determines whether it is commercially viable. As CMOS manufacturing technologies continue to advance, higher precision lithography is required to fabricate ever smaller devices. While transistors continue shrinking in size, photonic devices are fundamentally limited and remain approximately constant as a function of the technology node (e.g., a ring modulator has a fixed size determined by the design target free spectral range); the benefit of improved manufacturing is that it reduces the manufacturing variability and improves yield [8]. Still, silicon photonics processes are now considered to be sufficiently good for a number of applications, as is demonstrated by products released on the market. The various fabs provide processes for silicon waveguides with acceptable propagation losses around 1-2 dB/cm [9], thermal tuners with phase shifter efficiencies ranging from 100 \u03bcW/\u03c0 to 100 mW \u03c0 [10], carrier-based electro-optic modulators working in both travelling wave and resonant modes [11, 12], and Germanium photodetectors with effi1 Ghent University - IMEC, Photonics Research Group, Department of Information Technology, Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 2 Center for Nano and Biophotonics (NB-Photonics), Tech Lane Ghent Science Park - Campus A, 9052 Gent, Belgium 3 University of British Columbia, Department of Electrical and Computer Engineering, 2332 Main Mall, Vancouver, British Columbia, V6T 0A7, Canada * Corresponding author: e-mail: wim.bogaerts@ugent.be Copyright line will be provided by the publisher 2 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design ciencies of \u2248 1A/W [13\u201315], with both modulators and detectors operating at high-speeds of many tens of gigahertz. Spectral filters can be implemented using combinations of waveguides and coupling structures [16\u201319]. Only the integration of the laser source, optical amplifier, and optical isolator is somewhat lagging, but solutions are becoming available based either on external sources [20] or heterogeneous integration [21\u201324]. While the majority of silicon photonics technologies operate around wavelengths in the traditional telecommunication bands between 1.2 - 1.6 \u03bcm, the wavelength range can be extended to the visible domain using silicon nitride [25]. SOI wafers (silicon on insulator, with silicon as the waveguide core and silicon dioxide as the cladding) can be used up to 3.6 \u03bcm (limited by silicon dioxide absorption), and even longer wavelengths in the mid-infrared can be accessed using germanium waveguides on a silicon substrate [26, 27]. These technologies don\u2019t lose their compatibility with CMOS manufacturing technologies and dense integration, and therefore fall under the same definition of silicon photonics used at the start of this article. Even when silicon photonics enables high complexity and large circuits, today\u2019s circuit demonstrations are generally quite small and/or simple. For datacom applications, optical transceivers usually consist of a single light path between 3-10 optical elements. Larger optical circuits usually consist of simple repetitive scaling, such as switch matrices [28, 29] or phased arrays for beam steering [30]. While these circuits demonstrate the integration potential of silicon photonics, they are not very complex, and their functionality is limited. Other applications may leverage the potential of added complexity in photonic circuits. Silicon photonics is seen as an enabling technology for biosensing and diagnostics [31\u201333], spectroscopy [25], structural monitoring [34, 35], quantum information / quantum computing [36\u201338], microwave photonics [39\u201342], and can be applied for various sensor functions (accelerometers, gyroscopes, magnetic fields), etc. Such applications will require custom chip designs with very different requirements than transceivers for datacenter and telecom applications. Fabrication processes for silicon photonics have become good enough to make large, complex circuits, with waveguide losses smaller than 1dB/cm, low-loss crossings, splitters, couplers, as well as good modulators and excellent photodetectors, all integrated into technology platforms that are subject to statistical process control (SPC) [43\u201345]. Even though there is still ample headroom for technological improvements, the complexity of the optical circuits is now largely limited by the capability to design them, while taking into account the limitations of the fabrication process such as variability and parasitics. A reliable design flow, transforming a circuit concept into a working chip, should accurately predict the yield of a complex circuit. Today, many photonic circuit designers employ manual techniques to compose their photonic circuits, with a focus on the physical geometry. This is reminiscent of the first electronic circuit design in the 1960s and early 1970s. Photonic integrated circuits share many characteristics of electronic integrated circuits. They are defined by planar processes on semiconductor wafers. The functionality can be described and modelled as a circuit, with signals propagating between the functional building blocks. As with electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          },
          {
            "id": "E2210057765",
            "text": "..electronics, the functionality of a photonic circuit does not come from a single element, but from the connectivity between many functional building blocks and subcircuits. The design of the chips eventually translates into a set of geometric \u2018mask layers\u2019 with the patterns for each planar processing step. The first photonic integrated circuits were defined as a single device, and usually simulated using direct (but approximate) electromagnetic simulation techniques such as beam propagation methods (BPM) [46, 47]. But with the large number of process steps in silicon photonics, as well as the increasing size of the circuits, the PIC design process is evolving along the lines of electronic design automation (EDA), with circuit hierarchy and reusable parametric building blocks as used in analog electronics [48, 49]. In electronics, this has led to a situation where circuit designers can create a first-time-right design for extremely complex integrated circuits with billions of components. The scaling of circuit design has been enabled by a number of factors \u2013 A standardized workflow: most electronic IC design teams follow a similar workflow, separating the logical design from the actual physical implementation. \u2013 Accurate models: Circuit simulation can accurately predict the behavior of a large circuit because the building blocks have been thoroughly characterized and the models are very accurate. Models also contain statistical information on their components\u2019 performance, such as slow and fast corners. \u2013 Design kits and reusable IP blocks: Foundries provide design kits with building blocks that can be directly used by the designer. At a higher level, reusable subcircuits, socalled intellectual property (IP) blocks found in libraries, allow designers to focus on higher-level functionality. \u2013 Automation: Modern EDA tools help the designer to automate increasingly complex tasks, including the synthesis of circuits from high-level specifications. \u2013 Comprehensive verification allows designers to check the final design against the original specifications. Given the same technology foundation, it is no surprise that the silicon photonics ecosystem is evolving along the same lines as electronics, where a small number of foundries (\u2018fabs\u2019) manufacture the chips for a much larger community of designers [4]. In such a \u2018fabless\u2019 model, designers cannot steer fabrication process improvements, so they should have sufficient information about the process and qualified building blocks to reliably design circuits. For this, fabs supply process design kits (PDK) with details about the fabrication process and with building blocks that contain both the geometric layout, and in some cases behavioral models. It is with these behavioral models (also called compact models) that we identify some of the key limitations for photonic circuit design. While today there exist several powerful circuit simulation tools for photonics, they all have their own compact model implementation. There is no common definition of the models for even the simplest components (e.g., Copyright line will be provided by the publisher 3 waveguide, directional coupler), and the implementation of models in each tool is very different. This raises a significant barrier for fabs to invest in a compact model library for their PDK. A standard model implementation language (like Verilog-A for analog electronics [50]), or even an agreement on standard model definitions for the most common building blocks (like the BSIM transistor models [51]) would present a strong incentive to invest in compact model libraries for circuit-driven photonic design. Without reliable models, the added value of a photonic circuit design flow as in electronics is limited. Still, the parallels between electronic and photonic design automation are driving a convergence in design flows, as design tools for photonic circuits are now being coupled to established electronic design tools [49, 52\u201355]. This convergence is driven by necessity, and among all the different PIC technologies, this necessity is most acute in silicon photonics, because silicon photonics is both the most sensitive and most scalable of PIC technologies. First, silicon photonic circuits need electronic interfaces such as for the processing of high-speed signals and for electronic control loops that govern and stabilize the behavior of the photonic circuit. Second, electronics is also looking in the direction of silicon photonics to solve the interconnect bottlenecks [2, 56]. Photonic-electronic co-integration and codesign will make it possible to create integrated photonicelectronic-software systems with control and monitoring. These can compensate the process variability and enable larger, more complex circuits, and create opportunities to implement functionality that cannot be achieved with photonics or electronics separately. Photonics is in many ways very different from electronics, and these differences are also reflected in the design flows. Photonic layouts are usually not based on rectangular patterns, and this can create difficulties for design verification, and control of pattern density. Photonic signals are also different from electrical signals, and cannot be expressed as voltages and currents. Rather, the signal propagation bears a stronger resemblance to radio-frequency (RF) signals. True electronic-photonic co-design will therefore require a new mixed-signal model for co-simulation. In this paper, we present a review of the landscape of silicon photonics design methodologies, from the perspective of the circuit designer (as opposed to the device/component designer). First, we give a brief introduction about what constitutes a circuit design flow in section 2. In section 3 we start with an analysis of today\u2019s historically grown design processes, which are an evolution of component/device design. The requirements for component design, with a focus on geometrical optimization, are very different from those of circuit design, where circuit functionality is governed by the connectivity of functional building blocks. Section 4 then discusses the emerging trend towards an EDA-like design flow, with a focus on a schematic-based circuit design. Design tools are evolving at a rapid pace in this domain, but the necessary shift in mindset in the actual design community is experiencing some inertia, especially where designers have built custom tools for their specific needs, and where foundries do not yet supply PDKs compatible with schematic-driven design. In section 5 we discuss a number of significant challenges that will need to be addressed in the near future to give photonics circuit designers similar firsttime-right capabilities as electronics designers have today. Finally, section 6 presents a number of opportunities for the research community and the important actors in photonic design automation (PDA) to provide a dramatic boost to the photonic design community. 2. Design Flows The purpose of a design flow is to translate a functional idea into a working chip (i.e., the design), using a reproducible method (the flow). The final objective, i.e., a working chip, is important. While the design of simple photonic components can be done intuitively, a reproducible flow, backed up by efficient software tools, is important to guarantee that more complex chips and circuits are fabricated with sufficient yield. When implementing functionality on a photonic chip, the first step is to articulate the needed functionality. This system-level consideration is usually expressed as a relation between inputs and outputs: what behaviour or output signal is expected for a given input signal? From this abstract level, this functionality should be translated into a gradually more refined description (a circuit) until it can be implemented as a photonic integrated circuit (PIC). In a PIC, light is manipulated on the surface of a chip. At the basic level, this manipulation is done by the geometric distribution of material (or by locally changing material properties). At this detailed level, the exact behavior of the electromagnetic waves in the structure can be engineered. However, when the dimensions of the circuit become larger, this level of detail can no longer be captured efficiently, and a more abstract design approach is needed. The different levels of abstraction in a circuit design flow are illustrated in Fig. 1. We can roughly break down the design flow into the following steps: \u2013 Design Capture: the functional idea is converted into a logical circuit of functional building blocks or hierarchical subcircuits. There can be an exploration of different circuit architectures or topologies, with different choices of building blocks. \u2013 Circuit simulation: The logical circuit is simulated and its parameters are optimized so it will perform as intended. This can also include a yield analysis by introducing variability in the circuit parameters. \u2013 Circuit Layout: The logical circuit is converted into a mask layout representation that can be used for fabrication. This results eventually in a large number of polygons on different mask layers. \u2013 Global Chip Design: The logical circuits put together, and connected to a power supply distribution network, electrical I/Os, and generation of dummy tiling patterns to maintain uniform pattern density. \u2013 Verification: The layout is checked against errors, making sure it is compatible with the fabrication process and Copyright line will be provided by the publisher 4 W. Bogaerts and L. Chrostowski: Silicon Photonics Circuit Design Figure 1 Different levels of abstraction in a circuit design flow. The horizontal axis indicates the sequence of design steps, while the vertical axis indicates the level of abstraction. In a circuit design flow, the physical modelling of components is preferably avoided, and circuit simulations are based on compact models. post-layout simulations are performed to ensure that the layout will perform the intended function. \u2013 Tape-out and fabrication: The layout file undergoes a number of post-processing steps to convert it into the actual write patterns, and the chip is fabricated. \u2013 Testing and Packaging: The fabricated chip is packaged and tested, and the results are compared with the original design. If needed, the design information will be updated to improve the next generation of designs. Clearly identifying and separating these steps and levels of abstraction in the design is essential to the scaling of circuits. This is a lesson that has been learned in electronics [57]. Electronic circuits are not designed at the geometry of the individual transistors. Rather, known transistor devices, or known subcircuits consisting of many transistors, diodes and other electrical elements, are reused to compose larger circuits. The circuit designers trust that the building blocks have been properly designed and qualified by the fabs and device designers, and that the relevant geometries and models are supplied in a process design kit (PDK) and external libraries. A process design kit (PDK), in general, is an information package that contains sufficient information for a designer to create a chip design that can be fabricated..",
            "url": "https://biblio.ugent.be/publication/8578535/file/8578538.pdf"
          }
        ]
      }
    },
    "evaluation": {
      "scores": {
        "accuracy": {
          "reasoning": "The given answer accurately reflects the historical developments in silicon photonics, mentioning the emergence of photonic integrated circuits (PICs) and their significance in scalability and reconfigurability. It also correctly identifies current challenges such as energy consumption and the limitations of traditional architectures. However, it lacks some specific details and references found in the golden answer, such as the historical timeline of silicon photonics development and specific challenges related to integration and fabrication bottlenecks. Overall, the answer is factually correct but lacks some depth and specificity compared to the golden answer.",
          "score": 7
        },
        "completeness": {
          "reasoning": "The given answer covers several key aspects of the golden answer, including historical developments, current challenges, and emerging technologies. However, it does not fully address the innovative packaging solutions mentioned in the golden answer, which are crucial for enabling large-scale integration. Additionally, while it discusses future trends, it does not provide as comprehensive a view as the golden answer. Therefore, while it is fairly complete, it misses some critical elements that would provide a fuller picture.",
          "score": 6
        },
        "relevance": {
          "reasoning": "The given answer is relevant to the complex question, addressing historical developments, current challenges, and emerging technologies in silicon photonics. It discusses the integration of photonic interconnects and their impact on energy efficiency, which aligns well with the question's focus. However, it could improve by explicitly linking its points back to the specific challenges and solutions outlined in the golden answer, particularly regarding innovative packaging solutions. Overall, it is relevant but could be more tightly aligned with the question's specifics.",
          "score": 8
        },
        "evidence_quality": {
          "reasoning": "The given answer cites several sources to support its claims, which adds credibility. However, the sources are not as authoritative or diverse as those in the golden answer, which references foundational works and a broader range of studies. The golden answer's references are more established in the field of silicon photonics, while the given answer's references seem to be more recent and less widely recognized. This affects the overall reliability of the evidence presented in the given answer.",
          "score": 6
        },
        "clarity": {
          "reasoning": "The given answer is generally clear and well-structured, making it easy to follow. It uses straightforward language and presents its points logically. However, some sections could benefit from more explicit connections between ideas, particularly when transitioning between historical developments and current challenges. Overall, it is clear but could enhance clarity by improving transitions and connections between points.",
          "score": 7
        },
        "logical_structure": {
          "reasoning": "The given answer is organized into distinct sections that address historical developments, current challenges, and emerging technologies. This structure aids in understanding and allows for a logical flow of information. However, the transitions between sections could be smoother, and some points could be better integrated to enhance coherence. Overall, it is logically structured but could benefit from improved transitions and integration of ideas.",
          "score": 7
        },
        "evidence_support": {
          "reasoning": "The given answer uses evidence to support its claims, referencing various studies and advancements in silicon photonics. However, the depth of evidence support is not as strong as in the golden answer, which provides a more comprehensive range of references that substantiate its claims. The given answer could improve by incorporating more diverse and authoritative sources to strengthen its arguments.",
          "score": 6
        },
        "depth_of_analysis": {
          "reasoning": "The given answer provides a reasonable depth of analysis regarding the historical context and current challenges in silicon photonics. It discusses the integration of photonic interconnects and their implications for energy efficiency. However, it lacks the depth found in the golden answer, particularly in exploring the nuances of the challenges and the potential solutions offered by emerging technologies and innovative packaging. Therefore, while it is adequate, it does not reach the depth of analysis present in the golden answer.",
          "score": 6
        },
        "objectivity": {
          "reasoning": "The given answer maintains an objective tone, presenting information without apparent bias. It discusses both the advancements and challenges in silicon photonics fairly. However, it could enhance objectivity by acknowledging potential limitations or criticisms of the technologies discussed, which would provide a more balanced perspective. Overall, it is objective but could benefit from a more critical examination of the subject matter.",
          "score": 7
        },
        "synthesis": {
          "reasoning": "The given answer synthesizes information from various sources to present a coherent view of the developments in silicon photonics. However, it does not integrate these viewpoints as effectively as the golden answer, which weaves together historical context, current challenges, and future trends more seamlessly. The synthesis in the given answer is adequate but lacks the depth and integration seen in the golden answer, which connects various aspects of the topic more fluidly.",
          "score": 6
        }
      }
    },
    "average_score": 6.6
  }
]