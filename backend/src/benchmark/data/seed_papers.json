[
  {
    "id": "https://openalex.org/W4392203343",
    "text": "A Comprehensive Survey on Deep Graph Representation\nLearning\nWEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University,\nChina\nZIYUE QIAO, The Hong Kong University of Science and Technology, China\nYIFANG QIN and JIANHAO SHEN, Peking University, China\nFANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA\nJUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China\nYIFAN WANG, University of International Business and Economics, China\nXIAO LUO\u2217, University of California, Los Angeles, USA\nMING ZHANG\u2217, Peking University, China\nGraph representation learning aims to effectively encode high-dimensional sparse graph-structured data into\nlow-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields,\nincluding machine learning and data mining. Classic graph embedding methods follow the basic idea that the\nembedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby\npreserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i)\ntraditional methods have limited model capacity which limits the learning performance; (ii) existing techniques\ntypically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii)\nrepresentation learning and downstream tasks are dependent on each other which should be jointly enhanced.\nWith the remarkable success of deep learning, deep graph representation learning has shown great potential\nand advantages over shallow (traditional) methods, there exist a large number of deep graph representation\nlearning techniques have been proposed in the past decade, especially graph neural networks. In this survey,\nwe conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a\nnew taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential\ncomponents of graph representation learning and categorize existing approaches by the ways of graph neural\nnetwork architectures and the most recent advanced learning paradigms. Moreover, this survey also provides\nthe practical and promising applications of deep graph representation learning. Last but not least, we state\nnew perspectives and suggest challenging directions which deserve further investigations in the future.\nCCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations.\n\u2217Corresponding authors.\nAuthors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn;\nZequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871;\nZiyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453;\nYifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang\nSun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei\nYang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking\nUniversity, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics,\nBeijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang,\nmzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0004-5411/2024/2-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\narXiv:2304.05055v3 [cs.LG] 28 Feb 2024\n2 W. Ju, et al.\nAdditional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural\nNetwork, Survey\nACM Reference Format:\nWei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun,\nZhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024.\nA Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nGraphs have recently emerged as a powerful tool for representing a variety of structured and\ncomplex data, including social networks, traffic networks, information systems, knowledge graphs,\nprotein-protein interaction networks, and physical interaction networks. As a kind of general form\nof data organization, graph structures are capable of naturally expressing the intrinsic relationship\nof these data, and thus can characterize plenty of non-Euclidean structures that are crucial in\na variety of disciplines and domains due to their flexible adaptability. For example, to encode a\nsocial network as a graph, nodes on the graph are used to represent individual users, and edges are\nused to represent the relationship between two individuals, such as friends. In the field of biology,\nnodes can be used to represent proteins, and edges can be used to represent biological interactions\nbetween various proteins, such as the dynamic interactions between proteins. Thus, by analyzing\nand mining the graph-structured data, we can understand the deep meaning hidden behind the\ndata, and further discover valuable knowledge, so as to benefit society and human beings.\nIn the last decade years, a wide range of machine learning algorithms have been developed for\ngraph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410]\nusually break down graphs into different atomic substructures and then use kernel functions\nto measure the similarity between all pairs of them. Although graph kernels could provide a\nperspective on modeling graph topology, these approaches often generate substructures or feature\nrepresentations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer\nfrom high computational complexity, and therefore have weak scalability and subpar performance.\nIn the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have ever\u0002increasing emerged, which attempt to encode the structural information of the graph (usually a\nhigh-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to\npreserve the topology information and attribute information in the embedding space as much\nas possible, so that the learned graph embeddings can be naturally integrated into traditional\nmachine learning algorithms. Compared to previous works which use feature engineering in the\npre-processing phase to extract graph structural features, current graph embedding algorithms are\nconducted in a data-driven way leveraging machine learning algorithms (such as neural networks)\nto encode the structural information of the graph. Specifically, existing graph embedding methods\ncan be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354]\nthat factorize the matrix to learn node embedding which preserves the graph property; (ii) deep\nlearning based methods [155, 362, 443, 460] that apply deep learning techniques specifically de\u0002signed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either\nmaximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these\nmethods typically depend on shallow architectures, and fail to exploit the potential and capacity of\ndeep neural networks, resulting in sub-optimal representation quality and learning performance.\nInspired by the recent remarkable success of deep neural networks, a range of deep learning\nalgorithms has been developed for graph-structured data learning. The core of these methods is to\ngenerate effective node and graph representations using graph neural networks (GNNs), followed\nby a goal-oriented learning paradigm. In this way, the derived representations can be adaptively\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 3\ncoupled with a variety of downstream tasks and applications. Following this line of thought, in\nthis paper, we propose a new taxonomy to classify the existing graph representation learning\nalgorithms, i.e., graph neural network architectures, learning paradigms, and various promising\napplications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the\nstudies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer.\nFor the learning paradigms, we explore three advanced types namely supervised/semi-supervised\nlearning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate\nthe effectiveness of the learned graph representations, we provide several promising applications\nto build tight connections between representation learning and downstream tasks, such as social\nanalysis, molecular property prediction and generation, recommender systems, and traffic analysis.\nLast but not least, we present some perspectives for thought and suggest challenging directions\nthat deserve further study in the future.\nDifferences between this survey and existing ones. Up to now, there exist some other overview\npapers focusing on different perspectives of graph representation learning[17, 50, 53, 57, 227, 499,\n502, 577, 601, 603] that are closely related to ours. However, there are very few comprehensive\nreviews have summarized deep graph representation learning simultaneously from the perspective\nof diverse GNN architectures and corresponding up-to-date learning paradigms. Therefore, we\nhere clearly state their distinctions from our survey as follows. There have been several surveys\non classic graph embedding[42, 151], these works categorize graph embedding methods based on\ndifferent training objectives. Wang et al. [468] goes further and provides a comprehensive review of\nexisting heterogeneous graph embedding approaches. With the rapid development of deep learning,\nthere are a handful of surveys along this line. For example, Wu et al. [499] and Zhang et al. [577]\nmainly focus on several classical and representative GNN architectures without exploring deep\ngraph representation learning from a view of the most recent advanced learning paradigms such as\ngraph self-supervised learning and graph structure learning. Xia et al. [502] and Chami et al. [50]\njointly summarize the studies of graph embeddings and GNNs. Zhou et al. [601] explores different\ntypes of computational modules for GNNs. One recent survey under review [227] categorizes the\nexisting works in graph representation learning from both static and dynamic graphs. However,\nthese taxonomies emphasize the basic GNN methods but pay insufficient attention to the learning\nparadigms, and provide few discussions of the most promising applications, such as recommender\nsystems as well as molecular property prediction and generation. To the best of our knowledge, the\nmost relevant survey published formally is [603], which presents a review of GNN architectures\nand roughly discusses the corresponding applications. Nevertheless, this survey merely covers\nmethods up to the year of 2020, missing the latest developments in the past three years.\nTherefore, it is highly desired to summarize the representative GNN methods, the most recent\nadvanced learning paradigms, and promising applications into one unified and comprehensive\nframework. Moreover, we strongly believe this survey with a new taxonomy of literature and more\nthan 600 studies will strengthen future research on deep graph representation learning.\nContribution of this survey. The goal of this survey is to systematically review the literature\non the advances of deep graph representation learning and discuss further directions. It aims\nto help the researchers and practitioners who are interested in this area, and support them in\nunderstanding the panorama and the latest developments of deep graph representation learning.\nThe key contributions of this survey are summarized as follows:\n\u2022 Systematic Taxonomy. We propose a systematic taxonomy to organize the existing deep\ngraph representation learning approaches based on the ways of GNN architectures and the\nmost recent advanced learning paradigms via providing some representative branches of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n4 W. Ju, et al.\nGraph Self-Supervised Learning\nSemi-Supervised Learning on Graphs\nGraph Structure Learning\nLearning Paradigms\nGraph-Related Applications\nMolecular Generation\nMolecular Property Prediction\nSocial Analysis\nRecommender Systems\nTra c Analysis\nFuture Directions\nGraph Neural Network Architectures\nGraph Kernel Neural Networks\nGraph Pooling\nGraph Convolutions\nGraph Transformer\nGraph Representations\nGraph Data\nOptimized Graph Representations\nFig. 1. The architecture of this paper.\nmethods. Moreover, several promising applications are presented to illustrate the superiority\nand potential of graph representation learning.\n\u2022 Comprehensive Review. For each branch of this survey, we review the essential components\nand provide detailed descriptions of representative algorithms, and systematically summarize\nthe characteristics to make the overview comparison.\n\u2022 Future Directions. Based on the properties of existing deep graph representation learning\nalgorithms, we discuss the limitations and challenges of current methods and propose the\npotential as well as promising research directions deserving of future investigations.\n2 Background\nIn this section, we first briefly introduce some definitions in deep graph representation learning that\nneed to be clarified, and then we explain the reasons why we need graph representation learning.\n2.1 Problem Definition\nDefinition: Graph. Given a graph \ud835\udc3a = (\ud835\udc49 , \ud835\udc38, X), where \ud835\udc49 = {\ud835\udc631, \u00b7 \u00b7 \u00b7 , \ud835\udc63|\ud835\udc49 | } is the set of nodes,\n\ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52|\ud835\udc49 | } is the set of edges, and the edge \ud835\udc52 = (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38 represent the connection\nrelationship between nodes \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57in the graph. X \u2208 R\n|\ud835\udc49 |\u00d7\ud835\udc40 is the node feature matrix with\n\ud835\udc40 being the dimension of each node feature. The adjacency matrix of a graph can be defined as\nA \u2208 R\n|\ud835\udc49 |\u00d7 |\ud835\udc49 |\n, where A\ud835\udc56\ud835\udc57 = 1 if (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38, otherwise A\ud835\udc56\ud835\udc57 = 0.\nThe adjacency matrix can be regarded as the structural representation of the graph-structured\ndata, in which each row of the adjacency matrix A represents the connection relationship between\nthe corresponding node of the row and all other nodes, which can be regarded as a discrete repre\u0002sentation of the node. However, in real-life circumstances, the adjacency matrix A corresponding\nto \ud835\udc3a is a highly sparse matrix, and if A is used directly as node representations, it will be seriously\naffected by impractical storage demands and computational overhead. The storage space of the\nadjacency matrix A is |\ud835\udc49 |\u00d7 |\ud835\udc49 |, which is usually unacceptable when the total number of nodes grows\nto the order of millions. At the same time, the value of most dimensions in the node representation\nis 0. The sparsity will make subsequent machine learning tasks very difficult.\nGraph representation learning is a bridge between the original input data and the task objectives\nin the graph. The fundamental idea of the graph representation learning algorithm is first to learn\nthe embedded representations of nodes or the entire graph from the input graph structure data and\nthen apply these embedded representations to downstream related tasks, such as node classification,\ngraph classification, link prediction, community detection, and visualization, etc. Specifically, it\naims to learn low-dimensional, dense distributed embedding representations for nodes in the graph.\nFormally, the goal of graph representation learning is to learn its embedding vector representation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 5\nTable 1. Summary of traditional graph embedding methods.\nType Method Similarity measure Loss function (\ud835\udc3f)\nMatrix Factorization\nLLE [390] general |\ud835\udc67\ud835\udc56 \u2212\n\u00cd\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56 \ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57\n|\n2\nLE [11] general \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d,s.t.\ud835\udc4d\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c\nGF [4] \ud835\udc34\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nGraRep [46] \ud835\udc34\ud835\udc56,\ud835\udc57, \ud835\udc342\n\ud835\udc56,\ud835\udc57, ..., \ud835\udc34\ud835\udc58\n\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56\n, \ud835\udc67\ud835\udc57\u27e9|2\nHOPE [354] general |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nRandom Walk\nDeepWalk [362] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nNode2vec [155] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nHARP [59] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nLINE [443] Two-order Similarities Corresponding Loss\nNon-GNN Deep SDNE [460] Two-order Proximities Corresponding Loss\nDNGR [47] Two-order Proximities Corresponding Loss\n\ud835\udc45\ud835\udc63 \u2208 R\n\ud835\udc51\nfor each node \ud835\udc63 \u2208 \ud835\udc49 , where the dimension \ud835\udc51 of the vector is much smaller than the total\nnumber of nodes |\ud835\udc49 | in the graph.\n2.2 Traditional Graph Embedding\nTraditional graph embedding learning methods, as part of dimensionality reduction techniques,\naimed to embed graph data into a lower-dimensional vector space with the idea that connected nodes\nin the graph should still be closer to each other in this lower-dimensional space, thereby preserving\nthe structural information between nodes in the graph. Influenced by classical dimensionality\nreduction techniques, early graph embedding methods are primarily inspired by classic matrix\nfactorization techniques [25] and multi-dimensional scaling [245]. The following three sections\ndescribe these methods in more detail, distinguishing among matrix factorization-based methods,\nrandom walks-based methods and other non-GNN deep methods. In Table 1, we summarize different\ncategories of traditional graph embedding methods.\n2.2.1 Matrix factorization-based methods Matrix factorization-based methods are the early en\u0002deavors in graph embedding learning. These approaches can be outlined in a two-step process.\nIn the initial step, a proximity-based matrix is constructed for the graph, where each element of\nthe matrix represents the proximity measure between two nodes in the graph. Subsequently, a\ndimensionality reduction technique is employed on this matrix in the second step to generate the\nnode embeddings.\nLocally Linear Embedding (LLE) [390]. LLE assumes that node representations are sampled from\nthe same manifold space, and any node in the graph and its neighboring nodes are located in a\nlocal region of that manifold space. Therefore, node representations can be obtained by linearly\ncombining them with their neighboring nodes. LLE first constructs a local reconstruction weight\nmatrix, \ud835\udc4a\ud835\udc56\ud835\udc57 , for nodes in the graph to linearly combine neighboring nodes. By computing the\ndistance between the linear combination and the central node, the problem is reduced to solving\nfor matrix eigenvalues to learn low-dimensional vector representations for nodes. The objective\nfunction is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56 \u2212\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56\n\ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57|\n2\n, (1)\nwhere \ud835\udc67\ud835\udc56 represents the low-dimensional representation of the \ud835\udc56-th node, and \ud835\udc41\ud835\udc56is the set of\nneighboring nodes for the central node \ud835\udc56.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n6 W. Ju, et al.\nLaplacian Eigenmaps (LE) [11]. LE believes that nodes directly connected in graph data should\nbe kept as close as possible in the embedding space. Specifically, it achieves this by defining the\ndistance between connected nodes in the embedding space using the square of the Euclidean\ndistance. It transforms the final optimization objective into the computation of the Laplacian\nmatrix\u2019s eigenvectors. The objective function is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57\n|\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|\n2\ud835\udc4a\ud835\udc56\ud835\udc57 = \ud835\udc4d\ud835\udc47\n\ud835\udc3f\ud835\udc4d, s.t. \ud835\udc4d\n\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c, (2)\nwhere \ud835\udc4a\ud835\udc56\ud835\udc57 represents the connection weight between nodes \ud835\udc56 and \ud835\udc57 in the graph. After linear\ntransformation, the optimization of \ud835\udf19 (\ud835\udc4d) can be reformulated as \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d, where \ud835\udc3f = \ud835\udc37 \u2212\ud835\udc4a is the\nconstructed graph Laplacian matrix, and \ud835\udc37 is a symmetric matrix.\nGraph Factorization (GF) [4]. The matrix eigenvector-based methods mentioned before consider\nthe similarity between nodes throughout the entire graph, which can result in excellent node\nfeature representations. However, with the ever-growing scale of real-world graph data, computing\nmatrix eigenvectors for large graphs can be computationally expensive and memory-intensive.\nGF introduces a graph embedding method with a time complexity of \ud835\udc42(|\ud835\udc38|) by factorizing the\nadjacency matrix of the graph. The objective function is as follows:\n\ud835\udf19 (\ud835\udc4d, \ud835\udf06) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57 \u2208\ud835\udc38\n|\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2+\n\ud835\udf06\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56|\n2\n, (3)\nwhere \ud835\udf06 is a regularization coefficient, and \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9 represents the corresponding inner-product\noperation. Moreover, these inner-product methods also contain GraRep [46] and HOPE [354], which\nconsider higher-order and general node similarity respectively.\n2.2.2 Random walk-based methods Random walk-based methods have also attracted a lot of\nattention in graph embedding learning. The basic idea of these methods is to create random walks\namong nodes in the graph to capture its structural characteristics. Thus, nodes tend to have similar\nembedding if they co-occur on short random walks. Compared to fixed proximity measures in\ntraditional matrix factorization-based methods, these approaches use co-occurrence in a random\nwalk as a measure of node similarity, which is more flexible and has demonstrated promising\nperformance across various applications.\nDeepWalk [362]. DeepWalk analogizes nodes in a graph to words in text. It uses random walks\non the graph to generate numerous node sequences \ud835\udc46 = {\ud835\udc631, . . . , \ud835\udc63|\ud835\udc60 | }, treating these sequences as\nsentences, and then inputting them into the Word2vec [343], which aims to maximize the probability\nof node context given the target node \ud835\udc63\ud835\udc56. It can be written as:\n1\n|\ud835\udc46 |\n\u2211\ufe01\n|\ud835\udc46 |\n\ud835\udc56=1\n\u2211\ufe01\n\u2212\ud835\udc61 \u2264\ud835\udc57\u2264\ud835\udc61,\ud835\udc57\u22600\nlog \ud835\udc5d(\ud835\udc63\ud835\udc56+\ud835\udc57|\ud835\udc63\ud835\udc56), (4)\nwhere \ud835\udc61 is the context window size. Compared to matrix factorization-based methods, DeepWalk\nexhibits extremely low time complexity and is suitable for large-scale graph representation learning.\nHowever, DeepWalk only considers local information between nodes in the graph, making it\nchallenging to find the optimal random walk sampling sequences.\nNode2vec [155]. Based on DeepWalk, Node2vec utilizes parameters \ud835\udc5d and \ud835\udc5e to guide the random\nwalk. Parameter \ud835\udc5d allows the algorithm to revisit previously traversed nodes \ud835\udc61, with smaller\nvalues of \ud835\udc5d increasing the likelihood of returning to \ud835\udc61. Parameter \ud835\udc5e facilitates both inward and\noutward exploration; when \ud835\udc5e > 1, the algorithm tends to visit nodes closer to \ud835\udc61; while for \ud835\udc5e  1. And the first-order similarity can\nbe defined as:\n\ud835\udc3f2 =\n\u2211\ufe01\n(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57 ) \u2208\ud835\udc38\n\ud835\udc34\ud835\udc56\ud835\udc57 |\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|, (8)\nwhere \ud835\udc67\ud835\udc56is the learned representation of node \ud835\udc63\ud835\udc56.\nDeep Neural Graph Representations (DNGR) [47]. Similar to SDNE, DNGR utilizes pointwise\nmutual information between two nodes co-occurring in random walks instead of the adjacency\nmatrix values.\n2.3 Why study deep graph representation learning\nWith the rapid development of deep learning techniques, deep neural networks such as convolu\u0002tional neural networks and recurrent neural networks have made breakthroughs in the fields of\ncomputer vision, natural language processing, and speech recognition. They can well abstract the\nsemantic information of images, natural languages, and speeches. However, current deep learning\ntechniques fail to handle more complex and irregular graph-structured data. To effectively analyze\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n8 W. Ju, et al.\nand model this kind of non-Euclidean structure data, many graph representation learning algo\u0002rithms have emerged in recent years, including graph embedding and graph neural networks. At\npresent, compared with Euclidean-style data such as images, natural language, and speech, graph\u0002structured data is high-dimensional, complex, and irregular. Therefore, the graph representation\nlearning algorithm is a rather powerful tool for studying graph-structured data. To encode complex\ngraph-structured data, deep graph representation learning needs to meet several characteristics: (1)\ntopological properties: Graph representations need to capture the complex topological information\nof the graph, such as the relationship between nodes and nodes, and other substructure information,\nsuch as subgraphs, motif, etc; (2) feature attributes: It is necessary for graph representations to de\u0002scribe high-dimensional attribute features in the graph, including the attributes of nodes and edges\nthemselves; (3) scalability: Because different real graph data have different characteristics, graph\nrepresentation learning algorithms should be able to efficiently learn its embedding representation\non different graph structure data, making it universal and transferable.\n3 Graph Convolutions\nGraph convolutions have become the basic building blocks in many deep graph representation\nlearning algorithms and graph neural networks developed recently. In this section, we provide a\ncomprehensive review of graph convolutions, which generally fall into two categories: spectral\ngraph convolutions and spatial graph convolutions. Based on the solid mathematical foundations\nof Graph Signal Processing (GSP) [164, 396, 414], spectral graph convolutions seek to capture the\npatterns of the graph in the frequency domain. On the other hand, spatial graph convolutions\ninherit the idea of message passing from Recurrent Graph Neural Networks (RecGNNs), and they\ncompute node features by aggregating the features of their neighbors. Thus, the computation graph\nof a node is derived from the local graph structure around it, and the graph topology is naturally\nincorporated into the way node features are computed. In this section, we first introduce spectral\ngraph convolutions and then spatial graph convolutions, followed by a brief summary. In Table 2,\nwe summarize a number of graph convolutions proposed in recent years.\n3.1 Spectral Graph Convolutions\nWith the success of Convolutional Neural Networks (CNNs) in computer vision [244], efforts have\nbeen made to transfer the idea of convolution to the graph domain. However, this is not an easy task\nbecause of the non-Euclidean nature of graphical data. Graph signal processing (GSP) [164, 396, 414]\ndefines the Fourier Transform on graphs and thus provides a solid theoretical foundation of spectral\ngraph convolutions.\nIn graph signal processing, a graph signal refers to a set of scalars associated with every node\nin the graph, i.e. \ud835\udc53 (\ud835\udc63), \u2200\ud835\udc63 \u2208 \ud835\udc49 , and it can be written in the \ud835\udc5b-dimensional vector form x \u2208 R\n\ud835\udc5b\n,\nwhere \ud835\udc5b is the number of nodes in the graph. Another core concept of graph signal processing\nis the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian), defined as\nL = I \u2212 D\n\u22121/2AD\u22121/2\n, where I is the identity matrix, D is the degree matrix (i.e. a diagonal matrix\nD\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc57 A\ud835\udc56\ud835\udc57), and A is the adjacency matrix. In the typical setting of graph signal processing, the\ngraph \ud835\udc3a is undirected. Therefore, L is real symmetric and positive semi-definite. This guarantees\nthe eigen decomposition of the graph Laplacian: L = U\u039bU\ud835\udc47, where U = [u0, u1, ..., u\ud835\udc5b\u22121] is the\neigenvectors of the graph Laplacian and the diagonal elements of \u039b = diag(\ud835\udf060, \ud835\udf061, ..., \ud835\udf06\ud835\udc5b\u22121) are the\neigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal x is defined as x\u02dc = U\n\ud835\udc47 x,\nwhere x\u02dc is the graph frequencies of x. Correspondingly, the Inverse Graph Fourier Transform can\nbe written as x = Ux\u02dc.\nWith GFT and the Convolution Theorem, the graph convolution of a graph signal x and a filter\ng can be defined as g \u2217\ud835\udc3a x = U(U\n\ud835\udc47 g \u2299 U\ud835\udc47 x). To simplify this, let g\ud835\udf03 = diag(U\ud835\udc47 \ud835\udc54), the graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 9\nTable 2. Summary of graph convolution methods.\nMethod Category Aggregation Time Complexity\nSpectral CNN [39] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nHenaff et al. [172] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nChebNet [83] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGCN [230] Spectral / Spatial Weighted Average \ud835\udc42(\ud835\udc5a)\nCayleyNet [254] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGraphSAGE [163] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nGAT [452] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nDGCNN [477] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nLanzcosNet [280] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n2\n)\nSGC [493] Spatial Graph Convolution Weighted Average \ud835\udc42(\ud835\udc5a)\nGWNN [512] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGIN [518] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nGraphAIR [179] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nPNA [77] Spatial Graph Convolution Multiple \ud835\udc42(\ud835\udc5a)\nS\n2GC [606] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGNNML3 [21] Spatial / Spectral - \ud835\udc42(\ud835\udc5a)\nMSGNN [170] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nEGC [437] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nAPPNP [138] Spatial Graph Convolution (Approximate) Personalized Pagerank \ud835\udc42(\ud835\udc5a)\nGCNII [61] Spatial Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGATv2 [38] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nconvolution can be written as:\ng \u2217\ud835\udc3a x = Ug\ud835\udf03U\n\ud835\udc47\nx, (9)\nwhich is the general form of most spectral graph convolutions. The key of spectral graph convolu\u0002tions is to parameterize and learn the filter g\ud835\udf03 .\nSpectral Convolutional Neural Network (Spectral CNN) [39] sets graph filter as a learnable diagonal\nmatrix W. The convolution operation can be written as y = UWU\ud835\udc47 x. In practice, multi-channel\nsignals and activation functions are common, and the graph convolution can be written as\nY:,\ud835\udc57 = \ud835\udf0e\nU\n\u2211\ufe01\ud835\udc50\ud835\udc56\ud835\udc5b\n\ud835\udc56=1\nW\ud835\udc56,\ud835\udc57U\n\ud835\udc47 X:,\ud835\udc56!\n, \ud835\udc57 = 1, 2, ..., \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61, (10)\nwhere \ud835\udc50\ud835\udc56\ud835\udc5b is the number of input channel, \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 is the number of output channel, X is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc56\ud835\udc5b\nmatrix representing the input signal, Y is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix denoting the output signal, W\ud835\udc56,\ud835\udc57 is a\nparameterized diagonal matrix, and \ud835\udf0e(\u00b7) is the activation function. For mathematical convenience\nwe sometimes use single-channel versions of graph convolutions omitting activation functions,\nand the multi-channel versions are similar to Eq. 10.\nSpectral CNN has several limitations. Firstly, the filters are basis-dependent, which means that\nthey cannot be generalized across graphs. Secondly, the algorithm requires eigen decomposition,\nwhich is computationally expensive. Thirdly, it has no guarantee of spatial localization of filters.\nTo make filters spatially localized, Henaff et al. [172] propose to use a smooth spectral transfer\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n10 W. Ju, et al.\nfunction \u0398(\u039b) to parameterize the filter, and the convolution operation can be written as:\ny = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx. (11)\nChebyshev Spectral Convolutional Neural Network (ChebNet) [83] extends this idea by using\ntruncated Chebyshev polynomials to approximate the spectral transfer function. The Chebyshev\npolynomial is defined as \ud835\udc470 (\ud835\udc65) = 1, \ud835\udc471 (\ud835\udc65) = \ud835\udc65, \ud835\udc47\ud835\udc58 (\ud835\udc65) = 2\ud835\udc65\ud835\udc47\ud835\udc58\u22121 (\ud835\udc65) \u2212 \ud835\udc47\ud835\udc58\u22122 (\ud835\udc65), and the spectral\ntransfer function \ud835\udc39 (\u039b) is approximated to the order of \ud835\udc3e \u2212 1 as\n\ud835\udc39 (\u039b) =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc ), (12)\nwhere the model parameters \ud835\udf03\ud835\udc58, \ud835\udc58 \u2208 {0, 1, ..., \ud835\udc3e \u2212 1} are the Chebyshev coefficients, and \u039b\u02dc =\n2\u039b/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be written\nas:\ng \u2217\ud835\udc3a x = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx = U\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc )U\n\ud835\udc47\nx =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (L\u02dc)x, (13)\nwhere L\u02dc = 2L/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I.\nGraph Convolutional Network (GCN) [230] is proposed as the localized first-order approximation\nof ChebNet. Assuming \ud835\udc3e = 2 and \ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 = 2, Eq. 13 can be simplified as:\ng \u2217\ud835\udc3a x = \ud835\udf030x + \ud835\udf031 (L \u2212 I)x = \ud835\udf030x \u2212 \ud835\udf031D\n\u22121/2AD\u22121/2\nx. (14)\nTo further constraint the number of parameters, we assume \ud835\udf03 = \ud835\udf030 = \u2212\ud835\udf031, which gives a simpler\nform of graph convolution:\ng \u2217G x = \ud835\udf03 (I + D\n\u22121/2AD\u22121/2\n)x. (15)\nAs I + D\n\u22121/2AD\u22121/2 now has the eigenvalues in the range of [0, 2] and repeatedly multiplying\nthis matrix can lead to numerical instabilities, GCN empirically proposes a renormalization trick to\nsolve this problem by using D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2instead, where A\u02dc = A + I and D\u02dc\n\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc56 A\u02dc\n\ud835\udc56\ud835\udc57 .\nAllowing multi-channel signals and adding activation functions, the more common formula in\nliterature is:\nY = \ud835\udf0e( (D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2)X\u0398), (16)\nwhere X, Y have the same shape as in Eq. 10 and \u0398 is a \ud835\udc50\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix as model\u2019s parameters.\nApart from the aforementioned methods, other spectral graph convolutions have been proposed.\nLevie et al. [254] propose CayleyNets that utilize Cayley Polynomials to equip the filters with\nthe ability to detect narrow frequency bands. Liao et al. [280] propose LanczosNets that employ\nthe Lanczos algorithm to construct a low-rank approximation of graph Laplacian to improve the\ncomputation efficiency of graph convolutions. The proposed model is able to efficiently utilize the\nmulti-scale information in the graph data. Instead of using Graph Fourier Transform, Xu et al. [512]\npropose a Graph Wavelet Neural Network (GWNN) that uses graph wavelet transform to avoid\nmatrix eigendecomposition. Moreover, graph wavelets are sparse and localized, which provides\ngood interpretations for the convolution operation. Zhu and Koniusz [606] derive a Simple Spectral\nGraph Convolution (S2GC) from a modified Markov Diffusion Kernel, which achieves a trade-off\nbetween low-pass and high-pass filter bands.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 11\n3.2 Spatial Graph Convolutions\nInspired by the convolution on Euclidean data (e.g. images and texts), which applies data trans\u0002formation on a small region, spatial graph convolutions compute the central node\u2019s feature via\ntransforming and aggregating its neighbors\u2019 features. In this way, the graph structure is naturally\nembedded in the computation graph of node features. Moreover, the idea of sending one node\u2019s\nfeature to another node is similar to the message passing used in recurrent graph neural networks.\nIn the following, we will introduce several seminal spatial graph convolutions as well as some\nrecently proposed promising methods.\nSpatial graph convolutions generally follow a three-step paradigm: message generation, feature\naggregation and feature update. This can be mathematically written as:\ny\ud835\udc56 = UPDATE x\ud835\udc56, AGGREGATE {MESSAGE x\ud835\udc56, x\ud835\udc57, e\ud835\udc56\ud835\udc57\u0001, \ud835\udc57 \u2208 N (\ud835\udc56)}\u0001\u0001 , (17)\nwhere x\ud835\udc56 and y\ud835\udc56is the input and output feature vector of node \ud835\udc56, e\ud835\udc56\ud835\udc57 is the feature vector of the edge\n(or more generally, the relationship) between node \ud835\udc56 and its neighbor node \ud835\udc57, and N (\ud835\udc56) denote the\nneighbor of node \ud835\udc56, which could be more generally defined.\nIn the previous subsection, we show the spectral interpretation of GCN [230]. The model also\nhas its spatial interpretation, which can be mathematically written as:\ny\ud835\udc56 = \u0398\n\ud835\udc47 \u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\u222a\ud835\udc56\n1\n\u221a\ufe03\n\u02c6\ud835\udc51\ud835\udc56\u02c6\ud835\udc51\ud835\udc57\nx\ud835\udc57, (18)\nwhere \u02c6\ud835\udc51\ud835\udc56 and \u02c6\ud835\udc51\ud835\udc57is the \ud835\udc56-th and \ud835\udc57-th row sums of A\u02c6 in Eq. 16. For each node, the model takes a\nweighted sum of its neighbors\u2019 features as well as its own features and applies a linear transformation\nto obtain the result. In practice, multiple GCN layers are often stacked together with non-linear\nfunctions after convolution to encode complex and hierarchical features. Nonetheless, Wu et al.\n[493] show that the model still achieves competitive results without non-linearity.\nAlthough GCN as well as other spectral graph convolutions achieve competitive results on a\nnumber of benchmarks, these methods assume the presence of all nodes in the graph and fall in the\ncategory of transductive learning. Hamilton et al. [163] propose GraphSAGE that performs graph\nconvolutions in inductive settings, when there are new nodes during inference (e.g. newcomers\nin the social network). For each node, the model samples its \ud835\udc3e-hop neighbors and uses \ud835\udc3e graph\nconvolutions to aggregate their features hierarchically. Furthermore, the use of sampling also\nreduces the computation when a node has too many neighbors.\nThe attention mechanism has been successfully used in natural language processing [451], com\u0002puter vision [295] and multi-modal tasks [62, 168, 552, 591]. Graph Attention Networks (GAT) [452]\nintroduces the idea of attention to graphs. The attention mechanism uses an adaptive, feature\u0002dependent weight (i.e. attention coefficient) to aggregate a set of features, which can be mathemati\u0002cally written as:\n\ud835\udefc\ud835\udc56,\ud835\udc57 =\nexp LeakyReLU a\n\ud835\udc47\n[\u0398x\ud835\udc56||\u0398x\ud835\udc57]\n\u0001\u0001\n\u00cd\n\ud835\udc58 \u2208N (\ud835\udc56)\u222a{\ud835\udc56 } exp\nLeakyReLU a\n\ud835\udc47 [\u0398x\ud835\udc56\n||\u0398x\ud835\udc57]\n\u0001\u0001 , (19)\nwhere \ud835\udefc\ud835\udc56,\ud835\udc57 is the attention coefficient, a and \u0398 are model parameters, and [\u00b7||\u00b7] means concatenation.\nAfter the \ud835\udefcs are obtained, the new features are computed as a weighted sum of input node features,\nwhich is:\ny\ud835\udc56 = \ud835\udefc\ud835\udc56,\ud835\udc56\u0398x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\n\ud835\udefc\ud835\udc56,\ud835\udc57\u0398x\ud835\udc57. (20)\nXu et al. [518] explore the representational limitations of graph neural networks. What they\ndiscover is that message passing networks like GCN [230] and GraphSAGE [163] are incapable of\ndistinguishing certain graph structures. To improve the representational power of graph neural\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n12 W. Ju, et al.\nnetworks, they propose the Graph Isomorphism Network (GIN) that gives an adjustable weight to\nthe central node feature, which can be mathematically written as:\ny\ud835\udc56 = MLP \u00a9\n\u00ad\n\u00ab\n(1 + \ud835\udf16)x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\nx\ud835\udc57\n\u00aa\n\u00ae\n\u00ac\n, (21)\nwhere \ud835\udf16 is a learnable parameter.\nMore recently, efforts have been made to improve the representational power of graph neural\nnetworks. For example, Hu et al. [179] propose GraphAIR that explicitly models the neighborhood\ninteraction to better capture complex non-linear features. Specifically, they use the Hadamard\nproduct between pairs of nodes in the neighborhood to model the quadratic terms of neighborhood\ninteraction. Balcilar et al. [21] propose GNNML3 that breaks the limits of the first-order Weisfeiler\u0002Lehman test (1-WL) and reaches the third-order WL test (3-WL) experimentally. They also show\nthat the Hadamard product is required for the model to have more representational power than the\nfirst-order Weisfeiler-Lehman test. Other elements in spatial graph convolutions are widely studied.\nFor example, Corso et al. [77] explore the aggregation operation in GNN and proposes Principal\nNeighbourhood Aggregation (PNA) that uses multiple aggregators with degree-scalers. Tailor et al.\n[437] explore the anisotropism and isotropism in the message passing process of graph neural\nnetworks, and proposes Efficient Graph Convolution (EGC) that achieves promising results with\nreduced memory consumption due to isotropism. In order to increase the size of the neighborhood\nof a node, Gasteiger et al. [138] propose personalized propagation of neural predictions (PPNP) and\nits approximation using power iteration (APPNP). To increase the depth of graph neural networks,\nChen et al. [61] propose GCNII that uses initial residual and identity mapping to mitigate the over\u0002smoothing problem. Brody et al. [38] propose GATv2 that uses dynamic attention and improves\nthe expressive power of GAT [452].\n3.3 Summary\nThis section introduces graph convolutions. We provide the summary as follows:\n\u2022 Techniques. Graph convolutions mainly fall into two types, i.e. spectral graph convolu\u0002tions and spatial graph convolutions. Spectral graph convolutions have solid mathematical\nfoundations of Graph Signal Processing and therefore their operations have theoretical in\u0002terpretations. Spatial graph convolutions are inspired by Recurrent Graph Neural Networks\nand their computation is simple and straightforward, as their computation graph is derived\nfrom the local graph structure. Generally, spatial graph convolutions are more common in\napplications.\n\u2022 Challenges and Limitations. Despite the great success of graph convolutions, their perfor\u0002mance is unsatisfactory in more complicated applications. On the one hand, the performance\nof graph convolutions relies heavily on the construction of the graph. Different constructions\nof the graph might result in different performances of graph convolutions. On the other\nhand, graph convolutions are prone to over-smoothing when constructing very deep neural\nnetworks.\n\u2022 Future Works. In the future, we expect that more powerful graph convolutions will be\ndeveloped to mitigate the problem of over-smoothing and we also hope that techniques and\nmethodologies in Graph Structure Learning (GSL) can help learn more meaningful graph\nstructure to benefit the performance of graph convolutions.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 13\n4 Graph Kernel Neural Networks\nGraph kernels (GKs) are historically the most widely used technique on graph analyzing and\nrepresentation tasks [137, 243, 423, 601]. However, traditional graph kernels rely on hand-crafted\npatterns or domain knowledge on specific tasks[242, 410]. Over the years, an amount of research has\nbeen conducted on graph kernel neural networks (GKNNs), which has yielded promising results.\nResearchers have explored various aspects of GKNNs, including their theoretical foundations,\nalgorithmic design, and practical applications. These efforts have led to the development of a wide\nrange of GKNN-based models and methods that can be used for graph analysis and representation\ntasks, such as node classification [113, 222, 298, 534], link prediction [54, 300, 497, 525], and graph\nclustering [243, 299].\nThe success of GKNNs can be attributed to their ability to leverage the strengths of both graph\nkernels and neural networks [221, 299, 497]. By using kernel functions to measure similarity\nbetween graphs, GKNNs can capture the structural properties of graphs, while the use of neural\nnetworks enables them to learn more complex and abstract representations of graphs [56, 558].\nThis combination of techniques allows GKNNs to achieve state-of-the-art performance on a wide\nrange of graph-related tasks [216, 243, 479].\nIn this section, we begin with introducing the most representative traditional graph kernels.\nThen we summarize the basic framework for combining GNNs and graph kernels. Finally, we\ncategorize the popular graph kernel Neural networks into several categories and compare their\ndifferences.\n4.1 Graph Kernels\nGraph kernels generally evaluate pairwise similarity between nodes or graphs by decomposing\nthem into basic structural units. Random walks [223], subtrees [409], shortest paths [32] and\ngraphlets [410] are representative categories.\nGiven two graphs \ud835\udc3a1 = (\ud835\udc491, \ud835\udc381, \ud835\udc4b1) and \ud835\udc3a2 = (\ud835\udc492, \ud835\udc382, \ud835\udc4b2), a graph kernel function \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2)\nmeasures the similarity between \ud835\udc3a1 and \ud835\udc3a2 through the following formula:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 \ud835\udc59\ud835\udc3a1(\ud835\udc621),\ud835\udc59\ud835\udc3a2(\ud835\udc622)\n\u0001\n, (22)\nwhere \ud835\udc59\ud835\udc3a (\ud835\udc62) denotes a set of local substructures centered at node \ud835\udc62 in graph \ud835\udc3a, and \ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 is a base\nkernel measuring the similarity between the two sets of substructures. For simplicity, we may\nrewrite Eq. 22 as:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 (\ud835\udc621, \ud835\udc622), (23)\nthe uppercase letter \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) is denoted as graph kernels, \ud835\udf05(\ud835\udc621, \ud835\udc622) is denoted as node kernels,\nand lowercase \ud835\udc58 (\ud835\udc65, \ud835\udc66) is denoted as general kernel functions.\nThe kernel mapping of a kernel \ud835\udf13 maps a data point into its corresponding Reproducing Kernel\nHilbert Space (RKHS) H. Specifically, given a kernel \ud835\udc58\u2217 (\u00b7, \u00b7), its kernel mapping\ud835\udf13\u2217 can be formalized\nas,\n\u2200\ud835\udc651, \ud835\udc652, \ud835\udc58\u2217 (\ud835\udc651, \ud835\udc652) = \u27e8\ud835\udf13\u2217 (\ud835\udc651),\ud835\udf13\u2217 (\ud835\udc652)\u27e9H\u2217, (24)\nwhere H\u2217 is the RKHS of \ud835\udc58\u2217 (\u00b7, \u00b7).\nWe introduce several representative and popular graph kernels below.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n14 W. Ju, et al.\nWalk and Path Kernels. A \ud835\udc59-walk kernel \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 compares all length \ud835\udc59 walks starting from each\nnode in two graphs \ud835\udc3a1,\ud835\udc3a2,\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc641\u2208W\ud835\udc59(\ud835\udc3a1,\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc642\u2208W\ud835\udc59(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc4b1 (\ud835\udc641), \ud835\udc4b2 (\ud835\udc642)),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622).\n(25)\nSubstituting W with P is able to get the \ud835\udc59-path kernel.\nSubtree Kernels. The WL subtree kernel is the most popular one in subtree kernels. It is a finite\u0002depth kernel variant of the 1-WL test. The WL subtree kernel with depth \ud835\udc59, \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f compares all\nsubtrees with depth \u2264 \ud835\udc59 rooted at each node.\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc611\u2208 T\ud835\udc56(\ud835\udc3a1,\ud835\udc622 )\n\u2211\ufe01\n\ud835\udc612\u2208 T\ud835\udc56(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc611, \ud835\udc612),\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc59\n\ud835\udc56=0\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2),\n(26)\nwhere \ud835\udc61 \u2208 T (\ud835\udc56)(\ud835\udc3a, \ud835\udc62) denotes a subtree of depth \ud835\udc56 rooted at \ud835\udc62 in \ud835\udc3a.\n4.2 General Framework of GKNNs\nIn this section, we summarize the general framework of GKNNs. For the first step, a kernel that\nmeasures similarities of heterogeneous features from heterogeneous nodes and edges (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc622) and\n(\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622) should be defined. Take the inner product of neighbor tensors as an example, its neighbor\nkernel is defined as follows,\n\ud835\udf05( (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc621), (\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622)) = \u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc621), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc622)\u27e9.\nBased on the neighbor kernel, a kernel with two \ud835\udc59-hop neighborhoods for central node \ud835\udc621 and \ud835\udc622\ncan be defined as \ud835\udc3e\n(\ud835\udc59)\n(\ud835\udc621, \ud835\udc622) =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \ud835\udc59 = 0\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u2211\ufe01\n\ud835\udc631\u2208\ud835\udc41 (\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc632\u2208\ud835\udc41 (\ud835\udc622 )\n\ud835\udc3e\n(\ud835\udc59\u22121)\n(\ud835\udc631, \ud835\udc632) \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc631), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc632\n)\u27e9 \ud835\udc59 > 0\n, (27)\nBy regarding the lower-hop kernel \ud835\udf05\n(\ud835\udc59\u22121)\n(\ud835\udc621, \ud835\udc622), as the inner product of the (\ud835\udc59 \u2212 1)-th hidden\nrepresentations of \ud835\udc621 and \ud835\udc622. Furthermore, by recursively applying the neighborhood kernel, the\n\ud835\udc59-hop graph kernel can be derived as\n\ud835\udc3e\n\ud835\udc59\n(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc981\u2208W\ud835\udc59(\ud835\udc3a1 )\n\u2211\ufe01\n\ud835\udc982\u2208W\ud835\udc59(\ud835\udc3a2 )\n\u00d6\n\ud835\udc59\u22121\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n1\n), \ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n2\n)\u27e9 \u00d7\u00d6\n\ud835\udc59\u22122\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n1\n,\ud835\udc98\n(\ud835\udc56+1)\n1\n), \ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n2\n,\ud835\udc98\n(\ud835\udc56+1)\n2\n)\u27e9!,\n(28)\nwhere W\ud835\udc59(\ud835\udc3a) denotes the set of all walk sequences with length \ud835\udc59 in graph \ud835\udc3a, and \ud835\udc98\n(\ud835\udc56)\n1\ndenotes the\n\ud835\udc56-th node in sequence \ud835\udc981.\nAs shown in Eq. 24, kernel methods implicitly perform projections from original data spaces\nto their RKHS H. Hence, as GNNs also project nodes or graphs into vector spaces, connections\nhave been established between GKs and GNNs through the kernel mappings. And several works\nconducted research on the connections [253, 491], and found some foundation conclusions. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 15\nthe basic rule introduced in [253] as an example, the proposed graph kernel in Eq. 22 can be derived\nas the general formulas,\n\u210e\n(0)\n(\ud835\udc63) =\ud835\udc7e\n(0)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63),\n\u210e\n(\ud835\udc59)\n(\ud835\udc63) =\ud835\udc7e\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63) \u2299 \u2211\ufe01\n\ud835\udc62\u2208\ud835\udc41 (\ud835\udc63)\n(\ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\u210e\n(\ud835\udc59\u22121)\n(\ud835\udc62) \u2299 \ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc38 (\ud835\udc52\ud835\udc62,\ud835\udc63 )\n\ud835\udc53 (\ud835\udc52\ud835\udc62,\ud835\udc63 )), 1  1-WL\nGGT [102] \u2713 \u2713 structure only \u2713\nGTSA [241] \u2713 \u2713 \u2713 \u2713\nHGT [183] \u2713 \u2713\nG2SHGT [536] \u2713 \u2713 \u2713\nHINormer [335] \u2713 \u2713 \u2713\nGRUGT [41] \u2713 \u2713 \u2713\nGRIT [324] \u2713 \u2713 \u2713\nGraphormer-GD [560] \u2713 \u2713 \u2713\nGraphormer [540] \u2713 \u2713 \u2713 \u2713\nGSGT [191] \u2713 \u2713 \u2713\nTMDG [145] \u2713 \u2713 \u2713\nGraph-BERT [567] \u2713 \u2713 \u2713\nLRGT [498] \u2713 \u2713\nSAT [56] \u2713 \u2713 \u2713\n6.1 Transformer\nTransformer [451] was first applied to model machine translation, but two of the key mechanisms\nadopted in this work, attention operation and positional encoding, are highly compatible with the\ngraph modeling problem.\nTo be specific, we denote the input of attention layer in Transformer as X = [x0, x1, . . . , x\ud835\udc5b\u22121],\nx\ud835\udc56 \u2208 R\n\ud835\udc51\n, where \ud835\udc5b is the length of input sequence and \ud835\udc51 is the dimension of each input embedding\nx\ud835\udc56. Then the core operation of calculating new embedding x\u02c6\ud835\udc56 for each x\ud835\udc56in attention layer can be\nstreamlined as:\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57) = NORM\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\nQ\n\u210e\n(x\ud835\udc56)\nTK\u210e\n(x\ud835\udc58 )),\nx\n\u210e\n\ud835\udc56 =\n\u2211\ufe01\nx\ud835\udc57 \u2208X\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57)V\u210e(x\ud835\udc57),\nx\u02c6\ud835\udc56 = MERGE(x\n1\n\ud835\udc56\n, x\n2\n\ud835\udc56\n, . . . , x\n\ud835\udc3b\n\ud835\udc56\n),\n(57)\nwhere \u210e \u2208 {0, 1, . . . , \ud835\udc3b \u2212 1} represents the attention head number. Q\n\u210e\n, K\u210eand V\u210eare projection\nfunctions mapping a vector to the query space, key space and value space respectively. s\n\u210e\n(x\ud835\udc56\n, x\ud835\udc57) is\nscore function measuring the similarity between x\ud835\udc56 and x\ud835\udc57. NORM is the normalization operation\nensuring \u00cd\nx\ud835\udc57 \u2208X s\n\u210e\n(x\ud835\udc56, x\ud835\udc57) \u2261 1 to propel the stability of the output generated by a stack of attention\nlayers, it is usually performed as scaled softmax: NORM(\u00b7) = SoftMax(\u00b7/\u221a\ud835\udc51). And MERGE function\nis designed to combine the information extracted from multiple attention heads. Here, we omit\nfurther implementation details that do not affect our understanding of attention operation.\nThe attention process cannot encode the position information of each x\ud835\udc56, which is essential in\nmachine translation problems. So positional encoding is introduced to remedy this deficiency, and\nit\u2019s calculated as:\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57\n= sin(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), X\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57+1\n= cos(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), (58)\nwhere \ud835\udc56 is the position and \ud835\udc57 is the dimension. The positional encoding is added to the input before\nit is fed to the Transformer.\n6.2 Overview\nFrom the simplified process shown in Eq. 57, we can see that the core of the attention operation is\nto accomplish information transfer based on the similarity between the source and the target to be\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 25\nupdated. It\u2019s quite similar to the message-passing process on a fully-connected graph. However,\nthe direct application of this architecture to arbitrary graphs does not make use of structural\ninformation, so it may lead to poor performance when graph topology is important. On the other\nhand, the definition of positional encoding in graphs is not a trivial problem because the order or\ncoordinates of graph nodes are underdefined.\nAccording to these two challenges, Transformer-based methods for graph representation learning\ncan be classified into two major categories, one considering graph structure during the attention\nprocess, and the other encoding the topological information of the graph into initial node features.\nWe name the first one as Attention Modification and the second one as Encoding Enhancement. A\nsummarization is provided in Table 5. In the following discussion, if both methods are used in\none paper, we will list them in different subsections, and we will ignore the multi-head trick in\nattention operation.\n6.3 Attention Modification\nThis group of works attempts to modify the full attention operation to capture structure information.\nThe most prevalent approach is changing the score function, which is denoted as s(\u00b7, \u00b7) in Eq. 57.\nGGT [102] constrains each node feature can only attend to neighbors and enables the model to\nrepresent edge feature information by rewrite s(\u00b7, \u00b7) as:\ns\u02dc1 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\ud835\udc44x\ud835\udc56)\nT\n(W\ud835\udc3ex\ud835\udc57 \u2299 W\ud835\udc38e\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\u2212 \u221e, otherwise\n,\ns1 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc1 (x\ud835\udc56, x\ud835\udc58 )),\n(59)\nwhere \u2299 is Hadamard product and W\ud835\udc44,\ud835\udc3e,\ud835\udc38 represents trainable parameter matrix. This approach\nis not efficient yet to model long-distance dependencies since only 1st-neighbors are considered.\nThough it adopts Laplacian eigenvectors to gather global information (see Section 6.4), but only long\u0002distance structure information is remedied while the node and edge features are not. GTSA [241]\nimproves this approach by combining the original graph and the full graph. Specifically, it extends\ns1 (\u00b7, \u00b7) to:\ns\u02dc2 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\n\ud835\udc44\n1\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n1\nx\ud835\udc57 \u2299 W\ud835\udc38\n1\ne\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n(W\n\ud835\udc44\n0\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n0\nx\ud835\udc57 \u2299 W\ud835\udc38\n0\ne\ud835\udc57\ud835\udc56), otherwise\n,\ns2 (x\ud835\udc56, x\ud835\udc57) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\n\uf8f3\n1\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2208\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\ud835\udf06\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2209\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), otherwise\n,\n(60)\nwhere \ud835\udf06 is a hyperparameter representing the strength of the full connection.\nSome works try to reduce information-mixing problems [55] in heterogeneous graphs. HGT [183]\ndisentangles the attention of different node types and edge types by adopting additional attention\nheads. It defines W\ud835\udf0f (\ud835\udc63)\n\ud835\udc44,\ud835\udc3e,\ud835\udc49 for each node type \ud835\udf0f (\ud835\udc63) and W\n\ud835\udf19 (\ud835\udc52 )\n\ud835\udc38\nfor each edge type \ud835\udf19 (\ud835\udc52), \ud835\udf0f (\u00b7) and\n\ud835\udf19 (\u00b7) are type indicating function. G2SHGT [536] defines four types of subgraphs, fully-connected,\nconnected, default and reverse, to capture global, undirected, forward and backward information\nrespectively. Each subgraph is homogeneous, so it can reduce interactions between different classes.\nPath features between nodes are always treated as inductive bias added to the original score\nfunction. Let SP\ud835\udc56\ud835\udc57 = (\ud835\udc521, \ud835\udc522, . . . , \ud835\udc52\ud835\udc41 ) denote the shortest path between node pair (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57). GRUGT [41]\nuses GRU [74] to encode forward and backward features as: r\ud835\udc56\ud835\udc57 = GRU(SP\ud835\udc56\ud835\udc57), r\ud835\udc57\ud835\udc56 = GRU(SP\ud835\udc57\ud835\udc56).\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n26 W. Ju, et al.\nThen, the final attention score is calculated by adding up four components:\ns\u02dc3 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56, (61)\nfrom front to back, which represent content-based score, source-dependent bias, target-dependent\nbias and universal bias respectively. Graphormer [540] uses both path length and path embedding\nto introduce structural bias as:\ns\u02dc4 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 /\n\u221a\n\ud835\udc51 + \ud835\udc4f\ud835\udc41 + \ud835\udc50\ud835\udc56\ud835\udc57,\n\ud835\udc50\ud835\udc56\ud835\udc57 =\n1\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc58=1\n(e\ud835\udc58 )\nTw\ud835\udc38\n\ud835\udc58\n,\ns4 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc4 (x\ud835\udc56, x\ud835\udc58 )),\n(62)\nwhere \ud835\udc4f\ud835\udc41 is a trainable scalar indexed by \ud835\udc41, the length of SP\ud835\udc56\ud835\udc57 . e\ud835\udc58 is the embedding of the the\nedge \ud835\udc52\ud835\udc58 , and w\ud835\udc38\n\ud835\udc58\n\u2208 R\n\ud835\udc51\nis the \ud835\udc58-th edge parameter. If SP\ud835\udc56\ud835\udc57 does not exist, then \ud835\udc4f\ud835\udc41 and \ud835\udc50\ud835\udc56\ud835\udc57 are set to\nbe special values. GRIT [324] utilizes relative random walk probabilities as an inductive bias to\nencode relative path information. Graphormer-GD [560] also incorporates relative distance as bias,\nand rigorously proves that this bias is crucial for determining the biconnectivity of a graph.\n6.4 Encoding Enhancement\nThis kind of method intends to enhance initial node representations to enable the Transformer to\nencode structure information. They can be further divided into two categories, position-analogy\nmethods and structure-aware methods.\n6.4.1 Position-analogy methods In Euclidean space, the Laplacian operator corresponds to the\ndivergence of the gradient, whose eigenfunctions are sine/cosine functions. For the graph, the\nLaplacian operator is the Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.\nHence, inspired by Eq. 58, position-analogy methods utilize Laplacian eigenvectors to simulate\npositional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as they are the equivalents of sine/cosine functions.\nLaplacian eigenvectors can be calculated via the eigendecomposition of normalized graph Lapla\u0002cian matrix L\u02dc\n:\nL\u02dc \u225c I \u2212 D\n\u22121/2AD\u22121/2 = U\u039bUT\n, (63)\nwhere A is the adjacency matrix, D is the degree matrix, U = [u1, u2, . . . , u\ud835\udc5b\u22121] are eigenvectors\nand \u039b = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udf060, \ud835\udf061, . . . , \ud835\udf06\ud835\udc5b\u22121) are eigenvalues. With U and \u039b, GGT [102] uses eigenvectors of the\nk smallest non-trivial eigenvalues to denote the intermediate embedding X\n\ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\n, and maps it\nto d-dimensional space and gets the position encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\n. This process can be formalized\nas:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 = argmin\ud835\udc58({\ud835\udf06\ud835\udc56|0 \u2264 \ud835\udc56  0}),\nX\n\ud835\udc5a\ud835\udc56\ud835\udc51 = [u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc650\n, u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc651, . . . , u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc58\u22121]\nT\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = X\ud835\udc5a\ud835\udc56\ud835\udc51W\ud835\udc58\u00d7\ud835\udc51\n,\n(64)\nwhere \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 is the subscript of the selected eigenvectors. GTSA [241] puts eigenvector u\ud835\udc56 on\nthe frequency axis at \ud835\udf06\ud835\udc56 and uses sequence modeling methods to generate positional encoding.\nSpecifically, it extends X\n\ud835\udc5a\ud835\udc56\ud835\udc51 in Eq. 64 to X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\u00d72 by concatenating each value in eigenvectors\nwith corresponding eigenvalue, and then positional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\nare generated as:\nX\n\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 = X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51W2\u00d7\ud835\udc51\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = SumPooling(Transformer(X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61), dim = 1).\n(65)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 27\nHere, X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 R\n\ud835\udc5b\u00d7\ud835\udc58\u00d7\ud835\udc51\nis equivalent to the input matrix in sequence modeling with shape\n(\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e, \ud835\udc51\ud835\udc56\ud835\udc5a), and can be naturally processed by Transformer. Since the Laplacian\neigenvectors can be complex-valued for directed graph, GSGT [191] proposes to utilize SVD of\nadjacency matrix A, which is denoted as A = U\u03a3VT, and uses the largest \ud835\udc58 singular values \u03a3\ud835\udc58 and\nassociated left and right singular vectors U\ud835\udc58 and V\nT\n\ud835\udc58\nto output X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as X\ud835\udc5d\ud835\udc5c\ud835\udc60 = [U\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n\u2225V\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n],\nwhere \u2225 is the concatenation operation. In addition to SVD, TMDG [145] processes directed graphs\nby utilizing the Magnetic Laplacian. All these methods above randomly flip the signs of eigenvectors\nor singular vectors during the training phase to promote the invariance of the models to the sign\nambiguity.\n6.4.2 Structure-aware methods In contrast to position-analogy methods, structure-aware methods\ndo not attempt to mathematically rigorously simulate sequence positional encoding. They use some\nadditional mechanisms to directly calculate structure-related encoding.\nSome approaches compute extra encoding X\n\ud835\udc4e\ud835\udc51\ud835\udc51 and add it to the initial node representation.\nGraphormer [540] proposes to leverage node centrality as an additional signal to address the\nimportance of each node. Concretely, x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nis determined by the in-degree deg\u2212\n\ud835\udc56\nand outdegree deg+\n\ud835\udc56\n:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56 = P\n\u2212\n(deg\u2212\n\ud835\udc56\n) + P+(deg+\n\ud835\udc56\n), (66)\nwhere P\n\u2212\nand P\n+\nare learnable embedding function. Graph-BERT [567] employs Weisfeiler-Lehman\nalgorithm to label node \ud835\udc63\ud835\udc56 to a number WL(\ud835\udc63\ud835\udc56) \u2208 N and defines x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nas:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57 = sin(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n), x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57+1 = cos(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n). (67)\nThe other approaches try to leverage GNNs to initialize inputs to the Transformer. LRGT [498]\napplies GNN to get intermediate vectors as X\n\u2032 = GNN(X), and passes the concatenation of X\u2032\nand\na special vector xCLS to Transformer layer as: X\u02c6 = Transformer( [X\n\u2032\n\u2225xCLS]). Then x\u02c6CLS can be used\nas the representation of the entire graph for downstream tasks. This method cannot break the 1-WL\nbottleneck because it uses GCN [230] and GIN [518] as graph encoders in the first step, which\nare intrinsically limited by 1-WL test. SAT [56] improves this deficiency by using subgraph-GNN\nNGNN [569] for initialization, and achieves outstanding performance.\n6.5 Summary\nThis section introduces Transformer-based approaches for graph representation learning and we\nprovide the summary as follows:\n\u2022 Techniques. Graph Transformer methods modify two fundamental techniques in Trans\u0002former, attention operation and positional encoding, to enhance its ability to encode graph\ndata. Typically, they introduce fully connected attention to model long-distance relationships,\nutilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck, and separate\npoints and edges belonging to different classes to avoid over-mixing problems.\n\u2022 Challenges and Limitations. Though Graph Transformers achieve encouraging perfor\u0002mance, they still face two major challenges. The first challenge is the computational cost of\nthe quadratic attention mechanism and shortest path calculation. These operations require\nsignificant computing resources and can be a bottleneck, particularly for large graphs. The\nsecond is the reliance of Transformer-based models on large amounts of data for stable perfor\u0002mance. It poses a challenge when dealing with problems that lack sufficient data, especially\nfor few-shot and zero-shot settings.\n\u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further\nexplored. Additionally, there are some works using pre-training and fine-tuning frameworks\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n28 W. Ju, et al.\nto balance performance and complexity in downstream tasks [540], this may be a promising\nsolution to address the aforementioned two challenges.\n7 Semi-supervised Learning on Graphs\nWe have investigated various architectures of graph neural networks in which the parameters should\nbe tuned by a learning objective. The most prevalent optimization approach is supervised learning on\ngraph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention\nin the data mining community. In detail, these methods attempt to combine graph representation\nlearning with current semi-supervised techniques including pseudo-labeling, consistency learning,\nknowledge distillation and active learning. These works can be further subdivided into node-level\nrepresentation learning and graph-level representation learning. We would introduce both parts in\ndetail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6.\n7.1 Node Representation Learning\nTypically, node representation learning follows the concept of transductive learning, which has\naccess to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised\nloss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is\nusually adopted for optimization. In formulation,\nL\ud835\udc41 \ud835\udc46\ud835\udc3f = \u2212\n1\n|Y\ud835\udc3f |\n\u2211\ufe01\n\ud835\udc56\u2208Y\ud835\udc3f\ny\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (68)\nwhere Y\ud835\udc3f denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that\ncan be used to offer semantic information. To fully utilize these nodes, a range of methods attempt\nto combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a\nfundamental semi-supervised technique that uses the classifier to produce the label distribution of\nunlabeled examples and then adds appropriately labeled examples to the training set [265, 604].\nAnother line of semi-supervised learning is consistency regularization [247] that requires two\nexamples to have identical predictions under perturbation. This regularization is based on the\nassumption that each instance has a distinct label that is resistant to random perturbations [118, 357].\nThen, we show several representative works in detail.\nCooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-label\u0002based GNN approach for semi-supervised node classification. It employs two GNN classifiers to\njointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows:\n\ud835\udc36\ud835\udc49 (p\ud835\udc56) = p\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (69)\nwhere p\ud835\udc56 denotes the output label distribution. Then it selects the pseudo-labels with high confidence\ngenerated from one model to supervise the optimization of the other model. In particular, the\nobjective for unlabeled nodes is written as follows:\nL\ud835\udc36\ud835\udc5c\ud835\udc3a\ud835\udc41 \ud835\udc52\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc56\u2208V\ud835\udc48\n1\ud835\udc36\ud835\udc49 (p\ud835\udc56 )>\ud835\udf0fy\u02c6\n\ud835\udc47\n\ud835\udc56\n\ud835\udc59\ud835\udc5c\ud835\udc54q\ud835\udc56, (70)\nwhere y\u02c6\ud835\udc56 denotes the one-hot formulation of the pseudo-label \ud835\udc66\u02c6\ud835\udc56 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65p\ud835\udc56 and q\ud835\udc56 denotes\nthe label distribution predicted by the other classifier. \ud835\udf0f is a pre-defined temperature coefficient.\nThis cross supervision has been demonstrated effective in [64, 312] to prevent the provision of\nbiased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information\nfrom a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still\ngenerate the same prediction. In this way, CoGNet can illustrate the entire optimization process to\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 29\nTable 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered\nas a specific kind of consistency learning.\nApproach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning\nNode-level\nCoGNet [265] \u2713\nDSGCN [604] \u2713\nGRAND [118] \u2713\nAugGCR [357] \u2713\nHCPL [309] \u2713\nGraph-level\nSEAL [264] \u2713 \u2713\nInfoGraph [431] \u2713 \u2713\nDSGC [527] \u2713\nASGN [166] \u2713 \u2713\nTGNN [218] \u2713\nKGNN [221] \u2713\nHGMI [262] \u2713 \u2713\nASGNN [508] \u2713 \u2713\nDualGraph [310] \u2713 \u2713\nGLA [556] \u2713\nSS [507] \u2713\nenhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in\nsemi-supervised node classification, which can generate dynamics thresholds for reliable nodes.\nDynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive\nmanner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller\nweights to samples with lower confidence with the additional consideration of class balance. The\nweight is formulated as:\n\ud835\udf14\ud835\udc56 =\n1\n\ud835\udc5b\ud835\udc50\n\ud835\udc56\nmax (RELU (p\ud835\udc56 \u2212 \ud835\udefd \u00b7 1)) , (71)\nwhere \ud835\udc5b\ud835\udc50\n\ud835\udc56 denotes the number of unlabeled samples assigned to the class \ud835\udc50\n\ud835\udc56\n. This technique will\ndecrease the impact of wrong pseudo-labels during iterative training.\nGraph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learning\u0002based method. It first adds a variety of perturbations to the input graph to generate a list of\ngraph views. Each graph view \ud835\udc3a\n\ud835\udc5f\nis sent to a GNN classifier to produce a prediction matrix\nP\n\ud835\udc5f = [p\ud835\udc5f\n1\n, \u00b7 \u00b7 \u00b7 , p\n\ud835\udc5f\n\ud835\udc41\n]. Then it summarizes these matrices as:\nP =\n1\n\ud835\udc45\nP\n\ud835\udc5f\n. (72)\nTo provide more discriminative information and ensure that the matrix is row-normalized,\nGRAND sharpens the summarized label matrix into P\n\ud835\udc46\ud835\udc34 as:\nP\n\ud835\udc46\ud835\udc34\n\ud835\udc56\ud835\udc57 =\nP\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\n\u00cd\n\ud835\udc57\n\u2032=0 P\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\u2032\n, (73)\nwhere \ud835\udc47 is a given temperature parameter. Finally, consistency learning is performed by comparing\nthe sharpened summarized matrix with the matrix of each graph view. Formally, the objective is:\nL\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 =\n1\n\ud835\udc45\n\u2211\ufe01\n\ud835\udc45\n\ud835\udc5f=1\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n||P\n\ud835\udc46\ud835\udc34\n\ud835\udc56 \u2212 P\ud835\udc56\n||, (74)\nhere L\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 serves as a regularization which is combined with the standard supervised loss.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n30 W. Ju, et al.\nAugmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with\nthe generation of augmented graphs by random dropout and mixup of different order features. To\nenhance the model generalization, it borrows the idea of meta-learning to partition the training data,\nwhich improves the quality of graph augmentation. In addition, it utilizes consistency regularization\nto enhance the semi-supervised node classification.\n7.2 Graph Representation Learning\nThe objective of graph classification is to predict the property of the whole graph example.\nAssuming that the training set comprises \ud835\udc41\n\ud835\udc59\nand \ud835\udc41\n\ud835\udc62 graph samples G\ud835\udc59 = {\ud835\udc3a1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59\n} and\nG\n\ud835\udc62 = {\ud835\udc3a\ud835\udc41\n\ud835\udc59 +1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59 +\ud835\udc41\ud835\udc62\n}, the graph-level supervised loss for labeled data can be expressed as\nfollows:\nL\ud835\udc3a\ud835\udc46\ud835\udc3f = \u2212\n1\n|G\ud835\udc62 |\n\u2211\ufe01\n\ud835\udc3a\ud835\udc57 \u2208 G\ud835\udc3f\ny\n\ud835\udc57\ud835\udc47\n\ud835\udc59\ud835\udc5c\ud835\udc54p\n\ud835\udc57\n, (75)\nwhere y\n\ud835\udc57 denotes the one-hot label vector for the \ud835\udc57-th sample while p\ud835\udc57 denotes the predicted\ndistribution of \ud835\udc3a\n\ud835\udc57\n. When \ud835\udc41\n\ud835\udc62 = 0, this objective can be utilized to optimize supervised methods.\nHowever, due to the shortage of labels in graph data, supervised methods cannot reach exceptional\nperformance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph\nclassification has been developed extensively. These approaches can be categorized into pseudo\u0002labeling-based methods, knowledge distillation-based methods and contrastive learning-based\nmethods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph\nexamples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually\nutilize a teacher-student architecture, where the teacher model conducts graph representation\nlearning without label information to extract generalized knowledge while the student model\nfocuses on the downstream task. Due to the restricted number of labeled instances, the student\nmodel transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of\nthis topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning.\nTypically, these methods extract topological information from two perspectives (i.e., different\nperturbation strategies and graph encoders), and maximize the similarity of their representations\ncompared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique\nto improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166,\n508]. Then, we review these methods in detail.\nSEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node\nin a hierarchical graph. It builds two graph classifiers which generate graph representations and\nconduct semi-supervised graph classification respectively. SEAL employs a self-attention module\nto encode each graph into a graph-level representation, and then conducts message passing from a\ngraph level for final classification. SEAL can also be combined with cautious iteration and active\niteration. The former merely utilizes partial graph samples to optimize the parameters in the first\nclassifier due to the potential erroneous pseudo-labels. The second combines active learning with\nthe model, which increases the annotation efficiency in semi-supervised scenarios.\nInfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the\nsimilarity between summarized graph representations and their node representations. In particular,\nit generates node representations using the message passing mechanism and summarizes these\nnode representations into a graph representation. Let \u03a6(\u00b7, \u00b7) denote a discriminator to distinguish\nwhether a node belongs to the graph, and we have:\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 31\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e =\n| G\ud835\udc59|+| G\ud835\udc62 \u2211\ufe01|\n\ud835\udc57=1\n\u2211\ufe01\n\ud835\udc56\u2208 G\ud835\udc57\nh\n\u2212 sp \u0010\u2212\u03a6\n\u0010\nh\n\ud835\udc57\n\ud835\udc56\n, z\n\ud835\udc57\n\u0011 \u0011 i \u2212\n1\n|\ud835\udc41\n\ud835\udc57\n\ud835\udc56\n|\n\u2211\ufe01\n\ud835\udc56\n\u2032\ud835\udc57\n\u2032\n\u2208\ud835\udc41\n\ud835\udc57\n\ud835\udc56\nh\nsp \u0010\u03a6\n\u0010\nh\n\ud835\udc57\n\u2032\n\ud835\udc56\n\u2032\n, z\n\ud835\udc57\n\u0011 \u0011 i , (76)\nwhere sp(\u00b7) denotes the softplus function. \ud835\udc41\n\ud835\udc57\n\ud835\udc56\ndenotes the negative node set where nodes are not\nin \ud835\udc3a\n\ud835\udc57\n. This mutual information maximization formulation is originally developed for unsupervised\nlearning and it can be simply extended for semi-supervised graph classification. In particular,\nInfoGraph utilizes a teacher-student architecture that compares the representation across the\nteacher and student networks. The contrastive learning objective serves as a regularization by\ncombining with supervised loss.\nDual Space Graph Contrastive Learning (DSGC) [527]. DSGC is a representative contrastive\nlearning-based method. It utilizes two graph encoders. The first is a standard GNN encoder in the\nEuclidean space and the second is the hyperbolic GNN encoder. The hyperbolic GNN encoder first\nconverts graph embeddings into hyperbolic space and then measures the distance based on the\nlength of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic\nspace. Assuming the two GNNs are named as \ud835\udc531 (\u00b7) and \ud835\udc532 (\u00b7), the positive pair is denoted as:\nz\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n= exp\ud835\udc50\no\n(\ud835\udc531 (\ud835\udc3a\n\ud835\udc57\n)),\nz\n\ud835\udc57\n\ud835\udc3b\n= exp\ud835\udc50\no\n\ud835\udc532 (\ud835\udc3a\n\ud835\udc57\n)\n\u0001\n.\n(77)\nThen it selects one labeled sample and \ud835\udc41\ud835\udc35 unlabeled sample \ud835\udc3a\n\ud835\udc57\nfor graph contrastive learning in\nthe hyperbolic space. In formulation,\nL\ud835\udc37\ud835\udc46\ud835\udc3a\ud835\udc36 = \u2212 log e\n\ud835\udc51\n\ud835\udc3b\n(h\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f\ne\n\ud835\udc51\ud835\udc3b (z\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f +\n\u00cd\ud835\udc41\n\ud835\udc56=1\ne\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc3b\n\u0011\n/\ud835\udf0f\n\u2212\n\ud835\udf06\ud835\udc62\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56=1\nlog e\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\ne\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n+ e\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n,\n(78)\nwhere z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\nand z\n\ud835\udc56\n\ud835\udc3b\ndenote the embeddings for labeled graph sample\ud835\udc3a\n\ud835\udc56\nand \ud835\udc51\n\ud835\udc3b (\u00b7) denotes a distance\nmetric in the hyperbolic space. This contrastive learning objective maximizes the similarity between\nembeddings learned from two encoders compared with other samples. Finally, the contrastive\nlearning objective can be combined with the supervised loss to achieve effective semi-supervised\ncontrastive learning.\nActive Semi-supervised Graph Neural Network (ASGN) [166]. ASGN utilizes a teacher-student\narchitecture with the teacher model focusing on representation learning and the student model\ntargeting at molecular property prediction. In the teacher model, ASGN first employs a message\npassing neural network to learn node representations under the reconstruction task and then\nborrows the idea of balanced clustering to learn graph-level representations in a self-supervised\nfashion. In the student model, ASGN utilizes label information to monitor the model training based\non the weights of the teacher model. In addition, active learning is also used to minimize the\nannotation cost while maintaining sufficient performance. Typically, the teacher model seeks to\nprovide discriminative graph-level representations without labels, which transfer knowledge to the\nstudent model to overcome the potential overfitting in the presence of label scarcity.\nTwin Graph Neural Networks (TGNN) [218]. TGNN also uses two graph neural networks to\ngive different perspectives to learn graph representations. Differently, it adopts a graph kernel\nneural network to learn graph-level representations in virtue of random walk kernels. Rather than\ndirectly enforcing representation from two modules to be similar, TGNN exchanges information\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n32 W. Ju, et al.\nby contrasting the similarity structure of the two modules. In particular, it constructs a list of\nanchor graphs, \ud835\udc3a\n\ud835\udc4e1\n,\ud835\udc3a\ud835\udc4e2, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc4e\ud835\udc40 , and utilizes two graph encoders to produce their embeddings,\ni.e., {\ud835\udc67\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n, {\ud835\udc64\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n. Then it calculates the similarity distribution between each unlabeled\ngraph and anchor graphs for two modules. Formally,\n\ud835\udc5d\n\ud835\udc57\n\ud835\udc5a =\nexp cos \ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (\ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n, (79)\n\ud835\udc5e\n\ud835\udc57\n\ud835\udc5a =\nexp cos w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n. (80)\nThen, TGNN minimizes the distance between distributions from different modules as follows:\nL\ud835\udc47\ud835\udc3a\ud835\udc41 \ud835\udc41 =\n1\nG\ud835\udc48\n\u2211\ufe01\n\ud835\udc3a \ud835\udc57 \u2208 G\ud835\udc62\n1\n2\n\ud835\udc37KL\np\n\ud835\udc57\n\u2225q\n\ud835\udc57\n\u0001\n+ \ud835\udc37KL\nq\n\ud835\udc57\n\u2225p\n\ud835\udc57\n\u0001\u0001 , (81)\nwhich serves as a regularization term to combine with the supervised loss.\n7.3 Summary\nThis section introduces semi-supervised learning for graph representation learning and we provide\nthe summary as follows:\n\u2022 Techniques. Classic node classification aims to conduct transductive learning on graphs\nwith access to unlabeled data, which is a natural semi-supervised problem. Semi-supervised\ngraph classification aims to relieve the requirement of abundant labeled graphs. Here, a\nvariety of semi-supervised methods have been put forward to achieve better performance\nunder the label scarcity. Typically, they try to integrate semi-supervised techniques such as\nactive learning, pseudo-labeling, consistency learning, and consistency learning with graph\nrepresentation learning.\n\u2022 Challenges and Limitations. Despite their great success, the performance of these methods\nis still unsatisfactory, especially in graph-level representation learning. For example, DSGC\ncan only achieve an accuracy of 57% in a binary classification dataset REDDIT-BINARY. Even\nworse, label scarcity is often accompanied by unbalanced datasets and potential domain\nshifts, which provides more challenges from real-world applications.\n\u2022 Future Works. In the future, we expect that these methods can be applied to different\nproblems such as molecular property predictions. There are also works to extend graph\nrepresentation learning in more realistic scenarios like few-shot learning [51, 326]. A higher\naccuracy is always anticipated for more advanced and effective semi-supervised techniques.\n8 Graph Self-supervised Learning\nBesides supervised or semi-supervised methods, self-supervised learning (SSL) also has shown its\npowerful capability in data mining and representation embedding in recent years. In this section,\nwe investigated Graph Neural Networks based on SSL and provided a detailed introduction to a\nfew typical models. Graph SSL methods usually have a unified pipeline, which includes pretext\ntasks and downstream tasks. Pretext tasks help the model encoder to learn better representation,\nas a premise of better performance in downstream tasks. So a delicate design of pretext task is\ncrucial for Graph SSL. We would firstly introduce the overall framework of Graph SSL in Section 8.1,\nthen introduce the two kinds of pretext task design, generation-based methods and contrast-based\nmethods respectively in Section 8.2 and 8.3. A summarisation is provided in Table 7.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 33\nTable 7. Summary of methods for self-supervised Learning on Graphs. \"PT\", \"CT\" and \"UFE\" mean \"Pre\u0002training\", \"Collaborative Train\" and \"Unsupervised Feature Extracting\" respectively.\nApproach Augmentation Scheme Training Scheme Generation Target Objective Function\nGeneration-based\nGraph Completion [544] Feature Mask PT/CT Node Feature -\nAttributeMask [208] Feature Mask PT/CT PCA Node Feature -\nAttrMasking [181] Feature Mask PT Node/Edge Feature -\nMGAE [459] No Augmentation CT Node Feature -\nGAE [231] Feature Noise UFE Adjacency Matrix -\nContrast-based\nDeepWalk [362] Random Walk UFE - SkipGram\nLINE [443] Random Walk UFE - Jensen-Shannon\nGCC [375] Random Walk PT/URL - InfoNCE\nSimGCL [547] Embedding Noise UFE - InfoNCE\nSimGRACE [503] Model Noise UFE - InfoNCE\nGCA [612]\nFeature Masking &\nStructure Adjustment URL - InfoNCE\nBGRL [152]\nFeature Masking &\nStructure Adjustment URL - BYOL\n8.1 Overall framework\nConsider a featured graph G, we denote a graph encoder \ud835\udc53 to learn the representation of the\ngraph, and a pretext decoder \ud835\udc54 with specific architecture in different pretext tasks. Then the pretext\nself-supervised learning loss can be formulated as:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = \ud835\udc38G\u223cD [L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G)], (82)\nwhere D denotes the distribution of featured graph G. By minimizing L\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59 , we can learn encoder\n\ud835\udc53 with capacity to produce high-quality embedding. As for downstream tasks, we denote a graph\ndecoder \ud835\udc51 which transforms the output of graph encoder \ud835\udc53 into model prediction. The loss of\ndownstream tasks can be formulated as:\nL\ud835\udc60\ud835\udc62\ud835\udc5d = L\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66), (83)\nwhere \ud835\udc66 is the ground truth in downstream tasks. We can obverse that L\ud835\udc60\ud835\udc62\ud835\udc5d is a typical supervised\nloss. To ensure the model achieves wise graph representation extraction and optimistic prediction\nperformance, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d have to be minimized simultaneously. We introduce 3 different ways\nto minimize the two loss functions:\nPre-training. This strategy has two steps. In pre-training step, the L\ud835\udc60\ud835\udc60\ud835\udc59 is minimized to get \ud835\udc54\n\u2217\nand \ud835\udc53\n\u2217\n:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D). (84)\nThen the parameter of \ud835\udc53\n\u2217\nis kept to continue training in pretext supervised learning progress.\nThe supervised loss is minimized to get the final parameters of \ud835\udc53 and \ud835\udc51.\nmin\n\ud835\udc51,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc51, \ud835\udc53 |\ud835\udc530=\ud835\udc53\n\u2217 , G;\ud835\udc66). (85)\nCollaborative Train. In this strategy, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d are optimized simultaneously. A hyper\u0002parameter \ud835\udefc is used to balance the contribution of pretext task loss and downstream task loss.\nThe overall minimization strategy is like the traditional supervised strategy with a pretext task\nregularization:\nmin\n\ud835\udc54,\ud835\udc53 ,\ud835\udc51\n[L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G) + \ud835\udefcL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66)]. (86)\nUnsupervised Feature Extracting. This strategy is similar to the Pre-training and Fine-tuning\nstrategy in the first step to minimize pretext task loss L\ud835\udc60\ud835\udc60\ud835\udc59 and get \ud835\udc53\n\u2217\n. However, when minimizing\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n34 W. Ju, et al.\ndownstream loss L\ud835\udc60\ud835\udc62\ud835\udc5d , the encoder \ud835\udc53\n\u2217\nis fixed. Also, the training graph data are on the same dataset,\nwhich differs from the Pre-training and Fine-tuning strategy. The formulation is defined as:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D), (87)\nmin\n\ud835\udc51\nL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 \u2217, G;\ud835\udc66). (88)\n8.2 Generation-based pretext task design\nIf a model with an encoder-decoder structure can reproduce certain graph features from an in\u0002complete or perturbed graph, it indicates the encoder has the ability to extract useful graph\nrepresentation. This motivation is derived from Autoencoder [174] which originally learns on\nimage dataset. In such a case, Eq. 84 can be rewritten as:\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54(\ud835\udc53 (G)) \u02c6 , G), (89)\nwhere \ud835\udc53 (\u00b7) and \ud835\udc54(\u00b7) stand for the representation encoder and rebuilding decoder. However, feature\ninformation and structure information are both important compositions suitable to be rebuilt for\ngraph datasets. So generation-based pretext can be divided into two categories: feature rebuilding\nand structure rebuilding. We introduce several outstanding models in the following part.\nGraph Completion [544] is one of the representative methods of feature rebuilding. They mask\nsome node features to generate an incomplete graph. Then the pretext task is set as predicting the\nremoved node features. As shown in Eq. 90, this method can be formulated as a special case of\nEq. 90, letting G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and replacing G \u2212\u2192 \ud835\udc4b. The loss function is often Mean Squared Error or\nCross Entropy, depending on whether the feature is continuous or binary.\nmin\n\ud835\udc54,\ud835\udc53\nMSE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , X). (90)\nOther works make some changes to the feature settings. For example, AttrMasking [181] aims\nto rebuild both node representation and edge representation, AttributeMask [208] preprocess \ud835\udc4b\nfirstly by PCA to reduce the complexity of rebuilding features.\nOn the other hand, MGAE [459] modifies the original graph by adding noise in node representa\u0002tion, motivated by denoising autoencoder [454]. As shown in Eq. 90, we can also consider MGAE as\nan implement of Eq. 84 where G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and G \u2212\u2192 \ud835\udc4b. \ud835\udc4b\u02c6 stands for perturbed node representation.\nSince the noise is independent and random, the encoder is more robust to feature input.\nmin\n\ud835\udc54,\ud835\udc53\nBCE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , A). (91)\nAs for structure rebuilding methods, GAE [231] is the simplest instance, which can be regarded as\nan implement of Eq. 84 where G\u02c6 = G and G \u2212\u2192 \ud835\udc34. \ud835\udc34 is the adjacency matrix of the graph. Similar to\nfeature rebuilding methods, GAE compresses raw node representation vectors into low-dimensional\nembedding with its encoder. Then the adjacency matrix is rebuilt by computing node embedding\nsimilarity. The loss function is set to the error between the ground-truth adjacency matrix and\nthe recovered one, to help the model rebuild the correct graph structure. Other feature rebuilding\nmethods [558] and structure rebuilding methods [440, 487] are also increasingly being developed\nacross numerous related publications.\n8.3 Contrast-Based pretext task design\nThe mutual information maximization principle, which implements self-supervising by predicting\nthe similarity between the two augmented views, forms the foundation of contrast-based approaches.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 35\nSince mutual information represents the degree of correlation between two samples, we can\nmaximize it in augmented pairs and minimize it in random-selected pairs.\nThe contrast-based graph SSL taxonomy can be formulated as Eq. 92. The discriminator that\ncalculates the similarity of sample pairs is indicated by pretext decoder \ud835\udc54. G\n(1)\nand G\n(2)\nare two\nvariants of \ud835\udc3a that have been augmented. Since graph contrastive learning methods differ from each\nother in 1) view generation, 2) MI estimation method we introduce this methodology in these two\nperspectives.\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54[\ud835\udc53 (G\u02c6(1)), \ud835\udc53 (G\u02c6(2))]). (92)\nThe domain of contrastive-based graph SSL is witnessing an expanding body of work in a\ngrowing number of methods [176, 197, 531, 587] and applications [114, 136, 504].\n8.3.1 View generation. The traditional pipeline of contrastive learning-based models first involves\naugmenting the graph using well-crafted empirical methods, and then maximizing the consistency\nbetween different augmentations. Drawing from methods in the computer vision domain and\nconsidering the non-Euclidean structure of graph data, typical graph augmentation methods aim\nto modify the graph topologically or representationally.\nGiven graph G = (\ud835\udc34, \ud835\udc4b), the topologically augmentation methods usually modify the adjacency\nmatrix \ud835\udc34, which can be formulated as:\n\ud835\udc34\u02c6 = \ud835\udcaf\ud835\udc34 (\ud835\udc34), (93)\nwhere \ud835\udcaf\ud835\udc34 (\u00b7) is the transform function of adjacency matrix. Topology augmentation methods\nhave many variants, in which the most popular one is edge modification, given by \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\ud835\udc43 \u25e6 \ud835\udc34 + \ud835\udc44 \u25e6 (1 \u2212 \ud835\udc34). \ud835\udc43 and \ud835\udc44 are two matrices representing edge dropping and adding respectively.\nAnother method, graph diffusion, connect nodes with their k-hop neighbors with specific weight,\ndefined as: \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\u00cd\u221e\n\ud835\udc58=0\n\ud835\udefc\ud835\udc58\ud835\udc47\n\ud835\udc58\n. where \ud835\udefc and\ud835\udc47 are coefficient and transition matrix. Graph diffusion\nmethod can integrate broad topological information with local structure.\nOn the other hand, the representative augmentation modifies the node representation directly,\nwhich can be formulated as:\n\ud835\udc4b\u02c6 = \ud835\udcaf\ud835\udc4b (\ud835\udc4b), (94)\nusually \ud835\udcaf\ud835\udc4b (\u00b7) can be a simple masking operater, a.k.a. \ud835\udcaf\ud835\udc4b (\ud835\udc4b) = \ud835\udc40 \u25e6 \ud835\udc4b and \ud835\udc40 \u2208 {0, 1}\n\ud835\udc41 \u00d7\ud835\udc37 . Based\non such mask strategy, some methods propose ways to improve performance. GCA [612] preserves\ncritical nodes while giving less significant nodes a larger masking probability, where significance is\ndetermined by node centrality.\nAs introduced before, the paradigm of augmentation has been proven to be effective in contrastive\nlearning view generation. However, given the variety of graph data, it is challenging to maintain\nsemantics properly during augmentations. To preserve the valuable nature of specific graph datasets,\nThere are currently three mainly used methods: picking by trial-and-errors, trying laborious search,\nor seeking domain-specific information as guidance [214, 308, 311]. Such complicated augmentation\nmethods constrain the effectiveness and widespread application of graph contrastive learning. So\nmany newest works question the necessity of augmentation and seek other contrastive view\ngeneration methods.\nSimGCL [547] is one of the outstanding works challenging the effectiveness of graph augmenta\u0002tion. The author finds that noise can be a substitution to augmentation to produce graph views\nin specific tasks such as recommendation. After doing an ablation study about augmentation and\nInfoNCE [510], they find that the InfoNCE loss, not the augmentation of the graph, is what makes\nthe difference. It can be further explained by the importance of distribution uniformity. Contrastive\nlearning enhances model representation ability by intensifying two characteristics: The alignment\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n36 W. Ju, et al.\nof features from positive samples and the uniformity of the normalized feature distribution. SimGCL\ndirectly adds random noises to node embeddings as augmentation, to control the uniformity of the\nrepresentation distribution more effectively:\ne\n(1)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(1)\n\u2217 \ud835\udf0f\n(1)\n\ud835\udc56\n, e\n(2)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(2)\n\u2217 \ud835\udf0f\n(2)\n\ud835\udc56\n,\n\ud835\udf16 \u223c N (0, \ud835\udf0e2),\n(95)\nwhere e\ud835\udc56is a node representation in embedding space, \ud835\udf0f\n(1)\n\ud835\udc56\nand \ud835\udf0f\n(2)\n\ud835\udc56\nare two random sampled unit\nvector. The experiment results indicate that SimGCL performs better than its graph augmentation\u0002based competitors, while training time is significantly decreased.\nSimGRACE [503] is another graph contrastive learning framework without data augmentation.\nMotivated by the observation that despite encoder disruption, graph data can effectively maintain\ntheir semantics, SimGRACE takes GNN with its modified version as an encoder to produce two\ncontrastive embedding views by the same graph input. For GNN encoder \ud835\udc53 (\u00b7; \ud835\udf03), the two contrastive\nembedding views e, e\n\u2032\ncan be computed by:\ne\n(1) = \ud835\udc53 (G; \ud835\udf03), e(2) = \ud835\udc53 (G; \ud835\udf03 + \ud835\udf16 \u00b7 \u0394\ud835\udf03),\n\u0394\ud835\udf03\ud835\udc59 \u223c N (0, \ud835\udf0e2\n\ud835\udc59\n),\n(96)\nwhere \u0394\ud835\udf03\ud835\udc59 represents GNN parameter perturbation \u0394\ud835\udf03 in the \ud835\udc59th layer. SimGRACE can improve\nalignment and uniformity simultaneously, proving its capacity to produce high-quality embedding.\n8.3.2 MI estimation method. The mutual information \ud835\udc3c(\ud835\udc65, \ud835\udc66) measures the information that x and y\nshare, given a pair of random variables (\ud835\udc65, \ud835\udc66). As discussed before, mutual information is a significant\ncomponent of the contrast-based method by formulating the loss function. Mathematically rigorous\nMI is defined on the probability space, we can formulate mutual information between a pair of\ninstances (\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) as:\n\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \ud835\udc37\ud835\udc3e\ud835\udc3f (\ud835\udc5d(\ud835\udc65, \ud835\udc66)||\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66))\n= \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc5d(\ud835\udc65, \ud835\udc66)\n\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66)\n].\n(97)\nHowever, directly computing Eq. 97 is quite difficult, so we introduce several different types of\nestimation for MI:\nInfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimator. Given a\npositive sample \ud835\udc66 and several negative sample \ud835\udc66\n\u2032\n\ud835\udc56\n, a noise-contrastive estimator can be formulated\nas [611][375]:\nL = \u2212\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \u2212\ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66)\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66) +\n\u00cd\n\ud835\udc56\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66\u2032\n\ud835\udc56\n)\n], (98)\nusually the kernal function \ud835\udc54(\u00b7) can be cosine similarity or dot product.\nTriplet Loss. Intuitively, we can aim to create a distinct separation in the degree of similarity,\nensuring that positive samples are closer together and negative samples are further apart by a\ncertain distance. So we can define the loss function in the following manner [204]:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [max(\ud835\udc54(\ud835\udc65, \ud835\udc66) \u2212 \ud835\udc54(\ud835\udc65, \ud835\udc66\u2032) + \ud835\udf16, 0)], (99)\nwhere \ud835\udf16 is a hyperparameter. This function is straightforward to compute.\nBYOL Loss. Estimation without negative samples is investigated by BYOL [152]. The estimator\nis Asymmetrically structured:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [2 \u2212 2\n\ud835\udc54(\ud835\udc65) \u00b7 \ud835\udc66\n\u2225\ud835\udc54(\ud835\udc65) \u2225 \u2225\ud835\udc66\u2225\n], (100)\nnote that encoder \ud835\udc54 should keep the dimension of input and output the same.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 37\n8.4 Summary\nThis section introduces graph self-supervised learning and we provide the summary as follows:\n\u2022 Techniques. Differing from classic supervised and semi-supervised learning, self-supervised\nlearning increases a model\u2019s generalization ability and robustness while decreasing reliance\non labels. Graph SSL utilizes pretext tasks to extract inherent information from representation\ndistributions. Typical Graph SSL methods can be divided into generation-based and contrast\u0002based approaches. Generation-based methods learn an encoder with the ability to reconstruct\na graph as precisely as possible, motivated by the principles of Autoencoder. Contrast-based\nmethods, which have recently attracted significant interest, involve learning an encoder to\nminimize mutual information between relevant instances and maximize mutual information\nbetween unrelated instances.\n\u2022 Challenges and Limitations. Although graph SSL has achieved superior performance in\nmany tasks, its theoretical basis is not so solid. Many well-known methods are validated only\nthrough experiments, without providing theoretical explanations or mathematical proofs. It\nis imperative to establish a strong theoretical foundation for graph SSL.\n\u2022 Future Works. In the future we expect more graph ssl methods designed essentially by\ntheoretical proof, without dedicated designed augment process or pretext tasks by intuition.\nThis will bring us more definite mathematical properties and a less ambiguous empirical\nsense. Also, graphs are a prevalent form of data representation across diverse domains, yet\nobtaining manual labels can be prohibitively expensive. Expanding the applications of graph\nSSL to broader fields is a promising avenue for future research.\n9 Graph Structure Learning\nGraph structure determines how node features propagate and affect each other, playing a crucial\nrole in graph representation learning. In some scenarios the provided graph is incomplete, noisy, or\neven has no structure information at all. Recent research also finds that graph adversarial attacks\n(i.e., modifying a small number of node features or edges), can degrade learned representations\nsignificantly. These issues motivate graph structure learning (GSL), which aims to learn a new\ngraph structure to produce optimal graph representations. According to how edge connectivity is\nmodeled, there are three different approaches in GSL, namely metric-based approaches, model-based\napproaches, and direct approaches. Besides edge modeling, regularization is also a common trick to\nmake the learned graph satisfy some desired properties. We first present the basic framework and\nregularization methods for GSL in Sec. 9.1 and Sec. 9.2, respectively, and then introduce different\ncategories of GSL in Sec. 9.3, 9.4 and 9.5. We summarize GSL approaches in Table 8.\n9.1 Overall Framework\nWe denote a graph by G = (A, X), where A \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is the adjacency matrix and X \u2208 R\ud835\udc41 \u00d7\ud835\udc40 is\nthe node feature matrix with \ud835\udc40 being the dimension of each node feature. A graph encoder \ud835\udc53\ud835\udf03\nlearns to represent the graph based on node features and graph structure for task-specific objective\nL\ud835\udc61 (\ud835\udc53\ud835\udf03 (A, X)). In the GSL setting, there is also a graph structure learner which aims to build a\nnew graph adjacency matrix A\n\u2217\nto optimize the learned representation. Besides the task-specific\nobjective, a regularization term can be added to constrain the learned structure. So the overall\nobjective function of GSL can be formulated as\nmin\n\ud835\udf03,A\u2217\nL = L\ud835\udc61 (\ud835\udc53\ud835\udf03 (A\n\u2217\n, X)) + \ud835\udf06L\ud835\udc5f (A\n\u2217\n, A, X), (101)\nwhere L\ud835\udc61is the task-specific objective, L\ud835\udc5fis the regularization term and \ud835\udf06 is a hyperparameter for\nthe weight of regularization.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n38 W. Ju, et al.\nTable 8. Summary of graph structure learning methods.\nMethod Structure Learning Regularization\nSparsity Low-rank Smoothness\nMetric-based\nAGCN [267] Mahalanobis distance\nGRCN [546] Inner product \u2713\nCAGCN [613] Inner product \u2713\nGNNGUARD [571] Cosine similarity\nIDGL [65] Cosine similarity \u2713 \u2713 \u2713\nHGSL [586] Cosine similarity \u2713\nGDC [139] Graph diffusion \u2713\nModel-based\nGLN [364] Recurrent blocks\nGLCN [199] One-layer neural network \u2713 \u2713\nNeuralSparse [595] Multi-layer neural network \u2713\nGAT [452] Self-attention\nGaAN [566] Gated attention\nhGAO [132] Hard attention \u2713\nVIB-GSL [433] Dot-product attention \u2713\nMAGNA [461] Graph attention diffusion\nDirect\nGLNN [135] MAP estimation \u2713 \u2713\nPro-GNN [210] Direct optimization \u2713 \u2713 \u2713\nGSML [458] Bilevel optimization \u2713\nLSD-GNN [124] Bilevel optimization\nBGCNN [573] Bayesion optimization\nVGCN [104] Stochastic variational inference\n9.2 Regularization\nThe goal of regularization is to constrain the learned graph to satisfy some properties by adding\nsome penalties to the learned structure. The most common properties used in GSL are sparsity, low\nlank, and smoothness.\n9.2.1 Sparsity Noise or adversarial attacks will introduce redundant edges into graphs and degrade\nthe quality of graph representation. An effective technique to remove unnecessary edges is sparsity\nregularization, i.e., adding a penalty on the number of nonzero entries of the adjacency matrix\n(\u21130-norm) [458, 546, 586, 595]:\nL\ud835\udc60\ud835\udc5d = \u2225A\u22250, (102)\nhowever, \u21130-norm is not differentiable so optimizing it is difficult, and in many cases \u21131-norm\nis used instead as a convex relaxation. Other methods to impose sparsity include pruning and\ndiscretization [124, 613]. These processes are also called postprocessing since they usually happen\nafter the adjacency matrix is learned. Pruning removes part of the edges according to some crite\u0002ria [613]. For example, edges with weights lower than a threshold, or those not in the top-K edges\nof nodes or graphs. Discretization is applied to generate graph structure by sampling from some\ndistribution [124]. Compared to directly learning edge weights, sampling enjoys the advantage\nof controlling the generated graph, but has issues during optimizing since sampling itself is dis\u0002crete and hard to optimize. Reparameterization and Gumbel-softmax are two useful techniques to\novercome such issues, and are widely adopted in GSL.\n9.2.2 Low Rank In real-world graphs, similar nodes are likely to group together and form commu\u0002nities, which should lead to a low-rank adjacency matrix. Recent work also finds that adversarial\nattacks tend to increase the rank of the adjacency matrix quickly [65, 210]. Therefore, low-rank\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 39\nregularization is also a useful tool to make graph representation learning more robust:\nL\ud835\udc59\ud835\udc5f = \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc58 (A). (103)\nIt is hard to minimize matrix rank directly. A common technique is to optimize the nuclear norm,\nwhich is a convex envelope of the matrix rank:\nL\ud835\udc5b\ud835\udc50 = \u2225A\u2225\u2217 =\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56\n\ud835\udf0e\ud835\udc56, (104)\nwhere \ud835\udf0e\ud835\udc56 are singular values of A. Entezari et al. replaces the learned adjacency matrix with rank-r\napproximation by singular value decomposition (SVD) to achieve robust graph learning against\nadversarial attacks.\n9.2.3 Smoothness A common assumption is that connected nodes share similar features, or in other\nwords, the graph is \u201csmooth\u201d as the difference between local neighbors is small [65, 135, 199, 210].\nThe following metric is a natural way to measure graph smoothness:\nL\ud835\udc60\ud835\udc5a =\n1\n2\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56,\ud835\udc57=1\n\ud835\udc34\ud835\udc56\ud835\udc57 (\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n2 = \ud835\udc61\ud835\udc5f(X\u22a4\n(D \u2212 A)X) = \ud835\udc61\ud835\udc5f(X\n\u22a4\nLX), (105)\nwhere D is the degree matrix of A and L = D \u2212 A is called graph Laplac    ЭеучеЭЖ ЭФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр КузкуыутефешщтётДуфктштпётЦУШ ОГб ЯРУТП АФТПб НШНФТП ПГб ЯУЙГТ ДШГб фтв ЙШТПЙШТП ДЩТПб Зулштп ГтшмукышенбётСрштфётЯШНГУ ЙШФЩб Еру Рщтп Лщтп Гтшмукышен ща Ысшутсу фтв Еусртщдщпнб СрштфётНШАФТП ЙШТ фтв ОШФТРФЩ ЫРУТб Зулштп Гтшмукышенб СрштфётАФТП ЫГТ фтв ЯРШЗШТП ЧШФЩб Гтшмукышен ща Сфдшащктшфб Дщы Фтпудуыб ГЫФётОГТЦУШ НФТПб ОШТПНФТП НГФТб фтв НГЫРУТП ЯРФЩб Зулштп Гтшмукышенб СрштфётНШАФТ ЦФТПб Гтшмукышен ща Штеуктфешщтфд Игыштуыы фтв Усщтщьшсыб СрштфётЧШФЩ ДГЩёг2217б Гтшмукышен ща Сфдшащктшфб Дщы Фтпудуыб ГЫФётЬШТП ЯРФТПёг2217б Зулштп Гтшмукышенб СрштфётПкфзр кузкуыутефешщт дуфктштп фшьы ещ уааусешмудн утсщву ршпр-вшьутышщтфд ызфкыу пкфзр-ыекгсегкув вфеф штещётдщц-вшьутышщтфд вутыу мусещкыб цршср шы ф агтвфьутефд ефыл ерфе рфы иуут цшвудн ыегвшув шт ф кфтпу ща ашудвыбётштсдгвштп ьфсршту дуфктштп фтв вфеф ьштштпю Сдфыышс пкфзр уьиуввштп ьуерщвы ащддщц еру ифышс швуф ерфе еруётуьиуввштп мусещкы ща штеуксщттусеув тщвуы шт еру пкфзр сфт ыешдд ьфштефшт ф кудфешмудн сдщыу вшыефтсуб ерукуинётзкуыукмштп еру ыекгсегкфд штащкьфешщт иуецуут еру тщвуы шт еру пкфзрю Рщцумукб ершы шы ыги-щзешьфд вгу ещЖ (ш)ётекфвшешщтфд ьуерщвы рфму дшьшеув ьщвуд сфзфсшен цршср дшьшеы еру дуфктштп зукащкьфтсуж (шш) учшыештп еусртшйгуыётензшсфддн кудн щт гтыгзукмшыув дуфктштп ыекфеупшуы фтв афшд ещ сщгзду цшер еру дфеуые дуфктштп зфкфвшпьыж (шшш)ёткузкуыутефешщт дуфктштп фтв вщцтыекуфь ефылы фку вузутвуте щт уфср щерук цршср ырщгдв иу ощштедн утрфтсувюётЦшер еру куьфклфиду ыгссуыы ща вууз дуфктштпб вууз пкфзр кузкуыутефешщт дуфктштп рфы ырщцт пкуфе зщеутешфдётфтв фвмфтефпуы щмук ырфддщц (екфвшешщтфд) ьуерщвыб еруку учшые ф дфкпу тгьиук ща вууз пкфзр кузкуыутефешщтётдуфктштп еусртшйгуы рфму иуут зкщзщыув шт еру зфые вусфвуб уызусшфддн пкфзр тугкфд туецщклыю Шт ершы ыгкмунбётцу сщтвгсе ф сщьзкурутышму ыгкмун щт сгккуте вууз пкфзр кузкуыутефешщт дуфктштп фдпщкшерьы ин зкщзщыштп фёттуц ефчщтщьн ща учшыештп ыефеу-ща-еру-фке дшеукфегкую Ызусшашсфдднб цу ыныеуьфешсфддн ыгььфкшяу еру уыыутешфдётсщьзщтутеы ща пкфзр кузкуыутефешщт дуфктштп фтв сфеупщкшяу учшыештп фззкщфсруы ин еру цфны ща пкфзр тугкфдёттуецщкл фксршеусегкуы фтв еру ьщые кусуте фвмфтсув дуфктштп зфкфвшпьыю Ьщкущмукб ершы ыгкмун фдыщ зкщмшвуыётеру зкфсешсфд фтв зкщьшыштп фзздшсфешщты ща вууз пкфзр кузкуыутефешщт дуфктштпю Дфые иге тще дуфыеб цу ыефеуёттуц зукызусешмуы фтв ыгппуые срфддутпштп вшкусешщты цршср вуыукму агкерук штмуыешпфешщты шт еру агегкуюётССЫ СщтсузеыЖ ёг2022 Сщьзгештп ьуерщвщдщпшуы ёг2192 Тугкфд туецщклыж Дуфктштп дфеуте кузкуыутефешщтыюётёг2217Сщккуызщтвштп фгерщкыюётФгерщкыёг2019 фввкуыыуыЖ Цуш Огб огцуш"злгюувгюстж Ярутп Афтпб афтп_я"злгюувгюстж Ншнфтп Пгб ншнфтппг"злгюувгюстжётЯуйгт Дшгб яуйгтдшг"злгюувгюстж Йштпйштп Дщтпб йштпйштпдщтп"злгюувгюстб Зулштп Гтшмукышенб Иушоштпб Срштфб 100871жётЯшнгу Йшфщб яшнгуощу"пьфшдюсщьб Еру Рщтп Лщтп Гтшмукышен ща Ысшутсу фтв Еусртщдщпнб Пгфтпярщгб Срштфб 511453жётНшафтп Йштб йштншафтп"злгюувгюстж Ошфтрфщ Ырутб орырут"злгюувгюстб Зулштп Гтшмукышенб Иушоштпб Срштфб 100871* АфтпётЫгтб аеы"сыюгсдфюувгж Яршзштп Чшфщб зфекшсшфючшфщ"сыюгсдфюувгб Гтшмукышен ща Сфдшащктшфб Дщы Фтпудуыб ГЫФб 90095* ОгтцушётНфтпб ноцерущтдн"злгюувгюстж Оштпнфтп Нгфтб нгфтон"злгюувгюстж Нгырутп Ярфщб нгырутпюярфщ"ыегюзлгюувгюстб ЗулштпётГтшмукышенб Иушоштпб Срштфб 100871* Ншафт Цфтпб ншафтцфтп"гшиуюувгюстб Гтшмукышен ща Штеуктфешщтфд Игыштуыы фтв УсщтщьшсыбётИушоштпб Срштфб 100029* Чшфщ Дгщб чшфщдгщ"сыюгсдфюувгб Гтшмукышен ща Сфдшащктшфб Дщы Фтпудуыб ГЫФб 90095* Ьштп Ярфтпбётьярфтп_сы"злгюувгюстб Зулштп Гтшмукышенб Иушоштпб Срштфб 100871юётЗукьшыышщт ещ ьфлу вшпшефд щк рфкв сщзшуы ща фдд щк зфке ща ершы цщкл ащк зукыщтфд щк сдфыыкщщь гыу шы пкфтеув цшерщге аууётзкщмшвув ерфе сщзшуы фку тще ьфву щк вшыекшигеув ащк зкщаше щк сщььуксшфд фвмфтефпу фтв ерфе сщзшуы иуфк ершы тщешсу фтвётеру агдд сшефешщт щт еру ашкые зфпую Сщзнкшпреы ащк сщьзщтутеы ща ершы цщкл щцтув ин щерукы ерфт ФСЬ ьгые иу рщтщкувюётФиыекфсештп цшер скувше шы зукьшееувю Ещ сщзн щерукцшыуб щк кузгидшырб ещ зщые щт ыукмукы щк ещ кувшыекшигеу ещ дшыеыб куйгшкуыётзкшщк ызусшашс зукьшыышщт фтв/щк ф ауую Куйгуые зукьшыышщты акщь зукьшыышщты"фсьющкпюётёг00ф9 2024 Фыыщсшфешщт ащк Сщьзгештп Ьфсрштукнюёт0004-5411/2024/2-ФКЕ %15ю00ётреезыЖ//вщшющкп/ЧЧЧЧЧЧЧюЧЧЧЧЧЧЧётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётфкЧшмЖ2304ю05055м3 хсыюДПъ 28 Ауи 2024ёт2 Цю Огб уе фдюётФввшешщтфд Лун Цщквы фтв ЗркфыуыЖ Вууз Дуфктштп щт Пкфзрыб Пкфзр Кузкуыутефешщт Дуфктштпб Пкфзр ТугкфдётТуецщклб ЫгкмунётФСЬ Куаукутсу АщкьфеЖётЦуш Огб Ярутп Афтпб Ншнфтп Пгб Яуйгт Дшгб Йштпйштп Дщтпб Яшнгу Йшфщб Ншафтп Йштб Ошфтрфщ Ырутб Афтп ЫгтбётЯршзштп Чшфщб Огтцуш Нфтпб Оштпнфтп Нгфтб Нгырутп Ярфщб Ншафт Цфтпб Чшфщ Дгщб фтв Ьштп Ярфтпю 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштпю Ою ФСЬ 1^ 1 (Ауикгфкн 2024)^ 100 зфпуыюётреезыЖ//вщшющкп/ЧЧЧЧЧЧЧюЧЧЧЧЧЧЧёт1 ШтекщвгсешщтётПкфзры рфму кусутедн уьукпув фы ф зщцукагд ещщд ащк кузкуыутештп ф мфкшуен ща ыекгсегкув фтвётсщьздуч вфефб штсдгвштп ыщсшфд туецщклыб екфаашс туецщклыб штащкьфешщт ыныеуьыб лтщцдувпу пкфзрыбётзкщеушт-зкщеушт штеукфсешщт туецщклыб фтв зрнышсфд штеукфсешщт туецщклыю Фы ф лштв ща путукфд ащкьётща вфеф щкпфтшяфешщтб пкфзр ыекгсегкуы фку сфзфиду ща тфегкфддн учзкуыыштп еру штекштышс кудфешщтыршзётща еруыу вфефб фтв ергы сфт срфкфсеукшяу здутен ща тщт-Угсдшвуфт ыекгсегкуы ерфе фку скгсшфд штётф мфкшуен ща вшысшздштуы фтв вщьфшты вгу ещ ерушк адучшиду фвфзефишдшеню Ащк учфьздуб ещ утсщву фётыщсшфд туецщкл фы ф пкфзрб тщвуы щт еру пкфзр фку гыув ещ кузкуыуте штвшмшвгфд гыукыб фтв увпуы фкуётгыув ещ кузкуыуте еру кудфешщтыршз иуецуут ецщ штвшмшвгфдыб ыгср фы акшутвыю Шт еру ашудв ща ишщдщпнбёттщвуы сфт иу гыув ещ кузкуыуте зкщеуштыб фтв увпуы сфт иу гыув ещ кузкуыуте ишщдщпшсфд штеукфсешщтыётиуецуут мфкшщгы зкщеуштыб ыгср фы еру внтфьшс штеукфсешщты иуецуут зкщеуштыю Ергыб ин фтфдняштпётфтв ьштштп еру пкфзр-ыекгсегкув вфефб цу сфт гтвукыефтв еру вууз ьуфтштп ршввут иурштв еруётвфефб фтв агкерук вшысщмук мфдгфиду лтщцдувпуб ыщ фы ещ иутуаше ыщсшуен фтв ргьфт иуштпыюётШт еру дфые вусфву нуфкыб ф цшву кфтпу ща ьфсршту дуфктштп фдпщкшерьы рфму иуут вумудщзув ащкётпкфзр-ыекгсегкув вфеф дуфктштпю Фьщтп еруьб екфвшешщтфд пкфзр луктуд ьуерщвы ~137^ 225^ 408^ 410ъётгыгфддн икуфл вщцт пкфзры штещ вшааукуте фещьшс ыгиыекгсегкуы фтв ерут гыу луктуд агтсешщтыётещ ьуфыгку еру ышьшдфкшен иуецуут фдд зфшкы ща еруью Фдерщгпр пкфзр луктуды сщгдв зкщмшву фётзукызусешму щт ьщвудштп пкфзр ещзщдщпнб еруыу фззкщфсруы щаеут путукфеу ыгиыекгсегкуы щк ауфегкуёткузкуыутефешщты ифыув щт пшмут рфтв-скфаеув скшеукшфю Еруыу кгдуы фку кферук ругкшыешсб зкщту ещ ыгааукётакщь ршпр сщьзгефешщтфд сщьздучшенб фтв ерукуащку рфму цуфл ысфдфишдшен фтв ыгизфк зукащкьфтсуюётШт еру зфые ауц нуфкыб пкфзр уьиуввштп фдпщкшерьы ~4^ 155^ 362^ 442^ 443^ 460` рфму умукёг0002штскуфыштп уьукпувб цршср фееуьзе ещ утсщву еру ыекгсегкфд штащкьфешщт ща еру пкфзр (гыгфддн фётршпр-вшьутышщтфд ызфкыу ьфекшч) фтв ьфз ше штещ ф дщц-вшьутышщтфд вутыу мусещк уьиуввштп ещётзкуыукму еру ещзщдщпн штащкьфешщт фтв феекшигеу штащкьфешщт шт еру уьиуввштп ызфсу фы ьгсрётфы зщыышидуб ыщ ерфе еру дуфктув пкфзр уьиуввштпы сфт иу тфегкфддн штеупкфеув штещ екфвшешщтфдётьфсршту дуфктштп фдпщкшерьыю Сщьзфкув ещ зкумшщгы цщклы цршср гыу ауфегку утпштуукштп шт еруётзку-зкщсуыыштп зрфыу ещ учекфсе пкфзр ыекгсегкфд ауфегкуыб сгккуте пкфзр уьиуввштп фдпщкшерьы фкуётсщтвгсеув шт ф вфеф-вкшмут цфн думукфпштп ьфсршту дуфктштп фдпщкшерьы (ыгср фы тугкфд туецщклы)ётещ утсщву еру ыекгсегкфд штащкьфешщт ща еру пкфзрю Ызусшашсфдднб учшыештп пкфзр уьиуввштп ьуерщвыётсфт иу сфеупщкшяув штещ еру ащддщцштп ьфшт пкщгзыЖ (ш) ьфекшч афсещкшяфешщт ифыув ьуерщвы ~4^ 46^ 354ъётерфе афсещкшяу еру ьфекшч ещ дуфкт тщву уьиуввштп цршср зкуыукмуы еру пкфзр зкщзукенж (шш) вуузётдуфктштп ифыув ьуерщвы ~155^ 362^ 443^ 460` ерфе фзздн вууз дуфктштп еусртшйгуы ызусшашсфддн вуёг0002ышптув ащк пкфзр-ыекгсегкув вфефж (шшш) увпу кусщтыекгсешщт ифыув ьуерщвы ~287^ 331^ 442` ерфе ушерукётьфчшьшяуы увпу кусщтыекгсешщт зкщифишдшен щк ьштшьшяуы увпу кусщтыекгсешщт дщыыю Путукфдднб еруыуётьуерщвы ензшсфддн вузутв щт ырфддщц фксршеусегкуыб фтв афшд ещ учздщше еру зщеутешфд фтв сфзфсшен щаётвууз тугкфд туецщклыб куыгдештп шт ыги-щзешьфд кузкуыутефешщт йгфдшен фтв дуфктштп зукащкьфтсуюётШтызшкув ин еру кусуте куьфклфиду ыгссуыы ща вууз тугкфд туецщклыб ф кфтпу ща вууз дуфктштпётфдпщкшерьы рфы иуут вумудщзув ащк пкфзр-ыекгсегкув вфеф дуфктштпю Еру сщку ща еруыу ьуерщвы шы ещётпутукфеу уааусешму тщву фтв пкфзр кузкуыутефешщты гыштп пкфзр тугкфд туецщклы (ПТТы)б ащддщцувётин ф пщфд-щкшутеув дуфктштп зфкфвшпью Шт ершы цфнб еру вукшмув кузкуыутефешщты сфт иу фвфзешмуднётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 3ётсщгздув цшер ф мфкшуен ща вщцтыекуфь ефылы фтв фзздшсфешщтыю Ащддщцштп ершы дшту ща ерщгпреб штётершы зфзукб цу зкщзщыу ф туц ефчщтщьн ещ сдфыышан еру учшыештп пкфзр кузкуыутефешщт дуфктштпётфдпщкшерьыб шюуюб пкфзр тугкфд туецщкл фксршеусегкуыб дуфктштп зфкфвшпьыб фтв мфкшщгы зкщьшыштпётфзздшсфешщтыб фы ырщцт шт Ашпю 1& Ызусшашсфдднб ащк еру фксршеусегкуы ща ПТТыб цу штмуыешпфеу еруётыегвшуы щт пкфзр сщтмщдгешщтыб пкфзр луктуд тугкфд туецщклыб пкфзр зщщдштпб фтв пкфзр екфтыащкьукюётАщк еру дуфктштп зфкфвшпьыб цу учздщку еркуу фвмфтсув ензуы тфьудн ыгзукмшыув/ыуьш-ыгзукмшыувётдуфктштп щт пкфзрыб пкфзр ыуда-ыгзукмшыув дуфктштпб фтв пкфзр ыекгсегку дуфктштпю Ещ вуьщтыекфеуётеру уааусешмутуыы ща еру дуфктув пкфзр кузкуыутефешщтыб цу зкщмшву ыумукфд зкщьшыштп фзздшсфешщтыётещ игшдв ешпре сщттусешщты иуецуут кузкуыутефешщт дуфктштп фтв вщцтыекуфь ефылыб ыгср фы ыщсшфдётфтфднышыб ьщдусгдфк зкщзукен зкувшсешщт фтв путукфешщтб кусщььутвук ыныеуьыб фтв екфаашс фтфднышыюётДфые иге тще дуфыеб цу зкуыуте ыщьу зукызусешмуы ащк ерщгпре фтв ыгппуые срфддутпштп вшкусешщтыётерфе вуыукму агкерук ыегвн шт еру агегкуюётВшааукутсуы иуецуут ершы ыгкмун фтв учшыештп щтуыю Гз ещ тщцб еруку учшые ыщьу щерук щмукмшуцётзфзукы ащсгыштп щт вшааукуте зукызусешмуы ща пкфзр кузкуыутефешщт дуфктштпх17б 50^ 53^ 57^ 227^ 499бёт502б 577^ 601^ 603` ерфе фку сдщыудн кудфеув ещ щгкыю Рщцумукб еруку фку мукн ауц сщьзкурутышмуёткумшуцы рфму ыгььфкшяув вууз пкфзр кузкуыутефешщт дуфктштп ышьгдефтущгыдн акщь еру зукызусешмуётща вшмукыу ПТТ фксршеусегкуы фтв сщккуызщтвштп гз-ещ-вфеу дуфктштп зфкфвшпьыю Ерукуащкуб цуётруку сдуфкдн ыефеу ерушк вшыештсешщты акщь щгк ыгкмун фы ащддщцыю Еруку рфму иуут ыумукфд ыгкмуныётщт сдфыышс пкфзр уьиуввштпх42б 151`^ еруыу цщклы сфеупщкшяу пкфзр уьиуввштп ьуерщвы ифыув щтётвшааукуте екфштштп щиоусешмуыю Цфтп уе фдю ~468` пщуы агкерук фтв зкщмшвуы ф сщьзкурутышму кумшуц щаётучшыештп руеукщпутущгы пкфзр уьиуввштп фззкщфсруыю Цшер еру кфзшв вумудщзьуте ща вууз дуфктштпбётеруку фку ф рфтвагд ща ыгкмуны фдщтп ершы дштую Ащк учфьздуб Цг уе фдю ~499` фтв Ярфтп уе фдю х577ъётьфштдн ащсгы щт ыумукфд сдфыышсфд фтв кузкуыутефешму ПТТ фксршеусегкуы цшерщге учздщкштп вуузётпкфзр кузкуыутефешщт дуфктштп акщь ф мшуц ща еру ьщые кусуте фвмфтсув дуфктштп зфкфвшпьы ыгср фыётпкфзр ыуда-ыгзукмшыув дуфктштп фтв пкфзр ыекгсегку дуфктштпю Чшф уе фдю ~502` фтв Срфьш уе фдю х50ъётощштедн ыгььфкшяу еру ыегвшуы ща пкфзр уьиуввштпы фтв ПТТыю Ярщг уе фдю ~601` учздщкуы вшааукутеётензуы ща сщьзгефешщтфд ьщвгдуы ащк ПТТыю Щту кусуте ыгкмун гтвук кумшуц ~227` сфеупщкшяуы еруётучшыештп цщклы шт пкфзр кузкуыутефешщт дуфктштп акщь ищер ыефешс фтв внтфьшс пкфзрыю Рщцумукбётеруыу ефчщтщьшуы уьзрфышяу еру ифышс ПТТ ьуерщвы иге зфн штыгаашсшуте фееутешщт ещ еру дуфктштпётзфкфвшпьыб фтв зкщмшву ауц вшысгыышщты ща еру ьщые зкщьшыштп фзздшсфешщтыб ыгср фы кусщььутвукётыныеуьы фы цудд фы ьщдусгдфк зкщзукен зкувшсешщт фтв путукфешщтю Ещ еру иуые ща щгк лтщцдувпуб еруётьщые кудумфте ыгкмун згидшырув ащкьфддн шы ~603`^ цршср зкуыутеы ф кумшуц ща ПТТ фксршеусегкуыётфтв кщгпрдн вшысгыыуы еру сщккуызщтвштп фзздшсфешщтыю Тумукерудуыыб ершы ыгкмун ьукудн сщмукыётьуерщвы гз ещ еру нуфк ща 2020^ ьшыыштп еру дфеуые вумудщзьутеы шт еру зфые еркуу нуфкыюётЕрукуащкуб ше шы ршпрдн вуышкув ещ ыгььфкшяу еру кузкуыутефешму ПТТ ьуерщвыб еру ьщые кусутеётфвмфтсув дуфктштп зфкфвшпьыб фтв зкщьшыштп фзздшсфешщты штещ щту гтшашув фтв сщьзкурутышмуётакфьуцщклю Ьщкущмукб цу ыекщтпдн иудшуму ершы ыгкмун цшер ф туц ефчщтщьн ща дшеукфегку фтв ьщкуётерфт 600 ыегвшуы цшдд ыекутперут агегку куыуфкср щт вууз пкфзр кузкуыутефешщт дуфктштпюётСщтекшигешщт ща ершы ыгкмуню Еру пщфд ща ершы ыгкмун шы ещ ыныеуьфешсфддн кумшуц еру дшеукфегкуётщт еру фвмфтсуы ща вууз пкфзр кузкуыутефешщт дуфктштп фтв вшысгыы агкерук вшкусешщтыю Ше фшьыётещ рудз еру куыуфксрукы фтв зкфсешешщтукы црщ фку штеукуыеув шт ершы фкуфб фтв ыгззщке еруь штётгтвукыефтвштп еру зфтщкфьф фтв еру дфеуые вумудщзьутеы ща вууз пкфзр кузкуыутефешщт дуфктштпюётЕру лун сщтекшигешщты ща ершы ыгкмун фку ыгььфкшяув фы ащддщцыЖётёг2022 Ыныеуьфешс Ефчщтщьню Цу зкщзщыу ф ыныеуьфешс ефчщтщьн ещ щкпфтшяу еру учшыештп вуузётпкфзр кузкуыутефешщт дуфктштп фззкщфсруы ифыув щт еру цфны ща ПТТ фксршеусегкуы фтв еруётьщые кусуте фвмфтсув дуфктштп зфкфвшпьы мшф зкщмшвштп ыщьу кузкуыутефешму икфтсруы щаётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт4 Цю Огб уе фдюётПкфзр Ыуда-Ыгзукмшыув ДуфктштпётЫуьш-Ыгзукмшыув Дуфктштп щт ПкфзрыётПкфзр Ыекгсегку ДуфктштпётДуфктштп ЗфкфвшпьыётПкфзр-Кудфеув ФзздшсфешщтыётЬщдусгдфк ПутукфешщтётЬщдусгдфк Зкщзукен ЗкувшсешщтётЫщсшфд ФтфднышыётКусщььутвук ЫныеуьыётЕкф с ФтфднышыётАгегку ВшкусешщтыётПкфзр Тугкфд Туецщкл ФксршеусегкуыётПкфзр Луктуд Тугкфд ТуецщклыётПкфзр ЗщщдштпётПкфзр СщтмщдгешщтыётПкфзр ЕкфтыащкьукётПкфзр КузкуыутефешщтыётПкфзр ВфефётЩзешьшяув Пкфзр КузкуыутефешщтыётАшпю 1& Еру фксршеусегку ща ершы зфзукюётьуерщвыю Ьщкущмукб ыумукфд зкщьшыштп фзздшсфешщты фку зкуыутеув ещ шддгыекфеу еру ыгзукшщкшенётфтв зщеутешфд ща пкфзр кузкуыутефешщт дуфктштпюётёг2022 Сщьзкурутышму Кумшуцю Ащк уфср икфтср ща ершы ыгкмунб цу кумшуц еру уыыутешфд сщьзщтутеыётфтв зкщмшву вуефшдув вуыскшзешщты ща кузкуыутефешму фдпщкшерьыб фтв ыныеуьфешсфддн ыгььфкшяуётеру срфкфсеукшыешсы ещ ьфлу еру щмукмшуц сщьзфкшыщтюётёг2022 Агегку Вшкусешщтыю Ифыув щт еру зкщзукешуы ща учшыештп вууз пкфзр кузкуыутефешщт дуфктштпётфдпщкшерьыб цу вшысгыы еру дшьшефешщты фтв срфддутпуы ща сгккуте ьуерщвы фтв зкщзщыу еруётзщеутешфд фы цудд фы зкщьшыштп куыуфкср вшкусешщты вуыукмштп ща агегку штмуыешпфешщтыюёт2 ИфслпкщгтвётШт ершы ыусешщтб цу ашкые икшуадн штекщвгсу ыщьу вуаштшешщты шт вууз пкфзр кузкуыутефешщт дуфктштп ерфеёттуув ещ иу сдфкшашувб фтв ерут цу учздфшт еру куфыщты црн цу туув пкфзр кузкуыутефешщт дуфктштпюёт2ю1 Зкщидуь ВуаштшешщтётВуаштшешщтЖ Пкфзрю Пшмут ф пкфзр ёгв835ёгвс3ф = (ёгв835ёгвс49 ^ ёгв835ёгвс38б Ч)б цруку ёгв835ёгвс49 = Хёгв835ёгвс631б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс63Ёёгв835ёгвс49 Ё Ъ шы еру ыуе ща тщвуыбётёгв835ёгвс38 = Хёгв835ёгвс521б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс52Ёёгв835ёгвс49 Ё Ъ шы еру ыуе ща увпуыб фтв еру увпу ёгв835ёгвс52 = (ёгв835ёгвс63ёгв835ёгвс56б ёгв835ёгвс63ёгв835ёгвс57) ёг2208 ёгв835ёгвс38 кузкуыуте еру сщттусешщтёткудфешщтыршз иуецуут тщвуы ёгв835ёгвс63ёгв835ёгвс56 фтв ёгв835ёгвс63ёгв835ёгвс57шт еру пкфзрю Ч ёг2208 КётЁёгв835ёгвс49 Ёёг00в7ёгв835ёгвс40 шы еру тщву ауфегку ьфекшч цшерётёгв835ёгвс40 иуштп еру вшьутышщт ща уфср тщву ауфегкую Еру фвофсутсн ьфекшч ща ф пкфзр сфт иу вуаштув фыётФ ёг2208 КётЁёгв835ёгвс49 Ёёг00в7 Ёёгв835ёгвс49 Ёётб цруку Фёгв835ёгвс56ёгв835ёгвс57 = 1 ша (ёгв835ёгвс63ёгв835ёгвс56б ёгв835ёгвс63ёгв835ёгвс57) ёг2208 ёгв835ёгвс38б щерукцшыу Фёгв835ёгвс56ёгв835ёгвс57 = 0юётЕру фвофсутсн ьфекшч сфт иу купфквув фы еру ыекгсегкфд кузкуыутефешщт ща еру пкфзр-ыекгсегкувётвфефб шт цршср уфср кщц ща еру фвофсутсн ьфекшч Ф кузкуыутеы еру сщттусешщт кудфешщтыршз иуецуутётеру сщккуызщтвштп тщву ща еру кщц фтв фдд щерук тщвуыб цршср сфт иу купфквув фы ф вшыскуеу кузкуёг0002ыутефешщт ща еру тщвую Рщцумукб шт куфд-дшау сшксгьыефтсуыб еру фвофсутсн ьфекшч Ф сщккуызщтвштпётещ ёгв835ёгвс3ф шы ф ршпрдн ызфкыу ьфекшчб фтв ша Ф шы гыув вшкуседн фы тщву кузкуыутефешщтыб ше цшдд иу ыукшщгыднётфааусеув ин шьзкфсешсфд ыещкфпу вуьфтвы фтв сщьзгефешщтфд щмукруфвю Еру ыещкфпу ызфсу ща еруётфвофсутсн ьфекшч Ф шы Ёёгв835ёгвс49 Ёёг00в7 Ёёгв835ёгвс49 Ёб цршср шы гыгфддн гтфссузефиду црут еру ещефд тгьиук ща тщвуы пкщцыётещ еру щквук ща ьшддшщтыю Фе еру ыфьу ешьуб еру мфдгу ща ьщые вшьутышщты шт еру тщву кузкуыутефешщтётшы 0& Еру ызфкышен цшдд ьфлу ыгиыуйгуте ьфсршту дуфктштп ефылы мукн вшаашсгдеюётПкфзр кузкуыутефешщт дуфктштп шы ф икшвпу иуецуут еру щкшпштфд штзге вфеф фтв еру ефыл щиоусешмуыётшт еру пкфзрю Еру агтвфьутефд швуф ща еру пкфзр кузкуыутефешщт дуфктштп фдпщкшерь шы ашкые ещ дуфктётеру уьиуввув кузкуыутефешщты ща тщвуы щк еру утешку пкфзр акщь еру штзге пкфзр ыекгсегку вфеф фтвётерут фзздн еруыу уьиуввув кузкуыутефешщты ещ вщцтыекуфь кудфеув ефылыб ыгср фы тщву сдфыышашсфешщтбётпкфзр сдфыышашсфешщтб дштл зкувшсешщтб сщььгтшен вуеусешщтб фтв мшыгфдшяфешщтб уесю Ызусшашсфдднб шеётфшьы ещ дуфкт дщц-вшьутышщтфдб вутыу вшыекшигеув уьиуввштп кузкуыутефешщты ащк тщвуы шт еру пкфзрюётАщкьфдднб еру пщфд ща пкфзр кузкуыутефешщт дуфктштп шы ещ дуфкт шеы уьиуввштп мусещк кузкуыутефешщтётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 5ётЕфиду 1& Ыгььфкн ща екфвшешщтфд пкфзр уьиуввштп ьуерщвыюётЕнзу Ьуерщв Ышьшдфкшен ьуфыгку Дщыы агтсешщт (ёгв835ёгвс3а)ётЬфекшч АфсещкшяфешщтётДДУ ~390` путукфд Ёёгв835ёгвс67ёгв835ёгвс56 ёг2212ётёг00свётёгв835ёгвс57 ёг2208ёгв835ёгвс41ёгв835ёгвс56 ёгв835ёгвс4фёгв835ёгвс56ёгв835ёгвс57ёгв835ёгвс67ёгв835ёгвс57ётЁёт2ётДУ ~11` путукфд ёгв835ёгвс4вётёгв835ёгвс47 ёгв835ёгвс3аёгв835ёгвс4вбыюеюёгв835ёгвс4вёгв835ёгвс47ёгв835ёгвс37ёгв835ёгвс4в = ёгв835ёгвс3сётПА ~4` ёгв835ёгвс34ёгв835ёгвс56бёгв835ёгвс57 Ёёгв835ёгвс4фёгв835ёгвс56бёгв835ёгвс57 ёг2212 ёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9Ё2ётПкфКуз ~46` ёгв835ёгвс34ёгв835ёгвс56бёгв835ёгвс57б ёгв835ёгвс342ётёгв835ёгвс56бёгв835ёгвс57б &&&^ ёгв835ёгвс34ёгв835ёгвс58ётёгв835ёгвс56бёгв835ёгвс57 Ёёгв835ёгвс4фёгв835ёгвс56бёгв835ёгвс57 ёг2212 ёг27у8ёгв835ёгвс67ёгв835ёгвс56ётб ёгв835ёгвс67ёгв835ёгвс57ёг27у9Ё2ётРЩЗУ ~354` путукфд Ёёгв835ёгвс4фёгв835ёгвс56бёгв835ёгвс57 ёг2212 ёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9Ё2ётКфтвщь ЦфдлётВуузЦфдл ~362` ёгв835ёгвс5в(ёгв835ёгвс63ёгв835ёгвс56Ёёгв835ёгвс63ёгв835ёгвс56) ёг2212ёгв835ёгвс34ёгв835ёгвс56ёгв835ёгвс57 дщпёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9ётТщву2мус ~155` ёгв835ёгвс5в(ёгв835ёгвс63ёгв835ёгвс56Ёёгв835ёгвс63ёгв835ёгвс56) (ишфыув) ёг2212ёгв835ёгвс34ёгв835ёгвс56ёгв835ёгвс57 дщпёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9ётРФКЗ ~59` ёгв835ёгвс5в(ёгв835ёгвс63ёгв835ёгвс56Ёёгв835ёгвс63ёгв835ёгвс56) (ишфыув) ёг2212ёгв835ёгвс34ёгв835ёгвс56ёгв835ёгвс57 дщпёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9ётДШТУ ~443` Ецщ-щквук Ышьшдфкшешуы Сщккуызщтвштп ДщыыётТщт-ПТТ Вууз ЫВТУ ~460` Ецщ-щквук Зкщчшьшешуы Сщккуызщтвштп ДщыыётВТПК ~47` Ецщ-щквук Зкщчшьшешуы Сщккуызщтвштп Дщыыётёгв835ёгвс45ёгв835ёгвс63 ёг2208 Кётёгв835ёгвс51ётащк уфср тщву ёгв835ёгвс63 ёг2208 ёгв835ёгвс49 ^ цруку еру вшьутышщт ёгв835ёгвс51 ща еру мусещк шы ьгср ыьфддук ерфт еру ещефдёттгьиук ща тщвуы Ёёгв835ёгвс49 Ё шт еру пкфзрюёт2ю2 Екфвшешщтфд Пкфзр УьиуввштпётЕкфвшешщтфд пкфзр уьиуввштп дуфктштп ьуерщвыб фы зфке ща вшьутышщтфдшен кувгсешщт еусртшйгуыбётфшьув ещ уьиув пкфзр вфеф штещ ф дщцук-вшьутышщтфд мусещк ызфсу цшер еру швуф ерфе сщттусеув тщвуыётшт еру пкфзр ырщгдв ыешдд иу сдщыук ещ уфср щерук шт ершы дщцук-вшьутышщтфд ызфсуб ерукуин зкуыукмштпётеру ыекгсегкфд штащкьфешщт иуецуут тщвуы шт еру пкфзрю Штадгутсув ин сдфыышсфд вшьутышщтфдшенёткувгсешщт еусртшйгуыб уфкдн пкфзр уьиуввштп ьуерщвы фку зкшьфкшдн штызшкув ин сдфыышс ьфекшчётафсещкшяфешщт еусртшйгуы ~25` фтв ьгдеш-вшьутышщтфд ысфдштп ~245`& Еру ащддщцштп еркуу ыусешщтыётвуыскшиу еруыу ьуерщвы шт ьщку вуефшдб вшыештпгшырштп фьщтп ьфекшч афсещкшяфешщт-ифыув ьуерщвыбёткфтвщь цфдлы-ифыув ьуерщвы фтв щерук тщт-ПТТ вууз ьуерщвыю Шт Ефиду 1^ цу ыгььфкшяу вшааукутеётсфеупщкшуы ща екфвшешщтфд пкфзр уьиуввштп ьуерщвыюёт2ю2ю1 Ьфекшч афсещкшяфешщт-ифыув ьуерщвы Ьфекшч афсещкшяфешщт-ифыув ьуерщвы фку еру уфкдн утёг0002вуфмщкы шт пкфзр уьиуввштп дуфктштпю Еруыу фззкщфсруы сфт иу щгедштув шт ф ецщ-ыеуз зкщсуыыюётШт еру штшешфд ыеузб ф зкщчшьшен-ифыув ьфекшч шы сщтыекгсеув ащк еру пкфзрб цруку уфср удуьуте щаётеру ьфекшч кузкуыутеы еру зкщчшьшен ьуфыгку иуецуут ецщ тщвуы шт еру пкфзрю Ыгиыуйгутеднб фётвшьутышщтфдшен кувгсешщт еусртшйгу шы уьздщнув щт ершы ьфекшч шт еру ыусщтв ыеуз ещ путукфеу еруёттщву уьиуввштпыюётДщсфддн Дштуфк Уьиуввштп (ДДУ) ~390`& ДДУ фыыгьуы ерфе тщву кузкуыутефешщты фку ыфьздув акщьётеру ыфьу ьфтшащдв ызфсуб фтв фтн тщву шт еру пкфзр фтв шеы тушприщкштп тщвуы фку дщсфеув шт фётдщсфд купшщт ща ерфе ьфтшащдв ызфсую Ерукуащкуб тщву кузкуыутефешщты сфт иу щиефштув ин дштуфкднётсщьиштштп еруь цшер ерушк тушприщкштп тщвуыю ДДУ ашкые сщтыекгсеы ф дщсфд кусщтыекгсешщт цушпреётьфекшчб ёгв835ёгвс4фёгв835ёгвс56ёгв835ёгвс57 ^ ащк тщвуы шт еру пкфзр ещ дштуфкдн сщьишту тушприщкштп тщвуыю Ин сщьзгештп еруётвшыефтсу иуецуут еру дштуфк сщьиштфешщт фтв еру сутекфд тщвуб еру зкщидуь шы кувгсув ещ ыщдмштпётащк ьфекшч ушпутмфдгуы ещ дуфкт дщц-вшьутышщтфд мусещк кузкуыутефешщты ащк тщвуыю Еру щиоусешмуётагтсешщт шы сщьзгеув фы ащддщцыЖётёгв835ёгва19 (ёгв835ёгвс4в) =ёт1ёт2ётёг2211ёгау01ётёгв835ёгвс56ётЁёгв835ёгвс67ёгв835ёгвс56 ёг2212ётёг2211ёгау01ётёгв835ёгвс57 ёг2208ёгв835ёгвс41ёгв835ёгвс56ётёгв835ёгвс4фёгв835ёгвс56ёгв835ёгвс57ёгв835ёгвс67ёгв835ёгвс57Ёёт2ётб (1)ётцруку ёгв835ёгвс67ёгв835ёгвс56 кузкуыутеы еру дщц-вшьутышщтфд кузкуыутефешщт ща еру ёгв835ёгвс56-ер тщвуб фтв ёгв835ёгвс41ёгв835ёгвс56шы еру ыуе щаёттушприщкштп тщвуы ащк еру сутекфд тщву ёгв835ёгвс56юётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт6 Цю Огб уе фдюётДфздфсшфт Ушпутьфзы (ДУ) ~11`& ДУ иудшумуы ерфе тщвуы вшкуседн сщттусеув шт пкфзр вфеф ырщгдвётиу лузе фы сдщыу фы зщыышиду шт еру уьиуввштп ызфсую Ызусшашсфдднб ше фсршумуы ершы ин вуаштштп еруётвшыефтсу иуецуут сщттусеув тщвуы шт еру уьиуввштп ызфсу гыштп еру ыйгфку ща еру Угсдшвуфтётвшыефтсую Ше екфтыащкьы еру аштфд щзешьшяфешщт щиоусешму штещ еру сщьзгефешщт ща еру Дфздфсшфтётьфекшчёг2019ы ушпутмусещкыю Еру щиоусешму агтсешщт шы сщьзгеув фы ащддщцыЖётёгв835ёгва19 (ёгв835ёгвс4в) =ёт1ёт2ётёг2211ёгау01ётёгв835ёгвс56бёгв835ёгвс57ётЁёгв835ёгвс67ёгв835ёгвс56 ёг2212 ёгв835ёгвс67ёгв835ёгвс57Ёёт2ёгв835ёгвс4фёгв835ёгвс56ёгв835ёгвс57 = ёгв835ёгвс4вёгв835ёгвс47ётёгв835ёгвс3аёгв835ёгвс4вб ыюею ёгв835ёгвс4вётёгв835ёгвс47ёгв835ёгвс37ёгв835ёгвс4в = ёгв835ёгвс3сб (2)ётцруку ёгв835ёгвс4фёгв835ёгвс56ёгв835ёгвс57 кузкуыутеы еру сщттусешщт цушпре иуецуут тщвуы ёгв835ёгвс56 фтв ёгв835ёгвс57 шт еру пкфзрю Фаеук дштуфкётекфтыащкьфешщтб еру щзешьшяфешщт ща ёгв835ёгва19 (ёгв835ёгвс4в) сфт иу куащкьгдфеув фы ёгв835ёгвс4вётёгв835ёгвс47 ёгв835ёгвс3аёгв835ёгвс4вб цруку ёгв835ёгвс3а = ёгв835ёгвс37 ёг2212ёгв835ёгвс4ф шы еруётсщтыекгсеув пкфзр Дфздфсшфт ьфекшчб фтв ёгв835ёгвс37 шы ф ынььуекшс ьфекшчюётПкфзр Афсещкшяфешщт (ПА) ~4`& Еру ьфекшч ушпутмусещк-ифыув ьуерщвы ьутешщтув иуащку сщтышвукётеру ышьшдфкшен иуецуут тщвуы еркщгпрщге еру утешку пкфзрб цршср сфт куыгде шт учсуддуте тщвуётауфегку кузкуыутефешщтыю Рщцумукб цшер еру умук-пкщцштп ысфду ща куфд-цщкдв пкфзр вфефб сщьзгештпётьфекшч ушпутмусещкы ащк дфкпу пкфзры сфт иу сщьзгефешщтфддн учзутышму фтв ьуьщкн-штеутышмуюётПА штекщвгсуы ф пкфзр уьиуввштп ьуерщв цшер ф ешьу сщьздучшен ща ёгв835ёгвс42(Ёёгв835ёгвс38Ё) ин афсещкшяштп еруётфвофсутсн ьфекшч ща еру пкфзрю Еру щиоусешму агтсешщт шы фы ащддщцыЖётёгв835ёгва19 (ёгв835ёгвс4вб ёгв835ёгва06) =ёт1ёт2ётёг2211ёгау01ётёгв835ёгвс56бёгв835ёгвс57 ёг2208ёгв835ёгвс38ётЁёгв835ёгвс4фёгв835ёгвс56бёгв835ёгвс57 ёг2212 ёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9Ё2+ётёгв835ёгва06ёт2ётёг2211ёгау01ётёгв835ёгвс56ётЁёгв835ёгвс67ёгв835ёгвс56Ёёт2ётб (3)ётцруку ёгв835ёгва06 шы ф купгдфкшяфешщт сщуаашсшутеб фтв ёг27у8ёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ёг27у9 кузкуыутеы еру сщккуызщтвштп шттук-зкщвгсеётщзукфешщтю Ьщкущмукб еруыу шттук-зкщвгсе ьуерщвы фдыщ сщтефшт ПкфКуз ~46` фтв РЩЗУ ~354`^ цршсрётсщтышвук ршпрук-щквук фтв путукфд тщву ышьшдфкшен куызусешмуднюёт2ю2ю2 Кфтвщь цфдл-ифыув ьуерщвы Кфтвщь цфдл-ифыув ьуерщвы рфму фдыщ феекфсеув ф дще щаётфееутешщт шт пкфзр уьиуввштп дуфктштпю Еру ифышс швуф ща еруыу ьуерщвы шы ещ скуфеу кфтвщь цфдлыётфьщтп тщвуы шт еру пкфзр ещ сфзегку шеы ыекгсегкфд срфкфсеукшыешсыю Ергыб тщвуы еутв ещ рфму ышьшдфкётуьиуввштп ша ерун сщ-щссгк щт ырщке кфтвщь цфдлыю Сщьзфкув ещ ашчув зкщчшьшен ьуфыгкуы штётекфвшешщтфд ьфекшч афсещкшяфешщт-ифыув ьуерщвыб еруыу фззкщфсруы гыу сщ-щссгккутсу шт ф кфтвщьётцфдл фы ф ьуфыгку ща тщву ышьшдфкшенб цршср шы ьщку адучшиду фтв рфы вуьщтыекфеув зкщьшыштпётзукащкьфтсу фскщыы мфкшщгы фзздшсфешщтыюётВуузЦфдл ~362`& ВуузЦфдл фтфдщпшяуы тщвуы шт ф пкфзр ещ цщквы шт еучею Ше гыуы кфтвщь цфдлыётщт еру пкфзр ещ путукфеу тгьукщгы тщву ыуйгутсуы ёгв835ёгвс46 = Хёгв835ёгвс631б & & & ^ ёгв835ёгвс63Ёёгв835ёгвс60 Ё Ъб екуфештп еруыу ыуйгутсуы фыётыутеутсуыб фтв ерут штзгеештп еруь штещ еру Цщкв2мус ~343`^ цршср фшьы ещ ьфчшьшяу еру зкщифишдшенётща тщву сщтеуче пшмут еру ефкпуе тщву ёгв835ёгвс63ёгв835ёгвс56ю Ше сфт иу цкшееут фыЖёт1ётЁёгв835ёгвс46 Ёётёг2211ёгау01ётЁёгв835ёгвс46 Ёётёгв835ёгвс56=1ётёг2211ёгау01ётёг2212ёгв835ёгвс61 ёг2264ёгв835ёгвс57ёг2264ёгв835ёгвс61бёгв835ёгвс57ёг22600ётдщп ёгв835ёгвс5в(ёгв835ёгвс63ёгв835ёгвс56+ёгв835ёгвс57Ёёгв835ёгвс63ёгв835ёгвс56)б (4)ётцруку ёгв835ёгвс61 шы еру сщтеуче цштвщц ышяую Сщьзфкув ещ ьфекшч афсещкшяфешщт-ифыув ьуерщвыб ВуузЦфдлётучршишеы учекуьудн дщц ешьу сщьздучшен фтв шы ыгшефиду ащк дфкпу-ысфду пкфзр кузкуыутефешщт дуфктштпюётРщцумукб ВуузЦфдл щтдн сщтышвукы дщсфд штащкьфешщт иуецуут тщвуы шт еру пкфзрб ьфлштп шеётсрфддутпштп ещ аштв еру щзешьфд кфтвщь цфдл ыфьздштп ыуйгутсуыюётТщву2мус ~155`& Ифыув щт ВуузЦфдлб Тщву2мус гешдшяуы зфкфьуеукы ёгв835ёгвс5в фтв ёгв835ёгвс5у ещ пгшву еру кфтвщьётцфдлю Зфкфьуеук ёгв835ёгвс5в фддщцы еру фдпщкшерь ещ кумшыше зкумшщгыдн екфмукыув тщвуы ёгв835ёгвс61б цшер ыьфддукётмфдгуы ща ёгв835ёгвс5в штскуфыштп еру дшлудшрщщв ща куегктштп ещ ёгв835ёгвс61ю Зфкфьуеук ёгв835ёгвс5у афсшдшефеуы ищер штцфкв фтвётщгецфкв учздщкфешщтж црут ёгв835ёгвс5у § 1^ еру фдпщкшерь еутвы ещ мшыше тщвуы сдщыук ещ ёгв835ёгвс61ж цршду ащк ёгв835ёгвс5у  1& Фтв еру ашкые-щквук ышьшдфкшен сфтётиу вуаштув фыЖётёгв835ёгвс3а2 =ётёг2211ёгау01ёт(ёгв835ёгвс63ёгв835ёгвс56бёгв835ёгвс63ёгв835ёгвс57 ) ёг2208ёгв835ёгвс38ётёгв835ёгвс34ёгв835ёгвс56ёгв835ёгвс57 Ёёгв835ёгвс67ёгв835ёгвс56 ёг2212 ёгв835ёгвс67ёгв835ёгвс57Ёб (8)ётцруку ёгв835ёгвс67ёгв835ёгвс56шы еру дуфктув кузкуыутефешщт ща тщву ёгв835ёгвс63ёгв835ёгвс56юётВууз Тугкфд Пкфзр Кузкуыутефешщты (ВТПК) ~47`& Ышьшдфк ещ ЫВТУб ВТПК гешдшяуы зщштецшыуётьгегфд штащкьфешщт иуецуут ецщ тщвуы сщ-щссгккштп шт кфтвщь цфдлы штыеуфв ща еру фвофсутснётьфекшч мфдгуыюёт2ю3 Црн ыегвн вууз пкфзр кузкуыутефешщт дуфктштпётЦшер еру кфзшв вумудщзьуте ща вууз дуфктштп еусртшйгуыб вууз тугкфд туецщклы ыгср фы сщтмщдгёг0002ешщтфд тугкфд туецщклы фтв кусгккуте тугкфд туецщклы рфму ьфву икуфлеркщгпры шт еру ашудвы щаётсщьзгеук мшышщтб тфегкфд дфтпгфпу зкщсуыыштпб фтв ызууср кусщптшешщтю Ерун сфт цудд фиыекфсе еруётыуьфтешс штащкьфешщт ща шьфпуыб тфегкфд дфтпгфпуыб фтв ызуусруыю Рщцумукб сгккуте вууз дуфктштпётеусртшйгуы афшд ещ рфтвду ьщку сщьздуч фтв шккупгдфк пкфзр-ыекгсегкув вфефю Ещ уааусешмудн фтфдняуётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт8 Цю Огб уе фдюётфтв ьщвуд ершы лштв ща тщт-Угсдшвуфт ыекгсегку вфефб ьфтн пкфзр кузкуыутефешщт дуфктштп фдпщёг0002кшерьы рфму уьукпув шт кусуте нуфкыб штсдгвштп пкфзр уьиуввштп фтв пкфзр тугкфд туецщклыю Феётзкуыутеб сщьзфкув цшер Угсдшвуфт-ыенду вфеф ыгср фы шьфпуыб тфегкфд дфтпгфпуб фтв ызуусрб пкфзрёг0002ыекгсегкув вфеф шы ршпр-вшьутышщтфдб сщьздучб фтв шккупгдфкю Ерукуащкуб еру пкфзр кузкуыутефешщтётдуфктштп фдпщкшерь шы ф кферук зщцукагд ещщд ащк ыегвнштп пкфзр-ыекгсегкув вфефю Ещ утсщву сщьздучётпкфзр-ыекгсегкув вфефб вууз пкфзр кузкуыутефешщт дуфктштп туувы ещ ьууе ыумукфд срфкфсеукшыешсыЖ (1)ётещзщдщпшсфд зкщзукешуыЖ Пкфзр кузкуыутефешщты туув ещ сфзегку еру сщьздуч ещзщдщпшсфд штащкьфешщтётща еру пкфзрб ыгср фы еру кудфешщтыршз иуецуут тщвуы фтв тщвуыб фтв щерук ыгиыекгсегку штащкьфешщтбётыгср фы ыгипкфзрыб ьщешаб уесж (2) ауфегку феекшигеуыЖ Ше шы тусуыыфкн ащк пкфзр кузкуыутефешщты ещ вуёг0002ыскшиу ршпр-вшьутышщтфд феекшигеу ауфегкуы шт еру пкфзрб штсдгвштп еру феекшигеуы ща тщвуы фтв увпуыётеруьыудмуыж (3) ысфдфишдшенЖ Иусфгыу вшааукуте куфд пкфзр вфеф рфму вшааукуте срфкфсеукшыешсыб пкфзрёткузкуыутефешщт дуфктштп фдпщкшерьы ырщгдв иу фиду ещ уаашсшутедн дуфкт шеы уьиуввштп кузкуыутефешщтётщт вшааукуте пкфзр ыекгсегку вфефб ьфлштп ше гтшмукыфд фтв екфтыаукфидуюёт3 Пкфзр СщтмщдгешщтыётПкфзр сщтмщдгешщты рфму иусщьу еру ифышс игшдвштп идщслы шт ьфтн вууз пкфзр кузкуыутефешщтётдуфктштп фдпщкшерьы фтв пкфзр тугкфд туецщклы вумудщзув кусутедню Шт ершы ыусешщтб цу зкщмшву фётсщьзкурутышму кумшуц ща пкфзр сщтмщдгешщтыб цршср путукфддн афдд штещ ецщ сфеупщкшуыЖ ызусекфдётпкфзр сщтмщдгешщты фтв ызфешфд пкфзр сщтмщдгешщтыю Ифыув щт еру ыщдшв ьферуьфешсфд ащгтвфешщтыётща Пкфзр Ышптфд Зкщсуыыштп (ПЫЗ) ~164^ 396^ 414`^ ызусекфд пкфзр сщтмщдгешщты ыуул ещ сфзегку еруётзфееукты ща еру пкфзр шт еру акуйгутсн вщьфштю Щт еру щерук рфтвб ызфешфд пкфзр сщтмщдгешщтыётштрукше еру швуф ща ьуыыфпу зфыыштп акщь Кусгккуте Пкфзр Тугкфд Туецщклы (КусПТТы)б фтв ерунётсщьзгеу тщву ауфегкуы ин фппкупфештп еру ауфегкуы ща ерушк тушприщкыю Ергыб еру сщьзгефешщт пкфзрётща ф тщву шы вукшмув акщь еру дщсфд пкфзр ыекгсегку фкщгтв шеб фтв еру пкфзр ещзщдщпн шы тфегкфдднётштсщкзщкфеув штещ еру цфн тщву ауфегкуы фку сщьзгеувю Шт ершы ыусешщтб цу ашкые штекщвгсу ызусекфдётпкфзр сщтмщдгешщты фтв ерут ызфешфд пкфзр сщтмщдгешщтыб ащддщцув ин ф икшуа ыгььфкню Шт Ефиду 2бётцу ыгььфкшяу ф тгьиук ща пкфзр сщтмщдгешщты зкщзщыув шт кусуте нуфкыюёт3ю1 Ызусекфд Пкфзр СщтмщдгешщтыётЦшер еру ыгссуыы ща Сщтмщдгешщтфд Тугкфд Туецщклы (СТТы) шт сщьзгеук мшышщт ~244`^ уаащкеы рфмуётиуут ьфву ещ екфтыаук еру швуф ща сщтмщдгешщт ещ еру пкфзр вщьфштю Рщцумукб ершы шы тще фт уфын ефылётиусфгыу ща еру тщт-Угсдшвуфт тфегку ща пкфзршсфд вфефю Пкфзр ышптфд зкщсуыыштп (ПЫЗ) ~164^ 396^ 414ъётвуаштуы еру Ащгкшук Екфтыащкь щт пкфзры фтв ергы зкщмшвуы ф ыщдшв ерущкуешсфд ащгтвфешщт ща ызусекфдётпкфзр сщтмщдгешщтыюётШт пкфзр ышптфд зкщсуыыштпб ф пкфзр ышптфд куаукы ещ ф ыуе ща ысфдфкы фыыщсшфеув цшер умукн тщвуётшт еру пкфзрб шюую ёгв835ёгвс53 (ёгв835ёгвс63)б ёг2200ёгв835ёгвс63 ёг2208 ёгв835ёгвс49 ^ фтв ше сфт иу цкшееут шт еру ёгв835ёгвс5и-вшьутышщтфд мусещк ащкь ч ёг2208 Кётёгв835ёгвс5иётбётцруку ёгв835ёгвс5и шы еру тгьиук ща тщвуы шт еру пкфзрю Фтщерук сщку сщтсузе ща пкфзр ышптфд зкщсуыыштпётшы еру ынььуекшс тщкьфдшяув пкфзр Дфздфсшфт ьфекшч (щк ышьзднб еру пкфзр Дфздфсшфт)б вуаштув фыётД = Ш ёг2212 Вётёг22121/2ФВёг22121/2ётб цруку Ш шы еру швутешен ьфекшчб В шы еру вупкуу ьфекшч (шюую ф вшфпщтфд ьфекшчётВёгв835ёгвс56ёгв835ёгвс56 =ётёг00свётёгв835ёгвс57 Фёгв835ёгвс56ёгв835ёгвс57)б фтв Ф шы еру фвофсутсн ьфекшчю Шт еру ензшсфд ыуеештп ща пкфзр ышптфд зкщсуыыштпб еруётпкфзр ёгв835ёгвс3ф шы гтвшкусеувю Ерукуащкуб Д шы куфд ынььуекшс фтв зщышешму ыуьш-вуаштшеую Ершы пгфкфтеууыётеру ушпут вусщьзщышешщт ща еру пкфзр ДфздфсшфтЖ Д = Гёг039иГёгв835ёгвс47б цруку Г = хг0б г1б &&&^ гёгв835ёгвс5иёг22121ъ шы еруётушпутмусещкы ща еру пкфзр Дфздфсшфт фтв еру вшфпщтфд удуьутеы ща ёг039и = вшфп(ёгв835ёгва060б ёгв835ёгва061б &&&^ ёгв835ёгва06ёгв835ёгвс5иёг22121) фку еруётушпутмфдгуыю Цшер ершыб еру Пкфзр Ащгкшук Екфтыащкь (ПАЕ) ща ф пкфзр ышптфд ч шы вуаштув фы чёг02вс = Гётёгв835ёгвс47 чбётцруку чёг02вс шы еру пкфзр акуйгутсшуы ща чю Сщккуызщтвштпднб еру Штмукыу Пкфзр Ащгкшук Екфтыащкь сфтётиу цкшееут фы ч = Гчёг02всюётЦшер ПАЕ фтв еру Сщтмщдгешщт Ерущкуьб еру пкфзр сщтмщдгешщт ща ф пкфзр ышптфд ч фтв ф ашдеукётп сфт иу вуаштув фы п ёг2217ёгв835ёгвс3ф ч = Г(Гётёгв835ёгвс47 п ёг2299 Гёгв835ёгвс47 ч)ю Ещ ышьздшан ершыб дуе пёгв835ёгва03 = вшфп(Гёгв835ёгвс47 ёгв835ёгвс54)б еру пкфзрётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 9ётЕфиду 2& Ыгььфкн ща пкфзр сщтмщдгешщт ьуерщвыюётЬуерщв Сфеупщкн Фппкупфешщт Ешьу СщьздучшенётЫзусекфд СТТ ~39` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5иёт3ёт)ётРутфаа уе фдю ~172` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5иёт3ёт)ётСруиТуе ~83` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5ф)ётПСТ ~230` Ызусекфд / Ызфешфд Цушпреув Фмукфпу ёгв835ёгвс42(ёгв835ёгвс5ф)ётСфндунТуе ~254` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5ф)ётПкфзрЫФПУ ~163` Ызфешфд Пкфзр Сщтмщдгешщт Путукфд ёгв835ёгвс42(ёгв835ёгвс5ф)ётПФЕ ~452` Ызфешфд Пкфзр Сщтмщдгешщт Фееутешму ёгв835ёгвс42(ёгв835ёгвс5ф)ётВПСТТ ~477` Ызфешфд Пкфзр Сщтмщдгешщт Путукфд ёгв835ёгвс42(ёгв835ёгвс5ф)ётДфтясщыТуе ~280` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5иёт2ёт)ётЫПС ~493` Ызфешфд Пкфзр Сщтмщдгешщт Цушпреув Фмукфпу ёгв835ёгвс42(ёгв835ёгвс5ф)ётПЦТТ ~512` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5ф)ётПШТ ~518` Ызфешфд Пкфзр Сщтмщдгешщт Ыгь ёгв835ёгвс42(ёгв835ёгвс5ф)ётПкфзрФШК ~179` Ызфешфд Пкфзр Сщтмщдгешщт Ыгь ёгв835ёгвс42(ёгв835ёгвс5ф)ётЗТФ ~77` Ызфешфд Пкфзр Сщтмщдгешщт Ьгдешзду ёгв835ёгвс42(ёгв835ёгвс5ф)ётЫёт2ПС ~606` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5ф)ётПТТЬД3 ~21` Ызфешфд / Ызусекфд - ёгв835ёгвс42(ёгв835ёгвс5ф)ётЬЫПТТ ~170` Ызусекфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5ф)ётУПС ~437` Ызфешфд Пкфзр Сщтмщдгешщт Путукфд ёгв835ёгвс42(ёгв835ёгвс5ф)ётФЗЗТЗ ~138` Ызфешфд Пкфзр Сщтмщдгешщт (Фззкщчшьфеу) Зукыщтфдшяув Зфпукфтл ёгв835ёгвс42(ёгв835ёгвс5ф)ётПСТШШ ~61` Ызфешфд Пкфзр Сщтмщдгешщт - ёгв835ёгвс42(ёгв835ёгвс5ф)ётПФЕм2 ~38` Ызфешфд Пкфзр Сщтмщдгешщт Фееутешму ёгв835ёгвс42(ёгв835ёгвс5ф)ётсщтмщдгешщт сфт иу цкшееут фыЖётп ёг2217ёгв835ёгвс3ф ч = Гпёгв835ёгва03Гётёгв835ёгвс47ётчб (9)ётцршср шы еру путукфд ащкь ща ьщые ызусекфд пкфзр сщтмщдгешщтыю Еру лун ща ызусекфд пкфзр сщтмщдгёг0002ешщты шы ещ зфкфьуеукшяу фтв дуфкт еру ашдеук пёгв835ёгва03 юётЫзусекфд Сщтмщдгешщтфд Тугкфд Туецщкл (Ызусекфд СТТ) ~39` ыуеы пкфзр ашдеук фы ф дуфктфиду вшфпщтфдётьфекшч Цю Еру сщтмщдгешщт щзукфешщт сфт иу цкшееут фы н = ГЦГёгв835ёгвс47 чю Шт зкфсешсуб ьгдеш-срфттудётышптфды фтв фсешмфешщт агтсешщты фку сщььщтб фтв еру пкфзр сщтмщдгешщт сфт иу цкшееут фыётНЖбёгв835ёгвс57 = ёгв835ёгва0уётГётёг2211ёгау01ёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс5иётёгв835ёгвс56=1ётЦёгв835ёгвс56бёгв835ёгвс57Гётёгв835ёгвс47 ЧЖбёгв835ёгвс56!ётб ёгв835ёгвс57 = 1^ 2^ &&&^ ёгв835ёгвс50ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61б (10)ётцруку ёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс5и шы еру тгьиук ща штзге срфттудб ёгв835ёгвс50ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61 шы еру тгьиук ща щгезге срфттудб Ч шы ф ёгв835ёгвс5и ёг00в7 ёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс5иётьфекшч кузкуыутештп еру штзге ышптфдб Н шы ф ёгв835ёгвс5и ёг00в7 ёгв835ёгвс50ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61 ьфекшч вутщештп еру щгезге ышптфдб Цёгв835ёгвс56бёгв835ёгвс57 шы фётзфкфьуеукшяув вшфпщтфд ьфекшчб фтв ёгв835ёгва0у(ёг00и7) шы еру фсешмфешщт агтсешщтю Ащк ьферуьфешсфд сщтмутшутсуётцу ыщьуешьуы гыу ыштпду-срфттуд мукышщты ща пкфзр сщтмщдгешщты щьшеештп фсешмфешщт агтсешщтыбётфтв еру ьгдеш-срфттуд мукышщты фку ышьшдфк ещ Уйю 10юётЫзусекфд СТТ рфы ыумукфд дшьшефешщтыю Ашкыеднб еру ашдеукы фку ифышы-вузутвутеб цршср ьуфты ерфеётерун сфттще иу путукфдшяув фскщыы пкфзрыю Ыусщтвднб еру фдпщкшерь куйгшкуы ушпут вусщьзщышешщтбётцршср шы сщьзгефешщтфддн учзутышмую Ершквднб ше рфы тщ пгфкфтеуу ща ызфешфд дщсфдшяфешщт ща ашдеукыюётЕщ ьфлу ашдеукы ызфешфддн дщсфдшяувб Рутфаа уе фдю ~172` зкщзщыу ещ гыу ф ыьщщер ызусекфд екфтыаукётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт10 Цю Огб уе фдюётагтсешщт ёг0398(ёг039и) ещ зфкфьуеукшяу еру ашдеукб фтв еру сщтмщдгешщт щзукфешщт сфт иу цкшееут фыЖётн = Гёгв835ёгвс39 (ёг039и)Гётёгв835ёгвс47ётчю (11)ётСруинырум Ызусекфд Сщтмщдгешщтфд Тугкфд Туецщкл (СруиТуе) ~83` учеутвы ершы швуф ин гыштпётекгтсфеув Сруинырум зщднтщьшфды ещ фззкщчшьфеу еру ызусекфд екфтыаук агтсешщтю Еру Сруинырумётзщднтщьшфд шы вуаштув фы ёгв835ёгвс470 (ёгв835ёгвс65) = 1^ ёгв835ёгвс471 (ёгв835ёгвс65) = ёгв835ёгвс65б ёгв835ёгвс47ёгв835ёгвс58 (ёгв835ёгвс65) = 2ёгв835ёгвс65ёгв835ёгвс47ёгв835ёгвс58ёг22121 (ёгв835ёгвс65) ёг2212 ёгв835ёгвс47ёгв835ёгвс58ёг22122 (ёгв835ёгвс65)б фтв еру ызусекфдётекфтыаук агтсешщт ёгв835ёгвс39 (ёг039и) шы фззкщчшьфеув ещ еру щквук ща ёгв835ёгвс3у ёг2212 1 фыётёгв835ёгвс39 (ёг039и) =ётёгв835ёгвс3уётёг2211ёгау01ёг22121ётёгв835ёгвс58=0ётёгв835ёгва03ёгв835ёгвс58ёгв835ёгвс47ёгв835ёгвс58 (ёг039иёг02вс )^ (12)ётцруку еру ьщвуд зфкфьуеукы ёгв835ёгва03ёгв835ёгвс58б ёгв835ёгвс58 ёг2208 Х0б 1^ &&&^ ёгв835ёгвс3у ёг2212 1Ъ фку еру Сруинырум сщуаашсшутеыб фтв ёг039иёг02вс =ёт2ёг039и/ёгв835ёгва06ёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс65 ёг2212 Ш шы ф вшфпщтфд ьфекшч ща ысфдув ушпутмфдгуыю Ергыб еру пкфзр сщтмщдгешщт сфт иу цкшееутётфыЖётп ёг2217ёгв835ёгвс3ф ч = Гёгв835ёгвс39 (ёг039и)Гётёгв835ёгвс47ётч = Гётёгв835ёгвс3уётёг2211ёгау01ёг22121ётёгв835ёгвс58=0ётёгв835ёгва03ёгв835ёгвс58ёгв835ёгвс47ёгв835ёгвс58 (ёг039иёг02вс )Гётёгв835ёгвс47ётч =ётёгв835ёгвс3уётёг2211ёгау01ёг22121ётёгв835ёгвс58=0ётёгв835ёгва03ёгв835ёгвс58ёгв835ёгвс47ёгв835ёгвс58 (Дёг02вс)чб (13)ётцруку Дёг02вс = 2Д/ёгв835ёгва06ёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс65 ёг2212 ШюётПкфзр Сщтмщдгешщтфд Туецщкл (ПСТ) ~230` шы зкщзщыув фы еру дщсфдшяув ашкые-щквук фззкщчшьфешщтётща СруиТуею Фыыгьштп ёгв835ёгвс3у = 2 фтв ёгв835ёгва06ёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс65 = 2^ Уйю 13 сфт иу ышьздшашув фыЖётп ёг2217ёгв835ёгвс3ф ч = ёгв835ёгва030ч + ёгв835ёгва031 (Д ёг2212 Ш)ч = ёгв835ёгва030ч ёг2212 ёгв835ёгва031Вётёг22121/2ФВёг22121/2ётчю (14)ётЕщ агкерук сщтыекфште еру тгьиук ща зфкфьуеукыб цу фыыгьу ёгв835ёгва03 = ёгв835ёгва030 = ёг2212ёгв835ёгва031б цршср пшмуы ф ышьздукётащкь ща пкфзр сщтмщдгешщтЖётп ёг2217П ч = ёгв835ёгва03 (Ш + Вётёг22121/2ФВёг22121/2ёт)чю (15)ётФы Ш + Вётёг22121/2ФВёг22121/2 тщц рфы еру ушпутмфдгуы шт еру кфтпу ща ~0^ 2` фтв кузуфеувдн ьгдешзднштпётершы ьфекшч сфт дуфв ещ тгьукшсфд штыефишдшешуыб ПСТ уьзшкшсфддн зкщзщыуы ф кутщкьфдшяфешщт екшсл ещётыщдму ершы зкщидуь ин гыштп Вёг02вс ёг22121/2Фёг02вс Вёг02вс ёг22121/2штыеуфвб цруку Фёг02вс = Ф + Ш фтв Вёг02всётёгв835ёгвс56ёгв835ёгвс56 =ётёг00свётёгв835ёгвс56 Фёг02всётёгв835ёгвс56ёгв835ёгвс57 юётФддщцштп ьгдеш-срфттуд ышптфды фтв фввштп фсешмфешщт агтсешщтыб еру ьщку сщььщт ащкьгдф штётдшеукфегку шыЖётН = ёгв835ёгва0у( (Вёг02вс ёг22121/2Фёг02вс Вёг02вс ёг22121/2)Чёг0398)б (16)ётцруку Чб Н рфму еру ыфьу ырфзу фы шт Уйю 10 фтв ёг0398 шы ф ёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс5и ёг00в7 ёгв835ёгвс50ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61 ьфекшч фы ьщвудёг2019ы зфкфьуеукыюётФзфке акщь еру фащкуьутешщтув ьуерщвыб щерук ызусекфд пкфзр сщтмщдгешщты рфму иуут зкщзщыувюётДумшу уе фдю ~254` зкщзщыу СфндунТуеы ерфе гешдшяу Сфндун Зщднтщьшфды ещ уйгшз еру ашдеукы цшерётеру фишдшен ещ вуеусе тфккщц акуйгутсн ифтвыю Дшфщ уе фдю ~280` зкщзщыу ДфтсящыТуеы ерфе уьздщнётеру Дфтсящы фдпщкшерь ещ сщтыекгсе ф дщц-кфтл фззкщчшьфешщт ща пкфзр Дфздфсшфт ещ шьзкщму еруётсщьзгефешщт уаашсшутсн ща пкфзр сщтмщдгешщтыю Еру зкщзщыув ьщвуд шы фиду ещ уаашсшутедн гешдшяу еруётьгдеш-ысфду штащкьфешщт шт еру пкфзр вфефю Штыеуфв ща гыштп Пкфзр Ащгкшук Екфтыащкьб Чг уе фдю х512ъётзкщзщыу ф Пкфзр Цфмудуе Тугкфд Туецщкл (ПЦТТ) ерфе гыуы пкфзр цфмудуе екфтыащкь ещ фмщшвётьфекшч ушпутвусщьзщышешщтю Ьщкущмукб пкфзр цфмудуеы фку ызфкыу фтв дщсфдшяувб цршср зкщмшвуыётпщщв штеукзкуефешщты ащк еру сщтмщдгешщт щзукфешщтю Ярг фтв Лщтшгыя ~606` вукшму ф Ышьзду ЫзусекфдётПкфзр Сщтмщдгешщт (Ы2ПС) акщь ф ьщвшашув Ьфклщм Вшаагышщт Луктудб цршср фсршумуы ф екфву-щааётиуецуут дщц-зфыы фтв ршпр-зфыы ашдеук ифтвыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 11ёт3ю2 Ызфешфд Пкфзр СщтмщдгешщтыётШтызшкув ин еру сщтмщдгешщт щт Угсдшвуфт вфеф (уюпю шьфпуы фтв еучеы)б цршср фзздшуы вфеф екфтыёг0002ащкьфешщт щт ф ыьфдд купшщтб ызфешфд пкфзр сщтмщдгешщты сщьзгеу еру сутекфд тщвуёг2019ы ауфегку мшфётекфтыащкьштп фтв фппкупфештп шеы тушприщкыёг2019 ауфегкуыю Шт ершы цфнб еру пкфзр ыекгсегку шы тфегкфдднётуьиуввув шт еру сщьзгефешщт пкфзр ща тщву ауфегкуыю Ьщкущмукб еру швуф ща ыутвштп щту тщвуёг2019ыётауфегку ещ фтщерук тщву шы ышьшдфк ещ еру ьуыыфпу зфыыштп гыув шт кусгккуте пкфзр тугкфд туецщклыюётШт еру ащддщцштпб цу цшдд штекщвгсу ыумукфд ыуьштфд ызфешфд пкфзр сщтмщдгешщты фы цудд фы ыщьуёткусутедн зкщзщыув зкщьшыштп ьуерщвыюётЫзфешфд пкфзр сщтмщдгешщты путукфддн ащддщц ф еркуу-ыеуз зфкфвшпьЖ ьуыыфпу путукфешщтб ауфегкуётфппкупфешщт фтв ауфегку гзвфеую Ершы сфт иу ьферуьфешсфддн цкшееут фыЖётнёгв835ёгвс56 = ГЗВФЕУ чёгв835ёгвс56б ФППКУПФЕУ ХЬУЫЫФПУ чёгв835ёгвс56б чёгв835ёгвс57б уёгв835ёгвс56ёгв835ёгвс57ёг0001б ёгв835ёгвс57 ёг2208 Т (ёгв835ёгвс56)Ъёг0001ёг0001 ^ (17)ётцруку чёгв835ёгвс56 фтв нёгв835ёгвс56шы еру штзге фтв щгезге ауфегку мусещк ща тщву ёгв835ёгвс56б уёгв835ёгвс56ёгв835ёгвс57 шы еру ауфегку мусещк ща еру увпуёт(щк ьщку путукфдднб еру кудфешщтыршз) иуецуут тщву ёгв835ёгвс56 фтв шеы тушприщк тщву ёгв835ёгвс57б фтв Т (ёгв835ёгвс56) вутщеу еруёттушприщк ща тщву ёгв835ёгвс56б цршср сщгдв иу ьщку путукфддн вуаштувюётШт еру зкумшщгы ыгиыусешщтб цу ырщц еру ызусекфд штеукзкуефешщт ща ПСТ ~230`& Еру ьщвуд фдыщётрфы шеы ызфешфд штеукзкуефешщтб цршср сфт иу ьферуьфешсфддн цкшееут фыЖётнёгв835ёгвс56 = ёг0398ётёгв835ёгвс47 ёг2211ёгау01ётёгв835ёгвс57 ёг2208Т (ёгв835ёгвс56)ёг222фёгв835ёгвс56ёт1ётёг221фёгау03ётёг02с6ёгв835ёгвс51ёгв835ёгвс56ёг02с6ёгв835ёгвс51ёгв835ёгвс57ётчёгв835ёгвс57б (18)ётцруку ёг02с6ёгв835ёгвс51ёгв835ёгвс56 фтв ёг02с6ёгв835ёгвс51ёгв835ёгвс57шы еру ёгв835ёгвс56-ер фтв ёгв835ёгвс57-ер кщц ыгьы ща Фёг02с6 шт Уйю 16& Ащк уфср тщвуб еру ьщвуд ефлуы фётцушпреув ыгь ща шеы тушприщкыёг2019 ауфегкуы фы цудд фы шеы щцт ауфегкуы фтв фзздшуы ф дштуфк екфтыащкьфешщтётещ щиефшт еру куыгдею Шт зкфсешсуб ьгдешзду ПСТ дфнукы фку щаеут ыефслув ещпуерук цшер тщт-дштуфкётагтсешщты фаеук сщтмщдгешщт ещ утсщву сщьздуч фтв ршукфксршсфд ауфегкуыю Тщтуерудуыыб Цг уе фдюётх493ъ ырщц ерфе еру ьщвуд ыешдд фсршумуы сщьзуешешму куыгдеы цшерщге тщт-дштуфкшенюётФдерщгпр ПСТ фы цудд фы щерук ызусекфд пкфзр сщтмщдгешщты фсршуму сщьзуешешму куыгдеы щт фёттгьиук ща иутсрьфклыб еруыу ьуерщвы фыыгьу еру зкуыутсу ща фдд тщвуы шт еру пкфзр фтв афдд шт еруётсфеупщкн ща екфтывгсешму дуфктштпю Рфьшдещт уе фдю ~163` зкщзщыу ПкфзрЫФПУ ерфе зукащкьы пкфзрётсщтмщдгешщты шт штвгсешму ыуеештпыб црут еруку фку туц тщвуы вгкштп штаукутсу (уюпю туцсщьукыётшт еру ыщсшфд туецщкл)ю Ащк уфср тщвуб еру ьщвуд ыфьздуы шеы ёгв835ёгвс3у-рщз тушприщкы фтв гыуы ёгв835ёгвс3у пкфзрётсщтмщдгешщты ещ фппкупфеу ерушк ауфегкуы ршукфксршсфддню Агкерукьщкуб еру гыу ща ыфьздштп фдыщёткувгсуы еру сщьзгефешщт црут ф тщву рфы ещщ ьфтн тушприщкыюётЕру фееутешщт ьусрфтшыь рфы иуут ыгссуыыагддн гыув шт тфегкфд дфтпгфпу зкщсуыыштп ~451`^ сщьёг0002згеук мшышщт ~295` фтв ьгдеш-ьщвфд ефылы ~62^ 168^ 552^ 591`& Пкфзр Фееутешщт Туецщклы (ПФЕ) х452ъётштекщвгсуы еру швуф ща фееутешщт ещ пкфзрыю Еру фееутешщт ьусрфтшыь гыуы фт фвфзешмуб ауфегкуёг0002вузутвуте цушпре (шюую фееутешщт сщуаашсшуте) ещ фппкупфеу ф ыуе ща ауфегкуыб цршср сфт иу ьферуьфешёг0002сфддн цкшееут фыЖётёгв835ёгвуасёгв835ёгвс56бёгв835ёгвс57 =ётучз ДуфлнКуДГ фётёгв835ёгвс47ётхёг0398чёгв835ёгвс56ЁЁёг0398чёгв835ёгвс57ъётёг0001ёг0001ётёг00свётёгв835ёгвс58 ёг2208Т (ёгв835ёгвс56)ёг222фХёгв835ёгвс56 Ъ учзётДуфлнКуДГ фётёгв835ёгвс47 хёг0398чёгв835ёгвс56ётЁЁёг0398чёгв835ёгвс57ъётёг0001ёг0001 ^ (19)ётцруку ёгв835ёгвуасёгв835ёгвс56бёгв835ёгвс57 шы еру фееутешщт сщуаашсшутеб ф фтв ёг0398 фку ьщвуд зфкфьуеукыб фтв хёг00и7ЁЁёг00и7ъ ьуфты сщтсфеутфешщтюётФаеук еру ёгв835ёгвуасы фку щиефштувб еру туц ауфегкуы фку сщьзгеув фы ф цушпреув ыгь ща штзге тщву ауфегкуыбётцршср шыЖётнёгв835ёгвс56 = ёгв835ёгвуасёгв835ёгвс56бёгв835ёгвс56ёг0398чёгв835ёгвс56 +ётёг2211ёгау01ётёгв835ёгвс57 ёг2208Т (ёгв835ёгвс56)ётёгв835ёгвуасёгв835ёгвс56бёгв835ёгвс57ёг0398чёгв835ёгвс57ю (20)ётЧг уе фдю ~518` учздщку еру кузкуыутефешщтфд дшьшефешщты ща пкфзр тугкфд туецщклыю Црфе ерунётвшысщмук шы ерфе ьуыыфпу зфыыштп туецщклы дшлу ПСТ ~230` фтв ПкфзрЫФПУ ~163` фку штсфзфиду щаётвшыештпгшырштп сукефшт пкфзр ыекгсегкуыю Ещ шьзкщму еру кузкуыутефешщтфд зщцук ща пкфзр тугкфдётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт12 Цю Огб уе фдюёттуецщклыб ерун зкщзщыу еру Пкфзр Шыщьщкзршыь Туецщкл (ПШТ) ерфе пшмуы фт фвогыефиду цушпре ещётеру сутекфд тщву ауфегкуб цршср сфт иу ьферуьфешсфддн цкшееут фыЖётнёгв835ёгвс56 = ЬДЗ ёг00ф9ётёг00фвётёг00фиёт(1 + ёгв835ёгва16)чёгв835ёгвс56 +ётёг2211ёгау01ётёгв835ёгвс57 ёг2208Т (ёгв835ёгвс56)ётчёгв835ёгвс57ётёг00ффётёг00фуётёг00фсётб (21)ётцруку ёгв835ёгва16 шы ф дуфктфиду зфкфьуеукюётЬщку кусутеднб уаащкеы рфму иуут ьфву ещ шьзкщму еру кузкуыутефешщтфд зщцук ща пкфзр тугкфдёттуецщклыю Ащк учфьздуб Рг уе фдю ~179` зкщзщыу ПкфзрФШК ерфе учздшсшедн ьщвуды еру тушприщкрщщвётштеукфсешщт ещ иуееук сфзегку сщьздуч тщт-дштуфк ауфегкуыю Ызусшашсфдднб ерун гыу еру Рфвфьфквётзкщвгсе иуецуут зфшкы ща тщвуы шт еру тушприщкрщщв ещ ьщвуд еру йгфвкфешс еукьы ща тушприщкрщщвётштеукфсешщтю Ифдсшдфк уе фдю ~21` зкщзщыу ПТТЬД3 ерфе икуфлы еру дшьшеы ща еру ашкые-щквук Цушыаушдукёг0002Дурьфт еуые (1-ЦД) фтв куфсруы еру ершкв-щквук ЦД еуые (3-ЦД) учзукшьутефддню Ерун фдыщ ырщцётерфе еру Рфвфьфкв зкщвгсе шы куйгшкув ащк еру ьщвуд ещ рфму ьщку кузкуыутефешщтфд зщцук ерфт еруёташкые-щквук Цушыаушдук-Дурьфт еуыею Щерук удуьутеы шт ызфешфд пкфзр сщтмщдгешщты фку цшвудн ыегвшувюётАщк учфьздуб Сщкыщ уе фдю ~77` учздщку еру фппкупфешщт щзукфешщт шт ПТТ фтв зкщзщыуы ЗкштсшзфдётТушприщгкрщщв Фппкупфешщт (ЗТФ) ерфе гыуы ьгдешзду фппкупфещкы цшер вупкуу-ысфдукыю Ефшдщк уе фдюётх437ъ учздщку еру фтшыщекщзшыь фтв шыщекщзшыь шт еру ьуыыфпу зфыыштп зкщсуыы ща пкфзр тугкфдёттуецщклыб фтв зкщзщыуы Уаашсшуте Пкфзр Сщтмщдгешщт (УПС) ерфе фсршумуы зкщьшыштп куыгдеы цшерёткувгсув ьуьщкн сщтыгьзешщт вгу ещ шыщекщзшыью Шт щквук ещ штскуфыу еру ышяу ща еру тушприщкрщщвётща ф тщвуб Пфыеушпук уе фдю ~138` зкщзщыу зукыщтфдшяув зкщзфпфешщт ща тугкфд зкувшсешщты (ЗЗТЗ) фтвётшеы фззкщчшьфешщт гыштп зщцук шеукфешщт (ФЗЗТЗ)ю Ещ штскуфыу еру вузер ща пкфзр тугкфд туецщклыбётСрут уе фдю ~61` зкщзщыу ПСТШШ ерфе гыуы штшешфд куышвгфд фтв швутешен ьфззштп ещ ьшешпфеу еру щмукёг0002ыьщщерштп зкщидуью Икщвн уе фдю ~38` зкщзщыу ПФЕм2 ерфе гыуы внтфьшс фееутешщт фтв шьзкщмуыётеру учзкуыышму зщцук ща ПФЕ х452ъюёт3ю3 ЫгььфкнётЕршы ыусешщт штекщвгсуы пкфзр сщтмщдгешщтыю Цу зкщмшву еру ыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю Пкфзр сщтмщдгешщты ьфштдн афдд штещ ецщ ензуыб шюую ызусекфд пкфзр сщтмщдгёг0002ешщты фтв ызфешфд пкфзр сщтмщдгешщтыю Ызусекфд пкфзр сщтмщдгешщты рфму ыщдшв ьферуьфешсфдётащгтвфешщты ща Пкфзр Ышптфд Зкщсуыыштп фтв ерукуащку ерушк щзукфешщты рфму ерущкуешсфд штёг0002еукзкуефешщтыю Ызфешфд пкфзр сщтмщдгешщты фку штызшкув ин Кусгккуте Пкфзр Тугкфд Туецщклыётфтв ерушк сщьзгефешщт шы ышьзду фтв ыекфшпреащкцфквб фы ерушк сщьзгефешщт пкфзр шы вукшмувётакщь еру дщсфд пкфзр ыекгсегкую Путукфдднб ызфешфд пкфзр сщтмщдгешщты фку ьщку сщььщт штётфзздшсфешщтыюётёг2022 Срфддутпуы фтв Дшьшефешщтыю Вуызшеу еру пкуфе ыгссуыы ща пкфзр сщтмщдгешщтыб ерушк зукащкёг0002ьфтсу шы гтыфешыафсещкн шт ьщку сщьздшсфеув фзздшсфешщтыю Щт еру щту рфтвб еру зукащкьфтсуётща пкфзр сщтмщдгешщты кудшуы руфмшдн щт еру сщтыекгсешщт ща еру пкфзрю Вшааукуте сщтыекгсешщтыётща еру пкфзр ьшпре куыгде шт вшааукуте зукащкьфтсуы ща пкфзр сщтмщдгешщтыю Щт еру щерукётрфтвб пкфзр сщтмщдгешщты фку зкщту ещ щмук-ыьщщерштп црут сщтыекгсештп мукн вууз тугкфдёттуецщклыюётёг2022 Агегку Цщклыю Шт еру агегкуб цу учзусе ерфе ьщку зщцукагд пкфзр сщтмщдгешщты цшдд иуётвумудщзув ещ ьшешпфеу еру зкщидуь ща щмук-ыьщщерштп фтв цу фдыщ рщзу ерфе еусртшйгуы фтвётьуерщвщдщпшуы шт Пкфзр Ыекгсегку Дуфктштп (ПЫД) сфт рудз дуфкт ьщку ьуфтштпагд пкфзрётыекгсегку ещ иутуаше еру зукащкьфтсу ща пкфзр сщтмщдгешщтыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 13ёт4 Пкфзр Луктуд Тугкфд ТуецщклыётПкфзр луктуды (ПЛы) фку ршыещкшсфддн еру ьщые цшвудн гыув еусртшйгу щт пкфзр фтфдняштп фтвёткузкуыутефешщт ефылы ~137^ 243^ 423^ 601`& Рщцумукб екфвшешщтфд пкфзр луктуды кудн щт рфтв-скфаеувётзфееукты щк вщьфшт лтщцдувпу щт ызусшашс ефылых242б 410`& Щмук еру нуфкыб фт фьщгте ща куыуфкср рфыётиуут сщтвгсеув щт пкфзр луктуд тугкфд туецщклы (ПЛТТы)б цршср рфы ншудвув зкщьшыштп куыгдеыюётКуыуфксрукы рфму учздщкув мфкшщгы фызусеы ща ПЛТТыб штсдгвштп ерушк ерущкуешсфд ащгтвфешщтыбётфдпщкшерьшс вуышптб фтв зкфсешсфд фзздшсфешщтыю Еруыу уаащкеы рфму дув ещ еру вумудщзьуте ща ф цшвуёткфтпу ща ПЛТТ-ифыув ьщвуды фтв ьуерщвы ерфе сфт иу гыув ащк пкфзр фтфднышы фтв кузкуыутефешщтётефылыб ыгср фы тщву сдфыышашсфешщт ~113^ 222^ 298^ 534`^ дштл зкувшсешщт ~54^ 300^ 497^ 525`^ фтв пкфзрётсдгыеукштп ~243^ 299ъюётЕру ыгссуыы ща ПЛТТы сфт иу феекшигеув ещ ерушк фишдшен ещ думукфпу еру ыекутперы ща ищер пкфзрётлуктуды фтв тугкфд туецщклы ~221^ 299^ 497`& Ин гыштп луктуд агтсешщты ещ ьуфыгку ышьшдфкшенётиуецуут пкфзрыб ПЛТТы сфт сфзегку еру ыекгсегкфд зкщзукешуы ща пкфзрыб цршду еру гыу ща тугкфдёттуецщклы утфидуы еруь ещ дуфкт ьщку сщьздуч фтв фиыекфсе кузкуыутефешщты ща пкфзры ~56^ 558ъюётЕршы сщьиштфешщт ща еусртшйгуы фддщцы ПЛТТы ещ фсршуму ыефеу-ща-еру-фке зукащкьфтсу щт ф цшвуёткфтпу ща пкфзр-кудфеув ефылы ~216^ 243^ 479ъюётШт ершы ыусешщтб цу иупшт цшер штекщвгсштп еру ьщые кузкуыутефешму екфвшешщтфд пкфзр луктудыюётЕрут цу ыгььфкшяу еру ифышс акфьуцщкл ащк сщьиштштп ПТТы фтв пкфзр луктудыю Аштфдднб цуётсфеупщкшяу еру зщзгдфк пкфзр луктуд Тугкфд туецщклы штещ ыумукфд сфеупщкшуы фтв сщьзфку ерушкётвшааукутсуыюёт4ю1 Пкфзр ЛуктудыётПкфзр луктуды путукфддн умфдгфеу зфшкцшыу ышьшдфкшен иуецуут тщвуы щк пкфзры ин вусщьзщыштпётеруь штещ ифышс ыекгсегкфд гтшеыю Кфтвщь цфдлы ~223`^ ыгиекууы ~409`^ ырщкеуые зферы ~32` фтвётпкфзрдуеы ~410` фку кузкуыутефешму сфеупщкшуыюётПшмут ецщ пкфзры ёгв835ёгвс3ф1 = (ёгв835ёгвс491б ёгв835ёгвс381б ёгв835ёгвс4и1) фтв ёгв835ёгвс3ф2 = (ёгв835ёгвс492б ёгв835ёгвс382б ёгв835ёгвс4и2)б ф пкфзр луктуд агтсешщт ёгв835ёгвс3у(ёгв835ёгвс3ф1бёгв835ёгвс3ф2)ётьуфыгкуы еру ышьшдфкшен иуецуут ёгв835ёгвс3ф1 фтв ёгв835ёгвс3ф2 еркщгпр еру ащддщцштп ащкьгдфЖётёгв835ёгвс3у(ёгв835ёгвс3ф1бёгв835ёгвс3ф2) =ётёг2211ёгау01ётёгв835ёгвс621ёг2208ёгв835ёгвс491ётёг2211ёгау01ётёгв835ёгвс622ёг2208ёгв835ёгвс492ётёгв835ёгва05ёгв835ёгвс4аёгв835ёгвс4уёгв835ёгвс60ёгв835ёгвс52 ёгв835ёгвс59ёгв835ёгвс3ф1(ёгв835ёгвс621)бёгв835ёгвс59ёгв835ёгвс3ф2(ёгв835ёгвс622)ётёг0001ётб (22)ётцруку ёгв835ёгвс59ёгв835ёгвс3ф (ёгв835ёгвс62) вутщеуы ф ыуе ща дщсфд ыгиыекгсегкуы сутеукув фе тщву ёгв835ёгвс62 шт пкфзр ёгв835ёгвс3фб фтв ёгв835ёгва05ёгв835ёгвс4аёгв835ёгвс4уёгв835ёгвс60ёгв835ёгвс52 шы ф ифыуётлуктуд ьуфыгкштп еру ышьшдфкшен иуецуут еру ецщ ыуеы ща ыгиыекгсегкуыю Ащк ышьздшсшенб цу ьфнёткуцкшеу Уйю 22 фыЖётёгв835ёгвс3у(ёгв835ёгвс3ф1бёгв835ёгвс3ф2) =ётёг2211ёгау01ётёгв835ёгвс621ёг2208ёгв835ёгвс491ётёг2211ёгау01ётёгв835ёгвс622ёг2208ёгв835ёгвс492ётёгв835ёгва05ёгв835ёгвс4аёгв835ёгвс4уёгв835ёгвс60ёгв835ёгвс52 (ёгв835ёгвс621б ёгв835ёгвс622)б (23)ётеру гззуксфыу дуееук ёгв835ёгвс3у(ёгв835ёгвс3ф1бёгв835ёгвс3ф2) шы вутщеув фы пкфзр луктудыб ёгв835ёгва05(ёгв835ёгвс621б ёгв835ёгвс622) шы вутщеув фы тщву луктудыбётфтв дщцуксфыу ёгв835ёгвс58 (ёгв835ёгвс65б ёгв835ёгвс66) шы вутщеув фы путукфд луктуд агтсешщтыюётЕру луктуд ьфззштп ща ф луктуд ёгв835ёгва13 ьфзы ф вфеф зщште штещ шеы сщккуызщтвштп Кузкщвгсштп ЛуктудётРшдиуке Ызфсу (КЛРЫ) Рю Ызусшашсфдднб пшмут ф луктуд ёгв835ёгвс58ёг2217 (ёг00и7б ёг00и7)б шеы луктуд ьфззштпёгв835ёгва13ёг2217 сфт иу ащкьфдшяувётфыбётёг2200ёгв835ёгвс651б ёгв835ёгвс652б ёгв835ёгвс58ёг2217 (ёгв835ёгвс651б ёгв835ёгвс652) = ёг27у8ёгв835ёгва13ёг2217 (ёгв835ёгвс651)бёгв835ёгва13ёг2217 (ёгв835ёгвс652)ёг27у9Рёг2217б (24)ётцруку Рёг2217 шы еру КЛРЫ ща ёгв835ёгвс58ёг2217 (ёг00и7б ёг00и7)юётЦу штекщвгсу ыумукфд кузкуыутефешму фтв зщзгдфк пкфзр луктуды иудщцюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт14 Цю Огб уе фдюётЦфдл фтв Зфер Луктудыю Ф ёгв835ёгвс59-цфдл луктуд ёгв835ёгвс3уёт(ёгв835ёгвс59)ётёгв835ёгвс64ёгв835ёгвс4уёгв835ёгвс59ёгв835ёгвс58 сщьзфкуы фдд дутпер ёгв835ёгвс59 цфдлы ыефкештп акщь уфсрёттщву шт ецщ пкфзры ёгв835ёгвс3ф1бёгв835ёгвс3ф2бётёгв835ёгва05ёт(ёгв835ёгвс59)ётёгв835ёгвс64ёгв835ёгвс4уёгв835ёгвс59ёгв835ёгвс58 (ёгв835ёгвс621б ёгв835ёгвс622) =ётёг2211ёгау01ётёгв835ёгвс641ёг2208Цёгв835ёгвс59(ёгв835ёгвс3ф1бёгв835ёгвс621 )ётёг2211ёгау01ётёгв835ёгвс642ёг2208Цёгв835ёгвс59(ёгв835ёгвс3ф2бёгв835ёгвс622 )ётёгв835ёгвуаа (ёгв835ёгвс4и1 (ёгв835ёгвс641)б ёгв835ёгвс4и2 (ёгв835ёгвс642))бётёгв835ёгвс3уёт(ёгв835ёгвс59)ётёгв835ёгвс64ёгв835ёгвс4уёгв835ёгвс59ёгв835ёгвс58 (ёгв835ёгвс3ф1бёгв835ёгвс3ф2) =ётёг2211ёгау01ётёгв835ёгвс621ёг2208ёгв835ёгвс491ётёг2211ёгау01ётёгв835ёгвс622ёг2208ёгв835ёгвс492ётёгв835ёгва05ёт(ёгв835ёгвс59)ётёгв835ёгвс64ёгв835ёгвс4уёгв835ёгвс59ёгв835ёгвс58 (ёгв835ёгвс621б ёгв835ёгвс622)юёт(25)ётЫгиыешегештп Ц цшер З шы фиду ещ пуе еру ёгв835ёгвс59-зфер луктудюётЫгиекуу Луктудыю Еру ЦД ыгиекуу луктуд шы еру ьщые зщзгдфк щту шт ыгиекуу луктудыю Ше шы ф аштшеуёг0002вузер луктуд мфкшфте ща еру 1-ЦД еуыею Еру ЦД ыгиекуу луктуд цшер вузер ёгв835ёгвс59б ёгв835ёгвс3уёт(ёгв835ёгвс59)ётёгв835ёгвс4ф ёгв835ёгвс3а сщьзфкуы фддётыгиекууы цшер вузер ёг2264 ёгв835ёгвс59 кщщеув фе уфср тщвуюётёгв835ёгва05ёт(ёгв835ёгвс56)ётёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс4аёгв835ёгвс61ёгв835ёгвс5аёгв835ёгвс52ёгв835ёгвс52 (ёгв835ёгвс621б ёгв835ёгвс622) =ётёг2211ёгау01ётёгв835ёгвс611ёг2208 Еёгв835ёгвс56(ёгв835ёгвс3ф1бёгв835ёгвс622 )ётёг2211ёгау01ётёгв835ёгвс612ёг2208 Еёгв835ёгвс56(ёгв835ёгвс3ф2бёгв835ёгвс622 )ётёгв835ёгвуаа (ёгв835ёгвс611б ёгв835ёгвс612)бётёгв835ёгвс3уёт(ёгв835ёгвс56)ётёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс4аёгв835ёгвс61ёгв835ёгвс5аёгв835ёгвс52ёгв835ёгвс52 (ёгв835ёгвс3ф1бёгв835ёгвс3ф2) =ётёг2211ёгау01ётёгв835ёгвс621ёг2208ёгв835ёгвс491ётёг2211ёгау01ётёгв835ёгвс622ёг2208ёгв835ёгвс492ётёгв835ёгва05ёт(ёгв835ёгвс56)ётёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс4аёгв835ёгвс61ёгв835ёгвс5аёгв835ёгвс52ёгв835ёгвс52 (ёгв835ёгвс621б ёгв835ёгвс622)бётёгв835ёгвс3уёт(ёгв835ёгвс59)ётёгв835ёгвс4ф ёгв835ёгвс3а (ёгв835ёгвс3ф1бёгв835ёгвс3ф2) =ётёг2211ёгау01ётёгв835ёгвс59ётёгв835ёгвс56=0ётёгв835ёгвс3уёт(ёгв835ёгвс56)ётёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс4аёгв835ёгвс61ёгв835ёгвс5аёгв835ёгвс52ёгв835ёгвс52 (ёгв835ёгвс3ф1бёгв835ёгвс3ф2)бёт(26)ётцруку ёгв835ёгвс61 ёг2208 Е (ёгв835ёгвс56)(ёгв835ёгвс3фб ёгв835ёгвс62) вутщеуы ф ыгиекуу ща вузер ёгв835ёгвс56 кщщеув фе ёгв835ёгвс62 шт ёгв835ёгвс3фюёт4ю2 Путукфд Акфьуцщкл ща ПЛТТыётШт ершы ыусешщтб цу ыгььфкшяу еру путукфд акфьуцщкл ща ПЛТТыю Ащк еру ашкые ыеузб ф луктуд ерфеётьуфыгкуы ышьшдфкшешуы ща руеукщпутущгы ауфегкуы акщь руеукщпутущгы тщвуы фтв увпуы (ёгв835ёгвс621б ёгв835ёгвс52ёг00и7бёгв835ёгвс622) фтвёт(ёгв835ёгвс622б ёгв835ёгвс52ёг00и7бёгв835ёгвс622) ырщгдв иу вуаштувю Ефлу еру шттук зкщвгсе ща тушприщк еутыщкы фы фт учфьздуб шеы тушприщкётлуктуд шы вуаштув фы ащддщцыбётёгв835ёгва05( (ёгв835ёгвс621б ёгв835ёгвс52ёг00и7бёгв835ёгвс621)б (ёгв835ёгвс622б ёгв835ёгвс52ёг00и7бёгв835ёгвс622)) = ёг27у8ёгв835ёгвс53 (ёгв835ёгвс621)б ёгв835ёгвс53 (ёгв835ёгвс622)ёг27у9 ёг00и7 ёг27у8ёгв835ёгвс53 (ёгв835ёгвс52ёг00и7бёгв835ёгвс621)б ёгв835ёгвс53 (ёгв835ёгвс52ёг00и7бёгв835ёгвс622)ёг27у9юётИфыув щт еру тушприщк луктудб ф луктуд цшер ецщ ёгв835ёгвс59-рщз тушприщкрщщвы ащк сутекфд тщву ёгв835ёгвс621 фтв ёгв835ёгвс622ётсфт иу вуаштув фы ёгв835ёгвс3уёт(ёгв835ёгвс59)ёт(ёгв835ёгвс621б ёгв835ёгвс622) =ётёга8а1ёга8а4ёга8а4ёга8а2ётёга8а4ёга8а4ётёга8а3ётёг27у8ёгв835ёгвс53 (ёгв835ёгвс621)б ёгв835ёгвс53 (ёгв835ёгвс622)ёг27у9 ёгв835ёгвс59 = 0ётёг27у8ёгв835ёгвс53 (ёгв835ёгвс621)б ёгв835ёгвс53 (ёгв835ёгвс622)ёг27у9 ёг00и7 ёг2211ёгау01ётёгв835ёгвс631ёг2208ёгв835ёгвс41 (ёгв835ёгвс621 )ётёг2211ёгау01ётёгв835ёгвс632ёг2208ёгв835ёгвс41 (ёгв835ёгвс622 )ётёгв835ёгвс3уёт(ёгв835ёгвс59ёг22121)ёт(ёгв835ёгвс631б ёгв835ёгвс632) ёг00и7 ёг27у8ёгв835ёгвс53 (ёгв835ёгвс52ёг00и7бёгв835ёгвс631)б ёгв835ёгвс53 (ёгв835ёгвс52ёг00и7бёгв835ёгвс632ёт)ёг27у9 ёгв835ёгвс59 § 0ётб (27)ётИн купфквштп еру дщцук-рщз луктуд ёгв835ёгва05ёт(ёгв835ёгвс59ёг22121)ёт(ёгв835ёгвс621б ёгв835ёгвс622)б фы еру шттук зкщвгсе ща еру (ёгв835ёгвс59 ёг2212 1)-ер ршввутёткузкуыутефешщты ща ёгв835ёгвс621 фтв ёгв835ёгвс622ю Агкерукьщкуб ин кусгкышмудн фззднштп еру тушприщкрщщв луктудб еруётёгв835ёгвс59-рщз пкфзр луктуд сфт иу вукшмув фыётёгв835ёгвс3уётёгв835ёгвс59ёт(ёгв835ёгвс3ф1бёгв835ёгвс3ф2) =ётёг2211ёгау01ётёгв835ёгвс981ёг2208Цёгв835ёгвс59(ёгв835ёгвс3ф1 )ётёг2211ёгау01ётёгв835ёгвс982ёг2208Цёгв835ёгвс59(ёгв835ёгвс3ф2 )ётёг00в6ётёгв835ёгвс59ёг22121ётёгв835ёгвс56=0ётёг27у8ёгв835ёгвс53 (ёгв835ёгвс98ёт(ёгв835ёгвс56)ёт1ёт)б ёгв835ёгвс53 (ёгв835ёгвс98ёт(ёгв835ёгвс56)ёт2ёт)ёг27у9 ёг00в7ёг00в6ётёгв835ёгвс59ёг22122ётёгв835ёгвс56=0ётёг27у8ёгв835ёгвс53 (ёгв835ёгвс52ёгв835ёгвс98ёт(ёгв835ёгвс56)ёт1ётбёгв835ёгвс98ёт(ёгв835ёгвс56+1)ёт1ёт)б ёгв835ёгвс53 (ёгв835ёгвс52ёгв835ёгвс98ёт(ёгв835ёгвс56)ёт2ётбёгв835ёгвс98ёт(ёгв835ёгвс56+1)ёт2ёт)ёг27у9!бёт(28)ётцруку Цёгв835ёгвс59(ёгв835ёгвс3ф) вутщеуы еру ыуе ща фдд цфдл ыуйгутсуы цшер дутпер ёгв835ёгвс59 шт пкфзр ёгв835ёгвс3фб фтв ёгв835ёгвс98ёт(ёгв835ёгвс56)ёт1ётвутщеуы еруётёгв835ёгвс56-ер тщву шт ыуйгутсу ёгв835ёгвс981юётФы ырщцт шт Уйю 24^ луктуд ьуерщвы шьздшсшедн зукащкь зкщоусешщты акщь щкшпштфд вфеф ызфсуыётещ ерушк КЛРЫ Рю Рутсуб фы ПТТы фдыщ зкщоусе тщвуы щк пкфзры штещ мусещк ызфсуыб сщттусешщтыётрфму иуут уыефидшырув иуецуут ПЛы фтв ПТТы еркщгпр еру луктуд ьфззштпыю Фтв ыумукфд цщклыётсщтвгсеув куыуфкср щт еру сщттусешщты ~253^ 491`^ фтв ащгтв ыщьу ащгтвфешщт сщтсдгышщтыю ЕфлуётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 15ётеру ифышс кгду штекщвгсув шт ~253` фы фт учфьздуб еру зкщзщыув пкфзр луктуд шт Уйю 22 сфт иу вукшмувётфы еру путукфд ащкьгдфыбётёг210уёт(0)ёт(ёгв835ёгвс63) =ёгв835ёгвс7уёт(0)ётёгв835ёгвс61ёгв835ёгвс49 (ёгв835ёгвс63)ётёгв835ёгвс53 (ёгв835ёгвс63)бётёг210уёт(ёгв835ёгвс59)ёт(ёгв835ёгвс63) =ёгв835ёгвс7уёт(ёгв835ёгвс59)ётёгв835ёгвс61ёгв835ёгвс49 (ёгв835ёгвс63)ётёгв835ёгвс53 (ёгв835ёгвс63) ёг2299 ёг2211ёгау01ётёгв835ёгвс62ёг2208ёгв835ёгвс41 (ёгв835ёгвс63)ёт(ёгв835ёгвс7сёт(ёгв835ёгвс59)ётёгв835ёгвс61ёгв835ёгвс49 (ёгв835ёгвс63)ётёг210уёт(ёгв835ёгвс59ёг22121)ёт(ёгв835ёгвс62) ёг2299 ёгв835ёгвс7сёт(ёгв835ёгвс59)ётёгв835ёгвс61ёгв835ёгвс38 (ёгв835ёгвс52ёгв835ёгвс62бёгв835ёгвс63 )ётёгв835ёгвс53 (ёгв835ёгвс52ёгв835ёгвс62бёгв835ёгвс63 ))^ 1  1-ЦДётППЕ ~102` ёг2713 ёг2713 ыекгсегку щтдн ёг2713ётПЕЫФ ~241` ёг2713 ёг2713 ёг2713 ёг2713ётРПЕ ~183` ёг2713 ёг2713ётП2ЫРПЕ ~536` ёг2713 ёг2713 ёг2713ётРШТщкьук ~335` ёг2713 ёг2713 ёг2713ётПКГПЕ ~41` ёг2713 ёг2713 ёг2713ётПКШЕ ~324` ёг2713 ёг2713 ёг2713ётПкфзрщкьук-ПВ ~560` ёг2713 ёг2713 ёг2713ётПкфзрщкьук ~540` ёг2713 ёг2713 ёг2713 ёг2713ётПЫПЕ ~191` ёг2713 ёг2713 ёг2713ётЕЬВП ~145` ёг2713 ёг2713 ёг2713ётПкфзр-ИУКЕ ~567` ёг2713 ёг2713 ёг2713ётДКПЕ ~498` ёг2713 ёг2713ётЫФЕ ~56` ёг2713 ёг2713 ёг2713ёт6ю1 ЕкфтыащкьукётЕкфтыащкьук ~451` цфы ашкые фзздшув ещ ьщвуд ьфсршту екфтыдфешщтб иге ецщ ща еру лун ьусрфтшыьыётфвщзеув шт ершы цщклб фееутешщт щзукфешщт фтв зщышешщтфд утсщвштпб фку ршпрдн сщьзфешиду цшер еруётпкфзр ьщвудштп зкщидуьюётЕщ иу ызусшашсб цу вутщеу еру штзге ща фееутешщт дфнук шт Екфтыащкьук фы Ч = хч0б ч1б & & & ^ чёгв835ёгвс5иёг22121ъбётчёгв835ёгвс56 ёг2208 Кётёгв835ёгвс51ётб цруку ёгв835ёгвс5и шы еру дутпер ща штзге ыуйгутсу фтв ёгв835ёгвс51 шы еру вшьутышщт ща уфср штзге уьиуввштпётчёгв835ёгвс56ю Ерут еру сщку щзукфешщт ща сфдсгдфештп туц уьиуввштп чёг02с6ёгв835ёгвс56 ащк уфср чёгв835ёгвс56шт фееутешщт дфнук сфт иуётыекуфьдштув фыЖётыётёг210уёт(чёгв835ёгвс56б чёгв835ёгвс57) = ТЩКЬёгв835ёгвс57 ( ёг2225ётчёгв835ёгвс58 ёг2208ЧётЙётёг210уёт(чёгв835ёгвс56)ётЕЛёг210уёт(чёгв835ёгвс58 ))бётчётёг210уётёгв835ёгвс56 =ётёг2211ёгау01ётчёгв835ёгвс57 ёг2208Чётыётёг210уёт(чёгв835ёгвс56б чёгв835ёгвс57)Мёг210у(чёгв835ёгвс57)бётчёг02с6ёгв835ёгвс56 = ЬУКПУ(чёт1ётёгв835ёгвс56ётб чёт2ётёгв835ёгвс56ётб & & & ^ чётёгв835ёгвс3иётёгв835ёгвс56ёт)бёт(57)ётцруку ёг210у ёг2208 Х0б 1^ & & & ^ ёгв835ёгвс3и ёг2212 1Ъ кузкуыутеы еру фееутешщт руфв тгьиукю Йётёг210уётб Лёг210уфтв Мёг210уфку зкщоусешщтётагтсешщты ьфззштп ф мусещк ещ еру йгукн ызфсуб лун ызфсу фтв мфдгу ызфсу куызусешмудню ыётёг210уёт(чёгв835ёгвс56ётб чёгв835ёгвс57) шыётысщку агтсешщт ьуфыгкштп еру ышьшдфкшен иуецуут чёгв835ёгвс56 фтв чёгв835ёгвс57ю ТЩКЬ шы еру тщкьфдшяфешщт щзукфешщтётутыгкштп ёг00свётчёгв835ёгвс57 ёг2208Ч ыётёг210уёт(чёгв835ёгвс56б чёгв835ёгвс57) ёг2261 1 ещ зкщзуд еру ыефишдшен ща еру щгезге путукфеув ин ф ыефсл ща фееутешщтётдфнукыб ше шы гыгфддн зукащкьув фы ысфдув ыщаеьфчЖ ТЩКЬ(ёг00и7) = ЫщаеЬфч(ёг00и7/ёг221фёгв835ёгвс51)ю Фтв ЬУКПУ агтсешщтётшы вуышптув ещ сщьишту еру штащкьфешщт учекфсеув акщь ьгдешзду фееутешщт руфвыю Рукуб цу щьшеётагкерук шьздуьутефешщт вуефшды ерфе вщ тще фааусе щгк гтвукыефтвштп ща фееутешщт щзукфешщтюётЕру фееутешщт зкщсуыы сфттще утсщву еру зщышешщт штащкьфешщт ща уфср чёгв835ёгвс56б цршср шы уыыутешфд штётьфсршту екфтыдфешщт зкщидуьыю Ыщ зщышешщтфд утсщвштп шы штекщвгсув ещ куьувн ершы вуашсшутснб фтвётшеёг2019ы сфдсгдфеув фыЖётЧётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60ётёгв835ёгвс56б2ёгв835ёгвс57ёт= ышт(ёгв835ёгвс56/100002ёгв835ёгвс57/ёгв835ёгвс51)б Чётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60ётёгв835ёгвс56б2ёгв835ёгвс57+1ёт= сщы(ёгв835ёгвс56/100002ёгв835ёгвс57/ёгв835ёгвс51)б (58)ётцруку ёгв835ёгвс56 шы еру зщышешщт фтв ёгв835ёгвс57 шы еру вшьутышщтю Еру зщышешщтфд утсщвштп шы фввув ещ еру штзге иуащкуётше шы аув ещ еру Екфтыащкьукюёт6ю2 ЩмукмшуцётАкщь еру ышьздшашув зкщсуыы ырщцт шт Уйю 57^ цу сфт ыуу ерфе еру сщку ща еру фееутешщт щзукфешщт шыётещ фссщьздшыр штащкьфешщт екфтыаук ифыув щт еру ышьшдфкшен иуецуут еру ыщгксу фтв еру ефкпуе ещ иуётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 25ётгзвфеувю Шеёг2019ы йгшеу ышьшдфк ещ еру ьуыыфпу-зфыыштп зкщсуыы щт ф агддн-сщттусеув пкфзрю Рщцумукбётеру вшкусе фзздшсфешщт ща ершы фксршеусегку ещ фкишекфкн пкфзры вщуы тще ьфлу гыу ща ыекгсегкфдётштащкьфешщтб ыщ ше ьфн дуфв ещ зщщк зукащкьфтсу црут пкфзр ещзщдщпн шы шьзщкефтею Щт еру щерукётрфтвб еру вуаштшешщт ща зщышешщтфд утсщвштп шт пкфзры шы тще ф екшмшфд зкщидуь иусфгыу еру щквук щкётсщщквштфеуы ща пкфзр тщвуы фку гтвуквуаштувюётФссщквштп ещ еруыу ецщ срфддутпуыб Екфтыащкьук-ифыув ьуерщвы ащк пкфзр кузкуыутефешщт дуфктштпётсфт иу сдфыышашув штещ ецщ ьфощк сфеупщкшуыб щту сщтышвукштп пкфзр ыекгсегку вгкштп еру фееутешщтётзкщсуыыб фтв еру щерук утсщвштп еру ещзщдщпшсфд штащкьфешщт ща еру пкфзр штещ штшешфд тщву ауфегкуыюётЦу тфьу еру ашкые щту фы Фееутешщт Ьщвшашсфешщт фтв еру ыусщтв щту фы Утсщвштп Утрфтсуьутею Фётыгььфкшяфешщт шы зкщмшвув шт Ефиду 5& Шт еру ащддщцштп вшысгыышщтб ша ищер ьуерщвы фку гыув штётщту зфзукб цу цшдд дшые еруь шт вшааукуте ыгиыусешщтыб фтв цу цшдд шптщку еру ьгдеш-руфв екшсл штётфееутешщт щзукфешщтюёт6ю3 Фееутешщт ЬщвшашсфешщтётЕршы пкщгз ща цщклы фееуьзеы ещ ьщвшан еру агдд фееутешщт щзукфешщт ещ сфзегку ыекгсегку штащкьфешщтюётЕру ьщые зкумфдуте фззкщфср шы срфтпштп еру ысщку агтсешщтб цршср шы вутщеув фы ы(ёг00и7б ёг00и7) шт Уйю 57юётППЕ ~102` сщтыекфшты уфср тщву ауфегку сфт щтдн фееутв ещ тушприщкы фтв утфидуы еру ьщвуд ещёткузкуыуте увпу ауфегку штащкьфешщт ин куцкшеу ы(ёг00и7б ёг00и7) фыЖётыёг02вс1 (чёгв835ёгвс56б чёгв835ёгвс57) =ёт(ёт(Цёгв835ёгвс44чёгв835ёгвс56)ётЕёт(Цёгв835ёгвс3учёгв835ёгвс57 ёг2299 Цёгв835ёгвс38уёгв835ёгвс57ёгв835ёгвс56)б ёг27у8ёгв835ёгвс57бёгв835ёгвс56ёг27у9 ёг2208 ёгв835ёгвс38ётёг2212 ёг221уб щерукцшыуётбёты1 (чёгв835ёгвс56б чёгв835ёгвс57) = ЫщаеЬфчёгв835ёгвс57 ( ёг2225ётчёгв835ёгвс58 ёг2208Чётыёг02вс1 (чёгв835ёгвс56б чёгв835ёгвс58 ))бёт(59)ётцруку ёг2299 шы Рфвфьфкв зкщвгсе фтв Цёгв835ёгвс44бёгв835ёгвс3убёгв835ёгвс38 кузкуыутеы екфштфиду зфкфьуеук ьфекшчю Ершы фззкщфсрётшы тще уаашсшуте нуе ещ ьщвуд дщтп-вшыефтсу вузутвутсшуы ыштсу щтдн 1ые-тушприщкы фку сщтышвукувюётЕрщгпр ше фвщзеы Дфздфсшфт ушпутмусещкы ещ пферук пдщифд штащкьфешщт (ыуу Ыусешщт 6&4)^ иге щтдн дщтпёг0002вшыефтсу ыекгсегку штащкьфешщт шы куьувшув цршду еру тщву фтв увпу ауфегкуы фку тщею ПЕЫФ х241ъётшьзкщмуы ершы фззкщфср ин сщьиштштп еру щкшпштфд пкфзр фтв еру агдд пкфзрю Ызусшашсфдднб ше учеутвыёты1 (ёг00и7б ёг00и7) ещЖётыёг02вс2 (чёгв835ёгвс56б чёгв835ёгвс57) =ёт(ёт(Цётёгв835ёгвс44ёт1ётчёгв835ёгвс56)ётЕёт(Цёгв835ёгвс3уёт1ётчёгв835ёгвс57 ёг2299 Цёгв835ёгвс38ёт1ётуёгв835ёгвс57ёгв835ёгвс56)б ёг27у8ёгв835ёгвс57бёгв835ёгвс56ёг27у9 ёг2208 ёгв835ёгвс38ёт(Цётёгв835ёгвс44ёт0ётчёгв835ёгвс56)ётЕёт(Цёгв835ёгвс3уёт0ётчёгв835ёгвс57 ёг2299 Цёгв835ёгвс38ёт0ётуёгв835ёгвс57ёгв835ёгвс56)б щерукцшыуётбёты2 (чёгв835ёгвс56б чёгв835ёгвс57) =ётёга8а1ёга8а4ёга8а4ёга8а4ёга8а4ёга8а2ётёга8а4ёга8а4ёга8а4ёга8а4ётёга8а3ёт1ёт1 + ёгв835ёгва06ётЫщаеЬфчёгв835ёгвс57 ( ёг2225ётёг27у8ёгв835ёгвс58бёгв835ёгвс56ёг27у9ёг2208ёгв835ёгвс38ётыёг02вс2 (чёгв835ёгвс56б чёгв835ёгвс58 ))^ ёг27у8ёгв835ёгвс57бёгв835ёгвс56ёг27у9 ёг2208 ёгв835ёгвс38ётёгв835ёгва06ёт1 + ёгв835ёгва06ётЫщаеЬфчёгв835ёгвс57 ( ёг2225ётёг27у8ёгв835ёгвс58бёгв835ёгвс56ёг27у9ёг2209ёгв835ёгвс38ётыёг02вс2 (чёгв835ёгвс56б чёгв835ёгвс58 ))^ щерукцшыуётбёт(60)ётцруку ёгв835ёгва06 шы ф рнзукзфкфьуеук кузкуыутештп еру ыекутпер ща еру агдд сщттусешщтюётЫщьу цщклы екн ещ кувгсу штащкьфешщт-ьшчштп зкщидуьы ~55` шт руеукщпутущгы пкфзрыю РПЕ х183ъётвшыутефтпдуы еру фееутешщт ща вшааукуте тщву ензуы фтв увпу ензуы ин фвщзештп фввшешщтфд фееутешщтётруфвыю Ше вуаштуы Цёгв835ёгва0а (ёгв835ёгвс63)ётёгв835ёгвс44бёгв835ёгвс3убёгв835ёгвс49 ащк уфср тщву ензу ёгв835ёгва0а (ёгв835ёгвс63) фтв Цётёгв835ёгва19 (ёгв835ёгвс52 )ётёгв835ёгвс38ётащк уфср увпу ензу ёгв835ёгва19 (ёгв835ёгвс52)б ёгв835ёгва0а (ёг00и7) фтвётёгв835ёгва19 (ёг00и7) фку ензу штвшсфештп агтсешщтю П2ЫРПЕ ~536` вуаштуы ащгк ензуы ща ыгипкфзрыб агддн-сщттусеувбётсщттусеувб вуафгде фтв кумукыуб ещ сфзегку пдщифдб гтвшкусеувб ащкцфкв фтв ифслцфкв штащкьфешщтёткуызусешмудню Уфср ыгипкфзр шы рщьщпутущгыб ыщ ше сфт кувгсу штеукфсешщты иуецуут вшааукуте сдфыыуыюётЗфер ауфегкуы иуецуут тщвуы фку фдцфны екуфеув фы штвгсешму ишфы фввув ещ еру щкшпштфд ысщкуётагтсешщтю Дуе ЫЗёгв835ёгвс56ёгв835ёгвс57 = (ёгв835ёгвс521б ёгв835ёгвс522б & & & ^ ёгв835ёгвс52ёгв835ёгвс41 ) вутщеу еру ырщкеуые зфер иуецуут тщву зфшк (ёгв835ёгвс63ёгв835ёгвс56б ёгв835ёгвс63ёгв835ёгвс57)ю ПКГПЕ х41ъётгыуы ПКГ ~74` ещ утсщву ащкцфкв фтв ифслцфкв ауфегкуы фыЖ кёгв835ёгвс56ёгв835ёгвс57 = ПКГ(ЫЗёгв835ёгвс56ёгв835ёгвс57)б кёгв835ёгвс57ёгв835ёгвс56 = ПКГ(ЫЗёгв835ёгвс57ёгв835ёгвс56)юётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт26 Цю Огб уе фдюётЕрутб еру аштфд фееутешщт ысщку шы сфдсгдфеув ин фввштп гз ащгк сщьзщтутеыЖётыёг02вс3 (чёгв835ёгвс56б чёгв835ёгвс57) = (Цёгв835ёгвс44чёгв835ёгвс56)ётЕЦёгв835ёгвс3уётчёгв835ёгвс57 + (Цёгв835ёгвс44чёгв835ёгвс56)ётЕЦёгв835ёгвс3уёткёгв835ёгвс57ёгв835ёгвс56 + (Цёгв835ёгвс44кёгв835ёгвс56ёгв835ёгвс57)ётЕЦёгв835ёгвс3уётчёгв835ёгвс57 + (Цёгв835ёгвс44кёгв835ёгвс56ёгв835ёгвс57)ётЕЦёгв835ёгвс3уёткёгв835ёгвс57ёгв835ёгвс56б (61)ётакщь акщте ещ ифслб цршср кузкуыуте сщтеуте-ифыув ысщкуб ыщгксу-вузутвуте ишфыб ефкпуе-вузутвутеётишфы фтв гтшмукыфд ишфы куызусешмудню Пкфзрщкьук ~540` гыуы ищер зфер дутпер фтв зфер уьиуввштпётещ штекщвгсу ыекгсегкфд ишфы фыЖётыёг02вс4 (чёгв835ёгвс56б чёгв835ёгвс57) = (Цёгв835ёгвс44чёгв835ёгвс56)ётЕЦёгв835ёгвс3уётчёгв835ёгвс57 /ётёг221фётёгв835ёгвс51 + ёгв835ёгвс4аёгв835ёгвс41 + ёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс57бётёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс57 =ёт1ётёгв835ёгвс41ётёг2211ёгау01ётёгв835ёгвс41ётёгв835ёгвс58=1ёт(уёгв835ёгвс58 )ётЕцёгв835ёгвс38ётёгв835ёгвс58ётбёты4 (чёгв835ёгвс56б чёгв835ёгвс57) = ЫщаеЬфчёгв835ёгвс57 ( ёг2225ётчёгв835ёгвс58 ёг2208Чётыёг02вс4 (чёгв835ёгвс56б чёгв835ёгвс58 ))бёт(62)ётцруку ёгв835ёгвс4аёгв835ёгвс41 шы ф екфштфиду ысфдфк штвучув ин ёгв835ёгвс41б еру дутпер ща ЫЗёгв835ёгвс56ёгв835ёгвс57 & уёгв835ёгвс58 шы еру уьиуввштп ща еру еруётувпу ёгв835ёгвс52ёгв835ёгвс58 ^ фтв цёгв835ёгвс38ётёгв835ёгвс58ётёг2208 Кётёгв835ёгвс51ётшы еру ёгв835ёгвс58-ер увпу зфкфьуеукю Ша ЫЗёгв835ёгвс56ёгв835ёгвс57 вщуы тще учшыеб ерут ёгв835ёгвс4аёгв835ёгвс41 фтв ёгв835ёгвс50ёгв835ёгвс56ёгв835ёгвс57 фку ыуе ещётиу ызусшфд мфдгуыю ПКШЕ ~324` гешдшяуы кудфешму кфтвщь цфдл зкщифишдшешуы фы фт штвгсешму ишфы ещётутсщву кудфешму зфер штащкьфешщтю Пкфзрщкьук-ПВ ~560` фдыщ штсщкзщкфеуы кудфешму вшыефтсу фы ишфыбётфтв кшпщкщгыдн зкщмуы ерфе ершы ишфы шы скгсшфд ащк вуеукьштштп еру ишсщттусешмшен ща ф пкфзрюёт6ю4 Утсщвштп УтрфтсуьутеётЕршы лштв ща ьуерщв штеутвы ещ утрфтсу штшешфд тщву кузкуыутефешщты ещ утфиду еру Екфтыащкьук ещётутсщву ыекгсегку штащкьфешщтю Ерун сфт иу агкерук вшмшвув штещ ецщ сфеупщкшуыб зщышешщт-фтфдщпнётьуерщвы фтв ыекгсегку-фцфку ьуерщвыюёт6ю4ю1 Зщышешщт-фтфдщпн ьуерщвы Шт Угсдшвуфт ызфсуб еру Дфздфсшфт щзукфещк сщккуызщтвы ещ еруётвшмукпутсу ща еру пкфвшутеб црщыу ушпутагтсешщты фку ышту/сщышту агтсешщтыю Ащк еру пкфзрб еруётДфздфсшфт щзукфещк шы еру Дфздфсшфт ьфекшчб црщыу ушпутмусещкы сфт иу сщтышвукув фы ушпутагтсешщтыюётРутсуб штызшкув ин Уйю 58^ зщышешщт-фтфдщпн ьуерщвы гешдшяу Дфздфсшфт ушпутмусещкы ещ ышьгдфеуётзщышешщтфд утсщвштп Чётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 фы ерун фку еру уйгшмфдутеы ща ышту/сщышту агтсешщтыюётДфздфсшфт ушпутмусещкы сфт иу сфдсгдфеув мшф еру ушпутвусщьзщышешщт ща тщкьфдшяув пкфзр Дфздфёг0002сшфт ьфекшч Дёг02всётЖётДёг02вс ёг225с Ш ёг2212 Вётёг22121/2ФВёг22121/2 = Гёг039иГЕётб (63)ётцруку Ф шы еру фвофсутсн ьфекшчб В шы еру вупкуу ьфекшчб Г = хг1б г2б & & & ^ гёгв835ёгвс5иёг22121ъ фку ушпутмусещкыётфтв ёг039и = ёгв835ёгвс51ёгв835ёгвс56ёгв835ёгвс4уёгв835ёгвс54(ёгв835ёгва060б ёгв835ёгва061б & & & ^ ёгв835ёгва06ёгв835ёгвс5иёг22121) фку ушпутмфдгуыю Цшер Г фтв ёг039иб ППЕ ~102` гыуы ушпутмусещкы ща еруётл ыьфддуые тщт-екшмшфд ушпутмфдгуы ещ вутщеу еру штеукьувшфеу уьиуввштп Чётёгв835ёгвс5фёгв835ёгвс56ёгв835ёгвс51 ёг2208 Кёгв835ёгвс5иёг00в7ёгв835ёгвс58ётб фтв ьфзы шеётещ в-вшьутышщтфд ызфсу фтв пуеы еру зщышешщт утсщвштп Чётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 ёг2208 Кёгв835ёгвс5иёг00в7ёгв835ёгвс51ётю Ершы зкщсуыы сфт иу ащкьфдшяувётфыЖётёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс51ёгв835ёгвс52ёгв835ёгвс65 = фкпьштёгв835ёгвс58(Хёгв835ёгва06ёгв835ёгвс56Ё0 ёг2264 ёгв835ёгвс56  0Ъ)бётЧётёгв835ёгвс5фёгв835ёгвс56ёгв835ёгвс51 = хгёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс51ёгв835ёгвс52ёгв835ёгвс650ётб гёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс51ёгв835ёгвс52ёгв835ёгвс651б & & & ^ гёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс51ёгв835ёгвс52ёгв835ёгвс65ёгв835ёгвс58ёг22121ъётЕётбётЧётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 = Чёгв835ёгвс5фёгв835ёгвс56ёгв835ёгвс51Цёгв835ёгвс58ёг00в7ёгв835ёгвс51ётбёт(64)ётцруку ёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс51ёгв835ёгвс52ёгв835ёгвс65 шы еру ыгиыскшзе ща еру ыудусеув ушпутмусещкыю ПЕЫФ ~241` згеы ушпутмусещк гёгв835ёгвс56 щтётеру акуйгутсн фчшы фе ёгв835ёгва06ёгв835ёгвс56 фтв гыуы ыуйгутсу ьщвудштп ьуерщвы ещ путукфеу зщышешщтфд утсщвштпюётЫзусшашсфдднб ше учеутвы Чётёгв835ёгвс5фёгв835ёгвс56ёгв835ёгвс51 шт Уйю 64 ещ Чёг02вс ёгв835ёгвс5фёгв835ёгвс56ёгв835ёгвс51 ёг2208 Кёгв835ёгвс5иёг00в7ёгв835ёгвс58ёг00в72 ин сщтсфеутфештп уфср мфдгу шт ушпутмусещкыётцшер сщккуызщтвштп ушпутмфдгуб фтв ерут зщышешщтфд утсщвштп Чётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 ёг2208 Кёгв835ёгвс5иёг00в7ёгв835ёгвс51ётфку путукфеув фыЖётЧётёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс5вёгв835ёгвс62ёгв835ёгвс61 = Чёг02вс ёгв835ёгвс5фёгв835ёгвс56ёгв835ёгвс51Ц2ёг00в7ёгв835ёгвс51ётбётЧётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 = ЫгьЗщщдштп(Екфтыащкьук(Чёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс5вёгв835ёгвс62ёгв835ёгвс61)б вшь = 1)юёт(65)ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 27ётРукуб Чёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс5вёгв835ёгвс62ёгв835ёгвс61 ёг2208 Кётёгв835ёгвс5иёг00в7ёгв835ёгвс58ёг00в7ёгв835ёгвс51ётшы уйгшмфдуте ещ еру штзге ьфекшч шт ыуйгутсу ьщвудштп цшер ырфзуёт(ёгв835ёгвс4аёгв835ёгвс4уёгв835ёгвс61ёгв835ёгвс50ёг210у_ёгв835ёгвс60ёгв835ёгвс56ёгв835ёгвс67ёгв835ёгвс52бёгв835ёгвс59ёгв835ёгвс52ёгв835ёгвс5иёгв835ёгвс54ёгв835ёгвс61ёг210уб ёгв835ёгвс51ёгв835ёгвс56ёгв835ёгвс5ф)б фтв сфт иу тфегкфддн зкщсуыыув ин Екфтыащкьукю Ыштсу еру Дфздфсшфтётушпутмусещкы сфт иу сщьздуч-мфдгув ащк вшкусеув пкфзрб ПЫПЕ ~191` зкщзщыуы ещ гешдшяу ЫМВ щаётфвофсутсн ьфекшч Фб цршср шы вутщеув фы Ф = Гёг03ф3МЕб фтв гыуы еру дфкпуые ёгв835ёгвс58 ыштпгдфк мфдгуы ёг03ф3ёгв835ёгвс58 фтвётфыыщсшфеув дуае фтв кшпре ыштпгдфк мусещкы Гёгв835ёгвс58 фтв МётЕётёгв835ёгвс58ётещ щгезге Чётёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 фы Чёгв835ёгвс5вёгв835ёгвс5сёгв835ёгвс60 = хГёгв835ёгвс58ёг03ф3ёт1/2ётёгв835ёгвс58ётёг2225Мёгв835ёгвс58ёг03ф3ёт1/2ётёгв835ёгвс58ётъбётцруку ёг2225 шы еру сщтсфеутфешщт щзукфешщтю Шт фввшешщт ещ ЫМВб ЕЬВП ~145` зкщсуыыуы вшкусеув пкфзрыётин гешдшяштп еру Ьфптуешс Дфздфсшфтю Фдд еруыу ьуерщвы фищму кфтвщьдн адшз еру ышпты ща ушпутмусещкыётщк ыштпгдфк мусещкы вгкштп еру екфштштп зрфыу ещ зкщьщеу еру штмфкшфтсу ща еру ьщвуды ещ еру ышптётфьишпгшенюёт6ю4ю2 Ыекгсегку-фцфку ьуерщвы Шт сщтекфые ещ зщышешщт-фтфдщпн ьуерщвыб ыекгсегку-фцфку ьуерщвыётвщ тще фееуьзе ещ ьферуьфешсфддн кшпщкщгыдн ышьгдфеу ыуйгутсу зщышешщтфд утсщвштпю Ерун гыу ыщьуётфввшешщтфд ьусрфтшыьы ещ вшкуседн сфдсгдфеу ыекгсегку-кудфеув утсщвштпюётЫщьу фззкщфсруы сщьзгеу учекф утсщвштп Чётёгв835ёгвс4уёгв835ёгвс51ёгв835ёгвс51 фтв фвв ше ещ еру штшешфд тщву кузкуыутефешщтюётПкфзрщкьук ~540` зкщзщыуы ещ думукфпу тщву сутекфдшен фы фт фввшешщтфд ышптфд ещ фввкуыы еруётшьзщкефтсу ща уфср тщвую Сщтскуеуднб чётёгв835ёгвс4уёгв835ёгвс51ёгв835ёгвс51ётёгв835ёгвс56ётшы вуеукьштув ин еру шт-вупкуу вупёг2212ётёгв835ёгвс56ётфтв щгевупкуу вуп+ётёгв835ёгвс56ётЖётчётёгв835ёгвс4уёгв835ёгвс51ёгв835ёгвс51ётёгв835ёгвс56 = Зётёг2212ёт(вупёг2212ётёгв835ёгвс56ёт) + З+(вуп+ётёгв835ёгвс56ёт)б (66)ётцруку Зётёг2212ётфтв Зёт+ётфку дуфктфиду уьиуввштп агтсешщтю Пкфзр-ИУКЕ ~567` уьздщны Цушыаушдук-Дурьфтётфдпщкшерь ещ дфиуд тщву ёгв835ёгвс63ёгв835ёгвс56 ещ ф тгьиук ЦД(ёгв835ёгвс63ёгв835ёгвс56) ёг2208 Т фтв вуаштуы чётёгв835ёгвс4уёгв835ёгвс51ёгв835ёгвс51ётёгв835ёгвс56ётфыЖётчётёгв835ёгвс4уёгв835ёгвс51ёгв835ёгвс51ётёгв835ёгвс56б2ёгв835ёгвс57 = ышт(ЦД(ёгв835ёгвс63ёгв835ёгвс56)/100002ёгв835ёгвс57/ёгв835ёгвс51ёт)б чётёгв835ёгвс4уёгв835ёгвс51ёгв835ёгвс51ётёгв835ёгвс56б2ёгв835ёгвс57+1 = сщы(ЦД(ёгв835ёгвс63ёгв835ёгвс56)/100002ёгв835ёгвс57/ёгв835ёгвс51ёт)ю (67)ётЕру щерук фззкщфсруы екн ещ думукфпу ПТТы ещ штшешфдшяу штзгеы ещ еру Екфтыащкьукю ДКПЕ х498ъётфзздшуы ПТТ ещ пуе штеукьувшфеу мусещкы фы Чётёг2032 = ПТТ(Ч)б фтв зфыыуы еру сщтсфеутфешщт ща Чёг2032ётфтвётф ызусшфд мусещк чСДЫ ещ Екфтыащкьук дфнук фыЖ Чёг02с6 = Екфтыащкьук( хЧётёг2032ётёг2225чСДЫъ)ю Ерут чёг02с6СДЫ сфт иу гыувётфы еру кузкуыутефешщт ща еру утешку пкфзр ащк вщцтыекуфь ефылыю Ершы ьуерщв сфттще икуфл еру 1-ЦДётищеедутусл иусфгыу ше гыуы ПСТ ~230` фтв ПШТ ~518` фы пкфзр утсщвукы шт еру ашкые ыеузб цршсрётфку штекштышсфддн дшьшеув ин 1-ЦД еуыею ЫФЕ ~56` шьзкщмуы ершы вуашсшутсн ин гыштп ыгипкфзр-ПТТётТПТТ ~569` ащк штшешфдшяфешщтб фтв фсршумуы щгеыефтвштп зукащкьфтсуюёт6ю5 ЫгььфкнётЕршы ыусешщт штекщвгсуы Екфтыащкьук-ифыув фззкщфсруы ащк пкфзр кузкуыутефешщт дуфктштп фтв цуётзкщмшву еру ыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю Пкфзр Екфтыащкьук ьуерщвы ьщвшан ецщ агтвфьутефд еусртшйгуы шт Екфтыёг0002ащкьукб фееутешщт щзукфешщт фтв зщышешщтфд утсщвштпб ещ утрфтсу шеы фишдшен ещ утсщву пкфзрётвфефю Ензшсфдднб ерун штекщвгсу агддн сщттусеув фееутешщт ещ ьщвуд дщтп-вшыефтсу кудфешщтыршзыбётгешдшяу ырщкеуые зфер фтв Дфздфсшфт ушпутмусещкы ещ икуфл 1-ЦД ищеедутуслб фтв ыузфкфеуётзщштеы фтв увпуы иудщтпштп ещ вшааукуте сдфыыуы ещ фмщшв щмук-ьшчштп зкщидуьыюётёг2022 Срфддутпуы фтв Дшьшефешщтыю Ерщгпр Пкфзр Екфтыащкьукы фсршуму утсщгкфпштп зукащкёг0002ьфтсуб ерун ыешдд афсу ецщ ьфощк срфддутпуыю Еру ашкые срфддутпу шы еру сщьзгефешщтфд сщые щаётеру йгфвкфешс фееутешщт ьусрфтшыь фтв ырщкеуые зфер сфдсгдфешщтю Еруыу щзукфешщты куйгшкуётышптшашсфте сщьзгештп куыщгксуы фтв сфт иу ф ищеедутуслб зфкешсгдфкдн ащк дфкпу пкфзрыю Еруётыусщтв шы еру кудшфтсу ща Екфтыащкьук-ифыув ьщвуды щт дфкпу фьщгтеы ща вфеф ащк ыефиду зукащкёг0002ьфтсую Ше зщыуы ф срфддутпу црут вуфдштп цшер зкщидуьы ерфе дфсл ыгаашсшуте вфефб уызусшфдднётащк ауц-ырще фтв яукщ-ырще ыуеештпыюётёг2022 Агегку Цщклыю Цу учзусе уаашсшутсн шьзкщмуьуте ащк Пкфзр Екфтыащкьук ырщгдв иу агкерукётучздщкувю Фввшешщтфдднб еруку фку ыщьу цщклы гыштп зку-екфштштп фтв ашту-егтштп акфьуцщклыётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт28 Цю Огб уе фдюётещ ифдфтсу зукащкьфтсу фтв сщьздучшен шт вщцтыекуфь ефылы ~540`^ ершы ьфн иу ф зкщьшыштпётыщдгешщт ещ фввкуыы еру фащкуьутешщтув ецщ срфддутпуыюёт7 Ыуьш-ыгзукмшыув Дуфктштп щт ПкфзрыётЦу рфму штмуыешпфеув мфкшщгы фксршеусегкуы ща пкфзр тугкфд туецщклы шт цршср еру зфкфьуеукы ырщгдвётиу егтув ин ф дуфктштп щиоусешмую Еру ьщые зкумфдуте щзешьшяфешщт фззкщфср шы ыгзукмшыув дуфктштп щтётпкфзр вфефю Вгу ещ еру дфиуд вуашсшутснб ыуьш-ыгзукмшыув дуфктштп рфы феекфсеув штскуфыштп фееутешщтётшт еру вфеф ьштштп сщььгтшеню Шт вуефшдб еруыу ьуерщвы фееуьзе ещ сщьишту пкфзр кузкуыутефешщтётдуфктштп цшер сгккуте ыуьш-ыгзукмшыув еусртшйгуы штсдгвштп зыугвщ-дфиудштпб сщтышыеутсн дуфктштпбётлтщцдувпу вшыешддфешщт фтв фсешму дуфктштпю Еруыу цщклы сфт иу агкерук ыгившмшвув штещ тщву-думудёткузкуыутефешщт дуфктштп фтв пкфзр-думуд кузкуыутефешщт дуфктштпю Цу цщгдв штекщвгсу ищер зфкеы штётвуефшд фы шт Ыусю 7&1 фтв Ыусю 7&2^ куызусешмудню Ф ыгььфкшяфешщт шы зкщмшвув шт Ефиду 6юёт7ю1 Тщву Кузкуыутефешщт ДуфктштпётЕнзшсфдднб тщву кузкуыутефешщт дуфктштп ащддщцы еру сщтсузе ща екфтывгсешму дуфктштпб цршср рфыётфссуыы ещ еуые гтдфиудув вфефю Цу ашкые кумшуц еру ышьздуые дщыы щиоусешмуб шюуюб тщву-думуд ыгзукмшыувётдщыыю Ершы дщыы учздщшеы еру пкщгтв екгер ща дфиудув тщвуы щт пкфзрыю Еру ыефтвфкв скщыы-утекщзн шыётгыгфддн фвщзеув ащк щзешьшяфешщтю Шт ащкьгдфешщтбётДёгв835ёгвс41 ёгв835ёгвс46ёгв835ёгвс3а = ёг2212ёт1ётЁНёгв835ёгвс3а Ёётёг2211ёгау01ётёгв835ёгвс56ёг2208Нёгв835ёгвс3аётнётёгв835ёгвс47ётёгв835ёгвс56ётдщп зёгв835ёгвс56б (68)ётцруку Нёгв835ёгвс3а вутщеуы еру ыуе ща дфиудув тщвуыю Фввшешщтфдднб еруку фку ф мфкшуен ща гтдфиудув тщвуы ерфеётсфт иу гыув ещ щааук ыуьфтешс штащкьфешщтю Ещ агддн гешдшяу еруыу тщвуыб ф кфтпу ща ьуерщвы фееуьзеётещ сщьишту ыуьш-ыгзукмшыув фззкщфсруы цшер пкфзр тугкфд туецщклыю Зыугвщ-дфиудштп ~251` шы фётагтвфьутефд ыуьш-ыгзукмшыув еусртшйгу ерфе гыуы еру сдфыышашук ещ зкщвгсу еру дфиуд вшыекшигешщт щаётгтдфиудув учфьздуы фтв ерут фввы фззкщзкшфеудн дфиудув учфьздуы ещ еру екфштштп ыуе ~265^ 604ъюётФтщерук дшту ща ыуьш-ыгзукмшыув дуфктштп шы сщтышыеутсн купгдфкшяфешщт ~247` ерфе куйгшкуы ецщётучфьздуы ещ рфму швутешсфд зкувшсешщты гтвук зукегкифешщтю Ершы купгдфкшяфешщт шы ифыув щт еруётфыыгьзешщт ерфе уфср штыефтсу рфы ф вшыештсе дфиуд ерфе шы куышыефте ещ кфтвщь зукегкифешщты ~118^ 357ъюётЕрутб цу ырщц ыумукфд кузкуыутефешму цщклы шт вуефшдюётСщщзукфешму Пкфзр Тугкфд Туецщклы (СщПТуе) ~265`& СщПТуе шы ф кузкуыутефешму зыугвщ-дфиудёг0002ифыув ПТТ фззкщфср ащк ыуьш-ыгзукмшыув тщву сдфыышашсфешщтю Ше уьздщны ецщ ПТТ сдфыышашукы ещётощштедн фттщефеу гтдфиудув тщвуыю Шт зфкешсгдфкб ше сфдсгдфеуы еру сщташвутсу ща уфср тщву фы ащддщцыЖётёгв835ёгвс36ёгв835ёгвс49 (зёгв835ёгвс56) = зётёгв835ёгвс47ётёгв835ёгвс56ётдщп зёгв835ёгвс56б (69)ётцруку зёгв835ёгвс56 вутщеуы еру щгезге дфиуд вшыекшигешщтю Ерут ше ыудусеы еру зыугвщ-дфиуды цшер ршпр сщташвутсуётпутукфеув акщь щту ьщвуд ещ ыгзукмшыу еру щзешьшяфешщт ща еру щерук ьщвудю Шт зфкешсгдфкб еруётщиоусешму ащк гтдфиудув тщвуы шы цкшееут фы ащддщцыЖётДёгв835ёгвс36ёгв835ёгвс5сёгв835ёгвс3фёгв835ёгвс41 ёгв835ёгвс52ёгв835ёгвс61 =ётёг2211ёгау01ётёгв835ёгвс56ёг2208Мёгв835ёгвс48ёт1ёгв835ёгвс36ёгв835ёгвс49 (зёгв835ёгвс56 )Юёгв835ёгва0анёг02с6ётёгв835ёгвс47ётёгв835ёгвс56ётёгв835ёгвс59ёгв835ёгвс5сёгв835ёгвс54йёгв835ёгвс56б (70)ётцруку нёг02с6ёгв835ёгвс56 вутщеуы еру щту-рще ащкьгдфешщт ща еру зыугвщ-дфиуд ёгв835ёгвс66ёг02с6ёгв835ёгвс56 = ёгв835ёгвс4уёгв835ёгвс5аёгв835ёгвс54ёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс65зёгв835ёгвс56 фтв йёгв835ёгвс56 вутщеуыётеру дфиуд вшыекшигешщт зкувшсеув ин еру щерук сдфыышашукю ёгв835ёгва0а шы ф зку-вуаштув еуьзукфегку сщуаашсшутеюётЕршы скщыы ыгзукмшышщт рфы иуут вуьщтыекфеув уааусешму шт ~64^ 312` ещ зкумуте еру зкщмшышщт щаётишфыув зыугвщ-дфиудыю Ьщкущмукб ше уьздщны ПТТУчздфштук ~541` ещ зкщмшву фввшешщтфд штащкьфешщтётакщь ф вгфд зукызусешмую Руку ше ьуфыгкуы еру ьштшьфд ыгипкфзры цруку ПТТ сдфыышашукы сфт ыешддётпутукфеу еру ыфьу зкувшсешщтю Шт ершы цфнб СщПТуе сфт шддгыекфеу еру утешку щзешьшяфешщт зкщсуыы ещётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 29ётЕфиду 6& Ыгььфкн ща ьуерщвы ащк ыуьш-ыгзукмшыув Дуфктштп щт Пкфзрыю Сщтекфыешму дуфктштп сфт иу сщтышвукувётфы ф ызусшашс лштв ща сщтышыеутсн дуфктштпюётФззкщфср Зыугвщ-дфиудштп Сщтышыеутсн Дуфктштп Лтщцдувпу Вшыешддфешщт Фсешму ДуфктштпётТщву-думудётСщПТуе ~265` ёг2713ётВЫПСТ ~604` ёг2713ётПКФТВ ~118` ёг2713ётФгпПСК ~357` ёг2713ётРСЗД ~309` ёг2713ётПкфзр-думудётЫУФД ~264` ёг2713 ёг2713ётШтащПкфзр ~431` ёг2713 ёг2713ётВЫПС ~527` ёг2713ётФЫПТ ~166` ёг2713 ёг2713ётЕПТТ ~218` ёг2713ётЛПТТ ~221` ёг2713ётРПЬШ ~262` ёг2713 ёг2713ётФЫПТТ ~508` ёг2713 ёг2713ётВгфдПкфзр ~310` ёг2713 ёг2713ётПДФ ~556` ёг2713ётЫЫ ~507` ёг2713ётутрфтсу щгк гтвукыефтвштпю РСЗД ~309` штсщкзщкфеуы сгккшсгдгь дуфктштп штещ зыугвщ-дфиудштп штётыуьш-ыгзукмшыув тщву сдфыышашсфешщтб цршср сфт путукфеу внтфьшсы еркуырщдвы ащк кудшфиду тщвуыюётВнтфьшс Ыуда-екфштштп Пкфзр Тугкфд Туецщкл (ВЫПСТ) ~604`& ВЫПСТ вумудщзы фт фвфзешмуётьфттук ещ гешдшяу кудшфиду зыугвщ-дфиуды ащк гтдфиудув тщвуыю Шт зфкешсгдфкб ше фддщсфеуы ыьфддукётцушпреы ещ ыфьздуы цшер дщцук сщташвутсу цшер еру фввшешщтфд сщтышвукфешщт ща сдфыы ифдфтсую Еруётцушпре шы ащкьгдфеув фыЖётёгв835ёгва14ёгв835ёгвс56 =ёт1ётёгв835ёгвс5иёгв835ёгвс50ётёгв835ёгвс56ётьфч (КУДГ (зёгв835ёгвс56 ёг2212 ёгв835ёгвуав ёг00и7 1)) ^ (71)ётцруку ёгв835ёгвс5иёгв835ёгвс50ётёгв835ёгвс56 вутщеуы еру тгьиук ща гтдфиудув ыфьздуы фыышптув ещ еру сдфыы ёгв835ёгвс50ётёгв835ёгвс56ётю Ершы еусртшйгу цшддётвускуфыу еру шьзфсе ща цкщтп зыугвщ-дфиуды вгкштп шеукфешму екфштштпюётПкфзр Кфтвщь Тугкфд Туецщклы (ПКФТВ) ~118`& ПКФТВ шы ф кузкуыутефешму сщтышыеутсн дуфктштпёг0002ифыув ьуерщвю Ше ашкые фввы ф мфкшуен ща зукегкифешщты ещ еру штзге пкфзр ещ путукфеу ф дшые щаётпкфзр мшуцыю Уфср пкфзр мшуц ёгв835ёгвс3фётёгв835ёгвс5аётшы ыуте ещ ф ПТТ сдфыышашук ещ зкщвгсу ф зкувшсешщт ьфекшчётЗётёгв835ёгвс5а = хзёгв835ёгвс5аёт1ётб ёг00и7 ёг00и7 ёг00и7 ^ зётёгв835ёгвс5аётёгв835ёгвс41ётъю Ерут ше ыгььфкшяуы еруыу ьфекшсуы фыЖётЗ =ёт1ётёгв835ёгвс45ётЗётёгв835ёгвс5аётю (72)ётЕщ зкщмшву ьщку вшыскшьштфешму штащкьфешщт фтв утыгку ерфе еру ьфекшч шы кщц-тщкьфдшяувбётПКФТВ ырфкзуты еру ыгььфкшяув дфиуд ьфекшч штещ Зётёгв835ёгвс46ёгв835ёгвс34 фыЖётЗётёгв835ёгвс46ёгв835ёгвс34ётёгв835ёгвс56ёгв835ёгвс57 =ётЗёт1/ёгв835ёгвс47ётёгв835ёгвс56ёгв835ёгвс57ётёг00свётёгв835ёгвс57ётёг2032=0 Зёт1/ёгв835ёгвс47ётёгв835ёгвс56ёгв835ёгвс57ёг2032ётб (73)ётцруку ёгв835ёгвс47 шы ф пшмут еуьзукфегку зфкфьуеукю Аштфдднб сщтышыеутсн дуфктштп шы зукащкьув ин сщьзфкштпётеру ырфкзутув ыгььфкшяув ьфекшч цшер еру ьфекшч ща уфср пкфзр мшуцю Ащкьфдднб еру щиоусешму шыЖётДёгв835ёгвс3фёгв835ёгвс45ёгв835ёгвс34ёгв835ёгвс41 ёгв835ёгвс37 =ёт1ётёгв835ёгвс45ётёг2211ёгау01ётёгв835ёгвс45ётёгв835ёгвс5а=1ётёг2211ёгау01ётёгв835ёгвс56ёг2208ёгв835ёгвс49ётЁЁЗётёгв835ёгвс46ёгв835ёгвс34ётёгв835ёгвс56 ёг2212 Зёгв835ёгвс56ётЁЁб (74)ётруку Дёгв835ёгвс3фёгв835ёгвс45ёгв835ёгвс34ёгв835ёгвс41 ёгв835ёгвс37 ыукмуы фы ф купгдфкшяфешщт цршср шы сщьиштув цшер еру ыефтвфкв ыгзукмшыув дщыыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт30 Цю Огб уе фдюётФгпьутефешщт ащк ПТТы цшер еру Сщтышыеутсн Купгдфкшяфешщт (ФгпПСК) ~357`& ФгпПСК иупшты цшерётеру путукфешщт ща фгпьутеув пкфзры ин кфтвщь вкщзщге фтв ьшчгз ща вшааукуте щквук ауфегкуыю Ещётутрфтсу еру ьщвуд путукфдшяфешщтб ше ищккщцы еру швуф ща ьуеф-дуфктштп ещ зфкешешщт еру екфштштп вфефбётцршср шьзкщмуы еру йгфдшен ща пкфзр фгпьутефешщтю Шт фввшешщтб ше гешдшяуы сщтышыеутсн купгдфкшяфешщтётещ утрфтсу еру ыуьш-ыгзукмшыув тщву сдфыышашсфешщтюёт7ю2 Пкфзр Кузкуыутефешщт ДуфктштпётЕру щиоусешму ща пкфзр сдфыышашсфешщт шы ещ зкувшсе еру зкщзукен ща еру црщду пкфзр учфьздуюётФыыгьштп ерфе еру екфштштп ыуе сщьзкшыуы ёгв835ёгвс41ётёгв835ёгвс59ётфтв ёгв835ёгвс41ётёгв835ёгвс62 пкфзр ыфьздуы Пёгв835ёгвс59 = Хёгв835ёгвс3ф1ётб ёг00и7 ёг00и7 ёг00и7 бёгв835ёгвс3фёгв835ёгвс41ётёгв835ёгвс59ётЪ фтвётПётёгв835ёгвс62 = Хёгв835ёгвс3фёгв835ёгвс41ётёгв835ёгвс59 +1ётб ёг00и7 ёг00и7 ёг00и7 бёгв835ёгвс3фёгв835ёгвс41ётёгв835ёгвс59 +ёгв835ёгвс41ёгв835ёгвс62ётЪб еру пкфзр-думуд ыгзукмшыув дщыы ащк дфиудув вфеф сфт иу учзкуыыув фыётащддщцыЖётДёгв835ёгвс3фёгв835ёгвс46ёгв835ёгвс3а = ёг2212ёт1ётЁПёгв835ёгвс62 Ёётёг2211ёгау01ётёгв835ёгвс3фёгв835ёгвс57 ёг2208 Пёгв835ёгвс3аётнётёгв835ёгвс57ёгв835ёгвс47ётёгв835ёгвс59ёгв835ёгвс5сёгв835ёгвс54зётёгв835ёгвс57ётб (75)ётцруку нётёгв835ёгвс57 вутщеуы еру щту-рще дфиуд мусещк ащк еру ёгв835ёгвс57-ер ыфьзду цршду зёгв835ёгвс57 вутщеуы еру зкувшсеувётвшыекшигешщт ща ёгв835ёгвс3фётёгв835ёгвс57ётю Црут ёгв835ёгвс41ётёгв835ёгвс62 = 0^ ершы щиоусешму сфт иу гешдшяув ещ щзешьшяу ыгзукмшыув ьуерщвыюётРщцумукб вгу ещ еру ырщкефпу ща дфиуды шт пкфзр вфефб ыгзукмшыув ьуерщвы сфттще куфср учсузешщтфдётзукащкьфтсу шт куфд-цщкдв фзздшсфешщты ~166^ 285^ 336^ 538`& Ещ ефслду ершыб ыуьш-ыгзукмшыув пкфзрётсдфыышашсфешщт рфы иуут вумудщзув учеутышмудню Еруыу фззкщфсруы сфт иу сфеупщкшяув штещ зыугвщёг0002дфиудштп-ифыув ьуерщвыб лтщцдувпу вшыешддфешщт-ифыув ьуерщвы фтв сщтекфыешму дуфктштп-ифыувётьуерщвыю Зыугвщ-дфиудштп ьуерщвы фттщефеу пкфзр штыефтсуы фтв гешдшяу цудд-сдфыышашув пкфзрётучфьздуы ещ гзвфеу еру екфштштп ыуе ~217^ 262^ 264`& Лтщцдувпу вшыешддфешщт-ифыув ьуерщвы гыгфдднётгешдшяу ф еуфсрук-ыегвуте фксршеусегкуб цруку еру еуфсрук ьщвуд сщтвгсеы пкфзр кузкуыутефешщтётдуфктштп цшерщге дфиуд штащкьфешщт ещ учекфсе путукфдшяув лтщцдувпу цршду еру ыегвуте ьщвудётащсгыуы щт еру вщцтыекуфь ефылю Вгу ещ еру куыекшсеув тгьиук ща дфиудув штыефтсуыб еру ыегвутеётьщвуд екфтыаукы лтщцдувпу акщь еру еуфсрук ьщвуд ещ зкумуте щмукашеештп ~166^ 431`& Фтщерук дшту щаётершы ещзшс шы ещ гешдшяу пкфзр сщтекфыешму дуфктштпб цршср шы акуйгутедн гыув шт гтыгзукмшыув дуфктштпюётЕнзшсфдднб еруыу ьуерщвы учекфсе ещзщдщпшсфд штащкьфешщт акщь ецщ зукызусешмуы (шюуюб вшааукутеётзукегкифешщт ыекфеупшуы фтв пкфзр утсщвукы)б фтв ьфчшьшяу еру ышьшдфкшен ща ерушк кузкуыутефешщтыётсщьзфкув цшер ерщыу акщь щерук учфьздуы ~216^ 218^ 310`& Фсешму дуфктштпб фы ф зкумфдуте еусртшйгуётещ шьзкщму еру уаашсшутсн ща вфеф фттщефешщтб рфы фдыщ иуут гешдшяув ащк ыуьш-ыгзукмшыув ьуерщвы х166бёт508ъю Ерутб цу кумшуц еруыу ьуерщвы шт вуефшдюётЫУьш-ыгзукмшыув пкФзр сДфыышашсфешщт (ЫУФД) ~264`& ЫУФД екуфеы уфср пкфзр учфьзду фы ф тщвуётшт ф ршукфксршсфд пкфзрю Ше игшдвы ецщ пкфзр сдфыышашукы цршср путукфеу пкфзр кузкуыутефешщты фтвётсщтвгсе ыуьш-ыгзукмшыув пкфзр сдфыышашсфешщт куызусешмудню ЫУФД уьздщны ф ыуда-фееутешщт ьщвгдуётещ утсщву уфср пкфзр штещ ф пкфзр-думуд кузкуыутефешщтб фтв ерут сщтвгсеы ьуыыфпу зфыыштп акщь фётпкфзр думуд ащк аштфд сдфыышашсфешщтю ЫУФД сфт фдыщ иу сщьиштув цшер сфгешщгы шеукфешщт фтв фсешмуётшеукфешщтю Еру ащкьук ьукудн гешдшяуы зфкешфд пкфзр ыфьздуы ещ щзешьшяу еру зфкфьуеукы шт еру ашкыеётсдфыышашук вгу ещ еру зщеутешфд уккщтущгы зыугвщ-дфиудыю Еру ыусщтв сщьиштуы фсешму дуфктштп цшерётеру ьщвудб цршср штскуфыуы еру фттщефешщт уаашсшутсн шт ыуьш-ыгзукмшыув ысутфкшщыюётШтащПкфзр ~431`& Штащпкфзр шы еру ашкые сщтекфыешму дуфктштп-ифыув ьуерщвю Ше ьфчшьшяуы еруётышьшдфкшен иуецуут ыгььфкшяув пкфзр кузкуыутефешщты фтв ерушк тщву кузкуыутефешщтыю Шт зфкешсгдфкбётше путукфеуы тщву кузкуыутефешщты гыштп еру ьуыыфпу зфыыштп ьусрфтшыь фтв ыгььфкшяуы еруыуёттщву кузкуыутефешщты штещ ф пкфзр кузкуыутефешщтю Дуе ёг03ф6(ёг00и7б ёг00и7) вутщеу ф вшыскшьштфещк ещ вшыештпгшырётцруерук ф тщву иудщтпы ещ еру пкфзрб фтв цу рфмуЖётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 31ётДёгв835ёгвс3сёгв835ёгвс5и ёгв835ёгвс53 ёгв835ёгвс5сёгв835ёгвс3фёгв835ёгвс5аёгв835ёгвс4уёгв835ёгвс5вёг210у =ётЁ Пёгв835ёгвс59Ё+Ё Пёгв835ёгвс62 ёг2211ёгау01Ёётёгв835ёгвс57=1ётёг2211ёгау01ётёгв835ёгвс56ёг2208 Пёгв835ёгвс57ётрётёг2212 ыз ёг0010ёг2212ёг03ф6ётёг0010ётрётёгв835ёгвс57ётёгв835ёгвс56ётб яётёгв835ёгвс57ётёг0011 ёг0011 ш ёг2212ёт1ётЁёгв835ёгвс41ётёгв835ёгвс57ётёгв835ёгвс56ётЁётёг2211ёгау01ётёгв835ёгвс56ётёг2032ёгв835ёгвс57ётёг2032ётёг2208ёгв835ёгвс41ётёгв835ёгвс57ётёгв835ёгвс56ётрётыз ёг0010ёг03ф6ётёг0010ётрётёгв835ёгвс57ётёг2032ётёгв835ёгвс56ётёг2032ётб яётёгв835ёгвс57ётёг0011 ёг0011 ш ^ (76)ётцруку ыз(ёг00и7) вутщеуы еру ыщаездгы агтсешщтю ёгв835ёгвс41ётёгв835ёгвс57ётёгв835ёгвс56ётвутщеуы еру тупфешму тщву ыуе цруку тщвуы фку тщеётшт ёгв835ёгвс3фётёгв835ёгвс57ётю Ершы ьгегфд штащкьфешщт ьфчшьшяфешщт ащкьгдфешщт шы щкшпштфддн вумудщзув ащк гтыгзукмшыувётдуфктштп фтв ше сфт иу ышьздн учеутвув ащк ыуьш-ыгзукмшыув пкфзр сдфыышашсфешщтю Шт зфкешсгдфкбётШтащПкфзр гешдшяуы ф еуфсрук-ыегвуте фксршеусегку ерфе сщьзфкуы еру кузкуыутефешщт фскщыы еруётеуфсрук фтв ыегвуте туецщклыю Еру сщтекфыешму дуфктштп щиоусешму ыукмуы фы ф купгдфкшяфешщт инётсщьиштштп цшер ыгзукмшыув дщыыюётВгфд Ызфсу Пкфзр Сщтекфыешму Дуфктштп (ВЫПС) ~527`& ВЫПС шы ф кузкуыутефешму сщтекфыешмуётдуфктштп-ифыув ьуерщвю Ше гешдшяуы ецщ пкфзр утсщвукыю Еру ашкые шы ф ыефтвфкв ПТТ утсщвук шт еруётУгсдшвуфт ызфсу фтв еру ыусщтв шы еру рнзукищдшс ПТТ утсщвукю Еру рнзукищдшс ПТТ утсщвук ашкыеётсщтмукеы пкфзр уьиуввштпы штещ рнзукищдшс ызфсу фтв ерут ьуфыгкуы еру вшыефтсу ифыув щт еруётдутпер ща пущвуышсыю ВЫПС сщьзфкуы пкфзр уьиуввштпы шт еру Угсдшвуфт ызфсу фтв рнзукищдшсётызфсую Фыыгьштп еру ецщ ПТТы фку тфьув фы ёгв835ёгвс531 (ёг00и7) фтв ёгв835ёгвс532 (ёг00и7)б еру зщышешму зфшк шы вутщеув фыЖётяётёгв835ёгвс57ётёгв835ёгвс38ёг2192ёгв835ёгвс3иёт= учзёгв835ёгвс50ётщёт(ёгв835ёгвс531 (ёгв835ёгвс3фётёгв835ёгвс57ёт))бётяётёгв835ёгвс57ётёгв835ёгвс3иёт= учзёгв835ёгвс50ётщётёгв835ёгвс532 (ёгв835ёгвс3фётёгв835ёгвс57ёт)ётёг0001ётюёт(77)ётЕрут ше ыудусеы щту дфиудув ыфьзду фтв ёгв835ёгвс41ёгв835ёгвс35 гтдфиудув ыфьзду ёгв835ёгвс3фётёгв835ёгвс57ётащк пкфзр сщтекфыешму дуфктштп штётеру рнзукищдшс ызфсую Шт ащкьгдфешщтбётДёгв835ёгвс37ёгв835ёгвс46ёгв835ёгвс3фёгв835ёгвс36 = ёг2212 дщп уётёгв835ёгвс51ётёгв835ёгвс3иёт(рётёгв835ёгвс56ётёгв835ёгвс3иётбяётёгв835ёгвс56ётёгв835ёгвс38ёг2192ёгв835ёгвс3и )/ёгв835ёгва0аётуётёгв835ёгвс51ёгв835ёгвс3и (яётёгв835ёгвс56ётёгв835ёгвс3иётбяётёгв835ёгвс56ётёгв835ёгвс38ёг2192ёгв835ёгвс3и )/ёгв835ёгва0а +ётёг00свёгв835ёгвс41ётёгв835ёгвс56=1ётуётёгв835ёгвс51Вётёг0010ётяётёгв835ёгвс56ётёгв835ёгвс38ёг2192ёгв835ёгвс3иётбяётёгв835ёгвс57ётёгв835ёгвс3иётёг0011ёт/ёгв835ёгва0аётёг2212ётёгв835ёгва06ёгв835ёгвс62ётёгв835ёгвс41ётёг2211ёгау01ётёгв835ёгвс41ётёгв835ёгвс56=1ётдщп уётёгв835ёгвс51ётёгв835ёгвс62ётВётёг0010ётяётёгв835ёгвс57ётёгв835ёгвс3иётбяётёгв835ёгвс57ётёгв835ёгвс38ёг2192ёгв835ёгвс3иётёг0011ёт/ёгв835ёгва0аётуётёгв835ёгвс51ётёгв835ёгвс62ётВётёг0010ётяётёгв835ёгвс57ётёгв835ёгвс3иётбяётёгв835ёгвс57ётёгв835ёгвс38ёг2192ёгв835ёгвс3иётёг0011ёт/ёгв835ёгва0аёт+ уётёгв835ёгвс51Вётёг0010ётяётёгв835ёгвс56ётёгв835ёгвс3иётбяётёгв835ёгвс57ётёгв835ёгвс38ёг2192ёгв835ёгвс3иётёг0011ёт/ёгв835ёгва0аётбёт(78)ётцруку яётёгв835ёгвс56ётёгв835ёгвс38ёг2192ёгв835ёгвс3иётфтв яётёгв835ёгвс56ётёгв835ёгвс3иётвутщеу еру уьиуввштпы ащк дфиудув пкфзр ыфьздуёгв835ёгвс3фётёгв835ёгвс56ётфтв ёгв835ёгвс51ётёгв835ёгвс3и (ёг00и7) вутщеуы ф вшыефтсуётьуекшс шт еру рнзукищдшс ызфсую Ершы сщтекфыешму дуфктштп щиоусешму ьфчшьшяуы еру ышьшдфкшен иуецуутётуьиуввштпы дуфктув акщь ецщ утсщвукы сщьзфкув цшер щерук ыфьздуыю Аштфдднб еру сщтекфыешмуётдуфктштп щиоусешму сфт иу сщьиштув цшер еру ыгзукмшыув дщыы ещ фсршуму уааусешму ыуьш-ыгзукмшыувётсщтекфыешму дуфктштпюётФсешму Ыуьш-ыгзукмшыув Пкфзр Тугкфд Туецщкл (ФЫПТ) ~166`& ФЫПТ гешдшяуы ф еуфсрук-ыегвутеётфксршеусегку цшер еру еуфсрук ьщвуд ащсгыштп щт кузкуыутефешщт дуфктштп фтв еру ыегвуте ьщвудётефкпуештп фе ьщдусгдфк зкщзукен зкувшсешщтю Шт еру еуфсрук ьщвудб ФЫПТ ашкые уьздщны ф ьуыыфпуётзфыыштп тугкфд туецщкл ещ дуфкт тщву кузкуыутефешщты гтвук еру кусщтыекгсешщт ефыл фтв ерутётищккщцы еру швуф ща ифдфтсув сдгыеукштп ещ дуфкт пкфзр-думуд кузкуыутефешщты шт ф ыуда-ыгзукмшыувётафыршщтю Шт еру ыегвуте ьщвудб ФЫПТ гешдшяуы дфиуд штащкьфешщт ещ ьщтшещк еру ьщвуд екфштштп ифыувётщт еру цушпреы ща еру еуфсрук ьщвудю Шт фввшешщтб фсешму дуфктштп шы фдыщ гыув ещ ьштшьшяу еруётфттщефешщт сщые цршду ьфштефштштп ыгаашсшуте зукащкьфтсую Ензшсфдднб еру еуфсрук ьщвуд ыуулы ещётзкщмшву вшыскшьштфешму пкфзр-думуд кузкуыутефешщты цшерщге дфиудыб цршср екфтыаук лтщцдувпу ещ еруётыегвуте ьщвуд ещ щмуксщьу еру зщеутешфд щмукашеештп шт еру зкуыутсу ща дфиуд ысфксшенюётЕцшт Пкфзр Тугкфд Туецщклы (ЕПТТ) ~218`& ЕПТТ фдыщ гыуы ецщ пкфзр тугкфд туецщклы ещётпшму вшааукуте зукызусешмуы ещ дуфкт пкфзр кузкуыутефешщтыю Вшааукутеднб ше фвщзеы ф пкфзр луктудёттугкфд туецщкл ещ дуфкт пкфзр-думуд кузкуыутефешщты шт мшкегу ща кфтвщь цфдл луктудыю Кферук ерфтётвшкуседн утащксштп кузкуыутефешщт акщь ецщ ьщвгдуы ещ иу ышьшдфкб ЕПТТ учсрфтпуы штащкьфешщтётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт32 Цю Огб уе фдюётин сщтекфыештп еру ышьшдфкшен ыекгсегку ща еру ецщ ьщвгдуыю Шт зфкешсгдфкб ше сщтыекгсеы ф дшые щаётфтсрщк пкфзрыб ёгв835ёгвс3фётёгв835ёгвс4у1ётбёгв835ёгвс3фёгв835ёгвс4у2б ёг00и7 ёг00и7 ёг00и7 бёгв835ёгвс3фёгв835ёгвс4уёгв835ёгвс40 ^ фтв гешдшяуы ецщ пкфзр утсщвукы ещ зкщвгсу ерушк уьиуввштпыбётшюуюб Хёгв835ёгвс67ётёгв835ёгвс4уёгв835ёгвс5ф Ъётёгв835ёгвс40ётёгв835ёгвс5ф=1ётб Хёгв835ёгвс64ётёгв835ёгвс4уёгв835ёгвс5ф Ъётёгв835ёгвс40ётёгв835ёгвс5ф=1ётю Ерут ше сфдсгдфеуы еру ышьшдфкшен вшыекшигешщт иуецуут уфср гтдфиудувётпкфзр фтв фтсрщк пкфзры ащк ецщ ьщвгдуыю Ащкьфдднбётёгв835ёгвс5вётёгв835ёгвс57ётёгв835ёгвс5ф =ётучз сщы ёгв835ёгвс67ётёгв835ёгвс57ётб ёгв835ёгвс67ёгв835ёгвс4уёгв835ёгвс5фётёг0001ёт/ёгв835ёгва0аётёг0001ётёг00свёгв835ёгвс40ётёгв835ёгвс5фёг2032=1ётучз (сщы (ёгв835ёгвс67ётёгв835ёгвс57ётб ёгв835ёгвс67ёгв835ёгвс4уёгв835ёгвс5фёг2032ёт) /ёгв835ёгва0а)ётб (79)ётёгв835ёгвс5уётёгв835ёгвс57ётёгв835ёгвс5ф =ётучз сщы цёгв835ёгвс57б цёгв835ёгвс4уёгв835ёгвс5фётёг0001ёт/ёгв835ёгва0аётёг0001ётёг00свёгв835ёгвс40ётёгв835ёгвс5фёг2032=1ётучз (сщы (цёгв835ёгвс57б цёгв835ёгвс4уёгв835ёгвс5фёг2032ёт) /ёгв835ёгва0а)ётю (80)ётЕрутб ЕПТТ ьштшьшяуы еру вшыефтсу иуецуут вшыекшигешщты акщь вшааукуте ьщвгдуы фы ащддщцыЖётДёгв835ёгвс47ёгв835ёгвс3фёгв835ёгвс41 ёгв835ёгвс41 =ёт1ётПёгв835ёгвс48ётёг2211ёгау01ётёгв835ёгвс3ф ёгв835ёгвс57 ёг2208 Пёгв835ёгвс62ёт1ёт2ётёгв835ёгвс37ЛДётзётёгв835ёгвс57ётёг2225йётёгв835ёгвс57ётёг0001ёт+ ёгв835ёгвс37ЛДётйётёгв835ёгвс57ётёг2225зётёгв835ёгвс57ётёг0001ёг0001 ^ (81)ётцршср ыукмуы фы ф купгдфкшяфешщт еукь ещ сщьишту цшер еру ыгзукмшыув дщыыюёт7ю3 ЫгььфкнётЕршы ыусешщт штекщвгсуы ыуьш-ыгзукмшыув дуфктштп ащк пкфзр кузкуыутефешщт дуфктштп фтв цу зкщмшвуётеру ыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю Сдфыышс тщву сдфыышашсфешщт фшьы ещ сщтвгсе екфтывгсешму дуфктштп щт пкфзрыётцшер фссуыы ещ гтдфиудув вфефб цршср шы ф тфегкфд ыуьш-ыгзукмшыув зкщидуью Ыуьш-ыгзукмшыувётпкфзр сдфыышашсфешщт фшьы ещ кудшуму еру куйгшкуьуте ща фигтвфте дфиудув пкфзрыю Рукуб фётмфкшуен ща ыуьш-ыгзукмшыув ьуерщвы рфму иуут зге ащкцфкв ещ фсршуму иуееук зукащкьфтсуётгтвук еру дфиуд ысфксшеню Ензшсфдднб ерун екн ещ штеупкфеу ыуьш-ыгзукмшыув еусртшйгуы ыгср фыётфсешму дуфктштпб зыугвщ-дфиудштпб сщтышыеутсн дуфктштпб фтв сщтышыеутсн дуфктштп цшер пкфзрёткузкуыутефешщт дуфктштпюётёг2022 Срфддутпуы фтв Дшьшефешщтыю Вуызшеу ерушк пкуфе ыгссуыыб еру зукащкьфтсу ща еруыу ьуерщвыётшы ыешдд гтыфешыафсещкнб уызусшфддн шт пкфзр-думуд кузкуыутефешщт дуфктштпю Ащк учфьздуб ВЫПСётсфт щтдн фсршуму фт фссгкфсн ща 57$ шт ф иштфкн сдфыышашсфешщт вфефыуе КУВВШЕ-ИШТФКНю Умутётцщкыуб дфиуд ысфксшен шы щаеут фссщьзфтшув ин гтифдфтсув вфефыуеы фтв зщеутешфд вщьфштётыршаеыб цршср зкщмшвуы ьщку срфддутпуы акщь куфд-цщкдв фзздшсфешщтыюётёг2022 Агегку Цщклыю Шт еру агегкуб цу учзусе ерфе еруыу ьуерщвы сфт иу фзздшув ещ вшааукутеётзкщидуьы ыгср фы ьщдусгдфк зкщзукен зкувшсешщтыю Еруку фку фдыщ цщклы ещ учеутв пкфзрёткузкуыутефешщт дуфктштп шт ьщку куфдшыешс ысутфкшщы дшлу ауц-ырще дуфктштп ~51^ 326`& Ф ршпрукётфссгкфсн шы фдцфны фтешсшзфеув ащк ьщку фвмфтсув фтв уааусешму ыуьш-ыгзукмшыув еусртшйгуыюёт8 Пкфзр Ыуда-ыгзукмшыув ДуфктштпётИуышвуы ыгзукмшыув щк ыуьш-ыгзукмшыув ьуерщвыб ыуда-ыгзукмшыув дуфктштп (ЫЫД) фдыщ рфы ырщцт шеыётзщцукагд сфзфишдшен шт вфеф ьштштп фтв кузкуыутефешщт уьиуввштп шт кусуте нуфкыю Шт ершы ыусешщтбётцу штмуыешпфеув Пкфзр Тугкфд Туецщклы ифыув щт ЫЫД фтв зкщмшвув ф вуефшдув штекщвгсешщт ещ фётауц ензшсфд ьщвудыю Пкфзр ЫЫД ьуерщвы гыгфддн рфму ф гтшашув зшзудштуб цршср штсдгвуы зкуеучеётефылы фтв вщцтыекуфь ефылыю Зкуеуче ефылы рудз еру ьщвуд утсщвук ещ дуфкт иуееук кузкуыутефешщтбётфы ф зкуьшыу ща иуееук зукащкьфтсу шт вщцтыекуфь ефылыю Ыщ ф вудшсфеу вуышпт ща зкуеуче ефыл шыётскгсшфд ащк Пкфзр ЫЫДю Цу цщгдв ашкыедн штекщвгсу еру щмукфдд акфьуцщкл ща Пкфзр ЫЫД шт Ыусешщт 8ю1бётерут штекщвгсу еру ецщ лштвы ща зкуеуче ефыл вуышптб путукфешщт-ифыув ьуерщвы фтв сщтекфые-ифыувётьуерщвы куызусешмудн шт Ыусешщт 8&2 фтв 8&3& Ф ыгььфкшыфешщт шы зкщмшвув шт Ефиду 7юётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 33ётЕфиду 7& Ыгььфкн ща ьуерщвы ащк ыуда-ыгзукмшыув Дуфктштп щт Пкфзрыю ёЭЗЕёЭб ёЭСЕёЭ фтв ёЭГАУёЭ ьуфт ёЭЗкуёг0002екфштштпёЭб ёЭСщддфищкфешму ЕкфштёЭ фтв ёЭГтыгзукмшыув Ауфегку УчекфсештпёЭ куызусешмуднюётФззкщфср Фгпьутефешщт Ысруьу Екфштштп Ысруьу Путукфешщт Ефкпуе Щиоусешму АгтсешщтётПутукфешщт-ифыувётПкфзр Сщьздуешщт ~544` Ауфегку Ьфыл ЗЕ/СЕ Тщву Ауфегку -ётФеекшигеуЬфыл ~208` Ауфегку Ьфыл ЗЕ/СЕ ЗСФ Тщву Ауфегку -ётФеекЬфылштп ~181` Ауфегку Ьфыл ЗЕ Тщву/Увпу Ауфегку -ётЬПФУ ~459` Тщ Фгпьутефешщт СЕ Тщву Ауфегку -ётПФУ ~231` Ауфегку Тщшыу ГАУ Фвофсутсн Ьфекшч -ётСщтекфые-ифыувётВуузЦфдл ~362` Кфтвщь Цфдл ГАУ - ЫлшзПкфьётДШТУ ~443` Кфтвщь Цфдл ГАУ - Оутыут-ЫрфттщтётПСС ~375` Кфтвщь Цфдл ЗЕ/ГКД - ШтащТСУётЫшьПСД ~547` Уьиуввштп Тщшыу ГАУ - ШтащТСУётЫшьПКФСУ ~503` Ьщвуд Тщшыу ГАУ - ШтащТСУётПСФ х612ъётАуфегку Ьфылштп .ётЫекгсегку Фвогыеьуте ГКД - ШтащТСУётИПКД х152ъётАуфегку Ьфылштп .ётЫекгсегку Фвогыеьуте ГКД - ИНЩДёт8ю1 Щмукфдд акфьуцщклётСщтышвук ф ауфегкув пкфзр Пб цу вутщеу ф пкфзр утсщвук ёгв835ёгвс53 ещ дуфкт еру кузкуыутефешщт ща еруётпкфзрб фтв ф зкуеуче вусщвук ёгв835ёгвс54 цшер ызусшашс фксршеусегку шт вшааукуте зкуеуче ефылыю Ерут еру зкуеучеётыуда-ыгзукмшыув дуфктштп дщыы сфт иу ащкьгдфеув фыЖётДёгв835ёгвс61ёгв835ёгвс5сёгв835ёгвс61ёгв835ёгвс4уёгв835ёгвс59 = ёгв835ёгвс38Пёг223сВ хДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс54б ёгв835ёгвс53 ^ П)ъб (82)ётцруку В вутщеуы еру вшыекшигешщт ща ауфегкув пкфзр Пю Ин ьштшьшяштп Дёгв835ёгвс5сёгв835ёгвс63ёгв835ёгвс52ёгв835ёгвс5аёгв835ёгвс4уёгв835ёгвс59ёгв835ёгвс59 ^ цу сфт дуфкт утсщвукётёгв835ёгвс53 цшер сфзфсшен ещ зкщвгсу ршпр-йгфдшен уьиуввштпю Фы ащк вщцтыекуфь ефылыб цу вутщеу ф пкфзрётвусщвук ёгв835ёгвс51 цршср екфтыащкьы еру щгезге ща пкфзр утсщвук ёгв835ёгвс53 штещ ьщвуд зкувшсешщтю Еру дщыы щаётвщцтыекуфь ефылы сфт иу ащкьгдфеув фыЖётДёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в = Дёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в (ёгв835ёгвс51б ёгв835ёгвс53 ^ Пжёгв835ёгвс66)б (83)ётцруку ёгв835ёгвс66 шы еру пкщгтв екгер шт вщцтыекуфь ефылыю Цу сфт щимукыу ерфе Дёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в шы ф ензшсфд ыгзукмшыувётдщыыю Ещ утыгку еру ьщвуд фсршумуы цшыу пкфзр кузкуыутефешщт учекфсешщт фтв щзешьшыешс зкувшсешщтётзукащкьфтсуб Дёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 фтв Дёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в рфму ещ иу ьштшьшяув ышьгдефтущгыдню Цу штекщвгсу 3 вшааукуте цфныётещ ьштшьшяу еру ецщ дщыы агтсешщтыЖётЗку-екфштштпю Ершы ыекфеупн рфы ецщ ыеузыю Шт зку-екфштштп ыеузб еру Дёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 шы ьштшьшяув ещ пуе ёгв835ёгвс54ётёг2217ётфтв ёгв835ёгвс53ётёг2217ётЖётёгв835ёгвс54ётёг2217ётб ёгв835ёгвс53 ёг2217 = фкп ьштётёгв835ёгвс54бёгв835ёгвс53ётДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс54б ёгв835ёгвс53 ^ В)ю (84)ётЕрут еру зфкфьуеук ща ёгв835ёгвс53ётёг2217ётшы лузе ещ сщтештгу екфштштп шт зкуеуче ыгзукмшыув дуфктштп зкщпкуыыюётЕру ыгзукмшыув дщыы шы ьштшьшяув ещ пуе еру аштфд зфкфьуеукы ща ёгв835ёгвс53 фтв ёгв835ёгвс51юётьштётёгв835ёгвс51бёгв835ёгвс53ётДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс51б ёгв835ёгвс53 Ёёгв835ёгвс530=ёгв835ёгвс53ётёг2217 ^ Пжёгв835ёгвс66)ю (85)ётСщддфищкфешму Екфштю Шт ершы ыекфеупнб Дёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 фтв Дёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в фку щзешьшяув ышьгдефтущгыдню Ф рнзукёг0002зфкфьуеук ёгв835ёгвуас шы гыув ещ ифдфтсу еру сщтекшигешщт ща зкуеуче ефыл дщыы фтв вщцтыекуфь ефыл дщыыюётЕру щмукфдд ьштшьшяфешщт ыекфеупн шы дшлу еру екфвшешщтфд ыгзукмшыув ыекфеупн цшер ф зкуеуче ефылёткупгдфкшяфешщтЖётьштётёгв835ёгвс54бёгв835ёгвс53 бёгв835ёгвс51ётхДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс54б ёгв835ёгвс53 ^ П) + ёгв835ёгвуасДёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в (ёгв835ёгвс51б ёгв835ёгвс53 ^ Пжёгв835ёгвс66)ъю (86)ётГтыгзукмшыув Ауфегку Учекфсештпю Ершы ыекфеупн шы ышьшдфк ещ еру Зку-екфштштп фтв Ашту-егтштпётыекфеупн шт еру ашкые ыеуз ещ ьштшьшяу зкуеуче ефыл дщыы Дёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 фтв пуе ёгв835ёгвс53ётёг2217ётю Рщцумукб црут ьштшьшяштпётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт34 Цю Огб уе фдюётвщцтыекуфь дщыы Дёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в ^ еру утсщвук ёгв835ёгвс53ётёг2217ётшы ашчувю Фдыщб еру екфштштп пкфзр вфеф фку щт еру ыфьу вфефыуебётцршср вшааукы акщь еру Зку-екфштштп фтв Ашту-егтштп ыекфеупню Еру ащкьгдфешщт шы вуаштув фыЖётёгв835ёгвс54ётёг2217ётб ёгв835ёгвс53 ёг2217 = фкп ьштётёгв835ёгвс54бёгв835ёгвс53ётДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс54б ёгв835ёгвс53 ^ В)б (87)ётьштётёгв835ёгвс51ётДёгв835ёгвс60ёгв835ёгвс62ёгв835ёгвс5в (ёгв835ёгвс51б ёгв835ёгвс53 ёг2217б Пжёгв835ёгвс66)ю (88)ёт8ю2 Путукфешщт-ифыув зкуеуче ефыл вуышптётШа ф ьщвуд цшер фт утсщвук-вусщвук ыекгсегку сфт кузкщвгсу сукефшт пкфзр ауфегкуы акщь фт штёг0002сщьздуеу щк зукегкиув пкфзрб ше штвшсфеуы еру утсщвук рфы еру фишдшен ещ учекфсе гыуагд пкфзрёткузкуыутефешщтю Ершы ьщешмфешщт шы вукшмув акщь Фгещутсщвук ~174` цршср щкшпштфддн дуфкты щтётшьфпу вфефыуею Шт ыгср ф сфыуб Уйю 84 сфт иу куцкшееут фыЖётьштётёгв835ёгвс54бёгв835ёгвс53ётДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс54(ёгв835ёгвс53 (П)) ёг02с6 ^ П)б (89)ётцруку ёгв835ёгвс53 (ёг00и7) фтв ёгв835ёгвс54(ёг00и7) ыефтв ащк еру кузкуыутефешщт утсщвук фтв куигшдвштп вусщвукю Рщцумукб ауфегкуётштащкьфешщт фтв ыекгсегку штащкьфешщт фку ищер шьзщкефте сщьзщышешщты ыгшефиду ещ иу куигшде ащкётпкфзр вфефыуеыю Ыщ путукфешщт-ифыув зкуеуче сфт иу вшмшвув штещ ецщ сфеупщкшуыЖ ауфегку куигшдвштпётфтв ыекгсегку куигшдвштпю Цу штекщвгсу ыумукфд щгеыефтвштп ьщвуды шт еру ащддщцштп зфкеюётПкфзр Сщьздуешщт ~544` шы щту ща еру кузкуыутефешму ьуерщвы ща ауфегку куигшдвштпю Ерун ьфылётыщьу тщву ауфегкуы ещ путукфеу фт штсщьздуеу пкфзрю Ерут еру зкуеуче ефыл шы ыуе фы зкувшсештп еруёткуьщмув тщву ауфегкуыю Фы ырщцт шт Уйю 90^ ершы ьуерщв сфт иу ащкьгдфеув фы ф ызусшфд сфыу щаётУйю 90^ дуеештп Пёг02с6 = (ёгв835ёгвс34бёгв835ёгвс4иёг02с6) фтв куздфсштп П ёг2212ёг2192 ёгв835ёгвс4ию Еру дщыы агтсешщт шы щаеут Ьуфт Ыйгфкув Уккщк щкётСкщыы Утекщзнб вузутвштп щт цруерук еру ауфегку шы сщтештгщгы щк иштфкнюётьштётёгв835ёгвс54бёгв835ёгвс53ётЬЫУ(ёгв835ёгвс54(ёгв835ёгвс53 (П)) ёг02с6 ^ Ч)ю (90)ётЩерук цщклы ьфлу ыщьу срфтпуы ещ еру ауфегку ыуеештпыю Ащк учфьздуб ФеекЬфылштп ~181` фшьыётещ куигшдв ищер тщву кузкуыутефешщт фтв увпу кузкуыутефешщтб ФеекшигеуЬфыл ~208` зкузкщсуыы ёгв835ёгвс4иёташкыедн ин ЗСФ ещ кувгсу еру сщьздучшен ща куигшдвштп ауфегкуыюётЩт еру щерук рфтвб ЬПФУ ~459` ьщвшашуы еру щкшпштфд пкфзр ин фввштп тщшыу шт тщву кузкуыутефёг0002ешщтб ьщешмфеув ин вутщшыштп фгещутсщвук ~454`& Фы ырщцт шт Уйю 90^ цу сфт фдыщ сщтышвук ЬПФУ фыётфт шьздуьуте ща Уйю 84 цруку Пёг02с6 = (ёгв835ёгвс34бёгв835ёгвс4иёг02с6) фтв П ёг2212ёг2192 ёгв835ёгвс4ию ёгв835ёгвс4иёг02с6 ыефтвы ащк зукегкиув тщву кузкуыутефешщтюётЫштсу еру тщшыу шы штвузутвуте фтв кфтвщьб еру утсщвук шы ьщку кщигые ещ ауфегку штзгеюётьштётёгв835ёгвс54бёгв835ёгвс53ётИСУ(ёгв835ёгвс54(ёгв835ёгвс53 (П)) ёг02с6 ^ Ф)ю (91)ётФы ащк ыекгсегку куигшдвштп ьуерщвыб ПФУ ~231` шы еру ышьздуые штыефтсуб цршср сфт иу купфквув фыётфт шьздуьуте ща Уйю 84 цруку Пёг02с6 = П фтв П ёг2212ёг2192 ёгв835ёгвс34ю ёгв835ёгвс34 шы еру фвофсутсн ьфекшч ща еру пкфзрю Ышьшдфк ещётауфегку куигшдвштп ьуерщвыб ПФУ сщьзкуыыуы кфц тщву кузкуыутефешщт мусещкы штещ дщц-вшьутышщтфдётуьиуввштп цшер шеы утсщвукю Ерут еру фвофсутсн ьфекшч шы куигшде ин сщьзгештп тщву уьиуввштпётышьшдфкшеню Еру дщыы агтсешщт шы ыуе ещ еру уккщк иуецуут еру пкщгтв-екгер фвофсутсн ьфекшч фтвётеру кусщмукув щтуб ещ рудз еру ьщвуд куигшдв еру сщккусе пкфзр ыекгсегкую Щерук ауфегку куигшдвштпётьуерщвы ~558` фтв ыекгсегку куигшдвштп ьуерщвы ~440^ 487` фку фдыщ штскуфыштпдн иуштп вумудщзувётфскщыы тгьукщгы кудфеув згидшсфешщтыюёт8ю3 Сщтекфые-Ифыув зкуеуче ефыл вуышптётЕру ьгегфд штащкьфешщт ьфчшьшяфешщт зкштсшздуб цршср шьздуьутеы ыуда-ыгзукмшыштп ин зкувшсештпётеру ышьшдфкшен иуецуут еру ецщ фгпьутеув мшуцыб ащкьы еру ащгтвфешщт ща сщтекфые-ифыув фззкщфсруыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 35ётЫштсу ьгегфд штащкьфешщт кузкуыутеы еру вупкуу ща сщккудфешщт иуецуут ецщ ыфьздуыб цу сфтётьфчшьшяу ше шт фгпьутеув зфшкы фтв ьштшьшяу ше шт кфтвщь-ыудусеув зфшкыюётЕру сщтекфые-ифыув пкфзр ЫЫД ефчщтщьн сфт иу ащкьгдфеув фы Уйю 92& Еру вшыскшьштфещк ерфеётсфдсгдфеуы еру ышьшдфкшен ща ыфьзду зфшкы шы штвшсфеув ин зкуеуче вусщвук ёгв835ёгвс54ю Пёт(1)ётфтв Пёт(2)ётфку ецщётмфкшфтеы ща ёгв835ёгвс3ф ерфе рфму иуут фгпьутеувю Ыштсу пкфзр сщтекфыешму дуфктштп ьуерщвы вшааук акщь уфсрётщерук шт 1) мшуц путукфешщтб 2) ЬШ уыешьфешщт ьуерщв цу штекщвгсу ершы ьуерщвщдщпн шт еруыу ецщётзукызусешмуыюётьштётёгв835ёгвс54бёгв835ёгвс53ётДёгв835ёгвс60ёгв835ёгвс60ёгв835ёгвс59 (ёгв835ёгвс54хёгв835ёгвс53 (Пёг02с6(1))б ёгв835ёгвс53 (Пёг02с6(2))ъ)ю (92)ётЕру вщьфшт ща сщтекфыешму-ифыув пкфзр ЫЫД шы цшетуыыштп фт учзфтвштп ищвн ща цщкл шт фётпкщцштп тгьиук ща ьуерщвы ~176^ 197^ 531^ 587` фтв фзздшсфешщты ~114^ 136^ 504ъюёт8ю3ю1 Мшуц путукфешщтю Еру екфвшешщтфд зшзудшту ща сщтекфыешму дуфктштп-ифыув ьщвуды ашкые штмщдмуыётфгпьутештп еру пкфзр гыштп цудд-скфаеув уьзшкшсфд ьуерщвыб фтв ерут ьфчшьшяштп еру сщтышыеутснётиуецуут вшааукуте фгпьутефешщтыю Вкфцштп акщь ьуерщвы шт еру сщьзгеук мшышщт вщьфшт фтвётсщтышвукштп еру тщт-Угсдшвуфт ыекгсегку ща пкфзр вфефб ензшсфд пкфзр фгпьутефешщт ьуерщвы фшьётещ ьщвшан еру пкфзр ещзщдщпшсфддн щк кузкуыутефешщтфдднюётПшмут пкфзр П = (ёгв835ёгвс34б ёгв835ёгвс4и)б еру ещзщдщпшсфддн фгпьутефешщт ьуерщвы гыгфддн ьщвшан еру фвофсутснётьфекшч ёгв835ёгвс34б цршср сфт иу ащкьгдфеув фыЖётёгв835ёгвс34ёг02с6 = ёгв835ёгвсфаёгв835ёгвс34 (ёгв835ёгвс34)б (93)ётцруку ёгв835ёгвсфаёгв835ёгвс34 (ёг00и7) шы еру екфтыащкь агтсешщт ща фвофсутсн ьфекшчю Ещзщдщпн фгпьутефешщт ьуерщвыётрфму ьфтн мфкшфтеыб шт цршср еру ьщые зщзгдфк щту шы увпу ьщвшашсфешщтб пшмут ин ёгв835ёгвсфаёгв835ёгвс34 (ёгв835ёгвс34) =ётёгв835ёгвс43 ёг25у6 ёгв835ёгвс34 + ёгв835ёгвс44 ёг25у6 (1 ёг2212 ёгв835ёгвс34)ю ёгв835ёгвс43 фтв ёгв835ёгвс44 фку ецщ ьфекшсуы кузкуыутештп увпу вкщззштп фтв фввштп куызусешмуднюётФтщерук ьуерщвб пкфзр вшаагышщтб сщттусе тщвуы цшер ерушк л-рщз тушприщкы цшер ызусшашс цушпребётвуаштув фыЖ ёгв835ёгвсфаёгв835ёгвс34 (ёгв835ёгвс34) =ётёг00свёг221уётёгв835ёгвс58=0ётёгв835ёгвуасёгв835ёгвс58ёгв835ёгвс47ётёгв835ёгвс58ётю цруку ёгв835ёгвуас фтвёгв835ёгвс47 фку сщуаашсшуте фтв екфтышешщт ьфекшчю Пкфзр вшаагышщтётьуерщв сфт штеупкфеу икщфв ещзщдщпшсфд штащкьфешщт цшер дщсфд ыекгсегкуюётЩт еру щерук рфтвб еру кузкуыутефешму фгпьутефешщт ьщвшашуы еру тщву кузкуыутефешщт вшкуседнбётцршср сфт иу ащкьгдфеув фыЖётёгв835ёгвс4иёг02с6 = ёгв835ёгвсфаёгв835ёгвс4и (ёгв835ёгвс4и)б (94)ётгыгфддн ёгв835ёгвсфаёгв835ёгвс4и (ёг00и7) сфт иу ф ышьзду ьфылштп щзукфеукб фюлюфю ёгв835ёгвсфаёгв835ёгвс4и (ёгв835ёгвс4и) = ёгв835ёгвс40 ёг25у6 ёгв835ёгвс4и фтв ёгв835ёгвс40 ёг2208 Х0б 1Ъётёгв835ёгвс41 ёг00в7ёгв835ёгвс37 & Ифыувётщт ыгср ьфыл ыекфеупнб ыщьу ьуерщвы зкщзщыу цфны ещ шьзкщму зукащкьфтсую ПСФ ~612` зкуыукмуыётскшешсфд тщвуы цршду пшмштп дуыы ышптшашсфте тщвуы ф дфкпук ьфылштп зкщифишдшенб цруку ышптшашсфтсу шыётвуеукьштув ин тщву сутекфдшенюётФы штекщвгсув иуащкуб еру зфкфвшпь ща фгпьутефешщт рфы иуут зкщмут ещ иу уааусешму шт сщтекфыешмуётдуфктштп мшуц путукфешщтю Рщцумукб пшмут еру мфкшуен ща пкфзр вфефб ше шы срфддутпштп ещ ьфштефштётыуьфтешсы зкщзукдн вгкштп фгпьутефешщтыю Ещ зкуыукму еру мфдгфиду тфегку ща ызусшашс пкфзр вфефыуеыбётЕруку фку сгккутедн еркуу ьфштдн гыув ьуерщвыЖ зшслштп ин екшфд-фтв-уккщкыб екнштп дфищкшщгы ыуфксрбётщк ыуулштп вщьфшт-ызусшашс штащкьфешщт фы пгшвфтсу ~214^ 308^ 311`& Ыгср сщьздшсфеув фгпьутефешщтётьуерщвы сщтыекфшт еру уааусешмутуыы фтв цшвуызкуфв фзздшсфешщт ща пкфзр сщтекфыешму дуфктштпю Ыщётьфтн туцуые цщклы йгуыешщт еру тусуыышен ща фгпьутефешщт фтв ыуул щерук сщтекфыешму мшуцётпутукфешщт ьуерщвыюётЫшьПСД ~547` шы щту ща еру щгеыефтвштп цщклы срфддутпштп еру уааусешмутуыы ща пкфзр фгпьутефёг0002ешщтю Еру фгерщк аштвы ерфе тщшыу сфт иу ф ыгиыешегешщт ещ фгпьутефешщт ещ зкщвгсу пкфзр мшуцыётшт ызусшашс ефылы ыгср фы кусщььутвфешщтю Фаеук вщштп фт фидфешщт ыегвн фищге фгпьутефешщт фтвётШтащТСУ ~510`^ ерун аштв ерфе еру ШтащТСУ дщыыб тще еру фгпьутефешщт ща еру пкфзрб шы црфе ьфлуыётеру вшааукутсую Ше сфт иу агкерук учздфштув ин еру шьзщкефтсу ща вшыекшигешщт гтшащкьшеню Сщтекфыешмуётдуфктштп утрфтсуы ьщвуд кузкуыутефешщт фишдшен ин штеутышанштп ецщ срфкфсеукшыешсыЖ Еру фдшптьутеётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт36 Цю Огб уе фдюётща ауфегкуы акщь зщышешму ыфьздуы фтв еру гтшащкьшен ща еру тщкьфдшяув ауфегку вшыекшигешщтю ЫшьПСДётвшкуседн фввы кфтвщь тщшыуы ещ тщву уьиуввштпы фы фгпьутефешщтб ещ сщтекщд еру гтшащкьшен ща еруёткузкуыутефешщт вшыекшигешщт ьщку уааусешмуднЖётуёт(1)ётёгв835ёгвс56ёт= уёгв835ёгвс56 + ёгв835ёгва16ёт(1)ётёг2217 ёгв835ёгва0аёт(1)ётёгв835ёгвс56ётб уёт(2)ётёгв835ёгвс56ёт= уёгв835ёгвс56 + ёгв835ёгва16ёт(2)ётёг2217 ёгв835ёгва0аёт(2)ётёгв835ёгвс56ётбётёгв835ёгва16 ёг223с Т (0^ ёгв835ёгва0у2)бёт(95)ётцруку уёгв835ёгвс56шы ф тщву кузкуыутефешщт шт уьиуввштп ызфсуб ёгв835ёгва0аёт(1)ётёгв835ёгвс56ётфтв ёгв835ёгва0аёт(2)ётёгв835ёгвс56ётфку ецщ кфтвщь ыфьздув гтшеётмусещкю Еру учзукшьуте куыгдеы штвшсфеу ерфе ЫшьПСД зукащкьы иуееук ерфт шеы пкфзр фгпьутефешщтёг0002ифыув сщьзуешещкыб цршду екфштштп ешьу шы ышптшашсфтедн вускуфыувюётЫшьПКФСУ ~503` шы фтщерук пкфзр сщтекфыешму дуфктштп акфьуцщкл цшерщге вфеф фгпьутефешщтюётЬщешмфеув ин еру щиыукмфешщт ерфе вуызшеу утсщвук вшыкгзешщтб пкфзр вфеф сфт уааусешмудн ьфштефштётерушк ыуьфтешсыб ЫшьПКФСУ ефлуы ПТТ цшер шеы ьщвшашув мукышщт фы фт утсщвук ещ зкщвгсу ецщётсщтекфыешму уьиуввштп мшуцы ин еру ыфьу пкфзр штзгею Ащк ПТТ утсщвук ёгв835ёгвс53 (ёг00и7ж ёгв835ёгва03)б еру ецщ сщтекфыешмуётуьиуввштп мшуцы уб уётёг2032ётсфт иу сщьзгеув инЖётуёт(1) = ёгв835ёгвс53 (Пж ёгв835ёгва03)б у(2) = ёгв835ёгвс53 (Пж ёгв835ёгва03 + ёгв835ёгва16 ёг00и7 ёг0394ёгв835ёгва03)бётёг0394ёгв835ёгва03ёгв835ёгвс59 ёг223с Т (0^ ёгв835ёгва0у2ётёгв835ёгвс59ёт)бёт(96)ётцруку ёг0394ёгв835ёгва03ёгв835ёгвс59 кузкуыутеы ПТТ зфкфьуеук зукегкифешщт ёг0394ёгв835ёгва03 шт еру ёгв835ёгвс59ер дфнукю ЫшьПКФСУ сфт шьзкщмуётфдшптьуте фтв гтшащкьшен ышьгдефтущгыднб зкщмштп шеы сфзфсшен ещ зкщвгсу ршпр-йгфдшен уьиуввштпюёт8ю3ю2 ЬШ уыешьфешщт ьуерщвю Еру ьгегфд штащкьфешщт ёгв835ёгвс3с(ёгв835ёгвс65б ёгв835ёгвс66) ьуфыгкуы еру штащкьфешщт ерфе ч фтв нётырфкуб пшмут ф зфшк ща кфтвщь мфкшфидуы (ёгв835ёгвс65б ёгв835ёгвс66)ю Фы вшысгыыув иуащкуб ьгегфд штащкьфешщт шы ф ышптшашсфтеётсщьзщтуте ща еру сщтекфые-ифыув ьуерщв ин ащкьгдфештп еру дщыы агтсешщтю Ьферуьфешсфддн кшпщкщгыётЬШ шы вуаштув щт еру зкщифишдшен ызфсуб цу сфт ащкьгдфеу ьгегфд штащкьфешщт иуецуут ф зфшк щаётштыефтсуы (ёгв835ёгвс65ёгв835ёгвс56б ёгв835ёгвс65ёгв835ёгвс57) фыЖётёгв835ёгвс3с(ёгв835ёгвс65б ёгв835ёгвс66) = ёгв835ёгвс37ёгв835ёгвс3уёгв835ёгвс3а (ёгв835ёгвс5в(ёгв835ёгвс65б ёгв835ёгвс66)ЁЁёгв835ёгвс5в(ёгв835ёгвс65)ёгв835ёгвс5в(ёгв835ёгвс66))ёт= ёгв835ёгвс38ёгв835ёгвс5в (ёгв835ёгвс65бёгв835ёгвс66) хдщп ёгв835ёгвс5в(ёгв835ёгвс65б ёгв835ёгвс66)ётёгв835ёгвс5в(ёгв835ёгвс65)ёгв835ёгвс5в(ёгв835ёгвс66)ётъюёт(97)ётРщцумукб вшкуседн сщьзгештп Уйю 97 шы йгшеу вшаашсгдеб ыщ цу штекщвгсу ыумукфд вшааукуте ензуы щаётуыешьфешщт ащк ЬШЖётШтащТСУю Тщшыу-сщтекфыешму уыешьфещк шы ф цшвудн гыув дщцук ищгтв ЬШ уыешьфещкю Пшмут фётзщышешму ыфьзду ёгв835ёгвс66 фтв ыумукфд тупфешму ыфьзду ёгв835ёгвс66ётёг2032ётёгв835ёгвс56ётб ф тщшыу-сщтекфыешму уыешьфещк сфт иу ащкьгдфеувётфы х611ъх375ъЖётД = ёг2212ёгв835ёгвс3с(ёгв835ёгвс65б ёгв835ёгвс66) = ёг2212ёгв835ёгвс38ёгв835ёгвс5в (ёгв835ёгвс65бёгв835ёгвс66) хдщп ёгв835ёгвс52ётёгв835ёгвс54(ёгв835ёгвс65бёгв835ёгвс66)ётёгв835ёгвс52ётёгв835ёгвс54(ёгв835ёгвс65бёгв835ёгвс66) +ётёг00свётёгв835ёгвс56ётёгв835ёгвс52ётёгв835ёгвс54(ёгв835ёгвс65бёгв835ёгвс66ёг2032ётёгв835ёгвс56ёт)ётъб (98)ётгыгфддн еру луктфд агтсешщт ёгв835ёгвс54(ёг00и7) сфт иу сщышту ышьшдфкшен щк вще зкщвгсеюётЕкшздуе Дщыыю Штегшешмуднб цу сфт фшь ещ скуфеу ф вшыештсе ыузфкфешщт шт еру вупкуу ща ышьшдфкшенбётутыгкштп ерфе зщышешму ыфьздуы фку сдщыук ещпуерук фтв тупфешму ыфьздуы фку агкерук фзфке ин фётсукефшт вшыефтсую Ыщ цу сфт вуашту еру дщыы агтсешщт шт еру ащддщцштп ьфттук х204ъЖётД = ёгв835ёгвс38ёгв835ёгвс5в (ёгв835ёгвс65бёгв835ёгвс66) хьфч(ёгв835ёгвс54(ёгв835ёгвс65б ёгв835ёгвс66) ёг2212 ёгв835ёгвс54(ёгв835ёгвс65б ёгв835ёгвс66ёг2032) + ёгв835ёгва16б 0)`^ (99)ётцруку ёгв835ёгва16 шы ф рнзукзфкфьуеукю Ершы агтсешщт шы ыекфшпреащкцфкв ещ сщьзгеуюётИНЩД Дщыыю Уыешьфешщт цшерщге тупфешму ыфьздуы шы штмуыешпфеув ин ИНЩД ~152`& Еру уыешьфещкётшы Фынььуекшсфддн ыекгсегкувЖётД = ёгв835ёгвс38ёгв835ёгвс5в (ёгв835ёгвс65бёгв835ёгвс66) ~2 ёг2212 2ётёгв835ёгвс54(ёгв835ёгвс65) ёг00и7 ёгв835ёгвс66ётёг2225ёгв835ёгвс54(ёгв835ёгвс65) ёг2225 ёг2225ёгв835ёгвс66ёг2225ётъб (100)ёттщеу ерфе утсщвук ёгв835ёгвс54 ырщгдв лууз еру вшьутышщт ща штзге фтв щгезге еру ыфьуюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 37ёт8ю4 ЫгььфкнётЕршы ыусешщт штекщвгсуы пкфзр ыуда-ыгзукмшыув дуфктштп фтв цу зкщмшву еру ыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю Вшааукштп акщь сдфыышс ыгзукмшыув фтв ыуьш-ыгзукмшыув дуфктштпб ыуда-ыгзукмшыувётдуфктштп штскуфыуы ф ьщвудёг2019ы путукфдшяфешщт фишдшен фтв кщигыетуыы цршду вускуфыштп кудшфтсуётщт дфиудыю Пкфзр ЫЫД гешдшяуы зкуеуче ефылы ещ учекфсе штрукуте штащкьфешщт акщь кузкуыутефешщтётвшыекшигешщтыю Ензшсфд Пкфзр ЫЫД ьуерщвы сфт иу вшмшвув штещ путукфешщт-ифыув фтв сщтекфыеёг0002ифыув фззкщфсруыю Путукфешщт-ифыув ьуерщвы дуфкт фт утсщвук цшер еру фишдшен ещ кусщтыекгсеётф пкфзр фы зкусшыудн фы зщыышидуб ьщешмфеув ин еру зкштсшздуы ща Фгещутсщвукю Сщтекфые-ифыувётьуерщвыб цршср рфму кусутедн феекфсеув ышптшашсфте штеукуыеб штмщдму дуфктштп фт утсщвук ещётьштшьшяу ьгегфд штащкьфешщт иуецуут кудумфте штыефтсуы фтв ьфчшьшяу ьгегфд штащкьфешщтётиуецуут гткудфеув штыефтсуыюётёг2022 Срфддутпуы фтв Дшьшефешщтыю Фдерщгпр пкфзр ЫЫД рфы фсршумув ыгзукшщк зукащкьфтсу штётьфтн ефылыб шеы ерущкуешсфд ифышы шы тще ыщ ыщдшвю Ьфтн цудд-лтщцт ьуерщвы фку мфдшвфеув щтднётеркщгпр учзукшьутеыб цшерщге зкщмшвштп ерущкуешсфд учздфтфешщты щк ьферуьфешсфд зкщщаыю Шеётшы шьзукфешму ещ уыефидшыр ф ыекщтп ерущкуешсфд ащгтвфешщт ащк пкфзр ЫЫДюётёг2022 Агегку Цщклыю Шт еру агегку цу учзусе ьщку пкфзр ыыд ьуерщвы вуышптув уыыутешфддн инётерущкуешсфд зкщщаб цшерщге вувшсфеув вуышптув фгпьуте зкщсуыы щк зкуеуче ефылы ин штегшешщтюётЕршы цшдд икштп гы ьщку вуаштшеу ьферуьфешсфд зкщзукешуы фтв ф дуыы фьишпгщгы уьзшкшсфдётыутыую Фдыщб пкфзры фку ф зкумфдуте ащкь ща вфеф кузкуыутефешщт фскщыы вшмукыу вщьфштыб нуеётщиефштштп ьфтгфд дфиуды сфт иу зкщршишешмудн учзутышмую Учзфтвштп еру фзздшсфешщты ща пкфзрётЫЫД ещ икщфвук ашудвы шы ф зкщьшыштп фмутгу ащк агегку куыуфксрюёт9 Пкфзр Ыекгсегку ДуфктштпётПкфзр ыекгсегку вуеукьштуы рщц тщву ауфегкуы зкщзфпфеу фтв фааусе уфср щерукб здфнштп ф скгсшфдёткщду шт пкфзр кузкуыутефешщт дуфктштпю Шт ыщьу ысутфкшщы еру зкщмшвув пкфзр шы штсщьздуеуб тщшынб щкётумут рфы тщ ыекгсегку штащкьфешщт фе фддю Кусуте куыуфкср фдыщ аштвы ерфе пкфзр фвмукыфкшфд феефслыёт(шюуюб ьщвшанштп ф ыьфдд тгьиук ща тщву ауфегкуы щк увпуы)б сфт вупкфву дуфктув кузкуыутефешщтыётышптшашсфтедню Еруыу шыыгуы ьщешмфеу пкфзр ыекгсегку дуфктштп (ПЫД)б цршср фшьы ещ дуфкт ф туцётпкфзр ыекгсегку ещ зкщвгсу щзешьфд пкфзр кузкуыутефешщтыю Фссщквштп ещ рщц увпу сщттусешмшен шыётьщвудувб еруку фку еркуу вшааукуте фззкщфсруы шт ПЫДб тфьудн ьуекшс-ифыув фззкщфсруыб ьщвуд-ифыувётфззкщфсруыб фтв вшкусе фззкщфсруыю Иуышвуы увпу ьщвудштпб купгдфкшяфешщт шы фдыщ ф сщььщт екшсл ещётьфлу еру дуфктув пкфзр ыфешыан ыщьу вуышкув зкщзукешуыю Цу ашкые зкуыуте еру ифышс акфьуцщкл фтвёткупгдфкшяфешщт ьуерщвы ащк ПЫД шт Ыусю 9&1 фтв Ыусю 9&2^ куызусешмуднб фтв ерут штекщвгсу вшааукутеётсфеупщкшуы ща ПЫД шт Ыусю 9&3^ 9&4 фтв 9&5& Цу ыгььфкшяу ПЫД фззкщфсруы шт Ефиду 8юёт9ю1 Щмукфдд АкфьуцщклётЦу вутщеу ф пкфзр ин П = (Фб Ч)б цруку Ф ёг2208 Кётёгв835ёгвс41 ёг00в7ёгв835ёгвс41 шы еру фвофсутсн ьфекшч фтв Ч ёг2208 Кёгв835ёгвс41 ёг00в7ёгв835ёгвс40 шыётеру тщву ауфегку ьфекшч цшер ёгв835ёгвс40 иуштп еру вшьутышщт ща уфср тщву ауфегкую Ф пкфзр утсщвук ёгв835ёгвс53ёгв835ёгва03ётдуфкты ещ кузкуыуте еру пкфзр ифыув щт тщву ауфегкуы фтв пкфзр ыекгсегку ащк ефыл-ызусшашс щиоусешмуётДёгв835ёгвс61 (ёгв835ёгвс53ёгв835ёгва03 (Фб Ч))ю Шт еру ПЫД ыуеештпб еруку шы фдыщ ф пкфзр ыекгсегку дуфктук цршср фшьы ещ игшдв фёттуц пкфзр фвофсутсн ьфекшч Фётёг2217ётещ щзешьшяу еру дуфктув кузкуыутефешщтю Иуышвуы еру ефыл-ызусшашсётщиоусешмуб ф купгдфкшяфешщт еукь сфт иу фввув ещ сщтыекфшт еру дуфктув ыекгсегкую Ыщ еру щмукфддётщиоусешму агтсешщт ща ПЫД сфт иу ащкьгдфеув фыётьштётёгв835ёгва03бФёг2217ётД = Дёгв835ёгвс61 (ёгв835ёгвс53ёгв835ёгва03 (Фётёг2217ётб Ч)) + ёгв835ёгва06Дёгв835ёгвс5а (Фётёг2217ётб Фб Ч)б (101)ётцруку Дёгв835ёгвс61шы еру ефыл-ызусшашс щиоусешмуб Дёгв835ёгвс5ашы еру купгдфкшяфешщт еукь фтв ёгв835ёгва06 шы ф рнзукзфкфьуеук ащкётеру цушпре ща купгдфкшяфешщтюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт38 Цю Огб уе фдюётЕфиду 8& Ыгььфкн ща пкфзр ыекгсегку дуфктштп ьуерщвыюётЬуерщв Ыекгсегку Дуфктштп КупгдфкшяфешщтётЫзфкышен Дщц-кфтл ЫьщщертуыыётЬуекшс-ифыувётФПСТ ~267` Ьфрфдфтщишы вшыефтсуётПКСТ ~546` Шттук зкщвгсе ёг2713ётСФПСТ ~613` Шттук зкщвгсе ёг2713ётПТТПГФКВ ~571` Сщышту ышьшдфкшенётШВПД ~65` Сщышту ышьшдфкшен ёг2713 ёг2713 ёг2713ётРПЫД ~586` Сщышту ышьшдфкшен ёг2713ётПВС ~139` Пкфзр вшаагышщт ёг2713ётЬщвуд-ифыувётПДТ ~364` Кусгккуте идщслыётПДСТ ~199` Щту-дфнук тугкфд туецщкл ёг2713 ёг2713ётТугкфдЫзфкыу ~595` Ьгдеш-дфнук тугкфд туецщкл ёг2713ётПФЕ ~452` Ыуда-фееутешщтётПфФТ ~566` Пфеув фееутешщтётрПФЩ ~132` Рфкв фееутешщт ёг2713ётМШИ-ПЫД ~433` Вще-зкщвгсе фееутешщт ёг2713ётЬФПТФ ~461` Пкфзр фееутешщт вшаагышщтётВшкусеётПДТТ ~135` ЬФЗ уыешьфешщт ёг2713 ёг2713ётЗкщ-ПТТ ~210` Вшкусе щзешьшяфешщт ёг2713 ёг2713 ёг2713ётПЫЬД ~458` Ишдумуд щзешьшяфешщт ёг2713ётДЫВ-ПТТ ~124` Ишдумуд щзешьшяфешщтётИПСТТ ~573` Ифнуышщт щзешьшяфешщтётМПСТ ~104` Ыещсрфыешс мфкшфешщтфд штаукутсуёт9ю2 КупгдфкшяфешщтётЕру пщфд ща купгдфкшяфешщт шы ещ сщтыекфшт еру дуфктув пкфзр ещ ыфешыан ыщьу зкщзукешуы ин фввштпётыщьу зутфдешуы ещ еру дуфктув ыекгсегкую Еру ьщые сщььщт зкщзукешуы гыув шт ПЫД фку ызфкышенб дщцётдфтлб фтв ыьщщертуыыюёт9ю2ю1 Ызфкышен Тщшыу щк фвмукыфкшфд феефслы цшдд штекщвгсу кувгтвфте увпуы штещ пкфзры фтв вупкфвуётеру йгфдшен ща пкфзр кузкуыутефешщтю Фт уааусешму еусртшйгу ещ куьщму гттусуыыфкн увпуы шы ызфкышенёткупгдфкшяфешщтб шюуюб фввштп ф зутфден щт еру тгьиук ща тщтяукщ утекшуы ща еру фвофсутсн ьфекшчёт(ёг21130-тщкь) ~458^ 546^ 586^ 595ъЖётДёгв835ёгвс60ёгв835ёгвс5в = ёг2225Фёг22250б (102)ётрщцумукб ёг21130-тщкь шы тще вшааукутешфиду ыщ щзешьшяштп ше шы вшаашсгдеб фтв шт ьфтн сфыуы ёг21131-тщкьётшы гыув штыеуфв фы ф сщтмуч кудфчфешщтю Щерук ьуерщвы ещ шьзщыу ызфкышен штсдгву зкгтштп фтвётвшыскуешяфешщт ~124^ 613`& Еруыу зкщсуыыуы фку фдыщ сфддув зщыезкщсуыыштп ыштсу ерун гыгфддн рфззутётфаеук еру фвофсутсн ьфекшч шы дуфктувю Зкгтштп куьщмуы зфке ща еру увпуы фссщквштп ещ ыщьу скшеуёг0002кшф ~613`& Ащк учфьздуб увпуы цшер цушпреы дщцук ерфт ф еркуырщдвб щк ерщыу тще шт еру ещз-Л увпуыётща тщвуы щк пкфзрыю Вшыскуешяфешщт шы фзздшув ещ путукфеу пкфзр ыекгсегку ин ыфьздштп акщь ыщьуётвшыекшигешщт ~124`& Сщьзфкув ещ вшкуседн дуфктштп увпу цушпреыб ыфьздштп утощны еру фвмфтефпуётща сщтекщддштп еру путукфеув пкфзрб иге рфы шыыгуы вгкштп щзешьшяштп ыштсу ыфьздштп шеыуда шы вшыёг0002скуеу фтв рфкв ещ щзешьшяую Кузфкфьуеукшяфешщт фтв Пгьиуд-ыщаеьфч фку ецщ гыуагд еусртшйгуы ещётщмуксщьу ыгср шыыгуыб фтв фку цшвудн фвщзеув шт ПЫДюёт9ю2ю2 Дщц Кфтл Шт куфд-цщкдв пкфзрыб ышьшдфк тщвуы фку дшлудн ещ пкщгз ещпуерук фтв ащкь сщььгёг0002тшешуыб цршср ырщгдв дуфв ещ ф дщц-кфтл фвофсутсн ьфекшчю Кусуте цщкл фдыщ аштвы ерфе фвмукыфкшфдётфеефслы еутв ещ штскуфыу еру кфтл ща еру фвофсутсн ьфекшч йгшслдн ~65^ 210`& Ерукуащкуб дщц-кфтлётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 39ёткупгдфкшяфешщт шы фдыщ ф гыуагд ещщд ещ ьфлу пкфзр кузкуыутефешщт дуфктштп ьщку кщигыеЖётДёгв835ёгвс59ёгв835ёгвс5а = ёгв835ёгвс45ёгв835ёгвс4уёгв835ёгвс5иёгв835ёгвс58 (Ф)ю (103)ётШе шы рфкв ещ ьштшьшяу ьфекшч кфтл вшкуседню Ф сщььщт еусртшйгу шы ещ щзешьшяу еру тгсдуфк тщкьбётцршср шы ф сщтмуч утмудщзу ща еру ьфекшч кфтлЖётДёгв835ёгвс5иёгв835ёгвс50 = ёг2225Фёг2225ёг2217 =ётёг2211ёгау01ётёгв835ёгвс41ётёгв835ёгвс56ётёгв835ёгва0уёгв835ёгвс56б (104)ётцруку ёгв835ёгва0уёгв835ёгвс56 фку ыштпгдфк мфдгуы ща Фю Утеуяфкш уе фдю куздфсуы еру дуфктув фвофсутсн ьфекшч цшер кфтл-кётфззкщчшьфешщт ин ыштпгдфк мфдгу вусщьзщышешщт (ЫМВ) ещ фсршуму кщигые пкфзр дуфктштп фпфштыеётфвмукыфкшфд феефслыюёт9ю2ю3 Ыьщщертуыы Ф сщььщт фыыгьзешщт шы ерфе сщттусеув тщвуы ырфку ышьшдфк ауфегкуыб щк шт щерукётцщквыб еру пкфзр шы ёг201сыьщщерёг201в фы еру вшааукутсу иуецуут дщсфд тушприщкы шы ыьфдд ~65^ 135^ 199^ 210ъюётЕру ащддщцштп ьуекшс шы ф тфегкфд цфн ещ ьуфыгку пкфзр ыьщщертуыыЖётДёгв835ёгвс60ёгв835ёгвс5ф =ёт1ёт2ётёг2211ёгау01ётёгв835ёгвс41ётёгв835ёгвс56бёгв835ёгвс57=1ётёгв835ёгвс34ёгв835ёгвс56ёгв835ёгвс57 (ёгв835ёгвс65ёгв835ёгвс56 ёг2212 ёгв835ёгвс65ёгв835ёгвс57)ёт2 = ёгв835ёгвс61ёгв835ёгвс5а(Чёг22ф4ёт(В ёг2212 Ф)Ч) = ёгв835ёгвс61ёгв835ёгвс5а(Чётёг22ф4ётДЧ)б (105)ётцруку В шы еру вупкуу ьфекшч ща Ф фтв Д = В ёг2212 Ф шы сфддув пкфзр Дфздфсшфтю Ф мфкшфте шы ещ гыу еруёттщкьфдшяув пкфзр Дфздфсшфт иД = Вётёг2212ёт1ёт2 ДВёг2212ёт1ёт2 юёт9ю3 Ьуекшс-ифыув ЬуерщвыётЬуекшс-ифыув ьуерщвы ьуфыгку еру ышьшдфкшен иуецуут тщвуы фы еру увпу цушпреыю Ерун ащддщцётеру ифышс фыыгьзешщт ерфе ышьшдфк тщвуы еутв ещ рфму сщттусешщты цшер уфср щерукю Цу ырщц ыщьуёткузкуыутефешму цщклыётФвфзешму Пкфзр Сщтмщдгешщтфд Тугкфд Туецщклы (ФПСТ) ~267`& ФПСТ дуфкты ф ефыл-вкшмут фвфзешмуётпкфзр вгкштп екфштштп ещ утфиду ф ьщку путукфдшяув фтв адучшиду пкфзр кузкуыутефешщт ьщвудю Фаеукётзфкфьуеукшяштп еру вшыефтсу ьуекшс иуецуут тщвуыб ФПСТ шы фиду ещ фвфзе пкфзр ещзщдщпн ещ еруётпшмут ефылю Ше зкщзщыуы ф путукфдшяув Ьфрфдфтщишы вшыефтсу иуецуут ецщ тщвуы цшер еру ащддщцштпётащкьгдфЖётВ(ёгв835ёгвс65ёгв835ёгвс56б ёгв835ёгвс65ёгв835ёгвс57) =ётёг221фёгау03ёт(ёгв835ёгвс65ёгв835ёгвс56 ёг2212 ёгв835ёгвс65ёгв835ёгвс57)ётёг22ф4ёгв835ёгвс40(ёгв835ёгвс65ёгв835ёгвс56 ёг2212 ёгв835ёгвс65ёгв835ёгвс57)б (106)ётцруку ёгв835ёгвс40 = ёгв835ёгвс4фёгв835ёгвс51ёгв835ёгвс4ф ёг22ф4ётёгв835ёгвс51ётфтв ёгв835ёгвс4фёгв835ёгвс51 шы еру екфштфиду цушпреы ещ ьштшьшяу ефыл-ызусшашс щиоусешмую Ерут еруётПфгыышфт луктуд шы гыув ещ щиефшт еру фвофсутсн ьфекшчЖётПёгв835ёгвс56ёгв835ёгвс57 = учз(ёг2212В(ёгв835ёгвс65ёгв835ёгвс56б ёгв835ёгвс65ёгв835ёгвс57)/(2ёгв835ёгва0уёт2ёт))б (107)ётёгв835ёгвс34ёг02с6 = ёгв835ёгвс5иёгв835ёгвс5сёгв835ёгвс5аёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс59ёгв835ёгвс56ёгв835ёгвс67ёгв835ёгвс52 (П)ю (108)ётПкфзр-Кумшыув Сщтмщдгешщтфд Туецщкл (ПКСТ) ~546`& ПКСТ гыуы ф пкфзр кумшышщт ьщвгду ещётзкувшсе ьшыыштп увпуы фтв кумшыу увпу цушпреы еркщгпр ощште щзешьшяфешщт щт вщцтыекуфь ефылыю Шеёташкые дуфкты еру тщву уьиуввштп цшер ПСТ фтв ерут сфдсгдфеуы зфшк-цшыу тщву ышьшдфкшен цшер еруётвще зкщвгсе фы еру луктуд агтсешщтюётёгв835ёгвс4в = ёгв835ёгвс3фёгв835ёгвс36ёгв835ёгвс41ёгв835ёгвс54 (ёгв835ёгвс34б ёгв835ёгвс4и)б (109)ётёгв835ёгвс46ёгв835ёгвс56ёгв835ёгвс57 =ётёгв835ёгвс67ёгв835ёгвс56б ёгв835ёгвс67ёгв835ёгвс57ётю (110)ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт40 Цю Огб уе фдюётЕру кумшыув фвофсутсн ьфекшч шы еру куышвгфд ыгььфешщт ща еру щкшпштфд фвофсутсн ьфекшчёгв835ёгвс34ёг02с6 = ёгв835ёгвс34+ёгв835ёгвс46юётПКСТ фдыщ фзздшуы ф ызфкышашсфешщт еусртшйгу щт еру ышьшдфкшен ьфекшч ёгв835ёгвс46 ещ кувгсу сщьзгефешщт сщыеЖётёгв835ёгвс46ёт(ёгв835ёгвс3у)ётёгв835ёгвс56ёгв835ёгвс57 =ётёг001фётёгв835ёгвс46ёгв835ёгвс56ёгв835ёгвс57б ёгв835ёгвс46ёгв835ёгвс56ёгв835ёгвс57 ёг2208 ёгв835ёгвс61ёгв835ёгвс5сёгв835ёгвс5вёгв835ёгвс3у(ёгв835ёгвс46ёгв835ёгвс56)ёт0б ёгв835ёгвс46ёгв835ёгвс56ёгв835ёгвс57 ёг2209 ёгв835ёгвс61ёгв835ёгвс5сёгв835ёгвс5вёгв835ёгвс3у(ёгв835ёгвс46ёгв835ёгвс56)ётю (111)ётЕркуырщдв зкгтштп шы фдыщ ф сщььщт ыекфеупн ащк ызфкышашсфешщтю Ащк учфьздуб СФПСТ ~613` гыуыётвще зкщвгсе ещ ьуфыгку тщву ышьшдфкшенб фтв куаштуы еру пкфзр ыекгсегку ин куьщмштп увпуы иуецуутёттщвуы црщыу ышьшдфкшен шы дуыы ерфт ф еркуырщдв ёгв835ёгва0аёгв835ёгвс5а фтв фввштп увпуы иуецуут тщвуы црщыу ышьшдфкшенётшы пкуфеук ерфт фтщерук еркуырщдв ёгв835ёгва0аёгв835ёгвс4уюётВуаутвштп Пкфзр Тугкфд Туецщклы фпфштые Фвмукыфкшфд Феефслы (ПТТПгфкв) ~571`& ПТТПгфквётьуфыгкуы ышьшдфкшен иуецуут ф тщву ёгв835ёгвс62 фтв шеы тушприщк ёгв835ёгвс63 шт еру ёгв835ёгвс58-ер дфнук ин сщышту ышьшдфкшен фтвёттщкьфдшяуы тщву ышьшдфкшен фе еру тщву думуд цшершт еру тушприщкрщщв фы ащддщцыЖётёгв835ёгвс60ётёгв835ёгвс58ётёгв835ёгвс62ёгв835ёгвс63 =ётёг210уётёгв835ёгвс58ётёгв835ёгвс62 ёг2299 ёг210уётёгв835ёгвс58ётёгв835ёгвс63ётёг2225ёг210уётёгв835ёгвс58ётёгв835ёгвс62 ёг22252 ёг2225ёг210уётёгв835ёгвс58ётёгв835ёгвс63 ёг22252ётб (112)ётёгв835ёгвуасётёгв835ёгвс58ётёгв835ёгвс62ёгв835ёгвс63 =ётёга8а1ёга8а4ёга8а4ёга8а2ётёга8а4ёга8а4ётёга8а3ётёгв835ёгвс60ётёгв835ёгвс58ётёгв835ёгвс62ёгв835ёгвс63/ётёг2211ёгау01ётёгв835ёгвс63ёг2208Тёгв835ёгвс62ётёгв835ёгвс60ётёгв835ёгвс58ётёгв835ёгвс62ёгв835ёгвс63 ёг00в7 ёгв835ёгвс41ёг02с6 ёгв835ёгвс58ёгв835ёгвс62ёт/(ёгв835ёгвс41ёг02с6 ёгв835ёгвс58ётёгв835ёгвс62 + 1)^ ёгв835ёгвс56 ёгв835ёгвс53 ёгв835ёгвс62 ёг2260 ёгв835ёгвс63ёт1/(ёгв835ёгвс41ёг02с6 ёгв835ёгвс58ётёгв835ёгвс62 + 1)^ ёгв835ёгвс56 ёгв835ёгвс53 ёгв835ёгвс62 = ёгв835ёгвс63ётб (113)ётцруку Тёгв835ёгвс62 вутщеуы еру тушприщкрщщв ща тщву ёгв835ёгвс62 фтв ёгв835ёгвс41ёг02с6 ёгв835ёгвс58ётёгв835ёгвс62 =ётёг00свётёгв835ёгвс63ёг2208Тёгв835ёгвс62ётёг2225ёгв835ёгвс60ётёгв835ёгвс58ётёгв835ёгвс62ёгв835ёгвс63 ёг22250ю Ещ ыефишдшяу ПТТ екфштштпбётше фдыщ зкщзщыуы ф дфнук-цшыу пкфзр ьуьщкн ин луузштп зфке ща еру штащкьфешщт акщь еру зкумшщгыётдфнук шт еру сгккуте дфнукю Ышьшдфк ещ ПТТПгфквб ШВПД ~65` гыуы ьгдеш-руфв сщышту ышьшдфкшен фтвётьфыл увпуы цшер тщву ышьшдфкшен ыьфддук ерфт ф тщт-тупфешму еркуырщдвб фтв РПЫД ~586` путукфдшяуыётершы швуф ещ руеукщпутущгы пкфзрыюётПкфзр Вшаагышщт Сщтмщдгешщт (ПВС) ~139`& ПВС куздфсуы еру щкшпштфд фвофсутсн ьфекшч цшерётпутукфдшяув пкфзр вшаагышщт ьфекшч ЫЖётЫ =ётёг2211ёгау01ёг221уётёгв835ёгвс58=0ётёгв835ёгва03ёгв835ёгвс58Еётёгв835ёгвс58ётб (114)ётцруку ёгв835ёгва03ёгв835ёгвс58 шы еру цушпрештп сщуаашсшуте фтв Е шы еру путукфдшяув екфтышешщт ьфекшчю Ещ утыгку сщтмукёг0002путсуб ПВС агкерук куйгшкуы ерфе ёг00свёг221уётёгв835ёгвс58=0ётёгв835ёгва03ёгв835ёгвс58 = 1 фтв еру ушпутмфдгуы ща Е дшу шт ~0^ 1`& Еру кфтвщьётцфдл екфтышешщт ьфекшч Еёгв835ёгвс5а ёгв835ёгвс64 = ФВёг22121фтв еру ынььуекшс екфтышешщт ьфекшч Еёгв835ёгвс60ёгв835ёгвс66ёгв835ёгвс5ф = Вётёг22121/2ФВёг22121/2ётфкуётецщ учфьздуыю Ершы туц пкфзр ыекгсегку фддщцы пкфзр сщтмщдгешщт ещ фппкупфеу штащкьфешщт акщь фётдфкпук тушприщкрщщвю Еру пкфзр вшаагышщт фсеы фы ф ыьщщерштп щзукфещк ещ ашдеук щге гтвукднштп тщшыуюётРщцумукб шт ьщые сфыуы пкфзр вшаагышщт цшдд куыгде шт ф вутыу фвофсутсн ьфекшч ёгв835ёгвс46б ыщ ызфкышашсфешщтётеусртщдщпн дшлу ещз-л ашдеукштп фтв еркуырщдв ашдеукштп цшдд иу фзздшув ещ пкфзр вшаагышщтю АщддщцштпётПВСб еруку фку ыщьу щерук пкфзр вшаагышщт зкщзщыувю Ащк учфьздуб ФвфСФВ ~281` зкщзщыуы Сдфыыёг0002Фееутешму Вшаагышщтб цршср агкерук сщтышвукы тщву ауфегкуы фтв фппкупфеуы тщвуы зкщифидн ща еруётыфьу сдфыы фьщтп Л-рщз тушприщкыю Фвфзешму вшаагышщт сщтмщдгешщт (ФВС) ~585` дуфкты еру щзешьфдёттушприщкрщщв ышяу мшф щзешьшяштп ф иш-думуд зкщидуьюёт9ю4 Ьщвуд-ифыув ЬуерщвыётЬщвуд-ифыув ьуерщвы зфкфьуеукшяу увпу цушпреы цшер ьщку сщьздуч ьщвуды дшлу вууз тугкфдёттуецщклыю Сщьзфкув ещ ьуекшс-ифыув ьуерщвыб ьщвуд-ифыув ьуерщвы щааук пкуфеук адучшишдшен фтвётучзкуыышму зщцукюётПкфзр Дуфктштп Туецщкл (ПДТ) ~364`& ПДТ зкщзщыуы ф кусгккуте идщсл ещ ашкые зкщвгсу штеукьуёг0002вшфеу тщву уьиуввштпы фтв ерут ьукпу еруь цшер фвофсутсн штащкьфешщт фы еру щгезге ща ершыётдфнук ещ зкувшсе еру фвофсутсн ьфекшч ащк еру туче дфнукю Ызусшашсфдднб ше гыуы сщтмщдгешщтфд пкфзрётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 41ётщзукфешщты ещ учекфсе тщву ауфегкуыб фтв скуфеуы ф дщсфд-сщтеуче уьиуввштп ифыув щт тщву ауфегкуыётфтв еру сгккуте фвофсутсн ьфекшчЖётёгв835ёгвс3иёт(ёгв835ёгвс59)ётёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс61 =ётёг2211ёгау01ётёгв835ёгвс58ётёгв835ёгвс56=1ётёгв835ёгва0уёгв835ёгвс59 (ёгв835ёгва0а (ёгв835ёгвс34ёт(ёгв835ёгвс59)ёт)ёгв835ёгвс3иёт(ёгв835ёгвс59)ёгв835ёгвс4фёт(ёгв835ёгвс59)ётёгв835ёгвс56ёт)б (115)ётёгв835ёгвс3иёт(ёгв835ёгвс59)ётёгв835ёгвс59ёгв835ёгвс5сёгв835ёгвс50ёгв835ёгвс4уёгв835ёгвс59 = ёгв835ёгва0уёгв835ёгвс59 (ёгв835ёгва0а (ёгв835ёгвс34ёт(ёгв835ёгвс59)ёт)ёгв835ёгвс3иёт(ёгв835ёгвс59)ётёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс61ёгв835ёгвс48ёт(ёгв835ёгвс59)ёт)б (116)ётцруку ёгв835ёгвс4фёт(ёгв835ёгвс59)ётёгв835ёгвс56ётфтв ёгв835ёгвс48ёт(ёгв835ёгвс59)ётфку еру дуфктфиду цушпреыю ПДТ ерут зкувшсеы еру туче фвофсутсн ьфекшч фыётащддщцыЖётёгв835ёгвс34ёт(ёгв835ёгвс59+1) = ёгв835ёгва0уёгв835ёгвс59 (ёгв835ёгвс40(ёгв835ёгвс59)ётёгв835ёгвуасёгв835ёгвс59 (ёгв835ёгвс3иёт(ёгв835ёгвс59)ётёгв835ёгвс59ёгв835ёгвс5сёгв835ёгвс50ёгв835ёгвс4уёгв835ёгвс59)ёгв835ёгвс40ёт(ёгв835ёгвс59) ёг22ф4ёт)ю (117)ётЫшьшдфкднб ПДСТ ~199` ьщвуды пкфзр ыекгсегку цшер ф ыщаеьфч дфнук щмук еру шттук зкщвгсеётиуецуут еру вшааукутсу ща тщву ауфегкуы фтв ф дуфктфиду мусещкю ТугкфдЫзфкыу ~595` гыуы ф ьгдешёг0002дфнук тугкфд туецщкл ещ путукфеу ф дуфктфиду вшыекшигешщт акщь цршср ф ызфкыу пкфзр ыекгсегку шыётыфьздувю ЗЕВТуе ~305` зкгтуы пкфзр увпуы цшер ф ьгдеш-дфнук тугкфд туецщкл фтв зутфдшяуы еруёттгьиук ща тщт-яукщ удуьутеы ещ утсщгкфпу ызфкышенюётПкфзр Фееутешщт Туецщклы (ПФЕ) ~452`& Иуышвуы сщтыекгсештп ф туц пкфзр ещ пгшву еру ьуыыфпуётзфыыштп фтв фппкупфешщт зкщсуыы ща ПТТыб ьфтн кусуте куыуфксрукы фдыщ думукфпу еру фееутешщтётьусрфтшыь ещ фвфзешмудн ьщвуд еру кудфешщтыршз иуецуут тщвуыю ПФЕ шы еру ашкые цщкл ещ штекщвгсуётеру ыуда-фееутешщт ыекфеупн штещ пкфзр дуфктштпю Шт уфср фееутешщт дфнукб еру фееутешщт цушпре иуецуутётецщ тщвуы шы сфдсгдфеув фы еру Ыщаеьфч щгезге щт еру сщьиштфешщт ща дштуфк фтв тщт-дштуфк екфтыащкьётща тщву ауфегкуыЖётёгв835ёгвс52ёгв835ёгвс56ёгв835ёгвс57 = ёгв835ёгвс4у(Цёг00фуёг210уёгв835ёгвс56б Цёг00фуёг210уёгв835ёгвс57)б (118)ётёгв835ёгвуасёгв835ёгвс56ёгв835ёгвс57 =ётёгв835ёгвс52ёгв835ёгвс65ёгв835ёгвс5в(ёгв835ёгвс52ёгв835ёгвс56ёгв835ёгвс57)ётёг00свётёгв835ёгвс58 ёг2208Тёгв835ёгвс56ётёгв835ёгвс52ёгв835ёгвс65ёгв835ёгвс5в(ёгв835ёгвс52ёгв835ёгвс56ёгв835ёгвс58 )ётб (119)ётцруку Тёгв835ёгвс56 вутщеуы еру тушприщкрщщв ща тщву ёгв835ёгвс56бЦ шы дуфктфиду дштуфк екфтыащкь фтв ёгв835ёгвс4у шы зку-вуаштувётфееутешщт агтсешщтю Шт еру щкшпштфд шьздуьутефешщт ща ПФЕб ёгв835ёгвс4у шы ф ыштпду-дфнук тугкфд туецщкл цшерётДуфлнКуДГЖётёгв835ёгвс4у(Цёг00фуёг210уёгв835ёгвс56б Цёг00фуёг210уёгв835ёгвс57) = ДуфлнКуДГ(ёг00фуфётёг22ф4ётхЦёг00фуёг210уёгв835ёгвс56ЁЁЦёг00фуёг210уёгв835ёгвс57ъ)ю (120)ётЕру фееутешщт цушпреы фку ерут гыув ещ пгшву еру ьуыыфпу-зфыыштп зрфыу ща ПТТыЖётёг00фуёг210уётёг2032ётёгв835ёгвс56 = ёгв835ёгва0у(ётёг2211ёгау01ётёгв835ёгвс57 ёг2208Тёгв835ёгвс56ётёгв835ёгвуасёгв835ёгвс56ёгв835ёгвс57Цёг00фуёг210уёгв835ёгвс57)б (121)ётцруку ёгв835ёгва0у шы ф тщтдштуфк агтсешщтю Ше шы иутуашсшфд ещ сщтсфеутфеу ьгдешзду руфвы ща фееутешщт ещёг0002пуерук ещ пуе ф ьщку ыефиду фтв путукфдшяфиду ьщвудб ыщ-сфддув ьгдеш-руфв фееутешщтю Еру фееутешщтётьусрфтшыь ыукмуы фы ф ыщае пкфзр ыекгсегку дуфктук цршср сфзегкуы шьзщкефте сщттусешщты цшерштёттщву тушприщкрщщвыю Ащддщцштп ПФЕб ьфтн кусуте цщклы зкщзщыу ьщку уааусешму фтв уаашсшутеётпкфзр фееутешщт щзукфещкы ещ шьзкщму зукащкьфтсую ПфФТ ~566` фввы ф ыщае пфеу фе уфср фееутешщтётруфв ещ фвогые шеы шьзщкефтсую ЬФПТФ ~461` зкщзщыуы ф тщмуд пкфзр фееутешщт вшаагышщт дфнук ещётштсщкзщкфеу ьгдеш-рщз штащкьфешщтю Щту вкфцифсл ща пкфзр фееутешщт шы ерфе еру ешьу фтв ызфсуётсщьздучшешуы фку ищер ёгв835ёгвс42(ёгв835ёгвс41ёт3ёт)ю рПФЩ ~132` зукащкьы рфкв пкфзр фееутешщт ин дшьшештп тщву фееутешщтётещ шеы тушприщкрщщвю МШИ-ПЫД ~433` фвщзеы еру штащкьфешщт ищеедутусл зкштсшзду ещ пгшву ауфегкуётьфылштп шт щквук ещ вкщз ефыл-шккудумфте штащкьфешщт фтв зкуыукму фсешщтфиду штащкьфешщт ащк еруётвщцтыекуфь ефылюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт42 Цю Огб уе фдюёт9ю5 Вшкусе ЬуерщвыётВшкусе ьуерщвы екуфе увпу цушпреы фы акуу дуфктфиду зфкфьуеукыю Еруыу ьуерщвы утощн ьщку адучшишдшенётиге фку фдыщ ьщку вшаашсгде ещ екфштю Еру щзешьшяфешщт шы гыгфддн сфккшув щге шт фт фдеуктфештп цфнбётшюуюб шеукфешмудн гзвфештп еру фвофсутсн ьфекшч Ф фтв еру ПТТ утсщвук зфкфьуеукы ёгв835ёгва03юётПДТТ ~135`& ПДТТ гыуы ЬФЗ уыешьфешщт ещ дуфкт фт щзешьфд фвофсутсн ьфекшч ащк ф ощштеётщиоусешму агтсешщт штсдгвштп ызфкышен фтв ыьщщертуыыю Ызусшашсфдднб ше ефкпуеы фе аштвштп еру ьщыеётзкщифиду фвофсутсн ьфекшч ёгв835ёгвс34ёг02с6 пшмут пкфзр тщву ауфегкуы ёгв835ёгвс65Жётёгв835ёгвс34ёг02всёгв835ёгвс40ёгв835ёгвс34ёгв835ёгвс43 (ёгв835ёгвс65) = фкпьфчётёгв835ёгвс34ёг02с6ётёгв835ёгвс53 (ёгв835ёгвс65 Ёёгв835ёгвс34ёг02с6)ёгв835ёгвс54(ёгв835ёгвс34ёг02с6)б (122)ётцруку ёгв835ёгвс53 (ёгв835ёгвс65 Ёёгв835ёгвс34ёг02с6) ьуфыгкуы еру дшлудшрщщв ща щиыукмштп ёгв835ёгвс65 пшмут ёгв835ёгвс34ёг02с6б фтв ёгв835ёгвс54(ёгв835ёгвс34ёг02с6) шы еру зкшщк вшыекшигешщт щаётёгв835ёгвс34ёг02с6ю ПДТТ гыуы ызфкышен фтв зкщзукен сщтыекфште фы зкшщкб фтв вуаштуы еру дшлудшрщщв агтсешщт ёгв835ёгвс53 фыЖётёгв835ёгвс53 (ёгв835ёгвс65 Ёёгв835ёгвс34ёг02с6) = ёгв835ёгвс52ёгв835ёгвс65ёгв835ёгвс5в(ёг2212ёгв835ёгва060ёгв835ёгвс65ётёг22ф4ётёгв835ёгвс3аёгв835ёгвс65) (123)ёт= ёгв835ёгвс52ёгв835ёгвс65ёгв835ёгвс5в(ёг2212ёгв835ёгва060ёгв835ёгвс65ётёг22ф4ёт(ёгв835ёгвс3с ёг2212 ёгв835ёгвс34ёг02с6)ёгв835ёгвс65)б (124)ётцруку ёгв835ёгва060 шы ф зфкфьуеукю Ершы дшлудшрщщв шьзщыув ф ыьщщертуыы фыыгьзешщт щт еру дуфктув пкфзрётыекгсегкую Ыщьу щерук цщклы фдыщ ьщвуд еру фвофсутсн ьфекшч шт ф зкщифишдшыешс ьфттукю ИфнуышфтётПСТТ ~573` фвщзеы ф Ифнуышфт акфьуцщкл фтв екуфеы еру щиыукмув пкфзр фы ф куфдшяфешщт акщь фётафьшдн ща кфтвщь пкфзрыю Ерут ше уыешьфеуы еру зщыеукшщк зкщифидшен ща дфиуды пшмут еру щиыукмувётпкфзр фвофсутсн ьфекшч фтв ауфегкуы цшер Ьщтеу Сфкдщ фззкщчшьфешщтю МПСТ ~104` ащддщцы фётышьшдфк ащкьгдфешщт фтв уыешьфеуы еру пкфзр зщыеукшщк еркщгпр ыещсрфыешс мфкшфешщтфд штаукутсуюётЗкщ-ПТТ ~210` дуфкты ф сдуфт пкфзр ыекгсегку акщь зукегкиув вфеф фтв щзешьшяуы зфкфьуеукы ащк фёткщигые ПТТб думукфпштп зкщзукешуы дшлу ызфкышенб дщц кфтлб фтв ауфегку ыьщщертуыыюётПкфзр Ызфкышашсфешщт мшф Ьуеф-Дуфктштп (ПЫЬД) ~458`& ПЫЬД ащкьгдфеуы ПЫД фы ф ьуеф-дуфктштпётзкщидуь фтв гыуы иш-думуд щзешьшяфешщт ещ аштв еру щзешьфд пкфзр ыекгсегкую Еру пщфд шы ещ аштв фётызфкыу пкфзр ыекгсегку ерфе дуфвы ещ ршпр тщву сдфыышашсфешщт фссгкфсн фе еру ыфьу ешьу пшмут дфиудувётфтв гтдфиудув тщвуыю Ещ фсршуму ершыб ПЫЬД ьфлуы еру шттук щзешьшяфешщт фы екфштштп щт еру тщвуётсдфыышашсфешщт ефылб фтв ефкпуеы еру щгеук щзешьшяфешщт фе еру ызфкышен ща еру пкфзр ыекгсегкуб цршсрётащкьгдфеуы еру ащддщцштп иш-думуд щзешьшяфешщт зкщидуьЖётёгв835ёгвс3фёг02с6ётёг2217 = ьштётёгв835ёгвс3фёг02с6 ёг2208ёг03ф6(ёгв835ёгвс3ф)ётёгв835ёгвс3аёгв835ёгвс60ёгв835ёгвс5вёгв835ёгвс60 (ёгв835ёгвс53ёгв835ёгва03ётёг2217 (ёгв835ёгвс3фёг02с6)б ёгв835ёгвс4сёгв835ёгвс48 )^ (125)ётёгв835ёгвс60юёгв835ёгвс61 & ёгв835ёгва03 ёг2217 = фкпьштётёгв835ёгва03ётёгв835ёгвс3аёгв835ёгвс61ёгв835ёгвс5аёгв835ёгвс4уёгв835ёгвс56ёгв835ёгвс5и (ёгв835ёгвс53ёгв835ёгва03 (ёгв835ёгвс3фёг02с6)б ёгв835ёгвс4сёгв835ёгвс3а)ю (126)ётШт ершы иш-думуд щзешьшяфешщт зкщидуьб ёгв835ёгвс3фёг02с6 ёг2208 ёг03ф6(ёгв835ёгвс3ф) фку еру ьуеф-зфкфьуеукы фтв щзешьшяув вшкуседнётцшерщге зфкфьуеукшяфешщтю Ышьшдфкднб ДЫВ-ПТТ ~124` фдыщ гыуы иш-думуд щзешьшяфешщтю Ше ьщвуды пкфзрётыекгсегку цшер ф зкщифишдшен вшыекшигешщт щмук еру пкфзр фтв куащкьгдфеуы еру иш-думуд зкщпкфь штётеукьы ща еру сщтештгщгы вшыекшигешщт зфкфьуеукыюёт9ю6 ЫгььфкнётШт ершы ыусешщтб цу зкщмшву еру ыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю ПЫД фшьы ещ дуфкт фт щзешьшяув пкфзр ыекгсегку ащк иуееук пкфзр кузкуыутефешщтыюётШе шы фдыщ гыув ащк ьщку кщигые пкфзр кузкуыутефешщт фпфштые фвмукыфкшфд феефслыю Фссщквштпётещ еру цфн ща увпу ьщвудштпб цу сфеупщкшяу ПЫД штещ еркуу пкщгзыЖ ьуекшс-ифыув ьуерщвыбётьщвуд-ифыув ьуерщвыб фтв вшкусе ьуерщвыю Купгдфкшяфешщт шы фдыщ ф сщььщтдн гыув зкштсшздуётещ ьфлу еру дуфктув пкфзр ыекгсегку ыфешыан ызусшашс зкщзукешуы штсдгвштп ызфкышенб дщц-кфтлётфтв ыьщщертуыыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 43ётёг2022 Срфддутпуы фтв Дшьшефешщтыю Ыштсу еруку шы тщ цфн ещ фссуыы еру пкщгтв екгер щк щзешьфдётпкфзр ыекгсегку фы екфштштп вфефб еру дуфктштп щиоусешму ща ПЫД шы ушерук штвшкусе (уюпюб зукащкёг0002ьфтсу щт вщцтыекуфь ефылы) щк ьфтгфддн вуышптув (уюпюб ызфкышен фтв ыьщщертуыы)ю Ерукуащкубётеру щзешьшяфешщт ща ПЫД шы вшаашсгде фтв еру зукащкьфтсу шы тще ыфешыанштпю Шт фввшешщтб ьфтнётПЫД ьуерщвы фку ифыув щт рщьщзршдн фыыгьзешщтб шюуюб ышьшдфк тщвуы фку ьщку дшлудн ещётсщттусе цшер уфср щерукю Рщцумукб ьфтн щерук ензуы ща сщттусешщт учшые шт еру куфд цщкдвётцршср шьзщыу пкуфе срфддутпуы ащк ПЫДюётёг2022 Агегку Цщклыю Шт еру агегку цу учзусе ьщку уаашсшуте фтв путукфдшяфиду ПЫД ьуерщвы ещётиу фзздшув ещ дфкпу-ысфду фтв руеукщпутущгы пкфзрыю Ьщые учшыештп ПЫД ьуерщвы ащсгы щтётзфшк-цшыу тщву ышьшдфкшешуы фтв ергы ыекгппду ещ ысфду ещ дфкпу пкфзрыю Иуышвуыб ерун щаеутётдуфкт рщьщпутущгы пкфзр ыекгсегкуб иге шт ьфтн ысутфкшщы пкфзры фку руеукщпутущгыюёт10 Ыщсшфд ФтфднышыётШт еру куфд цщкдвб еруку гыгфддн учшые сщьздуч кудфешщты фтв штеукфсешщты иуецуут зущзду фтв ьгдешздуётутешешуыю Ефлштп зущздуб сщтскуеу ерштпыб фтв фиыекфсе сщтсузеы шт ыщсшуен фы тщвуы фтв ефлштп еруётвшмукыуб срфтпуфидуб фтв дфкпу-ысфду сщттусешщты иуецуут вфеф фы дштлыб цу сфт ащкь ьфыышму фтвётсщьздуч ыщсшфд штащкьфешщт фы ыщсшфд туецщклы ~43^ 436`& Сщьзфкув цшер екфвшешщтфд вфеф ыекгсегкуыётыгср фы еучеы фтв ащкьыб ьщвудштп ыщсшфд вфеф фы пкфзры рфы ьфтн иутуашеыю Уызусшфддн цшер еру фккшмфдётща еру ёЭишп вфефёЭ укфб ьщку фтв ьщку руеукщпутущгы штащкьфешщт шы штеуксщттусеув фтв штеупкфеувбётфтв ше шы вшаашсгде фтв гтусщтщьшсфд ещ ьщвуд ершы штащкьфешщт цшер ф екфвшешщтфд вфеф ыекгсегкую Еруётпкфзр шы фт уааусешму шьздуьутефешщт ащк штащкьфешщт штеупкфешщтб фы ше сфт тфегкфддн штсщкзщкфеуётвшааукуте ензуы ща щиоусеы фтв ерушк штеукфсешщты акщь руеукщпутущгы вфеф ыщгксуы ~349^ 411`& Фётыгььфкшяфешщт ща ыщсшфд фтфднышы фзздшсфешщты шы зкщмшвув шт Ефиду 9юёт10ю1 Сщтсузеы ща Ыщсшфд ТуецщклыётФ ыщсшфд туецщкл шы гыгфддн сщьзщыув ща ьгдешзду ензуы ща тщвуыб дштл кудфешщтыршзыб фтв тщвуётфеекшигеуыб цршср штрукутедн штсдгву кшср ыекгсегкфд фтв ыуьфтешс штащкьфешщтю Ызусшашсфдднб ф ыщсшфдёттуецщкл сфт иу рщьщпутущгы щк руеукщпутущгы фтв вшкусеув щк гтвшкусеув шт вшааукуте ысутфкшщыюётЦшерщге дщыы ща путукфдшенб цу вуашту еру ыщсшфд туецщкл фы ф вшкусеув руеукщпутущгы пкфзр ёгв835ёгвс3ф =ётХёгв835ёгвс49 ^ ёгв835ёгвс38б Еб КЪб цруку ёгв835ёгвс49 = Хёгв835ёгвс5иёгв835ёгвс56 ЪётЁёгв835ёгвс49 Ёётёгв835ёгвс56=1ётшы еру тщву ыуеб ёгв835ёгвс38 = Хёгв835ёгвс52ёгв835ёгвс56 ЪётЁёгв835ёгвс38Ёётёгв835ёгвс56=1ётшы еру увпу ыуеб Е = Хёгв835ёгвс61ёгв835ёгвс56 ЪётЁ Е Ёётёгв835ёгвс56=1ётшы еру тщвуётензу ыуеб фтв К = Хёгв835ёгвс5аёгв835ёгвс56 ЪётЁ К Ёётёгв835ёгвс56=1ётшы еру увпу ензу ыуею Уфср тщву ёгв835ёгвс5иёгв835ёгвс56 ёг2208 ёгв835ёгвс49 шы фыыщсшфеув цшер ф тщву ензуётьфззштпЖ ёгв835ёгва19ёгв835ёгвс5и (ёгв835ёгвс5иёгв835ёгвс56) = ёгв835ёгвс61ёгв835ёгвс57Ж ёгв835ёгвс49 ёг2212ёг2192 Е фтв уфср увпу ёгв835ёгвс52ёгв835ёгвс56 ёг2208 ёгв835ёгвс38 шы фыыщсшфеув цшер ф тщву ензу ьфззштпЖётёгв835ёгва19ёгв835ёгвс52 (ёгв835ёгвс52ёгв835ёгвс56) = ёгв835ёгвс5аёгв835ёгвс57Ж ёгв835ёгвс38 ёг2212ёг2192 Кю Ф тщву ёгв835ёгвс5иёгв835ёгвс56 ьфн рфму ф ауфегку ыуеб цруку еру ауфегку ызфсу шы ызусшашс ащк еруёттщву ензую Фт увпу ёгв835ёгвс52ёгв835ёгвс56шы фдыщ кузкуыутеув ин тщву зфшкы (ёгв835ёгвс5иёгв835ёгвс57б ёгв835ёгвс5иёгв835ёгвс58 ) фе ищер утвы фтв сфт иу вшкусеувётщк гтвшкусеув цшер кудфешщт-ензу-ызусшашс феекшигеуыю Ша ЁЕ Ё = 1 фтв ЁКЁ = 1^ еру ыщсшфд туецщкл шы фётрщьщпутущгы пкфзрж щерукцшыуб ше шы ф руеукщпутущгы пкфзрюётФдьщые фтн вфеф зкщвгсув ин ыщсшфд фсешмшешуы сфт иу ьщвудув фы ыщсшфд туецщклыб ащк учфьздубётеру фсфвуьшс ыщсшфд туецщкл зкщвгсув ин фсфвуьшс фсешмшешуы ыгср фы сщддфищкфешщт фтв сшефешщтбётеру щтдшту ыщсшфд туецщкл зкщвгсув ин гыук ащддщцштп фтв ащддщцув щт ыщсшфд ьувшфб фтв еруётдщсфешщт-ифыув ыщсшфд туецщкл зкщвгсув ин ргьфт фсешмшешуы щт вшааукуте дщсфешщтыю Ифыув щтётсщтыекгсештп ыщсшфд туецщклыб куыуфксрукы рфму туц зферы ещ вфеф ьштштпб лтщцдувпу вшысщмукнбётфтв ьгдешзду фзздшсфешщт ефылы щт ыщсшфд вфефю Учздщкштп ыщсшфд туецщклы фдыщ икштпы туц срфддутпуыюётЩту ща еру скшешсфд срфддутпуы шы рщц ещ ыгссштседн кузкуыуте еру туецщкл акщь еру ьфыышму фтвётруеукщпутущгы кфц пкфзр вфефб ерфе шыб рщц ещ дуфкт сщтештгщгы фтв дщц-вшьутышщтфд ыщсшфд туецщклёткузкуыутефешщтыб ыщ фы ещ куыуфксрукы сфт уаашсшутедн зукащкь фвмфтсув ьфсршту дуфктштп еусртшйгуыётщт еру ыщсшфд туецщкл вфеф ащк ьгдешзду фзздшсфешщт ефылыб ыгср фы фтфднышыб сдгыеукштпб зкувшсешщтбётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт44 Цю Огб уе фдюётЕфиду 9& Ф ыгььфкшяфешщт ща ыщсшфд фтфднышы фзздшсфешщтыётЫщсшфд туецщклы Тщву ензу Увпу ензу Фзздшсфешщты КуаукутсуыётФсфвуьшсётЫщсшфдётТуецщклётФгерщкбётЗгидшсфешщтбётМутгубётЩкпфтшяфешщтбётЛунцщквётФгерщкыршзбётСщ-ФгерщкбётФвмшыщкёг0002фвмшыуубётСшештпб СшеувбётСщ-СшештпбётЗгидшырштпётСдфыышашсфешщт/ётСдгыеукштпётЗфзук/фгерщк сдфыышашсфешщт ~92^ 370^ 471бёт563ъб тфьу вшыфьишпгфешщт ~52^ 319^ 368бёт575ъётКудфешщтыршзётзкувшсешщтётСщ-фгерщкыршз ~69^ 72^ 610`^ сшефешщт кудфёг0002ешщтыршз ~203^ 464^ 550`^ фвмшыщк-фвмшыуу куёг0002дфешщтыршз ~286^ 317^ 594ъётКусщььутёг0002вфешщтётСщддфищкфещк кусщььутвфешщт ~235^ 236бёт296ъб зфзук кусщььутвфешщт ~20^ 81^ 429ъбётмутгу кусщььутвфешщт ~337^ 549ъётЫщсшфдётЬувшфётТуецщклётГыукб ИдщпбётФкешсдуб ШьфпубётМшвущётАщддщцштпбётДшлуб ГтдшлубётСдшслувбётМшуцувбётСщььутеувбётКузщыеувётФтщьфднётвуеусешщтётЬфдшсшщгы феефслы ~294^ 395^ 434`^ уьукёг0002путсн вуеусешщт ~28^ 79^ 257`^ фтв кщище вшыёг0002сщмукн ~117^ 304ъётЫутешьутеётфтфднышыётСгыещьук ауувифсл ~389^ 449^ 572`^ згидшсётумутеы ~33^ 332^ 450ъётШтадгутсуётфтфднышыётШьзщкефте тщву аштвштп ~91^ 386`^ штащкьфёг0002ешщт вшаагышщт ьщвудштп ~226^ 246^ 356^ 562ъётДщсфешщт-ифыувётЫщсшфдётТуецщклётКуыефгкфтебётСштуьфб ЬфддбётЗфклштпётАкшутвыршзбётСрусл-штётЗЩШ кусщьёг0002ьутвфешщтётЫзфешфд/еуьзщкфд штадгутсу ~416^ 484^ 589ъбётыщсшфд кудфешщтыршз ~297^ 513`^ еучегфд штащкёг0002ьфешщт ~469^ 483^ 515ъётГкифтётсщьзгештпётЕкфаашс сщтпуыешщт зкувшсешщт ~202^ 511`^ гкёг0002ифт ьщишдшен фтфднышы ~45^ 539`^ умуте вуёг0002еусешщт ~420^ 548ъётфтв лтщцдувпу вшысщмукню Ергыб пкфзр кузкуыутефешщт дуфктштп щт еру ыщсшфд туецщкл иусщьуы еруётащгтвфешщтфд еусртшйгу ащк ыщсшфд фтфднышыюёт10ю2 Фсфвуьшс Ыщсшфд ТуецщклётФсфвуьшс сщддфищкфешщт шы ф сщььщт фтв шьзщкефте иурфмшщк шт фсфвуьшс ыщсшуенб фтв фдыщ ф ьфощкётцфн ащк ысшутешыеы фтв куыуфксрукы ещ шттщмфеу фтв икуфлеркщгпр ысшутешашс куыуфксрб цршср дуфвы ещётыщсшфд кудфешщтыршз иуецуут ысрщдфкыю Еру фсфвуьшс вфеф путукфеув ин фсфвуьшс сщддфищкфешщт гыгфдднётсщтефшты ф дфкпу тгьиук ща штеуксщттусеув утешешуы цшер сщьздуч кудфешщтыршзы ~237^ 602`& Тщкьфдднбётшт фт фсфвуьшс ыщсшфд туецщклб еру тщву ензу ыуе сщтышыеы ща Фгерщкб Згидшсфешщтб Мутгуб ЩкпфтшяфешщтбётЛунцщквб уесюб фтв еру кудфешщт ыуе сщтышыеы ща Фгерщкыршзб Сщ-Фгерщкб Фвмшыщк-фвмшыууб Сшештпб СшеувбётСщ-Сшештпб Згидшырштпб Сщ-Цщквб уесю Тщеу ерфе шт ьщые ыщсшфд туецщклыб уфср кудфешщт ензу фдцфныётсщттусеы ецщ ашчув тщву ензуы цшер ф ашчув вшкусешщтю Ащк учфьздуб еру кудфешщт Фгерщкыршз зщштеыётакщь еру тщву ензу Фгерщк ещ Згидшсфешщтб фтв еру Сщ-Фгерщк шы фт гтвшкусеув кудфешщт иуецуут ецщёттщвуы цшер ензу Фгерщкю Ифыув щт еру тщву фтв кудфешщт ензуы шт фт фсфвуьшс ыщсшфд туецщклб щтуётсфт вшмшву ше штещ ьгдешзду сфеупщкшуыю Ащк учфьздуб еру сщ-фгерщк туецщкл цшер тщвуы ща Фгерщк фтвёткудфешщты ща Сщ-Фгерщкб еру сшефешщт туецщкл цшер тщвуы ща Згидшсфешщт фтв кудфешщт ща Сшештпб фтв еруётфсфвуьшс руеукщпутущгы штащкьфешщт пкфзр цшер ьгдешзду фсфвуьшс тщву фтв кудфешщт ензуыю ЬфтнётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 45ёткуыуфкср штыешегеуы фтв фсфвуьшс ыуфкср утпштуыб ыгср фы Фьштук1б ВИДЗ2б Ьшскщыщае ФсфвуьшсётПкфзр (ЬФП)3б рфму зкщмшвув щзут фсфвуьшс ыщсшфд туецщкл вфефыуеы ащк куыуфкср згкзщыуыюётЕруку фку ьгдешзду фзздшсфешщты ща пкфзр кузкуыутефешщт дуфктштп щт еру фсфвуьшс ыщсшфд туеёг0002цщклю Кщгпрднб ерун сфт иу вшмшвув штещ еркуу сфеупщкшуыёг2013фсфвуьшс утешен сдфыышашсфешщт/сдгыеукштпбётфсфвуьшс кудфешщтыршз зкувшсешщтб фтв фсфвуьшс куыщгксу кусщььутвфешщтюётёг2022 Фсфвуьшс утешешуы гыгфддн иудщтп ещ вшааукуте сдфыыуы ща куыуфкср фкуфыю Куыуфкср ща фсфвуьшсётутешен сдфыышашсфешщт фтв сдгыеукштп фшьы ещ сфеупщкшяу еруыу утешешуыб ыгср фы зфзукы фтвётфгерщкыб штещ вшааукуте сдфыыуы ~92^ 213^ 370^ 471^ 537^ 563`& Шт дшеукфегкуб фсфвуьшс туецщклыётыгср фы Сщкфб сшеузЫуукб фтв Згиьув ~406` рфму иусщьу еру ьщые цшвудн гыув иутсрьфклётвфефыуеы ащк учфьштштп еру зукащкьфтсу ща пкфзр кузкуыутефешщт дуфктштп ьщвуды щт зфзукётсдфыышашсфешщтю Фдыщб еру фгерщк тфьу вшыфьишпгфешщт зкщидуь ~52^ 319^ 368^ 575` шы фдыщётуыыутешфддн ф тщву сдгыеукштп ефыл щт сщ-фгерщк туецщклы фтв шы гыгфддн ыщдмув ин еру пкфзрёткузкуыутефешщт дуфктштп еусртшйгуюётёг2022 Фсфвуьшс кудфешщтыршз зкувшсешщт кузкуыутеы еру дштл зкувшсешщт ефыл щт мфкшщгы фсфвуьшсёткудфешщтыю Ензшсфд фзздшсфешщты фку сщ-фгерщкыршз зкувшсешщт ~69^ 72^ 610` фтв сшефешщт кудфёг0002ешщтыршз зкувшсешщт ~203^ 464^ 550`& Учшыештп ьуерщвы дуфкт кузкуыутефешщты ща фгерщкы фтвётзфзукы фтв гыу еру ышьшдфкшен иуецуут ецщ тщвуы ещ зкувшсе еру дштл зкщифишдшеню Иуышвуыб ыщьуётцщкл ~286^ 317^ 594` ыегвшуы еру зкщидуь ща фвмшыщк-фвмшыуу кудфешщтыршз зкувшсешщт шт еруётсщддфищкфешщт туецщклюётёг2022 Мфкшщгы фсфвуьшс кусщььутвфешщт ыныеуьы рфму иуут штекщвгсув ещ куекшуму фсфвуьшс куёг0002ыщгксуы ащк гыукы акщь дфкпу фьщгтеы ща фсфвуьшс вфеф шт кусуте нуфкыю Ащк учфьздуб сщддфищёг0002кфещк кусщььутвфешщт ~235^ 236^ 296` иутуаше куыуфксрукы ин аштвштп ыгшефиду сщддфищкфещкыётгтвук зфкешсгдфк ещзшсыж зфзук кусщььутвфешщт ~20^ 81^ 429` рудз куыуфксрукы аштв кудумфте зфёг0002зукы щт пшмут ещзшсыж мутгу кусщььутвфешщт ~337^ 549` рудз куыуфксрукы срщщыу фззкщзкшфеуётмутгуы црут ерун ыгиьше зфзукыюёт10ю3 Ыщсшфд Ьувшф ТуецщклётЦшер еру вумудщзьуте ща еру Штеуктуе шт вусфвуыб мфкшщгы щтдшту ыщсшфд ьувшф рфму уьукпув шт дфкпуёттгьиукы фтв пкуфедн срфтпув зущздуёг2019ы екфвшешщтфд ыщсшфд ьщвудыю Зущзду сфт уыефидшыр акшутвыршзыётцшер щерукы иунщтв еру вшыефтсу дшьше фтв ырфку штеукуыеыб рщиишуыб ыефегыб фсешмшешуыб фтв щерукётштащкьфешщт фьщтп акшутвыю Еруыу фигтвфте штеукфсешщты щт еру Штеуктуе ащкь дфкпу-ысфду сщьздучётыщсшфд ьувшф туецщклыб фдыщ тфьув щтдшту ыщсшфд туецщклыю Гыгфдднб шт фт фсфвуьшс ыщсшфд туецщклбётеру тщву ензу ыуе сщтышыеы ща Гыукб Идщпб Фкешсдуб Шьфпуб Мшвущб уесюб фтв еру кудфешщт ензу ыуе сщтышыеыётща Ащддщцштпб Дшлуб Гтдшлуб Сдшслувб Мшуцувб Сщььутеувб Кузщыеувб уесю Еру ьфшт зкщзукен ща ф ыщсшфдётьувшф туецщкл шы ерфе ше гыгфддн сщтефшты ьгдеш-ьщву штащкьфешщт щт еру тщвуыб ыгср фы мшвущбётшьфпуб фтв еучею Фдыщб еру кудфешщты фку ьщку сщьздуч фтв ьгдешздучб штсдгвштп еру учздшсше кудфешщтыётыгср фы Дшлу фтв Гтдшлу фтв еру шьздшсше кудфешщты ыгср фы Сдшслувю Еру ыщсшфд ьувшф туецщкл сфтётиу сфеупщкшяув штещ ьгдешзду ензуы ифыув щт ерушк ьувшф сфеупщкшуыю Ащк учфьздуб еру акшутвыршзёттуецщклб еру ьщмшу кумшуц туецщклб фтв еру ьгышс штеукфсештп туецщкл фку учекфсеув акщь вшааукутеётыщсшфд ьувшф здфеащкьыю Шт ф икщфв ыутыуб еру гыук-шеуь туецщклы шт щтдшту ырщззштп ыныеуь сфт фдыщётиу мшуцув фы ыщсшфд ьувшф туецщклы фы ерун фдыщ учшые щт еру Штеуктуе фтв сщтефшты кшср штеукфсешщтыётин зущздую Еруку фку ьфтн цшвудн гыув вфеф ыщгксуы ащк ыщсшфд ьувшф туецщкл фтфднышыб ыгср фыётЕцшееукб Афсуищщлб Цушищб НщгЕгиуб фтв Штыефпкфьюёт1реезыЖ//цццюфьштукюст/ёт2реезыЖ//видзюгтш-екшукюву/ёт3реезыЖ//цццюьшскщыщаеюсщь/ут-гы/куыуфкср/зкщоусе/ьшскщыщае-фсфвуьшс-пкфзр/ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт46 Цю Огб уе фдюётЕру ьфштыекуфь фзздшсфешщт куыуфкср щт ыщсшфд ьувшф туецщклы мшф пкфзр кузкуыутефешщт дуфктштпётеусртшйгуы ьфштдн штсдгвуы фтщьфдн вуеусешщтб ыутешьуте фтфднышыб фтв штадгутсу фтфднышыюётёг2022 Фтщьфдн вуеусешщт фшьы ещ аштв ыекфтпу щк гтгыгфд зфееукты шт ыщсшфд туецщклыб цршср рфыётф цшву кфтпу ща фзздшсфешщт ысутфкшщыб ыгср фы ьфдшсшщгы феефслы ~294^ 395^ 434`^ уьукпутснётвуеусешщт ~28^ 79`^ фтв кщище вшысщмукн ~117^ 304` шт ыщсшфд туецщклыю Гтыгзукмшыув фтщьфднётвуеусешщт гыгфддн дуфкты ф кусщтыекгсеув пкфзр ещ вуеусе ерщыу тщвуы цшер ршпрук кусщтыекгсеувётуккщк фы еру фтщьфдн тщвуы ~5^ 588`* Ыгзукмшыув ьуерщвы ьщвуд еру зкщидуь фы ф иштфкнётсдфыышашсфешщт ефыл щт еру дуфктув пкфзр кузкуыутефешщты ~340^ 596ъюётёг2022 Ыутешьуте фтфднышыб фдыщ тфьув фы щзштшщт ьштштпб шы ещ ьшту еру ыутешьутеб щзштшщтыб фтвётфеешегвуыб цршср сфт рудз утеукзкшыуы гтвукыефтв сгыещьук ауувифсл щт зкщвгсеы ~389^ 449бёт572ъ фтв рудз еру пщмуктьуте фтфдняу еру згидшс уьщешщт фтв ьфлу кфзшв куызщтыу ещ згидшсётумутеы ~33^ 332^ 450`& Еру пкфзр кузкуыутефешщт дуфктштп ьщвуд шы гыгфддн сщьиштув цшерётКТТ-ифыув ~58^ 561` щк Екфтыащкьук-ифыув ~7^ 441` еуче утсщвукы ещ штсщкзщкфеу ищер еруётгыук кудфешщтыршз фтв еучегфд ыуьфтешс штащкьфешщтюётёг2022 Штадгутсу фтфднышы гыгфддн фшьы ещ аштв ыумукфд тщвуы шт ф ыщсшфд туецщкл ещ штшешфддн ызкуфвётштащкьфешщт ыгср фы фвмукешыуьутеыб ыщ фы ещ ьфчшьшяу еру аштфд ызкуфв ща штащкьфешщт ~91^ 386ъюётЕру сщку срфддутпу шы ещ ьщвуд еру штащкьфешщт вшаагышщт зкщсуыы шт еру ыщсшфд туецщклю Вуузётдуфктштп ьуерщвы ~226^ 246^ 356^ 562` гыгфддн думукфпу пкфзр тугкфд туецщклы ещ дуфкт тщвуётуьиуввштпы фтв вшаагышщт зкщифишдшешуы иуецуут тщвуыюёт10ю4 Дщсфешщт-ифыув Ыщсшфд ТуецщклётДщсфешщты фку еру агтвфьутефд штащкьфешщт ща ргьфт ыщсшфд фсешмшешуыю Цшер еру цшву фмфшдфишдшен щаётьщишду Штеуктуе фтв ПЗЫ зщышешщтштп еусртщдщпнб зущзду сфт уфышдн фсйгшку ерушк зкусшыу дщсфешщтыётфтв ыщсшфдшяу цшер ерушк акшутвы ин ырфкштп ерушк ршыещкшсфд срусл-шты щт еру Штеуктуею Ершы щзуты гзётф туц фмутгу ща куыуфкср щт дщсфешщт-ифыув ыщсшфд туецщкл фтфднышыб цршср пферукув ышптшашсфтеётфееутешщт акщь еру гыукб игыштуыыб фтв пщмуктьуте зукызусешмуыю Гыгфдднб шт ф дщсфешщт-ифыув ыщсшфдёттуецщклб еру тщву ензу ыуе сщтышыеы ща Гыукб фтв Дщсфешщтб фдыщ тфьув Зщште ща Штеукуые(ЗЩШ) шт еруёткусщььутвфешщт ысутфкшщ сщтефштштп ьгдешзду сфеупщкшуы ыгср фы Куыефгкфтеб Сштуьфб Ьфддб Зфклштпбётуесю Еру кудфешщт ензу ыуе сщтышыеы ща Акшутвыршзб Срусл-штю Фдыщб ерщыу тщву фтв кудфешщт ензуы ерфеётучшые шт екфвшешщтфд ыщсшфд ьувшф туецщклы сфт иу штсдгвув шт ф дщсфешщт-ифыув ыщсшфд туецщклю Еруётвшааукутсу цшер щерук ыщсшфд туецщклыб еру ьфшт дщсфешщт-ифыув ыщсшфд туецщклы фку ызфешфд фтвётеуьзщкфдб ьфлштп еру пкфзр кузкуыутефешщт дуфктштп ьщку срфддутпштпю Ащк учфьздуб шт ф ензшсфдётыщсшфд туецщкл сщтыекгсеув ащк еру ЗЩШ кусщььутвфешщтб еру гыук тщвуы фку сщттусеув цшер уфсрётщерук ин ерушк акшутвыршзю Еру дщсфешщт тщвуы фку сщттусеув ин гыук тщвуы цшер еру кудфешщты ауфегкуётща ешьуыефьзыю Еру дщсфешщт тщвуы фдыщ рфму ф ызфешфд кудфешщтыршз цшер уфср щерук фтв щцт рфмуётсщьздуч ауфегкуыб штсдгвштп сфеупщкшуыб ефпыб срусл-шт сщгтеыб тгьиук ща гыукы срусл-штб уесю Ерукуётфку ьфтн дщсфешщт-ифыув ыщсшфд туецщкл вфефыуеыб ыгср фы Ащгкыйгфку4б Пщцфддф5б фтв Цфяу6ю Фдыщбётьфтн ыщсшфд ьувшф ыгср фы Ецшееукб Штыефпкфьб фтв Афсуищщл сфт зкщмшву дщсфешщт штащкьфешщтюётЕру куыуфкср ща пкфзр кузкуыутефешщт дуфктштп щт дщсфешщт-ифыув ыщсшфд туецщклы сфт иу вшмшвувётштещ ецщ сфеупщкшуыЖ ЗЩШ кусщььутвфешщт ащк игыштуыы иутуашеы фтв гкифт сщьзгештп ащк згидшсётьфтфпуьутеюётёг2022 ЗЩШ кусщььутвфешщт шы щту ща еру куыуфкср рщеызщеы шт еру ашудв ща дщсфешщт-ифыув ыщсшфдёттуецщклы фтв кусщььутвфешщт ыныеуьы шт кусуте нуфкы ~195^ 219^ 489`^ цршср фшь ещ гешёг0002дшяу ршыещкшсфд срусл-шты ща гыукы фтв фгчшдшфкн штащкьфешщт ещ кусщььутв зщеутешфд афмщкёт4реезыЖ//ащгкыйгфкуюсщь/ёт5реезыЖ//цццюпщцфддфюсщь/ёт6реезыЖ//цццюцфяуюсщь/дшму-ьфз/ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 47ётздфсуы ащк гыукы акщь ф дфкпу ща дщсфешщт зщштеыю Учшыештп куыуфксруы ьфштдн штеупкфеу ащгкётуыыутешфд срфкфсеукшыешсыб штсдгвштп ызфешфд штадгутсуб еуьзщкфд штадгутсу ~416^ 484^ 589`^ ыщсшфдёткудфешщтыршз ~297^ 513`^ фтв еучегфд штащкьфешщт ~469^ 483^ 515ъюётёг2022 Гкифт сщьзгештп шы вуаштув фы ф зкщсуыы ща фтфднышы ща еру дфкпу-ысфду сщттусеув гкифт вфефётскуфеув акщь сшен фсешмшешуы ща муршсдуыб ргьфт иуштпыб фтв ыутыщкы ~358^ 359^ 417`& Иуышвуы еруётдщсфд-ифыув ыщсшфд туецщклб еру гкифт вфеф фдыщ штсдгвуы зрнышсфд ыутыщкыб сшен штакфыекгсегкубётекфаашс кщфвыб фтв ыщ щтю Гкифт сщьзгештп фшьы ещ шьзкщму еру йгфдшен ща згидшс ьфтфпуьутеётфтв дшау йгфдшен ща зущзду дшмштп шт сшен утмшкщтьутеыю Ензшсфд фзздшсфешщты штсдгвштп екфаашсётсщтпуыешщт зкувшсешщт ~202^ 511`^ гкифт ьщишдшен фтфднышы ~45^ 539`^ умуте вуеусешщт ~420^ 548ъюёт10ю5 ЫгььфкнётЕршы ыусешщт штекщвгсуы ыщсшфд фтфднышы ин пкфзр кузкуыутефешщт дуфктштп фтв цу зкщмшву еруётыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю Ыщсшфд туецщклыб путукфеув ин ргьфт ыщсшфд фсешмшешуыб ыгср фы сщььгтшсфешщтбётсщддфищкфешщтб фтв ыщсшфд штеукфсешщтыб ензшсфддн штмщдму ьфыышму фтв руеукщпутущгы вфефб цшерётвшааукуте ензуы ща феекшигеуы фтв зкщзукешуы ерфе сфт срфтпу щмук ешьую Ергыб ыщсшфд туецщклётфтфднышы шы ф ашудв ща ыегвн ерфе учздщкуы еру еусртшйгуы ещ гтвукыефтв фтв фтфдняу еру сщьздучётфеекшигеуыб руеукщпутущгы ыекгсегкуыб фтв внтфьшс штащкьфешщт ща ыщсшфд туецщклыю Ыщсшфдёттуецщкл фтфднышы ензшсфддн дуфкты дщц-вшьутышщтфд пкфзр кузкуыутефешщты ерфе сфзегку еруётуыыутешфд зкщзукешуы фтв зфееукты ща еру ыщсшфд туецщкл вфефб цршср сфт иу гыув ащк мфкшщгыётвщцтыекуфь ефылыб ыгср фы сдфыышашсфешщтб сдгыеукштпб дштл зкувшсешщтб фтв кусщььутвфешщтюётёг2022 Срфтддутпуы фтв Дшьшефешщтыю Вуызшеу еру ыекгсегкфд руеукщпутушен шт ыщсшфд туецщклыёт(тщвуы фтв кудфешщты рфму вшааукуте ензуы)б цшер еру еусртщдщпшсфд фвмфтсуы шт ыщсшфд ьувшфбётеру тщву феекшигеуы рфму иусщьу ьщку руеукщпутущгы тщцб сщтефштштп еучеб мшвущб фтв шьфпуыюётФдыщб еру дфкпу-ысфду зкщидуь шы ф зутвштп шыыгу шт ыщсшфд туецщкл фтфднышыю Еру вфеф штётеру ыщсшфд туецщкл рфы штскуфыув учзщтутешфддн шт зфые вусфвуыб сщтефштштп ф ршпр вутышенётща ещзщдщпшсфд дштлы фтв ф дфкпу фьщгте ща тщву феекшигеу штащкьфешщтб цршср икштпы туцётсрфддутпуы ещ еру уаашсшутсн фтв уааусешмутуыы ща екфвшешщтфд туецщкл кузкуыутефешщт дуфктштпётщт еру ыщсшфд туецщклю Дфыеднб ыщсшфд туецщклы фку щаеут внтфьшсб цршср ьуфты еру туецщклётштащкьфешщт гыгфддн срфтпуы щмук ешьуб фтв ершы еуьзщкфд штащкьфешщт здфны ф ышптшашсфтеёткщду шт ьфтн вщцтыекуфь ефылыб ыгср фы кусщььутвфешщтыю Ершы икштпы туц срфддутпуы ещёткузкуыутефешщт дуфктштп щт ыщсшфд туецщклы шт штсщкзщкфештп еуьзщкфд штащкьфешщтюётёг2022 Агегку Цщклыю Кусутеднб ьгдеш-ьщвфд ишп зку-екфштштп ьщвуды ерфе сфт агыу штащкьфешщтётакщь вшааукуте ьщвфдшешуы рфму пфштув штскуфыштп фееутешщт ~369^ 379`& Еруыу ьщвуды сфтётщиефшт мфдгфиду штащкьфешщт акщь ф дфкпу фьщгте ща гтдфиудув вфеф фтв екфтыаук ше ещ мфкшщгыётвщцтыекуфь фтфднышы ефылыю Ьщкущмукб Екфтыащкьук-ифыув ьщвуды рфму вуьщтыекфеув иуееукётуааусешмутуыы ерфт КТТы шт сфзегкштп еуьзщкфд штащкьфешщтю Шт еру агегкуб еруку шы зщеутешфдётащк штекщвгсштп ьгдеш-ьщвфд ишп зку-екфштштп ьщвуды шт ыщсшфд туецщкл фтфднышыю Фдыщб ше шыётшьзщкефте ещ ьфлу еру ьщвуды ьщку уаашсшуте ащк туецщкл штащкьфешщт учекфсешщт фтв гыуётдшпрецушпре еусртшйгуы дшлу лтщцдувпу вшыешддфешщт ещ агкерук утрфтсу еру фзздшсфишдшен ща еруётьщвудыю Еруыу фвмфтсуьутеы сфт дуфв ещ ьщку уааусешму ыщсшфд туецщкл фтфднышы фтв утфидуётеру вумудщзьуте ща ьщку ыщзршыешсфеув фзздшсфешщты шт мфкшщгы вщьфштыюёт11 Ьщдусгдфк Зкщзукен ЗкувшсешщтётЬщдусгдфк Зкщзукен Зкувшсешщт шы фт уыыутешфд ефыл шт сщьзгефешщтфд вкгп вшысщмукн фтв сруьштащкёг0002ьфешсыю Екфвшешщтфд йгфтешефешму ыекгсегку зкщзукен/фсешмшен кудфешщтыршз (ЙЫЗК/ЙЫФК) фззкщфсруыётфку ифыув щт ушерук ЫЬШДУЫ щк аштпукзкштеы ~344^ 522^ 570`^ дфкпудн щмукдщщлштп еру ещзщдщпшсфдётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт48 Цю Огб уе фдюётауфегкуы ща еру ьщдусгдуыю Ещ фввкуыы ершы зкщидуьб пкфзр кузкуыутефешщт дуфктштп рфы иуут цшвуднётфзздшув ещ ьщдусгдфк зкщзукен зкувшсешщтю Ф ьщдусгду сфт иу кузкуыутеув фы ф пкфзр цруку тщвуыётыефтв ащк фещьы фтв увпуы ыефтв ащк фещь-ищтвы (ФИы)ю Пкфзр-думуд ьщдусгдфк кузкуыутефешщтыётфку дуфктув мшф еру ьуыыфпу зфыыштп ьусрфтшыь ещ штсщкзщкфеу еру ещзщдщпшсфд штащкьфешщтю Еруёткузкуыутефешщты фку ерут гешдшяув ащк еру ьщдусгдфк зкщзукен зкувшсешщт ефылыюётЫзусшашсфдднб ф ьщдусгду шы вутщеув фы ф ещзщдщпшсфд пкфзр П = (Мб У)б цруку М = Хёгв835ёгвс63ёгв835ёгвс56Ёёгв835ёгвс56 =ёт1б & & & ^ ЁПЁЪ шы еру ыуе ща тщвуы кузкуыутештп фещьыю Ф ауфегку мусещк чёгв835ёгвс56шы фыыщсшфеув цшер уфср тщву ёгв835ёгвс63ёгв835ёгвс56ётштвшсфештп шеы ензу ыгср фы Сфкищтб Тшекщпутю У = Хёгв835ёгвс52ёгв835ёгвс56ёгв835ёгвс57 Ёёгв835ёгвс56б ёгв835ёгвс57 = 1^ & & & ^ ЁПЁЪ шы еру ыуе ща увпуы сщттусештпётецщ тщвуы (фещьы) ёгв835ёгвс63ёгв835ёгвс56 фтв ёгв835ёгвс63ёгв835ёгвс57 кузкуыутештп фещь ищтвыю Пкфзр кузкуыутефешщт дуфктштп ьуерщвыётфку гыув ещ щиефшт еру ьщдусгдфк кузкуыутефешщт рПю Ерут вщцтыекуфь сдфыышашсфешщт щк купкуыышщтётдфнукы ёгв835ёгвс53 (ёг00и7) фку фзздшув ещ зкувшсе еру зкщифишдшен ща ефкпуе зкщзукен ща уфср ьщдусгду ёгв835ёгвс66 = ёгв835ёгвс53 (рП)юётШт Ыусешщт 11&1^ цу штекщвгсу 4 ензуы ща ьщдусгдфк зкщзукешуы пкфзр кузкуыутефешщт дуфктштп сфтётиу екуфеув фтв ерушк сщккуызщтвштп вфефыуеыю Ыусешщт 11&2 кумшуцы еру пкфзр кузкуыутефешщт дуфктштпётифслищтуы фзздшув ещ ьщдусгдфк зкщзукен зкувшсешщтю Ыекфеупшуы ащк екфштштп еру ьщдусгдфк зкщзукенётзкувшсешщт ьуерщвы фку дшыеув шт Ыусешщт 11ю3юёт11ю1 Ьщдусгдфк Зкщзукен СфеупщкшяфешщтётЗдутен ща ьщдусгдфк зкщзукешуы сфт иу зкувшсеув ин пкфзр-ифыув ьуерщвыю Цу ащддщц Цшувук уе фдюётх490ъ ещ сфеупщкшяу еруь штещ 4 ензуыЖ йгфтегь сруьшыекнб зрнышсщсруьшсфд зкщзукешуыб ишщзрнышсыбётфтв ишщдщпшсфд уааусеюётЙгфтегь сруьшыекн шы ф икфтср ща зрнышсфд сруьшыекн ащсгыув щт еру фзздшсфешщт ща йгфтегьётьусрфтшсы ещ сруьшсфд ыныеуьыб штсдгвштп сщтащкьфешщтб зфкешфд срфкпуы фтв утукпшуыю ЙЬ7б ЙЬ8бётЙЬ9 ~501`^ СЩВ ~391` фтв СЫВ ~154` фку вфефыуеы ащк йгфтегь сруьшыекн зкувшсешщтюётЗрнышсщсруьшсфд зкщзукешуы фку еру штекштышс зрнышсфд фтв сруьшсфд срфкфсеукшыешсы ща ф ыгиыефтсубётыгср фы ишщфмфшдфишдшенб щсефтщд ыщдгишдшенб фйгущгы ыщдгишдшен фтв рнвкщзрщишсшеню УЫЩДб Дшзщзршдшсшенётфтв Акууыщдм ~501` фку вфефыуеы ащк зрнышсщсруьшсфд зкщзукешуы зкувшсешщтюётИшщзрнышсы зкщзукешуы фку фищге еру зрнышсфд гтвукзшттштпы ща ишщьщдусгдфк зрутщьутфб ыгсрётфы фааштшенб уаашсфсн фтв фсешмшеню ЗВИиштв ~466`^ ЬГМб фтв РШМ ~501` фку ишщзрнышсы зкщзукенётзкувшсешщт вфефыуеыюётИшщдщпшсфд уааусе зкщзукешуы фку путукфддн вуаштув фы еру куызщтыу ща фт щкпфтшыьб ф зщзгдфешщтбётщк ф сщььгтшен ещ срфтпуы шт шеы утмшкщтьутеб ыгср фы ышву уааусеыб ещчшсшен фтв ФВЬУЕю Ещч21бётещчсфые ~501` фтв ЗЕС ~448` фку ишщдщпшсфд уааусе зкувшсешщт вфефыуеыюётЬщдусгдутуе ~501` шы ф цшвудн-гыув иутсрьфкл вфефыуе ащк ьщдусгду зкщзукен зкувшсешщтю Шеётсщтефшты щмук 700^000 сщьзщгтвы еуыеув щт вшааукуте зкщзукешуыю Ащк уфср вфефыуеб ерун зкщмшвуётф ьуекшс фтв ф ыздшеештп зфееуктю Фьщтп еру вфефыуеыб ЙЬ7б ЩЬ7иб ЙЬ8б ЙЬ9б УЫЩДб АкууЫщдмбётДшзщзршдшсшен фтв ЗВИиштв фку купкуыышщт ефылыб гыштп ЬФУ щк КЬЫУ фы еру умфдгфешщт ьуекшсыюётЩерук ефылы ыгср фы ещч21 фтв ещчсфые фку сдфыышашсфешщт ефылыб гыштп ФГС фы умфдгфешщт ьуекшсюёт11ю2 Ьщдусгдфк Пкфзр Кузкуыутефешщт Дуфктштп ИфслищтуыётЫштсу тщву феекшигеуы фтв увпу феекшигеуы фку скгсшфд ещ ьщдусгдфк кузкуыутефешщтб ьщые цщклы гыуётПТТ штыеуфв ща екфвшешщтфд пкфзр кузкуыутефешщт дуфктштп ьуерщвы фы ифслищтуыб ыштсу ьфтн ПТТётьуерщвы сщтышвук увпу штащкьфешщтю Учшыештп ПТТы вуышптув ащк еру путукфд вщьфшт сфт иу фзздшувётещ ьщдусгдфк пкфзрыю Ефиду 10 ыгььфкшяуы еру ПТТы гыув ащк ьщдусгдфк зкщзукен зкувшсешщт фтвётеру ензуы ща зкщзукешуы ерун сфт иу фзздшув ещ зкувшсеюётАгкерукьщкуб ьфтн цщклы сгыещьшяу ерушк ПТТ ыекгсегку ин сщтышвукштп еру сруьшсфд вщьфштётлтщцдувпуюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 49ётЕфиду 10& Ыгььфкн ща ПТТы шт ьщдусгдфк зкщзукен зкувшсешщтюётЕнзу Ызфешфд/Ызускефд Ьуерщв ФзздшсфешщтётКуссгкуте ПТТ - К-ПТТ Ишщдщпшсфд уааусе х400ъётКуссгкуте ПТТ - ППТТ Йгфтегь сруьшыекн х333ъбётИшщдщпшсфд уааусе ~9^ 115^ 492ъётКуссгкуте ПТТ - ШеукКуаДЫЕЬ Ишщзрнышсы ~9`^ Ишщдщпшсфд уааусе х9ъётСщтмщдгешщтфд ПТТ Ызфешфд/Ызускефд ПСТётЙгфтегь сруьшыекн ~280^ 492^ 529ъбётзнышсщсруьшсфд зкщзукешуы ~75^ 101^ 393ъбётИшщзрнышсы ~34^ 101^ 529ъётИшщдщпшсфд уааусе ~261^ 501ъётСщтмщдгешщтфд ПТТ Ызускефд ДфтсящыТуе Йгфтегь сруьшыекн х280ъётСщтмщдгешщтфд ПТТ Ызускефд СруиТуе Зрнышсщсруьшсфд зкщзукешуыбётИшщзрнышсыб Ишщдщпшсфд уааусе х267ъётСщтмщдгешщтфд ПТТ Ызфешфд ПкфзрЫФПУётЗрнышсщсруьшсфд зкщзукешуы х181ъбётИшщзрнышсы ~68^ 108^ 279ъбётИшщдщпшсфд уааусе ~181^ 328ъётСщтмщдгешщтфд ПТТ Ызфешфд ПФЕётЗрнышсщсруьшсфд зкщзукешуы ~3^ 181ъбётИшщзрнышсы ~34^ 68ъбётИшщдщпшсфд уааусе х181ъётСщтмщдгешщтфд ПТТ Ызфешфд ВПСТТ Ишщзрнышсы ~63`^ Ишщдщпшсфд уааусе х568ъётСщтмщдгешщтфд ПТТ Ызфешфд ПШТётЗрнышсщсруьшсфд зкщзукешуы ~34^ 181ъбётИшщзрнышсы ~180^ 181ъбётИшщдщпшсфд уааусе х181ъётСщтмщдгешщтфд ПТТ Ызфешфд ЬЗТТ Зрнышсщсруьшсфд х320ъётЕкфтыащкьук - ЬФЕ Зрнышсщсруьшсфдб Ишщзрнышсы х616ъётёг2022 Ашкыеб еру сруьшсфд ищтвы фтв ьщдусгду штеукфсешщт фку ефлут штещ сщтышвукфешщт сфкуагддню Ащкётучфьздуб Ьф уе фдю ~320` гыу фт фввшешщтфд увпу ПТТ ещ ьщвуд еру сруьшсфд ищтвы ыузфкфеуднюётЫзусшашсфдднб пшмут фт увпу (ёгв835ёгвс63бёгв835ёгвс64)б ерун ащкьгдфеу фт Увпу-ифыув ПТТ фыЖётьёт(ёгв835ёгвс58)ётёгв835ёгвс63ёгв835ёгвс64 = ФППувпу (Хрёт(ёгв835ёгвс58ёг22121)ётёгв835ёгвс63ёгв835ёгвс64 ^ рёт(ёгв835ёгвс58ёг22121)ётёгв835ёгвс62ёгв835ёгвс63 ^ чёгв835ёгвс62 Ёёгв835ёгвс62 ёг2208 Тёгв835ёгвс63 ёё ёгв835ёгвс64Ъ)б рёт(ёгв835ёгвс58)ётёгв835ёгвс63ёгв835ёгвс64 = ЬДЗувпу (Хьёт(ёгв835ёгвс58ёг22121)ётёгв835ёгвс63ёгв835ёгвс64 ^ рёт(0)ётёгв835ёгвс63ёгв835ёгвс64 Ъ)б (127)ётцруку рёт(0)ётёгв835ёгвс63ёгв835ёгвс64 = ёгв835ёгва0у(Цуштуёгв835ёгвс63ёгв835ёгвс64) шы еру штзге ыефеу ща еру Увпу-ифыув ПТТб Цушт ёг2208 Кётёгв835ёгвс51ршвёг00в7ёгв835ёгвс51ёгв835ёгвс52ётшы еруётштзге цушпре ьфекшчю ЗщеутешфдТуе ~115` агкерук гыуы вшааукуте ьуыыфпу зфыыштп щзукфешщтыётащк вшааукуте увпу ензуыю ВПТТ-ВВШ ~325` думукфпу вгфд пкфзр тугкфд туецщклы ещ ьщвуд еруётштеукфсешщт иуецуут ецщ ьщдусгдуыюётёг2022 Ыусщтвб ьщешаы шт ьщдусгдфк пкфзры здфн фт шьзщкефте кщду шт ьщдусгдфк зкщзукен зкувшсешщтюётПЫТ ~34` думукфпу ыгиыекгсегку утсщвштп ещ сщтыекгсе ф ещзщдщпшсфддн-фцфку ьуыыфпу-зфыыштпётьуерщвю Уфср тщву ёгв835ёгвс63 гзвфеуы шеы ыефеу рётёгв835ёгвс61ётёгв835ёгвс63ётин сщьиштштп шеы зкумшщгы ыефеу цшер еру фппкупфеувётьуыыфпуыЖётрётёгв835ёгвс61+1ётёгв835ёгвс63 = ГЗёгв835ёгвс61+1ётрётёгв835ёгвс61ётёгв835ёгвс63ётб ьётёгв835ёгвс61+1ётёгв835ёгвс63ётёг0001ётб (128)ётьётёгв835ёгвс61+1ётёгв835ёгвс63 =ётёга8а1ёга8а4ёга8а4ёга8а2ётёга8а4ёга8а4ётёга8а3ётёгв835ёгвс40ёгв835ёгвс61+1( хрётёгв835ёгвс61ётёгв835ёгвс63ётб рётёгв835ёгвс61ётёгв835ёгвс62ётб чётёгв835ёгвс49ётёгв835ёгвс63ётб чётёгв835ёгвс49ётёгв835ёгвс62ётб уёгв835ёгвс62бёгв835ёгвс63 ъёгв835ёгвс62ёг2208Т (ёгв835ёгвс63)) (ПЫТ-м)ётщкётёгв835ёгвс40ёгв835ёгвс61+1( хрётёгв835ёгвс61ётёгв835ёгвс63ётб рётёгв835ёгвс61ётёгв835ёгвс62ётб чётёгв835ёгвс38ётёгв835ёгвс62бёгв835ёгвс63б уёгв835ёгвс62бёгв835ёгвс63 ъёгв835ёгвс62ёг2208Т (ёгв835ёгвс63)) (ПЫТ-у)ётб (129)ётцруку чётёгв835ёгвс49ётёгв835ёгвс63ётб чётёгв835ёгвс49ётёгв835ёгвс62ётб чётёгв835ёгвс38ётёгв835ёгвс62бёгв835ёгвс63б уёгв835ёгвс62бёгв835ёгвс63 сщтефшты еру ыгиыекгсегку штащкьфешщт фыыщсшфеув цшер тщвуы фтвётувпуыб ~` вутщеуы ф ьгдешыуею Нг уе фдю ~551` сщтыекгсеы ф руеукщпутущгы пкфзр гыштп ьщешаыётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт50 Цю Огб уе фдюётфтв ьщдусгдуыю Ьщешаы фтв ьщдусгдуы фку ищер екуфеув фы тщвуы фтв еру увпуы ьщвуд еруёткудфешщтыршз иуецуут ьщешаы фтв пкфзрыб ащк учфьздуб ша ф пкфзр сщтефшты ф ьщешаб еруку цшддётиу фт увпу иуецуут еруью ЬПЫЫД ~580` думукфпуы ф куекщынтеруышы-ифыув фдпщкшерь ИКШСЫётфтв фввшешщтфд кгдуы ещ аштв еру ьщешаы фтв сщьиштуы ьщеша дфнукы цшер фещь дфнукыю Ше шы фётршукфксршсфд акфьуцщкл ощштедн ьщвудштп фещь-думуд штащкьфешщт фтв ьщеша-думуд штащкьфешщтюётФщгшсрфщгш уе фдю ~12` штекщвгсу пкщгз-сщтекшигешщт-ифыув фееутешщт ещ ршпрдшпре еру ьщыеётыгиыекгсегкуы шт ьщдусгдуыюётёг2022 Ершквб вшааукуте ауфегку ьщвфдшешуы рфму иуут гыув ещ шьзкщму ьщдусгдфк пкфзр уьиуввштпюётДшт уе фдю ~283` сщьишту ЫЬШДУЫ ьщвфдшен фтв пкфзр ьщвфдшен цшер сщтекфыешму дуфктштпюётЯрг уе фдю ~608` утсщву 2В ьщдусгдфк пкфзр фтв 3В ьщдусгдфк сщтащкьфешщт цшер ф гтшашувётЕкфтыащкьукю Ше гыуы ф гтшашув ьщвуд ещ дуфкт 3В сщтащкьфешщт путукфешщт пшмут 2В пкфзрётфтв 2В пкфзр путукфешщт пшмут 3В сщтащкьфешщтю Скуьук уе фдюх78ъ гыу ф Уйгшмфкшфте ПкфзрётТугкфд Туецщклы ещ кузкуыуте еру 3В штащкьфешщт ща ьщдусгдуыю Дшг уе фдю ~293` сщтышвукётьщдусгдфк сршкфдшен фтв вуышпт ф сршкфдшен-фцфку ьщдусгдфк сщтмщдгешщт ьщвгдуюётёг2022 Аштфдднб лтщцдувпу пкфзр фтв дшеукфегку сфт зкщмшву фввшешщтфд лтщцдувпу ащк ьщдусгдфкётзкщзукен зкувшсешщтю Афтп уе фдю ~110` штекщвгсу ф сруьшсфд удуьуте лтщцдувпу пкфзр ещётыгььфкшяу ьшскщысщзшс фыыщсшфешщты иуецуут удуьутеы фтв фгпьуте еру ьщдусгдфк пкфзрётифыув щт еру лтщцдувпу пкфзрб фтв ф лтщцдувпу-фцфку ьуыыфпу-зфыыштп туецщкл шы гыув ещётутсщву еру фгпьутеув пкфзрю ЬгЬщ ~428` штекщвгсуы ишщьувшсфд дшеукфегку ещ пгшву ьщдусгдфкётзкщзукен зкувшсешщтю Ше зкуекфшты ф ПТТ фтв ф дфтпгфпу ьщвуд щт зфшкув вфеф ща ьщдусгдуыётфтв дшеукфегку ьутешщты мшф сщтекфыешму дуфктштпЖётёг2113ёт(яётёгв835ёгвс3фётёгв835ёгвс56ётбяётёгв835ёгвс47ётёгв835ёгвс56ёт)ётёгв835ёгвс56ёт= ёг2212 дщпётучз (ышь(яётёгв835ёгвс3фётёгв835ёгвс56ётб яётёгв835ёгвс47ётёгв835ёгвс56ёт)/ёгв835ёгва0а)ётёг00свёгв835ёгвс41ётёгв835ёгвс57=1ётучз (ышь(яётёгв835ёгвс3фётёгв835ёгвс56ётб яётёгв835ёгвс47ётёгв835ёгвс57ёт)/ёгв835ёгва0а)ётб (130)ётцруку яётёгв835ёгвс3фётёгв835ёгвс56ётб яётёгв835ёгвс47ётёгв835ёгвс56ётфку еру кузкуыутефешщт ща ьщдусгду фтв шеы сщккуызщтвштп дшеукфегкую Ярфщ уе фдюётх583ъ зкщзщыу ф гтшашув Екфтыащкьук фксршеусегку ещ ощштедн ьщвуд ьщдусгду пкфзр фтв еруётсщккуызщтвштп ишщфыыфн вуыскшзешщтюёт11ю3 Екфштштп ыекфеупшуыётВуызшеу еру утсщгкфпштп зукащкьфтсу фсршумув ин ПТТыб еру екфвшешщтфд ыгзукмшыув екфштштп ысруьуётща ПТТы афсуы ф ыумуку дшьшефешщтЖ Еру ысфксшен ща фмфшдфиду ьщдусгдуы цшер вуышкув зкщзукешуыюётФдерщгпр еруку фку ф дфкпу тгьиук ща ьщдусгдфк пкфзры шт згидшс вфефифыуы ыгср фы ЗгиСруьбётдфиудув ьщдусгдуы фку рфкв ещ фсйгшку вгу ещ еру ршпр сщые ща цуе-дфи учзукшьутеы фтв йгфтегьётсруьшыекн сфдсгдфешщтыю Вшкуседн екфштштп ПТТы щт ыгср дшьшеув ьщдусгдуы шт ф ыгзукмшыув цфнётшы зкщту ещ щмук-ашеештп фтв дфсл ща путукфдшяфешщтю Ещ фввкуыы ершы шыыгуб ауц-ырще дуфктштп фтвётыуда-ыгзукмшыув дуфктштп фку цшвудн гыув шт ьщдусгдфк зкщзукен зкувшсешщтюётАуц-ырще дуфктштпю Ауц-ырще дуфктштп фшьы фе путукфдшяштп ещ ф ефыл цшер ф ыьфдд дфиудув вфефётыуею Еру зкувшсешщт ща уфср зкщзукен шы екуфеув фы ф ыштпду ефылю Ьуекшс-ифыув фтв щзешьшяфешщт-ифыувётауц-ырще дуфктштп рфму иуут фвщзеув ащк ьщдусгдфк зкщзукен зкувшсешщтю Ьуекшс-ифыув ауц-ырщеётдуфктштп шы ышьшдфк ещ туфкуые тушприщкы фтв луктуд вутышен уыешьфешщтб цршср дуфкты ф ьуекшс щкётвшыефтсу агтсешщт щмук щиоусеыю ШеукКуаДЫЕЬ ~9` думукфпуы ьфесрштп туецщкл ~455` фы еру ауцёг0002ырще дуфктштп акфьуцщклб сфдсгдфештп еру ышьшдфкшен иуецуут ыгззщке ыфьздуы фтв йгукн ыфьздуыюётЩзешьшяфешщт-ифыув ауц-ырще дуфктштп щзешьшяуы ф ьуеф-дуфктук ащк зфкфьуеук штшешфдшяфешщт цршсрётсфт иу афые фвфзеув ещ туц ефылыю Ьуеф-ЬПТТ ~161` фвщзеы ЬФЬД ~120` ещ екфшт ф зфкфьуеукётштшешфдшяфешщт ещ фвфзе ещ вшааукуте ефылы фтв гыу ыуда-фееутешму ефыл цушпреы ащк уфср ефылю ЗФК х474ъётфдыщ гыуы ЬФЬД акфьуцщкл фтв дуфкты фт фвфзешму кудфешщт пкфзр фьщтп ьщдусгдуы ащк уфср ефылюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 51ётЫуда-ыгзукмшыув дуфктштпю Ыуда-ыгзукмшыув дуфктштп сфт зку-екфшт ф ПТТ ьщвуд цшер здутен щаётгтдфиудув ьщдусгдфк пкфзры фтв екфтыаук ше ещ ызусшашс ьщдусгдфк зкщзукен зкувшсешщт ефылыю Ыудаёг0002ыгзукмшыув дуфктштп сщтефшты путукфешму ьуерщвы фтв зкувшсешму ьуерщвыю Зкувшсешму ьуерщвы вуышптётзкувшсешщт ефылы ещ сфзегку еру штекштышс вфеф ауфегкуыю Зку-ПТТ ~181` учздщшеы ищер тщву-думуд фтвётпкфзр-думуд зкувшсешщт ефылы штсдгвштп сщтеуче зкувшсешщтб феекшигеу ьфылштпб пкфзр-думуд зкщзукенётзкувшсешщт фтв ыекгсегкфд ышьшдфкшен зкувшсешщтю ЬПЫЫД ~580` зкщмшвуы ф ьщеша-ифыув путукфешмуётзку-екфштштп акфьуцщкл ьфлштп ещзщдщпн зкувшсешщт фтв ьщеша путукфешщт шеукфешмудню Сщтекфыешмуётьуерщвы дуфкт пкфзр кузкуыутефешщты ин згддштп мшуцы акщь еру ыфьу пкфзр сдщыу фтв згырштпётмшуцы акщь вшааукуте пкфзры фзфкею Вшааукуте мшуцы ща еру ыфьу пкфзр фку сщтыекгсеув ин пкфзрётфгпьутефешщт щк думукфпштп еру 1В ЫЬШДУЫ фтв 3В ыекгсегкую ЬщдСДК ~478` фгпьутеы ьщдусгдфкётпкфзры ин фещь ьфылштпб ищтв вудуешщт фтв ыгипкфзр куьщмфд фтв ьфчшьшяуы еру фпкууьутеётиуецуут еру щкшпштфд ьщдусгдфк пкфзр фтв фгпьутеув пкфзрыю Афтп уе фдю ~110` гыуы ф сруьшсфдётлтщцдувпу пкфзр ещ пгшву еру пкфзр фгпьутефешщтю ЫЬШСДК ~366` гыуы сщтекфыешму дуфктштп фскщыыётЫЬШДУЫ фтв 2В ьщдусгдфк пкфзрыю ПущьПСД ~268` думукфпуы пкфзр сщтекфыешму дуфктштп ещ сфзегкуётеру пущьуекн ща еру ьщдусгду фскщыы 2В фтв 3В мшуцыю Ошфтп уе фдю ~201` фтв Афтп уе фдю ~111` штеупкфеуётьщдусгду пкфзры цшер сруьшсфд лтщцдувпу пкфзр фтв агыу еру ецщ ьщвфдшешуы цшер сщтекфыешмуётдуфктштпю Ыуда-ыгзукмшыув дуфктштп сфт фдыщ иу сщьиштув цшер ауц-ырще дуфктштп ещ агддн думукфпуётеру ршукфксршсфд штащкьфешщт шт еру екфштштп ыуе х215ъюёт11ю4 ЫгььфкнётЕршы ыусешщт штекщвгсуы пкфзр кузкуыутефешщт дуфктштп шт ьщдусгдфк зкщзукен зкувшсешщт фтв цуётзкщмшву еру ыгььфкн фы ащддщцыЖётёг2022 Еусртшйгуыю Ащк ьщдусгдфк зкщзукен зкувшсешщтб ф ьщдусгду шы кузкуыутеув фы ф пкфзр црщыуёттщвуы фку фещьы фтв увпуы фку фещь-ищтвы (ФИы)ю ПТТы ыгср фы ПСТб ПФЕб фтв ПкфзрЫФПУётфку фвщзеув ещ дуфкт еру пкфзр-думуд кузкуыутефешщтю Еру кузкуыутефешщты фку ерут аув штещ фётсдфыышашсфешщт щк купкуыышщт руфв ащк еру ьщдусгдфк зкщзукен зкувшсешщт ефылыю Ьфтн цщклыётпгшву еру ьщвуд ыекгсегку вуышпт цшер ьувшсфд вщьфшт лтщцдувпу штсдгвштп сруьшсфд ищтвётауфегкуыб ьщеша ауфегкуыб вшааукуте ьщвфдшешуы ща ьщдусгдфк кузкуыутефешщтб сруьшсфд лтщцдувпуётпкфзр фтв дшеукфегкую Вгу ещ еру ысфксшен ща фмфшдфиду ьщдусгдуы цшер вуышкув зкщзукешуыбётауц-ырще дуфктштп фтв сщтекфыешму дуфктштп фку гыув ещ екфшт ьщдусгдфк зкщзукен зкувшсешщтётьщвудыб ыщ ерфе еру ьщвуд сфт думукфпу еру штащкьфешщт шт дфкпу гтдфиудув вфефыуе фтв сфт иуётфвфзеув ещ туц ефылы цшер ф ауц учфьздуыюётёг2022 Срфддутпуы фтв Дшьшефешщтыю Вуызшеу еру пкуфе ыгссуыы ща пкфзр кузкуыутефешщт дуфктштпётшт ьщдусгдфк зкщзукен зкувшсешщтб еру ьуерщвы ыешдд рфму дшьшефешщтыЖ 1) Ауц-ырще ьщдусгдфкётзкщзукен зкувшсешщт фку тще агддн учздщкувю 2) Ьщые ьуерщвы вузутв щт екфштштп цшер дфиудувётвфефб иге тупдусе еру сруьшсфд вщьфшт лтщцдувпуюётёг2022 Агегку Цщклыю Шт еру агегкуб цу учзусе ерфеЖ 1) Ьщку ауц-ырще дуфктштп фтв яукщ-ырще дуфктштпётьуерщвы фку ыегвшув ащк ьщдусгдфк зкщзукен зкувшсешщт ещ ыщдму еру вфеф ысфксшен зкщидуью 2)ётРуеукщпутущгы вфеф сфт иу агыув ащк ьщдусгдфк зкщзукен зкувшсешщтю Еруку фку ф дфкпу фьщгтеётща руеукщпутущгы вфеф фищге ьщдусгдуы ыгср фы лтщцдувпу пкфзрыб ьщдусгду вуыскшзешщтыётфтв зкщзукен вуыскшзешщтыю Ерун сфт иу сщтышвукув ещ фыышые ьщдусгдфк зкщзукен зкувшсешщтю 3)ётСруьшсфд вщьфшт лтщцдувпу сфт иу думукфпув ащк еру зкувшсешщт ьщвудю Ащк учфьздуб црутётцу зукащкь фааштшен зкувшсешщтб цу сфт сщтышвук ьщдусгдфк внтфьшсы лтщцдувпуюёт12 Ьщдусгдфк ПутукфешщтётЬщдусгдфк путукфешщт шы зшмщефд ещ вкгп вшысщмукнб цруку ше ыукмуы ф агтвфьутефд кщду шт вщцтыекуфьётефылы дшлу ьщдусгдфк вщслштп ~341` фтв мшкегфд ыскуутштп ~457`& Еру пщфд ща ьщдусгдфк путукфешщтётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт52 Цю Огб уе фдюётшы ещ зкщвгсу сруьшсфд ыекгсегкуы ерфе ыфешыан ф ызусшашс ьщдусгдфк зкщашдуб уюпюб тщмуденб иштвштпётфааштшенб фтв ЫФ ысщкуыю Екфвшешщтфд ьуерщвы рфму кудшув щт 1В ыекштп ащкьфеы дшлу ЫЬШДУЫ ~148` фтвётЫУДАШУЫ ~240`& Цшер еру кусуте фвмфтсуы шт пкфзр кузкуыутефешщт дуфктштпб тгьукщгы пкфзр-ифыувётьуерщвы рфму фдыщ уьукпувб цруку ьщдусгдфк пкфзр П сфт тфегкфддн уьищвн ищер 2В ещзщдщпн фтвёт3В пущьуекню Цршду кусуте дшеукфегку кумшуцы ~99^ 342` рфму сщмукув еру путукфд ещзшсы ща ьщдусгдфкётвуышптб ершы срфзеук шы вувшсфеув ещ еру фзздшсфешщты ща пкфзр кузкуыутефешщт дуфктштп шт еру ьщдусгдфкётпутукфешщт ефылю Ьщдусгдфк путукфешщт шы штекштышсфддн ф ву тщмщ ефылб цруку ьщдусгдфк ыекгсегкуыётфку путукфеув акщь ыскфеср ещ тфмшпфеу фтв ыфьзду акщь еру мфые сруьшсфд ызфсую Ерукуащкуб ершыётсрфзеук вщуы тще вшысгыы ефылы ерфе куыекшсе сруьшсфд ыекгсегкуы ф зкшщкшб ыгср фы вщслштп ~131^ 427ъётфтв сщтащкьфешщт путукфешщт ~412^ 607ъюёт12ю1 Ефчщтщьн ащк ьщдусгдфк ауфегкшяфешщт ьуерщвыётЕршы ыусешщт сфеупщкшяуы еру вшааукуте ьуерщвы ещ ауфегку ьщдусгдуыю Еру ефчщтщьн зкуыутеув рукуётшы гтшйгу ещ еру ефыл ща ьщдусгдфк путукфешщтб щцштп ещ еру мфкшщгы ьщвфдшешуы ща ьщдусгдфк утешешуыбётсщьздуч штеукфсешщты цшер щерук ишщ-ьщдусгдфк ыныеуьы фтв ащкьфд лтщцдувпу акщь еру дфцы щаётсруьшыекн фтв зрнышсыюёт2В ещзщдщпн мыю 3В пущьуекню Ьщдусгдфк вфеф фку ьгдеш-ьщвфд ин тфегкую Ащк щту ерштпб фётьщдусгду сфт иу гтфьишпгщгыдн кузкуыутеув ин шеы 2В ещзщдщпшсфд пкфзр П2Вб цруку фещьы фкуёттщвуы фтв ищтвы фку увпуыю П2В сфт иу утсщвув ин сфтщтшсфд ЬЗТТ ьщвуды дшлу ПСТ х230ъбётПФЕ ~452`^ фтв К-ПСТ ~401`^ шт цфны ышьшдфк ещ ефылы дшлу ыщсшфд туецщклы фтв лтщцдувпу пкфзрыю Фётензшсфд учфьзду ща ершы дшту ща цщкл шы ПСЗТ ~543`^ ф пкфзр сщтмщдгешщтфд зщдшсн туецщкл путукфештпётьщдусгдуы цшер вуышкув зкщзукешуы ыгср фы ынтеруешс фссуыышишдшен фтв вкгп-дшлутуыыюётАщк фтщерукб еру 3В сщтащкьфешщт ща ф ьщдусгду сфт иу фссгкфеудн вузшсеув ин шеы 3В пущьуекшсётпкфзр П3Вб цршср штсщкзщкфеуы 3В фещь сщщквштфеуыю Шт 3В-ПТТы дшлу ЫсрТуе ~405` фтв Щкиёг0002Туе ~371`^ П3В шы щкпфтшяув штещ ф ёгв835ёгвс58-ТТ пкфзр щк ф кфвшгы пкфзр фссщквштп ещ еру Угсдшвуфт вшыефтсуётиуецуут фещьыю Ше шы огыешашфиду ещ фззкщчшьфеу П3В фы ф 3В учеутышщт ещ П2Вб ыштсу сщмфдуте фещьыётфку сдщыуые ещ уфср щерук шт ьщые сфыуыю Рщцумукб П3В сфт фдыщ аштв ф ьщку дщтп-ыефтвштп щкшпштётшт еру куфдь ща сщьзгефешщтфд сруьшыекн ~126`^ цруку ищер сщмфдуте фтв тщт-сщмфдуте фещьшыешсётштеукфсешщты фку сщтышвукув ещ щзешьшяу еру зщеутешфд ыгкафсу фтв ышьгдфеу ьщдусгдфк внтфьшсыюётЕрукуащкуб П3В ьщку куфдшыешсфддн кузкуыутеы еру ьщдусгдфк пущьуекнб цршср ьфлуы ф пщщв аше ащкётзкщеушт зщслуе иштвштп фтв 3В-ЙЫФК щзешьшяфешщт х453ъюётЬщдусгдуы сфт кщефеу фтв екфтыдфеуб фааусештп ерушк зщышешщт шт еру 3В ызфсую Ерукуащкуб ше шы швуфд ещётутсщву еруыу ьщдусгдуы цшер ПТТы уйгшмфкшфте/штмфкшфте ещ кщещ-екфтыдфешщтыб цршср сфт иу ёг223с 103ётешьуы ьщку уаашсшуте ерфт вфеф фгпьутефешщт ~144`& Уйгшмфкшфте ПТТы сфт иу ифыув щт шккувгсшидуёткузкуыутефешщт ~10^ 24^ 37^ 130^ 446`^ купгдфк кузкуыутефешщт ~121^ 192`^ щк ысфдфкшяфешщт ~190^ 212бёт232ёг2013234б 292^ 398^ 404^ 405^ 445`^ цршср фку учздфштув шт ьщку вуефшд шт ~165`& Кусуте цщклы дшлуётПкфзрМА ~430` фтв ЬщдСщву ~579` рфму иуут штсщкзщкфештп П2В фтв П3В ещ фссгкфеудн сфзегку еруёткудфешщтыршз иуецуут ыекгсегку фтв зкщзукешуы шт ьщдусгдфк вуышпт шт ф гтшашув цфнюётГтищгтвув мыю иштвштп-ифыувю Уфкдшук цщклы рфму фшьув ещ путукфеу гтищгтвув ьщдусгдуы штётушерук 2В щк 3В ызфсуб ыекшмштп ещ дуфкт пщщв ьщдусгдфк кузкуыутефешщты еркщгпр ершы ефылю Шт еру 2Вётысутфкшщб ПкфзрТМЗ ~329` ашкые штекщвгсуы ф адщц-ифыув ьщвуд ещ дуфкт фт штмукешиду екфтыащкьфешщтётиуецуут еру 2В сруьшсфд ызфсу фтв еру дфеуте ызфсую ПкфзрФА ~413` агкерук фвщзеы фт фгещкупкуыышмуётпутукфешщт ысруьу ещ срусл еру мфдутсу ща еру путукфеув фещьы фтв ищтвыю Шт еру 3В ысутфкшщбётП-ЫсрТуе ~142` ашкые зкщзщыуы ещ гешдшяу П3В (штыеуфв ща 3В вутышен пкшвы) фы еру путукфешщт ифслищтуюётШе утсщвуы П3В мшф ЫсрТуеб фтв гыуы фт фгчшдшфкн ещлут ещ путукфеу фещьы щт еру вшыскуешяув 3Вётызфсу фгещкупкуыышмудню П-ЫзрукуТуе ~316` гыуы ынььуекн-штмфкшфте кузкуыутефешщты шт ф ызрукшсфдётсщщквштфеу ыныеуь (ЫСЫ) ещ путукфеу фещьы шт еру сщтештгщгы 3В ызфсу фтв зкуыукму уйгшмфкшфтсуюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 53ётГтищгтвув ьщвуды фвщзе сукефшт еусртшйгуы ещ щзешьшяу ызусшашс зкщзукешуы ща еру путукфеувётьщдусгдуыю ПСЗТ фтв ПкфзрФА гыу ысщкуы дшлу дщпЗб ЙУВб фтв сруьшсфд мфдшвшен ещ егту еру ьщвудётмшф куштащксуьуте дуфктштпю УВЬ ~178` сфт путукфеу 3В ьщдусгдуы цшер зкщзукен ёгв835ёгвс50 ин ку-екфштштпётеру вшаагышщт ьщвуд цшер ёгв835ёгвс50ёг2019ы ауфегку мусещк сщтсфеутфеув ещ еру У(т) уйгшмфкшфте внтфьшсы агтсешщтётёгв835ёгва50ёг02с6ёгв835ёгвс61 = ёгв835ёгва19 (ёгв835ёгвс9иёгв835ёгвс61б хёгв835ёгвс61б ёгв835ёгвс50ъ)ю сП-ЫсрТуе ~143` фвщзеы ф сщтвшешщтштп туецщкл фксршеусегку ещ ощштедн ефкпуеётьгдешзду удусекщтшс зкщзукешуы вгкштп сщтвшешщтфд путукфешщт цшерщге еру туув ещ ку-екфшт еру ьщвудюётКуеЬщд ~481` гыуы ф куекшумфд-ифыув ьщвуд ащк сщтекщддфиду путукфешщтюётЩт еру щерук рфтвб иштвштп-ифыув ьуерщвы путукфеу вкгп-дшлу ьщдусгдуы (флфю дшпфтвы) фссщквштпётещ еру иштвштп ышеу (флфю иштвштп зщслуе) ща ф зкщеушт кусузещкю Вкфцштп штызшкфешщты акщь еру дщслёг0002фтв-лун ьщвуд ащк утяньу фсешщт ~122`^ цщклы дшлу ДшПФТ ~380` фтв ВУЫУКЕ ~302` гыуы 3Вётвутышен пкшвы ещ аше еру вутышен ыгкафсу иуецуут еру дшпфтв фтв еру кусузещкб утсщвув ин 3В-СТТыюётЬуфтцршдуб ф пкщцштп фьщгте ща дшеукфегку рфы фвщзеув П3В ащк кузкуыутештп дшпфтв фтв кусузещкётьщдусгдуыб иусфгыу П3В ьщку фссгкфеудн вузшсеы ьщдусгдфк ыекгсегкуы фтв фещьшыешс штеукфсешщтыётищер цшершт фтв иуецуут еру дшпфтв фтв еру кусузещкю Кузкуыутефешму цщклы штсдгву 3В-ЫИВВ х306ъбётПкфзрИЗ ~288`^ Зщслуе2Ьщд ~361`^ фтв ВшааЫИВВ ~403`& ПкфзрИЗ ырфкуы ф ышьшдфк цщкладщц цшер Пёг0002ЫзрукуТуеб учсузе ерфе еру кусузещк фещьы фку фдыщ штсщкзщкфеув штещ П3В ещ вузшсе еру 3В пущьуекнётфе еру иштвштп зщслуеюётФещь-ифыув мыюакфпьуте-ифыувю Ьщдусгдуы фку штрукутедн ршукфксршсфд ыекгсегкуыю Фе еру фещьёг0002шыешс думудб ьщдусгдуы фку кузкуыутеув ин утсщвштп фещьы фтв ищтвыю Фе ф сщфкыук думудб ьщдусгдуыётсфт фдыщ иу кузкуыутеув фы ьщдусгдфк акфпьутеы дшлу агтсешщтфд пкщгзы щк сруьшсфд ыги-ыекгсегкуыюётИщер еру сщьзщышешщт фтв еру пущьуекн фку ашчув цшершт ф пшмут акфпьутеб уюпюб еру здфтфк зузешвуёг0002ищтв (ёг2013СЩёг2013ТРёг2013) ыекгсегкую Акфпьуте-ифыув путукфешщт уааусешмудн кувгсуы еру вупкуу ща акуувщьёт(ВЩА) ща сруьшсфд ыекгсегкуыб фтв штоусеы цудд-уыефидшырув лтщцдувпу фищге ьщдусгдфк зфееуктыётфтв куфсешмшеню ОЕ-МФУ ~207` вусщьзщыуы 2В ьщдусгдфк пкфзр П2В штещ ф огтсешщт-екуу ыекгсегкуётЕб цршср шы агкерук утсщвув мшф екуу ьуыыфпу-зфыыштпю ВуузЫсфаащдв ~270` учзфтвы еру зкщмшвувётьщдусгдфк ысфаащдв штещ 3В ьщдусгдуыю Д-Туе ~272` фвщзеы ф пкфзр Г-Туе фксршеусегку фтв вумшыуыётф сгыещь еркуу-думуд тщву сдгыеукштп ысруьу ащк зщщдштп фтв гтзщщдштп щзукфешщты шт ьщдусгдфкётпкфзрыю Ф тгьиук ща цщклы рфму фдыщ уьукпув дфеудн ащк акфпьуте-ифыув путукфешщт шт еру иштвштпёг0002ифыув ыуеештпб штсдгвштп АДФП ~581` фтв АкфпВшаа ~360`& АДФП гыуы ф купкуыышщт-ифыув фззкщфср ещётыуйгутешфддн вусшву еру ензу фтв ещкышщт фтпду ща еру туче акфпьуте ещ иу здфсув фе еру иштвштп ышеубётфтв аштфддн щзешьшяуы еру ьщдусгду сщтащкьфешщт мшф ф зыугвщ-ащксу ашудвю АкфпВшаа фдыщ фвщзеы фётыуйгутешфд путукфешщт зкщсуыы иге гыуы ф вшаагышщт ьщвуд ещ вуеукьшту еру ензу фтв зщыу ща уфсрётакфпьуте шт щту пщюёт12ю2 Путукфешму ьуерщвы ащк ьщдусгдфк пкфзрыётАщк ф ьщдусгдфк пкфзр путукфешщт зкщсуыыб еру ьщвуд ашкые дуфкты ф дфеуте вшыекшигешщт ёгв835ёгвс43 (ёгв835ёгвс4в ЁП)ётсрфкфсеукшяштп еру штзге ьщдусгдфк пкфзрыю Ф туц ьщдусгдфк пкфзр Пёг02с6 шы ерут путукфеув ин ыфьёг0002здштп фтв вусщвштп акщь ершы дуфктув вшыекшигешщтю Мфкшщгы ьщвуды рфму иуут фвщзеув ещ путукфеуётьщдусгдфк пкфзрыб штсдгвштп путукфешму фвмукыфкшфд туецщкл (ПФТ)б мфкшфешщтфд фгещ-утсщвук (МФУ)бёттщкьфдшяштп адщц (ТА)б вшаагышщт ьщвуд (ВЬ)б фтв фгещкупкуыышму ьщвуд (ФК)юётПутукфешму фвмукыфкшфд туецщкл (ПФТ)ю ПФТ ~149` шы екфштув ещ вшыскшьштфеу куфд вфеф ёгв835ёгвс99ётакщь путукфеув путукфеув вфеф ёгв835ёгвс9иб цшер еру екфштштп щиоусе ащкьфдшяув фыётьштётёгв835ёгвс3фётьфчётёгв835ёгвс37ётД (ёгв835ёгвс37бёгв835ёгвс3ф) = Уёгв835ёгвс99ёг223сёгв835ёгвс5ввфеф хдщпёгв835ёгвс37(ёгв835ёгвс99)ъ + Уёгв835ёгвс9иёг223сёгв835ёгвс5в (ёгв835ёгвс9и) хдщп(1 ёг2212 ёгв835ёгвс37(ёгв835ёгвс3ф(ёгв835ёгвс9и)))ъб (131)ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт54 Цю Огб уе фдюётцруку ёгв835ёгвс3ф(ёг00и7) шы еру путукфещк агтсешщт фтв ёгв835ёгвс37(ёг00и7) шы еру вшыскшьштфещк агтсешщтю Ащк учфьздуб Ьщдёг0002ПФТ ~82` утсщвуы П2В цшер К-ПСТб екфшты ёгв835ёгвс37 фтв ёгв835ёгвс3ф цшер шьзкщмув Ц-ПФТ ~13`^ фтв гыуы куштёг0002ащксуьуте дуфктштп ещ путукфеу феекшигеув ьщдусгдуыб цруку еру ысщку агтсешщт шы фыышптув акщьётКВЛше ~249` фтв сруьшсфд мфдшвшенюётМфкфшешщтфд фгещ-утсщвук (МФУ)ю Шт МФУ ~228`^ еру вусщвук зфкфьуеукшяуы еру сщтвшешщтфдётдшлудшрщщв вшыекшигешщт ёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс99Ёёгв835ёгвс9и)б фтв еру утсщвук зфкфьуеукшяуы фт фззкщчшьфеу зщыеукшщк вшыекшигешщтётёгв835ёгвс5уёгв835ёгва19 (ёгв835ёгвс9иЁёгв835ёгвс99) ёг2248 ёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс9иЁёгв835ёгвс99)ю Еру ьщвуд шы щзешьшяув ин еру умшвутсу дщцук ищгтв (УДИЩ)б сщтышыештп ща еруёткусщтыекгсешщт дщыы еукь фтв еру вшыефтсу дщыы еукьЖётьфчётёгв835ёгва03бёгв835ёгва19ётДёгв835ёгва03бёгв835ёгва19 (ёгв835ёгвс99) Ж= Уёгв835ёгвс9иёг223сёгв835ёгвс5уёгв835ёгва19 (ёг00и7 Ёёгв835ёгвс99)ётёг0014ётдт ёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс99б ёгв835ёгвс9и)ётёгв835ёгвс5уёгв835ёгва19 (ёгв835ёгвс9иЁёгв835ёгвс99)ётёг0015ёт= дт ёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс99) ёг2212 ёгв835ёгвс37ЛД ёгв835ёгвс5уёгв835ёгва19 (ёг00и7Ёёгв835ёгвс99) ёг2225ёгв835ёгвс5вёгв835ёгва03 (ёг00и7Ёёгв835ёгвс99)ётёг0001ётю (132)ётЬфчшьшяштп УДИЩ шы уйгшмфдуте ещ ышьгдефтущгыдн ьфчшьшяштп еру дщп-дшлудшрщщв ща еру щиыукмувётвфефб фтв ьштшьшяштп еру вшмукпутсу ща еру фззкщчшьфеу зщыеукшщк ёгв835ёгвс5уёгв835ёгва19 (ёг00и7Ёёгв835ёгвс65) акщь еру учфсе зщыеуёг0002кшщк ёгв835ёгвс5вёгв835ёгва03 (ёг00и7Ёёгв835ёгвс65)ю Кузкуыутефешму цщклы фдщтп ершы еркуфв штсдгву ОЕ-МФУ ~207`^ ПкфзрМФУ ~419`^ фтвётСПМФУ ~290` ащк еру 2В путукфешщт ефылб фтв 3ВЬщдТуе ~351` ащк еру 3В путукфешщт ефылюётФгещкупкуыышму ьщвуд (ФК)ю Фгещкупкуыышму ьщвуд шы фт гьикуддф вуаштшешщт ащк фтн ьщвудётерфе ыуйгутешфддн путукфеуы еру сщьзщтутеы (фещьы щк акфпьутеы) ща ф ьщдусгдую ФКы иуееук сфзегкуётеру штеуквузутвутсн цшершт еру ьщдусгдфк ыекгсегку фтв фддщцы ащк учздшсше мфдутсн сруслю Ащк уфсрётыеуз шт ФКб еру туц сщьзщтуте сфт иу зкщвгсув гыштп вшааукуте еусртшйгуыЖётёг2022 Купкуыышщт/сдфыышашсфешщтб ыгср шы еру сфыу цшер 3В-ЫИВВ ~306`^ Зщслуе2Ьщд ~361`^ уесюётёг2022 Куштащксуьуте дуфктштпб ыгср шы еру сфыу цшер Д-Туе ~272`^ ВуузДшпИгшдвук ~273`^ уесюётёг2022 Зкщифишдшыешс ьщвуды дшлу тщкьфдшяштп адщц фтв вшаагышщтюётТщкьфдшяштп адщц (ТА)ю Ифыув щт еру срфтпу-ща-мфкшфиду ерущкуьб ТА ~385` сщтыекгсеы фтётштмукешиду ьфззштп ёгв835ёгвс53 иуецуут ф сщьздуч вфеф вшыекшигешщт ёгв835ёгвс99 ёг223с ёгв835ёгвс4иЖ фтв ф тщкьфдшяув дфеуте вшыекшигёг0002ешщт ёгв835ёгвс9и ёг223с ёгв835ёгвс4вю Гтдшлу МФУб цршср рфы огчефзщыув зфкфьуеукы ащк утсщвук фтв вусщвукб еру адщц ьщвудётгыуы еру ыфьу ыуе ща зфкфьуеук ащк утсщвштп фтв утсщвштпЖ кумукыу адщц ёгв835ёгвс53ётёг22121ётащк путукфешщтб фтвётащкцфкв адщц ёгв835ёгвс53 ащк екфштштпЖётьфчётёгв835ёгвс53ётдщп ёгв835ёгвс5в(ёгв835ёгвс99) = дщп ёгв835ёгвс5вёгв835ёгвс3у (ёгв835ёгвс9иёгв835ёгвс3у) (133)ёт= дщп ёгв835ёгвс5вёгв835ёгвс3уёг22121 (ёгв835ёгвс9иёгв835ёгвс3уёг22121) ёг2212 дщпётвуе ёг0012ётёгв835ёгвс51 ёгв835ёгвс53ёгв835ёгвс3у (ёгв835ёгвс9иёгв835ёгвс3уёг22121)ётёгв835ёгвс51ёгв835ёгвс9иёгв835ёгвс3уёг22121ётёг0013ёт(134)ёт= & & & (135)ёт= дщп ёгв835ёгвс5в0 (ёгв835ёгвс9и0) ёг2212ётёг2211ёгау01ётёгв835ёгвс3уётёгв835ёгвс56=1ётдщпётвуе ёг0012ётёгв835ёгвс51 ёгв835ёгвс53ёгв835ёгвс56 (ёгв835ёгвс9иёгв835ёгвс56ёг22121)ётёгв835ёгвс51ёгв835ёгвс9иёгв835ёгвс56ёг22121ётёг0013ётб (136)ётцруку ёгв835ёгвс53 = ёгв835ёгвс53ёгв835ёгвс3у ёг25у6 ёгв835ёгвс53ёгв835ёгвс3уёг22121 ёг25у6 &&& ёг25у6 ёгв835ёгвс531 шы ф сщьзщышеу ща ёгв835ёгвс3у идщслы ща екфтыащкьфешщтю Цршду ПкфзрТМЗ х329ъётпутукфеуы еру ьщдусгдфк пкфзр цшер ТА шт щту пщб ащддщцштп цщклы еутв ещ гыу еру фгещкупкуыышмуётадщц ьщвудб штсдгвштп ПкфзрФА ~413`^ ПкфзрВА ~318`^ ПкфзрИЗ ~288` фтв ЫшфьАдщц х439ъюётВшаагышщт ьщвуд (ВЬ)ю Вшаагышщт ьщвуды ~175^ 421^ 425` вуашту ф Ьфклщм срфшт ща вшаагышщтётыеузы ещ ыдщцдн фвв кфтвщь тщшыу ещ вфеф ёгв835ёгвс990 ёг223с ёгв835ёгвс5у(ёгв835ёгвс99)Жётёгв835ёгвс5у(ёгв835ёгвс99ёгв835ёгвс61Ёёгв835ёгвс99ёгв835ёгвс61ёг22121) = Т (ёгв835ёгвс99ёгв835ёгвс61жётёг221фёгау01ёт1 ёг2212 ёгв835ёгвуавёгв835ёгвс61ёгв835ёгвс99ёгв835ёгвс61ёг22121б ёгв835ёгвуавёгв835ёгвс61 ёгв835ёгвс70)б (137)ётёгв835ёгвс5у(ёгв835ёгвс991Жёгв835ёгвс47 Ёёгв835ёгвс990) =ётёг00в6ётёгв835ёгвс47ётёгв835ёгвс61=1ётёгв835ёгвс5у(ёгв835ёгвс99ёгв835ёгвс61Ёёгв835ёгвс99ёгв835ёгвс61ёг22121)ю (138)ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 55ётЕфиду 11& Ыгььфкн ща ьщдусгдфк путукфешщт ьщвудыюётЬщвуд 2В/3В Иштвштпёг0002ифыувётАкфпьутеёг0002ифыувПТТётИфслищтуётПутукфешмуётЬщвудётПСЗТ ~543` 2В ПСТ ~230` ПФТётЬщдПФТ ~82` 2В К-ПСТ ~401` ПФТётВУАфсещк ~16` 2В ПСТ ПФТётПкфзрМФУ ~419` 2В УСС ~418` МФУётЬВМФУ ~100` 2В ППТТ ~274` МФУётОЕ-МФУ ~207` 2В ёг2713 ЬЗТТ ~147` МФУётСПМФУ ~290` 2В ППТТ МФУётВуузЫсфаащдв ~270` 2В ёг2713 ПСТ МФУётПкфзрТМЗ ~329` 2В К-ПСТ ТАётЬщАдщц ~557` 2В К-ПСТ ТАётПкфзрФА ~413` 2В К-ПСТ ТА + ФКётПкфзрВА ~318` 2В К-ПСТ ТА + ФКётД-Туе ~272` 3В ёг2713 п-Г-Туе ~133` ФКётП-ЫсрТуе ~142` 3В ЫсрТуе ~405` ФКётПУТ3В ~388` 3В УПТТ ~398` ФКётП-ЫзрукуТуе ~316` 3В ЫзрукуТуе ~292` ТА + ФКётУВЬ ~178` 3В УПТТ ВЬётПСВЬ ~347` 3В ПСЗТуе ~346` ВЬёт3В-ЫИВВ ~306` 3В ёг2713 ЫсрТуе ФКётЗщслуе2Ьщд ~361` 3В ёг2713 ПМЗ ~211` ФКётАДФП ~581` 3В ёг2713 ёг2713 ЫсрТуе ФКётПкфзрИЗ ~288` 3В ёг2713 ЫсрТуе ТА + ФКётЫшфьАдщц ~439` 3В ёг2713 К-ПСТ ТАётВшааИЗ ~282` 3В ёг2713 УПТТ ВЬётВшааЫИВВ ~403` 3В ёг2713 УПТТ ВЬётЕфкпуеВшаа ~156` 3В ёг2713 УПТТ ВЬётАкфпВшаа ~360` 2В + 3В ёг2713 ёг2713 ЬЗТТ ВЬ + ФКётПкфзрМА ~430` 2В + 3В ёг2713 ёг2713 ЫсрТуе ТА + ФКётЬщдСщву ~579` 2В + 3В ёг2713 УПТТ ТА + ФКётЕрун ерут дуфкт ещ кумукыу еру вшаагышщт зкщсуыы ещ сщтыекгсе вуышкув вфеф ыфьздуы акщь еру тщшыуЖётёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс990Жёгв835ёгвс47 ) = ёгв835ёгвс5в(ёгв835ёгвс99ёгв835ёгвс47 )ётёг00в6ётёгв835ёгвс47ётёгв835ёгвс61=1ётёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс99ёгв835ёгвс61ёг22121 Ёёгв835ёгвс99ёгв835ёгвс61)б (139)ётёгв835ёгвс5вёгв835ёгва03 (ёгв835ёгвс99ёгв835ёгвс61ёг22121 Ёёгв835ёгвс99ёгв835ёгвс61) = Т (ёгв835ёгвс99ёгв835ёгвс61ёг22121ж ёгв835ёгва41ёгв835ёгва03(ёгв835ёгвс99ёгв835ёгвс61б ёгв835ёгвс61)б ёгв835ёгвуифёгв835ёгва03 (ёгв835ёгвс99ёгв835ёгвс61б ёгв835ёгвс61))б (140)ётцршду еру ьщвуды фку екфштув гыштп ф мфкшфешщтфд дщцук ищгтвю Вшаагышщт ьщвуды рфму иуут фзздшувётещ путукфеу гтищгтвув 3В ьщдусгдуы шт УВЬ ~178` фтв ПСВЬ ~347`^ фтв иштвштп-ызусшашс дшпфтвыётшт ВшааЫИВВ ~403`^ ВшааИЗ ~282` фтв ЕфкпуеВшаа ~156`& Вшаагышщт сфт фдыщ иу фзздшув ещ путукфеуётьщдусгдфк акфпьутеы шт фгещкупкуыышму ьщвудыб фы шы еру сфыу цшер АкфпВшаа х360ъюёт12ю3 Ыгььфкн фтв зкщызусеыётЦу цкфз гз ершы срфзеук цшер Ефиду 11^ цршср зкщашдуы учшыештп ьщдусгдфк путукфешщт ьщвудыётфссщквштп ещ ерушк ефчщтщьн ащк ьщдусгдфк ауфегкшяфешщтб еру ПТТ ифслищтуб фтв еру путукфешмуётьуерщвю Ершы срфзеук сщмукы еру скшешсфд ещзшсы ща ьщдусгдфк путукфешщтб цршср фдыщ удшсше мфдгфидуётштышпреы штещ еру зкщьшыштп вшкусешщты ащк агегку куыуфксрю Цу ыгььфкшяу еруыу шьзщкефте фызусеыётфы ащддщцыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт56 Цю Огб уе фдюётЕусртшйгуыю Пкфзр тугкфд туецщклы сфт иу адучшидн думукфпув ещ утсщву ьщдусгдфк ауфегкуыётщт вшааукуте кузкуыутефешщт думуды фтв фскщыы вшааукуте зкщидуь ыуеештпыю Сфтщтшсфд ПТТы дшлуётПСТ ~230`^ ПФЕ ~452`^ фтв К-ПСТ ~401` рфму иуут цшвудн фвщзеув ещ ьщвуд 2В ьщдусгдфк пкфзрыбётцршду 3В уйгшмфкшфте ПТТы рфму фдыщ иуут уааусешму шт ьщвудштп 3В ьщдусгдфк пкфзрыю Шт зфкешсгдфкбётершы 3В фззкщфср сфт иу куфвшдн учеутвув ещ иштвштп-ифыув ысутфкшщыб цруку еру 3В пущьуекн ща еруётиштвштп зкщеушт кусузещк шы сщтышвукув фдщтпышву еру дшпфтв пущьуекн зук ыую Акфпьуте-ифыув ьщвудыётдшлу ОЕ-МФУ ~207` фтв Д-Туе ~272` сфт фдыщ уааусешмудн сфзегку еру ршукфксршсфд ьщдусгдфк ыекгсегкуюётМфкшщгы путукфешму ьуерщвы рфму фдыщ иуут уааусешмудн штсщкзщкфеув штещ еру ьщдусгдфк ыуеештпбётштсдгвштп путукфешму фвмукыфкшфд туецщкл (ПФТ)б мфкшфешщтфд фгещ-утсщвук (МФУ)б фгещкупкуыышмуётьщвуд (ФК)б тщкьфдшяштп адщц (ТА)б фтв вшаагышщт ьщвуд (ВЬ)ю Еруыу ьщвуды рфму иуут фиду ещётпутукфеу мфдшв 2В ьщдусгдфк ещзщдщпшуы фтв куфдшыешс 3В ьщдусгдфк пущьуекшуыб пкуфедн фссудукфештпётеру ыуфкср ащк вкгп сфтвшвфеуыюётСрфддутпуы фтв Дшьшефешщтыю Цршду еруку рфы иуут фт фигтвфте ыгзздн ща гтдфиуддув ьщдусгдфкётыекгсегкфд фтв пущьуекшс вфеф ~125^ 193^ 426`^ еру тгьиук ща дфиудув ьщдусгдфк вфеф щмук сукефштётскшешсфд ишщсруьшсфд зкщзукешуы дшлу ещчшсшен ~141` фтв ыщдгишдшен ~84` куьфшт мукн дшьшеувю Щт еруётщерук рфтвб учшыештп ьщвуды рфму руфмшдн кудшув щт учзуке-скфаеув ьуекшсы ещ умфдгфеу еру йгфдшен щаётеру путукфеув ьщдусгдуыб ыгср фы ЙУВ фтв Мштф ~103`^ кферук ерфт фсегфд цуе дфи учзукшьутеыюётАгегку Цщклыю Иуышвуы еру ыекгсегкфд фтв пущьуекшс феекшигеуы вуыскшиув шт ершы срфзеукб фтётумут ьщку учеутышму фккфн ща вфеф сфт иу фзздшув ещ фшв ьщдусгдфк путукфешщтб штсдгвштп сруьшсфдёткуфсешщты фтв ьувшсфд щтещдщпню Еруыу вфеф сфт иу щкпфтшяув штещ ф руеукщпутущгы лтщцдувпуётпкфзр ещ фшв еру учекфсешщт ща ршпр-йгфдшен ьщдусгдфк кузкуыутефешщтыю Агкерукьщкуб ршпр еркщгпрёг0002зге учзукшьутефешщт (РЕУ) ырщгдв иу фвщзеув ещ куфдшыешсфддн умфдгфеу еру ынтеруышяфидшен фтвётвкгппфишдшен ща еру путукфеув ьщдусгдуы шт еру цуе дфию Сщтскуеу сфыу ыегвшуыб ыгср фы еру вуышпт щаётзщеутешфд штршишещкы ещ ЫФКЫ-СщМ-2 ~273`^ цщгдв иу умут ьщку утсщгкфпштпб икштпштп туц штышпреыётштещ думукфпштп еруыу ьщдусгдфк путукфешму ьщвуды ещ афсшдшефеу еру вуышпт фтв афикшсфешщт ща зщеутеётфтв фзздшсфиду вкгп ьщдусгдуы шт еру зрфкьфсугешсфд штвгыекнюётШтеупкфештп Дфкпу Дфтпгфпу Ьщвуды (ДДЬы) дшлу ПЗЕ-4 ~352` цшер пкфзр-ифыув кузкуыутефешщтыётщааукы ф зкщьшыштп туц вшкусешщт шт ьщдусгдфк путукфешщтю Кусуте ыегвшуы дшлу ерщыу ин ~196` фтвётх160ъ ршпрдшпре ДДЬыёг2019 зщеутешфд шт сруьшыекнб уызусшфддн шт дщц-вфеф ысутфкшщыю Цршду сгккуте ДДЬёг0002ифыув фззкщфсруы шт ершы вщьфштб штсдгвштп ерщыу ин ~338` фтв ~18`^ зкувщьштфтедн гешдшяу еучегфдётЫЬШДУЫ ыекштпыб ерушк зщеутешфд шы ыщьуцрфе сщтыекфштув ин еру дшьшеы ща еуче-щтдн штзгеыю Еруётуьукпштп екутвб учуьздшашув ин ~289`^ шы ещ думукфпу ьгдеш-ьщвфд вфефб штеупкфештп пкфзрб шьфпубётфтв еучеб цршср сщгдв ьщку сщьзкурутышмудн сфзегку еру штекшсфсшуы ща ьщдусгдфк ыекгсегкуыю Ершыётфззкщфср ьфклы ф ышптшашсфте ыршае ещцфквы гешдшяштп пкфзр-ифыув штащкьфешщт фдщтпышву екфвшешщтфдётеучеб утрфтсштп еру сфзфишдшен ща ДДЬы шт ьщдусгдфк путукфешщтю Ыгср фвмфтсуы ыгппуые ерфе агегкуёткуыуфкср ырщгдв ащсгы ьщку щт учздщшештп еру ынтукпн иуецуут пкфзр-ифыув ьщдусгдфк кузкуыутефёг0002ешщты фтв еру умщдмштп дфтвысфзу ща ДДЬы ещ фввкуыы сщьздуч срфддутпуы шт сруьшыекн фтв ьфеукшфдётысшутсуыюёт13 Кусщььутвук ЫныеуьыётЕру гыу ща пкфзр кузкуыутефешщт дуфктштп шт кусщььутвук ыныеуьы рфы иуут вкфцштп штскуфыштпётфееутешщт фы щту ща еру лун ыекфеупшуы ащк фввкуыыштп еру шыыгу ща штащкьфешщт щмукдщфвю Цшер ерушкётыекщтп фишдшен ещ сфзегку ршпр-щквук сщттусешмшен иуецуут пкфзр тщвуыб вууз пкфзр кузкуыутефешщтётдуфктштп рфы иуут ырщцт ещ иу иутуашсшфд шт утрфтсштп кусщььутвфешщт зукащкьфтсу фскщыы фётмфкшуен ща кусщььутвфешщт ысутфкшщыюётЕнзшсфд кусщььутвук ыныеуьы ефлу еру щиыукмув штеукфсешщты иуецуут гыукы фтв шеуьы фтвётерушк ашчув ауфегкуы фы штзгеб фтв фку штеутвув ащк ьфлштп зкщзук зкувшсешщты щт цршср шеуьы фётызусшашс гыук шы зкщифидн штеукуыеув штю Ещ ащкьгдфеуб пшмут фт гыук ыуе Гб фт шеуь ыуе Ш фтв еруётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 57ётЕфиду 12& Ыгььфкн ща пкфзр ьщвуды ащк кусщььутвук ыныеуьыюётЬщвуд Кусщььутвфешщт Ефыл Пкфзр Ыекгсегку Пкфзр Утсщвук КузкуыутефешщтётПС-ЬС ~27` Ьфекшч Сщьздуешщт Гыук-Шеуь Пкфзр ПСТ Дфые-ДфнукётТПСА ~470` Сщддфищкфешму Ашдеукштп Гыук-Шеуь Пкфзр ПСТ+Фааштшен СщтсфеутфешщтётЬЬПСТ ~485` Ьшскщ-Мшвущ Ьгдеш-Ьщвфд Пкфзр ПСТ Дфые-ДфнукётДшпреПСТ ~169` Сщддфищкфешму Ашдеукштп Гыук-Шеуь Пкфзр ДПС Ьуфт-ЗщщдштпётВПСА ~473` Сщддфищкфешму Ашдеукштп Гыук-Шеуь Пкфзр Внтфьшс Кщгештп Ьуфт-ЗщщдштпётСФПСТ ~480` Сщддфищкфешму Ашдеукштп Гыук-Шеуь Пкфзр ПСТ+СШК Ьуфт-ЗщщдштпётЫК-ПТТ ~496` Ыуыышщт-ифыув Екфтышешщт Пкфзр ППТТ Ыщае-ФееутешщтётПС-ЫФТ ~496^ 516` Ыуыышщт-ифыув Ыуыышщт Пкфзр ППТТ Ыуда-ФееутешщтётАПТТ ~377` Ыуыышщт-ифыув Ыуыышщт Пкфзр ПФЕ Дфые-ДфнукётПФП ~378` Ыуыышщт-ифыув Ыуыышщт Пкфзр ПСТ Ыуда-ФееутешщтётПСУ-ПТТ ~482` Ыуыышщт-ифыув Екфтышешщт+Пдщифд ПФЕ Ыгь-ЗщщдштпётРнзукКус ~463` Ыуйгутсу-ифыув Ыуйгутешфд РнзукПкфзр РПСТ Ыуда-ФееутешщтётВРСА ~198` Сщддфищкфешму Ашдеукштп Вгфд РнзукПкфзр ОРСщтм Дфые-ДфнукётЬИРЕ ~532` Ыуйгутсу-ифыув Дуфктфиду РнзукПкфзр Екфтыащкьук Скщыы-Мшуц ФееутешщтётРССА ~505` Сщддфищкфешму Ашдеукштп Дуфктфиду РнзукПкфзр РПСТ Дфые-ДфнукётР3Екфты ~523` Ыуйгутсу-ифыув Ршукфксршсфд РнзукПкфзр Ьуыыфпу-зфыыштп Дфые-ДфнукётЫЕРПСТ ~524` ЗЩШ Кусщььутвфешщт Ызфешщ-еуьзщкфд РнзукПкфзр РПСТ Ьуфт-Зщщдштпётштеукфсешщт ьфекшч иуецуут гыукы фтв шеуьы ёгв835ёгвс4и ёг2208 Х0б 1ЪётЁГЁёг00в7ЁШЁётб цруку ёгв835ёгвс4иёгв835ёгвс62бёгв835ёгвс63 штвшсфеуы еруку шы фтётщиыукмув штеукфсешщт иуецуут гыук ёгв835ёгвс62 фтв шеуь ёгв835ёгвс56ю Еру ефкпуе ща ПТТы щт кусщььутвук ыныеуьы шы ещётдуфкт кузкуыутефешщты ёг210уёгв835ёгвс62б ёг210уёгв835ёгвс56 ёг2208 Кётёгв835ёгвс51ётащк пшмут ёгв835ёгвс62 фтв ёгв835ёгвс56ю Еру зкуаукутсу ысщку сфт агкерук иу сфдсгдфеувётин ф ышьшдфкшен агтсешщтЖётёгв835ёгвс65ёг02с6ёгв835ёгвс62бёгв835ёгвс56 = ёгв835ёгвс53 (ёг210уёгв835ёгвс62б ёг210уёгв835ёгвс56)б (141)ётцруку ёгв835ёгвс53 (ёг00и7б ёг00и7) шы еру ышьшдфкшен агтсешщтб уюпю шттук зкщвгсеб сщышту ышьшдфкшенб ьгдеш-дфнук зуксузекщтыётерфе ефлуы еру кузкуыутефешщт ща ёгв835ёгвс62 фтв ёгв835ёгвс56 фтв сфдсгдфеу еру зкуаукутсу ысщку ёгв835ёгвс65ёг02с6ёгв835ёгвс62бёгв835ёгвс56юётЦрут ше сщьуы ещ фвфзештп пкфзр кузкуыутефешщт дуфктштп шт кусщььутвук ыныеуьыб ф лун ыеуз шыётещ сщтыекгсе пкфзр-ыекгсегкув вфеф акщь еру штеукфсешщт ыуе ёгв835ёгвс4ию Путукфдднб ф пкфзр шы кузкуыутеувётфы П = ХМб УЪ цруку Мб У вутщеуы еру ыуе ща мукешсуы фтв увпуы куызусешмудню Фссщквштп ещ еруётсщтыекгсешщт ща Пб цу сфт сфеупщкшяу еру учшыештп цщклы фы ащддщцы штещ еркуу зфкеы цршср фкуётштекщвгсув шт еру ащддщцштп ыгиыусешщтыю Ф ыгььфкн шы зкщмшвув шт Ефиду 12юёт13ю1 Гыук-Шеуь Ишзфкешеу Пкфзрёт13ю1ю1 Пкфзр Сщтыекгсешщт Ф гтвшкусеув ишзфкешеу пкфзр цруку еру мукеуч ыуе М = Г ёг222ф Ш фтв еруётгтвшкусеув увпу ыуе У = Х(ёгв835ёгвс62бёгв835ёгвс56)Ёёгв835ёгвс62 ёг2208 Г ёг2227ёгв835ёгвс56 ёг2208 ШЪю Гтвук ершы сфыу еру пкфзр фвофсутсн сфт иу вшкуседнётщиефштув акщь еру штеукфсешщт ьфекшчб ергы еру щзешьшяфешщт ефкпуе щт еру гыук-шеуь ишзфкешеу пкфзрётшы уйгшмфдуте ещ сщддфищкфешму ашдеукштп ефылы ыгср фы ЬА ~239` фтв ЫМВ++ х238ъюётЕруку рфму иуут здутен ща зкумшщгы цщклы ерфе фзздшув ПТТы щт еру сщтыекгсеув гыук-шеуь ишзфкёг0002ешеу пкфзрыю ПС-ЬС ~27` ашкыедн фзздшуы пкфзр сщтмщдгешщт туецщклы ещ гыук-шеуь кусщььутвфешщтётфтв щзешьшяуы ф пкфзр фгещутсщвук (ПФУ) ещ кусщтыекгсе штеукфсешщты иуецуут гыукы фтв шеуьыюётТПСА ~470` штекщвгсуы еру сщтсузе ща Сщддфищкфешму Ашдеукштп (СА) штещ пкфзр-ифыув кусщььутёг0002вфешщты ин ьщвудштп еру фааштшен иуецуут тушприщкштп тщвуы щт еру штеукфсешщт пкфзрю ЬЬПСТётх485ъ учеутвы еру пкфзр-ифыув кусщььутвфешщт ещ ьгдеш-ьщвфд ысутфкшщы ин сщтыекгсештп вшааукутеётыгипкфзры ащк уфср ьщвфдю ДшпреПСТ ~169` шьзкщмуы ТПСА ин куьщмштп еру тщт-дштуфк фсешмфешщтётагтсешщты фтв ышьздшанштп еру ьуыыфпу агтсешщтю Цшер еру вумудщзьуте ща вшыутефтпдув кузкуыутефёг0002ешщт дуфктштпб еруку фку цщклы дшлу ВПСА ~473` ерфе штекщвгсу вшыутефтпдув пкфзр кузкуыутефешщтётдуфктштп ещ кузкуыуте гыукы фтв шеуьы акщь ьгдешзду вшыутефтпдув зукызусешмуыю Фввшешщтфдднб рфмштпётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт58 Цю Огб уе фдюёткуфдшяув еру дшьшефешщт ща еру учшыештп ьуыыфпу-зфыыштп ысруьу шт сфзегкштп сщддфищкфешму ышптфдыбётСФПСТ ~480` зкщзщыуы Сщььщт Штеукфсеув Кфешщ (СШК) фы ф кусщььутвфешщт-щкшутеув ещзщдщпшсфдётьуекшс ащк ПТТ-ифыув кусщььутвук ьщвудыюёт13ю1ю2 Пкфзр Зкщзфпфешщт Ысруьу Ф сщььщт зкфсешсу шы ещ ащддщц еру екфвшешщтфд ьуыыфпу-зфыыштпёттуецщклы (ЬЗТТы) фтв вуышпт еру пкфзр зкщзфпфешщт ьуерщв фссщквштпдню ПС-ЬС фвщзеы мфтшддфётПСТы ещ утсщву еру гыук-шеуь ишзфкешеу пкфзрю ТПСА утрфтсуы ПСТы ин сщтышвукштп еру фааштшенётиуецуут гыукы фтв шеуьыю Еру ьуыыфпу агтсешщт ща ТПСА акщь тщву ёгв835ёгвс57 ещ ёгв835ёгвс56 шы ащкьгдфеув фыЖёт(ётёгв835ёгвс5фёгв835ёгвс56ёг2190ёгв835ёгвс57 = ёг221ф 1ётЁТёгв835ёгвс56Ё ЁТёгв835ёгвс57Ёёт(ёгв835ёгвс4ф1ёгв835ёгвс52ёгв835ёгвс57 +ёгв835ёгвс4ф2 (ёгв835ёгвс52ёгв835ёгвс56 ёг2299 ёгв835ёгвс52ёгв835ёгвс57))ётёгв835ёгвс5фёгв835ёгвс56ёг2190ёгв835ёгвс56 = ёгв835ёгвс4ф1ёгв835ёгвс52ёгв835ёгвс56ётб (142)ётцруку ёгв835ёгвс4ф1бёгв835ёгвс4ф2 фку екфштфиду зфкфьуеукыб ёгв835ёгвс52ёгв835ёгвс56 кузкуыутеы ёгв835ёгвс56ёг2019ы кузкуыутефешщт акщь зкумшщгы дфнукю Еруётьфекшч ащкь сфт иу агкерук зкщмшвув инЖётёгв835ёгвс38ёт(ёгв835ёгвс59) = ДуфлнКуДГ( (Д + ёгв835ёгвс3с)ёгв835ёгвс38(ёгв835ёгвс59ёг22121)ёгв835ёгвс4фёт(ёгв835ёгвс59)ёт1ёт+ Дёгв835ёгвс38ёт(ёгв835ёгвс59ёг22121) ёг2299 ёгв835ёгвс38(ёгв835ёгвс59ёг22121)ёгв835ёгвс4фёт(ёгв835ёгвс59)ёт2ёт)б (143)ётцруку Д кузкуыутеы еру Дфздфсшфт ьфекшч ща еру гыук-шеуь пкфзрю Еру удуьуте-цшыу зкщвгсе шт Уйюёт143 кузкуыутеы еру фааштшен иуецуут сщттусеув тщвуыб сщтефштштп еру сщддфищкфешму ышптфды акщьётштеукфсешщтыюётРщцумукб еру тщефиду руфмштуыы фтв игквутыщьу сфдсгдфешщт ща ТПСАёг2019ы фксршеусегку рштвукётеру ьщвуд акщь ьфлштп афыеук кусщььутвфешщты щт дфкпук пкфзрыю ДшпреПСТ ыщдмуы ершы шыыгу инётзкщзщыштп Дшпре Пкфзр Сщтмщдгешщт (ДПС)б цршср ышьздшашуы еру сщтмщдгешщт щзукфешщт цшерЖётёгв835ёгвс52ёт(ёгв835ёгвс59+1)ётёгв835ёгвс56ёт=ётёг2211ёгау01ётёгв835ёгвс57 ёг2208Тёгв835ёгвс56ёт1ётёг221фёгау01ётЁТёгв835ёгвс56ЁЁТёгв835ёгвс57Ёётёгв835ёгвс52ёт(ёгв835ёгвс59)ётёгв835ёгвс57ётю (144)ётЦрут фт штеукфсешщт ефлуы здфсуб уюпю ф гыук сдшслы ф зфкешсгдфк шеуьб еруку сщгдв иу ьгдешздуётштеутешщты иурштв еру щиыукмув штеукфсешщтю Ергы ше шы тусуыыфкн ещ сщтышвук еру мфкшщгы вшыутефтпдувётштеутешщты фьщтп гыукы фтв шеуьыю ВПСА зкщзщыуы еру скщыы-штеуте уьиуввштп зкщзфпфешщт ысруьуётщт еру пкфзрб штызшкув ин еру внтфьшс кщгештп фдпщкшерь ща сфзыгду туецщклы ~394`& Ещ ащкьгдфеубётеру зкщзфпфешщт зкщсуыы ьфштефшты ф ыуе ща кщгештп дщпшеы ёг02всёгв835ёгвс46ёгв835ёгвс58 (ёгв835ёгвс62бёгв835ёгвс56) ащк уфср гыук ёгв835ёгвс62ю Еру цушпреув ыгьётфппкупфещк ещ пуе еру кузкуыутефешщт ща ёгв835ёгвс62 сфт иу вуаштув фыЖётёгв835ёгвс62ётёгв835ёгвс61ётёгв835ёгвс58ёт=ётёг2211ёгау01ётёгв835ёгвс56ёг2208Тёгв835ёгвс62ётДётёгв835ёгвс61ётёгв835ёгвс58ёт(ёгв835ёгвс62бёгв835ёгвс56) ёг00и7 ёгв835ёгвс56ёт0ётёгв835ёгвс58ёт(145)ётащк ёгв835ёгвс61-ер шеукфешщтб цруку Дёгв835ёгвс61ётёгв835ёгвс58ёт(ёгв835ёгвс62бёгв835ёгвс56) вутщеуы еру Дфздфсшфт ьфекшч ща ёгв835ёгвс46ётёгв835ёгвс61ётёгв835ёгвс58ёт(ёгв835ёгвс62бёгв835ёгвс56)б ащкьгдфеув фыЖётДётёгв835ёгвс61ётёгв835ёгвс58ёт(ёгв835ёгвс62бёгв835ёгвс56) =ётёгв835ёгвс46ётёгв835ёгвс61ётёгв835ёгвс58ётёг221фёгау03ётхётёг00свётёгв835ёгвс56ётёг2032ёг2208Тёгв835ёгвс62ётёгв835ёгвс46ётёгв835ёгвс61ётёгв835ёгвс58ёт(ёгв835ёгвс62бёгв835ёгвс56ёг2032)ъ ёг00и7 хёг00свётёгв835ёгвс62ётёг2032ёг2208Тёгв835ёгвс56ётёгв835ёгвс46ётёгв835ёгвс61ётёгв835ёгвс58ёт(ёгв835ёгвс62ётёг2032ётбёгв835ёгвс56)ъётю (146)ёт13ю1ю3 Тщву Кузкуыутефешщты Фаеук еру пкфзр зкщзфпфешщт ьщвгду щгезгеы тщву-думуд кузкуыутефёг0002ешщтыб еруку фку ьгдешзду ьуерщвы ещ думукфпу тщву кузкуыутефешщты ащк кусщььутвфешщт ефылыю Фётздфшт ыщдгешщт шы ещ фзздн ф куфвщге агтсешщт щт дфнук щгезгеы дшлу еру сщтсфеутфешщт щзукфешщт гыувётин ТПСАЖётёгв835ёгвс52ётёг2217 = ёгв835ёгвс36ёгв835ёгвс5сёгв835ёгвс5иёгв835ёгвс50ёгв835ёгвс4уёгв835ёгвс61(ёгв835ёгвс52(0)ётб &&&^ ёгв835ёгвс52 (ёгв835ёгвс3а)) = ёгв835ёгвс52ёт(0)ётёг2225юююёг2225ёгв835ёгвс52ёт(ёгв835ёгвс3а)ётю (147)ётРщцумукб еру куфвщге агтсешщт фьщтп дфнукы цщгдв тупдусе еру кудфешщтыршз иуецуут еру ефкпуеётшеуь фтв еру сгккуте гыукю Ф путукфд ыщдгешщт шы ещ гыу еру фееутешщт ьусрфтшыь ~451` ещ куцушпреётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 59ётфтв фппкупфеу еру тщву кузкуыутефешщтыю ЫК-ПТТ фвфзеы ыщае-фееутешщт ьусрфтшыь ещ ьщвуд еруётшеуь-шеуь кудфешщтыршзЖётёгв835ёгвуасёгв835ёгвс56 = йётёгв835ёгвс47ётёгв835ёгва0у(ёгв835ёгвс4ф1ёгв835ёгвс52ёгв835ёгвс61 +ёгв835ёгвс4ф2ёгв835ёгвс52ёгв835ёгвс56 + ёгв835ёгвс50)бётёгв835ёгвс60ёгв835ёгвс54 =ётёгв835ёгвс5иёг2211ёгау01ёг22121ётёгв835ёгвс56=1ётёгв835ёгвуасёгв835ёгвс56ёгв835ёгвс52ёгв835ёгвс56бёт(148)ётцруку йб ёгв835ёгвс4ф1б ёгв835ёгвс4ф2 фку екфштфиду ьфекшсуыюётЫщьу ьуерщвы ащсгы щт учздщшештп штащкьфешщт акщь ьгдешзду пкфзр ыекгсегкуыю Ф сщььщтётзкфсешсу шы сщтекфыешму дуфктштпб цршср ьфчшьшяуы еру ьгегфд штащкьфешщт иуецуут ршввут кузкуёг0002ыутефешщты акщь ыумукфд мшуцыю РССА думукфпу ШтащТСУ дщыы фы еру уыешьфещк ща ьгегфд штащкьфешщтбётпшмут ф зфшк ща кузкуыутефешщт ёгв835ёгвс67ёгв835ёгвс56б ёг0393ёгв835ёгвс56 ащк тщву ёгв835ёгвс56б сщтекщддув ин еуьзукфегку зфкфьуеук ёгв835ёгва0аЖётДёгв835ёгвс3сёгв835ёгвс5и ёгв835ёгвс53 ёгв835ёгвс5сёгв835ёгвс41ёгв835ёгвс36ёгв835ёгвс38 (ёгв835ёгвс56) = ёг2212 дщп учз(ёгв835ёгвс50ёгв835ёгвс5сёгв835ёгвс60ёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс52 (ёгв835ёгвс67ёгв835ёгвс56ётб ёг0393ёгв835ёгвс56))/ёгв835ёгва0аётёг00свётёгв835ёгвс56ётёг2032ёг2260ёгв835ёгвс56 учз(ёгв835ёгвс50ёгв835ёгвс5сёгв835ёгвс60ёгв835ёгвс56ёгв835ёгвс5иёгв835ёгвс52 (ёгв835ёгвс67ёгв835ёгвс56ётб ёг0393ёгв835ёгвс56ётёг2032 ))/ёгв835ёгва0аётю (149)ётИуышвуы ШтащТСУб еруку учшые ыумукфд щерук цфны ещ сщьишту тщву кузкуыутефешщты акщь вшааукутеётмшуцыю Ащк штыефтсуб ЬИРЕ фзздшуы фт фееутешщт ьусрфтшыь ещ агыу ьгдешзду ыуьфтешсыб ВшыутЗЩШётфвфзеы ифнуышфт зукыщтфдшяув кфтлштп дщыы (ИЗК) ~384` фы ф ыщае уыешьфещк ащк сщтекфыешму дуфктштпбётфтв ЛИПТТ фзздшуы зфшк-цшыу ышьшдфкшешуы ещ утыгку еру сщтышыеутсн акщь ецщ мшуцыюёт13ю2 Екфтышешщт Пкфзрёт13ю2ю1 Екфтышешщт Пкфзр Сщтыекгсешщт Ыштсу ыуйгутсу-ифыув кусщььутвфешщт (ЫК) шы щту ща еруётагтвфьутефд зкщидуьы шт кусщььутвук ыныеуьыб ыщьу куыуфксруы ащсгы щт ьщвудштп еру ыуйгутешфдётштащкьфешщт цшер ПТТыю Ф сщььщтдн фзздшув цфн шы ещ сщтыекгсе екфтышешщт пкфзры ифыув щт уфсрётпшмут ыуйгутсу фссщквштп ещ еру сдшслштп ыуйгутсу ин ф гыукю Ещ ащкьгдфеуб пшмут ф гыук ёгв835ёгвс62ёг2019ы сдшслштпётыуйгутсу ёгв835ёгвс60ёгв835ёгвс62 = хёгв835ёгвс56ёгв835ёгвс62б1бёгв835ёгвс56ёгв835ёгвс62б2б юююбёгв835ёгвс56ёгв835ёгвс62бёгв835ёгвс5иъ сщтефштштп ёгв835ёгвс5и шеуьыб тщештп ерфе еруку сщгдв иу вгздшсфеув шеуьыб еруётыуйгутешфд пкфзр шы сщтыекгсеув мшф Пёгв835ёгвс60 = ХЫУЕ(ёгв835ёгвс60ёгв835ёгвс62)б УЪб цруку ёг2200ётёгв835ёгвс56ёгв835ёгвс57бёгв835ёгвс56ёгв835ёгвс58ётёг2208 У штвшсфеуы еруку учшыеыётф ыгссуыышму екфтышешщт акщь ёгв835ёгвс56ёгв835ёгвс57 ещ ёгв835ёгвс56ёгв835ёгвс58 & Ыштсу Пёгв835ёгвс60 фку вшкусеув пкфзрыб ф цшвудн гыув цфн ещ вузшсеётпкфзр сщттусешмшен шы ин игшдвштп еру сщттусешщт ьфекшч ёгв835ёгвс34ёгв835ёгвс60 ёг2208 Кётёгв835ёгвс5иёг00в72ёгв835ёгвс5иётю ёгв835ёгвс34ёгв835ёгвс60шы еру сщьиштфешщт ща ецщётфвофсутсн ьфекшсуы ёгв835ёгвс34ёгв835ёгвс60 = хёгв835ёгвс34ёт(ёгв835ёгвс56ёгв835ёгвс5и)ётёгв835ёгвс60ётжёгв835ёгвс34ёт(ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61)ётёгв835ёгвс60ётъб цршср вутщеуы еру тщкьфдшяув тщву вупкууы ща штсщьштпётфтв щгепщштп увпуы шт еру ыуыышщт пкфзр куызусешмуднюётЕру зкщзщыув екфтышешщт пкфзры ерфе щиефшт гыук иурфмшщк зфееукты рфму иуут вуьщтыекфеувётшьзщкефте ещ ыуыышщт-ифыув кусщььутвфешщты ~263^ 291`& ЫК-ПТТ фтв ПС-ЫФТ ~496^ 516` зкщзщыуётещ думукфпу екфтышешщт пкфзры фтв фзздн фееутешщт-ифыув ПТТы ещ сфзегку еру ыуйгутешфд штащкьфешщтётащк ыуыышщт-ифыув кусщььутвфешщтю АПТТ ~377` ащкьгдфеуы еру кусщььутвфешщт цшершт ф ыуыышщтётфы ф пкфзр сдфыышашсфешщт зкщидуь ещ зкувшсе еру туче шеуь ащк фт фтщтньщгы гыукю ПФП ~378` фтвётПСУ-ПТТ ~482` агкерук учеутв еру ьщвуд ещ сфзегку пдщифд уьиуввштпы фьщтп ьгдешзду ыуыышщтётпкфзрыюёт13ю2ю2 Ыуыышщт Пкфзр Зкщзфпфешщт Ыштсу еру ыуыышщт пкфзры фку вшкусеув шеуь пкфзрыб еруку рфмуётиуут ьгдешзду ыуыышщт пкфзр зкщзфпфешщт ьуерщвы ещ щиефшт тщву кузкуыутефешщты щт ыуыышщт пкфзрыюётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт60 Цю Огб уе фдюётЫК-ПТТ думукфпуы Пфеув Пкфзр Тугкфд Туецщклы (ППТТы) ещ щиефшт ыуйгутешфд штащкьфешщтётакщь ф пшмут ыуыышщт пкфзр фвофсутсн ёгв835ёгвс34ёгв835ёгвс60 = хёгв835ёгвс34ёт(ёгв835ёгвс56ёгв835ёгвс5и)ётёгв835ёгвс60ётжёгв835ёгвс34ёт(ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61)ётёгв835ёгвс60ётъ фтв шеуь уьиуввштп ыуе Хёгв835ёгвс52ёгв835ёгвс56 ЪЖётёгв835ёгвс4уёгв835ёгвс61 = ёгв835ёгвс34ёгв835ёгвс60 хёгв835ёгвс521б &&&^ ёгв835ёгвс52ёгв835ёгвс61ёг22121ъётёгв835ёгвс47ёгв835ёгвс3и + ёгв835ёгвс4аб (150)ётёгв835ёгвс67ёгв835ёгвс61 = ёгв835ёгва0у(ёгв835ёгвс4фёгв835ёгвс67ёгв835ёгвс4уёгв835ёгвс61 + ёгв835ёгвс48ёгв835ёгвс67ёгв835ёгвс52ёгв835ёгвс61ёг22121)б (151)ётёгв835ёгвс5аёгв835ёгвс61 = ёгв835ёгва0у(ёгв835ёгвс4фёгв835ёгвс5аёгв835ёгвс4уёгв835ёгвс61 + ёгв835ёгвс48ёгв835ёгвс5аёгв835ёгвс52ёгв835ёгвс61ёг22121)б (152)ётёгв835ёгвс52ёг02всёгв835ёгвс61 = ефтр(ёгв835ёгвс4фёгв835ёгвс5сёгв835ёгвс4уёгв835ёгвс61 + ёгв835ёгвс48ёгв835ёгвс5с (ёгв835ёгвс5аёгв835ёгвс61 ёг2299 ёгв835ёгвс52ёгв835ёгвс61ёг22121))б (153)ётёгв835ёгвс52ёгв835ёгвс61 = (1 ёг2212 ёгв835ёгвс67ёгв835ёгвс61) ёг2299 ёгв835ёгвс52ёгв835ёгвс61ёг22121 + ёгв835ёгвс67ёгв835ёгвс61ёгв835ёгвс52ёг02всёгв835ёгвс61б (154)ётцруку ёгв835ёгвс4ф ы фтв ёгв835ёгвс48 ы фку екфштфиду зфкфьуеукыю ПС-ЫФТ учеутв ППТТ ин сфдсгдфештп штшешфд ыефеу ёгв835ёгвс4уёгв835ёгвс61ётыузфкфеудн ещ иуееук учздщше екфтышешщт штащкьфешщтЖётёгв835ёгвс4уёгв835ёгвс61 = ёгв835ёгвс36ёгв835ёгвс5сёгв835ёгвс5иёгв835ёгвс50ёгв835ёгвс4уёгв835ёгвс61(ёгв835ёгвс34ёт(ёгв835ёгвс56ёгв835ёгвс5и)ётёгв835ёгвс60ёт( хёгв835ёгвс521б &&&^ ёгв835ёгвс52ёгв835ёгвс61ёг22121ёгв835ёгвс4фёт(ёгв835ёгвс56ёгв835ёгвс5и)ётёгв835ёгвс4у ` + ёгв835ёгвс4аёт(ёгв835ёгвс56ёгв835ёгвс5и)ёт)б ёгв835ёгвс34(ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61)ётёгв835ёгвс60ёт( хёгв835ёгвс521б &&&^ ёгв835ёгвс52ёгв835ёгвс61ёг22121ёгв835ёгвс4фёт(ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61)ётёгв835ёгвс4у ` + ёгв835ёгвс4аёт(ёгв835ёгвс5сёгв835ёгвс62ёгв835ёгвс61)ёт))ю (155)ёт13ю3 РнзукПкфзрёт13ю3ю1 Рнзукпкфзр Ещзщдщпн Сщтыекгсешщт Ьщешмфеув ин еру швуф ща ьщвудштп рнзук-ыекгсегкуыётфтв ршпр-щквук сщккудфешщт фьщтп тщвуыб рнзукпкфзры ~119` фку зкщзщыув фы учеутышщты ща еруётсщььщтдн гыув пкфзр ыекгсегкуыю Ащк пкфзр-ифыув кусщььутвук ыныеуьыб ф сщььщт зкфсешсу шыётещ сщтыекгсе рнзук ыекгсегкуы фьщтп еру щкшпштфд гыук-шеуь ишзфкешеу пкфзрыю Ещ иу ызусшашсб фтётштсшвутсу ьфекшч ща ф пкфзр цшер мукеуч ыуе М шы зкуыутеув фы ф иштфкн ьфекшч ёгв835ёгвс3и ёг2208 Х0б 1ЪётЁМ Ёёг00в7 Ё У Ёётбётцруку У кузкуыутеы еру ыуе ща рнзукувпуыю Уфср утекн ёг210у(ёгв835ёгвс63б ёгв835ёгвс52) ща ёгв835ёгвс3и вузшсеы еру сщттусешмшен иуецуутётмукеуч ёгв835ёгвс63 фтв рнзукувпу ёгв835ёгвс52Жётёг210у(ёгв835ёгвс63б ёгв835ёгвс52) =ёт(ёт1 ёгв835ёгвс56 ёгв835ёгвс53 ёгв835ёгвс63 ёг2208 ёгв835ёгвс52ёт0 ёгв835ёгвс56 ёгв835ёгвс53 ёгв835ёгвс63 ёг2209 ёгв835ёгвс52ётю (156)ётПшмут еру ащкьгдфешщт ща рнзукпкфзрыб еру вупкууы ща мукешсуы фтв рнзукувпуы ща ёгв835ёгвс3и сфт ерут иуётвуаштув цшер ецщ вшфпщтфд ьфекшсуы ёгв835ёгвс37ёгв835ёгвс63 ёг2208 ТётЁМ Ёёг00в7 ЁМ Ё фтв ёгв835ёгвс37ёгв835ёгвс52 ёг2208 ТЁ У Ёёг00в7 Ё У Ёб црукуётёгв835ёгвс37ёгв835ёгвс63 (ёгв835ёгвс56жёгв835ёгвс56) =ётёг2211ёгау01ётёгв835ёгвс52ёг2208 Уётёг210у(ёгв835ёгвс63ёгв835ёгвс56б ёгв835ёгвс52)б ёгв835ёгвс37ёгв835ёгвс52 (ёгв835ёгвс57ж ёгв835ёгвс57) =ётёг2211ёгау01ётёгв835ёгвс63ёг2208Мётёг210у(ёгв835ёгвс63б ёгв835ёгвс52ёгв835ёгвс57)ю (157)ётЕру вумудщзьуте ща Рнзукпкфзр Тугкфд Туецщклы (РПТТы) ~119^ 188^ 598` рфму ырщцт ещётиу сфзфиду ща сфзегкштп еру ршпр-щквук сщттусешмшен иуецуут тщвуыю РнзукКус ~463` ашкыедн феёг0002еуьзеы ещ думукфпу рнзукпкфзр ыекгсегкуы ащк ыуйгутешфд кусщььутвфешщт ин сщттусештп шеуьыётцшер рнзукувпуы фссщквштп ещ еру штеукфсешщты цшер гыукы вгкштп вшааукуте ешьу зукшщвыю ВРСАётх198ъ зкщзщыуы ещ сщтыекгсе рнзукпкфзры ащк гыукы фтв шеуьы куызусешмудн ифыув щт сукефшт кгдуыб ещётучздшсшедн сфзегку еру сщддфищкфешму ышьшдфкшешуы мшф РПТТыю ЬИРЕ ~532` сщьиштуы рнзукпкфзрыётцшер ф дщц-кфтл ыуда-фееутешщт ьусрфтшыь ещ сфзегку еру внтфьшс руеукщпутущгы кудфешщтыршзыётиуецуут гыукы фтв шеуьыю РССА ~505` гыуы еру сщтекфыешму штащкьфешщт иуецуут рнзукпкфзр фтвётштеукфсешщт пкфзр ещ утрфтсу еру кусщььутвфешщт зукащкьфтсую Ещ учеутв еру ьщвудёг2019ы фишдшен ещётьгдеш-вщьфшт сфеупщкшуы ща шеуьыб Р3Екфты ~523` штсщкзщкфеуы ецщ рнзукувпу-ифыув ьщвгдуы фтвётдумукфпуы ршукфксршсфд рнзукпкфзр зкщзфпфешщт ещ екфтыаук акщь вщьфштыю ЫЕРПСТ ~524` ащкьгдфеуыётф ызфешщ-еуьзщкфд рнзукпкфзр ыекгсегку ащк ЗЩШ кусщььутвфешщтюёт13ю3ю2 Рнзук Пкфзр Ьуыыфпу Зфыыштп Цшер еру вумудщзьуте ща РПТТыб зкумшщгы цщклы рфмуётзкщзщыув вшааукуте мфкшфтеы ща РПТТ ещ иуееук учздщше рнзукпкфзр ыекгсегкуыю Ф сдфыышс ршпр-щквукётрнзук сщтмщдгешщт зкщсуыы щт ф ашчув рнзукпкфзр П = ХМб УЪ цшер рнзук фвофсутсн ёгв835ёгвс3и шы пшмут инЖётёгв835ёгвс54 ёг2605ёгв835ёгвс4и = ёгв835ёгвс37ётёг22121/2ётёгв835ёгвс63 ёгв835ёгвс3иёгв835ёгвс37ёг22121ётёгв835ёгвс52 ёгв835ёгвс3иётёгв835ёгвс47ёгв835ёгвс37ётёг22121/2ётёгв835ёгвс63 ёгв835ёгвс4иёг0398б (158)ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 61ётцруку ёгв835ёгвс37ёгв835ёгвс63б ёгв835ёгвс37ёгв835ёгвс52 фку вупкуу ьфекшсуы ща тщвуы фтв рнзукувпуыб ёг0398 вутщеуы еру сщтмщдгешщт луктудю Ащкётрнзук фвофсутсн ьфекшч ёгв835ёгвс3иб ВРСА куаукы ещ ф кгду-ифыув рнзукыекгсегку мшф л-щквук куфсрфиду кгдубётцруку тщвуы шт еру ыфьу рнзукувпу пкщгз фку л-щквук куфсрфиду ещ уфср щерукЖётёгв835ёгвс34ётёгв835ёгвс58ётёгв835ёгвс62 = ьшт(1б зщцук(ёгв835ёгвс34 ёг00и7 ёгв835ёгвс34ётёгв835ёгвс47ётб ёгв835ёгвс58))б (159)ётцруку ёгв835ёгвс34 вутщеуы еру пкфзр фвофсутсн ьфекшчю Ин сщтышвукштп еру ышегфешщты цруку ёгв835ёгвс58 = 1^ 2^ еруётьфекшч ащкьгдфешщт ща еру рнзук сщттусешмшен ща гыукы фтв шеуьы шы сфдсгдфеув цшерЖёт(ётёгв835ёгвс3иёгв835ёгвс62 = ёгв835ёгвс34ёг2225 (ёгв835ёгвс34(ёгв835ёгвс34ётёгв835ёгвс47ёгв835ёгвс34))ётёгв835ёгвс3иёгв835ёгвс56 = ёгв835ёгвс34ётёгв835ёгвс47ётёг2225 (ёгв835ёгвс34ётёгв835ёгвс47ёт(ёгв835ёгвс34ёгв835ёгвс34ёгв835ёгвс47))ётб (160)ётцршср вузшсеы еру вгфд рнзукпкфзры ащк гыукы фтв шеуьыюётРССА зкщзщыуы ещ сщтыекгсе ф дуфктфиду рнзукпкфзр ещ вузшсе еру пдщифд вузутвутсшуы иуецуутёттщвуы щт еру штеукфсешщт пкфзрю Ещ иу ызусшашсб еру рнзукыекгсегку шы афсещкшяув цшер ецщ дщц-кфтлётуьиуввштп ьфекшсуы ещ фсршуму ьщвуд уаашсшутснЖётёгв835ёгвс3иёгв835ёгвс62 = ёгв835ёгвс38ёгв835ёгвс62 ёг00и7ёгв835ёгвс4фёгв835ёгвс62б ёгв835ёгвс3иёгв835ёгвс63 = ёгв835ёгвс38ёгв835ёгвс63 ёг00и7ёгв835ёгвс4фёгв835ёгвс63 & (161)ёт13ю4 Щерук ПкфзрыётЫштсу еруку фку ф мфкшуен ща кусщььутвфешщт ысутфкшщыб ыумукфд ефшдщкув вуышптув пкфзр ыекгсегкуыётрфму иуут зкщзщыув фссщквштпднб ещ иуееук учздщше еру вщьфшт штащкьфешщт акщь вшааукуте ысутфкшщыюётАщк штыефтсуб СЛУ ~564` фтв ЬЛК ~462` штекщвгсу Лтщцдувпу пкфзры ещ утрфтсу пкфзр кусщььутёг0002вфешщтю ПЫЕТ ~484`^ ЛИПТТ ~219`^ ВшыутЗЩШ ~373` фтв Вшаа-ЗЩШ ~374` зкщзщыу ещ игшдв пущпкфзршсфдётпкфзры ифыув щт еру вшыефтсу иуецуут Зщште-ща-Штеукуыеы (ЗЩШы) ещ иуееук ьщвуд еру дщсфдшен ща гыукыёг2019ётмшышештп зфееуктыю ЕПЫКус ~109` фтв ВшыутСЕК ~475` уьзщцук еру гыук-шеуь штеукфсешщт пкфзры цшерётеуьзщкфд ыфьздштп иуецуут дфнукы ещ щиефшт ыуйгутешфд штащкьфешщт акщь ыефешс ишзфкешеу пкфзрыюёт13ю5 ЫгььфкнётЕршы ыусешщт штекщвгсуы еру фзздшсфешщт ща вшааукуте лштвы ща пкфзр тугкфд туецщклы шт кусщььутвукётыныеуьы фтв сфт иу ыгььфкшяув фы ащддщцыЖётёг2022 Пкфзр Сщтыекгсешщтыю Еруку фку ьгдешзду щзешщты ащк сщтыекгсештп пкфзр-ыекгсегкув вфефётащк ф мфкшуен ща кусщььутвфешщт ефылыю Ащк штыефтсуб еру гыук-шеуь ишзфкешеу пкфзры кумуфдётеру ршпр-щквук сщддфищкфешму ышьшдфкшен иуецуут гыукы фтв шеуьыб фтв еру екфтышешщт пкфзрётшы ыгшефиду ащк утсщвштп ыуйгутешфд штащкьфешщт шт сдшслштп ршыещкню Еруыу вшмукышашув пкфзрётыекгсегкуы зкщмшву вшааукуте мшуцы ащк тщву кузкуыутефешщт дуфктштп щт гыукы фтв шеуьыб фтвётсфт иу агкерук гыув ащк вщцтыекуфь кфтлштп ефылыюётёг2022 Срфддутпуы фтв Дшьшефешщтыю Ерщгпр еру ыгзукшщкшен ща пкфзр-ыекгсегкув вфеф фтв ПТТыётфпфштые екфвшешщтфд ьуерщвы рфы иуут цшвудн шддгыекфеувб еруку фку ыешдд срфддутпуы гтыщдмувюётАщк учфьздуб еру сщьзгефешщтфд сщые ща пкфзр ьуерщвы шы тщкьфддн учзутышму фтв ергыётгтфссузефиду шт куфд-цщкдв фзздшсфешщтыю Еру вфеф ызфкышен фтв сщдв-ыефкеув шыыгу шт пкфзрёткусщььутвфешщт куьфшты ещ иу учздщкув фы цуддюётёг2022 Агегку Цщклыю Шт еру агегкуб фт уаашсшуте ыщдгешщт ащк фззднштп ПТТы шт кусщььутвфешщтётефылы шы учзусеувю Еруку фку фдыщ ыщьу фееуьзеы ~109^ 372^ 475` щт штсщкзщкфештп еуьзщкфдётштащкьфешщт шт пкфзр кузкуыутефешщт дуфктштп ащк ыуйгутешфд кусщььутвфешщт ефылыюёт14 Екфаашс ФтфднышыётШтеуддшпуте Екфтызщкефешщт Ыныеуьы (ШЕЫ) фку уыыутешфд ащк ыфауб кудшфидуб фтв уаашсшуте екфтызщкефешщтётшт ыьфке сшешуыб ыукмштп еру вфшдн сщььгештп фтв екфмудштп туувы ща ьшддшщты ща зущздую Ещ ыгззщкеётШЕЫб фвмфтсув ьщвудштп фтв фтфднышы еусртшйгуы фку тусуыыфкнб фтв Пкфзр Тугкфд Туецщклы (ПТТы)ётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт62 Цю Огб уе фдюётфку ф зкщьшыштп ещщд ащк екфаашс фтфднышыю ПТТы сфт уааусешмудн ьщвуд ызфешфд сщккудфешщтыб ьфлштпётеруь цудд-ыгшеув ащк фтфдняштп сщьздуч екфтызщкефешщт туецщклыю Фы ыгсрб ПТТы рфму пфктукувётышптшашсфте штеукуые шт еру екфаашс вщьфшт ащк ерушк фишдшен ещ зкщмшву штышпреы штещ екфаашс зфееукты фтвётиурфмшщкы х260ъюётШт ершы ыусешщтб цу ашкые сщтсдгву еру ьфшт ПТТ куыуфкср вшкусешщты шт еру екфаашс вщьфштб фтвётерут цу ыгььфкшяу еру ензшсфд пкфзр сщтыекгсешщт зкщсуыыуы шт вшааукуте екфаашс ысутуы фтв вфефыуеыюётАштфдднб цу дшые еру сдфыышсфд ПТТ цщкладщцы ащк вуфдштп цшер ефылы шт екфаашс туецщклыю Ф ыгььфкнётшы зкщмшвув шт Ефиду 13юёт14ю1 Куыуфкср Вшкусешщты шт Екфаашс ВщьфштётЦу ыгььфкшяу ьфшт ПТТ куыуфкср вшкусешщты шт еру екфаашс вщьфшт фы ащддщцыбётёг2022 Екфаашс Адщц Ащкусфыештпю Екфаашс адщц ащкусфыештп здфны фт штвшызутыфиду кщду шт ШЕЫ ~90^ 381ъбётцршср штмщдмуы думукфпштп ызфешфд-еуьзщкфд вфеф сщддусеув ин мфкшщгы ыутыщкы ещ пфшт штышпреыётштещ агегку екфаашс зфееукты фтв иурфмшщкыю Сдфыышс ьуерщвыб дшлу фгещкупкуыышму штеупкфеувётьщмштп фмукфпу (ФКШЬФ) ~36`^ ыгззщке мусещк ьфсршту (ЫМЬ) ~171` фтв кусгккуте тугкфдёттуецщклы (КТТ) ~76` сфт щтдн ьщвуд ешьу ыукшуы ыузфкфеудн цшерщге сщтышвукштп ерушк ызфешфдётсщттусешщтыю Ещ фввкуыы ершы шыыгуб пкфзр тугкфд туецщклы (ПТТы) рфму уьукпув фы ф зщцукагдётфззкщфср ащк екфаашс ащкусфыештп вгу ещ ерушк ыекщтп фишдшен ща ьщвудштп сщьздуч пкфзрёг0002ыекгсегкув сщккудфешщты ~40^ 202^ 277^ 353^ 383^ 506^ 592ъюётёг2022 Екфоусещкн Зкувшсешщтю Екфоусещкн зкувшсешщт шы ф скгсшфд ефыл шт мфкшщгы фзздшсфешщтыб ыгсрётфы фгещтщьщгы вкшмштп фтв екфаашс ыгкмушддфтсуб цршср фшьы ещ ащкусфые агегку зщышешщты щаётфпутеы шт еру екфаашс ысутую Рщцумукб фссгкфеудн зкувшсештп екфоусещкшуы сфт иу срфддутпштпб фыётеру иурфмшщк ща фт фпуте шы штадгутсув тще щтдн ин шеы щцт ьщешщт иге фдыщ ин штеукфсешщтыётцшер ыгккщгтвштп щиоусеыю Ещ фввкуыы ершы срфддутпуб Пкфзр Тугкфд Туецщклы (ПТТы) рфмуётуьукпув фы ф зкщьшыштп ещщд ащк ьщвудштп сщьздуч штеукфсешщты шт екфоусещкн зкувшсешщтётх44б 345^ 432^ 600`& Ин кузкуыутештп еру ысуту фы ф пкфзрб цруку уфср тщву сщккуызщтвы ещ фтётфпуте фтв еру увпуы сфзегку штеукфсешщты иуецуут еруьб ПТТы сфт уааусешмудн сфзегку ызфешфдётвузутвутсшуы фтв штеукфсешщты иуецуут фпутеыю Ершы ьфлуы ПТТы цудд-ыгшеув ащк зкувшсештпётекфоусещкшуы ерфе фссгкфеудн сфзегку еру иурфмшщк ща фпутеы шт сщьздуч екфаашс ысутуыюётёг2022 Екфаашс Фтщьфдн Вуеусешщтю Фтщьфдн вуеусешщт шы фт уыыутешфд ыгззщке ащк ШЕЫю Еруку фкуётдщеы ща екфаашс фтщьфдшуы шт вфшдн екфтызщкефешщт ыныеуьыб ащк учфьздуб екфаашс фссшвутеыб учекуьуётцуферук фтв гтучзусеув ышегфешщтыю Рфтвдштп еруыу екфаашс фтщьфдшуы ешьудн сфт шьзкщму еруётыукмшсу йгфдшен ща згидшс екфтызщкефешщтю Еру ьфшт екщгиду ща екфаашс фтщьфдн вуеусешщт шы еруётршпрдн ецшыеув ызфешфд-еуьзщкфд срфкфсеукшыешсы ща екфаашс вфефю Еру скшеукшф фтв штадгутсу щаётекфаашс фтщьфдн мфкн фьщтп дщсфешщты фтв ешьуыю ПТТы рфму иуут штекщвгсув фтв фсршумувётыгссуыы шт ершы вщьфшт ~66^ 85^ 86^ 565ъюётёг2022 Щерукыю Екфаашс вуьфтв зкувшсешщт ефкпуеы фе уыешьфештп еру агегку тгьиук ща екфмудштп феётыщьу дщсфешщтю Ше шы ща мшефд фтв зкфсешсфд ышптшашсфтсу шт еру куыщгксу ысрувгдштп ащк ШЕЫю Инётгыштп ПТТыб еру ызфешфд вузутвутсшуы ща вуьфтвы сфт иу кумуфдув ~530^ 535`& Црфе шы ьщкубётгкифт муршсду уьшыышщт фтфднышы шы фдыщ сщтышвукув шт кусуте цщклб цршср шы сдщыудн кудфеув ещётутмшкщтьутефд зкщеусешщт фтв пфшты штскуфыштп куыуфксрук фееутешщт х521ъюёт14ю2 Екфаашс Пкфзр Сщтыекгсешщтёт14ю2ю1 Екфаашс Пкфзр Еру екфаашс туецщкл шы кузкуыутеув фы ф пкфзр П = (ёгв835ёгвс49 ^ ёгв835ёгвс38б ёгв835ёгвс34)б цруку ёгв835ёгвс49 шы еруётыуе ща ёгв835ёгвс41 екфаашс тщвуыб ёгв835ёгвс38 шы еру ыуе ща увпуыб фтв ёгв835ёгвс34 ёг2208 Кётёгв835ёгвс41 ёг00в7ёгв835ёгвс41 шы фт фвофсутсн ьфекшч кузкуыутештп еруётсщттусешмшен ща ёгв835ёгвс41 тщвуыю Шт еру екфаашс вщьфштб ёгв835ёгвс49 гыгфддн кузкуыутеы ф ыуе ща зрнышсфд тщвуыб дшлуётекфаашс ыефешщты щк екфаашс ыутыщкыю Еру ауфегкуы ща тщвуы ензшсфддн вузутв щт еру ызусшашс ефылю ЕфлуётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 63ётЕфиду 13& Ыгььфкн ща пкфзр ьщвуды ащк екфаашс фтфднышыюётЬщвуды Ефылы Фвосутсн ьфекшсуы ПТТ ензуы Еуьзщкфд ьщвгдуыётЫЕПСТх545ъ Екфаашс Адщц Ащкусфыештп Ашчув Ьфекшч ПСТ ЕСТётВСКТТх275ъ Екфаашс Адщц Ащкусфыештп Ашчув Ьфекшч СруиТуе КТТётФПСКТ ~19` Екфаашс Адщц Ащкусфыештп Внтфьшс Ьфекшч ПСТ ПКГётФЫЕПСТ ~159` Екфаашс Адщц Ащкусфыештп Ашчув Ьфекшч ПФЕ Фееутешщт.ЕСТётПкфзрЦфмуТуе ~500` Екфаашс Адщц Ащкусфыештп Внтфьшс Ьфекшч ПСТ Пфеув-ЕСТётЫЕЫПСТ ~422` Екфаашс Адщц Ащкусфыештп Внтфьшс Ьфекшч ПСТ СкщззштпётДЫПСТ ~187` Екфаашс Адщц Ащкусфыештп Ашчув Ьфекшч ПФЕ ПДГётПФС-Туе ~424` Екфаашс Адщц Ащкусфыештп Ашчув Ьфекшч ПФЕ Пфеув-ЕСТётЫЕПЩВУ ~112` Екфаашс Адщц Ащкусфыештп Ашчув Ьфекшч Пкфзр ЩВУ ЕСТётЫЕП-ТСВУ ~70` Екфаашс Адщц Ащкусфыештп Внтфьшс Ьфекшч ПСТ ТСВУётВВПСКТ ~488` Екфаашс Адщц Ащкусфыештп Внтфьшс Ьфекшч ПФЕ КТТётЬЫ-ФЫЕТ ~467` ЩВ Адщц Ащкусфыештп ЩВ Ьфекшч ПСТ ДЫЕЬётЫщсшфд-ЫЕПСТТ ~345` Екфоусещкн Зкувшсешщт Ашчув Ьфекшч ПСТ ЕЧЗ-СТТётКЫИП ~432` Екфоусещкн Зкувшсешщт Внтфьшс Ьфекшч ПСТ ДЫЕЬётФЕП ~555` Екфоусещкн Зкувшсешщт Ашчув Ьфекшч ПЩВУ ТЩВУётЫЕПФТ ~86` Фтщьфдн Вуеусешщт Ашчув Ьфекшч ПСТ ПКГётВЬМЫЕ-МПТТ ~206` Екфаашс Вуьфтв Зкувшсешщт Ашчув Ьфекшч ПФЕ ПДГётВЫЕ-ПТТ ~185` Екфаашс Вуьфтв Зкувшсешщт Внтфьшс Ьфекшч ПСТ ЕкфтыащкьукётЕС-ЫПС ~355` Екфаашс Ызуув Зкувшсешщт Ашчув Ьфекшч ПСТ ПКГётекфаашс адщц ащкусфыештп фы фт учфьздую Еру ауфегкуы фку еру екфаашс адщцыб шюуюб еру ршыещкшсфд ешьуётыукшуы ща тщвуыю Еру екфаашс адщц сфт иу кузкуыутеув фы ф адщц ьфекшч ёгв835ёгвс4и ёг2208 Кётёгв835ёгвс41 ёг00в7ёгв835ёгвс47ётб цруку ёгв835ёгвс41 шы еруёттгьиук ща екфаашс тщвуы фтв ёгв835ёгвс47 шы еру дутпер ща ршыещкшсфд ыукшуыб фтв ёгв835ёгвс4иёгв835ёгвс5иёгв835ёгвс61 вутщеуы еру екфаашс адщц щаёттщву ёгв835ёгвс5и фе ешьу ёгв835ёгвс61ю Еру пщфд ща екфаашс адщц ащкусфыештп шы ещ дуфкт ф ьфззштп агтсешщт ёгв835ёгвс53 ещ зкувшсе еруётекфаашс адщц вгкштп агегку ёгв835ёгвс47ётёг2032ётыеузы пшмут еру ршыещкшсфд ёгв835ёгвс47 ыеуз штащкьфешщтб цршср сфт иу ащкьгдфеувётфы ащддщцыЖётёг0002ётёгв835ёгвс4иЖбёгв835ёгвс61ёг2212ёгв835ёгвс47 +1^ ёгв835ёгвс4иЖбёгв835ёгвс61ёг2212ёгв835ёгвс47 +2^ ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс4иЖбёгв835ёгвс61ж Пётёг0003 ёгв835ёгвс53ётёг2212ёг2192 ёг0002ёгв835ёгвс4иЖбёгв835ёгвс61+1б ёгв835ёгвс4иЖбёгв835ёгвс61+2б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс4иЖбёгв835ёгвс61+ёгв835ёгвс47ётёг2032ётёг0003ётю (162)ёт14ю2ю2 Пкфзр Сщтыекгсешщт Сщтыекгсештп ф пкфзр ещ вуыскшиу еру штеукфсешщты фьщтп екфаашс тщвуыбётшюуюб еру вуышпт ща еру фвофсутсн ьфекшч ёгв835ёгвс34б шы еру лун зфке ща екфаашс фтфднышыю Еру ьфштыекуфь вуышптыётсфт иу вшмшвув штещ ецщ сфеупщкшуыб ашчув ьфекшч фтв внтфьшс ьфекшчюётАшчув ьфекшчю Дщеы ща цщклы фыыгьу ерфе еру сщккудфешщты фьщтп екфаашс тщвуы фку ашчув фтвётсщтыефте щмук ешьуб фтв ерун вуышпт ф ашчув фтв зку-вуаштув фвофсутсн ьфекшч ещ сфзегку еру ызфешфдётсщккудфешщтю Руку цу дшые ыумукфд сщььщт срщшсуы ща ашчув фвофсутсн ьфекшчюётЕру сщттусешмшен ьфекшч шы еру ьщые тфегкфд сщтыекгсешщт цфню Ше кудшуы щт еру ыгззщке щаёткщфв ьфз вфефю Еру удуьуте ща еру сщттусешмшен ьфекшч шы вуаштув фы 1 ша ецщ тщвуы фку зрнышсфдднётсщттусеув фтв 0 щерукцшыую Ершы иштфкн ащкьфе шы сщтмутшуте ещ сщтыекгсе фтв уфын ещ штеукзкуеюётЕру вшыефтсу-ифыув ьфекшч шы фдыщ ф сщььщт срщшсуб цршср ырщцы еру сщттусешщт иуецуут ецщёттщвуы ьщку зкусшыудню Еру удуьутеы ща еру ьфекшч фку вуаштув фы еру агтсешщт ща вшыефтсу иуецуутётецщ тщвуы (вкшмштп вшыефтсу щк пущпкфзршсфд вшыефтсу)ю Ф ензшсфд цфн шы ещ гыу еру еркуырщдв Пфгыышфтётагтсешщт фы ащддщцыбётёгв835ёгвс34ёгв835ёгвс56ёгв835ёгвс57 =ёт(ётучз(ёг2212ёгв835ёгвс51ёт2ётёгв835ёгвс56 ёгв835ёгвс57ётёгв835ёгва0уёт2 )^ ёгв835ёгвс51ёгв835ёгвс56ёгв835ёгвс57  ёгв835ёгва16ётб (163)ётцруку ёгв835ёгвс51ёгв835ёгвс56ёгв835ёгвс57 шы еру вшыефтсу иуецуут тщву ёгв835ёгвс56 фтв ёгв835ёгвс57б фтв ёгв835ёгва0у фтв ёгв835ёгва16 фку ецщ рнзукзфкфьуеукы ещ сщтекщд еруётвшыекшигешщт фтв еру ызфкышен ща еру ьфекшчюётФтщерук лштв ща ашчув фвофсутсн ьфекшч шы еру ышьшдфкшен-ифыув ьфекшчю Шт афсеб ф ышьшдфкшенётьфекшч шы тще фт фвофсутсн ьфекшч ещ ыщьу учеутею Ше шы сщтыекгсеув фссщквштп ещ еру ышьшдфкшен щаётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юёт64 Цю Огб уе фдюётецщ тщвуыб цршср ьуфты еру тушприщкы шт еру ышьшдфкшен пкфзр ьфн иу афк фцфн шт еру куфд цщкдвюётЕруку фку мфкшщгы ышьшдфкшен ьуекшсыю Ащк учфьздуб ьфтн цщклы ьуфыгку еру ышьшдфкшен ща ецщ тщвуыётин ерушк агтсешщтфдшенб уюпюб еру вшыекшигешщт ща ыгккщгтвштп зщштеы ща штеукуые (ЗЩШы)ю Еру фыыгьзешщтётиурштв ершы шы ерфе тщвуы ерфе ырфку ышьшдфк агтсешщтфдшен ьфн ырфку ышьшдфк екфаашс зфееуктыю Цуётсфт фдыщ вуашту еру ышьшдфкшен еркщгпр еру ршыещкшсфд адщц зфееуктыю Ещ сщьзгеу еру ышьшдфкшен щаётецщ-ешьу ыукшуыб ф сщььщт зкфсешсу шы ещ гыу Внтфьшс Ешьу Цкфззштп (ВЕЦ) фдпщкшерь х350ъбётцршср шы ыгзукшщк ещ щерук ьуекшсы вгу ещ шеы ыутышешмшен ещ ырфзу ышьшдфкшен кферук ерфт зщште-цшыуётышьшдфкшеню Ызусшашсфдднб пшмут ецщ ешьу ыукшуы ёгв835ёгвс4и = (ёгв835ёгвс651б ёгв835ёгвс652б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс65ёгв835ёгвс5и) фтв ёгв835ёгвс4с = (ёгв835ёгвс661б ёгв835ёгвс662б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс66ёгв835ёгвс5и)б ВЕЦ шыётф внтфьшс зкщпкфььштп фдпщкшерь вуаштув фыётёгв835ёгвс37(ёгв835ёгвс56б ёгв835ёгвс57) = ёгв835ёгвс51ёгв835ёгвс56ёгв835ёгвс60ёгв835ёгвс61(ёгв835ёгвс65ёгв835ёгвс56б ёгв835ёгвс66ёгв835ёгвс57) + ьшт (ёгв835ёгвс37(ёгв835ёгвс56 ёг2212 1^ ёгв835ёгвс57)б ёгв835ёгвс37(ёгв835ёгвс56б ёгв835ёгвс57 ёг2212 1)^ ёгв835ёгвс37(ёгв835ёгвс56 ёг2212 1^ ёгв835ёгвс57 ёг2212 1)) ^ (164)ётцруку ёгв835ёгвс37(ёгв835ёгвс56б ёгв835ёгвс57) кузкуыутеы еру ырщкеуые вшыефтсу иуецуут ыгиыукшуы ёгв835ёгвс4и = (ёгв835ёгвс651б ёгв835ёгвс652б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс65ёгв835ёгвс56) фтв ёгв835ёгвс4с =ёт(ёгв835ёгвс661б ёгв835ёгвс662б ёг00и7 ёг00и7 ёг00и7 ^ ёгв835ёгвс66ёгв835ёгвс57)б фтвёгв835ёгвс51ёгв835ёгвс56ёгв835ёгвс60ёгв835ёгвс61(ёгв835ёгвс65ёгв835ёгвс56б ёгв835ёгвс66ёгв835ёгвс57) шы ыщьу вшыефтсу ьуекшс дшлу фиыщдгеу вшыефтсую Фы ф куыгдебёгв835ёгвс37ёгв835ёгвс47ёгв835ёгвс4ф (ёгв835ёгвс4иб ёгв835ёгвс4с) =ётёгв835ёгвс37(ёгв835ёгвс5иб ёгв835ёгвс5и) шы ыуе фы еру аштфд вшыефтсу иуецуут ёгв835ёгвс4и фтв ёгв835ёгвс4сб цршср иуееук куадусеы еру ышьшдфкшен ща еруётецщ-ешьу ыукшуы сщьзфкув ещ еру Угсдшвуфт вшыефтсуюётВнтфьшс ьфекшчю Еру зку-вуаштув ьфекшч шы ыщьуешьуы гтфмфшдфиду фтв сфттще куадусе сщьздуеуётштащкьфешщт ща ызфешфд сщккудфешщтыю Еру внтфьшс фвфзешму ьфекшч шы зкщзщыув ещ ыщдму еру шыыгуюётЕру внтфьшс ьфекшч шы дуфктув акщь штзге вфеф фгещьфешсфддню Ещ фсршуму еру иуые зкувшсешщтётзукащкьфтсуб еру внтфьшс ьфекшч цшдд ьфтфпу ещ штаук еру ршввут сщккудфешщты фьщтп тщвуыб ьщкуётерфт ерщыу зрнышсфд сщттусешщтыюётФ ензшсфд зкфсешсу шы дуфктштп фвофсутсн ьфекшч акщь тщву уьиуввштпы ~19`& Дуе ёгв835ёгвс38ёгв835ёгвс34 ёг2208 Кётёгв835ёгвс41 ёг00в7ёгв835ёгвс51 иу фётдуфктфиду тщву уьиуввштп вшсешщтфкнб цруку уфср кщц ща ёгв835ёгвс38ёгв835ёгвс34 кузкуыутеы еру уьиуввштп ща ф тщвубётёгв835ёгвс41 фтв ёгв835ёгвс51 вутщеу еру тгьиук ща тщвуы фтв еру вшьутышщт ща уьиуввштпы куызусешмудню Еру пкфзрётфвофсутсн ьфекшч шы вуаштув фы еру ышьшдфкшешуы фьщтп тщву уьиуввштпыбётёгв835ёгвс37ётёг2212ёт1ёт2ёгв835ёгвс34ёгв835ёгвс37ёг2212ёт1ёт2 = ёгв835ёгвс60ёгв835ёгвс5с ёгв835ёгвс53 ёгв835ёгвс61ёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс65 ёг0010ётёгв835ёгвс45ёгв835ёгвс52ёгв835ёгвс3аёгв835ёгвс48 (ёгв835ёгвс38ёгв835ёгвс34 ёг00и7 ёгв835ёгвс38ётёгв835ёгвс47ётёгв835ёгвс34ёт)ётёг0011ётб (165)ётцруку ёгв835ёгвс60ёгв835ёгвс5с ёгв835ёгвс53 ёгв835ёгвс61ёгв835ёгвс5фёгв835ёгвс4уёгв835ёгвс65 агтсешщт шы ещ зукащкь кщц-тщкьфдшяфешщтб фтв ёгв835ёгвс37ётёг2212ёт1ёт2ёгв835ёгвс34ёгв835ёгвс37ёг2212ёт1ёт2 шы еру Дфздфсшфт ьфекшчюёт14ю3 Ензшсфд ПТТ Акфьуцщклы шт Екфаашс ВщьфштётЫзфешфд Еуьзщкфд Пкфзр Сщтмщдгешщт Туецщкл (ЫЕПСТ) ~545`& ЫЕПСТ шы ф зшщтуукштп цщкл шт еруётызфешфд-еуьзщкфд ПТТ вщьфштю Ше гешдшяуы пкфзр сщтмщдгешщт ещ сфзегку ызфешфд ауфегкуыб фтв вуздщныётф пфеув сфгыфд сщтмщдгешщт ещ учекфсе еуьзщкфд зфееуктыю Ызусшашсфдднб еру пкфзр сщтмщдгешщт фтвётеуьзщкфд сщтмщдгешщт фку вуаштув фы ащддщцыбётёг0398 ёг2217П ёгв835ёгвс65 = ёгв835ёгва03 (ёгв835ёгвс3сёгв835ёгвс5и + ёгв835ёгвс37ётёг2212ёт1ёт2ёгв835ёгвс34ёгв835ёгвс37ёг2212ёт1ёт2 )ёгв835ёгвс65 = ёгв835ёгва03 (ёгв835ёгвс37ёг02вс ёг2212ёт1ёт2ёгв835ёгвс34ёг02всёгв835ёгвс37ёг02вс ёг2212ёт1ёт2 )ёгв835ёгвс65б (166)ётёг0393 ёг2217Е ёгв835ёгвс66 = ёгв835ёгвс43 ёг2299 ёгв835ёгва0у(ёгв835ёгвс44)б (167)ётцруку ёг0398 шы еру зфкфьуеук ща пкфзр сщтмщдгешщтб ёгв835ёгвс43 фтв ёгв835ёгвс44 фку еру щгезгеы ща ф 1-в сщтмщдгешщтётфдщтп еру еуьзщкфд вшьутышщтю Еру ышпьщшв пфеу ёгв835ёгва0у(ёгв835ёгвс44) сщтекщды рщц еру ыефеуы ща ёгв835ёгвс43 фку кудумфтеётащк вшысщмукштп ршввут еуьзщкфд зфееуктыю Шт щквук ещ агыу ауфегкуы акщь ищер ызфешфд фтв еуьзщкфдётвшьутышщтб еру ызфешфд сщтмщдгешщт дфнук фтв еру еуьзщкфд сщтмщдгешщт дфнук фку сщьиштув ещётсщтыекгсе ф ызфешфд еуьзщкфд идщсл ещ ощштедн вуфд цшер пкфзр-ыекгсегкув ешьу ыукшуыб фтв ьщку идщслыётсфт иу ыефслув ещ фсршуму ф ьщку ысфдфиду фтв сщьздуч ьщвудюётВшаагышщт Сщтмщдгешщтфд Кусгккуте Тугкфд Туецщкл (ВСКТТ) ~275`& ВСКТТ шы ф кузкуыутефешмуётыщдгешщт сщьиштштп пкфзр сщтмщдгешщт туецщклы цшер кусгккуте тугкфд туецщклыю Ше сфзегкуы ызфешфдётвузутвутсшуы ин ишвшкусешщтфд кфтвщь цфдлы щт еру пкфзрю Еру вшаагышщт сщтмщдгешщт щзукфешщт щтётОю ФСЬб Мщдю 1^ Тщю 1^ Фкешсду & Згидшсфешщт вфеуЖ Ауикгфкн 2024юётФ Сщьзкурутышму Ыгкмун щт Вууз Пкфзр Кузкуыутефешщт Дуфктштп 65ётф пкфзр шы вуаштув фыЖётёгв835ёгвс4и ёг2217П ёгв835ёгвс53ёгв835ёгва03 =ётёг2211ёгау01ётёгв835ёгвс3уётёгв835ёгвс58=0ётёг0010ётёгв835ёгва03ёгв835ёгвс58б1 (ёгв835ёгвс37ётёг22121ётёгв835ёгвс42 ёгв835ёгвс34)ётёгв835ёгвс58 + ёгв835ёгва03ёгв835ёгвс58б2 (ёгв835ёгвс37ёг22121ётёгв835ёгвс3с ёгв835ёгвс34)ётёгв835ёгвс58ётёг0011ётёгв835ёгвс4иб (168)ётцруку ёгв835ёгва03 фку зфкфьуеукы ащк еру сщтмщдгешщт ашдеукб фтв ёгв835ёгвс37ётёг22121ётёгв835ёгвс42ётёгв835ёгвс34б ёгв835ёгвс37ёг22121ётёгв835ёгвс3сётёгв835ёгвс34 кузкуыуте еру ишвшкусешщтфдётвшаагышщт зкщсуыыуы куызусешмудню Шт еукь ща еуьзщкфд вузутвутснб ВСКТТ гешдшяуы Пфеув КусгккутеётГтшеы (ПКГ)б фтв куздфсу еру дштуфк екфтыащкьфешщт шт еру ПКГ цшер еру вшаагышщт сщтмщдгешщт фыётащддщцыбётёгв835ёгвс5аёт(ёгв835ёгвс61) = ёгв835ёгва0у(ёг0398ёгв835ёгвс5а ёг2217П хёгв835ёгвс4и(ёгв835ёгвс61)ётб ёгв835ёгвс3и(ёгв835ёгвс61ёг22121)ъ + ёгв835ёгвс4аёгв835ёгвс5а)б (169)ётёгв835ёгвс62ёт(ёгв835ёгвс61) = ёгв835ёгва0у(ёг0398ёгв835ёгвс62 ёг2217П хёгв835ёгвс4и(ёгв835ёгвс61)ётб ёгв835ёгвс3и(ёгв835ёгвс61ёг22121)ъ + ёгв835ёгвс4аёгв835ёгвс62)б (170)ётёгв835ёгвс36ёт(ёгв835ёгвс61) = ефтр(ёг0398ёгв835ёгвс36 ёг2217П хёгв835ёгвс4и(ёгв835ёгвс61)ётб (ёгв835ёгвс5аёт(ёгв835ёгвс61) ёг2299 ёгв835ёгвс3и(ёгв835ёгвс61ёг22121)ётъ + ёгв835ёгвс4аёгв835ёгвс50 )^ (171)ётёгв835ёгвс3иёт(ёгв835ёгвс61) = ёгв835ёгвс62(ёгв835ёгвс61) ёг2299 ёгв835ёгвс3и(ёгв835ёгвс61ёг22121) + (1 ёг2212 ёгв835ёгвс62(ёгв835ёгвс61)ёт) ёг2299 ёгв835ёгвс36ёт(ёгв835ёгвс61)ётб (172)ётцруку ёгв835ёгвс4иёт(ёгв835ёгвс61)ётб ёгв835ёгвс3и(ёгв835ёгвс61) вутщеу еру штзге фтв щгезге фе ешьу ёгв835ёгвс61б ёгв835ёгвс5аёт(ёгв835ёгвс61)ётб ёгв835ёгвс62(ёгв835ёгвс61)фку еру куыуе фтв гзвфеу пфеуыёткуызусешмуднб фтв ёг0398ёгв835ёгвс5аб ёг0398ёгв835ёгвс62б ёг0398ёгв835ёгвс36 фку зфкфьуеукы ща сщтмщдгешщт ашдеукыю Ьщкущмукб ВСКТТ уьздщны фётыуйгутсу-ещ-ыуйгутсу фксршеусегку ещ зкувшсе агегку ыукшуыю Ищер еру утсщвук фтв еру вусщвук фкуётсщтыекгсеув цшер вшаагышщт сщтмщдгешщтфд кусгккуте дфнукыю Еру ршыещкшсфд ешьу ыукшуы фку аув штещётеру утсщвук фтв еру зкувшсешщты фку путукфеув ин еру вусщвукю Еру ысрувгдув ыфьздштп еусртшйгу шыётгешдшяув ещ ыщдму еру вшыскузфтсн зкщидуь иуецуут екфштштп фтв еуые вшыекшигешщтюётФвфзешму Пкфзр Сщтмщдгешщтфд Кусгккуте Туецщкл (ФПСКТ) ~19`& Еру ащсгыуы ща ФПСКТ фкуётецщ-ащдвю Щт еру щту рфтвб ше фкпгуы ерфе еру еуьзщкфд зфееукты фку вшмукышашув фтв ергы зфкфьуеукёг0002ырфкштп ащк уфср тщву шы штаукшщкж щт еру щерук рфтвб ше зкщзщыуы ерфе еру зку-вуаштув пкфзр ьфн иуётштегшешму фтв штсщьздуеу ащк еру ызусшашс зкувшсешщт ефылю Ещ ьшешпфеу еру ецщ шыыгуыб ше вуышпты фётТщву Фвфзешму Зфкфьуеук Дуфктштп (ТФЗД) ьщвгду ещ дуфкт тщву-ызусшашс зфееукты ащк уфср екфаашсётыукшуыб фтв ф Вфеф Фвфзешму Пкфзр Путукфешщт (ВФПП) ьщвгду ещ штаук еру ршввут сщккудфешщтыётфьщтп тщвуы акщь вфеф фтв ещ путукфеу еру пкфзр вгкштп екфштштпю Ызусшашсфдднб еру ТФЗД ьщвгдуётшы вуаштув фы ащддщцыбётёгв835ёгвс4в = (ёгв835ёгвс3сёгв835ёгвс5и + ёгв835ёгвс37ётёг2212ёт1ёт2ёгв835ёгвс34ёгв835ёгвс37ёг2212ёт1ёт2 )ёгв835ёгвс4и ёгв835ёгвс38Пёгв835ёгвс4фП + ёгв835ёгвс38Пёгв835ёгвс4а Пб (173)ётцруку ёгв835ёгвс4и ёг2208 Кётёгв835ёгвс41 ёг00в7ёгв835ёгвс36 шы еру штзге ауфегкуб ёгв835ёгвс38П ёг2208 Кёгв835ёгвс41 ёг00в7ёгв835ёгвс51ётшы ф тщву уьиуввштп вшсешщтфкнб ёгв835ёгвс51 шы еруётуьиуввштп вшьутышщт (ёгв835ёгвс51Эб
ian. A variant is to use the\nnormalized graph Laplacian bL = D\n\u2212\n1\n2 LD\u2212\n1\n2 .\n9.3 Metric-based Methods\nMetric-based methods measure the similarity between nodes as the edge weights. They follow\nthe basic assumption that similar nodes tend to have connections with each other. We show some\nrepresentative works\nAdaptive Graph Convolutional Neural Networks (AGCN) [267]. AGCN learns a task-driven adaptive\ngraph during training to enable a more generalized and flexible graph representation model. After\nparameterizing the distance metric between nodes, AGCN is able to adapt graph topology to the\ngiven task. It proposes a generalized Mahalanobis distance between two nodes with the following\nformula:\nD(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) =\n\u221a\ufe03\n(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n\u22a4\ud835\udc40(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57), (106)\nwhere \ud835\udc40 = \ud835\udc4a\ud835\udc51\ud835\udc4a \u22a4\n\ud835\udc51\nand \ud835\udc4a\ud835\udc51 is the trainable weights to minimize task-specific objective. Then the\nGaussian kernel is used to obtain the adjacency matrix:\nG\ud835\udc56\ud835\udc57 = exp(\u2212D(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57)/(2\ud835\udf0e\n2\n)), (107)\n\ud835\udc34\u02c6 = \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52 (G). (108)\nGraph-Revised Convolutional Network (GRCN) [546]. GRCN uses a graph revision module to\npredict missing edges and revise edge weights through joint optimization on downstream tasks. It\nfirst learns the node embedding with GCN and then calculates pair-wise node similarity with the\ndot product as the kernel function.\n\ud835\udc4d = \ud835\udc3a\ud835\udc36\ud835\udc41\ud835\udc54 (\ud835\udc34, \ud835\udc4b), (109)\n\ud835\udc46\ud835\udc56\ud835\udc57 =\n\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\n. (110)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n40 W. Ju, et al.\nThe revised adjacency matrix is the residual summation of the original adjacency matrix\ud835\udc34\u02c6 = \ud835\udc34+\ud835\udc46.\nGRCN also applies a sparsification technique on the similarity matrix \ud835\udc46 to reduce computation cost:\n\ud835\udc46\n(\ud835\udc3e)\n\ud835\udc56\ud835\udc57 =\n\u001a\n\ud835\udc46\ud835\udc56\ud835\udc57, \ud835\udc46\ud835\udc56\ud835\udc57 \u2208 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n0, \ud835\udc46\ud835\udc56\ud835\udc57 \u2209 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n. (111)\nThreshold pruning is also a common strategy for sparsification. For example, CAGCN [613] uses\ndot product to measure node similarity, and refines the graph structure by removing edges between\nnodes whose similarity is less than a threshold \ud835\udf0f\ud835\udc5f and adding edges between nodes whose similarity\nis greater than another threshold \ud835\udf0f\ud835\udc4e.\nDefending Graph Neural Networks against Adversarial Attacks (GNNGuard) [571]. GNNGuard\nmeasures similarity between a node \ud835\udc62 and its neighbor \ud835\udc63 in the \ud835\udc58-th layer by cosine similarity and\nnormalizes node similarity at the node level within the neighborhood as follows:\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\u210e\n\ud835\udc58\n\ud835\udc62 \u2299 \u210e\n\ud835\udc58\n\ud835\udc63\n\u2225\u210e\n\ud835\udc58\n\ud835\udc62 \u22252 \u2225\u210e\n\ud835\udc58\n\ud835\udc63 \u22252\n, (112)\n\ud835\udefc\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63/\n\u2211\ufe01\n\ud835\udc63\u2208N\ud835\udc62\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u00d7 \ud835\udc41\u02c6 \ud835\udc58\ud835\udc62\n/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 \u2260 \ud835\udc63\n1/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 = \ud835\udc63\n, (113)\nwhere N\ud835\udc62 denotes the neighborhood of node \ud835\udc62 and \ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 =\n\u00cd\n\ud835\udc63\u2208N\ud835\udc62\n\u2225\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u22250. To stabilize GNN training,\nit also proposes a layer-wise graph memory by keeping part of the information from the previous\nlayer in the current layer. Similar to GNNGuard, IDGL [65] uses multi-head cosine similarity and\nmask edges with node similarity smaller than a non-negative threshold, and HGSL [586] generalizes\nthis idea to heterogeneous graphs.\nGraph Diffusion Convolution (GDC) [139]. GDC replaces the original adjacency matrix with\ngeneralized graph diffusion matrix S:\nS =\n\u2211\ufe01\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58T\n\ud835\udc58\n, (114)\nwhere \ud835\udf03\ud835\udc58 is the weighting coefficient and T is the generalized transition matrix. To ensure conver\u0002gence, GDC further requires that \u00cd\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58 = 1 and the eigenvalues of T lie in [0, 1]. The random\nwalk transition matrix T\ud835\udc5f \ud835\udc64 = AD\u22121and the symmetric transition matrix T\ud835\udc60\ud835\udc66\ud835\udc5a = D\n\u22121/2AD\u22121/2\nare\ntwo examples. This new graph structure allows graph convolution to aggregate information from a\nlarger neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying noise.\nHowever, in most cases graph diffusion will result in a dense adjacency matrix \ud835\udc46, so sparsification\ntechnology like top-k filtering and threshold filtering will be applied to graph diffusion. Following\nGDC, there are some other graph diffusion proposed. For example, AdaCAD [281] proposes Class\u0002Attentive Diffusion, which further considers node features and aggregates nodes probably of the\nsame class among K-hop neighbors. Adaptive diffusion convolution (ADC) [585] learns the optimal\nneighborhood size via optimizing a bi-level problem.\n9.4 Model-based Methods\nModel-based methods parameterize edge weights with more complex models like deep neural\nnetworks. Compared to metric-based methods, model-based methods offer greater flexibility and\nexpressive power.\nGraph Learning Network (GLN) [364]. GLN proposes a recurrent block to first produce interme\u0002diate node embeddings and then merge them with adjacency information as the output of this\nlayer to predict the adjacency matrix for the next layer. Specifically, it uses convolutional graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 41\noperations to extract node features, and creates a local-context embedding based on node features\nand the current adjacency matrix:\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc56=1\n\ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\n), (115)\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 = \ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc48\n(\ud835\udc59)\n), (116)\nwhere \ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\nand \ud835\udc48\n(\ud835\udc59)\nare the learnable weights. GLN then predicts the next adjacency matrix as\nfollows:\n\ud835\udc34\n(\ud835\udc59+1) = \ud835\udf0e\ud835\udc59 (\ud835\udc40(\ud835\udc59)\n\ud835\udefc\ud835\udc59 (\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59)\ud835\udc40\n(\ud835\udc59) \u22a4\n). (117)\nSimilarly, GLCN [199] models graph structure with a softmax layer over the inner product\nbetween the difference of node features and a learnable vector. NeuralSparse [595] uses a multi\u0002layer neural network to generate a learnable distribution from which a sparse graph structure is\nsampled. PTDNet [305] prunes graph edges with a multi-layer neural network and penalizes the\nnumber of non-zero elements to encourage sparsity.\nGraph Attention Networks (GAT) [452]. Besides constructing a new graph to guide the message\npassing and aggregation process of GNNs, many recent researchers also leverage the attention\nmechanism to adaptively model the relationship between nodes. GAT is the first work to introduce\nthe self-attention strategy into graph learning. In each attention layer, the attention weight between\ntwo nodes is calculated as the Softmax output on the combination of linear and non-linear transform\nof node features:\n\ud835\udc52\ud835\udc56\ud835\udc57 = \ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57), (118)\n\ud835\udefc\ud835\udc56\ud835\udc57 =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc57)\n\u00cd\n\ud835\udc58 \u2208N\ud835\udc56\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc58 )\n, (119)\nwhere N\ud835\udc56 denotes the neighborhood of node \ud835\udc56,W is learnable linear transform and \ud835\udc4e is pre-defined\nattention function. In the original implementation of GAT, \ud835\udc4e is a single-layer neural network with\nLeakyReLU:\n\ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57) = LeakyReLU(\u00aea\n\u22a4\n[W\u00ae\u210e\ud835\udc56||W\u00ae\u210e\ud835\udc57]). (120)\nThe attention weights are then used to guide the message-passing phase of GNNs:\n\u00ae\u210e\n\u2032\n\ud835\udc56 = \ud835\udf0e(\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n\ud835\udefc\ud835\udc56\ud835\udc57W\u00ae\u210e\ud835\udc57), (121)\nwhere \ud835\udf0e is a nonlinear function. It is beneficial to concatenate multiple heads of attention to\u0002gether to get a more stable and generalizable model, so-called multi-head attention. The attention\nmechanism serves as a soft graph structure learner which captures important connections within\nnode neighborhoods. Following GAT, many recent works propose more effective and efficient\ngraph attention operators to improve performance. GaAN [566] adds a soft gate at each attention\nhead to adjust its importance. MAGNA [461] proposes a novel graph attention diffusion layer to\nincorporate multi-hop information. One drawback of graph attention is that the time and space\ncomplexities are both \ud835\udc42(\ud835\udc41\n3\n). hGAO [132] performs hard graph attention by limiting node attention\nto its neighborhood. VIB-GSL [433] adopts the information bottleneck principle to guide feature\nmasking in order to drop task-irrelevant information and preserve actionable information for the\ndownstream task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n42 W. Ju, et al.\n9.5 Direct Methods\nDirect methods treat edge weights as free learnable parameters. These methods enjoy more flexibility\nbut are also more difficult to train. The optimization is usually carried out in an alternating way,\ni.e., iteratively updating the adjacency matrix A and the GNN encoder parameters \ud835\udf03.\nGLNN [135]. GLNN uses MAP estimation to learn an optimal adjacency matrix for a joint\nobjective function including sparsity and smoothness. Specifically, it targets at finding the most\nprobable adjacency matrix \ud835\udc34\u02c6 given graph node features \ud835\udc65:\n\ud835\udc34\u02dc\ud835\udc40\ud835\udc34\ud835\udc43 (\ud835\udc65) = argmax\n\ud835\udc34\u02c6\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6)\ud835\udc54(\ud835\udc34\u02c6), (122)\nwhere \ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) measures the likelihood of observing \ud835\udc65 given \ud835\udc34\u02c6, and \ud835\udc54(\ud835\udc34\u02c6) is the prior distribution of\n\ud835\udc34\u02c6. GLNN uses sparsity and property constraint as prior, and defines the likelihood function \ud835\udc53 as:\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) = \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n\ud835\udc3f\ud835\udc65) (123)\n= \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n(\ud835\udc3c \u2212 \ud835\udc34\u02c6)\ud835\udc65), (124)\nwhere \ud835\udf060 is a parameter. This likelihood imposed a smoothness assumption on the learned graph\nstructure. Some other works also model the adjacency matrix in a probabilistic manner. Bayesian\nGCNN [573] adopts a Bayesian framework and treats the observed graph as a realization from a\nfamily of random graphs. Then it estimates the posterior probablity of labels given the observed\ngraph adjacency matrix and features with Monte Carlo approximation. VGCN [104] follows a\nsimilar formulation and estimates the graph posterior through stochastic variational inference.\nPro-GNN [210] learns a clean graph structure from perturbed data and optimizes parameters for a\nrobust GNN, leveraging properties like sparsity, low rank, and feature smoothness.\nGraph Sparsification via Meta-Learning (GSML) [458]. GSML formulates GSL as a meta-learning\nproblem and uses bi-level optimization to find the optimal graph structure. The goal is to find a\nsparse graph structure that leads to high node classification accuracy at the same time given labeled\nand unlabeled nodes. To achieve this, GSML makes the inner optimization as training on the node\nclassification task, and targets the outer optimization at the sparsity of the graph structure, which\nformulates the following bi-level optimization problem:\n\ud835\udc3a\u02c6\n\u2217 = min\n\ud835\udc3a\u02c6 \u2208\u03a6(\ud835\udc3a)\n\ud835\udc3f\ud835\udc60\ud835\udc5d\ud835\udc60 (\ud835\udc53\ud835\udf03\n\u2217 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc48 ), (125)\n\ud835\udc60.\ud835\udc61 . \ud835\udf03 \u2217 = argmin\n\ud835\udf03\n\ud835\udc3f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (\ud835\udc53\ud835\udf03 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc3f). (126)\nIn this bi-level optimization problem, \ud835\udc3a\u02c6 \u2208 \u03a6(\ud835\udc3a) are the meta-parameters and optimized directly\nwithout parameterization. Similarly, LSD-GNN [124] also uses bi-level optimization. It models graph\nstructure with a probability distribution over the graph and reformulates the bi-level program in\nterms of the continuous distribution parameters.\n9.6 Summary\nIn this section, we provide the summary as follows:\n\u2022 Techniques. GSL aims to learn an optimized graph structure for better graph representations.\nIt is also used for more robust graph representation against adversarial attacks. According\nto the way of edge modeling, we categorize GSL into three groups: metric-based methods,\nmodel-based methods, and direct methods. Regularization is also a commonly used principle\nto make the learned graph structure satisfy specific properties including sparsity, low-rank\nand smoothness.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 43\n\u2022 Challenges and Limitations. Since there is no way to access the ground truth or optimal\ngraph structure as training data, the learning objective of GSL is either indirect (e.g., perfor\u0002mance on downstream tasks) or manually designed (e.g., sparsity and smoothness). Therefore,\nthe optimization of GSL is difficult and the performance is not satisfying. In addition, many\nGSL methods are based on homophily assumption, i.e., similar nodes are more likely to\nconnect with each other. However, many other types of connection exist in the real world\nwhich impose great challenges for GSL.\n\u2022 Future Works. In the future we expect more efficient and generalizable GSL methods to\nbe applied to large-scale and heterogeneous graphs. Most existing GSL methods focus on\npair-wise node similarities and thus struggle to scale to large graphs. Besides, they often\nlearn homogeneous graph structure, but in many scenarios graphs are heterogeneous.\n10 Social Analysis\nIn the real world, there usually exist complex relations and interactions between people and multiple\nentities. Taking people, concrete things, and abstract concepts in society as nodes and taking the\ndiverse, changeable, and large-scale connections between data as links, we can form massive and\ncomplex social information as social networks [43, 436]. Compared with traditional data structures\nsuch as texts and forms, modeling social data as graphs has many benefits. Especially with the arrival\nof the \"big data\" era, more and more heterogeneous information is interconnected and integrated,\nand it is difficult and uneconomical to model this information with a traditional data structure. The\ngraph is an effective implementation for information integration, as it can naturally incorporate\ndifferent types of objects and their interactions from heterogeneous data sources [349, 411]. A\nsummarization of social analysis applications is provided in Table 9.\n10.1 Concepts of Social Networks\nA social network is usually composed of multiple types of nodes, link relationships, and node\nattributes, which inherently include rich structural and semantic information. Specifically, a social\nnetwork can be homogeneous or heterogeneous and directed or undirected in different scenarios.\nWithout loss of generality, we define the social network as a directed heterogeneous graph \ud835\udc3a =\n{\ud835\udc49 , \ud835\udc38, T, R}, where \ud835\udc49 = {\ud835\udc5b\ud835\udc56 }\n|\ud835\udc49 |\n\ud835\udc56=1\nis the node set, \ud835\udc38 = {\ud835\udc52\ud835\udc56 }\n|\ud835\udc38|\n\ud835\udc56=1\nis the edge set, T = {\ud835\udc61\ud835\udc56 }\n| T |\n\ud835\udc56=1\nis the node\ntype set, and R = {\ud835\udc5f\ud835\udc56 }\n| R |\n\ud835\udc56=1\nis the edge type set. Each node \ud835\udc5b\ud835\udc56 \u2208 \ud835\udc49 is associated with a node type\nmapping: \ud835\udf19\ud835\udc5b (\ud835\udc5b\ud835\udc56) = \ud835\udc61\ud835\udc57: \ud835\udc49 \u2212\u2192 T and each edge \ud835\udc52\ud835\udc56 \u2208 \ud835\udc38 is associated with a node type mapping:\n\ud835\udf19\ud835\udc52 (\ud835\udc52\ud835\udc56) = \ud835\udc5f\ud835\udc57: \ud835\udc38 \u2212\u2192 R. A node \ud835\udc5b\ud835\udc56 may have a feature set, where the feature space is specific for the\nnode type. An edge \ud835\udc52\ud835\udc56is also represented by node pairs (\ud835\udc5b\ud835\udc57, \ud835\udc5b\ud835\udc58 ) at both ends and can be directed\nor undirected with relation-type-specific attributes. If |T | = 1 and |R| = 1, the social network is a\nhomogeneous graph; otherwise, it is a heterogeneous graph.\nAlmost any data produced by social activities can be modeled as social networks, for example,\nthe academic social network produced by academic activities such as collaboration and citation,\nthe online social network produced by user following and followed on social media, and the\nlocation-based social network produced by human activities on different locations. Based on\nconstructing social networks, researchers have new paths to data mining, knowledge discovery,\nand multiple application tasks on social data. Exploring social networks also brings new challenges.\nOne of the critical challenges is how to succinctly represent the network from the massive and\nheterogeneous raw graph data, that is, how to learn continuous and low-dimensional social network\nrepresentations, so as to researchers can efficiently perform advanced machine learning techniques\non the social network data for multiple application tasks, such as analysis, clustering, prediction,\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n44 W. Ju, et al.\nTable 9. A summarization of social analysis applications\nSocial networks Node type Edge type Applications References\nAcademic\nSocial\nNetwork\nAuthor,\nPublication,\nVenue,\nOrganization,\nKeyword\nAuthorship,\nCo-Author,\nAdvisor\u0002advisee,\nCiting, Cited,\nCo-Citing,\nPublishing\nClassification/\nClustering\nPaper/author classification [92, 370, 471,\n563], name disambiguation [52, 319, 368,\n575]\nRelationship\nprediction\nCo-authorship [69, 72, 610], citation rela\u0002tionship [203, 464, 550], advisor-advisee re\u0002lationship [286, 317, 594]\nRecommen\u0002dation\nCollaborator recommendation [235, 236,\n296], paper recommendation [20, 81, 429],\nvenue recommendation [337, 549]\nSocial\nMedia\nNetwork\nUser, Blog,\nArticle, Image,\nVideo\nFollowing,\nLike, Unlike,\nClicked,\nViewed,\nCommented,\nReposted\nAnomaly\ndetection\nMalicious attacks [294, 395, 434], emer\u0002gency detection [28, 79, 257], and robot dis\u0002covery [117, 304]\nSentiment\nanalysis\nCustomer feedback [389, 449, 572], public\nevents [33, 332, 450]\nInfluence\nanalysis\nImportant node finding [91, 386], informa\u0002tion diffusion modeling [226, 246, 356, 562]\nLocation-based\nSocial\nNetwork\nRestaurant,\nCinema, Mall,\nParking\nFriendship,\nCheck-in\nPOI recom\u0002mendation\nSpatial/temporal influence [416, 484, 589],\nsocial relationship [297, 513], textual infor\u0002mation [469, 483, 515]\nUrban\ncomputing\nTraffic congestion prediction [202, 511], ur\u0002ban mobility analysis [45, 539], event de\u0002tection [420, 548]\nand knowledge discovery. Thus, graph representation learning on the social network becomes the\nfoundational technique for social analysis.\n10.2 Academic Social Network\nAcademic collaboration is a common and important behavior in academic society, and also a major\nway for scientists and researchers to innovate and breakthrough scientific research, which leads to\nsocial relationship between scholars. The academic data generated by academic collaboration usually\ncontains a large number of interconnected entities with complex relationships [237, 602]. Normally,\nin an academic social network, the node type set consists of Author, Publication, Venue, Organization,\nKeyword, etc., and the relation set consists of Authorship, Co-Author, Advisor-advisee, Citing, Cited,\nCo-Citing, Publishing, Co-Word, etc. Note that in most social networks, each relation type always\nconnects two fixed node types with a fixed direction. For example, the relation Authorship points\nfrom the node type Author to Publication, and the Co-Author is an undirected relation between two\nnodes with type Author. Based on the node and relation types in an academic social network, one\ncan divide it into multiple categories. For example, the co-author network with nodes of Author and\nrelations of Co-Author, the citation network with nodes of Publication and relation of Citing, and the\nacademic heterogeneous information graph with multiple academic node and relation types. Many\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 45\nresearch institutes and academic search engines, such as Aminer1, DBLP2, Microsoft Academic\nGraph (MAG)3, have provided open academic social network datasets for research purposes.\nThere are multiple applications of graph representation learning on the academic social net\u0002work. Roughly, they can be divided into three categories\u2013academic entity classification/clustering,\nacademic relationship prediction, and academic resource recommendation.\n\u2022 Academic entities usually belong to different classes of research areas. Research of academic\nentity classification and clustering aims to categorize these entities, such as papers and\nauthors, into different classes [92, 213, 370, 471, 537, 563]. In literature, academic networks\nsuch as Cora, citepSeer, and Pubmed [406] have become the most widely used benchmark\ndatasets for examining the performance of graph representation learning models on paper\nclassification. Also, the author name disambiguation problem [52, 319, 368, 575] is also\nessentially a node clustering task on co-author networks and is usually solved by the graph\nrepresentation learning technique.\n\u2022 Academic relationship prediction represents the link prediction task on various academic\nrelations. Typical applications are co-authorship prediction [69, 72, 610] and citation rela\u0002tionship prediction [203, 464, 550]. Existing methods learn representations of authors and\npapers and use the similarity between two nodes to predict the link probability. Besides, some\nwork [286, 317, 594] studies the problem of advisor-advisee relationship prediction in the\ncollaboration network.\n\u2022 Various academic recommendation systems have been introduced to retrieve academic re\u0002sources for users from large amounts of academic data in recent years. For example, collabo\u0002rator recommendation [235, 236, 296] benefit researchers by finding suitable collaborators\nunder particular topics; paper recommendation [20, 81, 429] help researchers find relevant pa\u0002pers on given topics; venue recommendation [337, 549] help researchers choose appropriate\nvenues when they submit papers.\n10.3 Social Media Network\nWith the development of the Internet in decades, various online social media have emerged in large\nnumbers and greatly changed people\u2019s traditional social models. People can establish friendships\nwith others beyond the distance limit and share interests, hobbies, status, activities, and other\ninformation among friends. These abundant interactions on the Internet form large-scale complex\nsocial media networks, also named online social networks. Usually, in an academic social network,\nthe node type set consists of User, Blog, Article, Image, Video, etc., and the relation type set consists\nof Following, Like, Unlike, Clicked, Viewed, Commented, Reposted, etc. The main property of a social\nmedia network is that it usually contains multi-mode information on the nodes, such as video,\nimage, and text. Also, the relations are more complex and multiplex, including the explicit relations\nsuch as Like and Unlike and the implicit relations such as Clicked. The social media network can\nbe categorized into multiple types based on their media categories. For example, the friendship\nnetwork, the movie review network, and the music interacting network are extracted from different\nsocial media platforms. In a broad sense, the user-item networks in online shopping system can also\nbe viewed as social media networks as they also exist on the Internet and contains rich interactions\nby people. There are many widely used data sources for social media network analysis, such as\nTwitter, Facebook, Weibo, YouTube, and Instagram.\n1https://www.aminer.cn/\n2https://dblp.uni-trier.de/\n3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n46 W. Ju, et al.\nThe mainstream application research on social media networks via graph representation learning\ntechniques mainly includes anomaly detection, sentiment analysis, and influence analysis.\n\u2022 Anomaly detection aims to find strange or unusual patterns in social networks, which has\na wide range of application scenarios, such as malicious attacks [294, 395, 434], emergency\ndetection [28, 79], and robot discovery [117, 304] in social networks. Unsupervised anomaly\ndetection usually learns a reconstructed graph to detect those nodes with higher reconstructed\nerror as the anomaly nodes [5, 588]; Supervised methods model the problem as a binary\nclassification task on the learned graph representations [340, 596].\n\u2022 Sentiment analysis, also named as opinion mining, is to mine the sentiment, opinions, and\nattitudes, which can help enterprises understand customer feedback on products [389, 449,\n572] and help the government analyze the public emotion and make rapid response to public\nevents [33, 332, 450]. The graph representation learning model is usually combined with\nRNN-based [58, 561] or Transformer-based [7, 441] text encoders to incorporate both the\nuser relationship and textual semantic information.\n\u2022 Influence analysis usually aims to find several nodes in a social network to initially spread\ninformation such as advertisements, so as to maximize the final spread of information [91, 386].\nThe core challenge is to model the information diffusion process in the social network. Deep\nlearning methods [226, 246, 356, 562] usually leverage graph neural networks to learn node\nembeddings and diffusion probabilities between nodes.\n10.4 Location-based Social Network\nLocations are the fundamental information of human social activities. With the wide availability of\nmobile Internet and GPS positioning technology, people can easily acquire their precise locations\nand socialize with their friends by sharing their historical check-ins on the Internet. This opens up\na new avenue of research on location-based social network analysis, which gathered significant\nattention from the user, business, and government perspectives. Usually, in a location-based social\nnetwork, the node type set consists of User, and Location, also named Point of Interest(POI) in the\nrecommendation scenario containing multiple categories such as Restaurant, Cinema, Mall, Parking,\netc. The relation type set consists of Friendship, Check-in. Also, those node and relation types that\nexist in traditional social media networks can be included in a location-based social network. The\ndifference with other social networks, the main location-based social networks are spatial and\ntemporal, making the graph representation learning more challenging. For example, in a typical\nsocial network constructed for the POI recommendation, the user nodes are connected with each\nother by their friendship. The location nodes are connected by user nodes with the relations feature\nof timestamps. The location nodes also have a spatial relationship with each other and own have\ncomplex features, including categories, tags, check-in counts, number of users check-in, etc. There\nare many location-based social network datasets, such as Foursquare4, Gowalla5, and Waze6. Also,\nmany social media such as Twitter, Instagram, and Facebook can provide location information.\nThe research of graph representation learning on location-based social networks can be divided\ninto two categories: POI recommendation for business benefits and urban computing for public\nmanagement.\n\u2022 POI recommendation is one of the research hotspots in the field of location-based social\nnetworks and recommendation systems in recent years [195, 219, 489], which aim to uti\u0002lize historical check-ins of users and auxiliary information to recommend potential favor\n4https://foursquare.com/\n5https://www.gowalla.com/\n6https://www.waze.com/live-map/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 47\nplaces for users from a large of location points. Existing researches mainly integrate four\nessential characteristics, including spatial influence, temporal influence [416, 484, 589], social\nrelationship [297, 513], and textual information [469, 483, 515].\n\u2022 Urban computing is defined as a process of analysis of the large-scale connected urban data\ncreated from city activities of vehicles, human beings, and sensors [358, 359, 417]. Besides the\nlocal-based social network, the urban data also includes physical sensors, city infrastructure,\ntraffic roads, and so on. Urban computing aims to improve the quality of public management\nand life quality of people living in city environments. Typical applications including traffic\ncongestion prediction [202, 511], urban mobility analysis [45, 539], event detection [420, 548].\n10.5 Summary\nThis section introduces social analysis by graph representation learning and we provide the\nsummary as follows:\n\u2022 Techniques. Social networks, generated by human social activities, such as communication,\ncollaboration, and social interactions, typically involve massive and heterogeneous data, with\ndifferent types of attributes and properties that can change over time. Thus, social network\nanalysis is a field of study that explores the techniques to understand and analyze the complex\nattributes, heterogeneous structures, and dynamic information of social networks. Social\nnetwork analysis typically learns low-dimensional graph representations that capture the\nessential properties and patterns of the social network data, which can be used for various\ndownstream tasks, such as classification, clustering, link prediction, and recommendation.\n\u2022 Chanllenges and Limitations. Despite the structural heterogeneity in social networks\n(nodes and relations have different types), with the technological advances in social media,\nthe node attributes have become more heterogeneous now, containing text, video, and images.\nAlso, the large-scale problem is a pending issue in social network analysis. The data in\nthe social network has increased exponentially in past decades, containing a high density\nof topological links and a large amount of node attribute information, which brings new\nchallenges to the efficiency and effectiveness of traditional network representation learning\non the social network. Lastly, social networks are often dynamic, which means the network\ninformation usually changes over time, and this temporal information plays a significant\nrole in many downstream tasks, such as recommendations. This brings new challenges to\nrepresentation learning on social networks in incorporating temporal information.\n\u2022 Future Works. Recently, multi-modal big pre-training models that can fuse information\nfrom different modalities have gained increasing attention [369, 379]. These models can\nobtain valuable information from a large amount of unlabeled data and transfer it to various\ndownstream analysis tasks. Moreover, Transformer-based models have demonstrated better\neffectiveness than RNNs in capturing temporal information. In the future, there is potential\nfor introducing multi-modal big pre-training models in social network analysis. Also, it is\nimportant to make the models more efficient for network information extraction and use\nlightweight techniques like knowledge distillation to further enhance the applicability of the\nmodels. These advancements can lead to more effective social network analysis and enable\nthe development of more sophisticated applications in various domains.\n11 Molecular Property Prediction\nMolecular Property Prediction is an essential task in computational drug discovery and cheminfor\u0002matics. Traditional quantitative structure property/activity relationship (QSPR/QSAR) approaches\nare based on either SMILES or fingerprints [344, 522, 570], largely overlooking the topological\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n48 W. Ju, et al.\nfeatures of the molecules. To address this problem, graph representation learning has been widely\napplied to molecular property prediction. A molecule can be represented as a graph where nodes\nstand for atoms and edges stand for atom-bonds (ABs). Graph-level molecular representations\nare learned via the message passing mechanism to incorporate the topological information. The\nrepresentations are then utilized for the molecular property prediction tasks.\nSpecifically, a molecule is denoted as a topological graph G = (V, E), where V = {\ud835\udc63\ud835\udc56|\ud835\udc56 =\n1, . . . , |G|} is the set of nodes representing atoms. A feature vector x\ud835\udc56is associated with each node \ud835\udc63\ud835\udc56\nindicating its type such as Carbon, Nitrogen. E = {\ud835\udc52\ud835\udc56\ud835\udc57 |\ud835\udc56, \ud835\udc57 = 1, . . . , |G|} is the set of edges connecting\ntwo nodes (atoms) \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57 representing atom bonds. Graph representation learning methods\nare used to obtain the molecular representation hG. Then downstream classification or regression\nlayers \ud835\udc53 (\u00b7) are applied to predict the probability of target property of each molecule \ud835\udc66 = \ud835\udc53 (hG).\nIn Section 11.1, we introduce 4 types of molecular properties graph representation learning can\nbe treated and their corresponding datasets. Section 11.2 reviews the graph representation learning\nbackbones applied to molecular property prediction. Strategies for training the molecular property\nprediction methods are listed in Section 11.3.\n11.1 Molecular Property Categorization\nPlenty of molecular properties can be predicted by graph-based methods. We follow Wieder et al.\n[490] to categorize them into 4 types: quantum chemistry, physicochemical properties, biophysics,\nand biological effect.\nQuantum chemistry is a branch of physical chemistry focused on the application of quantum\nmechanics to chemical systems, including conformation, partial charges and energies. QM7, QM8,\nQM9 [501], COD [391] and CSD [154] are datasets for quantum chemistry prediction.\nPhysicochemical properties are the intrinsic physical and chemical characteristics of a substance,\nsuch as bioavailability, octanol solubility, aqueous solubility and hydrophobicity. ESOL, Lipophilicity\nand Freesolv [501] are datasets for physicochemical properties prediction.\nBiophysics properties are about the physical underpinnings of biomolecular phenomena, such\nas affinity, efficacy and activity. PDBbind [466], MUV, and HIV [501] are biophysics property\nprediction datasets.\nBiological effect properties are generally defined as the response of an organism, a population,\nor a community to changes in its environment, such as side effects, toxicity and ADMET. Tox21,\ntoxcast [501] and PTC [448] are biological effect prediction datasets.\nMoleculenet [501] is a widely-used benchmark dataset for molecule property prediction. It\ncontains over 700,000 compounds tested on different properties. For each dataset, they provide\na metric and a splitting pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv,\nLipophilicity and PDBbind are regression tasks, using MAE or RMSE as the evaluation metrics.\nOther tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation metric.\n11.2 Molecular Graph Representation Learning Backbones\nSince node attributes and edge attributes are crucial to molecular representation, most works use\nGNN instead of traditional graph representation learning methods as backbones, since many GNN\nmethods consider edge information. Existing GNNs designed for the general domain can be applied\nto molecular graphs. Table 10 summarizes the GNNs used for molecular property prediction and\nthe types of properties they can be applied to predict.\nFurthermore, many works customize their GNN structure by considering the chemical domain\nknowledge.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 49\nTable 10. Summary of GNNs in molecular property prediction.\nType Spatial/Specrtal Method Application\nReccurent GNN - R-GNN Biological effect [400]\nReccurent GNN - GGNN Quantum chemistry [333],\nBiological effect [9, 115, 492]\nReccurent GNN - IterRefLSTM Biophysics [9], Biological effect [9]\nConvolutional GNN Spatial/Specrtal GCN\nQuantum chemistry [280, 492, 529],\npysicochemical properties [75, 101, 393],\nBiophysics [34, 101, 529]\nBiological effect [261, 501]\nConvolutional GNN Specrtal LanczosNet Quantum chemistry [280]\nConvolutional GNN Specrtal ChebNet Physicochemical properties,\nBiophysics, Biological effect [267]\nConvolutional GNN Spatial GraphSAGE\nPhysicochemical properties [181],\nBiophysics [68, 108, 279],\nBiological effect [181, 328]\nConvolutional GNN Spatial GAT\nPhysicochemical properties [3, 181],\nBiophysics [34, 68],\nBiological effect [181]\nConvolutional GNN Spatial DGCNN Biophysics [63], Biological effect [568]\nConvolutional GNN Spatial GIN\nPhysicochemical properties [34, 181],\nBiophysics [180, 181],\nBiological effect [181]\nConvolutional GNN Spatial MPNN Physicochemical [320]\nTransformer - MAT Physicochemical, Biophysics [616]\n\u2022 First, the chemical bonds and molecule interaction are taken into consideration carefully. For\nexample, Ma et al. [320] use an additional edge GNN to model the chemical bonds separately.\nSpecifically, given an edge (\ud835\udc63,\ud835\udc64), they formulate an Edge-based GNN as:\nm\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = AGGedge ({h\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(\ud835\udc58\u22121)\n\ud835\udc62\ud835\udc63 , x\ud835\udc62 |\ud835\udc62 \u2208 N\ud835\udc63 \\ \ud835\udc64}), h\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = MLPedge ({m\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(0)\n\ud835\udc63\ud835\udc64 }), (127)\nwhere h\n(0)\n\ud835\udc63\ud835\udc64 = \ud835\udf0e(Weine\ud835\udc63\ud835\udc64) is the input state of the Edge-based GNN, Wein \u2208 R\n\ud835\udc51hid\u00d7\ud835\udc51\ud835\udc52\nis the\ninput weight matrix. PotentialNet [115] further uses different message passing operations\nfor different edge types. DGNN-DDI [325] leverage dual graph neural networks to model the\ninteraction between two molecules.\n\u2022 Second, motifs in molecular graphs play an important role in molecular property prediction.\nGSN [34] leverage substructure encoding to construct a topologically-aware message-passing\nmethod. Each node \ud835\udc63 updates its state h\n\ud835\udc61\n\ud835\udc63\nby combining its previous state with the aggregated\nmessages:\nh\n\ud835\udc61+1\n\ud835\udc63 = UP\ud835\udc61+1\nh\n\ud835\udc61\n\ud835\udc63\n, m\n\ud835\udc61+1\n\ud835\udc63\n\u0001\n, (128)\nm\n\ud835\udc61+1\n\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-v)\nor\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-e)\n, (129)\nwhere x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 contains the substructure information associated with nodes and\nedges, [] denotes a multiset. Yu et al. [551] constructs a heterogeneous graph using motifs\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n50 W. Ju, et al.\nand molecules. Motifs and molecules are both treated as nodes and the edges model the\nrelationship between motifs and graphs, for example, if a graph contains a motif, there will\nbe an edge between them. MGSSL [580] leverages a retrosynthesis-based algorithm BRICS\nand additional rules to find the motifs and combines motif layers with atom layers. It is a\nhierarchical framework jointly modeling atom-level information and motif-level information.\nAouichaoui et al. [12] introduce group-contribution-based attention to highlight the most\nsubstructures in molecules.\n\u2022 Third, different feature modalities have been used to improve molecular graph embedding.\nLin et al. [283] combine SMILES modality and graph modality with contrastive learning.\nZhu et al. [608] encode 2D molecular graph and 3D molecular conformation with a unified\nTransformer. It uses a unified model to learn 3D conformation generation given 2D graph\nand 2D graph generation given 3D conformation. Cremer et al.[78] use a Equivariant Graph\nNeural Networks to represent the 3D information of molecules. Liu et al. [293] consider\nmolecular chirality and design a chirality-aware molecular convolution module.\n\u2022 Finally, knowledge graph and literature can provide additional knowledge for molecular\nproperty prediction. Fang et al. [110] introduce a chemical element knowledge graph to\nsummarize microscopic associations between elements and augment the molecular graph\nbased on the knowledge graph, and a knowledge-aware message-passing network is used to\nencode the augmented graph. MuMo [428] introduces biomedical literature to guide molecular\nproperty prediction. It pretrains a GNN and a language model on paired data of molecules\nand literature mentions via contrastive learning:\n\u2113\n(z\n\ud835\udc3a\n\ud835\udc56\n,z\n\ud835\udc47\n\ud835\udc56\n)\n\ud835\udc56\n= \u2212 log\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\n)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc57=1\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc57\n)/\ud835\udf0f)\n, (130)\nwhere z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\nare the representation of molecule and its corresponding literature. Zhao et al.\n[583] propose a unified Transformer architecture to jointly model molecule graph and the\ncorresponding bioassay description.\n11.3 Training strategies\nDespite the encouraging performance achieved by GNNs, the traditional supervised training scheme\nof GNNs faces a severe limitation: The scarcity of available molecules with desired properties.\nAlthough there are a large number of molecular graphs in public databases such as PubChem,\nlabeled molecules are hard to acquire due to the high cost of wet-lab experiments and quantum\nchemistry calculations. Directly training GNNs on such limited molecules in a supervised way\nis prone to over-fitting and lack of generalization. To address this issue, few-shot learning and\nself-supervised learning are widely used in molecular property prediction.\nFew-shot learning. Few-shot learning aims at generalizing to a task with a small labeled data\nset. The prediction of each property is treated as a single task. Metric-based and optimization-based\nfew-shot learning have been adopted for molecular property prediction. Metric-based few-shot\nlearning is similar to nearest neighbors and kernel density estimation, which learns a metric or\ndistance function over objects. IterRefLSTM [9] leverages matching network [455] as the few\u0002shot learning framework, calculating the similarity between support samples and query samples.\nOptimization-based few-shot learning optimizes a meta-learner for parameter initialization which\ncan be fast adapted to new tasks. Meta-MGNN [161] adopts MAML [120] to train a parameter\ninitialization to adapt to different tasks and use self-attentive task weights for each task. PAR [474]\nalso uses MAML framework and learns an adaptive relation graph among molecules for each task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 51\nSelf-supervised learning. Self-supervised learning can pre-train a GNN model with plenty of\nunlabeled molecular graphs and transfer it to specific molecular property prediction tasks. Self\u0002supervised learning contains generative methods and predictive methods. Predictive methods design\nprediction tasks to capture the intrinsic data features. Pre-GNN [181] exploits both node-level and\ngraph-level prediction tasks including context prediction, attribute masking, graph-level property\nprediction and structural similarity prediction. MGSSL [580] provides a motif-based generative\npre-training framework making topology prediction and motif generation iteratively. Contrastive\nmethods learn graph representations by pulling views from the same graph close and pushing\nviews from different graphs apart. Different views of the same graph are constructed by graph\naugmentation or leveraging the 1D SMILES and 3D structure. MolCLR [478] augments molecular\ngraphs by atom masking, bond deletion and subgraph removal and maximizes the agreement\nbetween the original molecular graph and augmented graphs. Fang et al. [110] uses a chemical\nknowledge graph to guide the graph augmentation. SMICLR [366] uses contrastive learning across\nSMILES and 2D molecular graphs. GeomGCL [268] leverages graph contrastive learning to capture\nthe geometry of the molecule across 2D and 3D views. Jiang et al. [201] and Fang et al. [111] integrate\nmolecule graphs with chemical knowledge graph and fuse the two modalities with contrastive\nlearning. Self-supervised learning can also be combined with few-shot learning to fully leverage\nthe hierarchical information in the training set [215].\n11.4 Summary\nThis section introduces graph representation learning in molecular property prediction and we\nprovide the summary as follows:\n\u2022 Techniques. For molecular property prediction, a molecule is represented as a graph whose\nnodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN, GAT, and GraphSAGE\nare adopted to learn the graph-level representation. The representations are then fed into a\nclassification or regression head for the molecular property prediction tasks. Many works\nguide the model structure design with medical domain knowledge including chemical bond\nfeatures, motif features, different modalities of molecular representation, chemical knowledge\ngraph and literature. Due to the scarcity of available molecules with desired properties,\nfew-shot learning and contrastive learning are used to train molecular property prediction\nmodels, so that the model can leverage the information in large unlabeled dataset and can be\nadapted to new tasks with a few examples.\n\u2022 Challenges and Limitations. Despite the great success of graph representation learning\nin molecular property prediction, the methods still have limitations: 1) Few-shot molecular\nproperty prediction are not fully explored. 2) Most methods depend on training with labeled\ndata, but neglect the chemical domain knowledge.\n\u2022 Future Works. In the future, we expect that: 1) More few-shot learning and zero-shot learning\nmethods are studied for molecular property prediction to solve the data scarcity problem. 2)\nHeterogeneous data can be fused for molecular property prediction. There are a large amount\nof heterogeneous data about molecules such as knowledge graphs, molecule descriptions\nand property descriptions. They can be considered to assist molecular property prediction. 3)\nChemical domain knowledge can be leveraged for the prediction model. For example, when\nwe perform affinity prediction, we can consider molecular dynamics knowledge.\n12 Molecular Generation\nMolecular generation is pivotal to drug discovery, where it serves a fundamental role in downstream\ntasks like molecular docking [341] and virtual screening [457]. The goal of molecular generation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n52 W. Ju, et al.\nis to produce chemical structures that satisfy a specific molecular profile, e.g., novelty, binding\naffinity, and SA scores. Traditional methods have relied on 1D string formats like SMILES [148] and\nSELFIES [240]. With the recent advances in graph representation learning, numerous graph-based\nmethods have also emerged, where molecular graph G can naturally embody both 2D topology and\n3D geometry. While recent literature reviews [99, 342] have covered the general topics of molecular\ndesign, this chapter is dedicated to the applications of graph representation learning in the molecular\ngeneration task. Molecular generation is intrinsically a de novo task, where molecular structures\nare generated from scratch to navigate and sample from the vast chemical space. Therefore, this\nchapter does not discuss tasks that restrict chemical structures a priori, such as docking [131, 427]\nand conformation generation [412, 607].\n12.1 Taxonomy for molecular featurization methods\nThis section categorizes the different methods to feature molecules. The taxonomy presented here\nis unique to the task of molecular generation, owing to the various modalities of molecular entities,\ncomplex interactions with other bio-molecular systems and formal knowledge from the laws of\nchemistry and physics.\n2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one thing, a\nmolecule can be unambiguously represented by its 2D topological graph G2D, where atoms are\nnodes and bonds are edges. G2D can be encoded by canonical MPNN models like GCN [230],\nGAT [452], and R-GCN [401], in ways similar to tasks like social networks and knowledge graphs. A\ntypical example of this line of work is GCPN [543], a graph convolutional policy network generating\nmolecules with desired properties such as synthetic accessibility and drug-likeness.\nFor another, the 3D conformation of a molecule can be accurately depicted by its 3D geometric\ngraph G3D, which incorporates 3D atom coordinates. In 3D-GNNs like SchNet [405] and Orb\u0002Net [371], G3D is organized into a \ud835\udc58-NN graph or a radius graph according to the Euclidean distance\nbetween atoms. It is justifiable to approximate G3D as a 3D extension to G2D, since covalent atoms\nare closest to each other in most cases. However, G3D can also find a more long-standing origin\nin the realm of computational chemistry [126], where both covalent and non-covalent atomistic\ninteractions are considered to optimize the potential surface and simulate molecular dynamics.\nTherefore, G3D more realistically represents the molecular geometry, which makes a good fit for\nprotein pocket binding and 3D-QSAR optimization [453].\nMolecules can rotate and translate, affecting their position in the 3D space. Therefore, it is ideal to\nencode these molecules with GNNs equivariant/invariant to roto-translations, which can be \u223c 103\ntimes more efficient than data augmentation [144]. Equivariant GNNs can be based on irreducible\nrepresentation [10, 24, 37, 130, 446], regular representation [121, 192], or scalarization [190, 212,\n232\u2013234, 292, 398, 404, 405, 445], which are explained in more detail in [165]. Recent works like\nGraphVF [430] and MolCode [579] have been incorporating G2D and G3D to accurately capture the\nrelationship between structure and properties in molecular design in a unified way.\nUnbounded vs. binding-based. Earlier works have aimed to generate unbounded molecules in\neither 2D or 3D space, striving to learn good molecular representations through this task. In the 2D\nscenario, GraphNVP [329] first introduces a flow-based model to learn an invertible transformation\nbetween the 2D chemical space and the latent space. GraphAF [413] further adopts an autoregressive\ngeneration scheme to check the valence of the generated atoms and bonds. In the 3D scenario,\nG-SchNet [142] first proposes to utilize G3D (instead of 3D density grids) as the generation backbone.\nIt encodes G3D via SchNet, and uses an auxiliary token to generate atoms on the discretized 3D\nspace autoregressively. G-SphereNet [316] uses symmetry-invariant representations in a spherical\ncoordinate system (SCS) to generate atoms in the continuous 3D space and preserve equivariance.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 53\nUnbounded models adopt certain techniques to optimize specific properties of the generated\nmolecules. GCPN and GraphAF use scores like logP, QED, and chemical validity to tune the model\nvia reinforcement learning. EDM [178] can generate 3D molecules with property \ud835\udc50 by re-training\nthe diffusion model with \ud835\udc50\u2019s feature vector concatenated to the E(n) equivariant dynamics function\n\ud835\udf50\u02c6\ud835\udc61 = \ud835\udf19 (\ud835\udc9b\ud835\udc61, [\ud835\udc61, \ud835\udc50]). cG-SchNet [143] adopts a conditioning network architecture to jointly target\nmultiple electronic properties during conditional generation without the need to re-train the model.\nRetMol [481] uses a retrieval-based model for controllable generation.\nOn the other hand, binding-based methods generate drug-like molecules (aka. ligands) according\nto the binding site (aka. binding pocket) of a protein receptor. Drawing inspirations from the lock\u0002and-key model for enzyme action [122], works like LiGAN [380] and DESERT [302] uses 3D\ndensity grids to fit the density surface between the ligand and the receptor, encoded by 3D-CNNs.\nMeanwhile, a growing amount of literature has adopted G3D for representing ligand and receptor\nmolecules, because G3D more accurately depicts molecular structures and atomistic interactions\nboth within and between the ligand and the receptor. Representative works include 3D-SBDD [306],\nGraphBP [288], Pocket2Mol [361], and DiffSBDD [403]. GraphBP shares a similar workflow with G\u0002SphereNet, except that the receptor atoms are also incorporated into G3D to depict the 3D geometry\nat the binding pocket.\nAtom-based vs.fragment-based. Molecules are inherently hierarchical structures. At the atom\u0002istic level, molecules are represented by encoding atoms and bonds. At a coarser level, molecules\ncan also be represented as molecular fragments like functional groups or chemical sub-structures.\nBoth the composition and the geometry are fixed within a given fragment, e.g., the planar peptide\u0002bond (\u2013CO\u2013NH\u2013) structure. Fragment-based generation effectively reduces the degree of freedom\n(DOF) of chemical structures, and injects well-established knowledge about molecular patterns\nand reactivity. JT-VAE [207] decomposes 2D molecular graph G2D into a junction-tree structure\nT, which is further encoded via tree message-passing. DeepScaffold [270] expands the provided\nmolecular scaffold into 3D molecules. L-Net [272] adopts a graph U-Net architecture and devises\na custom three-level node clustering scheme for pooling and unpooling operations in molecular\ngraphs. A number of works have also emerged lately for fragment-based generation in the binding\u0002based setting, including FLAG [581] and FragDiff [360]. FLAG uses a regression-based approach to\nsequentially decide the type and torsion angle of the next fragment to be placed at the binding site,\nand finally optimizes the molecule conformation via a pseudo-force field. FragDiff also adopts a\nsequential generation process but uses a diffusion model to determine the type and pose of each\nfragment in one go.\n12.2 Generative methods for molecular graphs\nFor a molecular graph generation process, the model first learns a latent distribution \ud835\udc43 (\ud835\udc4d |G)\ncharacterizing the input molecular graphs. A new molecular graph G\u02c6 is then generated by sam\u0002pling and decoding from this learned distribution. Various models have been adopted to generate\nmolecular graphs, including generative adversarial network (GAN), variational auto-encoder (VAE),\nnormalizing flow (NF), diffusion model (DM), and autoregressive model (AR).\nGenerative adversarial network (GAN). GAN [149] is trained to discriminate real data \ud835\udc99\nfrom generated generated data \ud835\udc9b, with the training object formalized as\nmin\n\ud835\udc3a\nmax\n\ud835\udc37\nL (\ud835\udc37,\ud835\udc3a) = E\ud835\udc99\u223c\ud835\udc5ddata [log\ud835\udc37(\ud835\udc99)] + E\ud835\udc9b\u223c\ud835\udc5d (\ud835\udc9b) [log(1 \u2212 \ud835\udc37(\ud835\udc3a(\ud835\udc9b)))], (131)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n54 W. Ju, et al.\nwhere \ud835\udc3a(\u00b7) is the generator function and \ud835\udc37(\u00b7) is the discriminator function. For example, Mol\u0002GAN [82] encodes G2D with R-GCN, trains \ud835\udc37 and \ud835\udc3a with improved W-GAN [13], and uses rein\u0002forcement learning to generate attributed molecules, where the score function is assigned from\nRDKit [249] and chemical validity.\nVaraitional auto-encoder (VAE). In VAE [228], the decoder parameterizes the conditional\nlikelihood distribution \ud835\udc5d\ud835\udf03 (\ud835\udc99|\ud835\udc9b), and the encoder parameterizes an approximate posterior distribution\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99) \u2248 \ud835\udc5d\ud835\udf03 (\ud835\udc9b|\ud835\udc99). The model is optimized by the evidence lower bound (ELBO), consisting of the\nreconstruction loss term and the distance loss term:\nmax\n\ud835\udf03,\ud835\udf19\nL\ud835\udf03,\ud835\udf19 (\ud835\udc99) := E\ud835\udc9b\u223c\ud835\udc5e\ud835\udf19 (\u00b7 |\ud835\udc99)\n\u0014\nln \ud835\udc5d\ud835\udf03 (\ud835\udc99, \ud835\udc9b)\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99)\n\u0015\n= ln \ud835\udc5d\ud835\udf03 (\ud835\udc99) \u2212 \ud835\udc37KL \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc99) \u2225\ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc99)\n\u0001\n. (132)\nMaximizing ELBO is equivalent to simultaneously maximizing the log-likelihood of the observed\ndata, and minimizing the divergence of the approximate posterior \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc65) from the exact poste\u0002rior \ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc65). Representative works along this thread include JT-VAE [207], GraphVAE [419], and\nCGVAE [290] for the 2D generation task, and 3DMolNet [351] for the 3D generation task.\nAutoregressive model (AR). Autoregressive model is an umbrella definition for any model\nthat sequentially generates the components (atoms or fragments) of a molecule. ARs better capture\nthe interdependency within the molecular structure and allows for explicit valency check. For each\nstep in AR, the new component can be produced using different techniques:\n\u2022 Regression/classification, such is the case with 3D-SBDD [306], Pocket2Mol [361], etc.\n\u2022 Reinforcement learning, such is the case with L-Net [272], DeepLigBuilder [273], etc.\n\u2022 Probabilistic models like normalizing flow and diffusion.\nNormalizing flow (NF). Based on the change-of-variable theorem, NF [385] constructs an\ninvertible mapping \ud835\udc53 between a complex data distribution \ud835\udc99 \u223c \ud835\udc4b: and a normalized latent distribu\u0002tion \ud835\udc9b \u223c \ud835\udc4d. Unlike VAE, which has juxtaposed parameters for encoder and decoder, the flow model\nuses the same set of parameter for encoding and encoding: reverse flow \ud835\udc53\n\u22121\nfor generation, and\nforward flow \ud835\udc53 for training:\nmax\n\ud835\udc53\nlog \ud835\udc5d(\ud835\udc99) = log \ud835\udc5d\ud835\udc3e (\ud835\udc9b\ud835\udc3e) (133)\n= log \ud835\udc5d\ud835\udc3e\u22121 (\ud835\udc9b\ud835\udc3e\u22121) \u2212 log\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc3e (\ud835\udc9b\ud835\udc3e\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc3e\u22121\n\u0013\n(134)\n= . . . (135)\n= log \ud835\udc5d0 (\ud835\udc9b0) \u2212\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc56=1\nlog\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc56 (\ud835\udc9b\ud835\udc56\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc56\u22121\n\u0013\n, (136)\nwhere \ud835\udc53 = \ud835\udc53\ud835\udc3e \u25e6 \ud835\udc53\ud835\udc3e\u22121 \u25e6 ... \u25e6 \ud835\udc531 is a composite of \ud835\udc3e blocks of transformation. While GraphNVP [329]\ngenerates the molecular graph with NF in one go, following works tend to use the autoregressive\nflow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439].\nDiffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion\nsteps to slowly add random noise to data \ud835\udc990 \u223c \ud835\udc5e(\ud835\udc99):\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121) = N (\ud835\udc99\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc99\ud835\udc61\u22121, \ud835\udefd\ud835\udc61 \ud835\udc70), (137)\n\ud835\udc5e(\ud835\udc991:\ud835\udc47 |\ud835\udc990) =\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121). (138)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 55\nTable 11. Summary of molecular generation models.\nModel 2D/3D Binding\u0002based\nFragment\u0002basedGNN\nBackbone\nGenerative\nModel\nGCPN [543] 2D GCN [230] GAN\nMolGAN [82] 2D R-GCN [401] GAN\nDEFactor [16] 2D GCN GAN\nGraphVAE [419] 2D ECC [418] VAE\nMDVAE [100] 2D GGNN [274] VAE\nJT-VAE [207] 2D \u2713 MPNN [147] VAE\nCGVAE [290] 2D GGNN VAE\nDeepScaffold [270] 2D \u2713 GCN VAE\nGraphNVP [329] 2D R-GCN NF\nMoFlow [557] 2D R-GCN NF\nGraphAF [413] 2D R-GCN NF + AR\nGraphDF [318] 2D R-GCN NF + AR\nL-Net [272] 3D \u2713 g-U-Net [133] AR\nG-SchNet [142] 3D SchNet [405] AR\nGEN3D [388] 3D EGNN [398] AR\nG-SphereNet [316] 3D SphereNet [292] NF + AR\nEDM [178] 3D EGNN DM\nGCDM [347] 3D GCPNet [346] DM\n3D-SBDD [306] 3D \u2713 SchNet AR\nPocket2Mol [361] 3D \u2713 GVP [211] AR\nFLAG [581] 3D \u2713 \u2713 SchNet AR\nGraphBP [288] 3D \u2713 SchNet NF + AR\nSiamFlow [439] 3D \u2713 R-GCN NF\nDiffBP [282] 3D \u2713 EGNN DM\nDiffSBDD [403] 3D \u2713 EGNN DM\nTargetDiff [156] 3D \u2713 EGNN DM\nFragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR\nGraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR\nMolCode [579] 2D + 3D \u2713 EGNN NF + AR\nThey then learn to reverse the diffusion process to construct desired data samples from the noise:\n\ud835\udc5d\ud835\udf03 (\ud835\udc990:\ud835\udc47 ) = \ud835\udc5d(\ud835\udc99\ud835\udc47 )\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61), (139)\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61) = N (\ud835\udc99\ud835\udc61\u22121; \ud835\udf41\ud835\udf03(\ud835\udc99\ud835\udc61, \ud835\udc61), \ud835\udeba\ud835\udf03 (\ud835\udc99\ud835\udc61, \ud835\udc61)), (140)\nwhile the models are trained using a variational lower bound. Diffusion models have been applied\nto generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands\nin DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate\nmolecular fragments in autoregressive models, as is the case with FragDiff [360].\n12.3 Summary and prospects\nWe wrap up this chapter with Table 11, which profiles existing molecular generation models\naccording to their taxonomy for molecular featurization, the GNN backbone, and the generative\nmethod. This chapter covers the critical topics of molecular generation, which also elicit valuable\ninsights into the promising directions for future research. We summarize these important aspects\nas follows.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n56 W. Ju, et al.\nTechniques. Graph neural networks can be flexibly leveraged to encode molecular features\non different representation levels and across different problem settings. Canonical GNNs like\nGCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs,\nwhile 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular,\nthis 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the\nbinding protein receptor is considered alongside the ligand geometry per se. Fragment-based models\nlike JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure.\nVarious generative methods have also been effectively incorporated into the molecular setting,\nincluding generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive\nmodel (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to\ngenerate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating\nthe search for drug candidates.\nChallenges and Limitations. While there has been an abundant supply of unlabelled molecular\nstructural and geometric data [125, 193, 426], the number of labeled molecular data over certain\ncritical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the\nother hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of\nthe generated molecules, such as QED and Vina [103], rather than actual wet lab experiments.\nFuture Works. Besides the structural and geometric attributes described in this chapter, an\neven more extensive array of data can be applied to aid molecular generation, including chemical\nreactions and medical ontology. These data can be organized into a heterogeneous knowledge\ngraph to aid the extraction of high-quality molecular representations. Furthermore, high through\u0002put experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and\ndruggability of the generated molecules in the wet lab. Concrete case studies, such as the design of\npotential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights\ninto leveraging these molecular generative models to facilitate the design and fabrication of potent\nand applicable drug molecules in the pharmaceutical industry.\nIntegrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations\noffers a promising new direction in molecular generation. Recent studies like those by [196] and\n[160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLM\u0002based approaches in this domain, including those by [338] and [18], predominantly utilize textual\nSMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The\nemerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image,\nand text, which could more comprehensively capture the intricacies of molecular structures. This\napproach marks a significant shift towards utilizing graph-based information alongside traditional\ntext, enhancing the capability of LLMs in molecular generation. Such advances suggest that future\nresearch should focus more on exploiting the synergy between graph-based molecular representa\u0002tions and the evolving landscape of LLMs to address complex challenges in chemistry and material\nsciences.\n13 Recommender Systems\nThe use of graph representation learning in recommender systems has been drawing increasing\nattention as one of the key strategies for addressing the issue of information overload. With their\nstrong ability to capture high-order connectivity between graph nodes, deep graph representation\nlearning has been shown to be beneficial in enhancing recommendation performance across a\nvariety of recommendation scenarios.\nTypical recommender systems take the observed interactions between users and items and\ntheir fixed features as input, and are intended for making proper predictions on which items a\nspecific user is probably interested in. To formulate, given an user set U, an item set I and the\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 57\nTable 12. Summary of graph models for recommender systems.\nModel Recommendation Task Graph Structure Graph Encoder Representation\nGC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer\nNGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation\nMMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer\nLightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling\nDGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling\nCAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling\nSR-GNN [496] Session-based Transition Graph GGNN Soft-Attention\nGC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention\nFGNN [377] Session-based Session Graph GAT Last-Layer\nGAG [378] Session-based Session Graph GCN Self-Attention\nGCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling\nHyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention\nDHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer\nMBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention\nHCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer\nH3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer\nSTHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling\ninteraction matrix between users and items \ud835\udc4b \u2208 {0, 1}\n|U|\u00d7|I|\n, where \ud835\udc4b\ud835\udc62,\ud835\udc63 indicates there is an\nobserved interaction between user \ud835\udc62 and item \ud835\udc56. The target of GNNs on recommender systems is to\nlearn representations \u210e\ud835\udc62, \u210e\ud835\udc56 \u2208 R\n\ud835\udc51\nfor given \ud835\udc62 and \ud835\udc56. The preference score can further be calculated\nby a similarity function:\n\ud835\udc65\u02c6\ud835\udc62,\ud835\udc56 = \ud835\udc53 (\u210e\ud835\udc62, \u210e\ud835\udc56), (141)\nwhere \ud835\udc53 (\u00b7, \u00b7) is the similarity function, e.g. inner product, cosine similarity, multi-layer perceptrons\nthat takes the representation of \ud835\udc62 and \ud835\udc56 and calculate the preference score \ud835\udc65\u02c6\ud835\udc62,\ud835\udc56.\nWhen it comes to adapting graph representation learning in recommender systems, a key step is\nto construct graph-structured data from the interaction set \ud835\udc4b. Generally, a graph is represented\nas G = {V, E} where V, E denotes the set of vertices and edges respectively. According to the\nconstruction of G, we can categorize the existing works as follows into three parts which are\nintroduced in the following subsections. A summary is provided in Table 12.\n13.1 User-Item Bipartite Graph\n13.1.1 Graph Construction A undirected bipartite graph where the vertex set V = U \u222a I and the\nundirected edge set E = {(\ud835\udc62,\ud835\udc56)|\ud835\udc62 \u2208 U \u2227\ud835\udc56 \u2208 I}. Under this case the graph adjacency can be directly\nobtained from the interaction matrix, thus the optimization target on the user-item bipartite graph\nis equivalent to collaborative filtering tasks such as MF [239] and SVD++ [238].\nThere have been plenty of previous works that applied GNNs on the constructed user-item bipar\u0002tite graphs. GC-MC [27] firstly applies graph convolution networks to user-item recommendation\nand optimizes a graph autoencoder (GAE) to reconstruct interactions between users and items.\nNGCF [470] introduces the concept of Collaborative Filtering (CF) into graph-based recommen\u0002dations by modeling the affinity between neighboring nodes on the interaction graph. MMGCN\n[485] extends the graph-based recommendation to multi-modal scenarios by constructing different\nsubgraphs for each modal. LightGCN [169] improves NGCF by removing the non-linear activation\nfunctions and simplifying the message function. With the development of disentangled representa\u0002tion learning, there are works like DGCF [473] that introduce disentangled graph representation\nlearning to represent users and items from multiple disentangled perspectives. Additionally, having\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n58 W. Ju, et al.\nrealized the limitation of the existing message-passing scheme in capturing collaborative signals,\nCAGCN [480] proposes Common Interacted Ratio (CIR) as a recommendation-oriented topological\nmetric for GNN-based recommender models.\n13.1.2 Graph Propagation Scheme A common practice is to follow the traditional message-passing\nnetworks (MPNNs) and design the graph propagation method accordingly. GC-MC adopts vanilla\nGCNs to encode the user-item bipartite graph. NGCF enhances GCNs by considering the affinity\nbetween users and items. The message function of NGCF from node \ud835\udc57 to \ud835\udc56 is formulated as:\n(\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc57 = \u221a 1\n|N\ud835\udc56| |N\ud835\udc57|\n(\ud835\udc4a1\ud835\udc52\ud835\udc57 +\ud835\udc4a2 (\ud835\udc52\ud835\udc56 \u2299 \ud835\udc52\ud835\udc57))\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc56 = \ud835\udc4a1\ud835\udc52\ud835\udc56\n, (142)\nwhere \ud835\udc4a1,\ud835\udc4a2 are trainable parameters, \ud835\udc52\ud835\udc56 represents \ud835\udc56\u2019s representation from previous layer. The\nmatrix form can be further provided by:\n\ud835\udc38\n(\ud835\udc59) = LeakyReLU( (L + \ud835\udc3c)\ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n1\n+ L\ud835\udc38\n(\ud835\udc59\u22121) \u2299 \ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n2\n), (143)\nwhere L represents the Laplacian matrix of the user-item graph. The element-wise product in Eq.\n143 represents the affinity between connected nodes, containing the collaborative signals from\ninteractions.\nHowever, the notable heaviness and burdensome calculation of NGCF\u2019s architecture hinder\nthe model from making faster recommendations on larger graphs. LightGCN solves this issue by\nproposing Light Graph Convolution (LGC), which simplifies the convolution operation with:\n\ud835\udc52\n(\ud835\udc59+1)\n\ud835\udc56\n=\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n1\n\u221a\ufe01\n|N\ud835\udc56||N\ud835\udc57|\n\ud835\udc52\n(\ud835\udc59)\n\ud835\udc57\n. (144)\nWhen an interaction takes place, e.g. a user clicks a particular item, there could be multiple\nintentions behind the observed interaction. Thus it is necessary to consider the various disentangled\nintentions among users and items. DGCF proposes the cross-intent embedding propagation scheme\non the graph, inspired by the dynamic routing algorithm of capsule networks [394]. To formulate,\nthe propagation process maintains a set of routing logits \u02dc\ud835\udc46\ud835\udc58 (\ud835\udc62,\ud835\udc56) for each user \ud835\udc62. The weighted sum\naggregator to get the representation of \ud835\udc62 can be defined as:\n\ud835\udc62\n\ud835\udc61\n\ud835\udc58\n=\n\u2211\ufe01\n\ud835\udc56\u2208N\ud835\udc62\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) \u00b7 \ud835\udc56\n0\n\ud835\udc58\n(145)\nfor \ud835\udc61-th iteration, where L\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) denotes the Laplacian matrix of \ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56), formulated as:\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) =\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n\u221a\ufe03\n[\n\u00cd\n\ud835\udc56\n\u2032\u2208N\ud835\udc62\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56\u2032)] \u00b7 [\u00cd\n\ud835\udc62\n\u2032\u2208N\ud835\udc56\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62\n\u2032\n,\ud835\udc56)]\n. (146)\n13.1.3 Node Representations After the graph propagation module outputs node-level representa\u0002tions, there are multiple methods to leverage node representations for recommendation tasks. A\nplain solution is to apply a readout function on layer outputs like the concatenation operation used\nby NGCF:\n\ud835\udc52\n\u2217 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc52(0)\n, ..., \ud835\udc52 (\ud835\udc3f)) = \ud835\udc52\n(0)\n\u2225...\u2225\ud835\udc52\n(\ud835\udc3f)\n. (147)\nHowever, the readout function among layers would neglect the relationship between the target\nitem and the current user. A general solution is to use the attention mechanism [451] to reweight\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 59\nand aggregate the node representations. SR-GNN adapts soft-attention mechanism to model the\nitem-item relationship:\n\ud835\udefc\ud835\udc56 = q\n\ud835\udc47\n\ud835\udf0e(\ud835\udc4a1\ud835\udc52\ud835\udc61 +\ud835\udc4a2\ud835\udc52\ud835\udc56 + \ud835\udc50),\n\ud835\udc60\ud835\udc54 =\n\ud835\udc5b\u2211\ufe01\u22121\n\ud835\udc56=1\n\ud835\udefc\ud835\udc56\ud835\udc52\ud835\udc56,\n(148)\nwhere q, \ud835\udc4a1, \ud835\udc4a2 are trainable matrices.\nSome methods focus on exploiting information from multiple graph structures. A common\npractice is contrastive learning, which maximizes the mutual information between hidden repre\u0002sentations from several views. HCCF leverage InfoNCE loss as the estimator of mutual information,\ngiven a pair of representation \ud835\udc67\ud835\udc56, \u0393\ud835\udc56 for node \ud835\udc56, controlled by temperature parameter \ud835\udf0f:\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc41\ud835\udc36\ud835\udc38 (\ud835\udc56) = \u2212 log exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56))/\ud835\udf0f\n\u00cd\n\ud835\udc56\n\u2032\u2260\ud835\udc56 exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56\n\u2032 ))/\ud835\udf0f\n. (149)\nBesides InfoNCE, there exist several other ways to combine node representations from different\nviews. For instance, MBHT applies an attention mechanism to fuse multiple semantics, DisenPOI\nadapts bayesian personalized ranking loss (BPR) [384] as a soft estimator for contrastive learning,\nand KBGNN applies pair-wise similarities to ensure the consistency from two views.\n13.2 Transition Graph\n13.2.1 Transition Graph Construction Since sequence-based recommendation (SR) is one of the\nfundamental problems in recommender systems, some researches focus on modeling the sequential\ninformation with GNNs. A commonly applied way is to construct transition graphs based on each\ngiven sequence according to the clicking sequence by a user. To formulate, given a user \ud835\udc62\u2019s clicking\nsequence \ud835\udc60\ud835\udc62 = [\ud835\udc56\ud835\udc62,1,\ud835\udc56\ud835\udc62,2, ...,\ud835\udc56\ud835\udc62,\ud835\udc5b] containing \ud835\udc5b items, noting that there could be duplicated items, the\nsequential graph is constructed via G\ud835\udc60 = {SET(\ud835\udc60\ud835\udc62), E}, where \u2200\n\ud835\udc56\ud835\udc57,\ud835\udc56\ud835\udc58\n\u2208 E indicates there exists\na successive transition from \ud835\udc56\ud835\udc57 to \ud835\udc56\ud835\udc58 . Since G\ud835\udc60 are directed graphs, a widely used way to depict\ngraph connectivity is by building the connection matrix \ud835\udc34\ud835\udc60 \u2208 R\n\ud835\udc5b\u00d72\ud835\udc5b\n. \ud835\udc34\ud835\udc60is the combination of two\nadjacency matrices \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n], which denotes the normalized node degrees of incoming\nand outgoing edges in the session graph respectively.\nThe proposed transition graphs that obtain user behavior patterns have been demonstrated\nimportant to session-based recommendations [263, 291]. SR-GNN and GC-SAN [496, 516] propose\nto leverage transition graphs and apply attention-based GNNs to capture the sequential information\nfor session-based recommendation. FGNN [377] formulates the recommendation within a session\nas a graph classification problem to predict the next item for an anonymous user. GAG [378] and\nGCE-GNN [482] further extend the model to capture global embeddings among multiple session\ngraphs.\n13.2.2 Session Graph Propagation Since the session graphs are directed item graphs, there have\nbeen multiple session graph propagation methods to obtain node representations on session graphs.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n60 W. Ju, et al.\nSR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information\nfrom a given session graph adjacency \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n] and item embedding set {\ud835\udc52\ud835\udc56 }:\n\ud835\udc4e\ud835\udc61 = \ud835\udc34\ud835\udc60 [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121]\n\ud835\udc47\ud835\udc3b + \ud835\udc4f, (150)\n\ud835\udc67\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc67\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc67\ud835\udc52\ud835\udc61\u22121), (151)\n\ud835\udc5f\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc5f\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc61\u22121), (152)\n\ud835\udc52\u02dc\ud835\udc61 = tanh(\ud835\udc4a\ud835\udc5c\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5c (\ud835\udc5f\ud835\udc61 \u2299 \ud835\udc52\ud835\udc61\u22121)), (153)\n\ud835\udc52\ud835\udc61 = (1 \u2212 \ud835\udc67\ud835\udc61) \u2299 \ud835\udc52\ud835\udc61\u22121 + \ud835\udc67\ud835\udc61\ud835\udc52\u02dc\ud835\udc61, (154)\nwhere \ud835\udc4a s and \ud835\udc48 s are trainable parameters. GC-SAN extend GGNN by calculating initial state \ud835\udc4e\ud835\udc61\nseparately to better exploit transition information:\n\ud835\udc4e\ud835\udc61 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc56\ud835\udc5b)\n), \ud835\udc34(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n)). (155)\n13.3 HyperGraph\n13.3.1 Hypergraph Topology Construction Motivated by the idea of modeling hyper-structures\nand high-order correlation among nodes, hypergraphs [119] are proposed as extensions of the\ncommonly used graph structures. For graph-based recommender systems, a common practice is\nto construct hyper structures among the original user-item bipartite graphs. To be specific, an\nincidence matrix of a graph with vertex set V is presented as a binary matrix \ud835\udc3b \u2208 {0, 1}\n|V |\u00d7 | E |\n,\nwhere E represents the set of hyperedges. Each entry \u210e(\ud835\udc63, \ud835\udc52) of \ud835\udc3b depicts the connectivity between\nvertex \ud835\udc63 and hyperedge \ud835\udc52:\n\u210e(\ud835\udc63, \ud835\udc52) =\n(\n1 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2208 \ud835\udc52\n0 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2209 \ud835\udc52\n. (156)\nGiven the formulation of hypergraphs, the degrees of vertices and hyperedges of \ud835\udc3b can then be\ndefined with two diagonal matrices \ud835\udc37\ud835\udc63 \u2208 N\n|V |\u00d7 |V | and \ud835\udc37\ud835\udc52 \u2208 N| E |\u00d7 | E |, where\n\ud835\udc37\ud835\udc63 (\ud835\udc56;\ud835\udc56) =\n\u2211\ufe01\n\ud835\udc52\u2208 E\n\u210e(\ud835\udc63\ud835\udc56, \ud835\udc52), \ud835\udc37\ud835\udc52 (\ud835\udc57; \ud835\udc57) =\n\u2211\ufe01\n\ud835\udc63\u2208V\n\u210e(\ud835\udc63, \ud835\udc52\ud835\udc57). (157)\nThe development of Hypergraph Neural Networks (HGNNs) [119, 188, 598] have shown to\nbe capable of capturing the high-order connectivity between nodes. HyperRec [463] firstly at\u0002tempts to leverage hypergraph structures for sequential recommendation by connecting items\nwith hyperedges according to the interactions with users during different time periods. DHCF\n[198] proposes to construct hypergraphs for users and items respectively based on certain rules, to\nexplicitly capture the collaborative similarities via HGNNs. MBHT [532] combines hypergraphs\nwith a low-rank self-attention mechanism to capture the dynamic heterogeneous relationships\nbetween users and items. HCCF [505] uses the contrastive information between hypergraph and\ninteraction graph to enhance the recommendation performance. To extend the model\u2019s ability to\nmulti-domain categories of items, H3Trans [523] incorporates two hyperedge-based modules and\nleverages hierarchical hypergraph propagation to transfer from domains. STHGCN [524] formulates\na spatio-temporal hypergraph structure for POI recommendation.\n13.3.2 Hyper Graph Message Passing With the development of HGNNs, previous works have\nproposed different variants of HGNN to better exploit hypergraph structures. A classic high-order\nhyper convolution process on a fixed hypergraph G = {V, E} with hyper adjacency \ud835\udc3b is given by:\n\ud835\udc54 \u2605\ud835\udc4b = \ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc3b\ud835\udc37\u22121\n\ud835\udc52 \ud835\udc3b\n\ud835\udc47\ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc4b\u0398, (158)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 61\nwhere \ud835\udc37\ud835\udc63, \ud835\udc37\ud835\udc52 are degree matrices of nodes and hyperedges, \u0398 denotes the convolution kernel. For\nhyper adjacency matrix \ud835\udc3b, DHCF refers to a rule-based hyperstructure via k-order reachable rule,\nwhere nodes in the same hyperedge group are k-order reachable to each other:\n\ud835\udc34\n\ud835\udc58\n\ud835\udc62 = min(1, power(\ud835\udc34 \u00b7 \ud835\udc34\n\ud835\udc47\n, \ud835\udc58)), (159)\nwhere \ud835\udc34 denotes the graph adjacency matrix. By considering the situations where \ud835\udc58 = 1, 2, the\nmatrix formulation of the hyper connectivity of users and items is calculated with:\n(\n\ud835\udc3b\ud835\udc62 = \ud835\udc34\u2225 (\ud835\udc34(\ud835\udc34\n\ud835\udc47\ud835\udc34))\n\ud835\udc3b\ud835\udc56 = \ud835\udc34\n\ud835\udc47\n\u2225 (\ud835\udc34\n\ud835\udc47\n(\ud835\udc34\ud835\udc34\ud835\udc47))\n, (160)\nwhich depicts the dual hypergraphs for users and items.\nHCCF proposes to construct a learnable hypergraph to depict the global dependencies between\nnodes on the interaction graph. To be specific, the hyperstructure is factorized with two low-rank\nembedding matrices to achieve model efficiency:\n\ud835\udc3b\ud835\udc62 = \ud835\udc38\ud835\udc62 \u00b7\ud835\udc4a\ud835\udc62, \ud835\udc3b\ud835\udc63 = \ud835\udc38\ud835\udc63 \u00b7\ud835\udc4a\ud835\udc63 . (161)\n13.4 Other Graphs\nSince there are a variety of recommendation scenarios, several tailored designed graph structures\nhave been proposed accordingly, to better exploit the domain information from different scenarios.\nFor instance, CKE [564] and MKR [462] introduce Knowledge graphs to enhance graph recommen\u0002dation. GSTN [484], KBGNN [219], DisenPOI [373] and Diff-POI [374] propose to build geographical\ngraphs based on the distance between Point-of-Interests (POIs) to better model the locality of users\u2019\nvisiting patterns. TGSRec [109] and DisenCTR [475] empower the user-item interaction graphs with\ntemporal sampling between layers to obtain sequential information from static bipartite graphs.\n13.5 Summary\nThis section introduces the application of different kinds of graph neural networks in recommender\nsystems and can be summarized as follows:\n\u2022 Graph Constructions. There are multiple options for constructing graph-structured data\nfor a variety of recommendation tasks. For instance, the user-item bipartite graphs reveal\nthe high-order collaborative similarity between users and items, and the transition graph\nis suitable for encoding sequential information in clicking history. These diversified graph\nstructures provide different views for node representation learning on users and items, and\ncan be further used for downstream ranking tasks.\n\u2022 Challenges and Limitations. Though the superiority of graph-structured data and GNNs\nagainst traditional methods has been widely illustrated, there are still challenges unsolved.\nFor example, the computational cost of graph methods is normally expensive and thus\nunacceptable in real-world applications. The data sparsity and cold-started issue in graph\nrecommendation remains to be explored as well.\n\u2022 Future Works. In the future, an efficient solution for applying GNNs in recommendation\ntasks is expected. There are also some attempts [109, 372, 475] on incorporating temporal\ninformation in graph representation learning for sequential recommendation tasks.\n14 Traffic Analysis\nIntelligent Transportation Systems (ITS) are essential for safe, reliable, and efficient transportation\nin smart cities, serving the daily commuting and traveling needs of millions of people. To support\nITS, advanced modeling and analysis techniques are necessary, and Graph Neural Networks (GNNs)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n62 W. Ju, et al.\nare a promising tool for traffic analysis. GNNs can effectively model spatial correlations, making\nthem well-suited for analyzing complex transportation networks. As such, GNNs have garnered\nsignificant interest in the traffic domain for their ability to provide insights into traffic patterns and\nbehaviors [260].\nIn this section, we first conclude the main GNN research directions in the traffic domain, and\nthen we summarize the typical graph construction processes in different traffic scenes and datasets.\nFinally, we list the classical GNN workflows for dealing with tasks in traffic networks. A summary\nis provided in Table 13.\n14.1 Research Directions in Traffic Domain\nWe summarize main GNN research directions in the traffic domain as follows,\n\u2022 Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role in ITS [90, 381],\nwhich involves leveraging spatial-temporal data collected by various sensors to gain insights\ninto future traffic patterns and behaviors. Classic methods, like autoregressive integrated\nmoving average (ARIMA) [36], support vector machine (SVM) [171] and recurrent neural\nnetworks (RNN) [76] can only model time series separately without considering their spatial\nconnections. To address this issue, graph neural networks (GNNs) have emerged as a powerful\napproach for traffic forecasting due to their strong ability of modeling complex graph\u0002structured correlations [40, 202, 277, 353, 383, 506, 592].\n\u2022 Trajectory Prediction. Trajectory prediction is a crucial task in various applications, such\nas autonomous driving and traffic surveillance, which aims to forecast future positions of\nagents in the traffic scene. However, accurately predicting trajectories can be challenging, as\nthe behavior of an agent is influenced not only by its own motion but also by interactions\nwith surrounding objects. To address this challenge, Graph Neural Networks (GNNs) have\nemerged as a promising tool for modeling complex interactions in trajectory prediction\n[44, 345, 432, 600]. By representing the scene as a graph, where each node corresponds to an\nagent and the edges capture interactions between them, GNNs can effectively capture spatial\ndependencies and interactions between agents. This makes GNNs well-suited for predicting\ntrajectories that accurately capture the behavior of agents in complex traffic scenes.\n\u2022 Traffic Anomaly Detection. Anomaly detection is an essential support for ITS. There are\nlots of traffic anomalies in daily transportation systems, for example, traffic accidents, extreme\nweather and unexpected situations. Handling these traffic anomalies timely can improve the\nservice quality of public transportation. The main trouble of traffic anomaly detection is the\nhighly twisted spatial-temporal characteristics of traffic data. The criteria and influence of\ntraffic anomaly vary among locations and times. GNNs have been introduced and achieved\nsuccess in this domain [66, 85, 86, 565].\n\u2022 Others. Traffic demand prediction targets at estimating the future number of traveling at\nsome location. It is of vital and practical significance in the resource scheduling for ITS. By\nusing GNNs, the spatial dependencies of demands can be revealed [530, 535]. What is more,\nurban vehicle emission analysis is also considered in recent work, which is closely related to\nenvironmental protection and gains increasing researcher attention [521].\n14.2 Traffic Graph Construction\n14.2.1 Traffic Graph The traffic network is represented as a graph G = (\ud835\udc49 , \ud835\udc38, \ud835\udc34), where \ud835\udc49 is the\nset of \ud835\udc41 traffic nodes, \ud835\udc38 is the set of edges, and \ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is an adjacency matrix representing the\nconnectivity of \ud835\udc41 nodes. In the traffic domain, \ud835\udc49 usually represents a set of physical nodes, like\ntraffic stations or traffic sensors. The features of nodes typically depend on the specific task. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 63\nTable 13. Summary of graph models for traffic analysis.\nModels Tasks Adjcency matrices GNN types Temporal modules\nSTGCN[545] Traffic Flow Forecasting Fixed Matrix GCN TCN\nDCRNN[275] Traffic Flow Forecasting Fixed Matrix ChebNet RNN\nAGCRN [19] Traffic Flow Forecasting Dynamic Matrix GCN GRU\nASTGCN [159] Traffic Flow Forecasting Fixed Matrix GAT Attention&TCN\nGraphWaveNet [500] Traffic Flow Forecasting Dynamic Matrix GCN Gated-TCN\nSTSGCN [422] Traffic Flow Forecasting Dynamic Matrix GCN Cropping\nLSGCN [187] Traffic Flow Forecasting Fixed Matrix GAT GLU\nGAC-Net [424] Traffic Flow Forecasting Fixed Matrix GAT Gated-TCN\nSTGODE [112] Traffic Flow Forecasting Fixed Matrix Graph ODE TCN\nSTG-NCDE [70] Traffic Flow Forecasting Dynamic Matrix GCN NCDE\nDDGCRN [488] Traffic Flow Forecasting Dynamic Matrix GAT RNN\nMS-ASTN [467] OD Flow Forecasting OD Matrix GCN LSTM\nSocial-STGCNN [345] Trajectory Prediction Fixed Matrix GCN TXP-CNN\nRSBG [432] Trajectory Prediction Dynamic Matrix GCN LSTM\nATG [555] Trajectory Prediction Fixed Matrix GODE NODE\nSTGAN [86] Anomaly Detection Fixed Matrix GCN GRU\nDMVST-VGNN [206] Traffic Demand Prediction Fixed Matrix GAT GLU\nDST-GNN [185] Traffic Demand Prediction Dynamic Matrix GCN Transformer\nTC-SGC [355] Traffic Speed Prediction Fixed Matrix GCN GRU\ntraffic flow forecasting as an example. The features are the traffic flows, i.e., the historical time\nseries of nodes. The traffic flow can be represented as a flow matrix \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc47\n, where \ud835\udc41 is the\nnumber of traffic nodes and \ud835\udc47 is the length of historical series, and \ud835\udc4b\ud835\udc5b\ud835\udc61 denotes the traffic flow of\nnode \ud835\udc5b at time \ud835\udc61. The goal of traffic flow forecasting is to learn a mapping function \ud835\udc53 to predict the\ntraffic flow during future \ud835\udc47\n\u2032\nsteps given the historical \ud835\udc47 step information, which can be formulated\nas follows:\n\u0002\n\ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +1, \ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61; G\n\u0003 \ud835\udc53\n\u2212\u2192 \u0002\ud835\udc4b:,\ud835\udc61+1, \ud835\udc4b:,\ud835\udc61+2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61+\ud835\udc47\n\u2032\n\u0003\n. (162)\n14.2.2 Graph Construction Constructing a graph to describe the interactions among traffic nodes,\ni.e., the design of the adjacency matrix \ud835\udc34, is the key part of traffic analysis. The mainstream designs\ncan be divided into two categories, fixed matrix and dynamic matrix.\nFixed matrix. Lots of works assume that the correlations among traffic nodes are fixed and\nconstant over time, and they design a fixed and pre-defined adjacency matrix to capture the spatial\ncorrelation. Here we list several common choices of fixed adjacency matrix.\nThe connectivity matrix is the most natural construction way. It relies on the support of\nroad map data. The element of the connectivity matrix is defined as 1 if two nodes are physically\nconnected and 0 otherwise. This binary format is convenient to construct and easy to interpret.\nThe distance-based matrix is also a common choice, which shows the connection between two\nnodes more precisely. The elements of the matrix are defined as the function of distance between\ntwo nodes (driving distance or geographical distance). A typical way is to use the threshold Gaussian\nfunction as follows,\n\ud835\udc34\ud835\udc56\ud835\udc57 =\n(\nexp(\u2212\ud835\udc51\n2\n\ud835\udc56 \ud835\udc57\n\ud835\udf0e\n2 ), \ud835\udc51\ud835\udc56\ud835\udc57  \ud835\udf16\n, (163)\nwhere \ud835\udc51\ud835\udc56\ud835\udc57 is the distance between node \ud835\udc56 and \ud835\udc57, and \ud835\udf0e and \ud835\udf16 are two hyperparameters to control the\ndistribution and the sparsity of the matrix.\nAnother kind of fixed adjacency matrix is the similarity-based matrix. In fact, a similarity\nmatrix is not an adjacency matrix to some extent. It is constructed according to the similarity of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n64 W. Ju, et al.\ntwo nodes, which means the neighbors in the similarity graph may be far away in the real world.\nThere are various similarity metrics. For example, many works measure the similarity of two nodes\nby their functionality, e.g., the distribution of surrounding points of interest (POIs). The assumption\nbehind this is that nodes that share similar functionality may share similar traffic patterns. We\ncan also define the similarity through the historical flow patterns. To compute the similarity of\ntwo-time series, a common practice is to use Dynamic Time Wrapping (DTW) algorithm [350],\nwhich is superior to other metrics due to its sensitivity to shape similarity rather than point-wise\nsimilarity. Specifically, given two time series \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc5b) and \ud835\udc4c = (\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc5b), DTW is\na dynamic programming algorithm defined as\n\ud835\udc37(\ud835\udc56, \ud835\udc57) = \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) + min (\ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57), \ud835\udc37(\ud835\udc56, \ud835\udc57 \u2212 1), \ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57 \u2212 1)) , (164)\nwhere \ud835\udc37(\ud835\udc56, \ud835\udc57) represents the shortest distance between subseries \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc56) and \ud835\udc4c =\n(\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc57), and\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) is some distance metric like absolute distance. As a result,\ud835\udc37\ud835\udc47\ud835\udc4a (\ud835\udc4b, \ud835\udc4c) =\n\ud835\udc37(\ud835\udc5b, \ud835\udc5b) is set as the final distance between \ud835\udc4b and \ud835\udc4c, which better reflects the similarity of the\ntwo-time series compared to the Euclidean distance.\nDynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect complete\ninformation of spatial correlations. The dynamic adaptive matrix is proposed to solve the issue.\nThe dynamic matrix is learned from input data automatically. To achieve the best prediction\nperformance, the dynamic matrix will manage to infer the hidden correlations among nodes, more\nthan those physical connections.\nA typical practice is learning adjacency matrix from node embeddings [19]. Let \ud835\udc38\ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc51 be a\nlearnable node embedding dictionary, where each row of \ud835\udc38\ud835\udc34 represents the embedding of a node,\n\ud835\udc41 and \ud835\udc51 denote the number of nodes and the dimension of embeddings respectively. The graph\nadjacency matrix is defined as the similarities among node embeddings,\n\ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 = \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \u0010\n\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc38\ud835\udc34 \u00b7 \ud835\udc38\n\ud835\udc47\n\ud835\udc34\n)\n\u0011\n, (165)\nwhere \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 function is to perform row-normalization, and \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 is the Laplacian matrix.\n14.3 Typical GNN Frameworks in Traffic Domain\nSpatial Temporal Graph Convolution Network (STGCN) [545]. STGCN is a pioneering work in the\nspatial-temporal GNN domain. It utilizes graph convolution to capture spatial features, and deploys\na gated causal convolution to extract temporal patterns. Specifically, the graph convolution and\ntemporal convolution are defined as follows,\n\u0398 \u2217G \ud835\udc65 = \ud835\udf03 (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc65 = \ud835\udf03 (\ud835\udc37\u02dc \u2212\n1\n2\ud835\udc34\u02dc\ud835\udc37\u02dc \u2212\n1\n2 )\ud835\udc65, (166)\n\u0393 \u2217T \ud835\udc66 = \ud835\udc43 \u2299 \ud835\udf0e(\ud835\udc44), (167)\nwhere \u0398 is the parameter of graph convolution, \ud835\udc43 and \ud835\udc44 are the outputs of a 1-d convolution\nalong the temporal dimension. The sigmoid gate \ud835\udf0e(\ud835\udc44) controls how the states of \ud835\udc43 are relevant\nfor discovering hidden temporal patterns. In order to fuse features from both spatial and temporal\ndimension, the spatial convolution layer and the temporal convolution layer are combined to\nconstruct a spatial temporal block to jointly deal with graph-structured time series, and more blocks\ncan be stacked to achieve a more scalable and complex model.\nDiffusion Convolutional Recurrent Neural Network (DCRNN) [275]. DCRNN is a representative\nsolution combining graph convolution networks with recurrent neural networks. It captures spatial\ndependencies by bidirectional random walks on the graph. The diffusion convolution operation on\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 65\na graph is defined as:\n\ud835\udc4b \u2217G \ud835\udc53\ud835\udf03 =\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc58=0\n\u0010\n\ud835\udf03\ud835\udc58,1 (\ud835\udc37\n\u22121\n\ud835\udc42 \ud835\udc34)\n\ud835\udc58 + \ud835\udf03\ud835\udc58,2 (\ud835\udc37\u22121\n\ud835\udc3c \ud835\udc34)\n\ud835\udc58\n\u0011\n\ud835\udc4b, (168)\nwhere \ud835\udf03 are parameters for the convolution filter, and \ud835\udc37\n\u22121\n\ud835\udc42\n\ud835\udc34, \ud835\udc37\u22121\n\ud835\udc3c\n\ud835\udc34 represent the bidirectional\ndiffusion processes respectively. In term of temporal dependency, DCRNN utilizes Gated Recurrent\nUnits (GRU), and replace the linear transformation in the GRU with the diffusion convolution as\nfollows,\n\ud835\udc5f\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc5f \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc5f), (169)\n\ud835\udc62\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc62 \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc62), (170)\n\ud835\udc36\n(\ud835\udc61) = tanh(\u0398\ud835\udc36 \u2217G [\ud835\udc4b(\ud835\udc61)\n, (\ud835\udc5f\n(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121)\n] + \ud835\udc4f\ud835\udc50 ), (171)\n\ud835\udc3b\n(\ud835\udc61) = \ud835\udc62(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121) + (1 \u2212 \ud835\udc62(\ud835\udc61)\n) \u2299 \ud835\udc36\n(\ud835\udc61)\n, (172)\nwhere \ud835\udc4b\n(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61) denote the input and output at time \ud835\udc61, \ud835\udc5f\n(\ud835\udc61)\n, \ud835\udc62(\ud835\udc61)are the reset and update gates\nrespectively, and \u0398\ud835\udc5f, \u0398\ud835\udc62, \u0398\ud835\udc36 are parameters of convolution filters. Moreover, DCRNN employs a\nsequence-to-sequence architecture to predict future series. Both the encoder and the decoder are\nconstructed with diffusion convolutional recurrent layers. The historical time series are fed into\nthe encoder and the predictions are generated by the decoder. The scheduled sampling technique is\nutilized to solve the discrepancy problem between training and test distribution.\nAdaptive Graph Convolutional Recurrent Network (AGCRN) [19]. The focuses of AGCRN are\ntwo-fold. On the one hand, it argues that the temporal patterns are diversified and thus parameter\u0002sharing for each node is inferior; on the other hand, it proposes that the pre-defined graph may be\nintuitive and incomplete for the specific prediction task. To mitigate the two issues, it designs a\nNode Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic\nseries, and a Data Adaptive Graph Generation (DAGG) module to infer the hidden correlations\namong nodes from data and to generate the graph during training. Specifically, the NAPL module\nis defined as follows,\n\ud835\udc4d = (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc4b \ud835\udc38G\ud835\udc4aG + \ud835\udc38G\ud835\udc4f G, (173)\nwhere \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc36 is the input feature, \ud835\udc38G \u2208 R\ud835\udc41 \u00d7\ud835\udc51\nis a node embedding dictionary, \ud835\udc51 is the\nembedding dimension (\ud835\udc51",
    "openalex_id": "https://openalex.org/W4392203343",
    "title": "A Comprehensive Survey on Deep Graph Representation Learning",
    "publication_date": "2024-02-27",
    "cited_by_count": 55,
    "topics": "Graph Neural Network Models and Applications, Statistical Mechanics of Complex Networks, Recommender System Technologies",
    "keywords": "Feature learning, Representation Learning, Knowledge Graph Embedding, Signal Processing on Graphs, Network Embedding, Deep Learning, Graph embedding",
    "concepts": "Computer science, Deep learning, Feature learning, Artificial intelligence, Graph, Theoretical computer science, Categorization, Embedding, Graph embedding, Machine learning",
    "pdf_urls_by_priority": [
      "https://arxiv.org/pdf/2304.05055"
    ],
    "text_type": "full_text",
    "successful_pdf_url": "https://arxiv.org/pdf/2304.05055",
    "referenced_works": [
      "https://openalex.org/W103666676",
      "https://openalex.org/W1510073064",
      "https://openalex.org/W1520469672",
      "https://openalex.org/W1614298861",
      "https://openalex.org/W1662382123",
      "https://openalex.org/W1816257748",
      "https://openalex.org/W1888005072",
      "https://openalex.org/W1924770834",
      "https://openalex.org/W1954735160",
      "https://openalex.org/W1959608418",
      "https://openalex.org/W1991252559",
      "https://openalex.org/W1992787746",
      "https://openalex.org/W1993046136",
      "https://openalex.org/W1994389483",
      "https://openalex.org/W1996058270",
      "https://openalex.org/W2006698588",
      "https://openalex.org/W2008056655",
      "https://openalex.org/W2009233867",
      "https://openalex.org/W2022638422",
      "https://openalex.org/W2027482274",
      "https://openalex.org/W2042123098",
      "https://openalex.org/W2053186076",
      "https://openalex.org/W2054141820",
      "https://openalex.org/W2056609785",
      "https://openalex.org/W2062340319",
      "https://openalex.org/W2076498053",
      "https://openalex.org/W2090891622",
      "https://openalex.org/W2095932468",
      "https://openalex.org/W2100495367",
      "https://openalex.org/W2101491865",
      "https://openalex.org/W2110242546",
      "https://openalex.org/W2113052721",
      "https://openalex.org/W2114704115",
      "https://openalex.org/W2115627867",
      "https://openalex.org/W2116341502",
      "https://openalex.org/W2121406124",
      "https://openalex.org/W2128332575",
      "https://openalex.org/W2140310134",
      "https://openalex.org/W2142498761",
      "https://openalex.org/W2142535891",
      "https://openalex.org/W2145658888",
      "https://openalex.org/W2148950790",
      "https://openalex.org/W2152184085",
      "https://openalex.org/W2152630148",
      "https://openalex.org/W2152825437",
      "https://openalex.org/W2153959628",
      "https://openalex.org/W2156718197",
      "https://openalex.org/W2158787690",
      "https://openalex.org/W2170057991",
      "https://openalex.org/W2184148260",
      "https://openalex.org/W2262123273",
      "https://openalex.org/W2319902168",
      "https://openalex.org/W2338678442",
      "https://openalex.org/W2387462954",
      "https://openalex.org/W2393319904",
      "https://openalex.org/W2415243320",
      "https://openalex.org/W2461620095",
      "https://openalex.org/W2481151430",
      "https://openalex.org/W2488133945",
      "https://openalex.org/W2493343568",
      "https://openalex.org/W2509893387",
      "https://openalex.org/W2520633135",
      "https://openalex.org/W2529996553",
      "https://openalex.org/W2531327146",
      "https://openalex.org/W2550925836",
      "https://openalex.org/W2551706664",
      "https://openalex.org/W2565684601",
      "https://openalex.org/W2594183968",
      "https://openalex.org/W2594899909",
      "https://openalex.org/W2602753196",
      "https://openalex.org/W2604738573",
      "https://openalex.org/W2606202972",
      "https://openalex.org/W2612872092",
      "https://openalex.org/W2618530766",
      "https://openalex.org/W2624407581",
      "https://openalex.org/W2700550412",
      "https://openalex.org/W2735246657",
      "https://openalex.org/W2743104969",
      "https://openalex.org/W2743930630",
      "https://openalex.org/W2749279690",
      "https://openalex.org/W2754490690",
      "https://openalex.org/W2767094836",
      "https://openalex.org/W2767404761",
      "https://openalex.org/W2772486182",
      "https://openalex.org/W2773515559",
      "https://openalex.org/W2788134583",
      "https://openalex.org/W2788775653",
      "https://openalex.org/W2788919350",
      "https://openalex.org/W2793544332",
      "https://openalex.org/W2798621783",
      "https://openalex.org/W2800415562",
      "https://openalex.org/W2803526748",
      "https://openalex.org/W2809279178",
      "https://openalex.org/W2809307135",
      "https://openalex.org/W2809435178",
      "https://openalex.org/W2884209963",
      "https://openalex.org/W2888164077",
      "https://openalex.org/W2888192920",
      "https://openalex.org/W2889337896",
      "https://openalex.org/W2892880750",
      "https://openalex.org/W2895744665",
      "https://openalex.org/W2895884529",
      "https://openalex.org/W2896202861",
      "https://openalex.org/W2896457183",
      "https://openalex.org/W2897862648",
      "https://openalex.org/W2897978524",
      "https://openalex.org/W2901454299",
      "https://openalex.org/W2903871660",
      "https://openalex.org/W2905432015",
      "https://openalex.org/W2907230994",
      "https://openalex.org/W2907492528",
      "https://openalex.org/W2911286998",
      "https://openalex.org/W2912323206",
      "https://openalex.org/W2912351665",
      "https://openalex.org/W2913015533",
      "https://openalex.org/W2913350752",
      "https://openalex.org/W2913825337",
      "https://openalex.org/W2914989158",
      "https://openalex.org/W2916446912",
      "https://openalex.org/W2944538680",
      "https://openalex.org/W2948684689",
      "https://openalex.org/W2948729509",
      "https://openalex.org/W2949103145",
      "https://openalex.org/W2949208225",
      "https://openalex.org/W2949243165",
      "https://openalex.org/W2949865801",
      "https://openalex.org/W2950898568",
      "https://openalex.org/W2951659295",
      "https://openalex.org/W2951970475",
      "https://openalex.org/W2959300817",
      "https://openalex.org/W2962756421",
      "https://openalex.org/W2962810718",
      "https://openalex.org/W2963017945",
      "https://openalex.org/W2963066159",
      "https://openalex.org/W2963084622",
      "https://openalex.org/W2963224980",
      "https://openalex.org/W2963241951",
      "https://openalex.org/W2963341924",
      "https://openalex.org/W2963358464",
      "https://openalex.org/W2963410212",
      "https://openalex.org/W2963456618",
      "https://openalex.org/W2963521729",
      "https://openalex.org/W2963639956",
      "https://openalex.org/W2963664410",
      "https://openalex.org/W2963703618",
      "https://openalex.org/W2963726920",
      "https://openalex.org/W2963919031",
      "https://openalex.org/W2964015378",
      "https://openalex.org/W2964044287",
      "https://openalex.org/W2964051675",
      "https://openalex.org/W2964113829",
      "https://openalex.org/W2964321699",
      "https://openalex.org/W2964568038",
      "https://openalex.org/W2964583308",
      "https://openalex.org/W2964926209",
      "https://openalex.org/W2965341826",
      "https://openalex.org/W2965857891",
      "https://openalex.org/W2966357564",
      "https://openalex.org/W2966683369",
      "https://openalex.org/W2966841471",
      "https://openalex.org/W2971126534",
      "https://openalex.org/W2971220558",
      "https://openalex.org/W2979750740",
      "https://openalex.org/W2979845147",
      "https://openalex.org/W2981536126",
      "https://openalex.org/W2981790137",
      "https://openalex.org/W2982108874",
      "https://openalex.org/W2982880755",
      "https://openalex.org/W2986423110",
      "https://openalex.org/W2986466936",
      "https://openalex.org/W2986515219",
      "https://openalex.org/W2988115728",
      "https://openalex.org/W2989285747",
      "https://openalex.org/W2992586577",
      "https://openalex.org/W2992613109",
      "https://openalex.org/W2994860160",
      "https://openalex.org/W2996635575",
      "https://openalex.org/W2996847713",
      "https://openalex.org/W2996910665",
      "https://openalex.org/W2997128522",
      "https://openalex.org/W2997574889",
      "https://openalex.org/W2997785591",
      "https://openalex.org/W2997997679",
      "https://openalex.org/W2998004401",
      "https://openalex.org/W2998122931",
      "https://openalex.org/W2998496395",
      "https://openalex.org/W2998604091",
      "https://openalex.org/W2998702685",
      "https://openalex.org/W3000301417",
      "https://openalex.org/W3000478925",
      "https://openalex.org/W3000577518",
      "https://openalex.org/W3000716014",
      "https://openalex.org/W3005552578",
      "https://openalex.org/W3007488165",
      "https://openalex.org/W3008194092",
      "https://openalex.org/W3011358689",
      "https://openalex.org/W3011667710",
      "https://openalex.org/W3012123536",
      "https://openalex.org/W3012871709",
      "https://openalex.org/W3013107657",
      "https://openalex.org/W3013888836",
      "https://openalex.org/W3016124664",
      "https://openalex.org/W3016427665",
      "https://openalex.org/W3025863369",
      "https://openalex.org/W3026887460",
      "https://openalex.org/W3031353169",
      "https://openalex.org/W3033039844",
      "https://openalex.org/W3033706928",
      "https://openalex.org/W3034231628",
      "https://openalex.org/W3034329572",
      "https://openalex.org/W3035060554",
      "https://openalex.org/W3035096461",
      "https://openalex.org/W3035237749",
      "https://openalex.org/W3035285524",
      "https://openalex.org/W3035523484",
      "https://openalex.org/W3035580605",
      "https://openalex.org/W3035649237",
      "https://openalex.org/W3035664258",
      "https://openalex.org/W3035666843",
      "https://openalex.org/W3035702572",
      "https://openalex.org/W3035740499",
      "https://openalex.org/W3036106327",
      "https://openalex.org/W3036167779",
      "https://openalex.org/W3036974265",
      "https://openalex.org/W3038719422",
      "https://openalex.org/W3038981236",
      "https://openalex.org/W3042918615",
      "https://openalex.org/W3044189835",
      "https://openalex.org/W3045200674",
      "https://openalex.org/W3045662942",
      "https://openalex.org/W3045928028",
      "https://openalex.org/W3046470859",
      "https://openalex.org/W3048817558",
      "https://openalex.org/W3080566854",
      "https://openalex.org/W3080997787",
      "https://openalex.org/W3081203761",
      "https://openalex.org/W3081325717",
      "https://openalex.org/W3081836708",
      "https://openalex.org/W3082154031",
      "https://openalex.org/W3082411326",
      "https://openalex.org/W3087318471",
      "https://openalex.org/W3092339997",
      "https://openalex.org/W3092462694",
      "https://openalex.org/W3093218977",
      "https://openalex.org/W3093687066",
      "https://openalex.org/W3094231942",
      "https://openalex.org/W3094500523",
      "https://openalex.org/W3095448863",
      "https://openalex.org/W3096831136",
      "https://openalex.org/W3098465726",
      "https://openalex.org/W3098797593",
      "https://openalex.org/W3099152386",
      "https://openalex.org/W3099414221",
      "https://openalex.org/W3100078588",
      "https://openalex.org/W3100278010",
      "https://openalex.org/W3100324210",
      "https://openalex.org/W3101707147",
      "https://openalex.org/W3102554291",
      "https://openalex.org/W3103523530",
      "https://openalex.org/W3103720336",
      "https://openalex.org/W3103736477",
      "https://openalex.org/W3104097132",
      "https://openalex.org/W3104644561",
      "https://openalex.org/W3104667978",
      "https://openalex.org/W3105259638",
      "https://openalex.org/W3105423481",
      "https://openalex.org/W3108202858",
      "https://openalex.org/W3108433857",
      "https://openalex.org/W3110901318",
      "https://openalex.org/W3111430045",
      "https://openalex.org/W3113177135",
      "https://openalex.org/W3114613321",
      "https://openalex.org/W3116239416",
      "https://openalex.org/W3117178429",
      "https://openalex.org/W3120567415",
      "https://openalex.org/W3122934853",
      "https://openalex.org/W3123909522",
      "https://openalex.org/W3124962940",
      "https://openalex.org/W3128443161",
      "https://openalex.org/W3129850062",
      "https://openalex.org/W3133780103",
      "https://openalex.org/W3134509497",
      "https://openalex.org/W3135205495",
      "https://openalex.org/W3135389928",
      "https://openalex.org/W3136999308",
      "https://openalex.org/W3137385578",
      "https://openalex.org/W3137928916",
      "https://openalex.org/W3138516171",
      "https://openalex.org/W3148711710",
      "https://openalex.org/W3151900735",
      "https://openalex.org/W3152893301",
      "https://openalex.org/W3154503084",
      "https://openalex.org/W3155056342",
      "https://openalex.org/W3155322940",
      "https://openalex.org/W3155577228",
      "https://openalex.org/W3156642753",
      "https://openalex.org/W3157039246",
      "https://openalex.org/W3157999218",
      "https://openalex.org/W3158827677",
      "https://openalex.org/W3160021293",
      "https://openalex.org/W3163426640",
      "https://openalex.org/W3164446335",
      "https://openalex.org/W3165171933",
      "https://openalex.org/W3165369424",
      "https://openalex.org/W3165924303",
      "https://openalex.org/W3167334189",
      "https://openalex.org/W3168436232",
      "https://openalex.org/W3169168872",
      "https://openalex.org/W3169450514",
      "https://openalex.org/W3169933688",
      "https://openalex.org/W3171581326",
      "https://openalex.org/W3171764584",
      "https://openalex.org/W3174163042",
      "https://openalex.org/W3174174150",
      "https://openalex.org/W3174823757",
      "https://openalex.org/W3175925542",
      "https://openalex.org/W3175971420",
      "https://openalex.org/W3176393519",
      "https://openalex.org/W3176806965",
      "https://openalex.org/W3176890989",
      "https://openalex.org/W3181414820",
      "https://openalex.org/W3184127157",
      "https://openalex.org/W3187985423",
      "https://openalex.org/W3190664711",
      "https://openalex.org/W3191962800",
      "https://openalex.org/W3192448376",
      "https://openalex.org/W3193553875",
      "https://openalex.org/W3194668998",
      "https://openalex.org/W3200806939",
      "https://openalex.org/W3201058350",
      "https://openalex.org/W3201249640",
      "https://openalex.org/W3204651332",
      "https://openalex.org/W3205227354",
      "https://openalex.org/W3206171352",
      "https://openalex.org/W3208638341",
      "https://openalex.org/W3209048663",
      "https://openalex.org/W3209056694",
      "https://openalex.org/W3209451568",
      "https://openalex.org/W3209764902",
      "https://openalex.org/W3210482950",
      "https://openalex.org/W3210611486",
      "https://openalex.org/W3210987203",
      "https://openalex.org/W3211394146",
      "https://openalex.org/W3211477647",
      "https://openalex.org/W3211973371",
      "https://openalex.org/W3213940558",
      "https://openalex.org/W3214642103",
      "https://openalex.org/W3214674106",
      "https://openalex.org/W3214872094",
      "https://openalex.org/W3215452784",
      "https://openalex.org/W4200635484",
      "https://openalex.org/W4205247958",
      "https://openalex.org/W4206174637",
      "https://openalex.org/W4206357214",
      "https://openalex.org/W4206445139",
      "https://openalex.org/W4206776774",
      "https://openalex.org/W4212805305",
      "https://openalex.org/W4213052788",
      "https://openalex.org/W4213147383",
      "https://openalex.org/W4213457653",
      "https://openalex.org/W4214868967",
      "https://openalex.org/W4220742022",
      "https://openalex.org/W4220933119",
      "https://openalex.org/W4221023051",
      "https://openalex.org/W4221138292",
      "https://openalex.org/W4221149947",
      "https://openalex.org/W4221155201",
      "https://openalex.org/W4221157965",
      "https://openalex.org/W4224309748",
      "https://openalex.org/W4224311348",
      "https://openalex.org/W4224311800",
      "https://openalex.org/W4224983022",
      "https://openalex.org/W4225090121",
      "https://openalex.org/W4225338086",
      "https://openalex.org/W4225405705",
      "https://openalex.org/W4225512856",
      "https://openalex.org/W4225596872",
      "https://openalex.org/W4225977739",
      "https://openalex.org/W4226058932",
      "https://openalex.org/W4226060238",
      "https://openalex.org/W4226208698",
      "https://openalex.org/W4229053887",
      "https://openalex.org/W4234842379",
      "https://openalex.org/W4239789016",
      "https://openalex.org/W4240185200",
      "https://openalex.org/W4240592325",
      "https://openalex.org/W4243799827",
      "https://openalex.org/W4246587917",
      "https://openalex.org/W4255866863",
      "https://openalex.org/W4280535976",
      "https://openalex.org/W4281387042",
      "https://openalex.org/W4281563651",
      "https://openalex.org/W4282913028",
      "https://openalex.org/W4282943426",
      "https://openalex.org/W4283121576",
      "https://openalex.org/W4283218438",
      "https://openalex.org/W4283462727",
      "https://openalex.org/W4283798273",
      "https://openalex.org/W4283810298",
      "https://openalex.org/W4283817628",
      "https://openalex.org/W4284666445",
      "https://openalex.org/W4284698122",
      "https://openalex.org/W4285428788",
      "https://openalex.org/W4286588524",
      "https://openalex.org/W4286795917",
      "https://openalex.org/W4286893581",
      "https://openalex.org/W4287123803",
      "https://openalex.org/W4287325738",
      "https://openalex.org/W4287780403",
      "https://openalex.org/W4287863694",
      "https://openalex.org/W4287991183",
      "https://openalex.org/W4287998109",
      "https://openalex.org/W4288052590",
      "https://openalex.org/W4288088467",
      "https://openalex.org/W4288346884",
      "https://openalex.org/W4289389616",
      "https://openalex.org/W4289533979",
      "https://openalex.org/W4289537189",
      "https://openalex.org/W4290648792",
      "https://openalex.org/W4290875097",
      "https://openalex.org/W4290877962",
      "https://openalex.org/W4293112739",
      "https://openalex.org/W4293370878",
      "https://openalex.org/W4293821372",
      "https://openalex.org/W4294170691",
      "https://openalex.org/W4294435970",
      "https://openalex.org/W4294558607",
      "https://openalex.org/W4295097398",
      "https://openalex.org/W4295728955",
      "https://openalex.org/W4295846611",
      "https://openalex.org/W4296047560",
      "https://openalex.org/W4296143727",
      "https://openalex.org/W4296185888",
      "https://openalex.org/W4297510052",
      "https://openalex.org/W4297733535",
      "https://openalex.org/W4297791874",
      "https://openalex.org/W4297946153",
      "https://openalex.org/W4297951436",
      "https://openalex.org/W4297999768",
      "https://openalex.org/W4298052734",
      "https://openalex.org/W4298312696",
      "https://openalex.org/W4301329292",
      "https://openalex.org/W4304097971",
      "https://openalex.org/W4306887124",
      "https://openalex.org/W4307416138",
      "https://openalex.org/W4308505492",
      "https://openalex.org/W4309635196",
      "https://openalex.org/W4309801650",
      "https://openalex.org/W4310012576",
      "https://openalex.org/W4311216457",
      "https://openalex.org/W4312126067",
      "https://openalex.org/W4312689497",
      "https://openalex.org/W4313201684",
      "https://openalex.org/W4315708854",
      "https://openalex.org/W4316495377",
      "https://openalex.org/W4317951161",
      "https://openalex.org/W4318150241",
      "https://openalex.org/W4318347779",
      "https://openalex.org/W4318540750",
      "https://openalex.org/W4318812521",
      "https://openalex.org/W4320814985",
      "https://openalex.org/W4321227311",
      "https://openalex.org/W4321367323",
      "https://openalex.org/W4321479940",
      "https://openalex.org/W4321480027",
      "https://openalex.org/W4321480031",
      "https://openalex.org/W4322614756",
      "https://openalex.org/W4322824839",
      "https://openalex.org/W4323650483",
      "https://openalex.org/W4327525152",
      "https://openalex.org/W4361247736",
      "https://openalex.org/W4362682267",
      "https://openalex.org/W4362714312",
      "https://openalex.org/W4366001157",
      "https://openalex.org/W4366198975",
      "https://openalex.org/W4366975604",
      "https://openalex.org/W4366986925",
      "https://openalex.org/W4367047082",
      "https://openalex.org/W4367047244",
      "https://openalex.org/W4367047306",
      "https://openalex.org/W4367060955",
      "https://openalex.org/W4367595602",
      "https://openalex.org/W4376121360",
      "https://openalex.org/W4378696994",
      "https://openalex.org/W4378766933",
      "https://openalex.org/W4378909195",
      "https://openalex.org/W4378976562",
      "https://openalex.org/W4379185762",
      "https://openalex.org/W4379506768",
      "https://openalex.org/W4380091056",
      "https://openalex.org/W4381573056",
      "https://openalex.org/W4381679608",
      "https://openalex.org/W4382202967",
      "https://openalex.org/W4382239955",
      "https://openalex.org/W4382240004",
      "https://openalex.org/W4382317956",
      "https://openalex.org/W4382468395",
      "https://openalex.org/W4383604692",
      "https://openalex.org/W4384895066",
      "https://openalex.org/W4384915287",
      "https://openalex.org/W4385245566",
      "https://openalex.org/W4385270450",
      "https://openalex.org/W4385568380",
      "https://openalex.org/W4386445678",
      "https://openalex.org/W4386494629",
      "https://openalex.org/W4386587901",
      "https://openalex.org/W4386620158",
      "https://openalex.org/W4386711666",
      "https://openalex.org/W4386746103",
      "https://openalex.org/W4387092606",
      "https://openalex.org/W4387969028",
      "https://openalex.org/W4388185880",
      "https://openalex.org/W4388537645",
      "https://openalex.org/W4388684696",
      "https://openalex.org/W4390534655",
      "https://openalex.org/W4390572965",
      "https://openalex.org/W4391345293",
      "https://openalex.org/W4391561379",
      "https://openalex.org/W637153065"
    ],
    "openalex_rank": 2,
    "num_tokens": 42218,
    "url": "https://arxiv.org/pdf/2304.05055",
    "best_oa_location_pdf_url": "https://arxiv.org/pdf/2304.05055"
  },
  {
    "id": "https://openalex.org/W4388017359",
    "text": "1\nEnd-to-End Speech Recognition: A Survey\nRohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE,\nRalf Schluter, \u00a8 Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE\nAbstract\u2014In the last decade of automatic speech recognition\n(ASR) research, the introduction of deep learning has brought\nconsiderable reductions in word error rate of more than 50%\nrelative, compared to modeling without deep learning. In the\nwake of this transition, a number of all-neural ASR architectures\nhave been introduced. These so-called end-to-end (E2E) models\nprovide highly integrated, completely neural ASR models, which\nrely strongly on general machine learning knowledge, learn more\nconsistently from data, with lower dependence on ASR domain\u0002specific experience. The success and enthusiastic adoption of deep\nlearning, accompanied by more generic model architectures has\nled to E2E models now becoming the prominent ASR approach.\nThe goal of this survey is to provide a taxonomy of E2E ASR\nmodels and corresponding improvements, and to discuss their\nproperties and their relationship to classical hidden Markov\nmodel (HMM) based ASR architectures. All relevant aspects\nof E2E ASR are covered in this work: modeling, training,\ndecoding, and external language model integration, discussions of\nperformance and deployment opportunities, as well as an outlook\ninto potential future developments.\nIndex Terms\u2014end-to-end, automatic speech recognition.\nI. INTRODUCTION\nThe classical1statistical architecture decomposes an auto\u0002matic speech recognition (ASR) system into four main compo\u0002nents: acoustic feature extraction from speech audio signals,\nacoustic modeling, language modeling and search based on\nBayes\u2019 decision rule [1], [2], [3]. Classical acoustic modeling\nis based on hidden Markov models (HMMs) to account for\nspeaking rate variation. Within the classical approach, deep\nlearning has been introduced into acoustic and language mod\u0002eling. In acoustic modeling, deep learning has replaced Gaus\u0002sian mixture distributions (hybrid HMM [4], [5]) or augmented\nthe acoustic feature set (e.g., non-linear discriminant/tandem\napproach [6], [7]). In language modeling, deep learning has re\u0002placed count-based approaches [8], [9], [10]. However, in these\nearly attempts at introducing deep learning, the classical ASR\narchitecture was unmodified. Classical state-of-the-art ASR\nsystems today are composed of many separate components and\nknowledge sources: especially speech signal preprocessing;\nmethods for robustness with respect to recording conditions;\nphoneme inventories and pronunciation lexica; phonetic clus\u0002tering; handling of out-of-vocabulary words; various methods\nfor adaptation/normalization; elaborate training schedules with\ndifferent objectives including sequence discriminative training,\netc. The potential of deep learning, on the other hand, initiated\nsuccessful approaches to integrate formerly separate modeling\nsteps, e.g., by integrating speech signal pre-processing and\nfeature extraction into acoustic modeling [11], [12].\n1 The term \u201cclassical\u201d here refers to the former, long-term, state-of-the-art\nASR architecture based on the decomposition into acoustic and language\nmodel, and with acoustic modeling based on hidden Markov models.\nMore consequently, the introduction of deep learning to\nASR also initiated research to replace classical ASR archi\u0002tectures based on hidden Markov models (HMM) with more\nintegrated joint neural network model structures [13], [14],\n[15], [16]. These ventures might be seen as trading specific\nspeech processing models for more generic machine learning\napproaches to sequence-to-sequence processing \u2013 akin to how\nstatistical approaches to natural language processing have\ncome to replace more linguistically oriented models. For these\nall-neural approaches recently the term end-to-end (E2E) [14],\n[17], [18], [19] has been established. Therefore, first of all\nan attempt to define the term end-to-end in the context of\nASR is due in this survey. According to the Cambridge\nDictionary, the adjective \u201cend-to-end\u201d is defined as: \u201cinclud\u0002ing all the stages of a process\u201d [20]. We therefore propose\nthe following definition of end-to-end ASR: an integrated\nASR model that enables joint training from scratch; avoids\nseparately obtained knowledge sources; and, provides single\u0002pass recognition consistent with the objective to optimize the\ntask-specific evaluation measure, i.e., usually label (word,\ncharacter, subword, etc.) error rate. While this definition\nsuffices for the present discussion, we note that such an\nidealized definition hides many nuances involved in the term\nE2E and lacks distinctiveness; we elaborate on some of these\nnuances in Sec. II to discuss the various connotations of the\nterm E2E in the context of ASR.\nWhat are potential benefits of E2E approaches to ASR?\nThe primary objective when developing an ASR systems is to\nminimize the expected word error rate; secondary objectives\nare to reduce time and memory complexity of the resulting\ndecoder, and \u2013 assuming a constrained development budget \u2013\ngenericity, and ease of modeling. First of all, an integrated\nASR system, defined in terms of a single neural network\nstructure supports genericity of modeling and may allow for\nfaster development cycles when building ASR systems for\nnew languages or domains. Similarly, ASR models defined\nby a single neural network structure may become more \u2018lean\u2019\ncompared to classical modeling, with a simpler decoding\nprocess, obviating the need to integrate separate models. The\nresulting reduction in memory footprint and power consump\u0002tion supports embedded ASR applications [21], [22]. Further\u0002more, end-to-end joint training may help to avoid spurious\noptima from intermediate training stages. Avoiding secondary\nknowledge sources like pronunciation lexica may be helpful\nfor languages/domains where such resources are not easily\navailable. Also, secondary knowledge sources may themselves\nbe erroneous; avoiding these may improve models trained\ndirectly from data, provided that sufficient amounts of task\u0002specific training data are available.\nWith the current surge of interest in E2E ASR models and an\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nincreasing diversity of corresponding work, the authors of this\nreview think it is time to provide an overview of this rapidly\nevolving domain of research. The goal of this survey is to\nprovide an in-depth overview of the current state of research\non E2E ASR systems, covering all relevant aspects of E2E\nASR, with a contrastive discussion of the different E2E and\nclassical ASR architectures.\nThis survey of E2E speech recognition is structured as fol\u0002lows. Sec. II discusses the nuances in the term E2E as it applies\nto ASR. Sec. III describes the historical evolution of E2E\nspeech recognition, with specific focus on the input-output\nalignment and an overview of prominent E2E ASR models.\nSec. IV discusses improvements of the basic E2E models,\nincluding E2E model combination, training loss functions,\ncontext, encoder/decoder structures and endpointing. Sec. V\nprovides an overview of E2E ASR model training. Decoding\nalgorithms for the different E2E approaches are discussed\nin Sec. VI. Sec. VII discusses the role and integration of\n(separate) language models in E2E ASR. Sec. VIII reviews\nexperimental comparisons of the different E2E as well as\nclassical ASR approaches. Sec. IX provides an overview of\napplications of E2E ASR. Sec. X investigates future directions\nof E2E research in ASR, before concluding in Sec. XI. Finally,\nwe note that this survey paper also includes comparative\ndiscussions between novel E2E models and classical HMM\u0002based ASR approaches in terms of various aspects; most\nsections end with a summarization of the relationship between\nE2E models and HMM-based ASR approaches in relation to\nthe topics covered within the respective sections.\nII. DISTINCTIVENESS OF THE TERM E2E\nAs noted in Sec. I the term E2E provides an idealized\ndefinition of ASR systems, and can benefit from a more\ndetailed discussion based on the following perspectives.\na) Joint Modeling: In terms of ASR, the E2E property\ncan be interpreted as considering all components of an ASR\nsystem jointly as a single computational graph. Even more so,\nthe common understanding of E2E in ASR is that of a single\njoint modeling approach that does not necessarily distinguish\nseparate components, which may also mean dropping the\nclassical separation of ASR into an acoustic model and a\nlanguage model. However, in practice E2E ASR systems are\noften combined with external language models trained on text\u0002only data, which weakens the end-to-end nature of the system\nto some extent.\nb) Joint Training: In terms of model training, E2E can\nbe interpreted as estimating all parameters, of all components\nof a model jointly using a single objective function that is\nconsistent with the task at hand, which in case of ASR means\nminimizing the expected word error rate2. However, the term\nlacks distinctiveness here, as classical and/or modular ASR\nmodel architectures also support joint training with a single\nobjective.\n2 Note that this does not necessarily require Bayes Risk training, as standard\ntraining criteria like cross entropy, maximum mutual information and max\u0002imum likelihood in case of classical ASR models asymptotically guarantee\noptimal performance in the sense of Bayes decision rule, also [23], [24].\nc) Training from Scratch: The E2E property can also be\ninterpreted with respect to the training process itself, by re\u0002quiring training from scratch, avoiding external knowledge like\nprior alignments or initial models pre-trained using different\ncriteria or knowledge sources. However, note that pre-training\nand fine-tuning strategies are also relevant, if the model has\nexplicit modularity, including self-supervised learning [25] or\njoint training of front-end and speech recognition models [26].\nEspecially in case of limited amounts of target task training\ndata, utilizing large pretrained models is important to obtain\nperformant E2E ASR systems.\nd) Avoiding Secondary Knowledge Sources: For ASR,\nstandard secondary knowledge sources are pronunciation lex\u0002ica and phoneme sets, as well as phonetic clustering, which\nin classical state-of-the-art ASR systems usually is based on\nclassification and regression trees (CART) [27]. Secondary\nknowledge sources and separately trained components may\nintroduce errors, might be inconsistent with the overall training\nobjective and/or may generate additional cost. Therefore, in\nan E2E approach, these would be avoided. Standard joint\ntraining of an E2E model requires using a single kind of\ntraining data, which in case of ASR would be transcribed\nspeech audio data. However, in ASR often even larger amounts\nof text-only data, as well as optional untranscribed speech\naudio are available. One of the challenges of E2E modeling\ntherefore is how to take advantage of text-only and audio-only\ndata jointly without introducing secondary (pretrained) models\nand/or training objectives [28], [29].\ne) Direct Vocabulary Modeling: Avoiding pronunciation\nlexica and corresponding subword units leave E2E recognition\nvocabularies to be derived from whole word or character\nrepresentations. Whole word models [30], according to Zipf\u2019s\nlaw [31], would require unrealistically high amounts of tran\u0002scribed training data for large vocabularies, which might not\nbe attainable for many tasks. On the other hand, methods\nto generate subword vocabularies based on characters, like\nthe currently popular byte pair encoding (BPE) approach\n[32], might be seen as secondary approaches outside the E2E\nobjective, even more so if acoustic data is considered for\nsubword derivation [33], [34], [35], [36].\nf) Generic Modeling: Finally, E2E modeling also re\u0002quires genericity of the underlying modeling: task-specific\nconstraints are learned completely from data, in contrast to\ntask-specific knowledge which influences the modeling of\nthe system architecture in the first place. For example, the\nmonotonicity constraint in ASR may be learned completely\nfrom data in an end-to-end fashion (e.g., in attention-based\napproaches [16]), or it may directly be implemented, as in\nclassical HMM structures. However, model constraints may\nbe considered by way of regularization in E2E ASR model\ntraining, and can thus provide an alternative way to introduce\ntask-specific knowledge.\ng) Single-Pass Search: In terms of the recognition/search\nproblem, the E2E property can be interpreted as integrating all\ncomponents (models, knowledge sources) of an ASR system\nbefore coming to a decision. This is in line with Bayes\u2019\ndecision rule, which exactly requires a single global decision\nintegrating all available knowledge sources, which is supported\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nby both classical ASR models as well as E2E models. On\nthe other hand, multipass search is not only exploited by\nclassical ASR models, but also by E2E ASR models, the\nmost prominent case here being (external) language model\nrescoring.\nAll in all, we need to conclude that a) \u201cE2E\u201d does not\nprovide a clear distinction between classical and novel, so\u0002called E2E models, and b) the E2E property often is weakened\nin practice, leaving the term as a more general, idealized\nperspective on ASR modeling.\nIII. A TAXONOMY OF E2E MODELS IN ASR\nBefore we derive a taxonomy of E2E ASR modeling\napproaches, we first introduce our notation. We denote the\ninput speech utterance as X, which we assume has been pa\u0002rameterized into D-dimensional acoustic frames (e.g., log-mel\nfeatures) of length T\n\u2032\n: X = (x1, \u00b7 \u00b7 \u00b7 , xT\u2032 ), where xt \u2208 R\nD.\nWe denote the corresponding word sequences as C, which can\nbe decomposed into a suitable sequence of labels of length L:\nC = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is\nagnostic to the specific representation used for decomposing\nthe word sequence into labels; popular choices include char\u0002acters, words, or sub-word sequences (e.g., BPE [32], word\u0002pieces [37]).\nASR may be viewed as a sequence classification problem\nwhich maps a variable length input, X, into an output,\nC, of unknown length. Following Bayes\u2019 decision rule, any\nstatistical approach to ASR must determine how to model the\nword sequence posterior probability, P(C|X). Thus, a natural\ntaxonomy of E2E ASR modeling can be based on the various\nstrategies for modeling this word sequence posterior: i.e., how\nthe alignment problem between input and output sequence is\nhandled; and, how sequence modeling is decomposed to the\nlevel of individual input vectors xt\n\u2032 and/or output labels cl\n.\nWe find that it is useful to distinguish implicit and explicit\nmodeling approaches, based on the modeling of the sequence\u0002to-sequence alignment:\na) Explicit Alignment Modeling: does not necessarily\nrefer to the determination of a single unique alignment, but\ninstead introduces an explicit alignment modeled as a latent\nvariable, A:\nP(C|X) = X\nA\nP(C, A|X)\nb) Implicit Alignment Modeling: does not introduce a\nlatent alignment variable, but models the label sequence pos\u0002terior P(C|X) directly.\nExplicit alignment modeling approaches can mainly be\ndistinguished by their choice of latent variable; these can be\nencoded in terms of valid emission paths in corresponding\nfinite state automata (FSA) [38] which relate the input and\noutput sequences \u2013 the approach taken in our article. Typically,\nlatent variables in explicit alignment modeling in transducer\nE2E models introduce extensions to the output label set\nwith different forms of continuation labels (including, but not\nlimited to so-called blank labels).3\n3 For example, these extensions may also include explicit duration variables,\nleading to segmental models [39]. Such models can be rewritten into equiv\u0002alent transducer models [40], and vice-versa.\nA. Encoder and Decoder Modules\nIrrespective of the alignment modeling approach, following\nthe notation introduced in [41], it is useful to view all E2E\nASR models as being composed of an encoder module and\na decoder module. The encoder module, denoted H(X),\nmaps an input acoustic frame sequence, X, of length T\n\u2032\ninto a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of\nlength T (typically T \u2264 T\n\u2032\n). Note that the encoder output is\nindependent of the hypothesized label sequence. The decoder\nmodule models the label sequence posterior on top of the\nencoder output:\nP(C|X) = P\nC\nH(X)\n\u0001\nThus, we may distinguish different approaches based upon\nhow the output label sequence distribution (including potential\nlatent variables resulting from the alignment modeling) are de\u0002composed into individual label (and alignment) contributions;\nthese may occur per output label position, per encoder frame\nposition, or combinations thereof:\nP\nC[, A]\nH(X)\n\u0001\n=\nY\nL\ni=1\nP\nci[, ai]\nc\ni\u22121\n1\n[, ai\u22121\n1\n], vi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X))\u0001\nwhere the notation mi\u22121\n1\ncorresponds to the sequence\nof i \u2212 1 previous instances of the variables m; and,\nvi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X)) denotes a context-vector that provides\nthe connection between encoder output, H(X), and the la\u0002bel output position, i. In general the context vector may\ndepend on the label context (and possibly the latent vari\u0002able context, for explicit alignment modeling approaches).\nApart from the underlying alignment model and corresponding\noutput label decomposition, decoder modules differ in terms\nof the assumptions on their label context c\ni\u22121\n1\n(and their\nlatent variable context a\ni\u22121\n1\n), which correspond to different\nconditional independence assumptions, and by their access to\nthe encoder output. For example, the local posterior may only\ndepend on a single encoder frame output (i.e., with the context\nvector being reduced to a single encoder frame\u2019s output):\nvi\nc\ni\u22121\n1\n, H(X)\n\u0001\n= hti(X). As we shall see in detail in the\nfollowing sections, the simplest case of an encoder frame\u0002level decomposition (with L = T, and ti = i) corresponds to\nCTC [13]; AED models [16] and their variants maintain the\nfull dependency of the context vector.\nFinally, different E2E models can also be distinguished by\nthe specific modeling choices that are involved in the design\nof the neural network used to implement the encoder and the\ndecoder. These might involve feed-forward neural networks,\nconvolutional neural networks, recurrent neural networks (ei\u0002ther uni-directional or bi-directional) [42], attention [43],\nand various combinations thereof (e.g., transformers [44] or\nconformers [45]). These modeling choices and corresponding\ntraining methods can be applied across E2E ASR models and\ntherefore do not enter the taxonomy of E2E ASR models\ndiscussed here. However, specific choices will be discussed as\npart of the exemplary E2E ASR models presented in Sec. VIII\nand Sec. IX and.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nB. Explicit Alignment Modeling Approaches\nEarly E2E modeling approaches modeled alignments explic\u0002itly through a latent variable, which is marginalized out (pos\u0002sibly, approximately) during training and inference. Examples\nof this family of approaches include connectionist temporal\nclassification (CTC) [13], the recurrent neural network trans\u0002ducer (RNN-T) [14], the recurrent neural aligner (RNA) [46],\nand the hybrid auto-regressive transducer [47] (HAT). As\nwill be discussed in subsequent sections, the latter modeling\napproaches in this family represent increasingly sophisticated\nmodeling of alignments, with fewer independence assumptions\nand are thus increasingly powerful. A common feature of all\nexplicit alignment models discussed in this section is that\nthey introduce an additional blank symbol, denoted \u27e8b\u27e9, and\ndefine an output probability distribution over symbols in the\nset Cb = C \u222a {\u27e8b\u27e9}. The interpretation of the \u27e8b\u27e9 symbol\nvaries slightly between each of these models, as we discuss in\ngreater details below. For now, it suffices to say that given\na specific training example, (X, C), each of these models\ndefines a set of valid alignments, denoted by A(T ,C), and\ndefine the conditional distribution P(C|X) by marginalizing\nover all valid alignment sequences:\nP(C|X) = X\nA\nP(C|A, H(X))P(A|H(X))\n=\nX\nA\u2208A(T =|H(X)|,C)\nP(A|H(X)) (1)\nwhere, by definition P(C|A, H(X)) = 1 if and only if A \u2208\nA(T ,C) and 0 otherwise.4 We discuss the specific formulations\nof each of these models in the subsequent sections.\n1) Connectionist Temporal Classification (CTC): Connec\u0002tionist Temporal Classification (CTC) was proposed by Graves\net al. [13] as a technique for mapping a sequence of input\ntokens to a corresponding sequence of output tokens. CTC ex\u0002plicitly models alignments between the encoder output, H(X),\nand the label sequence, C, by introducing a special \u201cblank\u201d la\u0002bel, denoted by \u27e8b\u27e9: Cb = C \u222a {\u27e8b\u27e9}. An alignment, A \u2208 C\u2217\nb\n, is\nthus a sequence of labels in C or \u27e8b\u27e9.\n5 Given a specific training\nexample, (X, C), we denote the set of all valid alignments,\nACTC\n(X,C) = {A = (a1, a2, . . . , aT )}, such that each at \u2208 Cb\nwith the additional constraint that A is identical to C after first\ncollapsing consecutive identical labels, and then removing all\nblank symbols. For example, if T = 10, and C = (s, e, e),\nthen A = (s,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ACTC\n(X,C)\n, as\nillustrated in Figure 1. As can be seen in this example, repeated\nlabels in the output can be represented by intervening blanks.\nFollowing Eq. (1), CTC defines the posterior probability\nof the label sequence C conditioned on the input, X, by\n4 This is equivalent to the assumption that the mapping from an alignment\nA to a label sequence C is unique, by definition. 5 S\n\u2217 denotes a Kleene\nclosure: the set of all possible sequences composed of tokens in the set S.\nTime\ns\ne\ne\n \ns\ne\ne\ns\ne\ne\ne\nFig. 1. Example alignment sequence for a CTC model with the target\nsequence C = (s, e, e) (right), alongside a (non-deterministic) finite state\nautomaton (FSA) [38] (left) representing the set of all valid alignment paths.\nEncoder H(X)\nSoftmax\nFig. 2. A representation of the CTC model consisting of an encoder which\nmaps the input speech into a higher-level representation, and a softmax layer\nwhich predicts frame-level probabilities over the set of output labels and blank.\nmarginalizing over all possible CTC alignments as:\nPCTC(C|X) = X\nA\u2208ACTC\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, \u00b7 \u00b7 \u00b7 , a1, H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|ht) (2)\nCritically, as can be seen in Eq. (2), CTC makes a strong\nindependence assumption that the model\u2019s output at time t is\nconditionally independent of the outputs at other timesteps,\ngiven the local encoder output at time t.\nThus, a CTC model consists of a neural network that\nmodels the distribution P(at|X), at each step as shown in\nFigure 2. The encoder is connected to a softmax layer with\n|Cb| targets representing the individual probabilities in Eq. (2):\nP(at = c|X) = P(at = c|H(X)), which comprises the\ndecoder module for CTC. Thus, at each step, t, the model\nconsumes a single encoded frame ht and outputs a distribution\nover the labels; in other words, the model \u201coutputs\u201d a single\nlabel either blank, \u27e8b\u27e9, or one of the targets in C.\n2) Recurrent Neural Network Transducer (RNN-T): The\nRecurrent Neural Network Transducer (RNN-T) [14], [48] was\nproposed by Graves as an improvement over the basic CTC\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 3. An RNN-T Model [14], [48] consists of an encoder which transforms\nthe input speech frames into a high-level representation, and a prediction\u0002network which models the sequence of non-blank labels that have been\noutput previously. The prediction network output, pit\n, represents the output\nafter producing the previous non-blank label sequence c1, . . . , cit\n. The\njoint network produces a probability distribution over the output symbols\n(augmented with blank) given the prediction network state and a specific\nencoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 4. Example alignment sequence (right) for an RNN-T model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs. The FSA (left) represents the set of all valid RNN-T\nalignment paths.\nmodel [13], by removing some of the conditional indepen\u0002dence assumptions that we discussed previously. The RNN\u0002T model, which is depicted in Figure 3, is best understood\nby contrasting it against the CTC model. As with CTC, the\nRNN-T model augments the output symbols with the blank\nsymbol, and thus defines a distribution over label sequences\nin Cb. Similarly, as with CTC, the model consists of an encoder\nwhich processes the input acoustic frames X to generate the\nencoded representation H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ).\nUnlike CTC, however, the blank symbol in RNN-T has a\nslightly different interpretation; for each input encoder frame,\nht, the RNN-T model outputs a sequence of zero or more\nsymbols in C which are terminated by a single blank symbol.\nThus, we may define the set of all valid alignment se\u0002quences in RNN-T as: ARNNT\n(X,C) = {A = (a1, a2, \u00b7 \u00b7 \u00b7 , aT +L)},\nthe set of all sequences of T + L symbols in C\n\u2217\nb\n, which\nare identical to C after removing all blanks. Finally, for a\ngiven output position \u03c4 , let i\u03c4 denote the number of non\u0002blank labels in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121). Thus, the\nnumber of blanks in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121) is\n\u03c4 \u2212 i\u03c4 \u2212 1. For example, if T = 7, and C = (s, e, e),\nthen A = (\u27e8b\u27e9, s,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ARNNT\n(X,C)\n.\nNote that, unlike the CTC model, repeated labels in the output\nrequire no special treatment as illustrated in Figure 4, where,\ni1 = i2 = 0;i3 = i4 = 1;i10 = 3; etc.\nWe may then define the posterior probability P(C|X) as\nbefore:\nPRNNT(C|X) = X\nA\u2208ARNNT\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |a\u03c4\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |ci\u03c4\n, ci\u03c4 \u22121, . . . , c0, h\u03c4\u2212i\u03c4\n)\n(3)\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |pi\u03c4, h\u03c4\u2212i\u03c4)\nwhere, P = (p1, \u00b7 \u00b7 \u00b7 , pL) represents the output of the predic\u0002tion network depicted in Figure 3 which summarizes the se\u0002quence of previously predicted non-blank labels, implemented\nas another neural network: pj = NN(\u00b7|c0, . . . , cj\u22121), where\nc0 is a special start-of-sentence label, \u27e8sos\u27e9. Thus, as can be\nseen in Eq. (2), RNN-T reduces some of the independence\nassumptions in CTC since the output at time t is conditionally\ndependent on the sequence of previous non-blank predictions,\nbut is independent of the specific choice of alignment (i.e.,\nthe choice of the frames at which the non-blank tokens were\nemitted).\nOur presentation of RNN-T alignments considers the\n\u201ccanonical\u201d case. In principle, however, the model can encode\nthe same set of conditional independence assumptions in\nRNN-T (i.e., the model structure), while considering alter\u0002native alignment structures as in the work of [49]. In their\nwork, Moritz et al., represent valid frame-level alignments as\nan arbitrary graph. This formulation, for example, allows for\nthe use of \u201cCTC-like\u201d alignments in the RNN-T model (i.e.,\noutputting a single label \u2013 blank, or non-blank \u2013 at each frame)\nwhile conditioning on the set of previous non-blank symbols\nas in the RNN-T model.\n3) Recurrent Neural Aligner (RNA): The recurrent neural\naligner (RNA) was proposed by Sak et al. [46]. The RNA\nmodel generalizes the RNN-T model by removing one of its\nconditional independence assumptions. The model, depicted\nin Figure 5, is best understood by considering how it differs\nfrom the RNN-T model. As with CTC and RNN-T, the RNA\nmodel defines a probability distribution over blank augmented\nlabels in the set Cb, where \u27e8b\u27e9 has the same semantics\nas in the CTC model: at each frame the model can only\noutput a single label \u2013 either blank, or non-blank \u2013 before\nadvancing to the next frame; unlike CTC (but as in RNN\u0002T) the model only outputs a single instance of each non\u0002blank label. More specifically, the set of valid alignments,\nARNA\n(X,C) = (a1, \u00b7 \u00b7 \u00b7 , aT ), in the RNA model consist of length T\nsequences in C\n\u2217\nb with exactly T \u2212L blank symbols, and which\nare identical to C after removing all blanks. Thus, the blank\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 5. An RNA Model [46] resembles the RNN-T model [14], [48] in\nterms of the model structure. However, this model is only permitted to output\na single label \u2013 either blank, or non-blank \u2013 in a single frame. Unlike\nRNN-T, the prediction network state in the RNA model, qt\u22121, depends on\nthe entire alignment sequence at\u22121, . . . , a1. The joint network produces a\nprobability distribution over the output symbols (augmented with blank) given\nthe prediction network state and a specific encoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 6. Example alignment sequence (right) for an RNA model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs; diagonal transitions correspond to outputting a non-blank\nsymbol. The FSA (left) represents the set of valid alignments for the RNA\nmodel. Although the FSA is identical to the corresponding FSA for RNN-T\nin Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases.\nsymbol has a different interpretation in RNA and the RNN\u0002T models: in RNN-T, outputting a blank symbol advances\nthe model to the next frame; in RNA, however, the model\nadvances to the next frame after outputting a single blank or\nnon-blank label. Restricting the model to output a single non\u0002blank label at each frame improves computational efficiency\nand simplifies the decoding process, by limiting the number\nof model expansions at each frame (in constrast to RNN-T\ndecoding). For example, if T = 8, and C = (s, e, e), then\nA = (\u27e8b\u27e9, s,\u27e8b\u27e9, e,\u27e8b\u27e9,\u27e8b\u27e9, e,\u27e8b\u27e9) \u2208 ARNA\n(X,C)\nas illustrated\nin Figure 6.\nThe RNA posterior probability, P(C|X), is defined as:\nPRNA(C|X) = X\nA\u2208ARNA\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|qt\u22121, ht) (4)\nwhere, as before it denotes the number of non-blank sym\u0002bols in the partial alignment sequence (a1, . . . , at\u22121), and\nqt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu\u0002ral network which summarizes the entire partial alignment\nsequence, where NN(\u00b7) represents a suitable neural network\n(an LSTM in [46]). Thus, RNA removes the one remaining\nconditional independence assumption of the RNN-T model,\nby conditioning on the sequence of previous non-blank labels\nas well as the alignment that generated them. However, this\ncomes at a cost: the exact computation of the log-likelihood in\nEq. (3) (and corresponding gradients) is intractable. Instead,\nRNA makes two simplifying assumption to ensure tractable\ntraining: by assuming that the model can only output a single\nlabel at each frame; and utilizing a straight-through estimator\nfor the alignment [50]. The latter constraint \u2013 allowing only a\nsingle label (blank or non-blank) at each frame \u2013 has also been\nexplored in the context of the monotonic RNN-T model [51].\nFinally, we note that the work in [52] further generalizes\nthe RNA model by employing two RNNs when defining the\nstate: a slow RNN (which corresponds to the sequence of\npreviously predicted non-blank labels), and a fast RNN (which\nalso conditions on the frames at which the non-blank labels\nwere output).\nC. Implicit Alignment Modeling Approaches\nOne of the main benefits of the explicit alignment ap\u0002proaches such as CTC, RNN-T, or RNA is that they result in\nASR models that are easily amenable to frame-synchronous\ndecoding6In this section, we discuss the attention-based\nencoder-decoder (AED) models (also known as, listen-attend\u0002and-spell (LAS)) [15], [16], [53], which employs the attention\nmechanism [43] to implicitly identify and model the portions\nof the input acoustics which are relevant to each output\nunit. These models were first popularized in the context of\nmachine translation [54]. Unlike explicit alignment modeling\napproaches, attention-based encoder-decoder models use an\nattention mechanism [43] to learn a correspondence between\nthe entire acoustic sequence and the individual labels. Such\nmodels support label-synchronous decoding, meaning that\nduring inference, each hypothesis in the beam is expanded\nby 1 label.\nIn the explicit alignment approaches presented in Sec\u0002tion III-B, during inference, the model continues to output\nsymbols until it has processed the final frame at which point\nthe decoding process is complete; similarly, during training,\nthe forward-backward algorithm aligns over all possible align\u0002ment sequences. Since an AED model processes the entire\nacoustic sequence at once, the model needs a mechanism\nby which it can indicate that it is done emitting all output\nsymbols. This is achieved by augmenting the set of outputs\nwith an end-of-sentence symbol, \u27e8eos\u27e9, so that the output\nvocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus,\nthe AED model, depicted in Figure 7, consists of an en\u0002coder network \u2013 which encodes the input acoustic frame\n6 By frame-synchronous decoding, we refer to the ability of the model to\nproduce output label for each input frame of speech. Models such as CTC,\nRNN-T, or RNA, support frame-synchronous decoding.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nSoftmax\nDecoder\nAttention\nEncoder H(X)\nFig. 7. An attention-based encoder decoder (AED) model [15], [16], [53].\nThe output distribution is conditioned on the decoder state, si (which\nsummarizes the previously decoded symbols), and the context vector, vi\n(which summarizes the encoder output based on the decoder state). In the\nseminal work of Chan et al., [16], for example, this is accomplished by\nconcatenating the two vectors, as denoted by the L symbol in the figure.\nsequence, X = (x1, . . . , xT\u2032 ), into a higher-level representa\u0002tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder\nwhich defines the probability distribution over the set of output\nsymbols, Ceos. Thus, given a paired training example, (X, C),\nwe denote by Ce = (c1, . . . , cL,\u27e8eos\u27e9), the ground-truth\nsymbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9\nsymbol. AED models compute the conditional probability of\nthe output sequence augmented with the \u27e8eos\u27e9 symbol as:\nP(Ce|X) = P(Ce|H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, vi)\n=\nL\nY\n+1\ni=1\nP(ci|si, vi) (5)\nwhere, vi corresponds to a context vector, which summarizes\nthe relevant portions of the encoder output, H(X), given\nthe sequence of previous predictions ci\u22121, . . . , c0; and, si\ncorresponds to the corresponding decoder state after outputting\nthe sequence of previous symbols, which is produced by\nupdating the decoder state based on the previous context vector\nand output label:\nsi = Decoder(vi\u22121, si\u22121, ci\u22121)\nThe symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol\nwhich serves as the first input to the attention-based decoder\nbefore it has produced any outputs. As can be seen in Eq. (5),\nan important benefit of AED models over models such as\nCTC or RNN-T is that they do not make any independence\nassumptions between model outputs and the input acoustics,\nTime\ns\ne\ne\nFig. 8. Unlike models such as RNN-T or CTC, AED models do not have\nexplicit alignment. However, it is possible to interpret the attention weights\n\u03b1t,i for a particular output symbol ci as an alignment weight which is\nrepresented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this\nrepresentation, the size of the circle and the darkness level are proportional\nto the corresponding attention weights; thus the total probability mass is the\nsame for each row. As illustrated above, the first few frames correspond to\nthe first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019:\nc3 = e.\nand are thus more general than the implicit alignment models,\nwhile being considerably easier to train and implement since\nwe do not have to explicitly marginalize over all possible\nalignment sequences. However, this comes at a cost: previ\u0002ously generated context vectors (which are analogous to the\ndecoded partial alignment in explicit alignment models) are\nnot revised as the decoding proceeds. Stated another way,\nwhile the encoder processing H(X) might be bi-directional,\nthe decoding process in AED models reveals a left-right\nasymmetry [55].\n1) Computing the Context Vector in AED Models: As\nwe mentioned before, the context vector, vi, is computed\nby employing the attention mechanism [43]. The central\nidea behind these approaches is to define a state vector si\nwhich corresponds to the state of the model after outputting\nc1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then\ndefines a score between the model state after outputting\ni \u2212 1 previous symbols, and each of the encoded frames in\nH(X). These scores can then be normalized using the softmax\nfunction to define a set of weights corresponding to each ht\nas:\n\u03b1t,i =\nexp {atten(ht, si)}\nPT\nt\n\u2032=1 exp {atten(ht\n\u2032 , si)}\nIntuitively, the weight \u03b1t,i represents the relevance of a\nparticular encoded frame ht when outputting the next symbol\nci, after the model has already output the symbols c1, . . . , ci\u22121,\nas illustrated in Figure 8. The context vector summarizes the\nencoder output based on the computed attention weights:\nvi =\nX\nt\n\u03b1t,iht\nA number of possible attention mechanisms have been\nexplored in the literature: the most common forms are called\n\u2018content-based attention\u2019, which include dot-product atten\u0002tion [16] and additive attention [43]. The content-based atten\u0002tion computes the attention score atten(ht, si) based on the\nrelevance between ht and si. However, the score does not\nconsider location information, i.e., it is determined by only the\ncontent, independent of the position. This can lead to incorrect\nattention weights with a large discrepancy against the previous\nsteps. Thus, location-based attention atten(si,fi,j ) has been\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nproposed [15], where fi,j is a convolutional feature vector\nextracted from \u03b1i\u22121, the attention weights in the previous step.\nThe hybrid attention, i.e., a combination of the content- and\nlocation-based attentions, has also been investigated in [15],\nshowing a higher accuracy than the separate ones. Besides,\nother location-based methods use a Gaussian (mixture) model\nestimated with sito obtain attention weights [56], [57].\nTransformer model [44] uses only content-based dot-product\nattention, but also takes location information into account\nthrough positional encoding. Apart from the specific choice\nof the attention mechanism, a common technique to improve\nperformance involves the use of multiple independent attention\nheads \u2013 v\n1\ni\n, . . . , v\nK\ni\n\u2013 which are then concatenated together\nto obtain the final context vector vi =\n\u0002\nv\n1\ni\n; . . . ; v\nK\ni\n\u0003\n, in\nthe so-called multi-head attention approach [44], or indeed\nby stacking together multiple attention-based layers in the\ntransformer decoder presented by Vaswani et al. [44].\nD. From Implicit to Explicit Alignment Modeling\nAED models, which make no conditional independence\nassumptions, are extremely powerful, often outperforming\nexplicit alignment E2E approaches such as CTC, or RNN\u0002T [41]. However, these models also have some significant\ndisadvantages, most notably that the models are typically non\u0002streaming: i.e., the models must process all acoustic frames\nbefore they can generate any output hypotheses. A somewhat\nrelated issue is that the models are extremely sensitive to\nthe length of the acoustic sequences, which requires special\nprocessing to be able to decode long-form audio [58]. There\nis a body of work that lies in between these two extremes:\nmodels such as the neural transducer [59], or those based on\nmonotonic alignments [60] and its variants (e.g., monotonic\nchunkwise alignments (MoChA) [61], monotonic infinite look\u0002back (MILK) [62] etc.) use an explicit alignment model, while\nalso utilizing an attention mechanism that allows the model\nto examine local acoustics in order to refine predictions. In\nother words, this corresponds to a class of streaming AED\nmodels. Generally speaking, these models are motivated by\nthe observation that speech (unlike tasks such as machine\ntranslation) exhibits a \u2018local\u2019 relationship between the encoded\nframes (assuming that the encoder is uni-directional) and\nthe output units; thus, unlike the general AED model which\ncomputes the context vector, vi, as a sum over all input\nframes ht, the various proposed models constrain this sum\nto be computed over a subset of frames to allow for streaming\ndecoding. In the context of our presentation, it is easiest to\nthink of these models as consisting of an underlying alignment\n(whether known or unknown) which can be used to perform\nstreaming inference.\nThe Neural Transducer (NT) [59] explicitly partitions the in\u0002put encoder frames into T W non-overlapping chunks of length\nW: HW\n1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; HW\nT W = [hT W +1, . . . , hT W W ],\nwhere T W =\n\u0006\nT\nW\n\u0007\n, and ht = 0 if t > T. Unlike the AED\nmodel which examines all encoded frames when computing\nthe context vector, the NT model is restricted to process\na single chunk at a time; the model only advances to the\nnext chunk when it outputs a special end-of-chunk symbol\n(analogous to \u27e8eos\u27e9 in the AED model); inference in the model\nterminates when the model has output the end-of-chunk sym\u0002bol in the final chunk HW\nT W . If the alignments of the ground\u0002truth output sequence, C, with respect to the W-length chunks\nare unknown, then it is possible to train the system by using a\nrough initial alignment where symbols are distributed equally\namong the T W chunks, followed by iterative refinement by\ncomputing the most likely output alignments given the current\nmodel parameters [59] similar to forced-alignments in HMM\u0002based systems. An alternate approach [63] consists of using a\nseparate system (e.g., a classical hybrid system) to get initial\nalignments (e.g., word-level alignments), which can be used\nto assign sub-word units to the individual chunks.\nAn alternative approach, proposed by Raffel et al. [60],\nmodifies the vanilla AED model by explicitly introducing an\nalignment module which scans the encoder frames, H(X),\nfrom left-to-right to identify whether the current frame should\nbe used to emit any outputs (modeled as a Bernoulli random\nvariable). If a frame, \u03c4 , is selected, then the model produces\nan output based on the local encoder frame, h\u03c4 . The process\nis then repeated starting from the currently selected frame,\nthus allowing multiple outputs to be generated at the same\nframe. This results in a model with hard monotonic alignments\nbetween the input speech and the output labels since the\nmodels are constrained to generate outputs in a streaming fash\u0002ion. A Monotonic Chunkwise Attention (MoChA) model [61]\nimproves upon the work of Raffel et al., by allowing the model\nto generate the next output using a context vector computed\nusing attention over a local window of frames to the left of the\nselected frame \u03c4 : h\u03c4\u2212W+1, . . . , h\u03c4 . Thus, the MoChA model\nconsists of a two-level process \u2013 identifying frames where\noutput should be produced following [60], followed by an\nAED model over frames to the left of the selected frame. A\nrefinement to the MoChA model, proposed by Arivazhagan et\nal. [62] \u2013 the monotonic infinite lookback (MILK) attention\nmodel \u2013 computes the context vector over all frames to the\nleft of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step.\nAnother two-fold approach to enable streaming operation is\npresented in [64] under the term of triggered attention, where\na CTC-network is used to trigger, i.e. control the activation\nof an AED model with a limited decoder delay. We also\ndirect interested readers to studies of various attention variants:\nMerboldt et al. [65] compare a number of local monotonic\nattention variants; Zeyer et al. [66] discuss segmental attention\nvariants; Zeyer et al. [67] study the related decoding and the\nrelevance of segment length modeling, leading to improved\ngeneralization towards long sequences. Segmental attention\nmodels are related to transducer models [68]. However, seg\u0002mental E2E ASR models are not limited to be realized based\non the attention mechanism and may not only be related to a\ndirect HMM [39], but have also been shown to be equivalent\nto neural transducer modeling [40], thus even providing a clear\nrelation between duration modeling and blank probabilities.\nRelationship to Classical ASR\nIn classical ASR models, these frame-level alignments can\nbe modeled with HMMs while using generative GMMs or\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nneural networks to model the output distribution of acous\u0002tic frames; frame-level alignments to train neural network\nacoustic models may be obtained by force-alignment from a\nbase GMM-HMM systems, but direct sequence training not\nrequiring initial alignments is also possible [69].\nE2E models introduce alternative alignment modeling ap\u0002proaches to ASR. While the attention mechanism provides\na qualitatively novel approach to map acoustic observation\nsequences to label sequences, transducer approaches [13], [14],\n[46], [70] handle the alignment problem in a way that can\nbe interpreted to be similar to HMMs with a specific model\ntopology, including marginalization over alignments [71], [72],\n[73]. CTC models can also be employed in an HMM-like fash\u0002ion during decoding [74]. Moreover, transducer approaches are\nequivalent to segmental models/direct HMM [40].\nAnother prominent feature of E2E systems besides the\nalignment property is their direct character-level modeling\navoiding phoneme-based modeling and pronunciation lexica\n[19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82],\nwith some even heading for whole-word modeling [76], [30].\nHowever, character-level modeling also is viable with classical\nhybrid HMM architectures [83] and has been shown to work\neven with standard HMM models w/o neural networks [84].\nIV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E\nMODELS\nIn this section, we describe various algorithmic changes\nto vanilla E2E models which are critical in order to obtain\nimproved performance over classical ASR systems. First, we\ndescribe various ways of combining different complementary\nE2E models to improve performance. Next, we introduce\nways to incorporate context into these models to improve\nperformance on rare proper noun entities. We then describe\nimproved encoder and decoder architectures that take better\nadvantage of the many cores on specialized architectures such\nas tensor processing units (TPUs) [85]. Finally, we discuss how\nto improve the latency of the model through an integrated E2E\nendpointer.\nA. Combinations of Models\nDifferent end-to-end models are complementary, and there\nhave been numerous attempts at combining these methods.\nFor example, Watanabe et al. [86] find that attention-based\nmodels perform poorly on long or noisy utterances, mainly\nbecause the model has too much flexibility in predicting\nalignments when presented with the entire input utterance.\nIn contrast, models such as CTC \u2013 which have left-to-right\nconstraints during decoding \u2013 perform much better in these\ncases. They propose to employ a multi-task learning strategy\nwith both CTC and attention-based losses, which provides a\n5\u201314% relative improvement in word error rate over attention\u0002based models on Wall Street Journal (WSJ) and Chime tasks.\nPang et al. [87] explore combining the benefits of RNN-T\nand AED. Specifically, RNN-T decodes utterances in a left\u0002to-right fashion, which works well for long utterances. On\nthe other hand, since AED sees the entire utterance, it often\nshows improvements for utterances where surrounding context\nis needed to predict the current word, e.g., \"one dollar\nand fifty cents\" \u2192 $1.50. To combine RNN-T and\nAED, the authors propose to produce a first-pass result with\nRNN-T, that is then rescored with AED in the second-pass.\nTo reduce computation, the authors share the encoder between\nRNN-T and AED. The authors find that RNN-T + AED\nprovides a 17\u201322% relative improvement in word error rate\nover RNN-T alone on a voice search task. Other flavors\nof streaming 1st-pass following by attention-based 2nd-pass\nrescoring, such as deliberation [88], have also been explored.\nOne of the issues with such rescoring approaches is that any\npotential improvements are limited to the lattice produced by\nthe 1st-pass system. To address this, methods which run a\n2nd-pass beam search have also been explored, particularly in\nthe context of streaming ASR \u2013 e.g. cascaded encoder [89],\nY-architecture [90] and Universal ASR [91].\nB. Incorporating Context\nContextual biasing to a specific domain, including a user\u2019s\nsong names, app names and contact names, is an impor\u0002tant component of any production-level automatic speech\nrecognition (ASR) system. Contextual biasing is particularly\nchallenging in E2E models because these models typically\nretain only a small list of candidates during beam search, and\ntend to perform poorly when recognizing words that are seen\ninfrequently during training (typically named entities), which\nis the main source of biasing phrases. There have been a few\napproaches in the literature to incorporate context.\nOne approach, known as shallow-fusion contextual bias\u0002ing [92], constructs a stand-alone weighted finite state trans\u0002ducer (FST) representing the biasing phrases. The scores from\nthe biasing FST are interpolated with the scores of the E2E\nmodel during beam search, with special care taken to ensure\nwe do not over- or under-bias phrases. An alternate approach\nproposes to inject biasing phrases into the model in an all\u0002neural fashion. For example, Pundak et al. [93] represent a\nset of biasing phrases by embedding vectors. These vectors\nare fed as additional input to an attention-based model, which\ncan then choose to attend to the phrases and hence boost the\nchances of predicting the phrases. Kim and Metze [94] propose\nto bias towards dialog context. In addition, Bruguier et al. [95]\nextend [93], by leveraging phonemic pronunciations for the\nbiasing phrases when constructing phrase embeddings. Finally,\nDelcroix et al. [96] use an utterance-wise context vector like an\ni-vector computed by a pooling across frame-by-frame hidden\nstate vectors obtained from a sub network (this sub-network\nis called a sequence-summary network).\nC. Encoder and Decoder Structure\nThere have been improvements to encoder architectures\nof E2E models over time. The first end-to-end models used\nlong short-term memory recurrent neural networks (LSTMs),\nfor both the encoder and decoder. The main drawback of\nthese sequential models is that each frame depends on the\ncomputation from the previous frame, and therefore multiple\nframes cannot be batched in parallel.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nWith the improvement of hardware, specifically on-device\nEdge Tensor Processing Units (TPUs), with thousands of\ncores, architectures that can better take advantage of the\nhardware, have been explored. Such architectures include\nconvolution-based architectures, such as ContextNet [97]. The\nuse of self-attention to replace the sequential recurrence\nin LSTMs was explored in Transformers for ASR [98],\n[99], [100]. Finally, combining self-attention with convolution,\nknown as Conformer [45], or multi-layer perceptron [101],\nwas also explored. Both Transformer and Conformer have\nshown competitive performance to LSTMs on many tasks\n[102], [103].\nOn the decoder side, research for transducer models has\nshown that a large LSTM decoder can be replaced with a sim\u0002ple embedding lookup table, that attends to only a few previous\ntokens from the model [47], [104], [105], [106], [107]. This\ndemonstrates that most of the power of the E2E model is in\nthe encoder, which has been a consistent theme of both E2E\nas well as classical hybrid HMM models. However, improved\ndecoder modeling may also be effective depending on the\nspecific downstream task. Research has shown that extended\ndecoder architectures enable pre-training and adaptation of the\ndecoder using extensive text-only data, leading to accuracy\ngains [108], [109]. For example, one approach separates RNN\u0002T\u2019s prediction network into separate blank and vocabulary\nprediction (LM) components, where the LM component can\nbe trained with text data [108]. In addition, in line with the\ngrowing interest in large language models in recent years,\nresearch has also begun on solving multiple tasks, including\nspeech recognition, using only an auto-regressive, GPT-style\ndecoder [110], [111].\nD. Integrated Endpointing\nAn important characteristic of streaming speech recognition\nsystems is that they must endpoint quickly, so that the ASR\nresult can be finalized and sent to the server for the appro\u0002priate action to be performed. Endpointing is typically done\nwith an external voice-activity detector. Since endpointing is\nboth an acoustic and language model decision, recent works\nin streaming RNN-T models [112], [113] have investigated\npredicting a microphone closing token \u27e8eos\u27e9 at the end of the\nutterance \u2013 e.g., \u201cWhat\u2019s the weather \u27e8eos\u27e9\u201d. Following the\nnotation from Section III, this is done by including an \u27e8eos\u27e9\ntoken as part of the set of class labels C and encouraging\nthe model to predict this token to terminate decoding. These\nmodels have shown improved latency and WER trade-off\nby having the endpointing decision predicted as part of the\nmodel. Furthermore, [114], [115] explored using the CTC\nblank symbol for endpoint detection.\nV. TRAINING E2E MODELS\nIn general, training of E2E models follows deep learn\u0002ing schemes [116], [117], with specific consideration of the\nsequential structure and the latent alignment problem to be\nhandled in ASR. E2E ASR models may be trained end-to\u0002end, notwithstanding potential elaborate training schedules\nand extensive data augmentation. Part of the appeal of end\u0002to-end models is that they do not assume conditional in\u0002dependence between the input frames. Given a training set\nT = {(Xn, Cn)}\nN\nn=1, the training criterion L to be minimized\ncan be written as: L = \u2212\nPN\nn=1 log P(Cn|Xn) (which is\nequivalent to maximizing the total conditional log-likelihood).\nA. Alignment in Training\nE2E models such as RNN-T and CTC introduce an addi\u0002tional blank token \u27e8b\u27e9 for alignment. Therefore optimization\nimplies marginalizing across all alignments, as follows:\nLex = \u2212\nX\nN\nn=1\nX\nAn\nlog P(Cn, An|Xn)\nThis requires the forward-backward algorithm [118], [119] for\nefficient computation of the training criterion and its gradient,\nwith minor modifications for CTC, RNN-T, and RNA models,\nas well as classical (full-sum) hybrid ANN/HMMs correspond\u0002ing to the differences in alignments defined in each of these\nmodels. In comparison, AED models are based on implicit\nalignment modeling approaches, and the training criterion does\nnot have a latent variable A for explicit alignment as:\nLim = \u2212\nX\nN\nn=1\nlog P(Cn|Xn)\nWe refer the interested reader to the individual papers for\nfurther details on the training algorithms [13], [14], [15], [16],\n[46], [48], [53], [71], [120]. As shown in Section III-A, in both\nexplicit and implicit alignment cases, P(C|X) is factorized\nwith respect to input time t and output position i, respectively,\nand the factorized distribution is conditioned on the label\ncontext c\ni\u22121\n1\n, except for CTC. For example, in the AED case:\nlog P(C|X) = PL\ni=1 log P(ci\n|X, ci\u22121\n1\n). During training, we\nuse a teacher-forcing technique where the ground truth history\nis used as a label context.\nAs part of the training procedure, all E2E as well as classical\nhidden Markov models for ASR provide mechanisms to solve\nthe underlying sequence alignment problem - either explicitly\nvia corresponding latent variables, as in CTC, RNN-T or RNA,\nand also hybrid ANN/HMM, or implicitly, as in AED models.\nAlso, the distinction between speech and silence needs to be\nconsidered, which may be handled explicitly by introducing\nsilence as a latent label (hybrid ANN/HMM), or implicitly\nby not labeling silence at all, as currently is the standard in\nvirtually all E2E models.\nE2E models also may take advantage of hierarchical training\nschedules. These schedules may comprise several separate\ntraining passes and explicit, initially generated alignments that\nare kept fixed for some Viterbi-style [121], [122], [123] train\u0002ing epochs before re-enabling E2E-style full-sum training that\nmarginalizes over all possible alignments. Such an alternative\napproach is employed by Zeyer et al. [52], where an initial\nfull-sum RNN-T model is used to generate an alignment and\ncontinue with framewise cross-entropy training. This greatly\nsimplifies the training process by replacing the summation\nover all possible alignments in Eq. (4) by a single term cor\u0002responding to the alignment sequence generated. Recently, a\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nsimilar procedure has been introduced in [124] also employing\nE2E models, only. In this work, CTC is used to initialize\nthe training and to generate an initial alignment, followed by\nintermediate Viterbi-style RNN-T training and final full-sum\nfine tuning, which improved convergence compared to full\u0002sum-only training approaches.\nIt is interesting to note that in contrast to the RNN-T and\nRNA label-topologies, CTC does not require alignments with\nsingle label emissions per label position. However, training\nCTC models eventually does lead to single label emissions\nper hypothesized label. An analysis of this property of CTC\ntraining which is usually called peaky behavior can be found\nin [125] and references therein. Laptev et al. [126] even\nintroduces a CTC variant without non-blank loop transitions.\nB. Training with External Language Models\nE2E ASR models generally are normalized on sequence\nlevel. Therefore, sequence training with the maximum mutual\ninformation criterion [127] is the same as standard cross en\u0002tropy/conditional likelihood training. However, once external\nlanguage models are included in the training phase, sequence\nnormalization needs to be included explicitly, leading to MMI\nsequence discriminative training. This has been exploited as\na further approach to combine E2E models with external\nlanguage models trained on text-only data during the training\nphase itself [128], [129], [130].\nC. Minimum Word Error Rate Training\nSince the objective of speech recognition is to minimize\nword error rate (WER), there has been a growing number of\nresearch studies that incorporate this into the objective function\nby minimizing the model-based expectation of the number of\nword errors, as follows:\nLmwer =\nX\nN\nn=1\nX\nC\u2032\nn\nW(Cn, C\u2032\nn\n)P(C\n\u2032\nn\n|Xn)\nwhere W(Cn, C\u2032\nn\n) is the word error count in a hypothesis\nC\n\u2032\nn given a reference Cn, and n is an index which iterates over\nthe entire training set. These methods, known as sequence\nor discriminative training, have shown great improvements\nfor classical ASR [131], [132], [133], [134], [135], and have\nsince been explored in E2E models. Typically these losses\nare constructed by running in \u2018beam-search\u2019 mode rather than\nteacher-forcing mode, and construct a loss from the errors\nmade from the candidate hypotheses in the beam. Thus, this\ntype of training first requires training the model to optimize\nP(C|X) in order to initialize the model with a good set\nof parameters to run a beam search. However, also direct\napproaches have been introduced that avoid this separation\nto train discriminatively from scratch [69], [136].\nPapers that explore penalizing word errors include, Mini\u0002mum Word Error Rate (MWER) training [137], where the loss\nfunction is constructed such that the expected number of word\nerrors are minimized. Further work includes MWER for RNN\u0002T and self-attention-T [138], as well as MWER using prefix\nsearch instead of n-best [139]. Also, there have been studies\nthat consider MWER in terms of reinforcement learning [140],\n[141]. Optimal Completion Distillation (OCD) [81] proposes\nto minimize the total edit distance using an efficient dynamic\nprogramming algorithm. Finally, another body of research with\nsequence training introduce a separate external language model\nat training time [142], which can also be done efficiently via\napproximate lattice recombination [129] and also lattice-free\napproaches [130].\nD. Pretraining\nAll E2E models as well as classical hidden Markov models\nfor ASR provide holistic models that in principle enable train\u0002ing from scratch. However, many strategies exist to initialize\nand guide the training process to reach optimal performance\nand/or to obtain efficient convergence by applying pretrain\u0002ing and model growing [143], [144]. Supervised layer-wise\npretraining has been successfully applied for classical [5],\n[145], as well as attention-based ASR models [146], which can\nbe combined with intermediate sub-sampling schemes [147],\nand model growing [148]. Pretraining approaches utilizing\nuntranscribed audio, large-scale semi-supervised data and/or\nmultilingual data [149], [150], [151], [152], [153], [154],\n[155], [156], [157], [158], [159], [160] would deserve a\nself-contained survey and they are applicable for hybrid\nDNN/HMM and E2E approaches likewise \u2013 they will not be\nfurther discussed here.\nE. Training Schedules and Curricula\nDedicated training schedules have been developed to guide\nthe optimization process and as part of that reach proper\nalignment behavior explicitly or implicitly [52], [124], [147].\nMany approaches exist for learning rate control [161], [162]:\nNewBob [163], [164] and enhancements [162]; global ver\u0002sus parameter-wise learning rate control (exponential decay,\npower decay, etc.) [165]; learning rate warm-up [44]; warm\nrestarts/cosine annealing [166]; weight decay versus gradually\ndecreasing batch size [167]; fine-tuning [168] or population\u0002based training [169]; etc. For a survey of meta learning\ncf. [170].\nSequence learning approaches also consider curriculum\nlearning [171], [172], e.g., by considering short sequences\nfirst [173], [174]; interim increase of sub-sampling [147]\ninitially more sub-sampling; or, for multi-speaker ASR training\nsort mixed speech by SNR and start with speakers of balanced\nenergy and mixed gender [175].\nF. Optimization and Regularization\nOptimization usually is based on stochastic gradient descent\n[176], with momentum [177], [178], and a number of corre\u0002sponding adaptive approaches, most prominently Adam [179]\nand variants thereof [145], [179], [180].\nInvesting more training epochs seems to provide improve\u0002ments [52, Table 8], and also averaging over epochs has been\nreported to help [102]. For a discussion of the double descent\neffect and its relation to the amount of training data, label\nnoise and early stopping cf. [181].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nRegularization strongly contributes to training performance:\ne.g., L2 and weight decay [182], [166]; weight noise [183];\nadaptive mean L1/L2 [184]; gradient noise [185]; dropout\n[186], [187], [188], layer dropout [189], [190], [191]; dropcon\u0002nect [192]; zoneout [193]; smoothing of attention scores [15];\nlabel smoothing [194]; scheduled sampling [195]; auxiliary\nloss [194], [196]; variable backpropagation through time [197],\n[198]; mixup [199]; increased frame rate [180]; or, batch\nnormalization [200].\nG. Data Augmentation\nTraining of E2E ASR models also benefit from data aug\u0002mentation methods, which might also be viewed as regu\u0002larization methods. However, their diversity and impact on\nperformance justifies a separate overview.\nMost data augmentation methods perform data perturbation\nby exploiting certain dimensions of speech signal variation:\nspeed perturbation [201], [202], vocal tract length perturbation\n[201], [203], frequency axis distortion [201], sequence noise\ninjection [204], SpecAugment [205], or semantic mask [206].\nAlso, text-only data may be used to generate data using text\u0002to-speech (TTS) on feature [207] or signal level [208]. In a\ncomparison of the effect of TTS-based data augmentation on\ndifferent E2E ASR architectures in [208], AED seemed to be\nthe only architecture that appeared to benefit significantly from\nthe TTS data.\nIn a recent study [174] and corresponding follow-up work\n[180], many of the regularization and data augmentation\nmethods listed here have been exploited jointly leading to\nstate-of-the-art performance on the Switchboard task for a\nsingle-headed AED model.\nRelationship to Classical ASR\nE2E systems attempt to define ASR models that integrate\nall knowledge sources into a single global joint model that\ndoes not utilize secondary knowledge sources and avoids the\nclassical separation into acoustic and language models. These\nglobal joint models are completely trained from scratch using\na single global training criterion based on a single kind of\n(transcribed) training data and thus require less ASR domain\u0002specific knowledge provided sufficient amounts of training\ndata are available.\nWhile standard hybrid ANN/HMM training for ASR using\nframe-wise cross entropy already is discriminative, it is not\nyet sequence discriminative, requires prior alignments and\nalso lacks consideration of an (external) language model\nduring training. However, these potential shortcomings may\nbe remedied by using sequence discriminative training criteria\n[127] and lattice-free training approaches [69].\nIn contrast to strict E2E systems, the classical ASR ar\u0002chitecture includes the use of secondary knowledge sources\nbeyond the primary training data, i.e. (transcribed) speech\naudio for acoustic model training, and textual data for language\nmodel training. Most prominently, this includes the use of a\npronunciation lexicon and the definition of a phoneme set.\nSecondary resources like pronunciation lexica may be helpful\nin low-resource scenarios. However, their generation often is\ncostly and may even introduce errors, like pronunciations from\na lexicon not reflecting the actual pronunciations observed.\nTherefore, for large enough training resources, secondary\nknowledge sources might become obsolete [209], or even\nharmful, in case of erroneous information introduced [210],\n[211].\nClassical ASR models usually are trained successively, with\nknowledge derived from models trained earlier injected into\nlater training stages, e.g. in the form of HMM state alignments.\nHowever, such approaches from classical ASR might also\nbe interpreted as specific training schedules. Initializing deep\nlearning models using HMM alignments obtained from acous\u0002tic models based on mixtures of Gaussians may be interpreted\nin this way, with the Gaussian mixtures serving as an initial\nshallow model. In classical ASR, also approaches training\ndeep neural networks from scratch while avoiding interme\u0002diate training of Gaussians has been proposed [212], [213],\n[214], also in combination with character-level modeling [83].\nAnother step towards more integrated training of classical\nsystems has been to apply discriminative training criteria\navoiding intermediate (usually lattice-based) representations of\ncompeting word sequences [215], [69], [216], [217], [136].\nThe training of classical ASR systems usually applies sec\u0002ondary objectives to solve subtasks like phonetic clustering.\nThe classification and regression trees (CART) approach is\nused to cluster triphone HMM states [27], [218]. More re\u0002cent approaches proposed clustering within a neural network\nmodeling framework, while still retaining secondary clustering\nobjectives [219], [213]. However, also in E2E approaches\nsecondary objectives are used, most prominently for subword\ngeneration, e.g. via byte-pair encoding [32]. Also, available\npronunciation lexica can be utilized indirectly for assisting\nsubword generation for E2E systems [35], [36], which are\nshown to outperform byte-pair encoding. Within classical ASR\nsystems, phonetic clustering also can be avoided completely\nby modeling phonemes in context directly [220].\nIt is interesting to observe that specifically attention-based\nencoder-decoder models require larger numbers of training\nepochs to reach high performance, e.g. for a comparison\nof systems trained on Switchboard 300h cf. Table 5 in\n[221]. Also, attention-based encoder-decoder models have\nbeen shown to suffer from low training resources [222], [223],\nwhich can be improved by a number of approaches, including\nregularization techniques [174] as well as data augmentation\nusing SpecAugment [224] and text-to-speech (TTS) [29].\nSpecAugment also is shown to improve classical hybrid HMM\nmodels [225]. TTS on the other hand considerably improved\nattention-based encoder-decoder models trained on limited\nresources, but did not reach the performance of other E2E\napproaches or hybrid HMM models, which in turn were not\nconsiderably improved by TTS [208]. Multilingual approaches\nalso help improve ASR development for low resource tasks,\nagain both for classical [226], as well as for E2E systems\n[227], [228].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nVI. DECODING E2E MODELS\nThis section describes several decoding algorithms for end\u0002to-end speech recognition. The basic decoding algorithm of\nend-to-end ASR tries to estimate the most likely sequence C\u02c6\namong all possible sequences, as follows:\nC\u02c6 = arg max\nC\u2208U\u2217\nP(C|X)\nThe following section describes how to obtain the recognition\nresult C\u02c6.\nA. Greedy Search\nThe Greedy search algorithm is mainly used in CTC, which\nignores the dependency of the output labels as follows:\nA\u02c6 =\nY\nT\nt=1\n\u0012\narg max\nat\nP(at|X)\n\u0013\nwhere at is an alignment token introduced in Section III-B1.\nThe original character sequence is obtained by converting\nalignment token sequence A\u02c6 to the corresponding token se\u0002quence C\u02c6. The argmax operation can be performed in parallel\nover input frame t, yielding fast decoding [13], [229], although\nthe lack of the output dependency causes relatively poor\nperformance than the attention and RNN-T based methods in\ngeneral.\nCTC\u2019s fast decoding is further boosted with transformer\n[44], [98], [102] and its variants [45], [103] since their entire\ncomputation across the frames is parallelized [190], [230].\nFor example, the non-autoregressive models, including Im\u0002puter [231], Mask-CTC [230], Insertion-based modeling [232],\nContinuous integrate-and-fire (CIF) [233] and other variants\n[234], [235] have been actively studied as an alternative non\u0002autoregressive model to CTC. [235] shows that CTC greedy\nsearch and its variants achieve 0.06 real-time factor (RTF)7\nby using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz. The\npaper also shows that the degradation of the non-autoregressive\nmodels from the attention/RNN-T methods with beam search\nis not extremely large (19.7% with self-conditioned CTC [234]\nversus 18.5 and 18.9% with AED and RNN-T, respectively).\nThe greedy search algorithm is also used as approximate\ndecoding for both implicit and explicit alignment modeling\napproaches, including AED, RNA, CTC, and RNN-T, as\nfollows:\nc\u02c6i = arg max\nci\nP(ci|C\u02c6\n1:i\u22121, X) for i = 1, . . . , N\na\u02c6t = arg max\nat\nP(at|A\u02c6\n1:t\u22121, X) for t = 1, . . . , T\nThe greedy search algorithm does not consider alternate\nhypotheses in a sequence compared with the beam search\nalgorithm described below. However, it is known that the\ndegradation of the greedy search algorithm is not very large\n[16], [46], especially when the model is well trained in\nmatched conditions8.\n7 The ratio of the actual decoding time to the duration of the input speech.\n8 On the other hand, in the AED models, increasing the search space does\nnot consistently improve the speech recognition performance [77], [236] \u2013 a\nfact also observed in neural machine translation [237].\nB. Beam Search\nThe beam search algorithm is introduced to approximately\nconsider a subset of possible hypotheses C\u02dc among all possible\nhypotheses U\n\u2217 during decoding, i.e., C \u2282 U \u02dc \u2217\n. A predicted\noutput sequence C\u02c6 is selected among a hypothesis subset C\u02dc\ninstead of all possible hypotheses U\n\u2217\n, i.e.,\nC\u02c6 = arg max\nC\u2208C\u02dc\nP(C|X) (6)\nThe beam search algorithm is to find a set of possible hy\u0002potheses C\u02dc, which can include promising hypotheses efficiently\nby avoiding the combinatorial explosion encountered with all\npossible hypotheses U\n\u2217\n.\nThere are two major beam search categories: 1) frame\nsynchronous beam search and 2) label synchronous beam\nsearch. The major difference between them is whether it\nperforms hypothesis pruning for every input frame t or every\noutput token i. The following sections describe these two\nalgorithms in more detail.\nC. Label Synchronous Beam Search\nSuppose we have a set of partial hypotheses up to (i \u2212 1)th\ntoken C\u02dc\n1:i\u22121. A set of all possible partial hypotheses up to ith\ntoken C1:iis expanded from C\u02dc\n1:i\u22121 as follows:\nC1:i = {(C\u02dc\n1:i\u22121, ci = c)}c\u2208U (7)\nThe number of hypotheses |C1:i| would be |C\u02dc\n1:i\u22121| \u00d7 |U|, at\nmost. The beam search algorithm prunes the low probability\nscore hypotheses from C1:i and only keeps a certain number\n(beam size \u2206) of hypotheses at i among C1:i. This pruning\nstep is represented as follows:\nC\u02dc\n1:i = NBESTC1:i\u2208C1:i P(C1:i\n|X), where |C\u02dc\n1:i\n| = \u2206 (8)\nNote that NBEST(\u00b7) is an operation to extract top \u2206 hypothe\u0002ses in terms of the probability score P(C1:i\n|X) computed from\nan end-to-end neural network, or a fusion of multiple scores\ndescribed in Section VII-B.\nIn the label synchronous beam search, the length of the out\u0002put sequence (N) is unknown. Therefore, during this pruning\nprocess, we also add the hypothesis that reaches the end of an\nutterance (i.e., predict the end of sentence symbol \u27e8eos\u27e9) to a\nset of hypotheses C\u02dc in Eq. (6) as a promising hypothesis.\nThe label synchronous beam search does not explicitly\ndepend on the alignment information; thus, it is often used\nin implicit alignment modeling approaches, including AED.\nDue to this nature, sequence hypotheses of the same length\nmight cover a completely different number of encoder frames,\nunlike the frame synchronous beam search, as pointed out by\n[40]. As a result, we observe that the scores of very short and\nlong segment hypotheses often become the same range, and\nthe beam search wrongly selects such hypotheses. [86] shows\nan example of such extreme cases, resulting in large deletion\nand insertion errors for short and long-segment hypotheses,\nrespectively. Thus, the label synchronous beam search requires\nheuristics to limit the output sequence length to avoid ex\u0002tremely long/short output sequences. Usually, the minimum\nand maximum length thresholds are determined proportionally\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nto the input frame length |X| with tunable parameters \u03c1min and\n\u03c1max as Lmin = \u230a\u03c1min|X|\u230b, Lmax = \u230a\u03c1max|X|\u230b. Although these\nare quite intuitive ways to control the length of a hypothesis,\nthe minimum and maximum output lengths depend on the\ntoken unit or type of script in each language. Another heuristic\nis to provide an additional score related to the output length\nor attention weights \u2013 e.g., a length penalty, and a coverage\nterm [77], [238]. The end-point detection [239] is also used to\nestimate the hypothesis length automatically. [236] redefines\nthe implicit length model of the attention decoder to take into\naccount beam search, resulting in consistent behavior without\ndegradation for increasing beam sizes.\nNote that there are several studies on applying label syn\u0002chronous beam search to explicit alignment modeling ap\u0002proaches. For example, label synchronous beam search al\u0002gorithms for CTC are realized by marginalizing all possible\nalignments for each label hypothesis [13]. [240] extends\nCIF [233] to produce label-level encoder representation and\nrealizes label synchronous beam search in RNN-T.\nD. Frame Synchronous Beam Search\nIn contrast to the label synchronous case in Eq. (8), the\nframe synchronous beam search algorithm performs pruning\nat every input frame t, as follows:\nC\u02dc\n1:i(t) = NBESTC1;i(t) P(C1;i(t)\n|X), where |C\u02dc\n1:i(t)\n| = \u2206\nwhere C1;i(t)is an i(t)-length label sequence obtained from\nthe alignment A1:t, which is introduced in Sec. III-B.\nP(C1;i(t)|X) is obtained by summing up all possible align\u0002ments A1:t \u2208 A(X,C1;i(t))\n. Unlike the label synchronous beam\nsearch, frame synchronous beam search depends on explicit\nalignment A; thus, it is often used for explicit alignment\nmodeling approaches, including CTC, RNN-T, and RNA.\nC1:i(t)is an expanded partial hypotheses up to input frame\nt, similar to Eq. (7).\nCompared with the label synchronous algorithm, the frame\nsynchronous algorithm needs to handle additional output to\u0002ken transitions inside the beam search algorithm. The frame\nsynchronous algorithm can be easily extended in online and/or\nstreaming decoding, thanks to the explicit alignment informa\u0002tion with input frame and output token.\nClassical approaches to beam search for HMM, but also\nCTC and RNN-T variants, are based on weighted finite state\ntransducers (WFST) [38], [74], [241] or lexical prefix trees\n[106], [242], [243]. They are categorized as frame synchronous\nbeam search. These methods are often combined with an N\u0002gram language model or a full-context neural language model\n[244], [245]. RNN-T [14], [246] and CTC prefix search [247]\ncan deal with a neural language model by incorporating the\nlanguage model score in the label transition state. Interestingly,\ntriggered attention approaches [248], [249] allow us to use\nimplicit alignment modeling approaches, including AED, in\nframe-synchronous beam search together with CTC and neural\nLM, which applies on-the-fly rescoring to the hypotheses given\nby CTC prefix search using the AED and LM scores.\nE. Block-wise Decoding\nAnother beam search implementation uses a fixed-length\nblock unit for the input feature. In this block processing, we\ncan use the future context inside the block by using the non\u0002causal encoder network based on the BLSTM, output-delayed\nunidirectional LSTM, or transformer (and its variants). This\nfuture context information avoids the degradation of the fully\ncausal network. In this setup, the chunk size becomes the\ntrade-off of controlling latency and accuracy. This technique is\nused in both RNN-T [100], [250], [251] and AED [61], [252],\n[253], [254]. Block-wise processing is especially important for\nimplicit alignment modeling approaches, including AED, since\nit can provide block-wise monotonic alignment constraint\nbetween the input feature and output label, and realize block\u0002wise streaming decoding.\nF. Model Fusion during Decoding\nSimilar to the classical HMM-based beam search, we com\u0002bine various scores obtained from different modules, including\nthe main end-to-end ASR and LM scores.\n1) Synchronous Score Fusion: The most simple score fu\u0002sion is performed when the scores of multiple modules are\nsynchronized. In this case, we can simply add the multiple\nscores at each frame t or label i. The most well-known score\ncombination is LM shallow fusion.\nLM shallow fusion: As discussed in Sec. VII, various neural\nLMs can be integrated with end-to-end ASR. The most simple\nintegration is based on LM shallow fusion [255][256][257], as\ndiscussed in Sec. VII-B1, which (log-) linearly adds the LM\nscore Plm(C1:i) to E2E ASR scores P(C1:i|X) during beam\nsearch in Eq. (8) as follows:\nlog P(C1:i|X) \u2192 log P(C1:i|X) + \u03b3 log Plm(C1:i)\nwhere \u03b3 is a language model weight. Of course, we can\ncombine other scores, such as the length penalty and coverage\nterms, as discussed in Sec. VI-C.u\n2) Asynchronous Score Fusion: If we combine the frame\u0002dependent score functions, P(at|\u00b7), used in explicit alignment\nmodeling approaches, e.g., CTC, RNN-T, and label-dependent\nscore functions, P(ci|\u00b7), used implicit alignment modeling\napproaches, e.g., AED, language model, we have to deal with\nthe mismatch between the frame and label time indices t and\ni, respectively.\nIn the time-synchronous beam search, this fusion is per\u0002formed by incorporating the language model score in the\nlabel transition state [70], [22], [258]. [247] also combines\na word-based language model and token-based CTC model\nby incorporating the language model score triggered by the\nword delimiter (space) symbol.\nIn the label-synchronous beam search, we first compute\nthe label-dependent scores from the frame-dependent score\nfunction by marginalizing all possible alignments given a\nhypothesis label sequence. CTC/attention joint decoding [86]\nis a typical example, where the CTC score is computed\nby marginalizing all possible alignments based on the CTC\nforward algorithm [229]. This approach eliminates the wrong\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nalignment issues and difficulties of finding the correct end of\nsentences in the label-synchronous beam search [86].\nNote that the model fusion method during beam search can\nrealize simple one-pass decoding, while it limits the time unit\nof the models to be the same or it requires additional dynamic\nprogramming to adjust the different time units, especially for\nthe label-synchronous beam search. This dynamic program\u0002ming computation becomes significantly large when the length\nof the utterance becomes larger and requires some heuristics\nto reduce the computational cost [259].\nG. Lexical Constraint during Score Fusion\nClassically, we use a word-based language model to cap\u0002ture the contextual information with the word unit, and also\nconsider the word-based lexical constraint for ASR. However,\nend-to-end ASR often uses a letter or token unit and it causes\nfurther unit mismatch during beam search. As described in\nprevious sections, the classical approach of incorporating the\nlexical constraint from the token unit to the word unit is\nbased on a WFST. This method first makes a TLG transducer\ncomposed of the token (T), word lexicon (L), and word-based\nlanguage transducers (G) [74]. This TLG transducer has been\nused for both CTC [74] and attention-based [53] models.\nAnother approach used in the time synchronous beam search\nis to insert the word-based language model score triggered by\nthe word delimiter (space) symbol [75]. To synchronize the\nword-based language model with a character-based end-to-end\nASR, [260] combines the word and character-based LMs with\nthe prefix tree representation, while [239], [261] uses look\u0002ahead word probabilities to predict next characters instead of\nusing the character-based LM. The prefix tree representation\nis also used for the sub-word token unit case [262], [263].\nH. Multi-pass Fusion\nThe previous fusion methods are performed during the\nbeam search, which enables a one-pass algorithm. The popular\nalternative methods are based on multi-pass algorithms where\nwe do not care about the synchronization and perform n-best or\nlattice scoring by considering the entire context within an ut\u0002terance. [16] uses the N-best rescoring techniques to integrate\na word-based language model. [55] combines forward and\nbackward searches within a multi-pass decoding framework to\ncombine bidirectional LSTM decoder networks. Recently two\u0002pass algorithms of switching different end-to-end ASR systems\nhave been investigated, including RNN-T \u2192 AED [264]; CTC\n\u2192 AED [265], [266]. This aims to provide streamed output in\nthe first pass and re-scoring with AED in the second pass to\nrefine the previous output, thus satisfying a real-time interface\nrequirement while providing high recognition performance.\nIn addition to the N-best output in the above discussion,\nthere is a strong demand for generating a lattice output\nfor better multi-pass decoding thanks to richer hypothesis\ninformation in a lattice. The lattice output can also be used\nfor spoken term detection, spoken language understanding,\nand word posteriors. However, due to the lack of Markov\nassumptions, RNN-T and AED cannot merge the hypothesis\nand cannot generate a lattice straightforwardly, unlike the\nHMM-based or CTC systems. To tackle this issue, there are\nseveral studies of modifying these models by limiting the\noutput dependencies in the fixed length (i.e., finite-history)\n[47], [267], or keeping the original RNN-T structure but\nmerging the similar hypotheses during beam search [107].\nI. Vectorization across both Hypotheses and Utterances\nWe can accelerate the decoding process by vectorizing\nmultiple hypotheses during the beam search, where we replace\nthe score accumulation steps for each hypothesis with vector\u0002matrix operations for the vectorized hypotheses. This has been\nstudied in RNN-T [22], [258], [268] and attention-based [259]\nmodels. This modification leverages the parallel computing\ncapabilities of multi-core CPUs, GPUs and TPUs, resulting in\nsignificant speedups, while enabling multiple utterances to be\nprocessed simultaneously in a batch. Major deep neural net\u0002work and end-to-end ASR toolkits support this vectorization.\nFor example, Tensorflow9[269], and FAIRESEQ10 [270] pro\u0002vide a vectorized beam search interface for a generic sequence\nto sequence task, and it can be used for attention-based end-to\u0002end ASR. End-to-end ASR toolkits including ESPnet11 [259],\nESPRESSO12[261], LINGVO [271], and, RETURNN13 [272]\nalso support the vectorized beam search algorithm.\nRelationship to Classical ASR\nOne of the most prominent properties shared between E2E\nand classical statistical ASR systems is the use of a single\u0002pass decoding strategy, which integrates all knowledge sources\ninvolved (models, components), before coming to a final\ndecision [123]. This includes the use of full label context\ndependency both for E2E systems [229], [51], [77], [273],\n[174], [262], [274], [275], as well as classical systems via full\u0002context language models [276], [244], [245], [277]. In classical\nASR systems, even HMM alignment path summation may be\nretained in search [278]. Both E2E as well as classical ASR\nsystems employ beam search in decoding. However, compared\nto classical search approaches, E2E beam search usually is\nhighly simplified with very small beam sizes around 1 to\n100 [15], [16], [77], [147]. Very small beam sizes also partly\nmask a length bias exhibited by E2E attention-based encoder\u0002decoder models [279], [280], thus trading model errors against\nsearch errors [281]. An overview of approaches to handle the\nlength bias beyond using small beam sizes in ASR is presented\nin [236].\nMany classical ASR search paradigms are based on mul\u0002tipass approaches that successively generate search space\nrepresentations applying increasingly complex acoustic and/or\nlanguage models [282], [283], [243]. However, multipass\nstrategies also are employed using E2E models, which how\u0002ever softens the E2E concept. Decoder model combination is\npursued in a two-pass approach, while even retaining latency\nconstraints as in [87]. Further multipass approaches include\nE2E adaptation approaches [284], [285], [286], [287].\n9 https://www.tensorflow.org/api docs/python/tf/contrib/seq2seq/BeamSearchDecoder\n10 https://github.com/pytorch/fairseq/blob/master/fairseq/sequence\ngenerator.py 11 https://github.com/espnet/espnet\n12 https://github.com/freewym/espresso\n13 https://github.com/rwth-i6/returnn\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nVII. LM INTEGRATION\nThis section discusses language models (LMs) used for E2E\nASR. Hybrid ASR systems have long been using a pretrained\nLM [2], whereas most end-to-end (E2E) ASR systems employ\na single E2E model that includes a network component acting\nas an LM.14 For example, the prediction network of RNN\u0002T and the decoder network of AED models take on the role\nof a LM covering label back-histories. Therefore, E2E ASR\ndoes not seem to require external LMs. Nevertheless, many\nstudies have demonstrated that external LMs help improve the\nrecognition accuracy in E2E ASR.\nThere are presumably three reasons that E2E ASR still\nrequires an external LM:\na) Compensation for poor generalization: E2E models\nneed to learn a more complicated mapping function than\nclassical modular-based models such as acoustic models. Con\u0002sequently, E2E models tend to face overfitting problems if\nthe amount of training data is not sufficient. Pretrained LMs\npotentially compensate for the less generalized predictions\nmade by E2E models.\nb) Use of external text data: E2E models need to be\ntrained using paired speech and text data, while LMs can\nbe trained with only text data. Generally, text data can be\ncollected more easily than the paired data. The training speed\nof an LM is also faster than that of E2E models for the same\nnumber of sentences. Accordingly, the LM can be improved\nmore effectively with external text data, providing additional\nperformance gain to the ASR system.\nc) Domain adaptation: Domain adaptation helps im\u0002prove recognition accuracy when the E2E model is applied\nto a specific domain. However, domain adaptation of the E2E\nmodel requires a certain amount of paired data in the target\ndomain. Also, when multiple domains are assumed, it may be\ncostly to maintain multiple E2E models for the domains the\nsystem supports. If a pretrained LM for the target domain is\navailable, it may more easily improve recognition accuracy for\ndomain-specific words and speaking styles without updating\nthe E2E model.\nThis section reviews various types of LMs used for E2E\nASR and fusion techniques to integrate LMs into E2E models.\nA. Language Models\nThe LMs provide a prior probability distribution, P(C). If\nthe sentence, C, can be decomposed into a sequence of tokens\nsuch as characters, subwords, and single words, the probability\ndistribution can be computed based on the chain rule as:\nP(C) =\nL\nY\n+1\ni=1\nP(ci|c0:i\u22121)\nwhere ci denotes the i-th token of C, and c0:i\u22121 represents\ntoken sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and\ncL+1 = \u27e8eos\u27e9.\nMost LMs are designed to provide the conditional probabil\u0002ity P(ci\n|c0:i\u22121), i.e., they are modeled to predict the next token\n14 In the simplest case of a CTC model as in Fig. 2, the included LM\ncomponent however is limited to a label prior without label context.\ngiven a sequence of the preceding tokens. We briefly review\nsuch LMs focusing on the different techniques to represent\neach token, ci, and back-history, c0:i\u22121.\n1) N-gram LM: N-gram LMs have long been used for\nASR [2]. Early E2E systems in [53], [74], [77] also employed\nan N-gram LM. The N-gram models rely on the Markov\nassumption that the probability distribution of the next token\ndepends only on the previous N\u22121 tokens, i.e., P(ci|c0:i\u22121) \u2248\nP(ci|ci\u2212N+1:i\u22121), where N is typically 3 to 5 for word-based\nmodels and higher for sub-word and character-based models.\nThe maximum likelihood estimates of N-gram probabilities\nare determined based on the counts of N sequential tokens in\nthe training data set as:\nP(ci|ci\u2212N+1:i\u22121) = K(ci\u2212N+1, . . . , ci)\nP\nci\nK(ci\u2212N+1, . . . , ci)\nwhere, K(\u00b7) denotes the count of each token sequence. Since\nthe data size is finite, it is important to apply a smoothing\ntechnique to avoid estimating the probabilities based on zero or\nvery small counts for rare token sequences. Those techniques\ncompensate the N-gram probabilities with lower order models,\ne.g., (N \u2212 1)-gram models, according to the magnitude of\nthe count [288]. However, since the N-gram probabilities\nstill rely on the discrete representation of each token and the\nhistory, they suffer from data sparsity problems, leading to\npoor generalization.\nThe advantage of the N-gram models is their simplicity,\nalthough they underperform state-of-the-art neural LMs. In the\ntraining, the main step is to just count the N tuples in the data\nset, which is required only once. During decoding, the LM\nprobabilities can be obtained very quickly by table lookup or\ncan be attached to a decoding graph, e.g., WFST, in advance.\n2) FNN-LM: The feed-forward neural network (FNN) LM\nwas proposed in [9], which estimates N-gram probabilities\nusing a neural network. The network accepts N \u2212 1 tokens,\nand predicts the next token as:\nP(ci|ci\u2212N+1:i\u22121) = softmax(Wohi + bo)\nhi = tanh(Whei + bh)\nei = concat(E(ci\u2212N+1), . . . , E(ci\u22121))\nwhere Wo and Wh are weight matrices, and bo and bh are\nbias vectors. E(y) provides an embedding vector of c, and\nconcat(\u00b7) operation concatenates given vectors 15. This model\nfirst maps each input token to an embedding space, and then\nobtains hidden vector, hi, as a context vector representing the\nprevious N \u22121 tokens. Finally, it outputs the probability distri\u0002bution of the next token through the softmax layer. Although\nthis LM still relies on the Markov assumption, it outperforms\nclassical N-gram LMs described in the previous section.\nThe superior performance of FNN-LM is primarily due to\nthe distributed representation of each token and the history.\nThe LM learns to represent token/context vectors such that\nsemantically similar tokens/histories are placed close to each\nother in the embedding space. Since this representation has a\nbetter smoothing effect than the count-based one used for N\u0002gram LMs, FNN-LM can provide a better generalization than\n15 We omit the optional direct connection from the embedding layer to the\nsoftmax layer in [9] for simplicity.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nN-gram LMs for predicting the next token. Neural network\u0002based LMs basically utilize this type of representation.\n3) RNN-LM: A recurrent neural network (RNN) LM was\nintroduced to exploit longer contextual information over N \u2212\n1 previous tokens using recurrent connections [289]. Unlike\nFNN-LM, the hidden vector is computed as:\nhi = recurrence(ei, hi\u22121)\nei = E(ci\u22121)\nwhere, recurrence(ei, hi\u22121) represents a recursive function,\nwhich accepts previous hidden vector hi\u22121 with input ei, and\noutputs next hidden vector hi. In the case of simple (Elman\u0002type) RNN, the function can be computed as\nrecurrence(e, h) = tanh(Whe + Wrh + bh)\nwhere, Wr is a weight matrix for the recurrent connection,\nwhich is applied to the previous hidden vector h. This recurrent\nloop makes it possible to hold the history information in the\nhidden vector without limiting the history to N \u2212 1 tokens.\nHowever, the history information decays exponentially as\ntokens are processed with this recursion. Therefore, currently\nstacked LSTM layers are more widely used for the recurrent\nnetwork, which have separate internal memory cells and gating\nmechanisms to keep long-range history information [290].\nWith this mechanism, RNN-LMs outperform other N-gram\u0002based models in many tasks.\n4) ConvLM: Convolutional neural networks (ConvLM)\nhave also been applied to LMs [291], [292], [293]. ConvLM\n[292] replace the recurrent connections used in RNN-LMs\nwith gated temporal convolutions. The hidden vector is com\u0002puted as\nhi =h\n\u2032\ni \u2297 \u03c3(gi)\nh\n\u2032\ni =ei\u2212k+1:i \u2217 W + b\ngi =ei\u2212k+1:i \u2217 V + c\nwhere \u2297 is element-wise multiplication, \u2217 is a temporal\nconvolution operation, and k is the patch size. \u03c3(gi) represents\na gating function of convoluted activation h\n\u2032\ni\n, and is modeled\nas a sigmoid function. W and V are matrices for convolution\nand b and c are bias vectors. The convolution and gating blocks\nare typically stacked multiple times with residual connections.\nIn [293], a ConvLM with 14 blocks has been applied for E2E\nASR. Similar to FNN-LM, ConvLM allow us to use only a\nfixed history size, but they are more parameter efficient and\neasier to utilize longer histories than the FNN-LM by stacking\nthe layers. Thus, they achieve competitive performance to that\nof RNN-LMs [292], even with the finite history consisting\nof short tokens such as characters [294]. Moreover, they are\nhighly parallelizable and thus suitable for training the model\nwith a large training data set.\n5) Transformer LM: Transformer architecture [44] has been\napplied to LMs [295] and used for ASR [102], [296], where\nthe LMs are designed as a Transformer decoder without any\ninputs from other modules such as encoders. The hidden vector\nis computed as:\nhi = FFN(h\n\u2032\ni\n) + h\n\u2032\ni\nh\n\u2032\ni = MHA(ei\n, e1:i, e1:i) + ei\nwhere FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network\nand a multi-head attention module, respectively. The multi\u0002head attention and feed-forward blocks are typically stacked\nmultiple times, e.g., 6 times [102], to obtain the final hidden\nvector. The advantage of Transformer LMs is that they can\ntake all tokens in the history into account through the self\u0002attention mechanism without summarizing them into a fixed\u0002size memory like RNN-LMs. Thus, the long history can be\nfully considered with attention to predict the next token,\nachieving better performance than RNN-LMs. However, the\ncomputational complexity increases quadratically as the length\nof the sequence. Therefore, the history length is typically\nlimited to a fixed size or within every single sentence. To\novercome this limitation, Transformer-XL [297] reuses already\ncomputed activations, which includes information on farther\nprevious tokens, and the model is trained with a truncated\nback-propagation through time (BPTT) algorithm [298]. Com\u0002pressive Transformer [299] extends this approach to utilize\neven longer contextual information by incorporating a com\u0002pression step to keep older, but important, information in a\nfixed-size memory network.\nB. Fusion Approaches\nThere are several ways to incorporate an external LM into\nE2E ASR, called LM fusion. Their purpose is to improve the\nrecognition accuracy of E2E ASR by leveraging the benefits\nof the external LM described in the first part of this section.\nHowever, there can be a mismatch in the prediction between\nthe E2E model and the LM when trained on different data\nsets, and therefore the LM may not collaborate well with\nthe E2E model. Researchers have investigated various LM\nfusion approaches to reduce the mismatch between models\nin different situations.\n1) Shallow Fusion: Shallow fusion is the most popular\napproach to combine the pretrained E2E model and LM in\nthe inference time. As we described in Sec. VI-F, shallow\nfusion simply combines the E2E and LM scores by a log\u0002linear combination as\nScore(C|X) = log P(C|X) + \u03b3 log P(C) (9)\nwhere \u03b3 is a scaling factor for the LM [255][256][257]. The\nadvantage of this approach is that it is easy and effective when\nthere are no major mismatches between the source and target\ndomains.\n2) Deep Fusion: Deep fusion [300] is an approach to\ncombine an LM with an E2E model using a joint network.\nGiven a pretrained E2E model and an LM, all the network\nparameters are fine-tuned jointly so that the models collaborate\nbetter to improve the recognition accuracy, where the joint\nnetwork is used to combine the E2E and LM states through\na gating mechanism that controls the contribution of the LM\naccording to the current state.\n3) Cold fusion: Cold fusion [301] is another approach to\ncombine a pretrained LM like deep fusion, but the E2E model\nis learned while freezing the LM parameters. Since the E2E\nmodel is aware of the LM throughout training, it learns to use\nthe LM to reduce language specific information and capture\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nonly the relevant information to map the source to the target\nsequence. This mechanism reduces the role of LM in the E2E\nmodel and alleviates the language bias of the training data.\nAccordingly, the E2E model becomes more robust to domain\nmismatches between the training data and the target domain.\nUnlike deep fusion, cold fusion makes it possible to combine\nthe E2E model with a pretrained LM for the target domain,\nimproving the recognition accuracy. Component fusion [302]\nextends cold fusion to use a pretrained LM with transcriptions\nof the training data for the E2E model, more focusing on\nreducing the bias of the training data.\n4) Internal LM Estimation: There is another approach to\nreduce language bias in training data through shallow fusion.\nThe language bias is a problem when a big domain mismatch\nexists between the source domain (training data) and the target\ndomain (test data) because the E2E model scores are strongly\ndependent on the language priors in the source domain. To\nremove such a bias from the score, we can explicitly estimate\nthe LM that represents the language priors, called Internal LM,\nand subtract the LM score from the ASR score of Eq. (9):\nScore(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C)\nwhere subscripts \u03c6 and \u03c4 indicate the source and target\ndomains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub\u0002tracting the internal LM score corresponds to approximating\nacoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d\nP\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR\nscore can be seen as a classical hybrid ASR system. Ac\u0002cordingly, the subtracted E2E model score plays a role of\nacoustic model and makes it more domain independent in\nterms of language, achieving a higher recognition accuracy\nin combination with the external LM P\u03c4 (C).\nThe density ratio method [303] trains an internal LM using\nthe transcript of the training data. Hybrid autoregressive trans\u0002ducer (HAT) [47] extends RNN-T so that the model becomes\nthe internal LM when the encoder output is eliminated, i.e., set\nto zero. This approach simplifies the framework by utilizing\nthe prediction network as the internal LM, which avoids\ntraining an additional LM and using it in the inference time.\nIn the work of [304], an approach similar to HAT has been\nproposed where the internal LM is formulated on top of\nstandard RNN-T and attention-based encoder-decoder models,\nrespectively. In [128], several techniques to estimate internal\nLMs have been proposed for AED models, where an estimated\nbias vector is fed to the LM instead of a zero vector. The bias\nvector can be estimated by averaging encoder states or context\nvectors, or by a small LSTM predicting the context vector\nbased on the decoder label context, only. These techniques to\nestimate the internal LM were also evaluated for RNN-T in\n[305].\nC. Use of Large-scale Pretrained LMs\nIn recent years, LMs trained with large-scale text data are\navailable for different NLP tasks. BERT [306] and GPT-2\n[307] are representative models based on Transformer LMs.\nSuch LMs have also been applied to E2E ASR systems in\ndifferent ways, e.g., N-best rescoring [308] and dialog context\nembedding [309].\nRelationship to Classical ASR\nThe architecture of classical ASR systems provides a sepa\u0002ration between the acoustic model and the language model.\nIn contrast to this, E2E models avoid this separation and\ndefine a joint model. While this allows for training with a\nsingle objective, it limits training of the (implicit) prior to\nthe transcriptions of the audio training data. To exploit further\ntext-only training data, usually a separate LM is combined with\nE2E models, nonetheless. However, due to the implicit prior\nof E2E models, i.e. the internal language model, combination\nwith separate language models is not straightforward and\nrequires corresponding internal language model estimation\nand compensation approaches, e.g. [303], [47], [304], [128],\n[310]. At least from the recognition accuracy perspective, it\nremains unclear, if the clear separation of acoustic modeling\nand language modeling in the classical ASR architecture is a\ndisadvantage because of separate training objectives, or rather\nan advantage, since text-only training data may be used easily.\nAlso, the language model training objective, i.e. language\nmodel perplexity, is observed to correlate well with word error\nrate [311], [312], [313], [314]. Furthermore, discriminative\napproaches to language modeling [315] may be viewed as a\nstep towards joint modeling.\nVIII. OVERALL PERFORMANCE TRENDS OF E2E\nAPPROACHES IN COMMON BENCHMARKS\nThis section summarizes various techniques with the com\u0002mon ASR benchmarks based on switchboard (SWBD) [316]\nin Figure 9 and Librispeech [317] in Figure 10 to see the\ntrajectory of the techniques developed in end-to-end ASR. We\nchoose these two databases because they are widely used in\nspeech and machine learning communities and cover sponta\u0002neous (SWBD) and read speech (Librispeech) speaking styles.\nFigures 9 and 10 show that the performance improvement\nrelative to the initial works [147], [79] based on the E2E\nmodels is significant, and the error rates of all tasks become\nless than half of the original error rates!16\nAlthough the overall trends show that the ASR performance\nhas steadily improved over time, there are several remarkable\ngains. One significant gain observed in both benchmarks in the\nmiddle of 2019 comes from the data augmentation method\nrepresented by SpecAugment [205], [206], as discussed in\nSection V-G. The subsequent gains mostly come from the\nexploration of the new neural network architectures, including\ntransformer [102], [318], conformer [45], [103], and contextnet\n[97] on top of SpecAugment, as discussed in Section IV-C.\nSuch an exploration is also performed in language modeling\nto improve the ASR performance [296], [102]. The final gain\nobserved in the Librispeech benchmark in 2021 is based\non self-supervised learning [25], [319] and semi-supervised\nlearning [320], [321]. These techniques utilize a considerable\n16 For readers who want to know the latest update of these\nbenchmarks can also check https://github.com/syhw/wer are we and\nhttps://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\nAEDAEDAEDAED\nHMM\nAEDAEDAEDAED AEDAED\nWER (%)\n0\n10\n20\n30\n40\n50\n1/1/2017 1/1/2018 1/1/2019 1/1/2020 1/1/2021\nswb chm\nFig. 9. E2E ASR performance improvement in the switchboard task. AED AED\nAEDAEDAED\ntransducer ContextNet Transducer\nCTC\nHMM\nAED transducer\nWER (%)\ntransformer\n0\n5\n10\n15\n1/1/2019 7/1/2019 1/1/2020 7/1/2020 1/1/2021 7/1/2021\ntest_clean test_other\nFig. 10. E2E ASR performance improvement in the Librispeech task.\namount of unlabeled in-domain speech data (e.g., Libri-light\n60K hours [322]).\nRelationship to Classical ASR\nSpeech recognition research has always been pushed by\ninternational evaluation campaigns (e.g. as lead by NIST)\nand corresponding benchmark tasks. The competition between\nclassical and E2E approaches is nicely reflected in the widely\nused Librispeech [317] and Switchboard [316] tasks, showing\nthat E2E models gain momentum. As shown in Figure 10,\non Librispeech, the current best-published classical hybrid\nsystems range around 2.3% (test-clean) and 4.9% (test-other)\nword error rate [323], [222], while there already are a number\nof E2E systems providing similar performance [224], [205],\n[320], [206], with some E2E systems clearly outperforming\nformer state-of-the-art results with word error rates down to\n1.8% (test-clean) and 3.7% (test-other) [324] with similar\nresults reported in [45], [97]. Merging insights from classical\nHMM-based and monotonic RNN-T provided similarly well\nresults with a limited training budget [124]. Finally, when\ntrained on Switchboard 300h, the current best result, obtained\nwith an E2E system [180] is 5.4% compared to 6.6% word\nerror rate for the best hybrid system result [325] on the\nHUB5\u201900 Switchboard test set, in Figure 9\nIX. DEPLOYMENT OF E2E MODELS\nMany of the ideas discussed in this paper have been\nexplored by various industry research labs [326], [327], [328],\n[329], [330], [331], [265], inter alia. In this section, we\nreview the development of on-device production-level systems\nat Google as a typical case study for deployment.\nThe first streaming E2E model, deployed to production,\nwas launched in 2019 for the Pixel 4 smartphone [22], [332].\nThis model used a streaming RNN-T first-pass system, while\nre-scoring first-pass hypotheses with an AED system in the\nsecond pass. In addition, FST-based contextual biasing [92]\nwas employed in the model, which was critical to obtain\naccurate results for diverse queries. This model ran on CPU\nand was much faster than real time.\nIn 2020, for the Pixel 5 smartphone [333], the system was\nimproved further to reduce user-perceived latency (i.e., the\ntime between when the user speaks, and when words appear\non the device). This included advancements such as end-to-end\nendpointing [113] to encourage faster microphone closing; as\nwell as FastEmit [91] to encourage the model to emit tokens\nearlier.\nFinally, in 2021 the model was further improved for the\nPixel 6 smartphone [334], to take advantage of the tensor\nprocessing unit (TPU) [85] on the device. Improvements\ninclude the use of conformer layers for the encoder [45]; a\nsmall embedding prediction network for the decoder [104]; a\n2-pass cascaded encoder to run a 2nd-pass beam search [89];\nand, a neural LM re-scorer to help improve accuracy long-tail\nnamed entities. This model is the best ASR system that Google\nhas released to date, both in terms of quality and latency.\nX. AREAS FOR FUTURE WORK\nCurrently, E2E models dominate the academic debate on\nASR. However, at least partly, this is not (yet?) reflected\nin the corresponding commercial deployment of E2E ASR\narchitectures. E2E models are not yet the perfect match for\nall ASR conditions and further research is needed to take full\nadvantage of the benefits of E2E modeling.\nE2E models seem to perform really well when training data\nis abundant, while not scaling well to low-resource conditions.\nSimilarly, domain change requires a flexible exchange of lan\u0002guage models, which is natural for classical ASR models based\non a separation of acoustic and language models. Ongoing\nresearch on the use of external language models in E2E models\nand internal language model estimation already is promising,\nbut can be expected to see further improvements.\nTop E2E ASR systems usually require orders of magnitude\nmore training epochs than comparable classical ASR systems,\nand further research into efficient and robust optimization and\ntraining schedules is needed.\nThe high level of integration of E2E models also involves a\nloss in modularity, which might support the explainability and\nreusability of models. Also, more efficient training schedules\nmight take advantage of modularity. One assumed advantage\nof E2E models is that everything is trained from data and\nsecondary knowledge sources (e.g. pronunciation lexica and\nphoneme sets) are avoided. However, rare events, like rare\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\nwords in ASR still provide a challenge, which needs further\nresearch.\nWith the missing separation of acoustic and language mod\u0002els, the question arises of how to exploit text-only resources in\nE2E model training - do we foresee solutions beyond training\ndata generation using TTS? We note that a number of recent\nworks have explored approaches to combine speech and text\nmodalities by attempting to implicitly or explicitly map them\ninto a shared space [159], [335], [336], [337], [338], [339],\n[340], [341]. Furthermore, high-performance E2E solutions\nexist for both discriminative problems like ASR, as well as\ngenerative problems like TTS, how can both be exploited\njointly to support semi-supervised training based on text-only\nand/or audio-only data on top of transcribed speech audio [28],\n[342]?\nFor AED architectures, we observe a length bias, which\ncomplicates the decoding process. Although many heuristics\nare known to tackle length bias in AED, we are still missing\na well-founded explanation for it, as well as a corresponding\nremedy of the original model.\nOther open research problems include speaker adaptation\nand robustness to recording conditions, especially in mismatch\nsituations. The E2E principle also provides a promising candi\u0002date to solve multichannel ASR by providing an E2E solution\njointly tackling the source separation, speaker diarization and\nspeech recognition problem [343], [26].\nFinally, we need to investigate, if E2E is a suitable guiding\nprinciple, and how different E2E ASR models relate to each\nother as well as to classical ASR approaches. The most\nimportant guiding principle of ASR research and development\nhas been performance, and ASR has been boosted strongly\nby widely used benchmark tasks and international evaluation\ncampaigns. With the current diversity of classical and E2E\nmodels, we also need to resolve the question of what con\u0002stitutes state-of-the-art in ASR today, and can we expect a\ncommon state-of-the-art ASR architecture in the future?\nXI. CONCLUSIONS\nIn this work, we presented a detailed overview of end-to\u0002end approaches to ASR. Such models, which have grown in\npopularity over the last few years, propose to use highly inte\u0002grated neural network components which allow input speech\nto be converted directly into output text sequences through\ncharacter-based output units. Thus, such models eschew the\nclassical modular ASR architecture consisting of an acoustic\nmodel, a pronunciation model, and a language model, in\nfavor of a single compact structure, and rely on the data to\nlearn effectively. These design choices enable the deployment\nof highly accurate on-device speech recognition models (see\nSection IX), but also come with a number of downsides which\nare still areas of active research (see Section X).\nFinally, we direct interested readers to Li\u2019s excellent\ncontemporaneous overview article on end-to-end ASR [344],\nwhich offers a complementary perspective to our own. In\nparticular, readers of [344] may find a more detailed exposition\non the choice of encoder structure, and the applications of\nE2E approaches to allied ASR areas (e.g., multi-speaker\nrecognition; multilingual ASR; adaptation to new application\ndomains, and speakers; etc.), which we do not cover due to\nspace limitations.\nACKNOWLEDGMENT\nThe authors would like to thank Julian Dierkes, Yifan Peng,\nZoltan T \u00b4 uske, Albert Zeyer, and Wei Zhou for their help on \u00a8\nrefining our manuscript.\nREFERENCES\n[1] T. Bayes, \u201cAn Essay Towards Solving a Problem in the Doctrine of\nChances,\u201d Philosophical Transactions of the Royal Society of London,\nvol. 53, pp. 370\u2013418, 1763.\n[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,\nMA: MIT Press, 1997.\n[3] L. R. Rabiner, \u201cA Tutorial on Hidden Markov Models and Selected\nApplications in Speech Recognition,\u201d Proc. of the IEEE, vol. 77, no. 2,\npp. 257\u2013286, Feb. 1989.\n[4] H. A. Bourlard and N. Morgan, Connectionist Speech Recognition: a\nHybrid Approach. Norwell, MA: Kluwer Academic Publishers, 1993.\n[5] F. Seide, G. Li, and D. Yu, \u201cConversational Speech Transcription Using\nContext-Dependent Deep Neural Networks,\u201d in Proc. Interspeech,\nFlorence, Italy, Aug. 2011, pp. 437\u2013440.\n[6] V. Fontaine, C. Ris, and H. Leich, \u201cNonlinear Discriminant Analysis for\nImproved Speech Recognition,\u201d in Proc. Eurospeech, Rhodes, Greece,\nSep. 1997, pp. 1\u20134.\n[7] H. Hermansky, D. Ellis, and S. Sharma, \u201cTandem connectionist Feature\nExtraction for Conventional HMM Systems,\u201d in Proc. IEEE ICASSP,\nvol. 3, Istanbul, Turkey, Jun. 2000, pp. 1635\u20131638.\n[8] M. Nakamura and K. Shikano, \u201cA Study of English Word Category\nPrediction Based on Neural Networks,\u201d in Proc. IEEE ICASSP, Glas\u0002glow, UK, May 1989, pp. 731\u2013734.\n[9] Y. Bengio, R. Ducharme, and P. Vincent, \u201cA Neural Probabilistic\nLanguage Model,\u201d in Proc. NIPS, vol. 13, Denver, CO, Nov. 2000,\npp. 932\u2013938.\n[10] H. Schwenk and J.-L. Gauvain, \u201cConnectionist Language Modeling\nfor Large Vocabulary Continuous Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 765\u2013768.\n[11] Z. Tuske, P. Golik, R. Schl \u00a8 uter, and H. Ney, \u201cAcoustic Modeling with \u00a8\nDeep Neural Networks Using Raw Time Signal for LVCSR,\u201d in Proc.\nInterspeech, Singapore, Sep. 2014, pp. 890\u2013894.\n[12] T. N. Sainath, R. J. Weiss, K. W. Wilson, A. Narayanan, M. Bacchiani,\nand A. Senior, \u201cSpeaker Location and Microphone Spacing Invariant\nAcoustic Modeling from Raw Multichannel Waveforms,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 30\u201336.\n[13] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \u201cConnection- \u00b4\nist temporal classification: labelling unsegmented sequence data with\nrecurrent neural networks,\u201d in Proc. ICML, Pittsburgh, PA, Jun. 2006,\npp. 369\u2013376.\n[14] A. Graves, \u201cSequence Transduction with Recurrent Neural Networks,\u201d\nin Proc. ICML, Edinburgh, Scotland, Jun. 2012, Workshop on Repre\u0002sentation Learning, arXiv:1211.3711.\n[15] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\n\u201cAttention-Based Models for Speech Recognition,\u201d in Proc. NIPS,\nvol. 28, Laval, Queebec, Canada, Dec. 2015, pp. 577\u2013585. `\n[16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n4960\u20134964.\n[17] P. Liang, A. Bouchard-Cot\u02c6 e, D. Klein, and B. Taskar, \u201cAn End-to- \u00b4\nEnd Discriminative Approach to Machine Translation,\u201d in Proc. ACL,\nSydney, Australia, Jul. 2006, p. 761\u2013768.\n[18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, \u201cNatural Language Processing (Almost) from Scratch,\u201d\nJournal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.\n[19] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recognition\nwith Recurrent Neural Networks,\u201d in Proc. ICML, Beijing, China, Jun.\n2014, pp. 1764\u20131772.\n[20] \u201cCambridge Dictionary,\u201d https://dictionary.cambridge.org/dictionary/\nenglish/end-to-end, accessed: 2020-02-21.\n[21] R. Pang, T. N. Sainath, R. Prabhavalkar, S. Gupta, Y. Wu, S. Zhang, and\nC.-c. Chiu, \u201cCompression of End-to-End Models,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 27\u201331.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[22] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,\nD. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shang\u0002guan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-y. Chang, K. Rao, and\nA. Gruenstein, \u201cStreaming End-to-End Speech Recognition for Mobile\nDevices,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6381\u2013\n6385.\n[23] R. Schluter and H. Ney, \u201cModel-based MCE Bound to the True Bayes\u2019 \u00a8\nError,\u201d IEEE Signal Processing Letters, vol. 8, no. 5, pp. 131\u2013133, May\n2001.\n[24] H. Ney, \u201cOn the Relationship between Classification Error Bounds\nand Training Criteria in Statistical Pattern Recognition,\u201d in Iberian\nConference on Pattern Recognition and Image Analysis (IbPRIA),\nPuerto de Andratx, Spain, Jun. 2003, pp. 636\u2013645.\n[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A\nFramework for Self-Supervised Learning of Speech Representations,\u201d\nin Proc. NeurIPS, Vancouver, BC, Canada, Dec. 2020, pp. 12 449\u2013\n12 460.\n[26] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, \u201cMIMO\u0002Speech: End-to-End Multi-Channel Multi-Speaker Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU. Sentosa, Singapore: IEEE, Dec. 2019,\npp. 237\u2013244.\n[27] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and\nRegression Trees. Belmont, CA: Taylor & Francis, 1984.\n[28] A. Tjandra, S. Sakti, and S. Nakamura, \u201cListening While Speaking:\nSpeech Chain by Deep Learning,\u201d in Proc. IEEE ASRU. Okinawa,\nJapan: IEEE, Dec. 2017, pp. 301\u2013308.\n[29] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and\nJ. Cernock \u02c7 y, \u201cSemi-Supervised Sequence-to-Sequence ASR Using \u00b4\nUnpaired Speech and Text,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3790\u20133794, arXiv:1905.01152.\n[30] H. Soltau, H. Liao, and H. Sak, \u201cNeural Speech Recognizer: Acoustic\u0002to-Word LSTM Model for Large Vocabulary Speech Recognition,\u201d in\nProc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1610.09975.\n[31] G. K. Zipf, Human Behavior and the Principle of Least Effort. Boston,\nMA: Addison-Wesley Press, 1949.\n[32] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural Machine Translation\nof Rare Words with Subword Units,\u201d in Proc. ACL, Berlin, Germany,\nAug. 2015, pp. 1715\u20131725.\n[33] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, \u201cLatent Sequence Decompo\u0002sitions,\u201d in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1610.03035.\n[34] H. Liu, Z. Zhu, X. Li, and S. Satheesh, \u201cGram-CTC: Automatic unit\nselection and target decomposition for sequence labelling,\u201d in Proc.\nICML, ser. Proceedings of Machine Learning Research, D. Precup\nand Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 2188\u20132197,\narXiv:1703.00096.\n[35] H. Xu, S. Ding, and S. Watanabe, \u201cImproving End-to-End Speech\nRecognition with Pronunciation-Assisted Sub-Word Modeling,\u201d in\nProc. IEEE ICASSP, Brighton, UK, Sep. 2019, pp. 7110\u20137114.\n[36] W. Zhou, M. Zeineldeen, Z. Zheng, R. Schluter, and H. Ney, \u201cAcoustic \u00a8\nData-Driven Subword Modeling for End-to-End Speech Recognition,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2886\u20132890.\n[37] M. Schuster and K. Nakajima, \u201cJapanese and Korean Voice Search,\u201d\nin Proc. IEEE ICASSP, Kyoto, Japan, Mar. 2012, pp. 5149\u20135152.\n[38] M. Mohri, F. Pereira, and M. Riley, \u201cWeighted Finite-State Transducers\nin Speech Recognition,\u201d Computer Speech & Language, vol. 16, no. 1,\npp. 69\u201388, 2002.\n[39] E. Beck, M. Hannemann, P. Doetsch, R. Schluter, and H. Ney, \u00a8\n\u201cSegmental Encoder-Decoder Models for Large Vocabulary Automatic\nSpeech Recognition,\u201d in Proc. Interspeech, Hyderabad, India, Sep.\n2018.\n[40] W. Zhou, A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cEquivalence \u00a8\nof Segmental and Neural Transducer Modeling: A Proof of Concept,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2891\u20132895.\n[41] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, \u201cA Comparison of Sequence-to-Sequence Models for Speech\nRecognition,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp.\n939\u2013943.\n[42] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural\nComputation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[43] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by\nJointly Learning to Align and Translate,\u201d in Proc. ICLR, San Diego,\nCA, May 2015, arXiv:1409.0473.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is All You Need,\u201d\nin Proc. NIPS, Los Angeles, CA, Dec. 2017, pp. 5998\u20136008.\n[45] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution\u0002Augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 5036\u20135040.\n[46] H. Sak, M. Shannon, K. Rao, and F. Beaufays, \u201cRecurrent Neural\nAligner: An Encoder-Decoder Neural Network Model for Sequence to\nSequence Mapping,\u201d in Proc. Interspeech, vol. 8, Stockhol, Sweden,\nAug. 2017, pp. 1298\u20131302.\n[47] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid Autore\u0002gressive Transducer (HAT),\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6139\u20136143.\n[48] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech Recognition with\nDeep Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP, Vancouver,\nBC, Canada, May 2013, pp. 6645\u20136649.\n[49] N. Moritz, T. Hori, S. Watanabe, and J. Le Roux, \u201cSequence Trans\u0002duction with Graph-Based Supervision,\u201d in Proc. IEEE ICASSP, Sin\u0002gapore, May 2022, pp. 7212\u20137216.\n[50] Y. Bengio, N. Leonard, and A. Courville, \u201cEstimating or Propagating \u00b4\nGradients through Stochastic Neurons for Conditional Computation,\u201d\nAug. 2013, arXiv:1308.3432.\n[51] A. Tripathi, H. Lu, H. Sak, and H. Soltau, \u201cMonotonic Recurrent\nNeural Network Transducer and Decoding Strategies,\u201d in Proc. IEEE\nASRU, Sentosa, Singapore, Dec. 2019, pp. 944\u2013948.\n[52] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA New Training \u00a8\nPipeline for an Improved Neural Transducer,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 2812\u20132816.\n[53] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-Based Large Vocabulary Speech Recognition,\u201d\nin Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4945\u20134949.\n[54] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, \u0141. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa,\nK. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean,\n\u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap Be\u0002tween Human and Machine Translation,\u201d Oct. 2016, arXiv:1609.08144.\n[55] M. Mimura, S. Sakai, and T. Kawahara, \u201cForward-Backward Attention\nDecoder,\u201d in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2232\u2013\n2236.\n[56] A. Graves, \u201cGenerating Sequences with Recurrent Neural Networks,\u201d\nAug. 2013, arXiv:1308.0850.\n[57] J. Hou, S. Zhang, and L.-R. Dai, \u201cGaussian Prediction Based At\u0002tention for Online End-to-End Speech Recognition,\u201d in Proc. In\u0002terspeech, Stockholm, Sweden, Aug. 2017, pp. 3692\u20133696, DOI:\n10.21437/Interspeech.2017-751.\n[58] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen,\nA. Narayanan, H. Liao, S. Zhang, A. Kannan, R. Prabhavalkar, Z. Chen,\nT. Sainath, and Y. Wu, \u201cA Comparison of End-to-End Models for Long\u0002Form Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 889\u2013896.\n[59] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,\n\u201cAn Online Sequence-to-Sequence Model Using Partial Conditioning,\u201d\nin Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 5067\u20135075.\n[60] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, \u201cOnline and\nLinear-Time Attention by Enforcing Monotonic Alignments,\u201d in Proc.\nICML, Sydney, Australia, Aug. 2017, pp. 2837\u20132846.\n[61] C.-C. Chiu and C. Raffel, \u201cMonotonic Chunkwise Attention,\u201d in Proc.\nICLR, Vancouver, Canada, Apr. 2018, arXiv:1712.05382.\n[62] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz,\nR. Pang, W. Li, and C. Raffel, \u201cMonotonic Infinite Lookback Attention\nfor Simultaneous Machine Translation,\u201d in Proc. ACL, Florence, Italy,\nJun. 2019, pp. 1313\u20131323.\n[63] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu,\nP. Nguyen, and Z. Chen, \u201cImproving the Performance of Online Neural\nTransducer Models,\u201d in Proc. IEEE ICASSP, Calgary, Alberta, Canada,\nApr. 2018, pp. 5864\u20135868.\n[64] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Brighton, England,\nMay 2019, pp. 5666\u20135670.\n[65] A. Merboldt, A. Zeyer, R. Schluter, and H. Ney, \u201cAn Analysis of Local \u00a8\nMonotonic Attention Variants,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 1398\u20131402.\n[66] A. Zeyer, R. Schluter, and H. Ney, \u201cA Study of Latent Monotonic \u00a8\nAttention Variants,\u201d Mar. 2021, arXiv:2103.16710.\n[67] A. Zeyer, R. Schmitt, W. Zhou, R. Schluter, and H. Ney, \u201cMonotonic \u00a8\nSegmental Attention for Automatic Speech Recognition,\u201d in Proc. IEEE\nSLT, Doha, Qatar, Jan. 2023, arXiv:2210.14742.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[68] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, \u201cSynchronous\nTransformers for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, arXiv:1912.02958.\n[69] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely Sequence-Trained Neural\nNetworks for ASR Based on Lattice-Free MMI,\u201d in Proc. Inter\u0002speech. San Francisco, CA: ISCA, Sep. 2016, pp. 2751\u20132755, DOI:\n10.21437/Interspeech.2016-595.\n[70] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: An End\u0002to-End Convnet-Based Speech Recognition System,\u201d Sep. 2016,\narXiv:1609.03193.\n[71] P. Haffner, \u201cConnectionist Speech Recognition with a Global MMI\nAlgorithm,\u201d in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp.\n1929\u20131932.\n[72] A. Zeyer, E. Beck, R. Schluter, and H. Ney, \u201cCTC in the Context of \u00a8\nGeneralized Full-Sum HMM Training,\u201d in Proc. Interspeech, Stock\u0002holm, Sweden, Aug. 2017, pp. 944\u2013948.\n[73] T. Raissi, W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cHMM vs. \u00a8\nCTC for Automatic Speech Recognition: Comparison Based on Full\u0002Sum Training from Scratch,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan.\n2023, arXiv:2210.09951.\n[74] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-End Speech\nRecognition Using Deep RNN Models and WFST-Based Decoding,\u201d\nin Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 167\u2013174.\n[75] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng,\n\u201cDeep Speech: Scaling up End-to-End Speech Recognition,\u201d Dec.\n2014, arXiv:1412.5567.\n[76] L. Lu, X. Zhang, and S. Renals, \u201cOn Training the Recurrent Neural\nNetwork Encoder-Decoder for Large Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n5060\u20135064.\n[77] J. Chorowski and N. Jaitly, \u201cTowards Better Decoding and Language\nModel Integration in Sequence to Sequence Models,\u201d in Proc. Inter\u0002speech, Stockhol, Sweden, Aug. 2017, pp. 523\u2013527.\n[78] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery Deep Convolutional Networks\nfor End-to-End Speech Recognition,\u201d in Proc. IEEE ICASSP, New\nOrleans, LA, Mar. 2017, pp. 4845\u20134849.\n[79] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learning\nwith Low-Level Auxiliary Tasks for Encoder-Decoder based Speech\nRecognition,\u201d in Proc. Interspeech, Stockholm, Sweden, Aug. 2017,\narXiv:1704.01631.\n[80] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti\u0002Modal Data Augmentation for End-to-End ASR,\u201d in Proc. Interspeech,\nHyderabad, India, Mar. 2018, pp. 2394\u20132398.\n[81] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Distillation\nfor Sequence Learning,\u201d in Proc. ICLR, New Orleans, LA, May 2019,\narXiv:1810.01398.\n[82] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu,\n\u201cImproving Attention Based Sequence-to-Sequence Models for End-to\u0002End English Conversational Speech Recognition,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 761\u2013765.\n[83] D. Le, X. Zhang, W. Zheng, C. Fugen, G. Zweig, and M. L. Seltzer, \u00a8\n\u201cFrom Senones to Chenones: Tied Context-Dependent Graphemes for\nHybrid Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 457\u2013464.\n[84] S. Kanthak and H. Ney, \u201cContext-Dependent Acoustic Modeling Using\nGraphemes for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 845\u2013848.\n[85] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., \u201cIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit,\u201d in Proc. of\nthe 44th Annual International Symposium on Computer Architecture,\nToronto, Ontario, Canada, Jun. 2017, pp. 1\u201312.\n[86] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, \u201cHybrid\nCTC Attention Architecture for End-to-End Speech Recognition,\u201d\nIEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8,\npp. 1240\u20131253, 2017.\n[87] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C. Chung\u0002Cheng, \u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech, Brighton, UK, May 2019, pp. 2773\u20132777.\n[88] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, \u201cDeliberation\nModel Based Two-Pass End-to-End Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 7799\u20137803.\n[89] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab\u0002havalkar, E. Variani, and T. Strohman, \u201cCascaded Encoders for Uni\u0002fying Streaming and Non-Streaming ASR,\u201d in Proc. IEEE ICASSP,\nToronto, Ontario, Canada, Jun. 2021, pp. 5629\u20135633.\n[90] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, \u201cTransformer Trans\u0002ducer: One Model Unifying Streaming and Non-Streaming Speech\nRecognition,\u201d Oct. 2020, arXiv:2010.03192.\n[91] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu, and\nR. Pang, \u201cUniversal ASR: Unify and Improve Streaming ASR with\nFull-Context Modeling,\u201d Oct. 2020, arXiv:2010.06030.\n[92] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, and\nR. Pang, \u201cShallow-Fusion End-to-End Contextual Biasing,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 1418\u20131422.\n[93] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao,\n\u201cDeep Context: End-to-end Contextual Speech Recognition,\u201d in Proc.\nIEEE SLT, Athens, Greece, Dec. 2018, pp. 418\u2013425.\n[94] S. Kim and F. Metze, \u201cDialog-Context Aware End-to-End Speech\nRecognition,\u201d in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 434\u2013\n440.\n[95] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, \u201cPhoebe:\nPronunciation-Aware Contextualization for End-to-End Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6171\u2013\n6175.\n[96] M. Delcroix, S. Watanabe, A. Ogawa, S. Karita, and T. Nakatani,\n\u201cAuxiliary Feature Based Adaptation of End-to-End ASR Systems,\u201d\nin Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2444\u20132448.\n[97] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextNet: Improving Convolutional Neural\nNetworks for Automatic Speech Recognition with Global Context,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 3610\u20133614.\n[98] L. Dong, S. Xu, and B. Xu, \u201cSpeech-Transformer: A No-Recurrence\nSequence-to-Sequence Model for Speech Recognition,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5884\u20135888.\n[99] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer Transducer: A Streamable Speech Recognition\nModel with Transformer Encoders and RNN-T Loss,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7829\u20137833.\n[100] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain,\nK. Schubert, C. Fuegen, and M. L. Seltzer, \u201cTransformer-Transducer:\nEnd-to-Snd Speech Recognition with Self-Attention,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 7829\u20137833.\n[101] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, \u201cBranchformer: Parallel\nMLP-Attention Architectures to Capture Local and Global Context for\nSpeech Recognition and Understanding,\u201d in Proc. ICML. Baltimore,\nMD: PMLR, Jul. 2022, pp. 17 627\u201317 643.\n[102] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang, \u201cA Comparative Study on Transformer\nvs RNN in Speech Applications,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 449\u2013456.\n[103] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,\nN. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei,\nW. Zhang, and Y. Zhang, \u201cRecent Developments on ESPNET Toolkit\nBoosted by Conformer,\u201d in Proc. IEEE ICASSP. Toronto, Ontario,\nCanada: IEEE, Jun. 2021, pp. 5874\u20135878.\n[104] R. Botros, T. Sainath, R. David, E. Guzman, W. Li, and Y. He, \u201cTied &\nReduced RNN-T Decoder,\u201d in Proc. Interspeech, Brno, Czechia, Sep.\n2021, pp. 4563\u20134567.\n[105] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, \u201cRNN\u0002Transducer with Stateless Prediction Network,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7049\u20137053.\n[106] W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cPhoneme Based Neural \u00a8\nTransducer for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5644\u20135648.\n[107] R. Prabhavalkar, Y. He, D. Rybach, S. Campbell, A. Narayanan,\nT. Strohman, and T. N. Sainath, \u201cLess is More: Improved RNN-T\nDecoding Using Limited Label Context and Path Merging,\u201d in Proc.\nIEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5659\u20135663.\n[108] X. Chen, Z. Meng, S. Parthasarathy, and J. Li, \u201cFactorized Neural\nTransducer for Efficient Language Model Adaptation,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8132\u20138136, arXiv:2110.01500.\n[109] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi,\nJ. Emond, T. Strohman, B. Ramabhadran, W. R. Huang et al., \u201cModular\nHybrid Autoregressive Transducer,\u201d in Proc. IEEE SLT, Doha, Qatar,\nJan. 2023, pp. 197\u2013204, https://arXiv:2210.17049.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[110] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and\nF. Wei, \u201cVioLA: Unified Codec Language Models for Speech Recog\u0002nition, Synthesis, and Translation,\u201d May 2023, arXiv:2305.16107.\n[111] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor\u0002sos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov\net al., \u201cAudioPaLM: A Large Language Model That Can Speak and\nListen,\u201d Jun. 2023, arXiv:2306.12925.\n[112] S.-Y. Chang, B. Li, and G. Simko, \u201cA Unified Endpointer Using\nMultitask and Multidomain Training,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 100\u2013106.\n[113] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and\nY. Wu, \u201cTowards Fast and Accurate Streaming End-To-End ASR,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6069\u20136073.\n[114] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, \u201cEnd-To\u0002End Automatic Speech Recognition Integrated with CTC-Based Voice\nActivity Detection,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6999\u20137003.\n[115] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, \u201cToward Stream\u0002ing ASR with Non-Autoregressive Insertion-Based Model,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3740\u20133744.\n[116] Y. Bengio, \u201cPractical Recommendations for Gradient-Based Training\nof Deep Architectures,\u201d Jun. 2012, arXiv:1206.5533.\n[117] J. Schmidhuber, \u201cDeep Learning in Neural Networks: An Overview,\u201d\nNeural Networks, vol. 61, pp. 85\u2013117, Jan. 2015, arXiv:1404.7828.\n[118] L. Baum, \u201cAn Inequality and Associated Maximization Technique in\nStatistical Estimation for Probabilistic Functions of Markov Processes,\u201d\nInequalities, vol. 3, pp. 1\u20138, 1972.\n[119] L. Rabiner and B.-H. Juang, \u201cAn Introduction to Hidden Markov Mod\u0002els,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing,\nvol. 3, no. 1, pp. 4\u201316, 1986.\n[120] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, \u201cNeural Network\u0002Gaussian Mixture Hybrid for Speech Recognition or Density Estima\u0002tion,\u201d in Proc. NIPS, vol. 4, Colorado, Dec. 1991, pp. 175\u2013182.\n[121] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton\nUniversity Press, 1957.\n[122] A. Viterbi, \u201cError Bounds for Convolutional Codes and an Asymptoti\u0002cally Optimal Decoding Algorithm,\u201d IEEE Transactions on Information\nTheory, vol. 13, pp. 260\u2013269, 1967.\n[123] H. Ney, \u201cThe Use of a One-Stage Dynamic Programming Algorithm\nfor Connected Word Recognition,\u201d IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, vol. 32, no. 2, pp. 263\u2013271, 1984.\n[124] W. Zhou, W. Michel, R. Schluter, and H. Ney, \u201cEfficient Training \u00a8\nof Neural Transducer for Speech Recognition,\u201d in Proc. Interspeech,\nIncheon, Korea, Sep. 2022, arXiv:2204.10586.\n[125] A. Zeyer, R. Schluter, and H. Ney, \u201cWhy does CTC Result in Peaky \u00a8\nBehavior?\u201d May 2021, arXiv:2105.14849.\n[126] A. Laptev, S. Majumdar, and B. Ginsburg, \u201cCTC Variations Through\nNew WFST Topologies,\u201d in Proc. Interspeech, Incheon, Korea, sep\n2022, DOI: 10.21437/interspeech.2022-10854.\n[127] X. He, L. Deng, and W. Chou, \u201cDiscriminative Learning in Sequential\nPattern Recognition \u2013 A Unifying Review for Optimization-Oriented\nSpeech Recognition,\u201d IEEE Signal Processing Magazine, vol. 25, no. 5,\npp. 14\u201336, 2008.\n[128] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schluter, and \u00a8\nH. Ney, \u201cInvestigating Methods to Improve Language Model Inte\u0002gration for Attention-Based Encoder-Decoder ASR Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Aug. 2021, pp. 2856\u20132860.\n[129] N.-P. Wynands, W. Michel, J. Rosendahl, R. Schluter, and H. Ney, \u00a8\n\u201cEfficient Sequence Training of Attention Models using Approxima\u0002tive Recombination,\u201d in Proc. IEEE ICASSP, Singapore, May 2022,\narXiv:2110.09245.\n[130] Z. Yang, W. Zhou, R. Schluter, and H. Ney, \u201cLattice-Free Sequence \u00a8\nDiscriminative Training for Phoneme-based Neural Transducers,\u201d in\nProc. IEEE ICASSP, Rhodes, Greece, Jun. 2023, arXiv:2212.04325.\n[131] V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young, \u201cMMIE\nTraining of Large Vocabulary Recognition Systems,\u201d Speech Commu\u0002nication, vol. 22, no. 4, pp. 303\u2013314, 1997.\n[132] D. Povey and P. Woodland, \u201cImproved Discriminative Training Tech\u0002niques for Large Vocabulary Continuous Speech Recognition,\u201d in Proc.\nIEEE ICASSP, Salt Lake City, UT, May 2001, pp. 45\u201348.\n[133] R. Schluter, W. Macherey, B. M \u00a8 uller, and H. Ney, \u201cComparison of \u00a8\nDiscriminative Training Criteria and Optimization Methods for Speech\nRecognition,\u201d Speech Communication, vol. 34, no. 3, pp. 287\u2013310, May\n2001, EURASIP Best Paper Award.\n[134] B. Kingsbury, \u201cLattice-Based Optimization of Sequence Classifica\u0002tion Criteria for Neural-Network Acoustic Modeling,\u201d in Proc. IEEE\nICASSP, Taipei, Taiwan, Apr. 2009, pp. 3761\u20133764.\n[135] G. Heigold, R. Schluter, H. Ney, and S. Wiesler, \u201cDiscriminative \u00a8\nTraining for Automatic Speech Recognition: Modeling, Criteria, Opti\u0002mization, Implementation, and Performance,\u201d IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 58\u201369, Nov. 2012.\n[136] W. Michel, R. Schluter, and H. Ney, \u201cComparison of Lattice-Free and \u00a8\nLattice-Based Sequence Discriminative Training Criteria for LVCSR,\u201d\nin Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1601\u20131605,\narXiv:1907.01409.\n[137] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu,\nand A. Kannan, \u201cMinimum Word Error Rate Training for Attention\u0002Based Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4839\u20134843.\n[138] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, \u201cMinimum Bayes Risk\nTraining of RNN-Transducer for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 966\u2013970, DOI:\n10.21437/Interspeech.2020-1221.\n[139] M. K. Baskar, L. Burget, S. Watanabe, M. Karafiat, T. Hori, and \u00b4\nJ. H. Cernock \u02c7 y, \u201cPromising Accurate Prefix Boosting for Sequence- `\nto-Sequence ASR,\u201d in Proc. IEEE ICASSP. Brighton, UK: IEEE,\nMay 2019, pp. 5646\u20135650.\n[140] A. Tjandra, S. Sakti, and S. Nakamura, \u201cSequence-to-Sequence ASR\nOptimization via Reinforcement Learning,\u201d in Proc. IEEE ICASSP.\nCalgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5829\u20135833.\n[141] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, \u201cSequence Training\nof Encoder-Decoder Model Using Policy Gradient for End-to-End\nSpeech Recognition,\u201d in Proc. IEEE ICASSP. Calgary, Alberta,\nCanada: IEEE, Apr. 2018, pp. 5839\u20135843.\n[142] W. Michel, R. Schluter, and H. Ney, \u201cEarly Stage LM Integration Using \u00a8\nLocal and Global Log-Linear Combination,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 3605\u20133609, arXiv:2005.10049.\n[143] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA Fast Learning Algorithm\nfor Deep Belief Nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u2013\n1554, Jul. 2006.\n[144] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy Layer\u0002Wise Training of Deep Networks,\u201d in Proc. NIPS, Barcelona, Spain,\nDec. 2006, pp. 153\u2013160.\n[145] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schluter, and H. Ney, \u00a8\n\u201cA Comprehensive Study of Deep Bidirectional LSTM RNNs for\nAcoustic Modeling in Speech Recognition,\u201d in Proc. IEEE ICASSP,\nNew Orleans, LA, Mar. 2017, pp. 2462\u20132466.\n[146] A. Zeyer, T. Alkhouli, and H. Ney, \u201cRETURNN as a Generic Flexible\nNeural Toolkit with Application to Translation and Speech Recogni\u0002tion,\u201d in Proc. ACL, Melbourne, Australia, Jul. 2018, pp. 128\u2013133.\n[147] A. Zeyer, K. Irie, R. Schluter, and H. Ney, \u201cImproved Training \u00a8\nof End-to-End Attention Models for Speech Recognition,\u201d in Proc.\nInterspeech, Hyderabad, India, Sep. 2018, pp. 7\u201311.\n[148] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA Comprehensive \u00a8\nAnalysis on Attention Models,\u201d in Proc. NIPS, Montreal, Canada, Dec.\n2018.\n[149] Y. Chung, C. Wu, C. Shen, H. Lee, and L. Lee, \u201cAudio Word2Vec:\nUnsupervised Learning of Audio Segment Representations using\nSequence-to-sequence Autoencoder,\u201d in Proc. Interspeech, San Fran\u0002cisco, CA, Sep. 2016, arXiv:1603.00982.\n[150] Y.-C. Chen, S.-F. Huang, H.-y. Lee, Y.-H. Wang, and C.-H. Shen, \u201cAu\u0002dio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised\nLearning of Audio Segmentation and Representation,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 27,\nno. 9, pp. 1481\u20131493, 2019, DOI: 10.1109/TASLP.2019.2922832.\n[151] S. Scanzio, P. Laface, L. Fissore, R. Gemello, and F. Mana, \u201cOn the\nUse of a Multilingual Neural Network Front-End,\u201d in Proc. Interspeech,\nBrisbane, Australia, Sep. 2008, pp. 2711\u20132714.\n[152] Z. Tuske, J. Pinto, D. Willett, and R. Schl \u00a8 uter, \u201cInvestigation on \u00a8\nCross- and Multilingual MLP features under matched and mismatched\nacoustical conditions,\u201d in IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, Vancouver, Canada, May 2013, pp.\n7349\u20137353.\n[153] S. Zhou, S. Xu, and B. Xu, \u201cMultilingual End-to-End Speech Recog\u0002nition with a Single Transformer on Low-Resource Languages,\u201d Jun.\n2018, arXiv:1806.05059.\n[154] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL-HLT,\nMinneapolis, MN, Jun. 2019, arXiv:1904.02210.\n[155] W. Hou, Y. Dong, B. Zhuang, L. Yang, J. Shi, and T. Shinozaki,\n\u201cLarge-scale end-to-end multilingual speech recognition and language\nidentification with multi-task learning,\u201d in Proc. Interspeech, Shanghai,\nChina, Oct. 2020, pp. 1037\u20131041.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\n[156] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Syn\u0002naeve, and R. Collobert, \u201cMassively Multilingual ASR: 50 Languages,\n1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, Shanghai, China,\nOct. 2020, arXiv:2007.03001.\n[157] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani,\nW. R. Huang, M. Ma, and J. Bai, \u201cScaling End-to-End Models for\nLarge-Scale Multilingual ASR,\u201d in Proc. IEEE ASRU, 2021, pp. 1011\u2013\n1018.\n[158] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen,\nY. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li, M. Ma, W. Chan,\nJ. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath,\nF. Beaufays, Z. Chen, Q. V. Le, C.-C. Chiu, R. Pang, and Y. Wu,\n\u201cBigSSL: Exploring the frontier of large-scale semi-supervised learning\nfor automatic speech recognition,\u201d IEEE Journal of Selected Topics\nin Signal Processing, vol. 16, no. 6, pp. 1519\u20131532, oct 2022,\narXiv:2109.13226.\n[159] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Repre\u0002sentations through Modality Matching,\u201d in Proc. Interspeech, Incheon,\nSouth Korea, Sep. 2022, arXiv:2204.03409.\n[160] A. Radford, J. W. Kim, C. McLeavey, P. Mishkin, T. Xu, G. Brockman,\nand I. Sutskever, \u201cIntroducing Whisper - Robust Speech Recognition\nvia Large-Scale Weak Supervision,\u201d Sep. 2022. [Online]. Available:\nhttps://openai.com/blog/whisper/\n[161] T. P. Vogl, J. Mangis, A. Rigler, W. Zink, and D. Alkon, \u201cAcceler\u0002ating the Convergence of the Back-Propagation Method,\u201d Biological\nCybernetics, vol. 59, no. 4, pp. 257\u2013263, 1988.\n[162] N. S. Keskar and G. Saon, \u201cA Nonmonotone Learning Rate Strategy\nfor SGD Training of Deep Neural Networks,\u201d in Proc. IEEE ICASSP.\nQueensland, Australia: IEEE, Apr. 2015, pp. 4974\u20134978.\n[163] S. Renals, N. Morgan, H. Bourlard, C. Wooters, and P. Kohn, \u201cConnec\u0002tionist Speech Recognition: Status and Prospects,\u201d ICSI, 1991, Tech.\nRep. TR-OI-070.\n[164] D. Johnson, D. Ellis, C. Oei, C. Wooters, and P. Faerber, \u201cQuickNet,\u201d\nICSI, Berkeley, 2004. [Online]. Available: http://www.icsi.berkeley.\nedu/Speech/qn.html\n[165] A. Senior, G. Heigold, M. Ranzato, and K. Yang, \u201cAn Empirical Study\nof Learning Rates in Deep Neural Networks for Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Vancouver, BC, Canada: IEEE, May 2013,\npp. 6724\u20136728.\n[166] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d\nin Proc. ICLR, New Orleans, LA, May 2019, arXiv:1711.05101.\n[167] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, \u201cDon\u2019t Decay the\nLearning Rate, Increase the Batch Size,\u201d in Proc. ICLR, New Orleans,\nLA, May 2018, arXiv:1711.00489.\n[168] J. Howard and S. Ruder, \u201cUniversal Language Model Fine-Tuning for\nText Classification,\u201d in Proc. ACL, Melbourne, Australia, Jun. 2018,\npp. 328\u2013339.\n[169] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fer\u0002nando, and K. Kavukcuoglu, \u201cPopulation Based Training of Neural\nNetworks,\u201d Nov. 2017, arXiv:1711.09846.\n[170] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta\u0002Learning in Neural Networks: A Survey,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PP, pp. 1\u201320, 2021.\n[171] J. L. Elman, \u201cLearning and Development in Neural Networks: The\nImportance of Starting Small,\u201d Cognition, vol. 48, no. 1, pp. 71\u201399,\n1993, DOI: 10.1016/0010-0277(93)90058-4.\n[172] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum\nLearning,\u201d in Proc. ICML, Montreal, Quebec, Canada, Jun. 2009, p.\n41\u201348.\n[173] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan\u0002zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen,\nJ. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley,\nL. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh,\nD. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao,\nD. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep Speech 2: End-to-End Speech\nRecognition in English and Mandarin,\u201d in Proc. ICML, New York City,\nNY, Jun. 2016, pp. 173\u2013182.\n[174] Z. Tuske, G. Saon, K. Audhkhasi, and B. Kingsbury, \u201cSingle Headed \u00a8\nAttention Based Sequence-to-Sequence Model for State-of-the-Art\nResults on Switchboard,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 551\u2013555.\n[175] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, \u201cImproving End\u0002to-End Single-Channel Multi-Talker Speech Recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 28, pp. 1385\u2013\n1394, 2020.\n[176] B. Polyak, \u201cSome Methods of Speeding up the Convergence of\nIteration Methods,\u201d USSR Computational Mathematics and Mathe\u0002matical Physics, vol. 4, no. 5, pp. 1\u201317, 1964, DOI: 10.1016/0041-\n5553(64)90137-5.\n[177] Y. Nesterov, \u201cA method of solving a convex programming problem\nwith convergence rate O( 1\nk2\n),\u201d Soviet Mathematics Doklady, vol. 27,\npp. 372\u2013376, 1983.\n[178] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, \u201cOn the Importance\nof Initialization and Momentum in Deep Learning,\u201d in Proc. ICML,\nAtlanta, GA, Jun. 2013, pp. 1139\u20131147.\n[179] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimiza\u0002tion,\u201d in Proc. ICLR, San Diego, CA, May 2015, arXiv:1412.6980.\n[180] Z. Tuske, G. Saon, and B. Kingsbury, \u201cOn the Limit of English Con- \u00a8\nversational Speech Recognition,\u201d in Proc. Interspeech, Brno, Czechia,\nSep. 2021, pp. 2062\u20132066.\n[181] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever,\n\u201cDeep Double Descent: Where Bigger Models and More Data Hurt,\u201d\nin Proc. ICLR, virtual, Apr. 2020, arXiv:1912.02292.\n[182] A. Krogh and J. Hertz, \u201cA Simple Weight Decay Can Improve\nGeneralization,\u201d in Neural Information Processing Systems (NIPS),\nDenver, CO, Dec. 1991, pp. 950\u2013957.\n[183] A. F. Murray and P. J. Edwards, \u201cEnhanced MLP Performance and\nFault Tolerance Resulting from Synaptic Weight Noise during Train\u0002ing,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 5, pp. 792\u2013\n802, Sep. 1994.\n[184] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nAdvances in Neural Information Processing Systems, vol. 24, 2011.\n[185] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach,\nand J. Martens, \u201cAdding Gradient Noise Improves Learning for Very\nDeep Networks,\u201d Nov. 2015, arXiv:1511.06807.\n[186] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, \u201cImproving Neural Networks by Preventing Co\u0002Adaptation of Feature Detectors,\u201d Jul. 2012, arXiv:1207.0580.\n[187] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification\nwith Deep Convolutional Neural Networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), vol. 25, Lake Tahoe, NV, Dec.\n2012.\n[188] Y. Gal and Z. Ghahramani, \u201cDropout as a Bayesian Approximation:\nRepresenting Model Uncertainty in Deep Learning,\u201d in Proc. ICML,\nNew York City, NY, Jun. 2016, pp. 1050\u20131059.\n[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep Net\u0002works with Stochastic Depth,\u201d in European Conference on Computer\nVision, Amsterdam, Netherlands, Oct. 2016, pp. 646\u2013661.\n[190] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Muller, and A. Waibel, \u00a8\n\u201cVery Deep Self-Attention Networks for End-to-End Speech Recogni\u0002tion,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 66\u201370.\n[191] J. Lee and S. Watanabe, \u201cIntermediate Loss Regularization for CTC\u0002Based Speech Recognition,\u201d in Proc. IEEE ICASSP, Toronto, Ontario,\nCanada, Jun. 2021, pp. 6224\u20136228.\n[192] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \u201cRegularization\nof Neural Networks using DropConnect,\u201d in Proc. ICML, 2013, pp.\n1058\u20131066.\n[193] D. Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, \u00b4\nA. Goyal, Y. Bengio, A. Courville, and C. Pal, \u201cZoneout: Regularizing\nRNNs by Randomly Preserving Hidden Activations,\u201d in Proc. ICLR,\nToulon, France, Apr. 2017, arXiv:1606.01305.\n[194] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink\u0002ing the Inception Architecture for Computer Vision,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition, Las Vegas, NV, Jun. 2016,\npp. 2818\u20132826.\n[195] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled Sampling\nfor Sequence Prediction with Recurrent Neural Networks,\u201d Proc. NIPS,\nvol. 28, Dec. 2015.\n[196] T. Trinh, A. Dai, T. Luong, and Q. Le, \u201cLearning Longer-Term Depen\u0002dencies in RNNs with Auxiliary Losses,\u201d in Proc. ICML, Stockholm,\nSweden, Jul. 2018, pp. 4965\u20134974.\n[197] R. J. Williams and J. Peng, \u201cAn Efficient Gradient-Based Algorithm\nfor On-Line Training of Recurrent Network Trajectories,\u201d IEEE Neural\nComputation, vol. 2, no. 4, pp. 490\u2013501, 1990.\n[198] S. Merity, N. S. Keskar, and R. Socher, \u201cAn Analysis of Neural\nLanguage Modeling at Multiple Scales,\u201d Mar. 2018, arXiv:1803.08240.\n[199] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, \u201cMixSpeech:\nData Augmentation for Low-resource Automatic Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021,\npp. 7008\u20137012.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n25\n[200] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift,\u201d in Proc.\nICML, Lille, France, Jul. 2015, pp. 448\u2013456.\n[201] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic Spectral Distortion for\nLow Resource Speech Recognition with Deep Neural Networks,\u201d in\nProc. IEEE ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 309\u2013\n314.\n[202] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmentation\nfor Speech Recognition,\u201d in Proc. Interspeech, Dresden, Germany, Sep.\n2015.\n[203] N. Jaitly and G. E. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nImproves Speech Recognition,\u201d in Proc. ICML, vol. 117, Jun. 2013,\np. 21.\n[204] G. Saon, Z. Tuske, K. Audhkhasi, and B. Kingsbury, \u201cSequence Noise \u00a8\nInjected Training for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Brighton, England, May 2019, pp. 6261\u20136265.\n[205] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le,\nand Y. Wu, \u201cSpecAugment on Large Scale Datasets,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 6879\u20136883.\n[206] C. Wang, Y. Wu, Y. Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao, and\nM. Zhou, \u201cSemantic Mask for Transformer Based End-to-End Speech\nRecognition,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n971\u2013975.\n[207] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo,\nand K. Takeda, \u201cBack-Translation-Style Data Augmentation for End\u0002to-End ASR,\u201d in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018,\npp. 426\u2013433.\n[208] N. Rossenbach, M. Zeineldeen, B. Hilmes, R. Schluter, and H. Ney, \u00a8\n\u201cComparing the Benefit of Synthetic Training Data for Various Au\u0002tomatic Speech Recognition Architectures,\u201d in Proc. IEEE ASRU,\nCartagena, Colombia, Dec. 2021, arXiv:2104.05379.\n[209] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen, and\nC.-C. Chiu, \u201cNo Need for a Lexicon? Evaluating the Value of\nthe Pronunciation Lexica in End-to-End Models,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5859\u20135863, DOI:\n10.1109/ICASSP.2018.8462380.\n[210] C. Wooters and A. Stolcke, \u201cMultiple-Pronunciation Lexical Modeling\nin a Speaker Independent Speech Understanding System,\u201d in Proc.\nICSLP, Yokohama, Japan, Sep. 1994, pp. 1363\u20131366.\n[211] I. McGraw, I. Badr, and J. R. Glass, \u201cLearning Lexicons From Speech\nUsing a Pronunciation Mixture Model,\u201d IEEE/ACM Trans. Audio,\nSpeech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2012.\n[212] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGMM-Free DNN\nAcoustic Model Training,\u201d in Proc. IEEE ICASSP, Florence, Italy, May\n2014, pp. 5602\u20135606, DOI: 10.1109/ICASSP.2014.6854675.\n[213] G. Gosztolya, T. Grosz, and L. T \u00b4 oth, \u201cGMM-Free Flat Start Sequence- \u00b4\nDiscriminative DNN Training,\u201d in Proc. Interspeech, N. Morgan,\nEd. San Francisco, CA: ISCA, Sep. 2016, pp. 3409\u20133413, DOI:\n10.21437/Interspeech.2016-391.\n[214] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cFlat-Start\nSingle-Stage Discriminatively Trained HMM-Based Models for ASR,\u201d\nIEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 26,\nno. 11, pp. 1949\u20131961, 2018.\n[215] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig,\n\u201cThe IBM 2004 Conversational Telephony System for Rich Transcrip\u0002tion,\u201d in Proc. IEEE ICASSP, Philadelphia, PA, Mar. 2005, pp. 205\u2013\n208.\n[216] H. Hadian, D. Povey, H. Sameti, J. Trmal, and S. Khudanpur,\n\u201cImproving LF-MMI Using Unconstrained Supervisions for ASR,\u201d\nin Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 43\u201347, DOI:\n10.1109/SLT.2018.8639684.\n[217] N. Kanda, Y. Fujita, and K. Nagamatsu, \u201cLattice-Free State-Level Min\u0002imum Bayes Risk Training of Acoustic Models,\u201d in Proc. Interspeech,\nB. Yegnanarayana, Ed. Hyderabad, India: ISCA, Sep. 2018, pp. 2923\u2013\n2927, DOI: 10.21437/Interspeech.2018-79.\n[218] S. J. Young and P. C. Woodland, \u201cThe Use of State Tying in Continuous\nSpeech Recognition,\u201d in Proc. Eurospeech, Berlin, Germany, Dec.\n1993, pp. 2203\u20132206.\n[219] S. Wiesler, G. Heigold, M. Nu\u00dfbaum-Thom, R. Schluter, and H. Ney, \u00a8\n\u201cA Discriminative Splitting Criterion for Phonetic Decision Trees,\u201d\nin Proc. Interspeech, Makuhari, Japan, Sep. 2010, pp. 54\u201357, one of\nshortlist for Best Student Paper Award.\n[220] T. Raissi, E. Beck, R. Schluter, and H. Ney, \u201cTowards Consistent \u00a8\nHybrid HMM Acoustic Modeling,\u201d Apr. 2021, arXiv:2104.02387.\n[221] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schluter, and H. Ney, \u00a8\n\u201cA Systematic Comparison of Grapheme-Based vs. Phoneme-Based\nLabel Units for Encoder-Decoder-Attention Models,\u201d Nov. 2020,\narXiv:2005.09336.\n[222] C. Luscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, \u00a8\nR. Schluter, and H. Ney, \u201cRWTH ASR Systems for LibriSpeech: \u00a8\nHybrid vs Attention,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 231\u2013235.\n[223] D. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. Le,\n\u201cImproved Noisy Student Training for Automatic Speech Recognition,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2817\u20132821.\n[224] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and\nQ. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for\nAutomatic Speech Recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 2613\u20132617.\n[225] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schluter, and H. Ney, \u201cThe \u00a8\nRWTH ASR System for TED-LIUM Release 2: Improving Hybrid\nHMM with SpecAugment,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 7839\u20137843.\n[226] J. Cui, B. Kingsbury, B. Ramabhadran, A. Sethy, K. Audhkhasi, X. Cui,\nE. Kislal, L. Mangu, M. Nussbaum-Thom, M. Picheny, Z. Tuske, \u00a8\nP. Golik, R. Schluter, H. Ney, M. J. F. Gales, K. M. Knill, A. Ragni, \u00a8\nH. Wang, and P. Woodland, \u201cMultilingual Representations for Low\nResource Speech Recognition and Keyword Search,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 259\u2013266.\n[227] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL, Min\u0002neapolis, MN, Jun. 2019, pp. 96\u2013108.\n[228] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. Ramabhadran,\nY. Wu, A. Bapna, Z. Chen, and S. Lee, \u201cLarge-Scale Multilingual\nSpeech Recognition with a Streaming End-to-End Model,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2130\u20132134.\n[229] A. Graves, \u201cConnectionist Temporal Classification,\u201d in Supervised\nSequence Labelling with Recurrent Neural Networks. Heidelberg,\nGermany: Springer, 2012, ch. Connectionist Temporal Classification,\npp. 61\u201393.\n[230] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,\n\u201cMask CTC: Non-Autoregressive End-to-End ASR with CTC and\nMask Predict,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n3655\u20133659.\n[231] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, \u201cImputer:\nSequence Modelling via Imputation and Dynamic Programming,\u201d in\nProc. ICML. PMLR, Jul. 2020, pp. 1403\u20131413.\n[232] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, \u201cInsertion-Based\nModeling for End-to-End Automatic Speech Recognition,\u201d in Proc.\nInterspeech, Shanghai, China, Oct. 2020, pp. 3660\u20133664.\n[233] L. Dong and B. Xu, \u201cCif: Continuous Integrate-and-Fire for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6079\u20136083.\n[234] J. Nozaki and T. Komatsu, \u201cRelaxing the Conditional Independence\nAssumption of CTC-Based ASR by Conditioning on Intermediate\nPredictions,\u201d in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3735\u2013\n3739.\n[235] Y. Higuchi, N. Chen, Y. Fujita, H. Inaguma, T. Komatsu, J. Lee,\nJ. Nozaki, T. Wang, and S. Watanabe, \u201cA Comparative Study on Non\u0002Autoregressive Modelings for Speech-to-Text Generation,\u201d in Proc.\nIEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2110.05249.\n[236] W. Zhou, R. Schluter, and H. Ney, \u201cRobust Beam Search for Encoder- \u00a8\nDecoder Attention Based Speech Recognition without Length Bias,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1768\u20131772.\n[237] P. Koehn and R. Knowles, \u201cSix Challenges for Neural Machine Transla\u0002tion,\u201d in First Workshop on Neural Machine Translation. Vancouver,\nBC, Canada: Association for Computational Linguistics, Aug. 2017,\npp. 28\u201339.\n[238] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, \u201cModeling Coverage for Neural\nMachine Translation,\u201d in Proc. ACL, Berlin, Germany, May 2016, pp.\n76\u201385.\n[239] T. Hori, J. Cho, and S. Watanabe, \u201cEnd-to-End Speech Recogni\u0002tion with Word-Based RNN Language Models,\u201d in Proc. IEEE SLT.\nAthens, Greece: IEEE, Dec. 2018, pp. 389\u2013396.\n[240] K. Deng and P. C. Woodland, \u201cLabel-Synchronous Neural Transducer\nfor End-to-End ASR,\u201d Jul. 2023, arXiv:2307.03088.\n[241] T. Hori and A. Nakamura, Speech Recognition Algorithms Using\nWeighted Finite-State Transducers. San Rafael, CA: Morgan &\nClaypool Publishers, 2013.\n[242] R. Haeb-Umbach and H. Ney, \u201cImprovements in Beam Search for\n10000-Word Continuous-Speech Recognition,\u201d IEEE Transactions on\nSpeech and Audio Processing, vol. 2, no. 2, pp. 353\u2013356, 1994.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n26\n[243] H. Ney and S. Ortmanns, \u201cProgress in Dynamic Programming Search\nfor LVCSR,\u201d Proc. of the IEEE, vol. 88, no. 8, pp. 1224\u20131240, Aug.\n2000, http://dx.doi.org/10.1109/5.880081.\n[244] T. Hori, Y. Kubo, and A. Nakamura, \u201cReal-Time One-Pass Decoding\nwith Recurrent Neural Network Language Model for Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6364\u2013\n6368.\n[245] E. Beck, W. Zhou, R. Schluter, and H. Ney, \u201cLSTM Language Models \u00a8\nfor LVCSR in First-Pass Decoding and Lattice-Rescoring,\u201d Jul. 2019,\narXiv:1907.01030.\n[246] G. Saon, Z. Tuske, and K. Audhkhasi, \u201cAlignment-Length Syn- \u00a8\nchronous Decoding for RNN Transducer,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7804\u20137808.\n[247] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, \u201cFirst\u0002Pass Large Vocabulary Continuous Speech Recognition Using Bi\u0002Directional Recurrent DNNs,\u201d Dec. 2014, arXiv:1408.2873.\n[248] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP. Brighton, UK:\nIEEE, May 2019, pp. 5666\u20135670.\n[249] N. Moritz, T. Hori, and J. Le, \u201cStreaming Automatic Speech Recogni\u0002tion with the Transformer Model,\u201d in Proc. IEEE ICASSP. Barcelona,\nSpain: IEEE, May 2020, pp. 6074\u20136078.\n[250] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kalgaonkar, A. Sri\u0002ram, C. Fuegen, and M. L. Seltzer, \u201cRNN-T for Latency Controlled\nASR with Improved Beam Search,\u201d Nov. 2019, arXiv:1911.01629.\n[251] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring Transformers for Large\u0002Scale Speech Recognition,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 5041\u20135045.\n[252] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, \u201cStreaming End-to\u0002End ASR Based on Blockwise Non-Autoregressive Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3755\u20133759.\n[253] H. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, \u201cOnline Hybrid\nCTC/Attention Architecture for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 2623\u20132627, DOI:\n10.21437/Interspeech.2019-2018.\n[254] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming Transformer\nASR with Blockwise Synchronous Beam Search,\u201d in Proc. IEEE SLT.\nShenzhen, China: IEEE, Jun. 2021, pp. 22\u201329.\n[255] K. Hwang and W. Sung, \u201cCharacter-level language modeling with\nhierarchical recurrent neural networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5720\u20135724.\n[256] T. Hori, S. Watanabe, Y. Zhang, and W. Chan, \u201cAdvances in Joint CTC\u0002Attention Based End-to-End Speech Recognition with a Deep CNN\nEncoder and RNN-LM,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug.\n2017, pp. 949\u2013953.\n[257] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, \u201cAn Analysis of Incorporating an External Lan\u0002guage Model into a Sequence-to-Sequence Model,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5824\u20135828, DOI:\n10.1109/ICASSP.2018.8462682.\n[258] G. Saon, Z. Tuske, D. Bolanos, and B. Kingsbury, \u201cAdvancing \u00a8\nRNN Transducer Technology for Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5654\u20135658,\narXiv:2103.09935.\n[259] H. Seki, T. Hori, S. Watanabe, N. Moritz, and J. Le Roux, \u201cVectorized\nBeam Search for CTC-Attention-Based Speech Recognition,\u201d in Proc.\nInterspeech, Brighton, UK, May 2019, pp. 3825\u20133829.\n[260] T. Hori, S. Watanabe, and J. R. Hershey, \u201cMulti-Level Language\nModeling and Decoding for Open Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec.\n2017, pp. 287\u2013293.\n[261] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie,\nS. Watanabe, and S. Khudanpur, \u201cEspresso: A Fast End-to-End Neural\nSpeech Recognition Toolkit,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 136\u2013143.\n[262] Z. Tuske, K. Audhkhasi, and G. Saon, \u201cAdvancing Sequence-to- \u00a8\nSequence Based Speech Recognition,\u201d in Proc. Interspeech, Graz,\nAustria, Sep. 2019, pp. 3780\u20133784.\n[263] J. Drexler and J. Glass, \u201cSubword Regularization and Beam Search\nDecoding for End-to-End Automatic Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 6266\u20136270.\n[264] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu,\n\u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Interspeech,\nGraz, Austria, Sep. 2019, pp. 2773\u20132777.\n[265] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen,\nL. Xie, and X. Lei, \u201cWeNet: Production Oriented Streaming and Non\u0002Streaming End-to-End Speech Recognition Toolkit,\u201d Brno, Czechia, pp.\n4054\u20134058, Sep. 2021.\n[266] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei,\n\u201cU2++: Unified Two-Pass Bidirectional End-to-End Model for Speech\nRecognition,\u201d Dec. 2021, arXiv:2106.05642.\n[267] M. Zapotoczny, P. Pietrzak, A. Lancucki, and J. Chorowski, \u201cLattice\nGeneration in Attention-Based Speech Recognition Models,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2225\u20132229.\n[268] J. Kim, Y. Lee, and E. Kim, \u201cAccelerating RNN Transducer Inference\nvia Adaptive Expansion Search,\u201d IEEE Signal Processing Letters,\nvol. 27, pp. 2019\u20132023, 2020.\n[269] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: A system for Large\u0002Scale Machine Learning,\u201d in Proc. OSDI, Savannah, GA, Nov. 2016,\npp. 265\u2013283.\n[270] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,\nand M. Auli, \u201cFAIRSEQ: A Fast, Extensible Toolkit for Sequence\nModeling,\u201d in Proc. NAACL, Minneapolis, MN, Jun. 2019, pp. 48\u201353.\n[271] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan,\nT. Sainath, Y. Cao, C.-C. Chiu et al., \u201cLingvo: a Modular and\nScalable Framework for Sequence-to-Sequence Modeling,\u201d Feb. 2019,\narXiv:1902.08295.\n[272] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schluter, and \u00a8\nH. Ney, \u201cRETURNN: The RWTH Extensible Training Framework for\nUniversal Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5345\u20135349.\n[273] A. Hannun, A. Lee, Q. Xu, and R. Collobert, \u201cSequence-to-Sequence\nSpeech Recognition with Time-Depth Separable Convolutions,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 3785\u20133789.\n[274] M. Li, M. Liu, and H. Masanori, \u201cEnd-to-End Speech Recognition with\nAdaptive Computation Steps,\u201d in Proc. IEEE ICASSP, Brighton, UK,\nMay 2019, pp. 6246\u20136250.\n[275] P. Bahar, N. Makarov, A. Zeyer, R. Schuter, and H. Ney, \u201cExploring \u00a8\na Zero-Order Direct HMM Based on Latent Attention for Automatic\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 7854\u20137858.\n[276] Z. Huang, G. Zweig, and B. Dumoulin, \u201cCache Based Recurrent\nNeural Network Language Model Inference for First Pass Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp.\n6354\u20136358.\n[277] J. Jorge, A. Gimenez, J. Iranzo-S \u00b4 anchez, J. Civera, A. Sanchis, and \u00b4\nA. Juan, \u201cReal-Time One-Pass Decoder for Speech Recognition Using\nLSTM Language Models,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3820\u20133824.\n[278] W. Zhou, R. Schluter, and H. Ney, \u201cFull-Sum Decoding for Hybrid \u00a8\nHMM Based Speech Recognition Using LSTM Language Model,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7834\u20137838.\n[279] P. Sountsov and S. Sarawagi, \u201cLength Bias in Encoder Decoder Models\nand a Case for Global Conditioning,\u201d in Proc. EMNLP, Austin, TX,\nNov. 2016, pp. 1516\u20131525.\n[280] K. Murray and D. Chiang, \u201cCorrecting Length Bias in Neural Machine\nTranslation,\u201d in Proc. WMT, Brussels, Belgium, Oct. 2018, pp. 212\u2013\n223.\n[281] F. Stahlberg and B. Byrne, \u201cOn NMT Search Errors and Model Errors:\nCat Got Your Tongue?\u201d in Proc. EMNLP. Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 3354\u20133360.\n[282] N. Deshmukh, A. Ganapathiraju, and J. Picone, \u201cHierarchical Search\nfor Large-Vocabulary Conversational Speech Recognition: Working\nToward a Solution to the Decoding Problem,\u201d IEEE Signal Processing\nMagazine, vol. 16, no. 5, pp. 84\u2013107, 1999.\n[283] L. Nguyen and R. Schwartz, \u201cSingle-Tree Method for Grammar\u0002Directed Search,\u201d in Proc. IEEE ICASSP, vol. 2, Phoenix, AZ, Mar.\n1999, pp. 613\u2013616.\n[284] L. Sar\u0131, N. Moritz, T. Hori, and J. Le Roux, \u201cUnsupervised Speaker\nAdaptation using Attention-based Speaker Memory for End-to-End\nASR,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 2\u2013\n6.\n[285] F. Weninger, J. Andres-Ferrer, X. Li, and P. Zhan, \u201cListen, Attend, \u00b4\nSpell and Adapt: Speaker Adapted Sequence-to-Sequence ASR,\u201d in\nProc. Interspeech. Graz, Austria: ISCA, Sep. 2019, pp. 3805\u20133809.\n[286] Z. Meng, Y. Gaur, J. Li, and Y. Gong, \u201cSpeaker Adaptation for\nAttention-Based End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech. Graz, Austria: ISCA, Sep. 2019, pp. 241\u2013245.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n27\n[287] N. Tomashenko and Y. Esteve, \u201cEvaluation of Feature-Space Speaker `\nAdaptation for End-to-End Acoustic Models,\u201d in Proc. LREC.\nMiyazaki, Japan: ELRA, May 2018, pp. 3163\u20133170.\n[288] S. F. Chen and J. Goodman, \u201cAn Empirical Study of Smoothing\nTechniques for Language Modeling,\u201d in Proc. ACL, Santa Cruz, CA,\nJun. 1996, pp. 310\u2013318.\n[289] T. Mikolov, M. Karafiat, L. Burget, J. \u00b4 Cernock \u02c7 y, and S. Khudanpur, `\n\u201cRecurrent Neural Network Based Language Model,\u201d in Proc. Inter\u0002speech, Makuhari, Japan, Sep. 2010, pp. 1045\u20131048.\n[290] M. Sundermeyer, R. Schluter, and H. Ney, \u201cLSTM Neural Networks for \u00a8\nLanguage Modeling,\u201d in Proc. Interspeech, Portland, OR, Sep. 2012,\npp. 194\u2013197.\n[291] N.-Q. Pham, G. Kruszewski, and G. Boleda, \u201cConvolutional Neural\nNetwork Language Models,\u201d in Proc. EMNLP, Austin, TX, Nov. 2016,\npp. 1153\u20131162.\n[292] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage Modeling\nwith Gated Convolutional Networks,\u201d in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. JMLR.\norg, 2017, pp. 933\u2013941.\n[293] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and\nR. Collobert, \u201cFully Convolutional Speech Recognition,\u201d Feb. 2018,\narXiv:1812.06864.\n[294] T. Likhomanenko, G. Synnaeve, and R. Collobert, \u201cWho needs words?\nlexicon-free speech recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 3915\u20133919, arXiv:1904.04479.\n[295] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, \u201cCharacter\u0002Level Language Modeling with Deeper Self-Attention,\u201d in Proc. AIII,\nvol. 33, Honolulu, Hawaii, Feb. 2019, pp. 3159\u20133166.\n[296] K. Irie, A. Zeyer, R. Schluter, and H. Ney, \u201cLanguage Modeling with \u00a8\nDeep Transformers,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 3905\u20133909.\n[297] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov,\n\u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 2978\u20132988.\n[298] P. Werbos, \u201cBackpropagation Through Time: What It Does and How\nto Do It,\u201d Proc. of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990,\nDOI: 10.1109/5.58337.\n[299] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n\u201cCompressive Transformers for Long-Range Sequence Modelling,\u201d\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6154\u2013\n6158, 2020.\n[300] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn Using Monolingual\nCorpora in Neural Machine Translation,\u201d Jun. 2015, arXiv:1503.03535.\n[301] A. Sriram, H. Jun, S. Satheesh, and A. Coates, \u201cCold Fusion: Training\nSeq2Seq Models Together with Language Models,\u201d in Proc. Inter\u0002speech, Hyderabad, India, Sep. 2018, pp. 387\u2013391.\n[302] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n\u201cComponent Fusion: Learning Replaceable Language Model Com\u0002ponent for End-to-End Speech Recognition System,\u201d in Proc. IEEE\nICASSP. Brighton, UK: IEEE, May 2019, pp. 5361\u20135635.\n[303] E. McDermott, H. Sak, and E. Variani, \u201cA Density Ratio Approach to\nLanguage Model Fusion in End-To-End Automatic Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 434\u2013\n441.\n[304] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen,\nR. Zhao, J. Li, and Y. Gong, \u201cInternal Language Model Estimation\nfor Domain-Adaptive End-to-End Speech Recognition,\u201d in Proc. IEEE\nSLT, Shenzhen , China, Dec. 2020, pp. 243\u2013250.\n[305] W. Zhou, Z. Zheng, R. Schluter, and H. Ney, \u201cOn Language Model Inte- \u00a8\ngration for RNN Transducer based Speech Recognition,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8407\u20138411, arXiv:2110.06841.\n[306] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre\u0002Training of Deep Bidirectional Transformers for Language Understand\u0002ing,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 4171\u20134186.\n[307] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, \u201cLanguage Models are Unsuper\u0002vised Multitask Learners,\u201d 2019, openAI blog. [Online].\nAvailable: https://cdn.openai.com/better-language-models/language\nmodels are unsupervised multitask learners.pdf\n[308] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, \u201cMasked\nLanguage Model Scoring,\u201d in Proc. ACL, Jul. 2020, pp. 2699\u20132712.\n[309] S. Kim, S. Dalmia, and F. Metze, \u201cGated Embeddings in End-to-End\nSpeech Recognition for Conversational-Context Fusion,\u201d in Proc. ACL,\nFlorence, Italy, Jul. 2019, pp. 1131\u20131141.\n[310] A. Zeyer, A. Merboldt, W. Michel, R. Schluter, and H. Ney, \u201cLib- \u00a8\nrispeech Transducer Model with Internal Language Model Prior Cor\u0002rection,\u201d in Proc. Interspeech, Brno, Czech Republic, Apr. 2021, pp.\n2052\u20132056.\n[311] L. R. Bahl, F. Jelinek, and R. L. Mercer, \u201cA Maximum Likelihood\nApproach to Continuous Speech Recognition,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 5, no. 2, pp. 179\u2013190,\nMar. 1983.\n[312] J. Makhoul and R. Schwartz, \u201cState of the Art in Continuous Speech\nRecognition,\u201d Proc. NAS, vol. 92, no. 22, pp. 9956\u20139963, Oct. 1995.\n[313] D. Klakow and J. Peters, \u201cTesting the Correlation of Word Error Rate\nand Perplexity,\u201d Speech Communication, vol. 38, no. 1, pp. 19\u201328,\n2002.\n[314] M. Sundermeyer, H. Ney, and R. Schluter, \u201cFrom Feedforward to Re- \u00a8\ncurrent LSTM Neural Networks for Language Modeling,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 23, no. 3, pp.\n517\u2013529, Mar. 2015.\n[315] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, \u201cMinimum Word\nError Training of Long Short-Term Memory Recurrent Neural Network\nLanguage Models for Speech Recognition,\u201d in Proc. IEEE ICASSP,\nShanghai, China, Mar. 2016, pp. 5990\u20135994.\n[316] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Tele\u0002phone Speech Corpus for Research and Development,\u201d in Proc. IEEE\nICASSP, vol. 1, San Francisco, CA, Mar. 1992, pp. 517\u2013520 vol.1.\n[317] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an\nASR Corpus Based on Public Domain Audio Books,\u201d in Proc. IEEE\nICASSP, Queensland, Australia, Apr. 2015, pp. 5206\u20135210.\n[318] A. Zeyer, P. Bahar, K. Irie, R. Schluter, and H. Ney, \u201cA Comparison of \u00a8\nTransformer and LSTM Encoder Decoder Models for ASR,\u201d in Proc.\nIEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 8\u201315.\n[319] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,\nand A. Mohamed, \u201cHuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans.\nAudio, Speech, and Language Processing, vol. 19, pp. 3451\u20133460,\n2021.\n[320] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap,\nA. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-End ASR: from\nSupervised to Semi-Supervised Learning with Modern Architectures,\u201d\nin Proc. ICML, Jul. 2020, arXiv:1911.08460.\n[321] E. G. Ng, C.-C. Chiu, Y. Zhang, and W. Chan, \u201cPushing the Limits of\nNon-Autoregressive Speech Recognition,\u201d in Proc. Interspeech, Brno,\nCzechia, Sep. 2021, pp. 3725\u20132729.\n[322] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazare,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-Light: A\nBenchmark for ASR with Limited or no Supervision,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7669\u20137673.\n[323] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig,\nand M. L. Seltzer, \u201cTransformer-Based Acoustic Modeling for Hybrid\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6874\u20136878.\n[324] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and\nS. Watanabe, \u201cE-branchformer: Branchformer with enhanced merging\nfor speech recognition,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.00077.\n[325] M. Kitza, P. Golik, R. Schluter, and H. Ney, \u201cCumulative Adaptation for \u00a8\nBLSTM Acoustic Models,\u201d in Interspeech, Graz, Austria, Sep. 2019,\npp. 754\u2013758.\n[326] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. Li,\nJ. Chorowski, and M. Bacchiani, \u201cState-of-the-Art Speech Recognition\nwith Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4774\u20134778.\n[327] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo,\nD. Kim, S. Jung, J. Lee, M. Han, and C. Kim, \u201cAttention Based On\u0002Device Streaming Speech Recognition with Large Speech Corpus,\u201d in\nProc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 956\u2013963.\n[328] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov,\nZ. Wang, L. He, S. Zhao et al., \u201cDeveloping RNN-T Models Surpassing\nHigh-Performance Hybrid Models with Customization Capability,\u201d in\nProc. Interspeech, Shanghai, China (virtual), Oct. 2020, pp. 3590\u2013\n3594, arXiv:2007.15188.\n[329] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, \u201cOnline\nAutomatic Speech Recognition with Listen, Attend and Spell Model,\u201d\nIEEE Signal Processing Letters, vol. 27, pp. 1889\u20131893, 2020.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n28\n[330] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and\nM. Seltzer, \u201cEmformer: Efficient Memory Transformer based Acoustic\nModel for Low Latency Streaming Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 6783\u20136787.\n[331] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping Real-Time\nStreaming Transformer Transducer for Speech Recognition on Large\u0002Scale Dataset,\u201d in Proc. IEEE ICASSP. Toronto, Ontario, Canada:\nIEEE, Jun. 2021, pp. 5904\u20135908.\n[332] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, C.-C. Chiu, D. Garcia,\nA. Gruenstein, K. Hu, M. Jin, A. Kannan, Q. Liang, I. McGraw,\nC. Peyser, R. Prabhavalkar, G. Pundak, D. Rybach, Y. Shangguan,\nY. Sheth, T. Strohman, M. Visontai, Y. Wu, Y. Zhang, and D. Zhao,\n\u201cA Streaming On-Device End-To-End Model Surpassing Server-Side\nConventional Model Quality and Latency,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, may 2020, pp. 6059\u20136063.\n[333] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan,\nS.-Y. Chang, R. Pang, Y. He, J. Qin, W. Han, Q. Liang, Y. Zhang,\nT. Strohman, and Y. Wu, \u201cA Better and Faster End-to-End Model for\nStreaming ASR,\u201d in Proc. IEEE ICASSP, Toronto, Ontario, Canada,\nJun. 2021, pp. 5634\u20135638.\n[334] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach,\nC. Allauzen, E. Variani, J. Qin, Q.-N. Le-The, S.-Y. Chang, B. Li,\nA. Gulati, J. Yu, C.-C. Chiu, D. Caseiro, W. Li, Q. Liang, and\nP. Rondon, \u201cAn Efficient Streaming Non-Recurrent On-Device End\u0002to-End Model with Improvements to Rare-Word Modeling,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 1777\u20131781.\n[335] A. Bapna, Y.-A. Chung, N. Wu, , A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSLAM: A Unified\nEncoder for Speech and Language Modeling via Speech-Text Joint\nPre-Training,\u201d Oct. 2021, arXiv:2110.10329.\n[336] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng,\nS. Khanuja, J. Riesa, and A. Conneau, \u201cmSLAM: Massively Mul\u0002tilingual Joint Pre-Training for Speech and Text,\u201d Feb. 2022,\narXiv:2202.01374.\n[337] Y. Tang, H. Gong, N. Dong, C. Wag, W. Hsu, J. Gu, A. Baevski, X. Li,\nA. Mohamed, M. Auli, and J. Pino, \u201cUnified Speech-Text Pre-training\nfor Speech Translation and Recognition,\u201d in Proc. ACL, Dublin, Ireland,\nMay 2022, pp. 1488\u20131499, arXiv:2204.05409.\n[338] Y.-A. Chung, C. Zhu, and M. Zeng, \u201cSPLAT: Speech-Language Joint\nPre-Training for Spoken Language Understanding,\u201d in Proc. NAACL,\nJun. 2021, pp. 1897\u20131907, arXiv:2010.02295.\n[339] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko,\nQ. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, \u201cSpeechT5:\nUnified-Modal Encoder-Decoder Pre-Training for Spoken Language\nProcessing,\u201d in Proc. ACL, Dublin, Ireland, May 2022, pp. 5723\u20135738,\narXiv:2110.07205.\n[340] S. Thomas, H. J. Kuo, B. Kingsbury, and G. Saon, \u201cTowards Reducing\nthe Need for Speech Training Data to Build Spoken Language Under\u0002standing Systems,\u201d in Proc. IEEE ICASSP, Singapore, May 2022, pp.\n7932\u20137936, arXiv:2203.00006.\n[341] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen,\nB. Li, W. Wang, and T. Strohman, \u201cJOIST: A joint speech and text\nstreaming model for ASR,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.07353.\n[342] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and\nJ. Le Roux, \u201cCycle-Consistency Training for End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp.\n6271\u20136275.\n[343] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, \u201cMultichannel\nEnd-to-End Speech Recognition,\u201d in Proc. ICML. Sydney, Australia:\nPMLR, Aug. 2017, pp. 2632\u20132641.\n[344] J. Li, \u201cRecent Advances in End-to-End Automatic Speech Recogni\u0002tion,\u201d APSIPA Trans. on Signal and Information Processing, vol. 11,\nno. 1, Nov. 2021, DOI: 10.1561/116.00000050, arXiv:2111.01690.\nPLACE\nPHOTO\nHERE\nRohit Prabhavalkar Rohit Prabhavalkar received\nhis PhD in Computer Science and Engineering from\nThe Ohio State University, USA, in 2013. Follow\u0002ing his PhD, Rohit joined the Speech Technologies\ngroup at Google where he is currently a Staff Re\u0002search Scientist. At Google, his research has focused\nprimarily on developing compact acoustic models\nwhich can run efficiently on mobile devices, and on\ndeveloping improved end-to-end automatic speech\nrecognition systems. Rohit has co-authored over 50\nrefereed papers, which have received two best paper\nawards (ASRU 2017; ICASSP 2018). He currently serves as a member of the\nIEEE Speech and Language Processing Technical Committee (2018\u20132024),\nand as an Associate Editor of the IEEE/ACM Transactions on Audio, Speech,\nand Language Processing.\nPLACE\nPHOTO\nHERE\nTakaaki Hori received his PhD degree in system\nand information engineering from Yamagata Uni\u0002versity, Yonezawa, Japan, in 1999. From 1999 to\n2015, he had been engaged in researches on speech\nrecognition and spoken language processing at Cy\u0002ber Space Laboratories and Communication Science\nLaboratories in Nippon Telegraph and Telephone\n(NTT) Corporation, Japan. From 2015 to 2021,\nhe was a Senior Principal Research Scientist at\nMitsubishi Electric Research Laboratories (MERL),\nUSA. He is currently a Machine Learning Re\u0002searcher at Apple. His research interests include automatic speech recognition,\nspoken language understanding, and language modeling. He served as a\nmember of the IEEE Speech and Language Processing Technical Committee\n(2020\u20132022).\nPLACE\nPHOTO\nHERE\nTara Sainath received her PhD in Electrical Engi\u0002neering and Computer Science from MIT in 2009.\nThe main focus of her PhD work was in acoustic\nmodeling for noise robust speech recognition. After\nher PhD, she spent 5 years at the Speech and\nLanguage Algorithms group at IBM T.J. Watson Re\u0002search Center, before joining Google Research. She\nhas served as a Program Chair for ICLR in 2017 and\n2018. Also, she has co-organized numerous special\nsessions and workshops, including Interspeech 2010,\nICML 2013, Interspeech 2016 and ICML 2017. In\naddition, she is a member of the IEEE Speech and Language Processing\nTechnical Committee (SLTC) as well as the Associate Editor for IEEE/ACM\nTransactions on Audio, Speech, and Language Processing.\nPLACE\nPHOTO\nHERE\nRalf Schluter \u00a8 Ralf Schluter received his Dr.rer.nat. \u00a8\ndegree in Computer Science in 2000 and habilitated\nin Computer Science in 2019, both at RWTH Aachen\nUniversity. In May 1996, Ralf Schluter joined the \u00a8\nComputer Science Department at RWTH Aachen\nUniversity, where he currently is Lecturer and\nAcademic Director, leading the Automatic Speech\nRecognition Group at the Chair Computer Science\n6 \u2013 Machine Learning and Human Language Tech\u0002nology. In 2019, Ralf also joined AppTek GmbH\nAachen as Senior Researcher. His research interests\ncover sequence classification, specifically all aspects of automatic speech\nrecognition, decision theory, stochastic modeling, and signal analysis. Ralf\nserved as Subject Editor for Speech Communication (2013-2019).\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n29\nPLACE\nPHOTO\nHERE\nShinji Watanabe is an Associate Professor at\nCarnegie Mellon University, Pittsburgh, PA. He re\u0002ceived his B.S., M.S., and Ph.D. (Dr. Eng.) degrees\nfrom Waseda University, Tokyo, Japan. He was a\nresearch scientist at NTT Communication Science\nLaboratories, Kyoto, Japan, from 2001 to 2011, a\nvisiting scholar at Georgia institute of technology,\nAtlanta, GA, in 2009, and a senior principal research\nscientist at Mitsubishi Electric Research Laborato\u0002ries (MERL), Cambridge, MA USA from 2012 to\n2017. Before Carnegie Mellon University, he was\nan associate research professor at Johns Hopkins University, Baltimore,\nMD, USA, from 2017 to 2020. His research interests include automatic\nspeech recognition, speech enhancement, spoken language understanding, and\nmachine learning for speech and language processing. He is an IEEE and\nISCA Fellow.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
    "openalex_id": "https://openalex.org/W4388017359",
    "title": "End-to-End Speech Recognition: A Survey",
    "publication_date": "2024-01-01",
    "cited_by_count": 33,
    "topics": "Speech Recognition Technology, Statistical Machine Translation and Natural Language Processing, Audio Signal Classification and Analysis",
    "keywords": "End-to-End Speech Recognition, Automatic Speech Recognition, Acoustic Modeling, Environmental Sound Recognition, End-to-end principle, Deep Learning, Word error rate, Deep neural networks",
    "concepts": "Computer science, Hidden Markov model, Deep learning, Artificial neural network, Language model, Software deployment, Artificial intelligence, End-to-end principle, Speech recognition, Word error rate, Domain (mathematical analysis), Deep neural networks, Natural language processing, Machine learning, Mathematical analysis, Mathematics, Operating system",
    "pdf_urls_by_priority": [
      "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
      "https://arxiv.org/pdf/2303.03329"
    ],
    "text_type": "full_text",
    "successful_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "referenced_works": [
      "https://openalex.org/W108866686",
      "https://openalex.org/W1494198834",
      "https://openalex.org/W1501286448",
      "https://openalex.org/W1508165687",
      "https://openalex.org/W1553004968",
      "https://openalex.org/W1583239513",
      "https://openalex.org/W1587755118",
      "https://openalex.org/W1588735863",
      "https://openalex.org/W1710082047",
      "https://openalex.org/W179875071",
      "https://openalex.org/W1806891645",
      "https://openalex.org/W1904365287",
      "https://openalex.org/W1915251500",
      "https://openalex.org/W1922655562",
      "https://openalex.org/W1966812932",
      "https://openalex.org/W1975550806",
      "https://openalex.org/W1979136262",
      "https://openalex.org/W1985258458",
      "https://openalex.org/W1986184096",
      "https://openalex.org/W1988720110",
      "https://openalex.org/W1989674786",
      "https://openalex.org/W1991133427",
      "https://openalex.org/W2000200144",
      "https://openalex.org/W2001679125",
      "https://openalex.org/W2008554732",
      "https://openalex.org/W2014151772",
      "https://openalex.org/W2024539680",
      "https://openalex.org/W2033245860",
      "https://openalex.org/W2033565080",
      "https://openalex.org/W2046932483",
      "https://openalex.org/W2050526637",
      "https://openalex.org/W2056590938",
      "https://openalex.org/W2057653135",
      "https://openalex.org/W2064675550",
      "https://openalex.org/W206545267",
      "https://openalex.org/W2066378046",
      "https://openalex.org/W2078354939",
      "https://openalex.org/W2080213370",
      "https://openalex.org/W2091981305",
      "https://openalex.org/W2097927681",
      "https://openalex.org/W2100180150",
      "https://openalex.org/W2105482032",
      "https://openalex.org/W2105594594",
      "https://openalex.org/W2110798204",
      "https://openalex.org/W2114016253",
      "https://openalex.org/W2121879602",
      "https://openalex.org/W2125838338",
      "https://openalex.org/W2127095586",
      "https://openalex.org/W2127141656",
      "https://openalex.org/W2129545859",
      "https://openalex.org/W2131968858",
      "https://openalex.org/W2136617108",
      "https://openalex.org/W2136922672",
      "https://openalex.org/W2143564602",
      "https://openalex.org/W2143612262",
      "https://openalex.org/W2145249131",
      "https://openalex.org/W2150355110",
      "https://openalex.org/W2151058131",
      "https://openalex.org/W2151834591",
      "https://openalex.org/W2155368638",
      "https://openalex.org/W2157749010",
      "https://openalex.org/W2165712214",
      "https://openalex.org/W2166637769",
      "https://openalex.org/W2183341477",
      "https://openalex.org/W2242818861",
      "https://openalex.org/W2288217446",
      "https://openalex.org/W2291975472",
      "https://openalex.org/W2296073425",
      "https://openalex.org/W2327501763",
      "https://openalex.org/W2331143823",
      "https://openalex.org/W2394932179",
      "https://openalex.org/W2396464458",
      "https://openalex.org/W2402268235",
      "https://openalex.org/W2407080277",
      "https://openalex.org/W2408093180",
      "https://openalex.org/W2411921399",
      "https://openalex.org/W2471933213",
      "https://openalex.org/W2514741789",
      "https://openalex.org/W2516457973",
      "https://openalex.org/W2520160253",
      "https://openalex.org/W2525778437",
      "https://openalex.org/W2530876040",
      "https://openalex.org/W2545177271",
      "https://openalex.org/W2566563465",
      "https://openalex.org/W2577366047",
      "https://openalex.org/W2606722458",
      "https://openalex.org/W2608712415",
      "https://openalex.org/W2618530766",
      "https://openalex.org/W2627092829",
      "https://openalex.org/W2745439869",
      "https://openalex.org/W2746192915",
      "https://openalex.org/W2748816379",
      "https://openalex.org/W2750499125",
      "https://openalex.org/W2766219058",
      "https://openalex.org/W2787663903",
      "https://openalex.org/W2792376130",
      "https://openalex.org/W2799800213",
      "https://openalex.org/W2808640845",
      "https://openalex.org/W2808939837",
      "https://openalex.org/W2883586237",
      "https://openalex.org/W2886025712",
      "https://openalex.org/W2886180730",
      "https://openalex.org/W2886319145",
      "https://openalex.org/W2888779557",
      "https://openalex.org/W2888909726",
      "https://openalex.org/W2889129739",
      "https://openalex.org/W2889163603",
      "https://openalex.org/W2889187401",
      "https://openalex.org/W2889374926",
      "https://openalex.org/W2889504751",
      "https://openalex.org/W2892009249",
      "https://openalex.org/W2892124901",
      "https://openalex.org/W2899879954",
      "https://openalex.org/W2900209846",
      "https://openalex.org/W2904818793",
      "https://openalex.org/W2914018192",
      "https://openalex.org/W2915977493",
      "https://openalex.org/W2928941594",
      "https://openalex.org/W2933138175",
      "https://openalex.org/W2936123380",
      "https://openalex.org/W2936774411",
      "https://openalex.org/W2937402758",
      "https://openalex.org/W2937780860",
      "https://openalex.org/W2938348542",
      "https://openalex.org/W2939111082",
      "https://openalex.org/W2940180244",
      "https://openalex.org/W2943845043",
      "https://openalex.org/W2949975180",
      "https://openalex.org/W2951974815",
      "https://openalex.org/W2952992734",
      "https://openalex.org/W2953561564",
      "https://openalex.org/W2962699523",
      "https://openalex.org/W2962728618",
      "https://openalex.org/W2962742956",
      "https://openalex.org/W2962745521",
      "https://openalex.org/W2962760690",
      "https://openalex.org/W2962784628",
      "https://openalex.org/W2962824709",
      "https://openalex.org/W2962826786",
      "https://openalex.org/W2963022149",
      "https://openalex.org/W2963026768",
      "https://openalex.org/W2963088785",
      "https://openalex.org/W2963144852",
      "https://openalex.org/W2963211739",
      "https://openalex.org/W2963240019",
      "https://openalex.org/W2963260202",
      "https://openalex.org/W2963303028",
      "https://openalex.org/W2963382396",
      "https://openalex.org/W2963431393",
      "https://openalex.org/W2963506925",
      "https://openalex.org/W2963571336",
      "https://openalex.org/W2963739817",
      "https://openalex.org/W2963747784",
      "https://openalex.org/W2964012862",
      "https://openalex.org/W2964103964",
      "https://openalex.org/W2964107261",
      "https://openalex.org/W2964110616",
      "https://openalex.org/W2970692082",
      "https://openalex.org/W2971840980",
      "https://openalex.org/W2972451902",
      "https://openalex.org/W2972528057",
      "https://openalex.org/W2972621414",
      "https://openalex.org/W2972625221",
      "https://openalex.org/W2972630480",
      "https://openalex.org/W2972692349",
      "https://openalex.org/W2972780808",
      "https://openalex.org/W2972799770",
      "https://openalex.org/W2972837679",
      "https://openalex.org/W2972889948",
      "https://openalex.org/W2972953886",
      "https://openalex.org/W2972977747",
      "https://openalex.org/W2972995428",
      "https://openalex.org/W2973122799",
      "https://openalex.org/W2981857663",
      "https://openalex.org/W2987019345",
      "https://openalex.org/W2995181338",
      "https://openalex.org/W2997617958",
      "https://openalex.org/W3005302685",
      "https://openalex.org/W3007328579",
      "https://openalex.org/W3007528493",
      "https://openalex.org/W3008037978",
      "https://openalex.org/W3008174054",
      "https://openalex.org/W3008191852",
      "https://openalex.org/W3008284571",
      "https://openalex.org/W3008525923",
      "https://openalex.org/W3008762051",
      "https://openalex.org/W3008898571",
      "https://openalex.org/W3008912312",
      "https://openalex.org/W3011339933",
      "https://openalex.org/W3015190365",
      "https://openalex.org/W3015194534",
      "https://openalex.org/W3015369343",
      "https://openalex.org/W3015383801",
      "https://openalex.org/W3015501067",
      "https://openalex.org/W3015671919",
      "https://openalex.org/W3015686596",
      "https://openalex.org/W3015726069",
      "https://openalex.org/W3015927303",
      "https://openalex.org/W3015974384",
      "https://openalex.org/W3015995734",
      "https://openalex.org/W3016010032",
      "https://openalex.org/W3016053754",
      "https://openalex.org/W3016167541",
      "https://openalex.org/W3016234571",
      "https://openalex.org/W3017474798",
      "https://openalex.org/W3026041220",
      "https://openalex.org/W3028545098",
      "https://openalex.org/W3034775979",
      "https://openalex.org/W3092122846",
      "https://openalex.org/W3094667432",
      "https://openalex.org/W3094713728",
      "https://openalex.org/W3094957294",
      "https://openalex.org/W3095173472",
      "https://openalex.org/W3095189764",
      "https://openalex.org/W3095376166",
      "https://openalex.org/W3095697114",
      "https://openalex.org/W3096032230",
      "https://openalex.org/W3096160024",
      "https://openalex.org/W3096215352",
      "https://openalex.org/W3097747488",
      "https://openalex.org/W3097777922",
      "https://openalex.org/W3097882114",
      "https://openalex.org/W3097973766",
      "https://openalex.org/W3100910367",
      "https://openalex.org/W3103005696",
      "https://openalex.org/W3105532142",
      "https://openalex.org/W3147187328",
      "https://openalex.org/W3147414526",
      "https://openalex.org/W3148001440",
      "https://openalex.org/W3148654612",
      "https://openalex.org/W3151269043",
      "https://openalex.org/W3152221657",
      "https://openalex.org/W3160551958",
      "https://openalex.org/W3160766462",
      "https://openalex.org/W3161375121",
      "https://openalex.org/W3161873870",
      "https://openalex.org/W3162249256",
      "https://openalex.org/W3162665866",
      "https://openalex.org/W3163203022",
      "https://openalex.org/W3163300396",
      "https://openalex.org/W3163793923",
      "https://openalex.org/W3163839574",
      "https://openalex.org/W3163842339",
      "https://openalex.org/W3167895882",
      "https://openalex.org/W3170405627",
      "https://openalex.org/W3197140813",
      "https://openalex.org/W3197304116",
      "https://openalex.org/W3197478142",
      "https://openalex.org/W3197507772",
      "https://openalex.org/W3197976839",
      "https://openalex.org/W3197991202",
      "https://openalex.org/W3198116002",
      "https://openalex.org/W3198439131",
      "https://openalex.org/W3198442913",
      "https://openalex.org/W3198455051",
      "https://openalex.org/W3198654230",
      "https://openalex.org/W3202184514",
      "https://openalex.org/W3202419788",
      "https://openalex.org/W3204696009",
      "https://openalex.org/W3205201903",
      "https://openalex.org/W3205644108",
      "https://openalex.org/W3206573929",
      "https://openalex.org/W3206876927",
      "https://openalex.org/W3207222250",
      "https://openalex.org/W3209059054",
      "https://openalex.org/W3211040052",
      "https://openalex.org/W3211278025",
      "https://openalex.org/W4206410067",
      "https://openalex.org/W4210463634",
      "https://openalex.org/W4221155340",
      "https://openalex.org/W4223622550",
      "https://openalex.org/W4224518768",
      "https://openalex.org/W4225319488",
      "https://openalex.org/W4225334634",
      "https://openalex.org/W4226120743",
      "https://openalex.org/W4240908132",
      "https://openalex.org/W4288290348",
      "https://openalex.org/W4297781872",
      "https://openalex.org/W4299649720",
      "https://openalex.org/W4319862255",
      "https://openalex.org/W4319862408",
      "https://openalex.org/W4319862418",
      "https://openalex.org/W4319862474",
      "https://openalex.org/W4319862683",
      "https://openalex.org/W4372259859",
      "https://openalex.org/W4378501656",
      "https://openalex.org/W4381827575",
      "https://openalex.org/W4383605108",
      "https://openalex.org/W4385245566",
      "https://openalex.org/W4394662461",
      "https://openalex.org/W4473315",
      "https://openalex.org/W66978610",
      "https://openalex.org/W98857008"
    ],
    "openalex_rank": 5,
    "num_tokens": 51849,
    "url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf"
  }
]