
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
    Find datasets for benchmarking an AI-powered deep search and discovery engine capable of expert-level research synthesis and multi-hop reasoning across diverse academic fields. The ideal dataset should include:

1) Complex, multi-part research questions requiring in-depth analysis
2) Long-form answers or full academic papers as ground truth
3) Multi-hop reasoning challenges necessitating information synthesis from multiple sources
4) Coverage of various scientific domains to test generalizability
5) Metrics for evaluating factual accuracy, coherence, and depth of research
6) Potential for assessing novel insight generation in academic contexts
7) Mixture of structured and unstructured data to mimic real-world research scenarios
8) Sufficient scale to test large language model capabilities (>10,000 question-answer pairs)
9) Inclusion of citation or source attribution for fact-checking
10) Compatibility with existing NLP benchmarks for comparative evaluation

Exclude simple question-answering datasets like SQuAD or TriviaQA. Priority given to datasets derived from peer-reviewed academic literature, scientific journals, or curated by domain experts. Consider both publicly available datasets and those requiring academic access or collaboration.
</title>
            <style>
                body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; }
                h1, h2, h3 { color: #333; }
                .nav-buttons { margin-bottom: 20px; }
                .nav-buttons button { margin-right: 10px; }
                .citation-tree { margin-top: 20px; }
                .citation-tree ul { list-style-type: none; }
                .citation-tree li { margin: 10px 0; }
                .back-to-top { text-decoration: none; color: #0066cc; }
            </style>
            <script>
                const answers = ["<p><strong>Datasets for Benchmarking AI-Powered Research Synthesis</strong></p>\n<p>To effectively benchmark an AI-powered deep search and discovery engine for expert-level research synthesis and multi-hop reasoning, several datasets have been identified that meet the specified criteria:</p>\n<h3>Key Datasets:</h3>\n<ul>\n<li>\n<p><strong>2WikiMultiHopQA</strong>: This dataset is comprehensive for evaluating multi-hop reasoning, requiring models to process multiple paragraphs and includes both structured and unstructured data for reasoning paths <a href=\"#tree-S2024664967\" class=\"citation\" id=\"cite-S2024664967\">[S2024664967]</a>.</p>\n</li>\n<li>\n<p><strong>Qasper</strong>: Contains 5,000 information-seeking questions and answers derived from scientific papers, making it a valuable resource for evaluating AI research synthesis <a href=\"#tree-S6354951537\" class=\"citation\" id=\"cite-S6354951537\">[S6354951537]</a>.</p>\n</li>\n<li>\n<p><strong>EBM-NLP</strong>: Offers approximately 5,000 annotated abstracts of randomized controlled trial reports, facilitating the development of automated extraction models for research synthesis <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</p>\n</li>\n<li>\n<p><strong>Systematic Review Toolbox</strong>: Provides a catalogue of tools that assist in systematic reviews and evidence synthesis, relevant for evaluating AI research synthesis <a href=\"#tree-S7667005754\" class=\"citation\" id=\"cite-S7667005754\">[S7667005754]</a>.</p>\n</li>\n</ul>\n<h3>Considerations for Dataset Selection:</h3>\n<ul>\n<li>\n<p><strong>Multi-Hop Reasoning</strong>: Existing datasets like WikiHop and HotpotQA may not effectively challenge multi-hop reasoning capabilities, indicating a need for improved dataset design <a href=\"#tree-S1798317770\" class=\"citation\" id=\"cite-S1798317770\">[S1798317770]</a>.</p>\n</li>\n<li>\n<p><strong>Complexity and Coverage</strong>: The datasets should encompass complex, multi-part research questions across various scientific domains to ensure generalizability <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</p>\n</li>\n<li>\n<p><strong>Evaluation Metrics</strong>: It is crucial to have metrics for assessing factual accuracy, coherence, and depth of research to evaluate the performance of AI models <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</p>\n</li>\n<li>\n<p><strong>Scale and Structure</strong>: Datasets should be sufficiently large (&gt;10,000 question-answer pairs) and include a mixture of structured and unstructured data to mimic real-world research scenarios <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</p>\n</li>\n<li>\n<p><strong>Citation and Attribution</strong>: Inclusion of citation or source attribution is essential for fact-checking and validating the information synthesized by AI models <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</p>\n</li>\n</ul>\n<h3>Conclusion:</h3>\n<p>The identified datasets, such as 2WikiMultiHopQA, Qasper, and EBM-NLP, provide a strong foundation for benchmarking AI capabilities in research synthesis and multi-hop reasoning. They collectively address the need for complexity, coverage, and evaluation metrics, while also emphasizing the importance of citation for factual accuracy.</p>", "<p><strong>Datasets for Benchmarking AI-Powered Research Synthesis</strong></p>\n<p>To effectively benchmark an AI-powered deep search and discovery engine capable of expert-level research synthesis and multi-hop reasoning, several datasets are recommended based on their features and relevance:</p>\n<h3>Recommended Datasets:</h3>\n<ol>\n<li><strong>Qasper Dataset</strong>  </li>\n<li>\n<p>Contains 5,000 information-seeking questions and answers derived from scientific literature, making it valuable for evaluating AI research synthesis <a href=\"#tree-S6354951537\" class=\"citation\" id=\"cite-S6354951537\">[S6354951537]</a> <a href=\"#tree-S7194843350\" class=\"citation\" id=\"cite-S7194843350\">[S7194843350]</a>.</p>\n</li>\n<li>\n<p><strong>EBM-NLP Dataset</strong>  </p>\n</li>\n<li>\n<p>Comprises approximately 5,000 annotated abstracts of randomized controlled trial reports, facilitating automated extraction models for research synthesis <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</p>\n</li>\n<li>\n<p><strong>AI2 Reasoning Challenge (ARC)</strong>  </p>\n</li>\n<li>\n<p>A comprehensive resource for evaluating multi-domain reasoning capabilities, featuring multiple-choice questions that require varying levels of reasoning across scientific disciplines <a href=\"#tree-S3746529778\" class=\"citation\" id=\"cite-S3746529778\">[S3746529778]</a>.</p>\n</li>\n<li>\n<p><strong>LogiQA Dataset</strong>  </p>\n</li>\n<li>\n<p>Contains 8,678 question-answer pairs that challenge models to demonstrate deductive reasoning skills, useful for assessing multi-hop reasoning <a href=\"#tree-S2500722700\" class=\"citation\" id=\"cite-S2500722700\">[S2500722700]</a>.</p>\n</li>\n<li>\n<p><strong>2WikiMultiHopQA Dataset</strong>  </p>\n</li>\n<li>A comprehensive resource for evaluating multi-hop reasoning, requiring models to read multiple paragraphs and includes both structured and unstructured data <a href=\"#tree-S2024664967\" class=\"citation\" id=\"cite-S2024664967\">[S2024664967]</a>.</li>\n</ol>\n<h3>Key Features of Ideal Datasets:</h3>\n<ul>\n<li><strong>Complex Research Questions</strong>: Datasets should include complex, multi-part questions requiring in-depth analysis <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n<li><strong>Long-form Answers</strong>: Inclusion of full academic papers or long-form answers as ground truth is essential <a href=\"#tree-S6354951537\" class=\"citation\" id=\"cite-S6354951537\">[S6354951537]</a>.</li>\n<li><strong>Multi-hop Reasoning</strong>: Datasets must present challenges that necessitate synthesizing information from multiple sources <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n<li><strong>Diverse Scientific Domains</strong>: Coverage across various scientific fields is crucial for testing the generalizability of AI models <a href=\"#tree-S7208994926\" class=\"citation\" id=\"cite-S7208994926\">[S7208994926]</a>.</li>\n<li><strong>Evaluation Metrics</strong>: Datasets should provide metrics for evaluating factual accuracy, coherence, and depth of research <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n<li><strong>Novel Insight Generation</strong>: Potential for assessing the generation of novel insights in academic contexts is important <a href=\"#tree-S6354951537\" class=\"citation\" id=\"cite-S6354951537\">[S6354951537]</a>.</li>\n<li><strong>Structured and Unstructured Data</strong>: A mixture of data types to mimic real-world research scenarios enhances the dataset's applicability <a href=\"#tree-S2024664967\" class=\"citation\" id=\"cite-S2024664967\">[S2024664967]</a>.</li>\n<li><strong>Scale</strong>: Datasets should contain more than 10,000 question-answer pairs to adequately test large language model capabilities <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n<li><strong>Citation Inclusion</strong>: Including citation or source attribution is vital for fact-checking <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n<li><strong>NLP Benchmark Compatibility</strong>: Datasets should be compatible with existing NLP benchmarks for comparative evaluation <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n</ul>\n<h3>Conclusion:</h3>\n<p>These datasets provide a robust foundation for benchmarking AI systems in research synthesis and multi-hop reasoning across diverse academic fields. They are derived from peer-reviewed literature and curated by domain experts, ensuring high quality and relevance for AI research applications.</p>", "<h3>Datasets for Benchmarking AI-Powered Research Synthesis</h3>\n<p>To effectively benchmark an AI-powered deep search and discovery engine capable of expert-level research synthesis and multi-hop reasoning, several datasets have been identified that meet the necessary criteria:</p>\n<h4>Key Datasets</h4>\n<ul>\n<li><strong>Qasper Dataset</strong>: Contains 5,000 information-seeking questions and answers derived from scientific literature, making it valuable for evaluating AI research synthesis capabilities <a href=\"#tree-S6354951537\" class=\"citation\" id=\"cite-S6354951537\">[S6354951537]</a> <a href=\"#tree-S7194843350\" class=\"citation\" id=\"cite-S7194843350\">[S7194843350]</a>.</li>\n<li><strong>EBM-NLP Dataset</strong>: Comprises approximately 5,000 annotated abstracts of randomized controlled trial reports, facilitating the development of automated extraction models for research synthesis <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n<li><strong>AI2 Reasoning Challenge (ARC)</strong>: A comprehensive resource for evaluating multi-domain reasoning capabilities, featuring multiple-choice questions that require varying levels of reasoning across scientific disciplines <a href=\"#tree-S3746529778\" class=\"citation\" id=\"cite-S3746529778\">[S3746529778]</a> <a href=\"#tree-S2311603242\" class=\"citation\" id=\"cite-S2311603242\">[S2311603242]</a>.</li>\n<li><strong>LogiQA</strong>: This dataset includes 8,678 question-answer pairs designed to evaluate logical reasoning in AI systems, challenging models to demonstrate deductive reasoning skills <a href=\"#tree-S2500722700\" class=\"citation\" id=\"cite-S2500722700\">[S2500722700]</a>.</li>\n<li><strong>2WikiMultiHopQA</strong>: A dataset that evaluates multi-hop reasoning, requiring models to read multiple paragraphs and integrate structured and unstructured data for reasoning paths <a href=\"#tree-S2024664967\" class=\"citation\" id=\"cite-S2024664967\">[S2024664967]</a>.</li>\n</ul>\n<h4>Evaluation Metrics</h4>\n<ul>\n<li>Datasets should include metrics for evaluating factual accuracy, coherence, and depth of research <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n<li>The potential for assessing novel insight generation in academic contexts is crucial for understanding AI capabilities in synthesizing research <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n</ul>\n<h4>Coverage and Generalizability</h4>\n<ul>\n<li>The datasets must cover various scientific domains to test the generalizability of the AI models <a href=\"#tree-S7208994926\" class=\"citation\" id=\"cite-S7208994926\">[S7208994926]</a>.</li>\n<li>Inclusion of citation or source attribution is necessary for fact-checking and ensuring the reliability of the synthesized information <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n</ul>\n<h4>Data Structure</h4>\n<ul>\n<li>A mixture of structured and unstructured data is essential to mimic real-world research scenarios, enhancing the relevance of the datasets for practical applications <a href=\"#tree-S2024664967\" class=\"citation\" id=\"cite-S2024664967\">[S2024664967]</a>.</li>\n</ul>\n<h4>Scale Requirements</h4>\n<ul>\n<li>The datasets should be sufficiently large, ideally containing more than 10,000 question-answer pairs, to adequately test the capabilities of large language models <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n</ul>\n<h3>Conclusion</h3>\n<p>These datasets represent a robust foundation for benchmarking AI-powered research synthesis and multi-hop reasoning across diverse academic fields, ensuring comprehensive evaluation and development of AI capabilities in this domain.</p>", "<p><strong>Datasets for Benchmarking AI-Powered Research Synthesis</strong></p>\n<p>To effectively benchmark an AI-powered deep search and discovery engine capable of expert-level research synthesis and multi-hop reasoning across diverse academic fields, several datasets are particularly relevant:</p>\n<h3>Key Datasets:</h3>\n<ul>\n<li><strong>Qasper Dataset</strong>: Contains 5,000 information-seeking questions and answers derived from scientific literature, making it valuable for evaluating AI research synthesis capabilities <a href=\"#tree-S6354951537\" class=\"citation\" id=\"cite-S6354951537\">[S6354951537]</a> <a href=\"#tree-S7194843350\" class=\"citation\" id=\"cite-S7194843350\">[S7194843350]</a>.</li>\n<li><strong>EBM-NLP Dataset</strong>: Comprises approximately 5,000 annotated abstracts from randomized controlled trial reports, facilitating automated extraction models for research synthesis <a href=\"#tree-S9198903639\" class=\"citation\" id=\"cite-S9198903639\">[S9198903639]</a>.</li>\n<li><strong>AI2 Reasoning Challenge (ARC)</strong>: A comprehensive resource featuring 7,787 grade-school level, multiple-choice science questions, which evaluates multi-domain reasoning capabilities in AI <a href=\"#tree-S2311603242\" class=\"citation\" id=\"cite-S2311603242\">[S2311603242]</a> <a href=\"#tree-S3746529778\" class=\"citation\" id=\"cite-S3746529778\">[S3746529778]</a>.</li>\n<li><strong>LogiQA</strong>: Contains 8,678 question-answer pairs that challenge models to demonstrate deductive reasoning skills, essential for multi-hop reasoning evaluation <a href=\"#tree-S2500722700\" class=\"citation\" id=\"cite-S2500722700\">[S2500722700]</a>.</li>\n<li><strong>2WikiMultiHopQA</strong>: A dataset designed for evaluating multi-hop reasoning, requiring models to read multiple paragraphs and integrate structured and unstructured data <a href=\"#tree-S2024664967\" class=\"citation\" id=\"cite-S2024664967\">[S2024664967]</a>.</li>\n</ul>\n<h3>Evaluation Metrics:</h3>\n<ul>\n<li>Datasets should include metrics for evaluating factual accuracy, coherence, and depth of research, ensuring that AI systems can be rigorously assessed <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n<li>Potential for assessing novel insight generation in academic contexts is also crucial, as it reflects the AI's ability to synthesize information creatively <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n</ul>\n<h3>Interdisciplinary and Generalizability Aspects:</h3>\n<ul>\n<li>Datasets should cover various scientific domains to test the generalizability of AI models across disciplines <a href=\"#tree-S7208994926\" class=\"citation\" id=\"cite-S7208994926\">[S7208994926]</a>.</li>\n<li>Datasets adhering to the FAIR principles (Findable, Accessible, Interoperable, and Reusable) are vital for effective AI research synthesis <a href=\"#tree-S9313841483\" class=\"citation\" id=\"cite-S9313841483\">[S9313841483]</a>.</li>\n</ul>\n<h3>Data Characteristics:</h3>\n<ul>\n<li>A mixture of structured and unstructured data is necessary to mimic real-world research scenarios, enhancing the applicability of AI models <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n<li>The datasets should be sufficiently large (&gt;10,000 question-answer pairs) to effectively test the capabilities of large language models <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n</ul>\n<h3>Citation and Source Attribution:</h3>\n<ul>\n<li>Inclusion of citation or source attribution for fact-checking is essential to ensure the reliability of the information synthesized by AI systems <a href=\"#tree-S4715449019\" class=\"citation\" id=\"cite-S4715449019\">[S4715449019]</a>.</li>\n</ul>\n<h3>Exclusions:</h3>\n<ul>\n<li>Simple question-answering datasets like SQuAD or TriviaQA should be excluded, as they do not meet the complexity requirements for multi-hop reasoning and in-depth analysis <a href=\"#tree-S1798317770\" class=\"citation\" id=\"cite-S1798317770\">[S1798317770]</a>.</li>\n</ul>\n<p>In conclusion, leveraging these datasets will provide a robust foundation for benchmarking AI-powered research synthesis engines, ensuring they can handle complex, interdisciplinary research questions effectively.</p>"];
                const answerCitations = [["S2024664967", "S6354951537", "S9198903639", "S7667005754", "S1798317770", "S4715449019", "S4715449019", "S4715449019", "S4715449019"], ["S6354951537", "S7194843350", "S9198903639", "S3746529778", "S2500722700", "S2024664967", "S4715449019", "S6354951537", "S4715449019", "S7208994926", "S9198903639", "S6354951537", "S2024664967", "S9198903639", "S9198903639", "S9198903639"], ["S6354951537", "S7194843350", "S9198903639", "S3746529778", "S2311603242", "S2500722700", "S2024664967", "S9198903639", "S9198903639", "S7208994926", "S9198903639", "S2024664967", "S9198903639"], ["S6354951537", "S7194843350", "S9198903639", "S2311603242", "S3746529778", "S2500722700", "S2024664967", "S4715449019", "S4715449019", "S7208994926", "S9313841483", "S4715449019", "S4715449019", "S4715449019", "S1798317770"]];
                let currentIndex = 3;

                function showAnswer(index) {
                    if (index >= 0 && index < answers.length) {
                        document.getElementById('answer-content').innerHTML = answers[index];
                        document.getElementById('current-answer-index').textContent = `Answer ${index + 1} of ${answers.length}`;
                        
                        document.getElementById('prev-btn').disabled = (index === 0);
                        document.getElementById('next-btn').disabled = (index === answers.length - 1);
                        currentIndex = index;

                        // Update citation trees
                        const citationTrees = document.querySelectorAll('.citation-tree');
                        citationTrees.forEach(tree => {
                            const treeId = tree.id.replace('tree-', '');
                            if (answerCitations[index].includes(treeId)) {
                                tree.style.display = 'block';
                            } else {
                                tree.style.display = 'none';
                            }
                        });
                    }
                }

                window.onload = function() {
                    showAnswer(currentIndex);
                };
            </script>
        </head>
        <body>
            <h1>
    Find datasets for benchmarking an AI-powered deep search and discovery engine capable of expert-level research synthesis and multi-hop reasoning across diverse academic fields. The ideal dataset should include:

1) Complex, multi-part research questions requiring in-depth analysis
2) Long-form answers or full academic papers as ground truth
3) Multi-hop reasoning challenges necessitating information synthesis from multiple sources
4) Coverage of various scientific domains to test generalizability
5) Metrics for evaluating factual accuracy, coherence, and depth of research
6) Potential for assessing novel insight generation in academic contexts
7) Mixture of structured and unstructured data to mimic real-world research scenarios
8) Sufficient scale to test large language model capabilities (>10,000 question-answer pairs)
9) Inclusion of citation or source attribution for fact-checking
10) Compatibility with existing NLP benchmarks for comparative evaluation

Exclude simple question-answering datasets like SQuAD or TriviaQA. Priority given to datasets derived from peer-reviewed academic literature, scientific journals, or curated by domain experts. Consider both publicly available datasets and those requiring academic access or collaboration.
</h1>
            <h2>Generated Answers</h2>
            <div class="nav-buttons">
                <button id="prev-btn" onclick="showAnswer(currentIndex - 1)">Previous</button>
                <span id="current-answer-index">Answer 4 of 4</span>
                <button id="next-btn" onclick="showAnswer(currentIndex + 1)">Next</button>
            </div>
            <div id="answer-content"></div>
            <h2>Citation Trees</h2>
            <div class="citation-tree" id="tree-S2024664967"><h3>Citation Tree for Statement S2024664967</h3><ul><li id="tree-S2024664967"><strong>S2024664967:</strong> The 2WikiMultiHopQA dataset is a comprehensive resource for evaluating multi-hop reasoning in AI, as it requires models to read multiple paragraphs and includes structured and unstructured data for reasoning paths.<ul><li id="tree-E7575733223"><strong>E7575733223:</strong> A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Abstract:A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning.<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li><li id="tree-E5542306969"><strong>E5542306969:</strong> In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model.<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li><li id="tree-E0591823375"><strong>E0591823375:</strong> Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data.<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li></ul></li></ul><a href="#cite-S2024664967" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S1798317770"><h3>Citation Tree for Statement S1798317770</h3><ul><li id="tree-S1798317770"><strong>S1798317770:</strong> Existing datasets like WikiHop and HotpotQA, while designed for multi-hop reasoning, may not effectively require such reasoning, suggesting a need for improved dataset design in AI research.<ul><li id="tree-E2147018756"><strong>E2147018756:</strong> Ideally, a model should not be able to perform well on a multi-hop question answering task without doing multi-hop reasoning. In this paper, we investigate two recently proposed datasets, WikiHop and HotpotQA. First, we explore sentence-factored models for these tasks; by design, these models cannot do multi-hop reasoning, but they are still able to solve a large number of examples in both datasets.<br><a href="https://aclanthology.org/N19-1405/" target="_blank">https://aclanthology.org/N19-1405/</a></li><li id="tree-E5737666655"><strong>E5737666655:</strong> Learning multi-hop reasoning has been a key challenge for reading comprehension models, leading to the design of datasets that explicitly focus on it. Ideally, a model should not be able to perform well on a multi-hop question answering task without doing multi-hop reasoning.<br><a href="https://aclanthology.org/N19-1405/" target="_blank">https://aclanthology.org/N19-1405/</a></li><li id="tree-E0616144519"><strong>E0616144519:</strong> This capability could unlock new applications in areas such as open-domain question answering, conversational AI, and hypothesis generation for complex tasks. Any domain requiring dynamic synthesis of evidence andfacts could benefit. Multi-hop reasoning also remains a challenge for current AI systems, suggesting continued research is needed to achieve more advanced, human-like reasoning.<br><a href="https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning" target="_blank">https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning</a></li></ul></li></ul><a href="#cite-S1798317770" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S4715449019"><h3>Citation Tree for Statement S4715449019</h3><ul><li id="tree-S4715449019"><strong>S4715449019:</strong> Multi-hop reasoning remains a challenge for AI systems, necessitating datasets that can robustly evaluate reasoning capabilities across various contexts and complexities.<ul><li id="tree-E4902332994"><strong>E4902332994:</strong> Abstract:A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer.<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li><li id="tree-E0616144519"><strong>E0616144519:</strong> This capability could unlock new applications in areas such as open-domain question answering, conversational AI, and hypothesis generation for complex tasks. Any domain requiring dynamic synthesis of evidence andfacts could benefit. Multi-hop reasoning also remains a challenge for current AI systems, suggesting continued research is needed to achieve more advanced, human-like reasoning.<br><a href="https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning" target="_blank">https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning</a></li><li id="tree-E0314530154"><strong>E0314530154:</strong> The ability to consult context, combine disparate information sources, and make logical connections is essential for robust intelligence. Multi-hop reasoning research aims to move beyond systems that simply retrieve facts, towards models that can comprehend relationships, draw inferences, and bring together diverse knowledge to solve problems.<br><a href="https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning" target="_blank">https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning</a></li></ul></li></ul><a href="#cite-S4715449019" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S6354951537"><h3>Citation Tree for Statement S6354951537</h3><ul><li id="tree-S6354951537"><strong>S6354951537:</strong> The Qasper dataset is a valuable resource for evaluating AI research synthesis, containing 5,000 information-seeking questions and answers based on scientific papers.<ul><li id="tree-E0290570856"><strong>E0290570856:</strong> 5K information-seeking questions over 1.5K scientific papers. Each question is asked by an expert researcher and answered by a different expert researcher using supporting evidence from the paper&#x27;s full text. Qasper has been included in long-context benchmarks such as SCROLLS. ... 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset ... 5K information-seeking questions over 1.5K scientific papers. Each question is asked by an expert researcher and answered by a different expert researcher using supporting evidence from the paper's full text. Qasper has been included in long-context benchmarks such as SCROLLS. ... 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. WinoGrande is a collection of 44K problems, inspired by Winograd Schema Challenge, but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning. ... 137K instruction-following demonstrations for 54 scientific literature understanding tasks. The tasks cover five essential scientific literature categories and span five domains. ... Instruction data collected for writing paragraph-level answers to multiple document-grounded NLP research questions. It aims to facilitate research and development of tools for text mining over academic text. ... A collection of over 200M paper titles, abstracts, citations, and other metadata of open-access papers from the Semantic Scholar Academic Graph. ... A challenge dataset of questions that are trivial for humans (>95% accuracy) but that state-of-the-art models struggle with (<48%), created through a collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-E8032253673"><strong>E8032253673:</strong> 5K information-seeking questions over 1.5K scientific papers. Each question is asked by an expert researcher and answered by a different expert researcher using supporting evidence from the paper&#x27;s full text. Qasper has been included in long-context benchmarks such as SCROLLS. ... 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset ...<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li></ul></li></ul><a href="#cite-S6354951537" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S9198903639"><h3>Citation Tree for Statement S9198903639</h3><ul><li id="tree-S9198903639"><strong>S9198903639:</strong> The EBM-NLP dataset offers approximately 5,000 annotated abstracts of randomized controlled trial reports, facilitating the development of automated extraction models for research synthesis.<ul><li id="tree-E4290222644"><strong>E4290222644:</strong> Recently, Nye et al. released the EBM-NLP dataset [23], which comprises ~ 5000 abstracts of RCT reports manually annotated in detail. This may provide training data helpful for moving automated extraction models forward. Although software tools that support the data synthesis component of reviews have long existed (especially for performing meta-analysis), methods for automating this are beyond the capabilities of currently available ML and NLP tools. Nonetheless, research into these areas continues rapidly, and computational methods may allow new forms of synthesis unachievable manually, particularly around visualization [37, 38] and automatic summarization [39, 40] of large volumes of research evidence.<br><a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9" target="_blank">https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9</a></li><li id="tree-E5855592299"><strong>E5855592299:</strong> Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. In ML, one writes programs that specify parameterized models to perform particular tasks; these parameters are then estimated using (ideally large) datasets. In practice, ML methods resemble statistical models used in epidemiological research (e.g. logistic regression is a common method in both disciplines). Recently, Nye et al. released the EBM-NLP dataset [23], which comprises ~ 5000 abstracts of RCT reports manually annotated in detail. This may provide training data helpful for moving automated extraction models forward. Although software tools that support the data synthesis component of reviews have long existed (especially for performing meta-analysis), methods for automating this are beyond the capabilities of currently available ML and NLP tools. Nonetheless, research into these areas continues rapidly, and computational methods may allow new forms of synthesis unachievable manually, particularly around visualization [37, 38] and automatic summarization [39, 40] of large volumes of research evidence. The torrential volume of unstructured published evidence has rendered existing (rigorous, but manual) approaches to evidence synthesis increasingly costly and impractical. Consequently, researchers have developed methods that aim to semi-automate different steps of the evidence synthesis pipeline via machine learning. Most of the tools we encountered were written by academic groups involved in research into evidence synthesis and machine learning. Very often, these groups have produced prototype software to demonstrate a method.<br><a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9" target="_blank">https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9</a></li></ul></li></ul><a href="#cite-S9198903639" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S7667005754"><h3>Citation Tree for Statement S7667005754</h3><ul><li id="tree-S7667005754"><strong>S7667005754:</strong> The Systematic Review Toolbox provides a catalogue of tools that assist in systematic reviews and evidence synthesis, relevant for evaluating AI research synthesis.<ul><li id="tree-E6675849286"><strong>E6675849286:</strong> This guide brings together information and guidance on effective searching for journal articles and grey literature for those undertaking a systematic review, scoping review or other evidence synthesis<br><a href="https://libguides.kcl.ac.uk/systematicreview/ai" target="_blank">https://libguides.kcl.ac.uk/systematicreview/ai</a></li><li id="tree-E3625623377"><strong>E3625623377:</strong> The system is trained to find key information from scientific clinical trial publications, namely the descriptions of the trial's interventions, population, outcome measures, funding sources, and other critical characteristics. Please note you will need to request a free account. ... RobotReviewer is a machine learning system which aims to support evidence synthesis.<br><a href="https://libguides.kcl.ac.uk/systematicreview/ai" target="_blank">https://libguides.kcl.ac.uk/systematicreview/ai</a></li></ul></li></ul><a href="#cite-S7667005754" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S8897963012"><h3>Citation Tree for Statement S8897963012</h3><ul><li id="tree-S8897963012"><strong>S8897963012:</strong> Compiling representative test datasets is essential for evaluating AI solutions in pathology, underscoring the need for high-quality datasets in research synthesis.<ul><li id="tree-E5855592299"><strong>E5855592299:</strong> Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. In ML, one writes programs that specify parameterized models to perform particular tasks; these parameters are then estimated using (ideally large) datasets. In practice, ML methods resemble statistical models used in epidemiological research (e.g. logistic regression is a common method in both disciplines). Recently, Nye et al. released the EBM-NLP dataset [23], which comprises ~ 5000 abstracts of RCT reports manually annotated in detail. This may provide training data helpful for moving automated extraction models forward. Although software tools that support the data synthesis component of reviews have long existed (especially for performing meta-analysis), methods for automating this are beyond the capabilities of currently available ML and NLP tools. Nonetheless, research into these areas continues rapidly, and computational methods may allow new forms of synthesis unachievable manually, particularly around visualization [37, 38] and automatic summarization [39, 40] of large volumes of research evidence. The torrential volume of unstructured published evidence has rendered existing (rigorous, but manual) approaches to evidence synthesis increasingly costly and impractical. Consequently, researchers have developed methods that aim to semi-automate different steps of the evidence synthesis pipeline via machine learning. Most of the tools we encountered were written by academic groups involved in research into evidence synthesis and machine learning. Very often, these groups have produced prototype software to demonstrate a method.<br><a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9" target="_blank">https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9</a></li><li id="tree-E3741730989"><strong>E3741730989:</strong> In ML, one writes programs that specify parameterized models to perform particular tasks; these parameters are then estimated using (ideally large) datasets. In practice, ML methods resemble statistical models used in epidemiological research (e.g. logistic regression is a common method in both disciplines).<br><a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9" target="_blank">https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9</a></li></ul></li></ul><a href="#cite-S8897963012" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S7208994926"><h3>Citation Tree for Statement S7208994926</h3><ul><li id="tree-S7208994926"><strong>S7208994926:</strong> Interdisciplinary datasets are essential for evaluating AI research synthesis, as they enable insights from various scientific domains to inform AI development and evaluation methodologies.<ul><li id="tree-E9048570669"><strong>E9048570669:</strong> The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, p... To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our recommendations are a result of in-person discussions within a diverse group of researchers, educators, and students, during a 3-day thematic workshop, which has been collectively written and edited during and after the meeting. While not comprehensive, we believe they capture a broad range of opinions from multiple stakeholders and synthesize a feasible way forward. The relationship between AI and interdisciplinary research must be considered as a two-way street. For instance, interdisciplinary approaches, e.g., including art and science, as well as ensuring minorities are well represented among both the users and the evaluators of the latest eXplainable AI techniques (Arrieta et al., 2020), can make AI more accessible and inclusive to otherwise unreachable communities. We focused our recommendations on mutual benefits that can be harnessed from these interactions and emphasized the important role of interdisciplinarity in this process. AI systems have complex life cycles, including data acquisition, training, testing and deployment, ultimately demanding an interdisciplinary approach to audit and evaluate the quality and safety of these AI products or services.<br><a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.577974/full" target="_blank">https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.577974/full</a></li><li id="tree-E5029013784"><strong>E5029013784:</strong> To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities.<br><a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.577974/full" target="_blank">https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.577974/full</a></li><li id="tree-S6354951537"><strong>S6354951537:</strong> The Qasper dataset is a valuable resource for evaluating AI research synthesis, containing 5,000 information-seeking questions and answers based on scientific papers.<ul><li id="tree-E0290570856"><strong>E0290570856:</strong> 5K information-seeking questions over 1.5K scientific papers. Each question is asked by an expert researcher and answered by a different expert researcher using supporting evidence from the paper&#x27;s full text. Qasper has been included in long-context benchmarks such as SCROLLS. ... 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset ... 5K information-seeking questions over 1.5K scientific papers. Each question is asked by an expert researcher and answered by a different expert researcher using supporting evidence from the paper's full text. Qasper has been included in long-context benchmarks such as SCROLLS. ... 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. WinoGrande is a collection of 44K problems, inspired by Winograd Schema Challenge, but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning. ... 137K instruction-following demonstrations for 54 scientific literature understanding tasks. The tasks cover five essential scientific literature categories and span five domains. ... Instruction data collected for writing paragraph-level answers to multiple document-grounded NLP research questions. It aims to facilitate research and development of tools for text mining over academic text. ... A collection of over 200M paper titles, abstracts, citations, and other metadata of open-access papers from the Semantic Scholar Academic Graph. ... A challenge dataset of questions that are trivial for humans (>95% accuracy) but that state-of-the-art models struggle with (<48%), created through a collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-E8032253673"><strong>E8032253673:</strong> 5K information-seeking questions over 1.5K scientific papers. Each question is asked by an expert researcher and answered by a different expert researcher using supporting evidence from the paper&#x27;s full text. Qasper has been included in long-context benchmarks such as SCROLLS. ... 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset ...<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li></ul></li></ul></li></ul><a href="#cite-S7208994926" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S9313841483"><h3>Citation Tree for Statement S9313841483</h3><ul><li id="tree-S9313841483"><strong>S9313841483:</strong> Datasets adhering to the FAIR principles are vital for effective AI research synthesis, ensuring that data is findable, accessible, interoperable, and reusable across disciplines.<ul><li id="tree-E6799375552"><strong>E6799375552:</strong> Snippet text not found for ID: E6799375552</li><li id="tree-E6799371945"><strong>E6799371945:</strong> Snippet text not found for ID: E6799371945</li><li id="tree-E6799371945"><strong>E6799371945:</strong> Snippet text not found for ID: E6799371945</li></ul></li></ul><a href="#cite-S9313841483" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S0010026175"><h3>Citation Tree for Statement S0010026175</h3><ul><li id="tree-S0010026175"><strong>S0010026175:</strong> Datasets that ensure explainability and bias transparency are crucial for ethical AI research synthesis, as they facilitate the development of automated extraction models while addressing potential biases.<ul><li id="tree-E2594205819"><strong>E2594205819:</strong> The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses ...<br><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931862/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931862/</a></li><li id="tree-E1466919038"><strong>E1466919038:</strong> The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses ... To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our recommendations are a result of in-person discussions within a diverse group of researchers, educators, and students, during a 3-day thematic workshop, which has been collectively written and edited during and after the meeting. While not comprehensive, we believe they capture a broad range of opinions from multiple stakeholders and synthesize a feasible way forward. The relationship between AI and interdisciplinary research must be considered as a two-way street. For instance, interdisciplinary approaches, e.g., including art and science, as well as ensuring minorities are well represented among both the users and the evaluators of the latest eXplainable AI techniques (Arrieta et al., 2020), can make AI more accessible and inclusive to otherwise unreachable communities. We focused our recommendations on mutual benefits that can be harnessed from these interactions and emphasized the important role of interdisciplinarity in this process. AI systems have complex life cycles, including data acquisition, training, testing and deployment, ultimately demanding an interdisciplinary approach to audit and evaluate the quality and safety of these AI products or services.<br><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931862/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7931862/</a></li><li id="tree-S9198903639"><strong>S9198903639:</strong> The EBM-NLP dataset offers approximately 5,000 annotated abstracts of randomized controlled trial reports, facilitating the development of automated extraction models for research synthesis.<ul><li id="tree-E4290222644"><strong>E4290222644:</strong> Recently, Nye et al. released the EBM-NLP dataset [23], which comprises ~ 5000 abstracts of RCT reports manually annotated in detail. This may provide training data helpful for moving automated extraction models forward. Although software tools that support the data synthesis component of reviews have long existed (especially for performing meta-analysis), methods for automating this are beyond the capabilities of currently available ML and NLP tools. Nonetheless, research into these areas continues rapidly, and computational methods may allow new forms of synthesis unachievable manually, particularly around visualization [37, 38] and automatic summarization [39, 40] of large volumes of research evidence.<br><a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9" target="_blank">https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9</a></li><li id="tree-E5855592299"><strong>E5855592299:</strong> Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. In ML, one writes programs that specify parameterized models to perform particular tasks; these parameters are then estimated using (ideally large) datasets. In practice, ML methods resemble statistical models used in epidemiological research (e.g. logistic regression is a common method in both disciplines). Recently, Nye et al. released the EBM-NLP dataset [23], which comprises ~ 5000 abstracts of RCT reports manually annotated in detail. This may provide training data helpful for moving automated extraction models forward. Although software tools that support the data synthesis component of reviews have long existed (especially for performing meta-analysis), methods for automating this are beyond the capabilities of currently available ML and NLP tools. Nonetheless, research into these areas continues rapidly, and computational methods may allow new forms of synthesis unachievable manually, particularly around visualization [37, 38] and automatic summarization [39, 40] of large volumes of research evidence. The torrential volume of unstructured published evidence has rendered existing (rigorous, but manual) approaches to evidence synthesis increasingly costly and impractical. Consequently, researchers have developed methods that aim to semi-automate different steps of the evidence synthesis pipeline via machine learning. Most of the tools we encountered were written by academic groups involved in research into evidence synthesis and machine learning. Very often, these groups have produced prototype software to demonstrate a method.<br><a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9" target="_blank">https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9</a></li></ul></li></ul></li></ul><a href="#cite-S0010026175" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S3746529778"><h3>Citation Tree for Statement S3746529778</h3><ul><li id="tree-S3746529778"><strong>S3746529778:</strong> The AI2 Reasoning Challenge (ARC) dataset is a comprehensive resource for evaluating multi-domain reasoning capabilities, featuring multiple-choice questions that require varying levels of reasoning across scientific disciplines.<ul><li id="tree-E3444441510"><strong>E3444441510:</strong> We’re on a journey to advance and democratize artificial intelligence through open source and open science. A new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where the former contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. We are also including a corpus of over 14 million science sentences relevant to the task, and an implementation of three neural baseline models for this dataset. We pose ARC as a challenge to the community. @article{allenai:arc, author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord}, title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, journal = {arXiv:1803.05457v1}, year = {2018}, } An example of 'train' looks as follows.<br><a href="https://allenai.org/data/arc" target="_blank">https://allenai.org/data/arc</a></li><li id="tree-E6266081007"><strong>E6266081007:</strong> The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require ...<br><a href="https://paperswithcode.com/dataset/arc" target="_blank">https://paperswithcode.com/dataset/arc</a></li><li id="tree-E4605751630"><strong>E4605751630:</strong> Introduced by Clark et al. in Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge<br><a href="https://paperswithcode.com/dataset/arc" target="_blank">https://paperswithcode.com/dataset/arc</a></li></ul></li></ul><a href="#cite-S3746529778" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S2500722700"><h3>Citation Tree for Statement S2500722700</h3><ul><li id="tree-S2500722700"><strong>S2500722700:</strong> LogiQA is a valuable dataset for evaluating logical reasoning in AI systems, comprising 8,678 question-answer pairs that challenge models to demonstrate deductive reasoning skills.<ul><li id="tree-E5417788336"><strong>E5417788336:</strong> Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging ... Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. View a PDF of the paper titled LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning, by Jian Liu and 5 other authors View PDF We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.<br><a href="https://arxiv.org/abs/2007.08124" target="_blank">https://arxiv.org/abs/2007.08124</a></li><li id="tree-E6890456282"><strong>E6890456282:</strong> Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging ...<br><a href="https://arxiv.org/abs/2007.08124" target="_blank">https://arxiv.org/abs/2007.08124</a></li><li id="tree-E7945676172"><strong>E7945676172:</strong> Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning.<br><a href="https://arxiv.org/abs/2007.08124" target="_blank">https://arxiv.org/abs/2007.08124</a></li></ul></li></ul><a href="#cite-S2500722700" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S3166321474"><h3>Citation Tree for Statement S3166321474</h3><ul><li id="tree-S3166321474"><strong>S3166321474:</strong> Multi-hop question answering datasets are essential for evaluating AI systems' reasoning capabilities, as they require models to integrate information from multiple sources to answer complex questions.<ul><li id="tree-E8372566228"><strong>E8372566228:</strong> A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required. From: Xanh Ho Thi [view email] [v1] Mon, 2 Nov 2020 15:42:40 UTC (572 KB) [v2] Thu, 12 Nov 2020 07:47:48 UTC (572 KB) ... View a PDF of the paper titled Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps, by Xanh Ho and 2 other authors View a PDF of the paper titled Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps, by Xanh Ho and 2 other authors View PDF We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li><li id="tree-E8392606101"><strong>E8392606101:</strong> A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer.<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li><li id="tree-E8435859387"><strong>E8435859387:</strong> Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required. From: Xanh Ho Thi [view email] [v1] Mon, 2 Nov 2020 15:42:40 UTC (572 KB) [v2] Thu, 12 Nov 2020 07:47:48 UTC (572 KB) ... View a PDF of the paper titled Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps, by Xanh Ho and 2 other authors<br><a href="https://arxiv.org/abs/2011.01060" target="_blank">https://arxiv.org/abs/2011.01060</a></li></ul></li></ul><a href="#cite-S3166321474" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S7194843350"><h3>Citation Tree for Statement S7194843350</h3><ul><li id="tree-S7194843350"><strong>S7194843350:</strong> The Qasper dataset is a comprehensive resource for evaluating AI research synthesis, containing 5,000 information-seeking questions and answers derived from scientific literature.<ul><li id="tree-E3159156247"><strong>E3159156247:</strong> 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-E5514195194"><strong>E5514195194:</strong> 7,787 genuine grade-school level, multiple-choice science questions partitioned into a Challenge Set and an Easy Set, along with a corpus of over 14 million science sentences relevant to the task. Offered as a challenge to the machine reasoning community. ... A QA dataset that tests the comprehensive understanding of paragraphs.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-E3444441510"><strong>E3444441510:</strong> We’re on a journey to advance and democratize artificial intelligence through open source and open science. A new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where the former contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. We are also including a corpus of over 14 million science sentences relevant to the task, and an implementation of three neural baseline models for this dataset. We pose ARC as a challenge to the community. @article{allenai:arc, author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord}, title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, journal = {arXiv:1803.05457v1}, year = {2018}, } An example of 'train' looks as follows.<br><a href="https://allenai.org/data/arc" target="_blank">https://allenai.org/data/arc</a></li></ul></li></ul><a href="#cite-S7194843350" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S4626855544"><h3>Citation Tree for Statement S4626855544</h3><ul><li id="tree-S4626855544"><strong>S4626855544:</strong> WinoGrande is an emerging dataset that challenges AI reasoning capabilities by requiring commonsense reasoning across 44,000 problems.<ul><li id="tree-E0004076894"><strong>E0004076894:</strong> WinoGrande is a collection of 44K ... and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning.... WinoGrande is a collection of 44K problems, inspired by Winograd Schema Challenge, but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning. A collection of over 200M paper titles, abstracts, citations, and other metadata of open-access papers from the Semantic Scholar Academic Graph. ... A challenge dataset of questions that are trivial for humans (>95% accuracy) but that state-of-the-art models struggle with (<48%), created through a collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. Dolma is a dataset from a diverse mix of web content, academic publications, code, books, and encyclopedic materials. 7,787 genuine grade-school level, multiple-choice science questions partitioned into a Challenge Set and an Easy Set, along with a corpus of over 14 million science sentences relevant to the task. Offered as a challenge to the machine reasoning community. ... A QA dataset that tests the comprehensive understanding of paragraphs.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-E9788747091"><strong>E9788747091:</strong> WinoGrande is a collection of 44K ... and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning....<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li></ul></li></ul><a href="#cite-S4626855544" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S2311603242"><h3>Citation Tree for Statement S2311603242</h3><ul><li id="tree-S2311603242"><strong>S2311603242:</strong> The AI2 Reasoning Challenge (ARC) dataset is a comprehensive resource for evaluating multi-domain reasoning capabilities in AI.<ul><li id="tree-E3746529778"><strong>E3746529778:</strong> Snippet text not found for ID: E3746529778</li><li id="tree-E2500722700"><strong>E2500722700:</strong> Snippet text not found for ID: E2500722700</li></ul></li></ul><a href="#cite-S2311603242" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S2403814904"><h3>Citation Tree for Statement S2403814904</h3><ul><li id="tree-S2403814904"><strong>S2403814904:</strong> Emerging datasets from AI applications in fundamental sciences, such as mathematics and medical science, are relevant for evaluating AI reasoning capabilities.<ul><li id="tree-E1527139803"><strong>E1527139803:</strong> Artificial intelligence (AI) coupled with promising machine learning (ML) techniques well known from computer science is broadly affecting many aspect… “Can machines think?” The goal of artificial intelligence (AI) is to enable machines to mimic human thoughts and behaviors, including learning, reasoning, predicting, and so on. ... “Can AI do fundamental research?” AI coupled with machine learning techniques is impacting a wide range of fundamental sciences, including mathematics, medical science, physics, etc. ... “How does AI accelerate fundamental research?” New research and applications are emerging rapidly with the support by AI infrastructure, including data storage, computing power, AI algorithms, and frameworks. This paper undertakes a comprehensive survey on the development and application of AI in different aspects of fundamental sciences, including information science, mathematics, medical science, materials science, geoscience, life science, physics, and chemistry. The challenges that each discipline of science meets, and the potentials of AI techniques to handle these challenges, are discussed in detail. Moreover, we shed light on new research trends entailing the integration of AI into each scientific discipline. Author links open overlay panelYongjun Xu 1 35 42, Xin Liu 5 35 42, Xin Cao 10 42, Changping Huang 18 35 42, Enke Liu 11 37 42, Sen Qian 26 42, Xingchen Liu 28 42, Yanjun Wu 2 35, Fengliang Dong 3 35, Cheng-Wei Qiu 4, Junjun Qiu 6 36, Keqin Hua 6 36, Wentao Su 7, Jian Wu 41, Huiyu Xu 8, Yong Han 9, Chenguang Fu 12, Zhigang Yin 13, Miao Liu 11 37, Ronald Roepman 14…Jiabao Zhang 33 35Show more<br><a href="https://www.sciencedirect.com/science/article/pii/S2666675821001041" target="_blank">https://www.sciencedirect.com/science/article/pii/S2666675821001041</a></li><li id="tree-E2239608731"><strong>E2239608731:</strong> This paper undertakes a comprehensive survey on the development and application of AI in different aspects of fundamental sciences, including information science, mathematics, medical science, materials science, geoscience, life science, physics, and chemistry. The challenges that each discipline of science meets, and the potentials of AI techniques to handle these challenges, are discussed in detail.<br><a href="https://www.sciencedirect.com/science/article/pii/S2666675821001041" target="_blank">https://www.sciencedirect.com/science/article/pii/S2666675821001041</a></li></ul></li></ul><a href="#cite-S2403814904" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S2704435204"><h3>Citation Tree for Statement S2704435204</h3><ul><li id="tree-S2704435204"><strong>S2704435204:</strong> Datasets derived from AI applications in higher education, spanning various disciplines, are emerging as valuable resources for AI reasoning evaluation.<ul><li id="tree-E5915511800"><strong>E5915511800:</strong> This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles were identified for a full examination. Using a priori, and grounded coding, the data ... This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles were identified for a full examination. Using a priori, and grounded coding, the data from the 138 articles were extracted, analyzed, and coded. AI is a tool used across subject disciplines, including language education (Liang et al., 2021), engineering education (Shukla et al., 2019), mathematics education (Hwang & Tu, 2021) and medical education (Winkler-Schwartz et al., 2019), The term artificial intelligence is not new. It was coined in 1956 by McCarthy (Cristianini, 2016) who followed up on the work of Turing (e.g., Turing, 1937, 1950). Turing described the existence of intelligent reasoning and thinking that could go into intelligent machines. AI has great potential to collect, cross reference and examine data across large datasets that can allow data to be used for actionable insight. More focus on the use of AI by managers would tap into this potential. Using grounded coding, the use of AIEd from each of the 138 articles was examined and six major codes emerged from the data. This focus on undergraduate students may be due to the variety of affordances offered by AIEd, such as predictive analytics on dropouts and academic performance. These uses of AI may be less required for graduate students who already have a record of performance from their undergraduate years. Another reason for this demographic focus can also be convenience sampling, as researchers in HE typically has a much larger and accessible undergraduate population than graduates.<br><a href="https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00392-8" target="_blank">https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00392-8</a></li><li id="tree-E1511369124"><strong>E1511369124:</strong> According to various international reports, Artificial Intelligence in Education (AIEd) is one of the currently emerging fields in educational technology. Whilst it has been around for about 30 years, it is still unclear for educators how to make pedagogical advantage of it on a broader scale, ... According to various international reports, Artificial Intelligence in Education (AIEd) is one of the currently emerging fields in educational technology. Whilst it has been around for about 30 years, it is still unclear for educators how to make pedagogical advantage of it on a broader scale, and how it can actually impact meaningfully on teaching and learning in higher education. It is an important implication of this systematic review, that researchers are encouraged to be explicit about the theories that underpin empirical studies about the development and implementation of AIEd projects, in order to expand research to a broader level, helping us to understand the reasons and mechanisms behind this dynamic development that will have an enormous impact on higher education institutions in the various areas we have covered in this review. The datasets used and/or analysed during the current study (the bibliography of included studies) are available from the corresponding author upon request. The descriptive results show that most of the disciplines involved in AIEd papers come from Computer Science and STEM, and that quantitative methods were the most frequently used in empirical studies. The synthesis of results presents four areas of AIEd applications in academic support services, and institutional and administrative services: 1. Articles focusing on assessment and evaluation applications of AI at the teaching and learning level, were classified into four sub-categories; automated grading (n = 13), feedback (n = 8), evaluation of student understanding, engagement and academic integrity (n = 5), and evaluation of teaching (n = 5). Articles that utilised automated grading, or Automated Essay Scoring (AES) systems, came from a range of disciplines (e.g.<br><a href="https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-019-0171-0" target="_blank">https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-019-0171-0</a></li></ul></li></ul><a href="#cite-S2704435204" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S4478862442"><h3>Citation Tree for Statement S4478862442</h3><ul><li id="tree-S4478862442"><strong>S4478862442:</strong> The PAIR framework provides a structured approach for evaluating AI-generated content, emphasizing principles that enhance the effectiveness of AI research synthesis.<ul><li id="tree-E8404945464"><strong>E8404945464:</strong> This guide brings together information and guidance on effective searching for journal articles and grey literature for those undertaking a systematic review, scoping review or other evidence synthesis Note PICO Portal is a systematic review platform that leverages artificial intelligence to accelerate research and innovation. Moderator Dr Greg Martin. Presenters: Eitan Agai - PICO Portal Founder & AI Expert; Riaz Qureshi - U. of Colorado Anschutz Medical Campus; Kevin Kallmes - Chief Executive Officer, Cofounder; Jeff Johnson - Chef Design Officer. PAIR (problem, AI, interaction, reflection) framework guidance The framework encompasses five core principles—Concise, Logical, Explicit, Adaptive, and Reflective—that facilitate more effective AI-generated content evaluation and creation. ... Lo, L. S. (2023). The CLEAR path: A framework for enhancing information literacy through prompt engineering. The Journal of Academic Librarianship, 49(4), 102720. ... The Systematic Review Toolbox is an online catalogue of tools that support various tasks within the systematic review and wider evidence synthesis process. Part 1: How Cochrane currently uses machine learning: implementing innovative technology Part 2: What generative AI is, the opportunities it brings and the challenges regarding its safe use Part 3: Cochrane's focus on the responsible use of AI in systematic reviews Part 4: Questions and answers · << Previous: Library Workshops, Drop ins and 1-2-1s ... Report a problem. Subjects: Systematic Reviews, Scoping Reviews & Evidence Synthesis These may be used to assist with developing a search strategy; locating relevant articles or resources; or during the data screening, data extraction or synthesis stage. They can also be used to draft plain language summaries. The overall consensus is that the AI tools can be very useful in different stages of the systematic or other evidence review but that it is important to fully understand any bias and weakness they may bring to the process. In many cases using new AI tools, which previous research has not assessed rigorously, should happen in conjunction with existing validated methods.<br><a href="https://libguides.kcl.ac.uk/systematicreview/ai" target="_blank">https://libguides.kcl.ac.uk/systematicreview/ai</a></li><li id="tree-E6520916441"><strong>E6520916441:</strong> The framework encompasses five core principles—Concise, Logical, Explicit, Adaptive, and Reflective—that facilitate more effective AI-generated content evaluation and creation. ... Lo, L. S. (2023). The CLEAR path: A framework for enhancing information literacy through prompt engineering. The Journal of Academic Librarianship, 49(4), 102720. ... The Systematic Review Toolbox is an online catalogue of tools that support various tasks within the systematic review and wider evidence synthesis process.<br><a href="https://libguides.kcl.ac.uk/systematicreview/ai" target="_blank">https://libguides.kcl.ac.uk/systematicreview/ai</a></li><li id="tree-E6675849286"><strong>E6675849286:</strong> This guide brings together information and guidance on effective searching for journal articles and grey literature for those undertaking a systematic review, scoping review or other evidence synthesis<br><a href="https://libguides.kcl.ac.uk/systematicreview/ai" target="_blank">https://libguides.kcl.ac.uk/systematicreview/ai</a></li></ul></li></ul><a href="#cite-S4478862442" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S7038525479"><h3>Citation Tree for Statement S7038525479</h3><ul><li id="tree-S7038525479"><strong>S7038525479:</strong> The SUDO framework offers a novel approach for evaluating clinical AI systems, focusing on performance estimation in real-world settings without ground-truth annotations.<ul><li id="tree-E2557519187"><strong>E2557519187:</strong> A clinical artificial intelligence (AI) system is often validated on data withheld during its development. This provides an estimate of its performance upon future deployment on data in the wild; those currently unseen but are expected to be encountered in a clinical setting. Estimating the performance of clinical AI systems on data in the wild is complicated by distribution shift and the absence of ground-truth annotations. Here, we introduce SUDO, a framework for more reliably evaluating AI systems on data in the wild. Nature Communications - Estimating the performance of clinical AI systems on data in the wild is complicated by distribution shift and the absence of ground-truth annotations. Here, we introduce... 1b, Step 4) is less restrictive for researchers and can obviate the need to (re)train potentially computationally expensive inference models. This line of argument also extends to settings with different data modalities (e.g., images, time-series, etc.), and, as such, makes SUDO agnostic to the modality of the data used for training and evaluating the model. Such calibration methods, however, can be ineffective when deployed on data in the wild that exhibit distribution shift11. Regardless, quantifying the effectiveness of calibration methods would still require ground-truth labels, a missing element of data in the wild. Another line of research focuses on estimating the overall performance of models with unlabelled data12,13.<br><a href="https://www.nature.com/articles/s41467-024-46000-9" target="_blank">https://www.nature.com/articles/s41467-024-46000-9</a></li><li id="tree-E1417711701"><strong>E1417711701:</strong> A clinical artificial intelligence (AI) system is often validated on data withheld during its development. This provides an estimate of its performance upon future deployment on data in the wild; those currently unseen but are expected to be encountered in a clinical setting.<br><a href="https://www.nature.com/articles/s41467-024-46000-9" target="_blank">https://www.nature.com/articles/s41467-024-46000-9</a></li><li id="tree-E5794598850"><strong>E5794598850:</strong> User acceptance is the traditional focus of evaluation in determining the success of an information system [15,17,32]. User acceptance is a synthesized concept—we used expectation confirmation, user satisfaction, and intention of use as secondary indicators.<br><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/</a></li></ul></li></ul><a href="#cite-S7038525479" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S0135190247"><h3>Citation Tree for Statement S0135190247</h3><ul><li id="tree-S0135190247"><strong>S0135190247:</strong> A framework for AI-powered service innovation capability has been proposed, synthesizing current knowledge and outlining future research directions in AI synthesis.<ul><li id="tree-E9992067860"><strong>E9992067860:</strong> Artificial intelligence (AI)-powered service innovation (e.g., OpenAI&#x27;s ChatGPT, Google&#x27;s Bard and Microsoft&#x27;s Sydney) has become one of the most sign… However, extant studies on this topic show that research studies hitherto have been ad-hoc, lacking a conceptual framework for the strategic management of AI-powered service innovation capability in dynamic markets. Thus, this study synthesises the current body of knowledge, proposes a framework, and develops an agenda to advance our knowledge. AI research on service innovation capability still seems to be emerging, scattered across many fields, thus lacking a clear conceptual framework. This paper contributes to innovation management and commercialization literature in several ways. First, this study addresses the research questions by undertaking a systematic review process of (1) planning and searching, (2) screening and extraction, and (3) synthesis and reporting. Drawing on a systematic literature review (SLR) and thematic analysis, this study critically assesses and synthesises AI-powered service innovation in a transparent, rigorous, robust and replicable manner (Vrontis and Christofi, 2021). Hence, this study aims to answer three research questions: This study proposes and evaluates a novel approach utilizing ensemble machine learning techniques for personalized meal services to address a critical gap in understanding AI-powered decision-making within the food delivery and restaurant industry. We draw inspiration from diverse fields, including non-traditional simulation methodologies and open innovation dynamics, to create a framework that leverages the combined strengths of individual algorithms.<br><a href="https://www.sciencedirect.com/science/article/abs/pii/S0166497223000792" target="_blank">https://www.sciencedirect.com/science/article/abs/pii/S0166497223000792</a></li><li id="tree-E0554131902"><strong>E0554131902:</strong> However, extant studies on this topic show that research studies hitherto have been ad-hoc, lacking a conceptual framework for the strategic management of AI-powered service innovation capability in dynamic markets. Thus, this study synthesises the current body of knowledge, proposes a framework, and develops an agenda to advance our knowledge.<br><a href="https://www.sciencedirect.com/science/article/abs/pii/S0166497223000792" target="_blank">https://www.sciencedirect.com/science/article/abs/pii/S0166497223000792</a></li><li id="tree-E7706374271"><strong>E7706374271:</strong> Drawing on a systematic literature review (SLR) and thematic analysis, this study critically assesses and synthesises AI-powered service innovation in a transparent, rigorous, robust and replicable manner (Vrontis and Christofi, 2021). Hence, this study aims to answer three research questions:<br><a href="https://www.sciencedirect.com/science/article/abs/pii/S0166497223000792" target="_blank">https://www.sciencedirect.com/science/article/abs/pii/S0166497223000792</a></li></ul></li></ul><a href="#cite-S0135190247" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S9319867109"><h3>Citation Tree for Statement S9319867109</h3><ul><li id="tree-S9319867109"><strong>S9319867109:</strong> A comprehensive framework is necessary for assessing organizations' readiness to leverage AI technologies effectively, focusing on internal capabilities and data governance.<ul><li id="tree-E3651176520"><strong>E3651176520:</strong> This emphasizes the need for a comprehensive approach to assess the organization’s data infrastructure, governance practices, and existing AI capabilities. Furthermore, the research work focuses on the evaluation of AI talent and skills within the organization, considering the significance of fostering an innovative culture and addressing change management challenges.<br><a href="https://www.mdpi.com/2571-5577/7/1/14" target="_blank">https://www.mdpi.com/2571-5577/7/1/14</a></li><li id="tree-E2197715945"><strong>E2197715945:</strong> Our study provides practitioners and researchers with a tailored framework that offers specific methods for evaluating processes, existing systems, data landscapes, and internal AI capabilities. Unlike generic guidelines, our approach delves into organizational intricacies, ensuring a nuanced assessment.<br><a href="https://www.mdpi.com/2571-5577/7/1/14" target="_blank">https://www.mdpi.com/2571-5577/7/1/14</a></li><li id="tree-E3656063237"><strong>E3656063237:</strong> Louise Barry, Rose Galvin, Sylvia Murphy Tighe, Margaret O'Connor, Damian Ryan, Pauline Meskell, The barriers and facilitators to screening in emergency departments: a qualitative evidence synthesis (QES) protocol, HRB Open Research, 10.12688/hrbopenres.13073.1, 3, (50), (2020).<br><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399" target="_blank">https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399</a></li></ul></li></ul><a href="#cite-S9319867109" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S0568789065"><h3>Citation Tree for Statement S0568789065</h3><ul><li id="tree-S0568789065"><strong>S0568789065:</strong> WinoGrande is a new dataset that challenges AI reasoning capabilities by requiring commonsense reasoning across 44,000 problems.<ul><li id="tree-E8573025640"><strong>E8573025640:</strong> WinoGrande is a collection of 44K ... and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning.... WinoGrande is a collection of 44K problems, inspired by Winograd Schema Challenge, but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning. 7,787 genuine grade-school level, multiple-choice science questions partitioned into a Challenge Set and an Easy Set, along with a corpus of over 14 million science sentences relevant to the task. Offered as a challenge to the machine reasoning community. ... A QA dataset that tests the comprehensive understanding of paragraphs. 20K biomedical literature review summaries synthesizing information from over 470K studies. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. A challenge dataset of questions that are trivial for humans (>95% accuracy) but that state-of-the-art models struggle with (<48%), created through a collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-S4626855544"><strong>S4626855544:</strong> WinoGrande is an emerging dataset that challenges AI reasoning capabilities by requiring commonsense reasoning across 44,000 problems.<ul><li id="tree-E0004076894"><strong>E0004076894:</strong> WinoGrande is a collection of 44K ... and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning.... WinoGrande is a collection of 44K problems, inspired by Winograd Schema Challenge, but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning. A collection of over 200M paper titles, abstracts, citations, and other metadata of open-access papers from the Semantic Scholar Academic Graph. ... A challenge dataset of questions that are trivial for humans (>95% accuracy) but that state-of-the-art models struggle with (<48%), created through a collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. Dolma is a dataset from a diverse mix of web content, academic publications, code, books, and encyclopedic materials. 7,787 genuine grade-school level, multiple-choice science questions partitioned into a Challenge Set and an Easy Set, along with a corpus of over 14 million science sentences relevant to the task. Offered as a challenge to the machine reasoning community. ... A QA dataset that tests the comprehensive understanding of paragraphs.<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li><li id="tree-E9788747091"><strong>E9788747091:</strong> WinoGrande is a collection of 44K ... and robustness against the dataset-specific bias. Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning....<br><a href="https://allenai.org/open-data" target="_blank">https://allenai.org/open-data</a></li></ul></li></ul></li></ul><a href="#cite-S0568789065" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S0333270852"><h3>Citation Tree for Statement S0333270852</h3><ul><li id="tree-S0333270852"><strong>S0333270852:</strong> The AI2 Reasoning Challenge (ARC) dataset is a comprehensive resource for evaluating multi-domain reasoning capabilities in AI, featuring 7,787 grade-school level, multiple-choice science questions.<ul><li id="tree-E0412517453"><strong>E0412517453:</strong> We’re on a journey to advance and democratize artificial intelligence through open source and open science. A new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where the former contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. We are also including a corpus of over 14 million science sentences relevant to the task, and an implementation of three neural baseline models for this dataset. @article{allenai:arc, author = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord}, title = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, journal = {arXiv:1803.05457v1}, year = {2018}, }<br><a href="https://allenai.org/data/arc" target="_blank">https://allenai.org/data/arc</a></li><li id="tree-S2311603242"><strong>S2311603242:</strong> The AI2 Reasoning Challenge (ARC) dataset is a comprehensive resource for evaluating multi-domain reasoning capabilities in AI.<ul><li id="tree-E3746529778"><strong>E3746529778:</strong> Snippet text not found for ID: E3746529778</li><li id="tree-E2500722700"><strong>E2500722700:</strong> Snippet text not found for ID: E2500722700</li></ul></li></ul></li></ul><a href="#cite-S0333270852" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S2352574967"><h3>Citation Tree for Statement S2352574967</h3><ul><li id="tree-S2352574967"><strong>S2352574967:</strong> The Sherlock dataset is a new dataset designed to enhance AI's commonsense reasoning capabilities, consisting of over 100,000 images paired with clues for abductive reasoning.<ul><li id="tree-E3201343958"><strong>E3201343958:</strong> In a new study, researchers attempt to measure the extent to which AI systems can abductively reason like humans. For example, given a photo showing a toppled truck and a police cruiser on a snowy freeway, abductive reasoning may lead someone to infer that dangerous road conditions caused an accident. Humans can quickly consider this sort of context to arrive at a hypothesis. But AI struggles, despite recent technical advances. Motivated to explore the challenge, researchers at the Allen Institute for Artificial Intelligence, the University of California, Berkeley, and the MIT-IBM Watson AI lab created a dataset called Sherlock, a collection of over 100,000 images of scenes paired with clues a viewer could use to answer questions about the scenes. Over the course of several experiments, Hessel and colleagues tested AI systems’ abilities to abductively reason from the Sherlock examples. They had the systems look at specific regions within images in the dataset — for example, a coffee cup sitting on a table in a café — and asked questions like “What do you think of this coffee cup?” Hessel hopes that Sherlock will spur progress toward systems — neurosymbolic or no — with stronger commonsense reasoning. He concedes that the dataset contains best-guess, probable conclusions about scenes drawn by humans who have their own biases and perspectives (abduction, by its nature, carries with it uncertainty). An example of a scene in the Sherlock abductive reasoning dataset.<br><a href="https://venturebeat.com/ai/this-new-dataset-shows-that-ai-still-lacks-commonsense-reasoning/" target="_blank">https://venturebeat.com/ai/this-new-dataset-shows-that-ai-still-lacks-commonsense-reasoning/</a></li><li id="tree-E3752056934"><strong>E3752056934:</strong> In a new study, researchers attempt to measure the extent to which AI systems can abductively reason like humans.<br><a href="https://venturebeat.com/ai/this-new-dataset-shows-that-ai-still-lacks-commonsense-reasoning/" target="_blank">https://venturebeat.com/ai/this-new-dataset-shows-that-ai-still-lacks-commonsense-reasoning/</a></li></ul></li></ul><a href="#cite-S2352574967" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S5073583715"><h3>Citation Tree for Statement S5073583715</h3><ul><li id="tree-S5073583715"><strong>S5073583715:</strong> An innovative evaluation framework for AI-enabled clinical decision support systems emphasizes user acceptance and adaptability, which can inform evaluation methodologies for AI research synthesis in academia.<ul><li id="tree-E5383547351"><strong>E5383547351:</strong> This study is an innovative attempt and pilot examination of an evaluation framework in relation to AI-enabled clinical decision support system success. This evaluation framework is widely applicable, with a broad scope in clinically common and multidisciplinary interoperable scenarios.<br><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/</a></li><li id="tree-E5794598850"><strong>E5794598850:</strong> User acceptance is the traditional focus of evaluation in determining the success of an information system [15,17,32]. User acceptance is a synthesized concept—we used expectation confirmation, user satisfaction, and intention of use as secondary indicators.<br><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/</a></li><li id="tree-E3100071286"><strong>E3100071286:</strong> At the same time, if the measurement instrument is to be used for AI-enabled clinical decision support system products with different functionalities in a specific scenario, item modifications, cross-cultural adaptation, and tests of reliability and validity testing (in accordance with scale development guidelines [52]) is needed. This work was supported by the Doctoral Innovation Fund in Shanghai Jiao Tong University School of Medicine 2019 [BXJ201906]; the Shanghai Municipal Education Commission-Gaoyuan Nursing Grant Support [Hlgy1906dxk]; and the Shanghai Municipal Commission of Health and Family Planning (Grant No.<br><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/" target="_blank">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8209524/</a></li></ul></li></ul><a href="#cite-S5073583715" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S8539006966"><h3>Citation Tree for Statement S8539006966</h3><ul><li id="tree-S8539006966"><strong>S8539006966:</strong> A comprehensive AI education policy framework for higher education emphasizes pilot testing and monitoring to evaluate the effectiveness of AI tools, relevant for assessing AI research synthesis capabilities.<ul><li id="tree-E7107363137"><strong>E7107363137:</strong> This study aims to develop an AI education policy for higher education by examining the perceptions and implications of text generative AI technologies. Data was collected from 457 students and 180 teachers and staff across various disciplines in Hong Kong universities, using both quantitative ... This study aims to develop an AI education policy for higher education by examining the perceptions and implications of text generative AI technologies. Data was collected from 457 students and 180 teachers and staff across various disciplines in Hong Kong universities, using both quantitative and qualitative research methods. Pilot testing, monitoring and evaluation, and building an evidence base: This recommendation highlights the importance of testing and evaluating the use of AI in education through pilot projects to build an evidence base for its effectiveness. For example, policymakers could fund pilot projects that test the use of AI tools in specific educational contexts or with specific learner populations. ... Fostering local AI innovations for education: This recommendation suggests that policymakers should encourage the development of local innovations in AI for education to ensure that it meets the specific needs of their communities. Conducting research in AI policy in education within Hong Kong is specifically justified due to the city’s unique position as a global hub of technology, commerce, and education, coupled with its evolving education landscape. As a dynamic metropolis with a strong commitment to technological innovation and a richly diverse education system, Hong Kong presents a compelling case study for the exploration of AI policies in education. However, more research is necessary to fully comprehend the potential advantages and risks associated with AI in academic settings. Merely advocating for AI implementation in education is insufficient; stakeholders need to carefully evaluate which AI technologies to employ, determine the best methods for their use, and understand their true capabilities.<br><a href="https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3" target="_blank">https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3</a></li><li id="tree-E1445652134"><strong>E1445652134:</strong> This study aims to develop an AI education policy for higher education by examining the perceptions and implications of text generative AI technologies. Data was collected from 457 students and 180 teachers and staff across various disciplines in Hong Kong universities, using both quantitative ...<br><a href="https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3" target="_blank">https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3</a></li><li id="tree-E1737150604"><strong>E1737150604:</strong> Pilot testing, monitoring and evaluation, and building an evidence base: This recommendation highlights the importance of testing and evaluating the use of AI in education through pilot projects to build an evidence base for its effectiveness. For example, policymakers could fund pilot projects that test the use of AI tools in specific educational contexts or with specific learner populations. ... Fostering local AI innovations for education: This recommendation suggests that policymakers should encourage the development of local innovations in AI for education to ensure that it meets the specific needs of their communities.<br><a href="https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3" target="_blank">https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00408-3</a></li></ul></li></ul><a href="#cite-S8539006966" class="back-to-top">↑ Back to citation</a></div><div class="citation-tree" id="tree-S5903586217"><h3>Citation Tree for Statement S5903586217</h3><ul><li id="tree-S5903586217"><strong>S5903586217:</strong> Framework synthesis is an emerging systematic review method that can be applied to evaluate AI research synthesis capabilities, particularly in healthcare and policy contexts.<ul><li id="tree-E4382235507"><strong>E4382235507:</strong> Framework synthesis is one systematic review method employed to address health care practice and policy. Adapted from framework analysis methods, it has been used increasingly, using both qualitati... University College London Institute of Education, EPPI‐Centre, London, UKSearch for more papers by this author ... Framework synthesis is one systematic review method employed to address health care practice and policy. Adapted from framework analysis methods, it has been used increasingly, using both qualitative and mixed‐method systematic review methods. Louise Barry, Rose Galvin, Sylvia Murphy Tighe, Margaret O'Connor, Damian Ryan, Pauline Meskell, The barriers and facilitators to screening in emergency departments: a qualitative evidence synthesis (QES) protocol, HRB Open Research, 10.12688/hrbopenres.13073.1, 3, (50), (2020). Tracey Pérez Koehlmoos, Miranda Lynn Janvrin, Jessica Korona-Bailey, Cathaleen Madsen, Rodney Sturdivant, COVID-19 Self-Reported Symptom Tracking Programs in the United States: A Framework Synthesis (Preprint), Journal of Medical Internet Research, 10.2196/23297, (2020). This article demonstrates a spectrum of approaches to framework synthesis that are dependent on the extent to which theory is tentative, emergent, refined, or established; and that stakeholder involvement may help to understand the topic's complexity where theory is more nascent.<br><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399" target="_blank">https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399</a></li><li id="tree-E9510977536"><strong>E9510977536:</strong> University College London Institute of Education, EPPI‐Centre, London, UKSearch for more papers by this author ... Framework synthesis is one systematic review method employed to address health care practice and policy. Adapted from framework analysis methods, it has been used increasingly, using both qualitative and mixed‐method systematic review methods.<br><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399" target="_blank">https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399</a></li><li id="tree-E3656063237"><strong>E3656063237:</strong> Louise Barry, Rose Galvin, Sylvia Murphy Tighe, Margaret O'Connor, Damian Ryan, Pauline Meskell, The barriers and facilitators to screening in emergency departments: a qualitative evidence synthesis (QES) protocol, HRB Open Research, 10.12688/hrbopenres.13073.1, 3, (50), (2020).<br><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399" target="_blank">https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1399</a></li></ul></li></ul><a href="#cite-S5903586217" class="back-to-top">↑ Back to citation</a></div>
        </body>
        </html>
        