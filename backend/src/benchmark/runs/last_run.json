[
  {
    "id": "https://openalex.org/W4392203343",
    "text": "A Comprehensive Survey on Deep Graph Representation\nLearning\nWEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University,\nChina\nZIYUE QIAO, The Hong Kong University of Science and Technology, China\nYIFANG QIN and JIANHAO SHEN, Peking University, China\nFANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA\nJUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China\nYIFAN WANG, University of International Business and Economics, China\nXIAO LUO\u2217, University of California, Los Angeles, USA\nMING ZHANG\u2217, Peking University, China\nGraph representation learning aims to effectively encode high-dimensional sparse graph-structured data into\nlow-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields,\nincluding machine learning and data mining. Classic graph embedding methods follow the basic idea that the\nembedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby\npreserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i)\ntraditional methods have limited model capacity which limits the learning performance; (ii) existing techniques\ntypically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii)\nrepresentation learning and downstream tasks are dependent on each other which should be jointly enhanced.\nWith the remarkable success of deep learning, deep graph representation learning has shown great potential\nand advantages over shallow (traditional) methods, there exist a large number of deep graph representation\nlearning techniques have been proposed in the past decade, especially graph neural networks. In this survey,\nwe conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a\nnew taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential\ncomponents of graph representation learning and categorize existing approaches by the ways of graph neural\nnetwork architectures and the most recent advanced learning paradigms. Moreover, this survey also provides\nthe practical and promising applications of deep graph representation learning. Last but not least, we state\nnew perspectives and suggest challenging directions which deserve further investigations in the future.\nCCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations.\n\u2217Corresponding authors.\nAuthors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn;\nZequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871;\nZiyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453;\nYifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang\nSun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei\nYang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking\nUniversity, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics,\nBeijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang,\nmzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0004-5411/2024/2-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\narXiv:2304.05055v3 [cs.LG] 28 Feb 2024\n2 W. Ju, et al.\nAdditional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural\nNetwork, Survey\nACM Reference Format:\nWei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun,\nZhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024.\nA Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nGraphs have recently emerged as a powerful tool for representing a variety of structured and\ncomplex data, including social networks, traffic networks, information systems, knowledge graphs,\nprotein-protein interaction networks, and physical interaction networks. As a kind of general form\nof data organization, graph structures are capable of naturally expressing the intrinsic relationship\nof these data, and thus can characterize plenty of non-Euclidean structures that are crucial in\na variety of disciplines and domains due to their flexible adaptability. For example, to encode a\nsocial network as a graph, nodes on the graph are used to represent individual users, and edges are\nused to represent the relationship between two individuals, such as friends. In the field of biology,\nnodes can be used to represent proteins, and edges can be used to represent biological interactions\nbetween various proteins, such as the dynamic interactions between proteins. Thus, by analyzing\nand mining the graph-structured data, we can understand the deep meaning hidden behind the\ndata, and further discover valuable knowledge, so as to benefit society and human beings.\nIn the last decade years, a wide range of machine learning algorithms have been developed for\ngraph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410]\nusually break down graphs into different atomic substructures and then use kernel functions\nto measure the similarity between all pairs of them. Although graph kernels could provide a\nperspective on modeling graph topology, these approaches often generate substructures or feature\nrepresentations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer\nfrom high computational complexity, and therefore have weak scalability and subpar performance.\nIn the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have ever\u0002increasing emerged, which attempt to encode the structural information of the graph (usually a\nhigh-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to\npreserve the topology information and attribute information in the embedding space as much\nas possible, so that the learned graph embeddings can be naturally integrated into traditional\nmachine learning algorithms. Compared to previous works which use feature engineering in the\npre-processing phase to extract graph structural features, current graph embedding algorithms are\nconducted in a data-driven way leveraging machine learning algorithms (such as neural networks)\nto encode the structural information of the graph. Specifically, existing graph embedding methods\ncan be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354]\nthat factorize the matrix to learn node embedding which preserves the graph property; (ii) deep\nlearning based methods [155, 362, 443, 460] that apply deep learning techniques specifically de\u0002signed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either\nmaximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these\nmethods typically depend on shallow architectures, and fail to exploit the potential and capacity of\ndeep neural networks, resulting in sub-optimal representation quality and learning performance.\nInspired by the recent remarkable success of deep neural networks, a range of deep learning\nalgorithms has been developed for graph-structured data learning. The core of these methods is to\ngenerate effective node and graph representations using graph neural networks (GNNs), followed\nby a goal-oriented learning paradigm. In this way, the derived representations can be adaptively\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 3\ncoupled with a variety of downstream tasks and applications. Following this line of thought, in\nthis paper, we propose a new taxonomy to classify the existing graph representation learning\nalgorithms, i.e., graph neural network architectures, learning paradigms, and various promising\napplications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the\nstudies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer.\nFor the learning paradigms, we explore three advanced types namely supervised/semi-supervised\nlearning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate\nthe effectiveness of the learned graph representations, we provide several promising applications\nto build tight connections between representation learning and downstream tasks, such as social\nanalysis, molecular property prediction and generation, recommender systems, and traffic analysis.\nLast but not least, we present some perspectives for thought and suggest challenging directions\nthat deserve further study in the future.\nDifferences between this survey and existing ones. Up to now, there exist some other overview\npapers focusing on different perspectives of graph representation learning[17, 50, 53, 57, 227, 499,\n502, 577, 601, 603] that are closely related to ours. However, there are very few comprehensive\nreviews have summarized deep graph representation learning simultaneously from the perspective\nof diverse GNN architectures and corresponding up-to-date learning paradigms. Therefore, we\nhere clearly state their distinctions from our survey as follows. There have been several surveys\non classic graph embedding[42, 151], these works categorize graph embedding methods based on\ndifferent training objectives. Wang et al. [468] goes further and provides a comprehensive review of\nexisting heterogeneous graph embedding approaches. With the rapid development of deep learning,\nthere are a handful of surveys along this line. For example, Wu et al. [499] and Zhang et al. [577]\nmainly focus on several classical and representative GNN architectures without exploring deep\ngraph representation learning from a view of the most recent advanced learning paradigms such as\ngraph self-supervised learning and graph structure learning. Xia et al. [502] and Chami et al. [50]\njointly summarize the studies of graph embeddings and GNNs. Zhou et al. [601] explores different\ntypes of computational modules for GNNs. One recent survey under review [227] categorizes the\nexisting works in graph representation learning from both static and dynamic graphs. However,\nthese taxonomies emphasize the basic GNN methods but pay insufficient attention to the learning\nparadigms, and provide few discussions of the most promising applications, such as recommender\nsystems as well as molecular property prediction and generation. To the best of our knowledge, the\nmost relevant survey published formally is [603], which presents a review of GNN architectures\nand roughly discusses the corresponding applications. Nevertheless, this survey merely covers\nmethods up to the year of 2020, missing the latest developments in the past three years.\nTherefore, it is highly desired to summarize the representative GNN methods, the most recent\nadvanced learning paradigms, and promising applications into one unified and comprehensive\nframework. Moreover, we strongly believe this survey with a new taxonomy of literature and more\nthan 600 studies will strengthen future research on deep graph representation learning.\nContribution of this survey. The goal of this survey is to systematically review the literature\non the advances of deep graph representation learning and discuss further directions. It aims\nto help the researchers and practitioners who are interested in this area, and support them in\nunderstanding the panorama and the latest developments of deep graph representation learning.\nThe key contributions of this survey are summarized as follows:\n\u2022 Systematic Taxonomy. We propose a systematic taxonomy to organize the existing deep\ngraph representation learning approaches based on the ways of GNN architectures and the\nmost recent advanced learning paradigms via providing some representative branches of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n4 W. Ju, et al.\nGraph Self-Supervised Learning\nSemi-Supervised Learning on Graphs\nGraph Structure Learning\nLearning Paradigms\nGraph-Related Applications\nMolecular Generation\nMolecular Property Prediction\nSocial Analysis\nRecommender Systems\nTra c Analysis\nFuture Directions\nGraph Neural Network Architectures\nGraph Kernel Neural Networks\nGraph Pooling\nGraph Convolutions\nGraph Transformer\nGraph Representations\nGraph Data\nOptimized Graph Representations\nFig. 1. The architecture of this paper.\nmethods. Moreover, several promising applications are presented to illustrate the superiority\nand potential of graph representation learning.\n\u2022 Comprehensive Review. For each branch of this survey, we review the essential components\nand provide detailed descriptions of representative algorithms, and systematically summarize\nthe characteristics to make the overview comparison.\n\u2022 Future Directions. Based on the properties of existing deep graph representation learning\nalgorithms, we discuss the limitations and challenges of current methods and propose the\npotential as well as promising research directions deserving of future investigations.\n2 Background\nIn this section, we first briefly introduce some definitions in deep graph representation learning that\nneed to be clarified, and then we explain the reasons why we need graph representation learning.\n2.1 Problem Definition\nDefinition: Graph. Given a graph \ud835\udc3a = (\ud835\udc49 , \ud835\udc38, X), where \ud835\udc49 = {\ud835\udc631, \u00b7 \u00b7 \u00b7 , \ud835\udc63|\ud835\udc49 | } is the set of nodes,\n\ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52|\ud835\udc49 | } is the set of edges, and the edge \ud835\udc52 = (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38 represent the connection\nrelationship between nodes \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57in the graph. X \u2208 R\n|\ud835\udc49 |\u00d7\ud835\udc40 is the node feature matrix with\n\ud835\udc40 being the dimension of each node feature. The adjacency matrix of a graph can be defined as\nA \u2208 R\n|\ud835\udc49 |\u00d7 |\ud835\udc49 |\n, where A\ud835\udc56\ud835\udc57 = 1 if (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38, otherwise A\ud835\udc56\ud835\udc57 = 0.\nThe adjacency matrix can be regarded as the structural representation of the graph-structured\ndata, in which each row of the adjacency matrix A represents the connection relationship between\nthe corresponding node of the row and all other nodes, which can be regarded as a discrete repre\u0002sentation of the node. However, in real-life circumstances, the adjacency matrix A corresponding\nto \ud835\udc3a is a highly sparse matrix, and if A is used directly as node representations, it will be seriously\naffected by impractical storage demands and computational overhead. The storage space of the\nadjacency matrix A is |\ud835\udc49 |\u00d7 |\ud835\udc49 |, which is usually unacceptable when the total number of nodes grows\nto the order of millions. At the same time, the value of most dimensions in the node representation\nis 0. The sparsity will make subsequent machine learning tasks very difficult.\nGraph representation learning is a bridge between the original input data and the task objectives\nin the graph. The fundamental idea of the graph representation learning algorithm is first to learn\nthe embedded representations of nodes or the entire graph from the input graph structure data and\nthen apply these embedded representations to downstream related tasks, such as node classification,\ngraph classification, link prediction, community detection, and visualization, etc. Specifically, it\naims to learn low-dimensional, dense distributed embedding representations for nodes in the graph.\nFormally, the goal of graph representation learning is to learn its embedding vector representation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 5\nTable 1. Summary of traditional graph embedding methods.\nType Method Similarity measure Loss function (\ud835\udc3f)\nMatrix Factorization\nLLE [390] general |\ud835\udc67\ud835\udc56 \u2212\n\u00cd\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56 \ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57\n|\n2\nLE [11] general \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d,s.t.\ud835\udc4d\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c\nGF [4] \ud835\udc34\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nGraRep [46] \ud835\udc34\ud835\udc56,\ud835\udc57, \ud835\udc342\n\ud835\udc56,\ud835\udc57, ..., \ud835\udc34\ud835\udc58\n\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56\n, \ud835\udc67\ud835\udc57\u27e9|2\nHOPE [354] general |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nRandom Walk\nDeepWalk [362] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nNode2vec [155] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nHARP [59] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nLINE [443] Two-order Similarities Corresponding Loss\nNon-GNN Deep SDNE [460] Two-order Proximities Corresponding Loss\nDNGR [47] Two-order Proximities Corresponding Loss\n\ud835\udc45\ud835\udc63 \u2208 R\n\ud835\udc51\nfor each node \ud835\udc63 \u2208 \ud835\udc49 , where the dimension \ud835\udc51 of the vector is much smaller than the total\nnumber of nodes |\ud835\udc49 | in the graph.\n2.2 Traditional Graph Embedding\nTraditional graph embedding learning methods, as part of dimensionality reduction techniques,\naimed to embed graph data into a lower-dimensional vector space with the idea that connected nodes\nin the graph should still be closer to each other in this lower-dimensional space, thereby preserving\nthe structural information between nodes in the graph. Influenced by classical dimensionality\nreduction techniques, early graph embedding methods are primarily inspired by classic matrix\nfactorization techniques [25] and multi-dimensional scaling [245]. The following three sections\ndescribe these methods in more detail, distinguishing among matrix factorization-based methods,\nrandom walks-based methods and other non-GNN deep methods. In Table 1, we summarize different\ncategories of traditional graph embedding methods.\n2.2.1 Matrix factorization-based methods Matrix factorization-based methods are the early en\u0002deavors in graph embedding learning. These approaches can be outlined in a two-step process.\nIn the initial step, a proximity-based matrix is constructed for the graph, where each element of\nthe matrix represents the proximity measure between two nodes in the graph. Subsequently, a\ndimensionality reduction technique is employed on this matrix in the second step to generate the\nnode embeddings.\nLocally Linear Embedding (LLE) [390]. LLE assumes that node representations are sampled from\nthe same manifold space, and any node in the graph and its neighboring nodes are located in a\nlocal region of that manifold space. Therefore, node representations can be obtained by linearly\ncombining them with their neighboring nodes. LLE first constructs a local reconstruction weight\nmatrix, \ud835\udc4a\ud835\udc56\ud835\udc57 , for nodes in the graph to linearly combine neighboring nodes. By computing the\ndistance between the linear combination and the central node, the problem is reduced to solving\nfor matrix eigenvalues to learn low-dimensional vector representations for nodes. The objective\nfunction is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56 \u2212\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56\n\ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57|\n2\n, (1)\nwhere \ud835\udc67\ud835\udc56 represents the low-dimensional representation of the \ud835\udc56-th node, and \ud835\udc41\ud835\udc56is the set of\nneighboring nodes for the central node \ud835\udc56.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n6 W. Ju, et al.\nLaplacian Eigenmaps (LE) [11]. LE believes that nodes directly connected in graph data should\nbe kept as close as possible in the embedding space. Specifically, it achieves this by defining the\ndistance between connected nodes in the embedding space using the square of the Euclidean\ndistance. It transforms the final optimization objective into the computation of the Laplacian\nmatrix\u2019s eigenvectors. The objective function is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57\n|\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|\n2\ud835\udc4a\ud835\udc56\ud835\udc57 = \ud835\udc4d\ud835\udc47\n\ud835\udc3f\ud835\udc4d, s.t. \ud835\udc4d\n\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c, (2)\nwhere \ud835\udc4a\ud835\udc56\ud835\udc57 represents the connection weight between nodes \ud835\udc56 and \ud835\udc57 in the graph. After linear\ntransformation, the optimization of \ud835\udf19 (\ud835\udc4d) can be reformulated as \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d, where \ud835\udc3f = \ud835\udc37 \u2212\ud835\udc4a is the\nconstructed graph Laplacian matrix, and \ud835\udc37 is a symmetric matrix.\nGraph Factorization (GF) [4]. The matrix eigenvector-based methods mentioned before consider\nthe similarity between nodes throughout the entire graph, which can result in excellent node\nfeature representations. However, with the ever-growing scale of real-world graph data, computing\nmatrix eigenvectors for large graphs can be computationally expensive and memory-intensive.\nGF introduces a graph embedding method with a time complexity of \ud835\udc42(|\ud835\udc38|) by factorizing the\nadjacency matrix of the graph. The objective function is as follows:\n\ud835\udf19 (\ud835\udc4d, \ud835\udf06) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57 \u2208\ud835\udc38\n|\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2+\n\ud835\udf06\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56|\n2\n, (3)\nwhere \ud835\udf06 is a regularization coefficient, and \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9 represents the corresponding inner-product\noperation. Moreover, these inner-product methods also contain GraRep [46] and HOPE [354], which\nconsider higher-order and general node similarity respectively.\n2.2.2 Random walk-based methods Random walk-based methods have also attracted a lot of\nattention in graph embedding learning. The basic idea of these methods is to create random walks\namong nodes in the graph to capture its structural characteristics. Thus, nodes tend to have similar\nembedding if they co-occur on short random walks. Compared to fixed proximity measures in\ntraditional matrix factorization-based methods, these approaches use co-occurrence in a random\nwalk as a measure of node similarity, which is more flexible and has demonstrated promising\nperformance across various applications.\nDeepWalk [362]. DeepWalk analogizes nodes in a graph to words in text. It uses random walks\non the graph to generate numerous node sequences \ud835\udc46 = {\ud835\udc631, . . . , \ud835\udc63|\ud835\udc60 | }, treating these sequences as\nsentences, and then inputting them into the Word2vec [343], which aims to maximize the probability\nof node context given the target node \ud835\udc63\ud835\udc56. It can be written as:\n1\n|\ud835\udc46 |\n\u2211\ufe01\n|\ud835\udc46 |\n\ud835\udc56=1\n\u2211\ufe01\n\u2212\ud835\udc61 \u2264\ud835\udc57\u2264\ud835\udc61,\ud835\udc57\u22600\nlog \ud835\udc5d(\ud835\udc63\ud835\udc56+\ud835\udc57|\ud835\udc63\ud835\udc56), (4)\nwhere \ud835\udc61 is the context window size. Compared to matrix factorization-based methods, DeepWalk\nexhibits extremely low time complexity and is suitable for large-scale graph representation learning.\nHowever, DeepWalk only considers local information between nodes in the graph, making it\nchallenging to find the optimal random walk sampling sequences.\nNode2vec [155]. Based on DeepWalk, Node2vec utilizes parameters \ud835\udc5d and \ud835\udc5e to guide the random\nwalk. Parameter \ud835\udc5d allows the algorithm to revisit previously traversed nodes \ud835\udc61, with smaller\nvalues of \ud835\udc5d increasing the likelihood of returning to \ud835\udc61. Parameter \ud835\udc5e facilitates both inward and\noutward exploration; when \ud835\udc5e > 1, the algorithm tends to visit nodes closer to \ud835\udc61; while for \ud835\udc5e  1. And the first-order similarity can\nbe defined as:\n\ud835\udc3f2 =\n\u2211\ufe01\n(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57 ) \u2208\ud835\udc38\n\ud835\udc34\ud835\udc56\ud835\udc57 |\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|, (8)\nwhere \ud835\udc67\ud835\udc56is the learned representation of node \ud835\udc63\ud835\udc56.\nDeep Neural Graph Representations (DNGR) [47]. Similar to SDNE, DNGR utilizes pointwise\nmutual information between two nodes co-occurring in random walks instead of the adjacency\nmatrix values.\n2.3 Why study deep graph representation learning\nWith the rapid development of deep learning techniques, deep neural networks such as convolu\u0002tional neural networks and recurrent neural networks have made breakthroughs in the fields of\ncomputer vision, natural language processing, and speech recognition. They can well abstract the\nsemantic information of images, natural languages, and speeches. However, current deep learning\ntechniques fail to handle more complex and irregular graph-structured data. To effectively analyze\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n8 W. Ju, et al.\nand model this kind of non-Euclidean structure data, many graph representation learning algo\u0002rithms have emerged in recent years, including graph embedding and graph neural networks. At\npresent, compared with Euclidean-style data such as images, natural language, and speech, graph\u0002structured data is high-dimensional, complex, and irregular. Therefore, the graph representation\nlearning algorithm is a rather powerful tool for studying graph-structured data. To encode complex\ngraph-structured data, deep graph representation learning needs to meet several characteristics: (1)\ntopological properties: Graph representations need to capture the complex topological information\nof the graph, such as the relationship between nodes and nodes, and other substructure information,\nsuch as subgraphs, motif, etc; (2) feature attributes: It is necessary for graph representations to de\u0002scribe high-dimensional attribute features in the graph, including the attributes of nodes and edges\nthemselves; (3) scalability: Because different real graph data have different characteristics, graph\nrepresentation learning algorithms should be able to efficiently learn its embedding representation\non different graph structure data, making it universal and transferable.\n3 Graph Convolutions\nGraph convolutions have become the basic building blocks in many deep graph representation\nlearning algorithms and graph neural networks developed recently. In this section, we provide a\ncomprehensive review of graph convolutions, which generally fall into two categories: spectral\ngraph convolutions and spatial graph convolutions. Based on the solid mathematical foundations\nof Graph Signal Processing (GSP) [164, 396, 414], spectral graph convolutions seek to capture the\npatterns of the graph in the frequency domain. On the other hand, spatial graph convolutions\ninherit the idea of message passing from Recurrent Graph Neural Networks (RecGNNs), and they\ncompute node features by aggregating the features of their neighbors. Thus, the computation graph\nof a node is derived from the local graph structure around it, and the graph topology is naturally\nincorporated into the way node features are computed. In this section, we first introduce spectral\ngraph convolutions and then spatial graph convolutions, followed by a brief summary. In Table 2,\nwe summarize a number of graph convolutions proposed in recent years.\n3.1 Spectral Graph Convolutions\nWith the success of Convolutional Neural Networks (CNNs) in computer vision [244], efforts have\nbeen made to transfer the idea of convolution to the graph domain. However, this is not an easy task\nbecause of the non-Euclidean nature of graphical data. Graph signal processing (GSP) [164, 396, 414]\ndefines the Fourier Transform on graphs and thus provides a solid theoretical foundation of spectral\ngraph convolutions.\nIn graph signal processing, a graph signal refers to a set of scalars associated with every node\nin the graph, i.e. \ud835\udc53 (\ud835\udc63), \u2200\ud835\udc63 \u2208 \ud835\udc49 , and it can be written in the \ud835\udc5b-dimensional vector form x \u2208 R\n\ud835\udc5b\n,\nwhere \ud835\udc5b is the number of nodes in the graph. Another core concept of graph signal processing\nis the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian), defined as\nL = I \u2212 D\n\u22121/2AD\u22121/2\n, where I is the identity matrix, D is the degree matrix (i.e. a diagonal matrix\nD\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc57 A\ud835\udc56\ud835\udc57), and A is the adjacency matrix. In the typical setting of graph signal processing, the\ngraph \ud835\udc3a is undirected. Therefore, L is real symmetric and positive semi-definite. This guarantees\nthe eigen decomposition of the graph Laplacian: L = U\u039bU\ud835\udc47, where U = [u0, u1, ..., u\ud835\udc5b\u22121] is the\neigenvectors of the graph Laplacian and the diagonal elements of \u039b = diag(\ud835\udf060, \ud835\udf061, ..., \ud835\udf06\ud835\udc5b\u22121) are the\neigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal x is defined as x\u02dc = U\n\ud835\udc47 x,\nwhere x\u02dc is the graph frequencies of x. Correspondingly, the Inverse Graph Fourier Transform can\nbe written as x = Ux\u02dc.\nWith GFT and the Convolution Theorem, the graph convolution of a graph signal x and a filter\ng can be defined as g \u2217\ud835\udc3a x = U(U\n\ud835\udc47 g \u2299 U\ud835\udc47 x). To simplify this, let g\ud835\udf03 = diag(U\ud835\udc47 \ud835\udc54), the graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 9\nTable 2. Summary of graph convolution methods.\nMethod Category Aggregation Time Complexity\nSpectral CNN [39] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nHenaff et al. [172] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nChebNet [83] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGCN [230] Spectral / Spatial Weighted Average \ud835\udc42(\ud835\udc5a)\nCayleyNet [254] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGraphSAGE [163] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nGAT [452] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nDGCNN [477] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nLanzcosNet [280] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n2\n)\nSGC [493] Spatial Graph Convolution Weighted Average \ud835\udc42(\ud835\udc5a)\nGWNN [512] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGIN [518] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nGraphAIR [179] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nPNA [77] Spatial Graph Convolution Multiple \ud835\udc42(\ud835\udc5a)\nS\n2GC [606] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGNNML3 [21] Spatial / Spectral - \ud835\udc42(\ud835\udc5a)\nMSGNN [170] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nEGC [437] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nAPPNP [138] Spatial Graph Convolution (Approximate) Personalized Pagerank \ud835\udc42(\ud835\udc5a)\nGCNII [61] Spatial Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGATv2 [38] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nconvolution can be written as:\ng \u2217\ud835\udc3a x = Ug\ud835\udf03U\n\ud835\udc47\nx, (9)\nwhich is the general form of most spectral graph convolutions. The key of spectral graph convolu\u0002tions is to parameterize and learn the filter g\ud835\udf03 .\nSpectral Convolutional Neural Network (Spectral CNN) [39] sets graph filter as a learnable diagonal\nmatrix W. The convolution operation can be written as y = UWU\ud835\udc47 x. In practice, multi-channel\nsignals and activation functions are common, and the graph convolution can be written as\nY:,\ud835\udc57 = \ud835\udf0e\nU\n\u2211\ufe01\ud835\udc50\ud835\udc56\ud835\udc5b\n\ud835\udc56=1\nW\ud835\udc56,\ud835\udc57U\n\ud835\udc47 X:,\ud835\udc56!\n, \ud835\udc57 = 1, 2, ..., \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61, (10)\nwhere \ud835\udc50\ud835\udc56\ud835\udc5b is the number of input channel, \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 is the number of output channel, X is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc56\ud835\udc5b\nmatrix representing the input signal, Y is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix denoting the output signal, W\ud835\udc56,\ud835\udc57 is a\nparameterized diagonal matrix, and \ud835\udf0e(\u00b7) is the activation function. For mathematical convenience\nwe sometimes use single-channel versions of graph convolutions omitting activation functions,\nand the multi-channel versions are similar to Eq. 10.\nSpectral CNN has several limitations. Firstly, the filters are basis-dependent, which means that\nthey cannot be generalized across graphs. Secondly, the algorithm requires eigen decomposition,\nwhich is computationally expensive. Thirdly, it has no guarantee of spatial localization of filters.\nTo make filters spatially localized, Henaff et al. [172] propose to use a smooth spectral transfer\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n10 W. Ju, et al.\nfunction \u0398(\u039b) to parameterize the filter, and the convolution operation can be written as:\ny = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx. (11)\nChebyshev Spectral Convolutional Neural Network (ChebNet) [83] extends this idea by using\ntruncated Chebyshev polynomials to approximate the spectral transfer function. The Chebyshev\npolynomial is defined as \ud835\udc470 (\ud835\udc65) = 1, \ud835\udc471 (\ud835\udc65) = \ud835\udc65, \ud835\udc47\ud835\udc58 (\ud835\udc65) = 2\ud835\udc65\ud835\udc47\ud835\udc58\u22121 (\ud835\udc65) \u2212 \ud835\udc47\ud835\udc58\u22122 (\ud835\udc65), and the spectral\ntransfer function \ud835\udc39 (\u039b) is approximated to the order of \ud835\udc3e \u2212 1 as\n\ud835\udc39 (\u039b) =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc ), (12)\nwhere the model parameters \ud835\udf03\ud835\udc58, \ud835\udc58 \u2208 {0, 1, ..., \ud835\udc3e \u2212 1} are the Chebyshev coefficients, and \u039b\u02dc =\n2\u039b/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be written\nas:\ng \u2217\ud835\udc3a x = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx = U\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc )U\n\ud835\udc47\nx =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (L\u02dc)x, (13)\nwhere L\u02dc = 2L/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I.\nGraph Convolutional Network (GCN) [230] is proposed as the localized first-order approximation\nof ChebNet. Assuming \ud835\udc3e = 2 and \ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 = 2, Eq. 13 can be simplified as:\ng \u2217\ud835\udc3a x = \ud835\udf030x + \ud835\udf031 (L \u2212 I)x = \ud835\udf030x \u2212 \ud835\udf031D\n\u22121/2AD\u22121/2\nx. (14)\nTo further constraint the number of parameters, we assume \ud835\udf03 = \ud835\udf030 = \u2212\ud835\udf031, which gives a simpler\nform of graph convolution:\ng \u2217G x = \ud835\udf03 (I + D\n\u22121/2AD\u22121/2\n)x. (15)\nAs I + D\n\u22121/2AD\u22121/2 now has the eigenvalues in the range of [0, 2] and repeatedly multiplying\nthis matrix can lead to numerical instabilities, GCN empirically proposes a renormalization trick to\nsolve this problem by using D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2instead, where A\u02dc = A + I and D\u02dc\n\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc56 A\u02dc\n\ud835\udc56\ud835\udc57 .\nAllowing multi-channel signals and adding activation functions, the more common formula in\nliterature is:\nY = \ud835\udf0e( (D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2)X\u0398), (16)\nwhere X, Y have the same shape as in Eq. 10 and \u0398 is a \ud835\udc50\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix as model\u2019s parameters.\nApart from the aforementioned methods, other spectral graph convolutions have been proposed.\nLevie et al. [254] propose CayleyNets that utilize Cayley Polynomials to equip the filters with\nthe ability to detect narrow frequency bands. Liao et al. [280] propose LanczosNets that employ\nthe Lanczos algorithm to construct a low-rank approximation of graph Laplacian to improve the\ncomputation efficiency of graph convolutions. The proposed model is able to efficiently utilize the\nmulti-scale information in the graph data. Instead of using Graph Fourier Transform, Xu et al. [512]\npropose a Graph Wavelet Neural Network (GWNN) that uses graph wavelet transform to avoid\nmatrix eigendecomposition. Moreover, graph wavelets are sparse and localized, which provides\ngood interpretations for the convolution operation. Zhu and Koniusz [606] derive a Simple Spectral\nGraph Convolution (S2GC) from a modified Markov Diffusion Kernel, which achieves a trade-off\nbetween low-pass and high-pass filter bands.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 11\n3.2 Spatial Graph Convolutions\nInspired by the convolution on Euclidean data (e.g. images and texts), which applies data trans\u0002formation on a small region, spatial graph convolutions compute the central node\u2019s feature via\ntransforming and aggregating its neighbors\u2019 features. In this way, the graph structure is naturally\nembedded in the computation graph of node features. Moreover, the idea of sending one node\u2019s\nfeature to another node is similar to the message passing used in recurrent graph neural networks.\nIn the following, we will introduce several seminal spatial graph convolutions as well as some\nrecently proposed promising methods.\nSpatial graph convolutions generally follow a three-step paradigm: message generation, feature\naggregation and feature update. This can be mathematically written as:\ny\ud835\udc56 = UPDATE x\ud835\udc56, AGGREGATE {MESSAGE x\ud835\udc56, x\ud835\udc57, e\ud835\udc56\ud835\udc57\u0001, \ud835\udc57 \u2208 N (\ud835\udc56)}\u0001\u0001 , (17)\nwhere x\ud835\udc56 and y\ud835\udc56is the input and output feature vector of node \ud835\udc56, e\ud835\udc56\ud835\udc57 is the feature vector of the edge\n(or more generally, the relationship) between node \ud835\udc56 and its neighbor node \ud835\udc57, and N (\ud835\udc56) denote the\nneighbor of node \ud835\udc56, which could be more generally defined.\nIn the previous subsection, we show the spectral interpretation of GCN [230]. The model also\nhas its spatial interpretation, which can be mathematically written as:\ny\ud835\udc56 = \u0398\n\ud835\udc47 \u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\u222a\ud835\udc56\n1\n\u221a\ufe03\n\u02c6\ud835\udc51\ud835\udc56\u02c6\ud835\udc51\ud835\udc57\nx\ud835\udc57, (18)\nwhere \u02c6\ud835\udc51\ud835\udc56 and \u02c6\ud835\udc51\ud835\udc57is the \ud835\udc56-th and \ud835\udc57-th row sums of A\u02c6 in Eq. 16. For each node, the model takes a\nweighted sum of its neighbors\u2019 features as well as its own features and applies a linear transformation\nto obtain the result. In practice, multiple GCN layers are often stacked together with non-linear\nfunctions after convolution to encode complex and hierarchical features. Nonetheless, Wu et al.\n[493] show that the model still achieves competitive results without non-linearity.\nAlthough GCN as well as other spectral graph convolutions achieve competitive results on a\nnumber of benchmarks, these methods assume the presence of all nodes in the graph and fall in the\ncategory of transductive learning. Hamilton et al. [163] propose GraphSAGE that performs graph\nconvolutions in inductive settings, when there are new nodes during inference (e.g. newcomers\nin the social network). For each node, the model samples its \ud835\udc3e-hop neighbors and uses \ud835\udc3e graph\nconvolutions to aggregate their features hierarchically. Furthermore, the use of sampling also\nreduces the computation when a node has too many neighbors.\nThe attention mechanism has been successfully used in natural language processing [451], com\u0002puter vision [295] and multi-modal tasks [62, 168, 552, 591]. Graph Attention Networks (GAT) [452]\nintroduces the idea of attention to graphs. The attention mechanism uses an adaptive, feature\u0002dependent weight (i.e. attention coefficient) to aggregate a set of features, which can be mathemati\u0002cally written as:\n\ud835\udefc\ud835\udc56,\ud835\udc57 =\nexp LeakyReLU a\n\ud835\udc47\n[\u0398x\ud835\udc56||\u0398x\ud835\udc57]\n\u0001\u0001\n\u00cd\n\ud835\udc58 \u2208N (\ud835\udc56)\u222a{\ud835\udc56 } exp\nLeakyReLU a\n\ud835\udc47 [\u0398x\ud835\udc56\n||\u0398x\ud835\udc57]\n\u0001\u0001 , (19)\nwhere \ud835\udefc\ud835\udc56,\ud835\udc57 is the attention coefficient, a and \u0398 are model parameters, and [\u00b7||\u00b7] means concatenation.\nAfter the \ud835\udefcs are obtained, the new features are computed as a weighted sum of input node features,\nwhich is:\ny\ud835\udc56 = \ud835\udefc\ud835\udc56,\ud835\udc56\u0398x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\n\ud835\udefc\ud835\udc56,\ud835\udc57\u0398x\ud835\udc57. (20)\nXu et al. [518] explore the representational limitations of graph neural networks. What they\ndiscover is that message passing networks like GCN [230] and GraphSAGE [163] are incapable of\ndistinguishing certain graph structures. To improve the representational power of graph neural\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n12 W. Ju, et al.\nnetworks, they propose the Graph Isomorphism Network (GIN) that gives an adjustable weight to\nthe central node feature, which can be mathematically written as:\ny\ud835\udc56 = MLP \u00a9\n\u00ad\n\u00ab\n(1 + \ud835\udf16)x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\nx\ud835\udc57\n\u00aa\n\u00ae\n\u00ac\n, (21)\nwhere \ud835\udf16 is a learnable parameter.\nMore recently, efforts have been made to improve the representational power of graph neural\nnetworks. For example, Hu et al. [179] propose GraphAIR that explicitly models the neighborhood\ninteraction to better capture complex non-linear features. Specifically, they use the Hadamard\nproduct between pairs of nodes in the neighborhood to model the quadratic terms of neighborhood\ninteraction. Balcilar et al. [21] propose GNNML3 that breaks the limits of the first-order Weisfeiler\u0002Lehman test (1-WL) and reaches the third-order WL test (3-WL) experimentally. They also show\nthat the Hadamard product is required for the model to have more representational power than the\nfirst-order Weisfeiler-Lehman test. Other elements in spatial graph convolutions are widely studied.\nFor example, Corso et al. [77] explore the aggregation operation in GNN and proposes Principal\nNeighbourhood Aggregation (PNA) that uses multiple aggregators with degree-scalers. Tailor et al.\n[437] explore the anisotropism and isotropism in the message passing process of graph neural\nnetworks, and proposes Efficient Graph Convolution (EGC) that achieves promising results with\nreduced memory consumption due to isotropism. In order to increase the size of the neighborhood\nof a node, Gasteiger et al. [138] propose personalized propagation of neural predictions (PPNP) and\nits approximation using power iteration (APPNP). To increase the depth of graph neural networks,\nChen et al. [61] propose GCNII that uses initial residual and identity mapping to mitigate the over\u0002smoothing problem. Brody et al. [38] propose GATv2 that uses dynamic attention and improves\nthe expressive power of GAT [452].\n3.3 Summary\nThis section introduces graph convolutions. We provide the summary as follows:\n\u2022 Techniques. Graph convolutions mainly fall into two types, i.e. spectral graph convolu\u0002tions and spatial graph convolutions. Spectral graph convolutions have solid mathematical\nfoundations of Graph Signal Processing and therefore their operations have theoretical in\u0002terpretations. Spatial graph convolutions are inspired by Recurrent Graph Neural Networks\nand their computation is simple and straightforward, as their computation graph is derived\nfrom the local graph structure. Generally, spatial graph convolutions are more common in\napplications.\n\u2022 Challenges and Limitations. Despite the great success of graph convolutions, their perfor\u0002mance is unsatisfactory in more complicated applications. On the one hand, the performance\nof graph convolutions relies heavily on the construction of the graph. Different constructions\nof the graph might result in different performances of graph convolutions. On the other\nhand, graph convolutions are prone to over-smoothing when constructing very deep neural\nnetworks.\n\u2022 Future Works. In the future, we expect that more powerful graph convolutions will be\ndeveloped to mitigate the problem of over-smoothing and we also hope that techniques and\nmethodologies in Graph Structure Learning (GSL) can help learn more meaningful graph\nstructure to benefit the performance of graph convolutions.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 13\n4 Graph Kernel Neural Networks\nGraph kernels (GKs) are historically the most widely used technique on graph analyzing and\nrepresentation tasks [137, 243, 423, 601]. However, traditional graph kernels rely on hand-crafted\npatterns or domain knowledge on specific tasks[242, 410]. Over the years, an amount of research has\nbeen conducted on graph kernel neural networks (GKNNs), which has yielded promising results.\nResearchers have explored various aspects of GKNNs, including their theoretical foundations,\nalgorithmic design, and practical applications. These efforts have led to the development of a wide\nrange of GKNN-based models and methods that can be used for graph analysis and representation\ntasks, such as node classification [113, 222, 298, 534], link prediction [54, 300, 497, 525], and graph\nclustering [243, 299].\nThe success of GKNNs can be attributed to their ability to leverage the strengths of both graph\nkernels and neural networks [221, 299, 497]. By using kernel functions to measure similarity\nbetween graphs, GKNNs can capture the structural properties of graphs, while the use of neural\nnetworks enables them to learn more complex and abstract representations of graphs [56, 558].\nThis combination of techniques allows GKNNs to achieve state-of-the-art performance on a wide\nrange of graph-related tasks [216, 243, 479].\nIn this section, we begin with introducing the most representative traditional graph kernels.\nThen we summarize the basic framework for combining GNNs and graph kernels. Finally, we\ncategorize the popular graph kernel Neural networks into several categories and compare their\ndifferences.\n4.1 Graph Kernels\nGraph kernels generally evaluate pairwise similarity between nodes or graphs by decomposing\nthem into basic structural units. Random walks [223], subtrees [409], shortest paths [32] and\ngraphlets [410] are representative categories.\nGiven two graphs \ud835\udc3a1 = (\ud835\udc491, \ud835\udc381, \ud835\udc4b1) and \ud835\udc3a2 = (\ud835\udc492, \ud835\udc382, \ud835\udc4b2), a graph kernel function \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2)\nmeasures the similarity between \ud835\udc3a1 and \ud835\udc3a2 through the following formula:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 \ud835\udc59\ud835\udc3a1(\ud835\udc621),\ud835\udc59\ud835\udc3a2(\ud835\udc622)\n\u0001\n, (22)\nwhere \ud835\udc59\ud835\udc3a (\ud835\udc62) denotes a set of local substructures centered at node \ud835\udc62 in graph \ud835\udc3a, and \ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 is a base\nkernel measuring the similarity between the two sets of substructures. For simplicity, we may\nrewrite Eq. 22 as:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 (\ud835\udc621, \ud835\udc622), (23)\nthe uppercase letter \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) is denoted as graph kernels, \ud835\udf05(\ud835\udc621, \ud835\udc622) is denoted as node kernels,\nand lowercase \ud835\udc58 (\ud835\udc65, \ud835\udc66) is denoted as general kernel functions.\nThe kernel mapping of a kernel \ud835\udf13 maps a data point into its corresponding Reproducing Kernel\nHilbert Space (RKHS) H. Specifically, given a kernel \ud835\udc58\u2217 (\u00b7, \u00b7), its kernel mapping\ud835\udf13\u2217 can be formalized\nas,\n\u2200\ud835\udc651, \ud835\udc652, \ud835\udc58\u2217 (\ud835\udc651, \ud835\udc652) = \u27e8\ud835\udf13\u2217 (\ud835\udc651),\ud835\udf13\u2217 (\ud835\udc652)\u27e9H\u2217, (24)\nwhere H\u2217 is the RKHS of \ud835\udc58\u2217 (\u00b7, \u00b7).\nWe introduce several representative and popular graph kernels below.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n14 W. Ju, et al.\nWalk and Path Kernels. A \ud835\udc59-walk kernel \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 compares all length \ud835\udc59 walks starting from each\nnode in two graphs \ud835\udc3a1,\ud835\udc3a2,\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc641\u2208W\ud835\udc59(\ud835\udc3a1,\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc642\u2208W\ud835\udc59(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc4b1 (\ud835\udc641), \ud835\udc4b2 (\ud835\udc642)),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622).\n(25)\nSubstituting W with P is able to get the \ud835\udc59-path kernel.\nSubtree Kernels. The WL subtree kernel is the most popular one in subtree kernels. It is a finite\u0002depth kernel variant of the 1-WL test. The WL subtree kernel with depth \ud835\udc59, \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f compares all\nsubtrees with depth \u2264 \ud835\udc59 rooted at each node.\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc611\u2208 T\ud835\udc56(\ud835\udc3a1,\ud835\udc622 )\n\u2211\ufe01\n\ud835\udc612\u2208 T\ud835\udc56(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc611, \ud835\udc612),\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc59\n\ud835\udc56=0\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2),\n(26)\nwhere \ud835\udc61 \u2208 T (\ud835\udc56)(\ud835\udc3a, \ud835\udc62) denotes a subtree of depth \ud835\udc56 rooted at \ud835\udc62 in \ud835\udc3a.\n4.2 General Framework of GKNNs\nIn this section, we summarize the general framework of GKNNs. For the first step, a kernel that\nmeasures similarities of heterogeneous features from heterogeneous nodes and edges (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc622) and\n(\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622) should be defined. Take the inner product of neighbor tensors as an example, its neighbor\nkernel is defined as follows,\n\ud835\udf05( (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc621), (\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622)) = \u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc621), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc622)\u27e9.\nBased on the neighbor kernel, a kernel with two \ud835\udc59-hop neighborhoods for central node \ud835\udc621 and \ud835\udc622\ncan be defined as \ud835\udc3e\n(\ud835\udc59)\n(\ud835\udc621, \ud835\udc622) =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \ud835\udc59 = 0\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u2211\ufe01\n\ud835\udc631\u2208\ud835\udc41 (\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc632\u2208\ud835\udc41 (\ud835\udc622 )\n\ud835\udc3e\n(\ud835\udc59\u22121)\n(\ud835\udc631, \ud835\udc632) \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc631), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc632\n)\u27e9 \ud835\udc59 > 0\n, (27)\nBy regarding the lower-hop kernel \ud835\udf05\n(\ud835\udc59\u22121)\n(\ud835\udc621, \ud835\udc622), as the inner product of the (\ud835\udc59 \u2212 1)-th hidden\nrepresentations of \ud835\udc621 and \ud835\udc622. Furthermore, by recursively applying the neighborhood kernel, the\n\ud835\udc59-hop graph kernel can be derived as\n\ud835\udc3e\n\ud835\udc59\n(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc981\u2208W\ud835\udc59(\ud835\udc3a1 )\n\u2211\ufe01\n\ud835\udc982\u2208W\ud835\udc59(\ud835\udc3a2 )\n\u00d6\n\ud835\udc59\u22121\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n1\n), \ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n2\n)\u27e9 \u00d7\u00d6\n\ud835\udc59\u22122\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n1\n,\ud835\udc98\n(\ud835\udc56+1)\n1\n), \ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n2\n,\ud835\udc98\n(\ud835\udc56+1)\n2\n)\u27e9!,\n(28)\nwhere W\ud835\udc59(\ud835\udc3a) denotes the set of all walk sequences with length \ud835\udc59 in graph \ud835\udc3a, and \ud835\udc98\n(\ud835\udc56)\n1\ndenotes the\n\ud835\udc56-th node in sequence \ud835\udc981.\nAs shown in Eq. 24, kernel methods implicitly perform projections from original data spaces\nto their RKHS H. Hence, as GNNs also project nodes or graphs into vector spaces, connections\nhave been established between GKs and GNNs through the kernel mappings. And several works\nconducted research on the connections [253, 491], and found some foundation conclusions. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 15\nthe basic rule introduced in [253] as an example, the proposed graph kernel in Eq. 22 can be derived\nas the general formulas,\n\u210e\n(0)\n(\ud835\udc63) =\ud835\udc7e\n(0)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63),\n\u210e\n(\ud835\udc59)\n(\ud835\udc63) =\ud835\udc7e\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63) \u2299 \u2211\ufe01\n\ud835\udc62\u2208\ud835\udc41 (\ud835\udc63)\n(\ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\u210e\n(\ud835\udc59\u22121)\n(\ud835\udc62) \u2299 \ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc38 (\ud835\udc52\ud835\udc62,\ud835\udc63 )\n\ud835\udc53 (\ud835\udc52\ud835\udc62,\ud835\udc63 )), 1  1-WL\nGGT [102] \u2713 \u2713 structure only \u2713\nGTSA [241] \u2713 \u2713 \u2713 \u2713\nHGT [183] \u2713 \u2713\nG2SHGT [536] \u2713 \u2713 \u2713\nHINormer [335] \u2713 \u2713 \u2713\nGRUGT [41] \u2713 \u2713 \u2713\nGRIT [324] \u2713 \u2713 \u2713\nGraphormer-GD [560] \u2713 \u2713 \u2713\nGraphormer [540] \u2713 \u2713 \u2713 \u2713\nGSGT [191] \u2713 \u2713 \u2713\nTMDG [145] \u2713 \u2713 \u2713\nGraph-BERT [567] \u2713 \u2713 \u2713\nLRGT [498] \u2713 \u2713\nSAT [56] \u2713 \u2713 \u2713\n6.1 Transformer\nTransformer [451] was first applied to model machine translation, but two of the key mechanisms\nadopted in this work, attention operation and positional encoding, are highly compatible with the\ngraph modeling problem.\nTo be specific, we denote the input of attention layer in Transformer as X = [x0, x1, . . . , x\ud835\udc5b\u22121],\nx\ud835\udc56 \u2208 R\n\ud835\udc51\n, where \ud835\udc5b is the length of input sequence and \ud835\udc51 is the dimension of each input embedding\nx\ud835\udc56. Then the core operation of calculating new embedding x\u02c6\ud835\udc56 for each x\ud835\udc56in attention layer can be\nstreamlined as:\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57) = NORM\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\nQ\n\u210e\n(x\ud835\udc56)\nTK\u210e\n(x\ud835\udc58 )),\nx\n\u210e\n\ud835\udc56 =\n\u2211\ufe01\nx\ud835\udc57 \u2208X\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57)V\u210e(x\ud835\udc57),\nx\u02c6\ud835\udc56 = MERGE(x\n1\n\ud835\udc56\n, x\n2\n\ud835\udc56\n, . . . , x\n\ud835\udc3b\n\ud835\udc56\n),\n(57)\nwhere \u210e \u2208 {0, 1, . . . , \ud835\udc3b \u2212 1} represents the attention head number. Q\n\u210e\n, K\u210eand V\u210eare projection\nfunctions mapping a vector to the query space, key space and value space respectively. s\n\u210e\n(x\ud835\udc56\n, x\ud835\udc57) is\nscore function measuring the similarity between x\ud835\udc56 and x\ud835\udc57. NORM is the normalization operation\nensuring \u00cd\nx\ud835\udc57 \u2208X s\n\u210e\n(x\ud835\udc56, x\ud835\udc57) \u2261 1 to propel the stability of the output generated by a stack of attention\nlayers, it is usually performed as scaled softmax: NORM(\u00b7) = SoftMax(\u00b7/\u221a\ud835\udc51). And MERGE function\nis designed to combine the information extracted from multiple attention heads. Here, we omit\nfurther implementation details that do not affect our understanding of attention operation.\nThe attention process cannot encode the position information of each x\ud835\udc56, which is essential in\nmachine translation problems. So positional encoding is introduced to remedy this deficiency, and\nit\u2019s calculated as:\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57\n= sin(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), X\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57+1\n= cos(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), (58)\nwhere \ud835\udc56 is the position and \ud835\udc57 is the dimension. The positional encoding is added to the input before\nit is fed to the Transformer.\n6.2 Overview\nFrom the simplified process shown in Eq. 57, we can see that the core of the attention operation is\nto accomplish information transfer based on the similarity between the source and the target to be\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 25\nupdated. It\u2019s quite similar to the message-passing process on a fully-connected graph. However,\nthe direct application of this architecture to arbitrary graphs does not make use of structural\ninformation, so it may lead to poor performance when graph topology is important. On the other\nhand, the definition of positional encoding in graphs is not a trivial problem because the order or\ncoordinates of graph nodes are underdefined.\nAccording to these two challenges, Transformer-based methods for graph representation learning\ncan be classified into two major categories, one considering graph structure during the attention\nprocess, and the other encoding the topological information of the graph into initial node features.\nWe name the first one as Attention Modification and the second one as Encoding Enhancement. A\nsummarization is provided in Table 5. In the following discussion, if both methods are used in\none paper, we will list them in different subsections, and we will ignore the multi-head trick in\nattention operation.\n6.3 Attention Modification\nThis group of works attempts to modify the full attention operation to capture structure information.\nThe most prevalent approach is changing the score function, which is denoted as s(\u00b7, \u00b7) in Eq. 57.\nGGT [102] constrains each node feature can only attend to neighbors and enables the model to\nrepresent edge feature information by rewrite s(\u00b7, \u00b7) as:\ns\u02dc1 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\ud835\udc44x\ud835\udc56)\nT\n(W\ud835\udc3ex\ud835\udc57 \u2299 W\ud835\udc38e\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\u2212 \u221e, otherwise\n,\ns1 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc1 (x\ud835\udc56, x\ud835\udc58 )),\n(59)\nwhere \u2299 is Hadamard product and W\ud835\udc44,\ud835\udc3e,\ud835\udc38 represents trainable parameter matrix. This approach\nis not efficient yet to model long-distance dependencies since only 1st-neighbors are considered.\nThough it adopts Laplacian eigenvectors to gather global information (see Section 6.4), but only long\u0002distance structure information is remedied while the node and edge features are not. GTSA [241]\nimproves this approach by combining the original graph and the full graph. Specifically, it extends\ns1 (\u00b7, \u00b7) to:\ns\u02dc2 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\n\ud835\udc44\n1\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n1\nx\ud835\udc57 \u2299 W\ud835\udc38\n1\ne\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n(W\n\ud835\udc44\n0\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n0\nx\ud835\udc57 \u2299 W\ud835\udc38\n0\ne\ud835\udc57\ud835\udc56), otherwise\n,\ns2 (x\ud835\udc56, x\ud835\udc57) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\n\uf8f3\n1\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2208\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\ud835\udf06\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2209\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), otherwise\n,\n(60)\nwhere \ud835\udf06 is a hyperparameter representing the strength of the full connection.\nSome works try to reduce information-mixing problems [55] in heterogeneous graphs. HGT [183]\ndisentangles the attention of different node types and edge types by adopting additional attention\nheads. It defines W\ud835\udf0f (\ud835\udc63)\n\ud835\udc44,\ud835\udc3e,\ud835\udc49 for each node type \ud835\udf0f (\ud835\udc63) and W\n\ud835\udf19 (\ud835\udc52 )\n\ud835\udc38\nfor each edge type \ud835\udf19 (\ud835\udc52), \ud835\udf0f (\u00b7) and\n\ud835\udf19 (\u00b7) are type indicating function. G2SHGT [536] defines four types of subgraphs, fully-connected,\nconnected, default and reverse, to capture global, undirected, forward and backward information\nrespectively. Each subgraph is homogeneous, so it can reduce interactions between different classes.\nPath features between nodes are always treated as inductive bias added to the original score\nfunction. Let SP\ud835\udc56\ud835\udc57 = (\ud835\udc521, \ud835\udc522, . . . , \ud835\udc52\ud835\udc41 ) denote the shortest path between node pair (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57). GRUGT [41]\nuses GRU [74] to encode forward and backward features as: r\ud835\udc56\ud835\udc57 = GRU(SP\ud835\udc56\ud835\udc57), r\ud835\udc57\ud835\udc56 = GRU(SP\ud835\udc57\ud835\udc56).\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n26 W. Ju, et al.\nThen, the final attention score is calculated by adding up four components:\ns\u02dc3 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56, (61)\nfrom front to back, which represent content-based score, source-dependent bias, target-dependent\nbias and universal bias respectively. Graphormer [540] uses both path length and path embedding\nto introduce structural bias as:\ns\u02dc4 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 /\n\u221a\n\ud835\udc51 + \ud835\udc4f\ud835\udc41 + \ud835\udc50\ud835\udc56\ud835\udc57,\n\ud835\udc50\ud835\udc56\ud835\udc57 =\n1\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc58=1\n(e\ud835\udc58 )\nTw\ud835\udc38\n\ud835\udc58\n,\ns4 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc4 (x\ud835\udc56, x\ud835\udc58 )),\n(62)\nwhere \ud835\udc4f\ud835\udc41 is a trainable scalar indexed by \ud835\udc41, the length of SP\ud835\udc56\ud835\udc57 . e\ud835\udc58 is the embedding of the the\nedge \ud835\udc52\ud835\udc58 , and w\ud835\udc38\n\ud835\udc58\n\u2208 R\n\ud835\udc51\nis the \ud835\udc58-th edge parameter. If SP\ud835\udc56\ud835\udc57 does not exist, then \ud835\udc4f\ud835\udc41 and \ud835\udc50\ud835\udc56\ud835\udc57 are set to\nbe special values. GRIT [324] utilizes relative random walk probabilities as an inductive bias to\nencode relative path information. Graphormer-GD [560] also incorporates relative distance as bias,\nand rigorously proves that this bias is crucial for determining the biconnectivity of a graph.\n6.4 Encoding Enhancement\nThis kind of method intends to enhance initial node representations to enable the Transformer to\nencode structure information. They can be further divided into two categories, position-analogy\nmethods and structure-aware methods.\n6.4.1 Position-analogy methods In Euclidean space, the Laplacian operator corresponds to the\ndivergence of the gradient, whose eigenfunctions are sine/cosine functions. For the graph, the\nLaplacian operator is the Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.\nHence, inspired by Eq. 58, position-analogy methods utilize Laplacian eigenvectors to simulate\npositional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as they are the equivalents of sine/cosine functions.\nLaplacian eigenvectors can be calculated via the eigendecomposition of normalized graph Lapla\u0002cian matrix L\u02dc\n:\nL\u02dc \u225c I \u2212 D\n\u22121/2AD\u22121/2 = U\u039bUT\n, (63)\nwhere A is the adjacency matrix, D is the degree matrix, U = [u1, u2, . . . , u\ud835\udc5b\u22121] are eigenvectors\nand \u039b = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udf060, \ud835\udf061, . . . , \ud835\udf06\ud835\udc5b\u22121) are eigenvalues. With U and \u039b, GGT [102] uses eigenvectors of the\nk smallest non-trivial eigenvalues to denote the intermediate embedding X\n\ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\n, and maps it\nto d-dimensional space and gets the position encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\n. This process can be formalized\nas:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 = argmin\ud835\udc58({\ud835\udf06\ud835\udc56|0 \u2264 \ud835\udc56  0}),\nX\n\ud835\udc5a\ud835\udc56\ud835\udc51 = [u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc650\n, u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc651, . . . , u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc58\u22121]\nT\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = X\ud835\udc5a\ud835\udc56\ud835\udc51W\ud835\udc58\u00d7\ud835\udc51\n,\n(64)\nwhere \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 is the subscript of the selected eigenvectors. GTSA [241] puts eigenvector u\ud835\udc56 on\nthe frequency axis at \ud835\udf06\ud835\udc56 and uses sequence modeling methods to generate positional encoding.\nSpecifically, it extends X\n\ud835\udc5a\ud835\udc56\ud835\udc51 in Eq. 64 to X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\u00d72 by concatenating each value in eigenvectors\nwith corresponding eigenvalue, and then positional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\nare generated as:\nX\n\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 = X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51W2\u00d7\ud835\udc51\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = SumPooling(Transformer(X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61), dim = 1).\n(65)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 27\nHere, X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 R\n\ud835\udc5b\u00d7\ud835\udc58\u00d7\ud835\udc51\nis equivalent to the input matrix in sequence modeling with shape\n(\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e, \ud835\udc51\ud835\udc56\ud835\udc5a), and can be naturally processed by Transformer. Since the Laplacian\neigenvectors can be complex-valued for directed graph, GSGT [191] proposes to utilize SVD of\nadjacency matrix A, which is denoted as A = U\u03a3VT, and uses the largest \ud835\udc58 singular values \u03a3\ud835\udc58 and\nassociated left and right singular vectors U\ud835\udc58 and V\nT\n\ud835\udc58\nto output X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as X\ud835\udc5d\ud835\udc5c\ud835\udc60 = [U\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n\u2225V\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n],\nwhere \u2225 is the concatenation operation. In addition to SVD, TMDG [145] processes directed graphs\nby utilizing the Magnetic Laplacian. All these methods above randomly flip the signs of eigenvectors\nor singular vectors during the training phase to promote the invariance of the models to the sign\nambiguity.\n6.4.2 Structure-aware methods In contrast to position-analogy methods, structure-aware methods\ndo not attempt to mathematically rigorously simulate sequence positional encoding. They use some\nadditional mechanisms to directly calculate structure-related encoding.\nSome approaches compute extra encoding X\n\ud835\udc4e\ud835\udc51\ud835\udc51 and add it to the initial node representation.\nGraphormer [540] proposes to leverage node centrality as an additional signal to address the\nimportance of each node. Concretely, x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nis determined by the in-degree deg\u2212\n\ud835\udc56\nand outdegree deg+\n\ud835\udc56\n:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56 = P\n\u2212\n(deg\u2212\n\ud835\udc56\n) + P+(deg+\n\ud835\udc56\n), (66)\nwhere P\n\u2212\nand P\n+\nare learnable embedding function. Graph-BERT [567] employs Weisfeiler-Lehman\nalgorithm to label node \ud835\udc63\ud835\udc56 to a number WL(\ud835\udc63\ud835\udc56) \u2208 N and defines x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nas:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57 = sin(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n), x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57+1 = cos(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n). (67)\nThe other approaches try to leverage GNNs to initialize inputs to the Transformer. LRGT [498]\napplies GNN to get intermediate vectors as X\n\u2032 = GNN(X), and passes the concatenation of X\u2032\nand\na special vector xCLS to Transformer layer as: X\u02c6 = Transformer( [X\n\u2032\n\u2225xCLS]). Then x\u02c6CLS can be used\nas the representation of the entire graph for downstream tasks. This method cannot break the 1-WL\nbottleneck because it uses GCN [230] and GIN [518] as graph encoders in the first step, which\nare intrinsically limited by 1-WL test. SAT [56] improves this deficiency by using subgraph-GNN\nNGNN [569] for initialization, and achieves outstanding performance.\n6.5 Summary\nThis section introduces Transformer-based approaches for graph representation learning and we\nprovide the summary as follows:\n\u2022 Techniques. Graph Transformer methods modify two fundamental techniques in Trans\u0002former, attention operation and positional encoding, to enhance its ability to encode graph\ndata. Typically, they introduce fully connected attention to model long-distance relationships,\nutilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck, and separate\npoints and edges belonging to different classes to avoid over-mixing problems.\n\u2022 Challenges and Limitations. Though Graph Transformers achieve encouraging perfor\u0002mance, they still face two major challenges. The first challenge is the computational cost of\nthe quadratic attention mechanism and shortest path calculation. These operations require\nsignificant computing resources and can be a bottleneck, particularly for large graphs. The\nsecond is the reliance of Transformer-based models on large amounts of data for stable perfor\u0002mance. It poses a challenge when dealing with problems that lack sufficient data, especially\nfor few-shot and zero-shot settings.\n\u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further\nexplored. Additionally, there are some works using pre-training and fine-tuning frameworks\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n28 W. Ju, et al.\nto balance performance and complexity in downstream tasks [540], this may be a promising\nsolution to address the aforementioned two challenges.\n7 Semi-supervised Learning on Graphs\nWe have investigated various architectures of graph neural networks in which the parameters should\nbe tuned by a learning objective. The most prevalent optimization approach is supervised learning on\ngraph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention\nin the data mining community. In detail, these methods attempt to combine graph representation\nlearning with current semi-supervised techniques including pseudo-labeling, consistency learning,\nknowledge distillation and active learning. These works can be further subdivided into node-level\nrepresentation learning and graph-level representation learning. We would introduce both parts in\ndetail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6.\n7.1 Node Representation Learning\nTypically, node representation learning follows the concept of transductive learning, which has\naccess to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised\nloss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is\nusually adopted for optimization. In formulation,\nL\ud835\udc41 \ud835\udc46\ud835\udc3f = \u2212\n1\n|Y\ud835\udc3f |\n\u2211\ufe01\n\ud835\udc56\u2208Y\ud835\udc3f\ny\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (68)\nwhere Y\ud835\udc3f denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that\ncan be used to offer semantic information. To fully utilize these nodes, a range of methods attempt\nto combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a\nfundamental semi-supervised technique that uses the classifier to produce the label distribution of\nunlabeled examples and then adds appropriately labeled examples to the training set [265, 604].\nAnother line of semi-supervised learning is consistency regularization [247] that requires two\nexamples to have identical predictions under perturbation. This regularization is based on the\nassumption that each instance has a distinct label that is resistant to random perturbations [118, 357].\nThen, we show several representative works in detail.\nCooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-label\u0002based GNN approach for semi-supervised node classification. It employs two GNN classifiers to\njointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows:\n\ud835\udc36\ud835\udc49 (p\ud835\udc56) = p\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (69)\nwhere p\ud835\udc56 denotes the output label distribution. Then it selects the pseudo-labels with high confidence\ngenerated from one model to supervise the optimization of the other model. In particular, the\nobjective for unlabeled nodes is written as follows:\nL\ud835\udc36\ud835\udc5c\ud835\udc3a\ud835\udc41 \ud835\udc52\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc56\u2208V\ud835\udc48\n1\ud835\udc36\ud835\udc49 (p\ud835\udc56 )>\ud835\udf0fy\u02c6\n\ud835\udc47\n\ud835\udc56\n\ud835\udc59\ud835\udc5c\ud835\udc54q\ud835\udc56, (70)\nwhere y\u02c6\ud835\udc56 denotes the one-hot formulation of the pseudo-label \ud835\udc66\u02c6\ud835\udc56 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65p\ud835\udc56 and q\ud835\udc56 denotes\nthe label distribution predicted by the other classifier. \ud835\udf0f is a pre-defined temperature coefficient.\nThis cross supervision has been demonstrated effective in [64, 312] to prevent the provision of\nbiased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information\nfrom a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still\ngenerate the same prediction. In this way, CoGNet can illustrate the entire optimization process to\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 29\nTable 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered\nas a specific kind of consistency learning.\nApproach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning\nNode-level\nCoGNet [265] \u2713\nDSGCN [604] \u2713\nGRAND [118] \u2713\nAugGCR [357] \u2713\nHCPL [309] \u2713\nGraph-level\nSEAL [264] \u2713 \u2713\nInfoGraph [431] \u2713 \u2713\nDSGC [527] \u2713\nASGN [166] \u2713 \u2713\nTGNN [218] \u2713\nKGNN [221] \u2713\nHGMI [262] \u2713 \u2713\nASGNN [508] \u2713 \u2713\nDualGraph [310] \u2713 \u2713\nGLA [556] \u2713\nSS [507] \u2713\nenhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in\nsemi-supervised node classification, which can generate dynamics thresholds for reliable nodes.\nDynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive\nmanner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller\nweights to samples with lower confidence with the additional consideration of class balance. The\nweight is formulated as:\n\ud835\udf14\ud835\udc56 =\n1\n\ud835\udc5b\ud835\udc50\n\ud835\udc56\nmax (RELU (p\ud835\udc56 \u2212 \ud835\udefd \u00b7 1)) , (71)\nwhere \ud835\udc5b\ud835\udc50\n\ud835\udc56 denotes the number of unlabeled samples assigned to the class \ud835\udc50\n\ud835\udc56\n. This technique will\ndecrease the impact of wrong pseudo-labels during iterative training.\nGraph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learning\u0002based method. It first adds a variety of perturbations to the input graph to generate a list of\ngraph views. Each graph view \ud835\udc3a\n\ud835\udc5f\nis sent to a GNN classifier to produce a prediction matrix\nP\n\ud835\udc5f = [p\ud835\udc5f\n1\n, \u00b7 \u00b7 \u00b7 , p\n\ud835\udc5f\n\ud835\udc41\n]. Then it summarizes these matrices as:\nP =\n1\n\ud835\udc45\nP\n\ud835\udc5f\n. (72)\nTo provide more discriminative information and ensure that the matrix is row-normalized,\nGRAND sharpens the summarized label matrix into P\n\ud835\udc46\ud835\udc34 as:\nP\n\ud835\udc46\ud835\udc34\n\ud835\udc56\ud835\udc57 =\nP\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\n\u00cd\n\ud835\udc57\n\u2032=0 P\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\u2032\n, (73)\nwhere \ud835\udc47 is a given temperature parameter. Finally, consistency learning is performed by comparing\nthe sharpened summarized matrix with the matrix of each graph view. Formally, the objective is:\nL\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 =\n1\n\ud835\udc45\n\u2211\ufe01\n\ud835\udc45\n\ud835\udc5f=1\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n||P\n\ud835\udc46\ud835\udc34\n\ud835\udc56 \u2212 P\ud835\udc56\n||, (74)\nhere L\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 serves as a regularization which is combined with the standard supervised loss.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n30 W. Ju, et al.\nAugmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with\nthe generation of augmented graphs by random dropout and mixup of different order features. To\nenhance the model generalization, it borrows the idea of meta-learning to partition the training data,\nwhich improves the quality of graph augmentation. In addition, it utilizes consistency regularization\nto enhance the semi-supervised node classification.\n7.2 Graph Representation Learning\nThe objective of graph classification is to predict the property of the whole graph example.\nAssuming that the training set comprises \ud835\udc41\n\ud835\udc59\nand \ud835\udc41\n\ud835\udc62 graph samples G\ud835\udc59 = {\ud835\udc3a1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59\n} and\nG\n\ud835\udc62 = {\ud835\udc3a\ud835\udc41\n\ud835\udc59 +1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59 +\ud835\udc41\ud835\udc62\n}, the graph-level supervised loss for labeled data can be expressed as\nfollows:\nL\ud835\udc3a\ud835\udc46\ud835\udc3f = \u2212\n1\n|G\ud835\udc62 |\n\u2211\ufe01\n\ud835\udc3a\ud835\udc57 \u2208 G\ud835\udc3f\ny\n\ud835\udc57\ud835\udc47\n\ud835\udc59\ud835\udc5c\ud835\udc54p\n\ud835\udc57\n, (75)\nwhere y\n\ud835\udc57 denotes the one-hot label vector for the \ud835\udc57-th sample while p\ud835\udc57 denotes the predicted\ndistribution of \ud835\udc3a\n\ud835\udc57\n. When \ud835\udc41\n\ud835\udc62 = 0, this objective can be utilized to optimize supervised methods.\nHowever, due to the shortage of labels in graph data, supervised methods cannot reach exceptional\nperformance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph\nclassification has been developed extensively. These approaches can be categorized into pseudo\u0002labeling-based methods, knowledge distillation-based methods and contrastive learning-based\nmethods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph\nexamples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually\nutilize a teacher-student architecture, where the teacher model conducts graph representation\nlearning without label information to extract generalized knowledge while the student model\nfocuses on the downstream task. Due to the restricted number of labeled instances, the student\nmodel transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of\nthis topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning.\nTypically, these methods extract topological information from two perspectives (i.e., different\nperturbation strategies and graph encoders), and maximize the similarity of their representations\ncompared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique\nto improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166,\n508]. Then, we review these methods in detail.\nSEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node\nin a hierarchical graph. It builds two graph classifiers which generate graph representations and\nconduct semi-supervised graph classification respectively. SEAL employs a self-attention module\nto encode each graph into a graph-level representation, and then conducts message passing from a\ngraph level for final classification. SEAL can also be combined with cautious iteration and active\niteration. The former merely utilizes partial graph samples to optimize the parameters in the first\nclassifier due to the potential erroneous pseudo-labels. The second combines active learning with\nthe model, which increases the annotation efficiency in semi-supervised scenarios.\nInfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the\nsimilarity between summarized graph representations and their node representations. In particular,\nit generates node representations using the message passing mechanism and summarizes these\nnode representations into a graph representation. Let \u03a6(\u00b7, \u00b7) denote a discriminator to distinguish\nwhether a node belongs to the graph, and we have:\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 31\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e =\n| G\ud835\udc59|+| G\ud835\udc62 \u2211\ufe01|\n\ud835\udc57=1\n\u2211\ufe01\n\ud835\udc56\u2208 G\ud835\udc57\nh\n\u2212 sp \u0010\u2212\u03a6\n\u0010\nh\n\ud835\udc57\n\ud835\udc56\n, z\n\ud835\udc57\n\u0011 \u0011 i \u2212\n1\n|\ud835\udc41\n\ud835\udc57\n\ud835\udc56\n|\n\u2211\ufe01\n\ud835\udc56\n\u2032\ud835\udc57\n\u2032\n\u2208\ud835\udc41\n\ud835\udc57\n\ud835\udc56\nh\nsp \u0010\u03a6\n\u0010\nh\n\ud835\udc57\n\u2032\n\ud835\udc56\n\u2032\n, z\n\ud835\udc57\n\u0011 \u0011 i , (76)\nwhere sp(\u00b7) denotes the softplus function. \ud835\udc41\n\ud835\udc57\n\ud835\udc56\ndenotes the negative node set where nodes are not\nin \ud835\udc3a\n\ud835\udc57\n. This mutual information maximization formulation is originally developed for unsupervised\nlearning and it can be simply extended for semi-supervised graph classification. In particular,\nInfoGraph utilizes a teacher-student architecture that compares the representation across the\nteacher and student networks. The contrastive learning objective serves as a regularization by\ncombining with supervised loss.\nDual Space Graph Contrastive Learning (DSGC) [527]. DSGC is a representative contrastive\nlearning-based method. It utilizes two graph encoders. The first is a standard GNN encoder in the\nEuclidean space and the second is the hyperbolic GNN encoder. The hyperbolic GNN encoder first\nconverts graph embeddings into hyperbolic space and then measures the distance based on the\nlength of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic\nspace. Assuming the two GNNs are named as \ud835\udc531 (\u00b7) and \ud835\udc532 (\u00b7), the positive pair is denoted as:\nz\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n= exp\ud835\udc50\no\n(\ud835\udc531 (\ud835\udc3a\n\ud835\udc57\n)),\nz\n\ud835\udc57\n\ud835\udc3b\n= exp\ud835\udc50\no\n\ud835\udc532 (\ud835\udc3a\n\ud835\udc57\n)\n\u0001\n.\n(77)\nThen it selects one labeled sample and \ud835\udc41\ud835\udc35 unlabeled sample \ud835\udc3a\n\ud835\udc57\nfor graph contrastive learning in\nthe hyperbolic space. In formulation,\nL\ud835\udc37\ud835\udc46\ud835\udc3a\ud835\udc36 = \u2212 log e\n\ud835\udc51\n\ud835\udc3b\n(h\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f\ne\n\ud835\udc51\ud835\udc3b (z\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f +\n\u00cd\ud835\udc41\n\ud835\udc56=1\ne\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc3b\n\u0011\n/\ud835\udf0f\n\u2212\n\ud835\udf06\ud835\udc62\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56=1\nlog e\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\ne\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n+ e\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n,\n(78)\nwhere z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\nand z\n\ud835\udc56\n\ud835\udc3b\ndenote the embeddings for labeled graph sample\ud835\udc3a\n\ud835\udc56\nand \ud835\udc51\n\ud835\udc3b (\u00b7) denotes a distance\nmetric in the hyperbolic space. This contrastive learning objective maximizes the similarity between\nembeddings learned from two encoders compared with other samples. Finally, the contrastive\nlearning objective can be combined with the supervised loss to achieve effective semi-supervised\ncontrastive learning.\nActive Semi-supervised Graph Neural Network (ASGN) [166]. ASGN utilizes a teacher-student\narchitecture with the teacher model focusing on representation learning and the student model\ntargeting at molecular property prediction. In the teacher model, ASGN first employs a message\npassing neural network to learn node representations under the reconstruction task and then\nborrows the idea of balanced clustering to learn graph-level representations in a self-supervised\nfashion. In the student model, ASGN utilizes label information to monitor the model training based\non the weights of the teacher model. In addition, active learning is also used to minimize the\nannotation cost while maintaining sufficient performance. Typically, the teacher model seeks to\nprovide discriminative graph-level representations without labels, which transfer knowledge to the\nstudent model to overcome the potential overfitting in the presence of label scarcity.\nTwin Graph Neural Networks (TGNN) [218]. TGNN also uses two graph neural networks to\ngive different perspectives to learn graph representations. Differently, it adopts a graph kernel\nneural network to learn graph-level representations in virtue of random walk kernels. Rather than\ndirectly enforcing representation from two modules to be similar, TGNN exchanges information\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n32 W. Ju, et al.\nby contrasting the similarity structure of the two modules. In particular, it constructs a list of\nanchor graphs, \ud835\udc3a\n\ud835\udc4e1\n,\ud835\udc3a\ud835\udc4e2, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc4e\ud835\udc40 , and utilizes two graph encoders to produce their embeddings,\ni.e., {\ud835\udc67\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n, {\ud835\udc64\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n. Then it calculates the similarity distribution between each unlabeled\ngraph and anchor graphs for two modules. Formally,\n\ud835\udc5d\n\ud835\udc57\n\ud835\udc5a =\nexp cos \ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (\ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n, (79)\n\ud835\udc5e\n\ud835\udc57\n\ud835\udc5a =\nexp cos w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n. (80)\nThen, TGNN minimizes the distance between distributions from different modules as follows:\nL\ud835\udc47\ud835\udc3a\ud835\udc41 \ud835\udc41 =\n1\nG\ud835\udc48\n\u2211\ufe01\n\ud835\udc3a \ud835\udc57 \u2208 G\ud835\udc62\n1\n2\n\ud835\udc37KL\np\n\ud835\udc57\n\u2225q\n\ud835\udc57\n\u0001\n+ \ud835\udc37KL\nq\n\ud835\udc57\n\u2225p\n\ud835\udc57\n\u0001\u0001 , (81)\nwhich serves as a regularization term to combine with the supervised loss.\n7.3 Summary\nThis section introduces semi-supervised learning for graph representation learning and we provide\nthe summary as follows:\n\u2022 Techniques. Classic node classification aims to conduct transductive learning on graphs\nwith access to unlabeled data, which is a natural semi-supervised problem. Semi-supervised\ngraph classification aims to relieve the requirement of abundant labeled graphs. Here, a\nvariety of semi-supervised methods have been put forward to achieve better performance\nunder the label scarcity. Typically, they try to integrate semi-supervised techniques such as\nactive learning, pseudo-labeling, consistency learning, and consistency learning with graph\nrepresentation learning.\n\u2022 Challenges and Limitations. Despite their great success, the performance of these methods\nis still unsatisfactory, especially in graph-level representation learning. For example, DSGC\ncan only achieve an accuracy of 57% in a binary classification dataset REDDIT-BINARY. Even\nworse, label scarcity is often accompanied by unbalanced datasets and potential domain\nshifts, which provides more challenges from real-world applications.\n\u2022 Future Works. In the future, we expect that these methods can be applied to different\nproblems such as molecular property predictions. There are also works to extend graph\nrepresentation learning in more realistic scenarios like few-shot learning [51, 326]. A higher\naccuracy is always anticipated for more advanced and effective semi-supervised techniques.\n8 Graph Self-supervised Learning\nBesides supervised or semi-supervised methods, self-supervised learning (SSL) also has shown its\npowerful capability in data mining and representation embedding in recent years. In this section,\nwe investigated Graph Neural Networks based on SSL and provided a detailed introduction to a\nfew typical models. Graph SSL methods usually have a unified pipeline, which includes pretext\ntasks and downstream tasks. Pretext tasks help the model encoder to learn better representation,\nas a premise of better performance in downstream tasks. So a delicate design of pretext task is\ncrucial for Graph SSL. We would firstly introduce the overall framework of Graph SSL in Section 8.1,\nthen introduce the two kinds of pretext task design, generation-based methods and contrast-based\nmethods respectively in Section 8.2 and 8.3. A summarisation is provided in Table 7.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 33\nTable 7. Summary of methods for self-supervised Learning on Graphs. \"PT\", \"CT\" and \"UFE\" mean \"Pre\u0002training\", \"Collaborative Train\" and \"Unsupervised Feature Extracting\" respectively.\nApproach Augmentation Scheme Training Scheme Generation Target Objective Function\nGeneration-based\nGraph Completion [544] Feature Mask PT/CT Node Feature -\nAttributeMask [208] Feature Mask PT/CT PCA Node Feature -\nAttrMasking [181] Feature Mask PT Node/Edge Feature -\nMGAE [459] No Augmentation CT Node Feature -\nGAE [231] Feature Noise UFE Adjacency Matrix -\nContrast-based\nDeepWalk [362] Random Walk UFE - SkipGram\nLINE [443] Random Walk UFE - Jensen-Shannon\nGCC [375] Random Walk PT/URL - InfoNCE\nSimGCL [547] Embedding Noise UFE - InfoNCE\nSimGRACE [503] Model Noise UFE - InfoNCE\nGCA [612]\nFeature Masking &\nStructure Adjustment URL - InfoNCE\nBGRL [152]\nFeature Masking &\nStructure Adjustment URL - BYOL\n8.1 Overall framework\nConsider a featured graph G, we denote a graph encoder \ud835\udc53 to learn the representation of the\ngraph, and a pretext decoder \ud835\udc54 with specific architecture in different pretext tasks. Then the pretext\nself-supervised learning loss can be formulated as:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = \ud835\udc38G\u223cD [L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G)], (82)\nwhere D denotes the distribution of featured graph G. By minimizing L\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59 , we can learn encoder\n\ud835\udc53 with capacity to produce high-quality embedding. As for downstream tasks, we denote a graph\ndecoder \ud835\udc51 which transforms the output of graph encoder \ud835\udc53 into model prediction. The loss of\ndownstream tasks can be formulated as:\nL\ud835\udc60\ud835\udc62\ud835\udc5d = L\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66), (83)\nwhere \ud835\udc66 is the ground truth in downstream tasks. We can obverse that L\ud835\udc60\ud835\udc62\ud835\udc5d is a typical supervised\nloss. To ensure the model achieves wise graph representation extraction and optimistic prediction\nperformance, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d have to be minimized simultaneously. We introduce 3 different ways\nto minimize the two loss functions:\nPre-training. This strategy has two steps. In pre-training step, the L\ud835\udc60\ud835\udc60\ud835\udc59 is minimized to get \ud835\udc54\n\u2217\nand \ud835\udc53\n\u2217\n:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D). (84)\nThen the parameter of \ud835\udc53\n\u2217\nis kept to continue training in pretext supervised learning progress.\nThe supervised loss is minimized to get the final parameters of \ud835\udc53 and \ud835\udc51.\nmin\n\ud835\udc51,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc51, \ud835\udc53 |\ud835\udc530=\ud835\udc53\n\u2217 , G;\ud835\udc66). (85)\nCollaborative Train. In this strategy, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d are optimized simultaneously. A hyper\u0002parameter \ud835\udefc is used to balance the contribution of pretext task loss and downstream task loss.\nThe overall minimization strategy is like the traditional supervised strategy with a pretext task\nregularization:\nmin\n\ud835\udc54,\ud835\udc53 ,\ud835\udc51\n[L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G) + \ud835\udefcL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66)]. (86)\nUnsupervised Feature Extracting. This strategy is similar to the Pre-training and Fine-tuning\nstrategy in the first step to minimize pretext task loss L\ud835\udc60\ud835\udc60\ud835\udc59 and get \ud835\udc53\n\u2217\n. However, when minimizing\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n34 W. Ju, et al.\ndownstream loss L\ud835\udc60\ud835\udc62\ud835\udc5d , the encoder \ud835\udc53\n\u2217\nis fixed. Also, the training graph data are on the same dataset,\nwhich differs from the Pre-training and Fine-tuning strategy. The formulation is defined as:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D), (87)\nmin\n\ud835\udc51\nL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 \u2217, G;\ud835\udc66). (88)\n8.2 Generation-based pretext task design\nIf a model with an encoder-decoder structure can reproduce certain graph features from an in\u0002complete or perturbed graph, it indicates the encoder has the ability to extract useful graph\nrepresentation. This motivation is derived from Autoencoder [174] which originally learns on\nimage dataset. In such a case, Eq. 84 can be rewritten as:\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54(\ud835\udc53 (G)) \u02c6 , G), (89)\nwhere \ud835\udc53 (\u00b7) and \ud835\udc54(\u00b7) stand for the representation encoder and rebuilding decoder. However, feature\ninformation and structure information are both important compositions suitable to be rebuilt for\ngraph datasets. So generation-based pretext can be divided into two categories: feature rebuilding\nand structure rebuilding. We introduce several outstanding models in the following part.\nGraph Completion [544] is one of the representative methods of feature rebuilding. They mask\nsome node features to generate an incomplete graph. Then the pretext task is set as predicting the\nremoved node features. As shown in Eq. 90, this method can be formulated as a special case of\nEq. 90, letting G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and replacing G \u2212\u2192 \ud835\udc4b. The loss function is often Mean Squared Error or\nCross Entropy, depending on whether the feature is continuous or binary.\nmin\n\ud835\udc54,\ud835\udc53\nMSE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , X). (90)\nOther works make some changes to the feature settings. For example, AttrMasking [181] aims\nto rebuild both node representation and edge representation, AttributeMask [208] preprocess \ud835\udc4b\nfirstly by PCA to reduce the complexity of rebuilding features.\nOn the other hand, MGAE [459] modifies the original graph by adding noise in node representa\u0002tion, motivated by denoising autoencoder [454]. As shown in Eq. 90, we can also consider MGAE as\nan implement of Eq. 84 where G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and G \u2212\u2192 \ud835\udc4b. \ud835\udc4b\u02c6 stands for perturbed node representation.\nSince the noise is independent and random, the encoder is more robust to feature input.\nmin\n\ud835\udc54,\ud835\udc53\nBCE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , A). (91)\nAs for structure rebuilding methods, GAE [231] is the simplest instance, which can be regarded as\nan implement of Eq. 84 where G\u02c6 = G and G \u2212\u2192 \ud835\udc34. \ud835\udc34 is the adjacency matrix of the graph. Similar to\nfeature rebuilding methods, GAE compresses raw node representation vectors into low-dimensional\nembedding with its encoder. Then the adjacency matrix is rebuilt by computing node embedding\nsimilarity. The loss function is set to the error between the ground-truth adjacency matrix and\nthe recovered one, to help the model rebuild the correct graph structure. Other feature rebuilding\nmethods [558] and structure rebuilding methods [440, 487] are also increasingly being developed\nacross numerous related publications.\n8.3 Contrast-Based pretext task design\nThe mutual information maximization principle, which implements self-supervising by predicting\nthe similarity between the two augmented views, forms the foundation of contrast-based approaches.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 35\nSince mutual information represents the degree of correlation between two samples, we can\nmaximize it in augmented pairs and minimize it in random-selected pairs.\nThe contrast-based graph SSL taxonomy can be formulated as Eq. 92. The discriminator that\ncalculates the similarity of sample pairs is indicated by pretext decoder \ud835\udc54. G\n(1)\nand G\n(2)\nare two\nvariants of \ud835\udc3a that have been augmented. Since graph contrastive learning methods differ from each\nother in 1) view generation, 2) MI estimation method we introduce this methodology in these two\nperspectives.\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54[\ud835\udc53 (G\u02c6(1)), \ud835\udc53 (G\u02c6(2))]). (92)\nThe domain of contrastive-based graph SSL is witnessing an expanding body of work in a\ngrowing number of methods [176, 197, 531, 587] and applications [114, 136, 504].\n8.3.1 View generation. The traditional pipeline of contrastive learning-based models first involves\naugmenting the graph using well-crafted empirical methods, and then maximizing the consistency\nbetween different augmentations. Drawing from methods in the computer vision domain and\nconsidering the non-Euclidean structure of graph data, typical graph augmentation methods aim\nto modify the graph topologically or representationally.\nGiven graph G = (\ud835\udc34, \ud835\udc4b), the topologically augmentation methods usually modify the adjacency\nmatrix \ud835\udc34, which can be formulated as:\n\ud835\udc34\u02c6 = \ud835\udcaf\ud835\udc34 (\ud835\udc34), (93)\nwhere \ud835\udcaf\ud835\udc34 (\u00b7) is the transform function of adjacency matrix. Topology augmentation methods\nhave many variants, in which the most popular one is edge modification, given by \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\ud835\udc43 \u25e6 \ud835\udc34 + \ud835\udc44 \u25e6 (1 \u2212 \ud835\udc34). \ud835\udc43 and \ud835\udc44 are two matrices representing edge dropping and adding respectively.\nAnother method, graph diffusion, connect nodes with their k-hop neighbors with specific weight,\ndefined as: \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\u00cd\u221e\n\ud835\udc58=0\n\ud835\udefc\ud835\udc58\ud835\udc47\n\ud835\udc58\n. where \ud835\udefc and\ud835\udc47 are coefficient and transition matrix. Graph diffusion\nmethod can integrate broad topological information with local structure.\nOn the other hand, the representative augmentation modifies the node representation directly,\nwhich can be formulated as:\n\ud835\udc4b\u02c6 = \ud835\udcaf\ud835\udc4b (\ud835\udc4b), (94)\nusually \ud835\udcaf\ud835\udc4b (\u00b7) can be a simple masking operater, a.k.a. \ud835\udcaf\ud835\udc4b (\ud835\udc4b) = \ud835\udc40 \u25e6 \ud835\udc4b and \ud835\udc40 \u2208 {0, 1}\n\ud835\udc41 \u00d7\ud835\udc37 . Based\non such mask strategy, some methods propose ways to improve performance. GCA [612] preserves\ncritical nodes while giving less significant nodes a larger masking probability, where significance is\ndetermined by node centrality.\nAs introduced before, the paradigm of augmentation has been proven to be effective in contrastive\nlearning view generation. However, given the variety of graph data, it is challenging to maintain\nsemantics properly during augmentations. To preserve the valuable nature of specific graph datasets,\nThere are currently three mainly used methods: picking by trial-and-errors, trying laborious search,\nor seeking domain-specific information as guidance [214, 308, 311]. Such complicated augmentation\nmethods constrain the effectiveness and widespread application of graph contrastive learning. So\nmany newest works question the necessity of augmentation and seek other contrastive view\ngeneration methods.\nSimGCL [547] is one of the outstanding works challenging the effectiveness of graph augmenta\u0002tion. The author finds that noise can be a substitution to augmentation to produce graph views\nin specific tasks such as recommendation. After doing an ablation study about augmentation and\nInfoNCE [510], they find that the InfoNCE loss, not the augmentation of the graph, is what makes\nthe difference. It can be further explained by the importance of distribution uniformity. Contrastive\nlearning enhances model representation ability by intensifying two characteristics: The alignment\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n36 W. Ju, et al.\nof features from positive samples and the uniformity of the normalized feature distribution. SimGCL\ndirectly adds random noises to node embeddings as augmentation, to control the uniformity of the\nrepresentation distribution more effectively:\ne\n(1)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(1)\n\u2217 \ud835\udf0f\n(1)\n\ud835\udc56\n, e\n(2)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(2)\n\u2217 \ud835\udf0f\n(2)\n\ud835\udc56\n,\n\ud835\udf16 \u223c N (0, \ud835\udf0e2),\n(95)\nwhere e\ud835\udc56is a node representation in embedding space, \ud835\udf0f\n(1)\n\ud835\udc56\nand \ud835\udf0f\n(2)\n\ud835\udc56\nare two random sampled unit\nvector. The experiment results indicate that SimGCL performs better than its graph augmentation\u0002based competitors, while training time is significantly decreased.\nSimGRACE [503] is another graph contrastive learning framework without data augmentation.\nMotivated by the observation that despite encoder disruption, graph data can effectively maintain\ntheir semantics, SimGRACE takes GNN with its modified version as an encoder to produce two\ncontrastive embedding views by the same graph input. For GNN encoder \ud835\udc53 (\u00b7; \ud835\udf03), the two contrastive\nembedding views e, e\n\u2032\ncan be computed by:\ne\n(1) = \ud835\udc53 (G; \ud835\udf03), e(2) = \ud835\udc53 (G; \ud835\udf03 + \ud835\udf16 \u00b7 \u0394\ud835\udf03),\n\u0394\ud835\udf03\ud835\udc59 \u223c N (0, \ud835\udf0e2\n\ud835\udc59\n),\n(96)\nwhere \u0394\ud835\udf03\ud835\udc59 represents GNN parameter perturbation \u0394\ud835\udf03 in the \ud835\udc59th layer. SimGRACE can improve\nalignment and uniformity simultaneously, proving its capacity to produce high-quality embedding.\n8.3.2 MI estimation method. The mutual information \ud835\udc3c(\ud835\udc65, \ud835\udc66) measures the information that x and y\nshare, given a pair of random variables (\ud835\udc65, \ud835\udc66). As discussed before, mutual information is a significant\ncomponent of the contrast-based method by formulating the loss function. Mathematically rigorous\nMI is defined on the probability space, we can formulate mutual information between a pair of\ninstances (\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) as:\n\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \ud835\udc37\ud835\udc3e\ud835\udc3f (\ud835\udc5d(\ud835\udc65, \ud835\udc66)||\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66))\n= \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc5d(\ud835\udc65, \ud835\udc66)\n\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66)\n].\n(97)\nHowever, directly computing Eq. 97 is quite difficult, so we introduce several different types of\nestimation for MI:\nInfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimator. Given a\npositive sample \ud835\udc66 and several negative sample \ud835\udc66\n\u2032\n\ud835\udc56\n, a noise-contrastive estimator can be formulated\nas [611][375]:\nL = \u2212\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \u2212\ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66)\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66) +\n\u00cd\n\ud835\udc56\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66\u2032\n\ud835\udc56\n)\n], (98)\nusually the kernal function \ud835\udc54(\u00b7) can be cosine similarity or dot product.\nTriplet Loss. Intuitively, we can aim to create a distinct separation in the degree of similarity,\nensuring that positive samples are closer together and negative samples are further apart by a\ncertain distance. So we can define the loss function in the following manner [204]:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [max(\ud835\udc54(\ud835\udc65, \ud835\udc66) \u2212 \ud835\udc54(\ud835\udc65, \ud835\udc66\u2032) + \ud835\udf16, 0)], (99)\nwhere \ud835\udf16 is a hyperparameter. This function is straightforward to compute.\nBYOL Loss. Estimation without negative samples is investigated by BYOL [152]. The estimator\nis Asymmetrically structured:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [2 \u2212 2\n\ud835\udc54(\ud835\udc65) \u00b7 \ud835\udc66\n\u2225\ud835\udc54(\ud835\udc65) \u2225 \u2225\ud835\udc66\u2225\n], (100)\nnote that encoder \ud835\udc54 should keep the dimension of input and output the same.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 37\n8.4 Summary\nThis section introduces graph self-supervised learning and we provide the summary as follows:\n\u2022 Techniques. Differing from classic supervised and semi-supervised learning, self-supervised\nlearning increases a model\u2019s generalization ability and robustness while decreasing reliance\non labels. Graph SSL utilizes pretext tasks to extract inherent information from representation\ndistributions. Typical Graph SSL methods can be divided into generation-based and contrast\u0002based approaches. Generation-based methods learn an encoder with the ability to reconstruct\na graph as precisely as possible, motivated by the principles of Autoencoder. Contrast-based\nmethods, which have recently attracted significant interest, involve learning an encoder to\nminimize mutual information between relevant instances and maximize mutual information\nbetween unrelated instances.\n\u2022 Challenges and Limitations. Although graph SSL has achieved superior performance in\nmany tasks, its theoretical basis is not so solid. Many well-known methods are validated only\nthrough experiments, without providing theoretical explanations or mathematical proofs. It\nis imperative to establish a strong theoretical foundation for graph SSL.\n\u2022 Future Works. In the future we expect more graph ssl methods designed essentially by\ntheoretical proof, without dedicated designed augment process or pretext tasks by intuition.\nThis will bring us more definite mathematical properties and a less ambiguous empirical\nsense. Also, graphs are a prevalent form of data representation across diverse domains, yet\nobtaining manual labels can be prohibitively expensive. Expanding the applications of graph\nSSL to broader fields is a promising avenue for future research.\n9 Graph Structure Learning\nGraph structure determines how node features propagate and affect each other, playing a crucial\nrole in graph representation learning. In some scenarios the provided graph is incomplete, noisy, or\neven has no structure information at all. Recent research also finds that graph adversarial attacks\n(i.e., modifying a small number of node features or edges), can degrade learned representations\nsignificantly. These issues motivate graph structure learning (GSL), which aims to learn a new\ngraph structure to produce optimal graph representations. According to how edge connectivity is\nmodeled, there are three different approaches in GSL, namely metric-based approaches, model-based\napproaches, and direct approaches. Besides edge modeling, regularization is also a common trick to\nmake the learned graph satisfy some desired properties. We first present the basic framework and\nregularization methods for GSL in Sec. 9.1 and Sec. 9.2, respectively, and then introduce different\ncategories of GSL in Sec. 9.3, 9.4 and 9.5. We summarize GSL approaches in Table 8.\n9.1 Overall Framework\nWe denote a graph by G = (A, X), where A \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is the adjacency matrix and X \u2208 R\ud835\udc41 \u00d7\ud835\udc40 is\nthe node feature matrix with \ud835\udc40 being the dimension of each node feature. A graph encoder \ud835\udc53\ud835\udf03\nlearns to represent the graph based on node features and graph structure for task-specific objective\nL\ud835\udc61 (\ud835\udc53\ud835\udf03 (A, X)). In the GSL setting, there is also a graph structure learner which aims to build a\nnew graph adjacency matrix A\n\u2217\nto optimize the learned representation. Besides the task-specific\nobjective, a regularization term can be added to constrain the learned structure. So the overall\nobjective function of GSL can be formulated as\nmin\n\ud835\udf03,A\u2217\nL = L\ud835\udc61 (\ud835\udc53\ud835\udf03 (A\n\u2217\n, X)) + \ud835\udf06L\ud835\udc5f (A\n\u2217\n, A, X), (101)\nwhere L\ud835\udc61is the task-specific objective, L\ud835\udc5fis the regularization term and \ud835\udf06 is a hyperparameter for\nthe weight of regularization.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n38 W. Ju, et al.\nTable 8. Summary of graph structure learning methods.\nMethod Structure Learning Regularization\nSparsity Low-rank Smoothness\nMetric-based\nAGCN [267] Mahalanobis distance\nGRCN [546] Inner product \u2713\nCAGCN [613] Inner product \u2713\nGNNGUARD [571] Cosine similarity\nIDGL [65] Cosine similarity \u2713 \u2713 \u2713\nHGSL [586] Cosine similarity \u2713\nGDC [139] Graph diffusion \u2713\nModel-based\nGLN [364] Recurrent blocks\nGLCN [199] One-layer neural network \u2713 \u2713\nNeuralSparse [595] Multi-layer neural network \u2713\nGAT [452] Self-attention\nGaAN [566] Gated attention\nhGAO [132] Hard attention \u2713\nVIB-GSL [433] Dot-product attention \u2713\nMAGNA [461] Graph attention diffusion\nDirect\nGLNN [135] MAP estimation \u2713 \u2713\nPro-GNN [210] Direct optimization \u2713 \u2713 \u2713\nGSML [458] Bilevel optimization \u2713\nLSD-GNN [124] Bilevel optimization\nBGCNN [573] Bayesion optimization\nVGCN [104] Stochastic variational inference\n9.2 Regularization\nThe goal of regularization is to constrain the learned graph to satisfy some properties by adding\nsome penalties to the learned structure. The most common properties used in GSL are sparsity, low\nlank, and smoothness.\n9.2.1 Sparsity Noise or adversarial attacks will introduce redundant edges into graphs and degrade\nthe quality of graph representation. An effective technique to remove unnecessary edges is sparsity\nregularization, i.e., adding a penalty on the number of nonzero entries of the adjacency matrix\n(\u21130-norm) [458, 546, 586, 595]:\nL\ud835\udc60\ud835\udc5d = \u2225A\u22250, (102)\nhowever, \u21130-norm is not differentiable so optimizing it is difficult, and in many cases \u21131-norm\nis used instead as a convex relaxation. Other methods to impose sparsity include pruning and\ndiscretization [124, 613]. These processes are also called postprocessing since they usually happen\nafter the adjacency matrix is learned. Pruning removes part of the edges according to some crite\u0002ria [613]. For example, edges with weights lower than a threshold, or those not in the top-K edges\nof nodes or graphs. Discretization is applied to generate graph structure by sampling from some\ndistribution [124]. Compared to directly learning edge weights, sampling enjoys the advantage\nof controlling the generated graph, but has issues during optimizing since sampling itself is dis\u0002crete and hard to optimize. Reparameterization and Gumbel-softmax are two useful techniques to\novercome such issues, and are widely adopted in GSL.\n9.2.2 Low Rank In real-world graphs, similar nodes are likely to group together and form commu\u0002nities, which should lead to a low-rank adjacency matrix. Recent work also finds that adversarial\nattacks tend to increase the rank of the adjacency matrix quickly [65, 210]. Therefore, low-rank\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 39\nregularization is also a useful tool to make graph representation learning more robust:\nL\ud835\udc59\ud835\udc5f = \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc58 (A). (103)\nIt is hard to minimize matrix rank directly. A common technique is to optimize the nuclear norm,\nwhich is a convex envelope of the matrix rank:\nL\ud835\udc5b\ud835\udc50 = \u2225A\u2225\u2217 =\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56\n\ud835\udf0e\ud835\udc56, (104)\nwhere \ud835\udf0e\ud835\udc56 are singular values of A. Entezari et al. replaces the learned adjacency matrix with rank-r\napproximation by singular value decomposition (SVD) to achieve robust graph learning against\nadversarial attacks.\n9.2.3 Smoothness A common assumption is that connected nodes share similar features, or in other\nwords, the graph is \u201csmooth\u201d as the difference between local neighbors is small [65, 135, 199, 210].\nThe following metric is a natural way to measure graph smoothness:\nL\ud835\udc60\ud835\udc5a =\n1\n2\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56,\ud835\udc57=1\n\ud835\udc34\ud835\udc56\ud835\udc57 (\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n2 = \ud835\udc61\ud835\udc5f(X\u22a4\n(D \u2212 A)X) = \ud835\udc61\ud835\udc5f(X\n\u22a4\nLX), (105)\nwhere D is the degree matrix of A and L = D \u2212 A is called graph Laplacian. A variant is to use the\nnormalized graph Laplacian bL = D\n\u2212\n1\n2 LD\u2212\n1\n2 .\n9.3 Metric-based Methods\nMetric-based methods measure the similarity between nodes as the edge weights. They follow\nthe basic assumption that similar nodes tend to have connections with each other. We show some\nrepresentative works\nAdaptive Graph Convolutional Neural Networks (AGCN) [267]. AGCN learns a task-driven adaptive\ngraph during training to enable a more generalized and flexible graph representation model. After\nparameterizing the distance metric between nodes, AGCN is able to adapt graph topology to the\ngiven task. It proposes a generalized Mahalanobis distance between two nodes with the following\nformula:\nD(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) =\n\u221a\ufe03\n(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n\u22a4\ud835\udc40(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57), (106)\nwhere \ud835\udc40 = \ud835\udc4a\ud835\udc51\ud835\udc4a \u22a4\n\ud835\udc51\nand \ud835\udc4a\ud835\udc51 is the trainable weights to minimize task-specific objective. Then the\nGaussian kernel is used to obtain the adjacency matrix:\nG\ud835\udc56\ud835\udc57 = exp(\u2212D(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57)/(2\ud835\udf0e\n2\n)), (107)\n\ud835\udc34\u02c6 = \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52 (G). (108)\nGraph-Revised Convolutional Network (GRCN) [546]. GRCN uses a graph revision module to\npredict missing edges and revise edge weights through joint optimization on downstream tasks. It\nfirst learns the node embedding with GCN and then calculates pair-wise node similarity with the\ndot product as the kernel function.\n\ud835\udc4d = \ud835\udc3a\ud835\udc36\ud835\udc41\ud835\udc54 (\ud835\udc34, \ud835\udc4b), (109)\n\ud835\udc46\ud835\udc56\ud835\udc57 =\n\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\n. (110)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n40 W. Ju, et al.\nThe revised adjacency matrix is the residual summation of the original adjacency matrix\ud835\udc34\u02c6 = \ud835\udc34+\ud835\udc46.\nGRCN also applies a sparsification technique on the similarity matrix \ud835\udc46 to reduce computation cost:\n\ud835\udc46\n(\ud835\udc3e)\n\ud835\udc56\ud835\udc57 =\n\u001a\n\ud835\udc46\ud835\udc56\ud835\udc57, \ud835\udc46\ud835\udc56\ud835\udc57 \u2208 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n0, \ud835\udc46\ud835\udc56\ud835\udc57 \u2209 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n. (111)\nThreshold pruning is also a common strategy for sparsification. For example, CAGCN [613] uses\ndot product to measure node similarity, and refines the graph structure by removing edges between\nnodes whose similarity is less than a threshold \ud835\udf0f\ud835\udc5f and adding edges between nodes whose similarity\nis greater than another threshold \ud835\udf0f\ud835\udc4e.\nDefending Graph Neural Networks against Adversarial Attacks (GNNGuard) [571]. GNNGuard\nmeasures similarity between a node \ud835\udc62 and its neighbor \ud835\udc63 in the \ud835\udc58-th layer by cosine similarity and\nnormalizes node similarity at the node level within the neighborhood as follows:\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\u210e\n\ud835\udc58\n\ud835\udc62 \u2299 \u210e\n\ud835\udc58\n\ud835\udc63\n\u2225\u210e\n\ud835\udc58\n\ud835\udc62 \u22252 \u2225\u210e\n\ud835\udc58\n\ud835\udc63 \u22252\n, (112)\n\ud835\udefc\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63/\n\u2211\ufe01\n\ud835\udc63\u2208N\ud835\udc62\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u00d7 \ud835\udc41\u02c6 \ud835\udc58\ud835\udc62\n/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 \u2260 \ud835\udc63\n1/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 = \ud835\udc63\n, (113)\nwhere N\ud835\udc62 denotes the neighborhood of node \ud835\udc62 and \ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 =\n\u00cd\n\ud835\udc63\u2208N\ud835\udc62\n\u2225\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u22250. To stabilize GNN training,\nit also proposes a layer-wise graph memory by keeping part of the information from the previous\nlayer in the current layer. Similar to GNNGuard, IDGL [65] uses multi-head cosine similarity and\nmask edges with node similarity smaller than a non-negative threshold, and HGSL [586] generalizes\nthis idea to heterogeneous graphs.\nGraph Diffusion Convolution (GDC) [139]. GDC replaces the original adjacency matrix with\ngeneralized graph diffusion matrix S:\nS =\n\u2211\ufe01\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58T\n\ud835\udc58\n, (114)\nwhere \ud835\udf03\ud835\udc58 is the weighting coefficient and T is the generalized transition matrix. To ensure conver\u0002gence, GDC further requires that \u00cd\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58 = 1 and the eigenvalues of T lie in [0, 1]. The random\nwalk transition matrix T\ud835\udc5f \ud835\udc64 = AD\u22121and the symmetric transition matrix T\ud835\udc60\ud835\udc66\ud835\udc5a = D\n\u22121/2AD\u22121/2\nare\ntwo examples. This new graph structure allows graph convolution to aggregate information from a\nlarger neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying noise.\nHowever, in most cases graph diffusion will result in a dense adjacency matrix \ud835\udc46, so sparsification\ntechnology like top-k filtering and threshold filtering will be applied to graph diffusion. Following\nGDC, there are some other graph diffusion proposed. For example, AdaCAD [281] proposes Class\u0002Attentive Diffusion, which further considers node features and aggregates nodes probably of the\nsame class among K-hop neighbors. Adaptive diffusion convolution (ADC) [585] learns the optimal\nneighborhood size via optimizing a bi-level problem.\n9.4 Model-based Methods\nModel-based methods parameterize edge weights with more complex models like deep neural\nnetworks. Compared to metric-based methods, model-based methods offer greater flexibility and\nexpressive power.\nGraph Learning Network (GLN) [364]. GLN proposes a recurrent block to first produce interme\u0002diate node embeddings and then merge them with adjacency information as the output of this\nlayer to predict the adjacency matrix for the next layer. Specifically, it uses convolutional graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 41\noperations to extract node features, and creates a local-context embedding based on node features\nand the current adjacency matrix:\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc56=1\n\ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\n), (115)\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 = \ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc48\n(\ud835\udc59)\n), (116)\nwhere \ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\nand \ud835\udc48\n(\ud835\udc59)\nare the learnable weights. GLN then predicts the next adjacency matrix as\nfollows:\n\ud835\udc34\n(\ud835\udc59+1) = \ud835\udf0e\ud835\udc59 (\ud835\udc40(\ud835\udc59)\n\ud835\udefc\ud835\udc59 (\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59)\ud835\udc40\n(\ud835\udc59) \u22a4\n). (117)\nSimilarly, GLCN [199] models graph structure with a softmax layer over the inner product\nbetween the difference of node features and a learnable vector. NeuralSparse [595] uses a multi\u0002layer neural network to generate a learnable distribution from which a sparse graph structure is\nsampled. PTDNet [305] prunes graph edges with a multi-layer neural network and penalizes the\nnumber of non-zero elements to encourage sparsity.\nGraph Attention Networks (GAT) [452]. Besides constructing a new graph to guide the message\npassing and aggregation process of GNNs, many recent researchers also leverage the attention\nmechanism to adaptively model the relationship between nodes. GAT is the first work to introduce\nthe self-attention strategy into graph learning. In each attention layer, the attention weight between\ntwo nodes is calculated as the Softmax output on the combination of linear and non-linear transform\nof node features:\n\ud835\udc52\ud835\udc56\ud835\udc57 = \ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57), (118)\n\ud835\udefc\ud835\udc56\ud835\udc57 =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc57)\n\u00cd\n\ud835\udc58 \u2208N\ud835\udc56\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc58 )\n, (119)\nwhere N\ud835\udc56 denotes the neighborhood of node \ud835\udc56,W is learnable linear transform and \ud835\udc4e is pre-defined\nattention function. In the original implementation of GAT, \ud835\udc4e is a single-layer neural network with\nLeakyReLU:\n\ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57) = LeakyReLU(\u00aea\n\u22a4\n[W\u00ae\u210e\ud835\udc56||W\u00ae\u210e\ud835\udc57]). (120)\nThe attention weights are then used to guide the message-passing phase of GNNs:\n\u00ae\u210e\n\u2032\n\ud835\udc56 = \ud835\udf0e(\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n\ud835\udefc\ud835\udc56\ud835\udc57W\u00ae\u210e\ud835\udc57), (121)\nwhere \ud835\udf0e is a nonlinear function. It is beneficial to concatenate multiple heads of attention to\u0002gether to get a more stable and generalizable model, so-called multi-head attention. The attention\nmechanism serves as a soft graph structure learner which captures important connections within\nnode neighborhoods. Following GAT, many recent works propose more effective and efficient\ngraph attention operators to improve performance. GaAN [566] adds a soft gate at each attention\nhead to adjust its importance. MAGNA [461] proposes a novel graph attention diffusion layer to\nincorporate multi-hop information. One drawback of graph attention is that the time and space\ncomplexities are both \ud835\udc42(\ud835\udc41\n3\n). hGAO [132] performs hard graph attention by limiting node attention\nto its neighborhood. VIB-GSL [433] adopts the information bottleneck principle to guide feature\nmasking in order to drop task-irrelevant information and preserve actionable information for the\ndownstream task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n42 W. Ju, et al.\n9.5 Direct Methods\nDirect methods treat edge weights as free learnable parameters. These methods enjoy more flexibility\nbut are also more difficult to train. The optimization is usually carried out in an alternating way,\ni.e., iteratively updating the adjacency matrix A and the GNN encoder parameters \ud835\udf03.\nGLNN [135]. GLNN uses MAP estimation to learn an optimal adjacency matrix for a joint\nobjective function including sparsity and smoothness. Specifically, it targets at finding the most\nprobable adjacency matrix \ud835\udc34\u02c6 given graph node features \ud835\udc65:\n\ud835\udc34\u02dc\ud835\udc40\ud835\udc34\ud835\udc43 (\ud835\udc65) = argmax\n\ud835\udc34\u02c6\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6)\ud835\udc54(\ud835\udc34\u02c6), (122)\nwhere \ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) measures the likelihood of observing \ud835\udc65 given \ud835\udc34\u02c6, and \ud835\udc54(\ud835\udc34\u02c6) is the prior distribution of\n\ud835\udc34\u02c6. GLNN uses sparsity and property constraint as prior, and defines the likelihood function \ud835\udc53 as:\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) = \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n\ud835\udc3f\ud835\udc65) (123)\n= \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n(\ud835\udc3c \u2212 \ud835\udc34\u02c6)\ud835\udc65), (124)\nwhere \ud835\udf060 is a parameter. This likelihood imposed a smoothness assumption on the learned graph\nstructure. Some other works also model the adjacency matrix in a probabilistic manner. Bayesian\nGCNN [573] adopts a Bayesian framework and treats the observed graph as a realization from a\nfamily of random graphs. Then it estimates the posterior probablity of labels given the observed\ngraph adjacency matrix and features with Monte Carlo approximation. VGCN [104] follows a\nsimilar formulation and estimates the graph posterior through stochastic variational inference.\nPro-GNN [210] learns a clean graph structure from perturbed data and optimizes parameters for a\nrobust GNN, leveraging properties like sparsity, low rank, and feature smoothness.\nGraph Sparsification via Meta-Learning (GSML) [458]. GSML formulates GSL as a meta-learning\nproblem and uses bi-level optimization to find the optimal graph structure. The goal is to find a\nsparse graph structure that leads to high node classification accuracy at the same time given labeled\nand unlabeled nodes. To achieve this, GSML makes the inner optimization as training on the node\nclassification task, and targets the outer optimization at the sparsity of the graph structure, which\nformulates the following bi-level optimization problem:\n\ud835\udc3a\u02c6\n\u2217 = min\n\ud835\udc3a\u02c6 \u2208\u03a6(\ud835\udc3a)\n\ud835\udc3f\ud835\udc60\ud835\udc5d\ud835\udc60 (\ud835\udc53\ud835\udf03\n\u2217 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc48 ), (125)\n\ud835\udc60.\ud835\udc61 . \ud835\udf03 \u2217 = argmin\n\ud835\udf03\n\ud835\udc3f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (\ud835\udc53\ud835\udf03 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc3f). (126)\nIn this bi-level optimization problem, \ud835\udc3a\u02c6 \u2208 \u03a6(\ud835\udc3a) are the meta-parameters and optimized directly\nwithout parameterization. Similarly, LSD-GNN [124] also uses bi-level optimization. It models graph\nstructure with a probability distribution over the graph and reformulates the bi-level program in\nterms of the continuous distribution parameters.\n9.6 Summary\nIn this section, we provide the summary as follows:\n\u2022 Techniques. GSL aims to learn an optimized graph structure for better graph representations.\nIt is also used for more robust graph representation against adversarial attacks. According\nto the way of edge modeling, we categorize GSL into three groups: metric-based methods,\nmodel-based methods, and direct methods. Regularization is also a commonly used principle\nto make the learned graph structure satisfy specific properties including sparsity, low-rank\nand smoothness.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 43\n\u2022 Challenges and Limitations. Since there is no way to access the ground truth or optimal\ngraph structure as training data, the learning objective of GSL is either indirect (e.g., perfor\u0002mance on downstream tasks) or manually designed (e.g., sparsity and smoothness). Therefore,\nthe optimization of GSL is difficult and the performance is not satisfying. In addition, many\nGSL methods are based on homophily assumption, i.e., similar nodes are more likely to\nconnect with each other. However, many other types of connection exist in the real world\nwhich impose great challenges for GSL.\n\u2022 Future Works. In the future we expect more efficient and generalizable GSL methods to\nbe applied to large-scale and heterogeneous graphs. Most existing GSL methods focus on\npair-wise node similarities and thus struggle to scale to large graphs. Besides, they often\nlearn homogeneous graph structure, but in many scenarios graphs are heterogeneous.\n10 Social Analysis\nIn the real world, there usually exist complex relations and interactions between people and multiple\nentities. Taking people, concrete things, and abstract concepts in society as nodes and taking the\ndiverse, changeable, and large-scale connections between data as links, we can form massive and\ncomplex social information as social networks [43, 436]. Compared with traditional data structures\nsuch as texts and forms, modeling social data as graphs has many benefits. Especially with the arrival\nof the \"big data\" era, more and more heterogeneous information is interconnected and integrated,\nand it is difficult and uneconomical to model this information with a traditional data structure. The\ngraph is an effective implementation for information integration, as it can naturally incorporate\ndifferent types of objects and their interactions from heterogeneous data sources [349, 411]. A\nsummarization of social analysis applications is provided in Table 9.\n10.1 Concepts of Social Networks\nA social network is usually composed of multiple types of nodes, link relationships, and node\nattributes, which inherently include rich structural and semantic information. Specifically, a social\nnetwork can be homogeneous or heterogeneous and directed or undirected in different scenarios.\nWithout loss of generality, we define the social network as a directed heterogeneous graph \ud835\udc3a =\n{\ud835\udc49 , \ud835\udc38, T, R}, where \ud835\udc49 = {\ud835\udc5b\ud835\udc56 }\n|\ud835\udc49 |\n\ud835\udc56=1\nis the node set, \ud835\udc38 = {\ud835\udc52\ud835\udc56 }\n|\ud835\udc38|\n\ud835\udc56=1\nis the edge set, T = {\ud835\udc61\ud835\udc56 }\n| T |\n\ud835\udc56=1\nis the node\ntype set, and R = {\ud835\udc5f\ud835\udc56 }\n| R |\n\ud835\udc56=1\nis the edge type set. Each node \ud835\udc5b\ud835\udc56 \u2208 \ud835\udc49 is associated with a node type\nmapping: \ud835\udf19\ud835\udc5b (\ud835\udc5b\ud835\udc56) = \ud835\udc61\ud835\udc57: \ud835\udc49 \u2212\u2192 T and each edge \ud835\udc52\ud835\udc56 \u2208 \ud835\udc38 is associated with a node type mapping:\n\ud835\udf19\ud835\udc52 (\ud835\udc52\ud835\udc56) = \ud835\udc5f\ud835\udc57: \ud835\udc38 \u2212\u2192 R. A node \ud835\udc5b\ud835\udc56 may have a feature set, where the feature space is specific for the\nnode type. An edge \ud835\udc52\ud835\udc56is also represented by node pairs (\ud835\udc5b\ud835\udc57, \ud835\udc5b\ud835\udc58 ) at both ends and can be directed\nor undirected with relation-type-specific attributes. If |T | = 1 and |R| = 1, the social network is a\nhomogeneous graph; otherwise, it is a heterogeneous graph.\nAlmost any data produced by social activities can be modeled as social networks, for example,\nthe academic social network produced by academic activities such as collaboration and citation,\nthe online social network produced by user following and followed on social media, and the\nlocation-based social network produced by human activities on different locations. Based on\nconstructing social networks, researchers have new paths to data mining, knowledge discovery,\nand multiple application tasks on social data. Exploring social networks also brings new challenges.\nOne of the critical challenges is how to succinctly represent the network from the massive and\nheterogeneous raw graph data, that is, how to learn continuous and low-dimensional social network\nrepresentations, so as to researchers can efficiently perform advanced machine learning techniques\non the social network data for multiple application tasks, such as analysis, clustering, prediction,\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n44 W. Ju, et al.\nTable 9. A summarization of social analysis applications\nSocial networks Node type Edge type Applications References\nAcademic\nSocial\nNetwork\nAuthor,\nPublication,\nVenue,\nOrganization,\nKeyword\nAuthorship,\nCo-Author,\nAdvisor\u0002advisee,\nCiting, Cited,\nCo-Citing,\nPublishing\nClassification/\nClustering\nPaper/author classification [92, 370, 471,\n563], name disambiguation [52, 319, 368,\n575]\nRelationship\nprediction\nCo-authorship [69, 72, 610], citation rela\u0002tionship [203, 464, 550], advisor-advisee re\u0002lationship [286, 317, 594]\nRecommen\u0002dation\nCollaborator recommendation [235, 236,\n296], paper recommendation [20, 81, 429],\nvenue recommendation [337, 549]\nSocial\nMedia\nNetwork\nUser, Blog,\nArticle, Image,\nVideo\nFollowing,\nLike, Unlike,\nClicked,\nViewed,\nCommented,\nReposted\nAnomaly\ndetection\nMalicious attacks [294, 395, 434], emer\u0002gency detection [28, 79, 257], and robot dis\u0002covery [117, 304]\nSentiment\nanalysis\nCustomer feedback [389, 449, 572], public\nevents [33, 332, 450]\nInfluence\nanalysis\nImportant node finding [91, 386], informa\u0002tion diffusion modeling [226, 246, 356, 562]\nLocation-based\nSocial\nNetwork\nRestaurant,\nCinema, Mall,\nParking\nFriendship,\nCheck-in\nPOI recom\u0002mendation\nSpatial/temporal influence [416, 484, 589],\nsocial relationship [297, 513], textual infor\u0002mation [469, 483, 515]\nUrban\ncomputing\nTraffic congestion prediction [202, 511], ur\u0002ban mobility analysis [45, 539], event de\u0002tection [420, 548]\nand knowledge discovery. Thus, graph representation learning on the social network becomes the\nfoundational technique for social analysis.\n10.2 Academic Social Network\nAcademic collaboration is a common and important behavior in academic society, and also a major\nway for scientists and researchers to innovate and breakthrough scientific research, which leads to\nsocial relationship between scholars. The academic data generated by academic collaboration usually\ncontains a large number of interconnected entities with complex relationships [237, 602]. Normally,\nin an academic social network, the node type set consists of Author, Publication, Venue, Organization,\nKeyword, etc., and the relation set consists of Authorship, Co-Author, Advisor-advisee, Citing, Cited,\nCo-Citing, Publishing, Co-Word, etc. Note that in most social networks, each relation type always\nconnects two fixed node types with a fixed direction. For example, the relation Authorship points\nfrom the node type Author to Publication, and the Co-Author is an undirected relation between two\nnodes with type Author. Based on the node and relation types in an academic social network, one\ncan divide it into multiple categories. For example, the co-author network with nodes of Author and\nrelations of Co-Author, the citation network with nodes of Publication and relation of Citing, and the\nacademic heterogeneous information graph with multiple academic node and relation types. Many\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 45\nresearch institutes and academic search engines, such as Aminer1, DBLP2, Microsoft Academic\nGraph (MAG)3, have provided open academic social network datasets for research purposes.\nThere are multiple applications of graph representation learning on the academic social net\u0002work. Roughly, they can be divided into three categories\u2013academic entity classification/clustering,\nacademic relationship prediction, and academic resource recommendation.\n\u2022 Academic entities usually belong to different classes of research areas. Research of academic\nentity classification and clustering aims to categorize these entities, such as papers and\nauthors, into different classes [92, 213, 370, 471, 537, 563]. In literature, academic networks\nsuch as Cora, citepSeer, and Pubmed [406] have become the most widely used benchmark\ndatasets for examining the performance of graph representation learning models on paper\nclassification. Also, the author name disambiguation problem [52, 319, 368, 575] is also\nessentially a node clustering task on co-author networks and is usually solved by the graph\nrepresentation learning technique.\n\u2022 Academic relationship prediction represents the link prediction task on various academic\nrelations. Typical applications are co-authorship prediction [69, 72, 610] and citation rela\u0002tionship prediction [203, 464, 550]. Existing methods learn representations of authors and\npapers and use the similarity between two nodes to predict the link probability. Besides, some\nwork [286, 317, 594] studies the problem of advisor-advisee relationship prediction in the\ncollaboration network.\n\u2022 Various academic recommendation systems have been introduced to retrieve academic re\u0002sources for users from large amounts of academic data in recent years. For example, collabo\u0002rator recommendation [235, 236, 296] benefit researchers by finding suitable collaborators\nunder particular topics; paper recommendation [20, 81, 429] help researchers find relevant pa\u0002pers on given topics; venue recommendation [337, 549] help researchers choose appropriate\nvenues when they submit papers.\n10.3 Social Media Network\nWith the development of the Internet in decades, various online social media have emerged in large\nnumbers and greatly changed people\u2019s traditional social models. People can establish friendships\nwith others beyond the distance limit and share interests, hobbies, status, activities, and other\ninformation among friends. These abundant interactions on the Internet form large-scale complex\nsocial media networks, also named online social networks. Usually, in an academic social network,\nthe node type set consists of User, Blog, Article, Image, Video, etc., and the relation type set consists\nof Following, Like, Unlike, Clicked, Viewed, Commented, Reposted, etc. The main property of a social\nmedia network is that it usually contains multi-mode information on the nodes, such as video,\nimage, and text. Also, the relations are more complex and multiplex, including the explicit relations\nsuch as Like and Unlike and the implicit relations such as Clicked. The social media network can\nbe categorized into multiple types based on their media categories. For example, the friendship\nnetwork, the movie review network, and the music interacting network are extracted from different\nsocial media platforms. In a broad sense, the user-item networks in online shopping system can also\nbe viewed as social media networks as they also exist on the Internet and contains rich interactions\nby people. There are many widely used data sources for social media network analysis, such as\nTwitter, Facebook, Weibo, YouTube, and Instagram.\n1https://www.aminer.cn/\n2https://dblp.uni-trier.de/\n3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n46 W. Ju, et al.\nThe mainstream application research on social media networks via graph representation learning\ntechniques mainly includes anomaly detection, sentiment analysis, and influence analysis.\n\u2022 Anomaly detection aims to find strange or unusual patterns in social networks, which has\na wide range of application scenarios, such as malicious attacks [294, 395, 434], emergency\ndetection [28, 79], and robot discovery [117, 304] in social networks. Unsupervised anomaly\ndetection usually learns a reconstructed graph to detect those nodes with higher reconstructed\nerror as the anomaly nodes [5, 588]; Supervised methods model the problem as a binary\nclassification task on the learned graph representations [340, 596].\n\u2022 Sentiment analysis, also named as opinion mining, is to mine the sentiment, opinions, and\nattitudes, which can help enterprises understand customer feedback on products [389, 449,\n572] and help the government analyze the public emotion and make rapid response to public\nevents [33, 332, 450]. The graph representation learning model is usually combined with\nRNN-based [58, 561] or Transformer-based [7, 441] text encoders to incorporate both the\nuser relationship and textual semantic information.\n\u2022 Influence analysis usually aims to find several nodes in a social network to initially spread\ninformation such as advertisements, so as to maximize the final spread of information [91, 386].\nThe core challenge is to model the information diffusion process in the social network. Deep\nlearning methods [226, 246, 356, 562] usually leverage graph neural networks to learn node\nembeddings and diffusion probabilities between nodes.\n10.4 Location-based Social Network\nLocations are the fundamental information of human social activities. With the wide availability of\nmobile Internet and GPS positioning technology, people can easily acquire their precise locations\nand socialize with their friends by sharing their historical check-ins on the Internet. This opens up\na new avenue of research on location-based social network analysis, which gathered significant\nattention from the user, business, and government perspectives. Usually, in a location-based social\nnetwork, the node type set consists of User, and Location, also named Point of Interest(POI) in the\nrecommendation scenario containing multiple categories such as Restaurant, Cinema, Mall, Parking,\netc. The relation type set consists of Friendship, Check-in. Also, those node and relation types that\nexist in traditional social media networks can be included in a location-based social network. The\ndifference with other social networks, the main location-based social networks are spatial and\ntemporal, making the graph representation learning more challenging. For example, in a typical\nsocial network constructed for the POI recommendation, the user nodes are connected with each\nother by their friendship. The location nodes are connected by user nodes with the relations feature\nof timestamps. The location nodes also have a spatial relationship with each other and own have\ncomplex features, including categories, tags, check-in counts, number of users check-in, etc. There\nare many location-based social network datasets, such as Foursquare4, Gowalla5, and Waze6. Also,\nmany social media such as Twitter, Instagram, and Facebook can provide location information.\nThe research of graph representation learning on location-based social networks can be divided\ninto two categories: POI recommendation for business benefits and urban computing for public\nmanagement.\n\u2022 POI recommendation is one of the research hotspots in the field of location-based social\nnetworks and recommendation systems in recent years [195, 219, 489], which aim to uti\u0002lize historical check-ins of users and auxiliary information to recommend potential favor\n4https://foursquare.com/\n5https://www.gowalla.com/\n6https://www.waze.com/live-map/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 47\nplaces for users from a large of location points. Existing researches mainly integrate four\nessential characteristics, including spatial influence, temporal influence [416, 484, 589], social\nrelationship [297, 513], and textual information [469, 483, 515].\n\u2022 Urban computing is defined as a process of analysis of the large-scale connected urban data\ncreated from city activities of vehicles, human beings, and sensors [358, 359, 417]. Besides the\nlocal-based social network, the urban data also includes physical sensors, city infrastructure,\ntraffic roads, and so on. Urban computing aims to improve the quality of public management\nand life quality of people living in city environments. Typical applications including traffic\ncongestion prediction [202, 511], urban mobility analysis [45, 539], event detection [420, 548].\n10.5 Summary\nThis section introduces social analysis by graph representation learning and we provide the\nsummary as follows:\n\u2022 Techniques. Social networks, generated by human social activities, such as communication,\ncollaboration, and social interactions, typically involve massive and heterogeneous data, with\ndifferent types of attributes and properties that can change over time. Thus, social network\nanalysis is a field of study that explores the techniques to understand and analyze the complex\nattributes, heterogeneous structures, and dynamic information of social networks. Social\nnetwork analysis typically learns low-dimensional graph representations that capture the\nessential properties and patterns of the social network data, which can be used for various\ndownstream tasks, such as classification, clustering, link prediction, and recommendation.\n\u2022 Chanllenges and Limitations. Despite the structural heterogeneity in social networks\n(nodes and relations have different types), with the technological advances in social media,\nthe node attributes have become more heterogeneous now, containing text, video, and images.\nAlso, the large-scale problem is a pending issue in social network analysis. The data in\nthe social network has increased exponentially in past decades, containing a high density\nof topological links and a large amount of node attribute information, which brings new\nchallenges to the efficiency and effectiveness of traditional network representation learning\non the social network. Lastly, social networks are often dynamic, which means the network\ninformation usually changes over time, and this temporal information plays a significant\nrole in many downstream tasks, such as recommendations. This brings new challenges to\nrepresentation learning on social networks in incorporating temporal information.\n\u2022 Future Works. Recently, multi-modal big pre-training models that can fuse information\nfrom different modalities have gained increasing attention [369, 379]. These models can\nobtain valuable information from a large amount of unlabeled data and transfer it to various\ndownstream analysis tasks. Moreover, Transformer-based models have demonstrated better\neffectiveness than RNNs in capturing temporal information. In the future, there is potential\nfor introducing multi-modal big pre-training models in social network analysis. Also, it is\nimportant to make the models more efficient for network information extraction and use\nlightweight techniques like knowledge distillation to further enhance the applicability of the\nmodels. These advancements can lead to more effective social network analysis and enable\nthe development of more sophisticated applications in various domains.\n11 Molecular Property Prediction\nMolecular Property Prediction is an essential task in computational drug discovery and cheminfor\u0002matics. Traditional quantitative structure property/activity relationship (QSPR/QSAR) approaches\nare based on either SMILES or fingerprints [344, 522, 570], largely overlooking the topological\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n48 W. Ju, et al.\nfeatures of the molecules. To address this problem, graph representation learning has been widely\napplied to molecular property prediction. A molecule can be represented as a graph where nodes\nstand for atoms and edges stand for atom-bonds (ABs). Graph-level molecular representations\nare learned via the message passing mechanism to incorporate the topological information. The\nrepresentations are then utilized for the molecular property prediction tasks.\nSpecifically, a molecule is denoted as a topological graph G = (V, E), where V = {\ud835\udc63\ud835\udc56|\ud835\udc56 =\n1, . . . , |G|} is the set of nodes representing atoms. A feature vector x\ud835\udc56is associated with each node \ud835\udc63\ud835\udc56\nindicating its type such as Carbon, Nitrogen. E = {\ud835\udc52\ud835\udc56\ud835\udc57 |\ud835\udc56, \ud835\udc57 = 1, . . . , |G|} is the set of edges connecting\ntwo nodes (atoms) \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57 representing atom bonds. Graph representation learning methods\nare used to obtain the molecular representation hG. Then downstream classification or regression\nlayers \ud835\udc53 (\u00b7) are applied to predict the probability of target property of each molecule \ud835\udc66 = \ud835\udc53 (hG).\nIn Section 11.1, we introduce 4 types of molecular properties graph representation learning can\nbe treated and their corresponding datasets. Section 11.2 reviews the graph representation learning\nbackbones applied to molecular property prediction. Strategies for training the molecular property\nprediction methods are listed in Section 11.3.\n11.1 Molecular Property Categorization\nPlenty of molecular properties can be predicted by graph-based methods. We follow Wieder et al.\n[490] to categorize them into 4 types: quantum chemistry, physicochemical properties, biophysics,\nand biological effect.\nQuantum chemistry is a branch of physical chemistry focused on the application of quantum\nmechanics to chemical systems, including conformation, partial charges and energies. QM7, QM8,\nQM9 [501], COD [391] and CSD [154] are datasets for quantum chemistry prediction.\nPhysicochemical properties are the intrinsic physical and chemical characteristics of a substance,\nsuch as bioavailability, octanol solubility, aqueous solubility and hydrophobicity. ESOL, Lipophilicity\nand Freesolv [501] are datasets for physicochemical properties prediction.\nBiophysics properties are about the physical underpinnings of biomolecular phenomena, such\nas affinity, efficacy and activity. PDBbind [466], MUV, and HIV [501] are biophysics property\nprediction datasets.\nBiological effect properties are generally defined as the response of an organism, a population,\nor a community to changes in its environment, such as side effects, toxicity and ADMET. Tox21,\ntoxcast [501] and PTC [448] are biological effect prediction datasets.\nMoleculenet [501] is a widely-used benchmark dataset for molecule property prediction. It\ncontains over 700,000 compounds tested on different properties. For each dataset, they provide\na metric and a splitting pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv,\nLipophilicity and PDBbind are regression tasks, using MAE or RMSE as the evaluation metrics.\nOther tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation metric.\n11.2 Molecular Graph Representation Learning Backbones\nSince node attributes and edge attributes are crucial to molecular representation, most works use\nGNN instead of traditional graph representation learning methods as backbones, since many GNN\nmethods consider edge information. Existing GNNs designed for the general domain can be applied\nto molecular graphs. Table 10 summarizes the GNNs used for molecular property prediction and\nthe types of properties they can be applied to predict.\nFurthermore, many works customize their GNN structure by considering the chemical domain\nknowledge.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 49\nTable 10. Summary of GNNs in molecular property prediction.\nType Spatial/Specrtal Method Application\nReccurent GNN - R-GNN Biological effect [400]\nReccurent GNN - GGNN Quantum chemistry [333],\nBiological effect [9, 115, 492]\nReccurent GNN - IterRefLSTM Biophysics [9], Biological effect [9]\nConvolutional GNN Spatial/Specrtal GCN\nQuantum chemistry [280, 492, 529],\npysicochemical properties [75, 101, 393],\nBiophysics [34, 101, 529]\nBiological effect [261, 501]\nConvolutional GNN Specrtal LanczosNet Quantum chemistry [280]\nConvolutional GNN Specrtal ChebNet Physicochemical properties,\nBiophysics, Biological effect [267]\nConvolutional GNN Spatial GraphSAGE\nPhysicochemical properties [181],\nBiophysics [68, 108, 279],\nBiological effect [181, 328]\nConvolutional GNN Spatial GAT\nPhysicochemical properties [3, 181],\nBiophysics [34, 68],\nBiological effect [181]\nConvolutional GNN Spatial DGCNN Biophysics [63], Biological effect [568]\nConvolutional GNN Spatial GIN\nPhysicochemical properties [34, 181],\nBiophysics [180, 181],\nBiological effect [181]\nConvolutional GNN Spatial MPNN Physicochemical [320]\nTransformer - MAT Physicochemical, Biophysics [616]\n\u2022 First, the chemical bonds and molecule interaction are taken into consideration carefully. For\nexample, Ma et al. [320] use an additional edge GNN to model the chemical bonds separately.\nSpecifically, given an edge (\ud835\udc63,\ud835\udc64), they formulate an Edge-based GNN as:\nm\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = AGGedge ({h\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(\ud835\udc58\u22121)\n\ud835\udc62\ud835\udc63 , x\ud835\udc62 |\ud835\udc62 \u2208 N\ud835\udc63 \\ \ud835\udc64}), h\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = MLPedge ({m\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(0)\n\ud835\udc63\ud835\udc64 }), (127)\nwhere h\n(0)\n\ud835\udc63\ud835\udc64 = \ud835\udf0e(Weine\ud835\udc63\ud835\udc64) is the input state of the Edge-based GNN, Wein \u2208 R\n\ud835\udc51hid\u00d7\ud835\udc51\ud835\udc52\nis the\ninput weight matrix. PotentialNet [115] further uses different message passing operations\nfor different edge types. DGNN-DDI [325] leverage dual graph neural networks to model the\ninteraction between two molecules.\n\u2022 Second, motifs in molecular graphs play an important role in molecular property prediction.\nGSN [34] leverage substructure encoding to construct a topologically-aware message-passing\nmethod. Each node \ud835\udc63 updates its state h\n\ud835\udc61\n\ud835\udc63\nby combining its previous state with the aggregated\nmessages:\nh\n\ud835\udc61+1\n\ud835\udc63 = UP\ud835\udc61+1\nh\n\ud835\udc61\n\ud835\udc63\n, m\n\ud835\udc61+1\n\ud835\udc63\n\u0001\n, (128)\nm\n\ud835\udc61+1\n\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-v)\nor\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-e)\n, (129)\nwhere x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 contains the substructure information associated with nodes and\nedges, [] denotes a multiset. Yu et al. [551] constructs a heterogeneous graph using motifs\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n50 W. Ju, et al.\nand molecules. Motifs and molecules are both treated as nodes and the edges model the\nrelationship between motifs and graphs, for example, if a graph contains a motif, there will\nbe an edge between them. MGSSL [580] leverages a retrosynthesis-based algorithm BRICS\nand additional rules to find the motifs and combines motif layers with atom layers. It is a\nhierarchical framework jointly modeling atom-level information and motif-level information.\nAouichaoui et al. [12] introduce group-contribution-based attention to highlight the most\nsubstructures in molecules.\n\u2022 Third, different feature modalities have been used to improve molecular graph embedding.\nLin et al. [283] combine SMILES modality and graph modality with contrastive learning.\nZhu et al. [608] encode 2D molecular graph and 3D molecular conformation with a unified\nTransformer. It uses a unified model to learn 3D conformation generation given 2D graph\nand 2D graph generation given 3D conformation. Cremer et al.[78] use a Equivariant Graph\nNeural Networks to represent the 3D information of molecules. Liu et al. [293] consider\nmolecular chirality and design a chirality-aware molecular convolution module.\n\u2022 Finally, knowledge graph and literature can provide additional knowledge for molecular\nproperty prediction. Fang et al. [110] introduce a chemical element knowledge graph to\nsummarize microscopic associations between elements and augment the molecular graph\nbased on the knowledge graph, and a knowledge-aware message-passing network is used to\nencode the augmented graph. MuMo [428] introduces biomedical literature to guide molecular\nproperty prediction. It pretrains a GNN and a language model on paired data of molecules\nand literature mentions via contrastive learning:\n\u2113\n(z\n\ud835\udc3a\n\ud835\udc56\n,z\n\ud835\udc47\n\ud835\udc56\n)\n\ud835\udc56\n= \u2212 log\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\n)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc57=1\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc57\n)/\ud835\udf0f)\n, (130)\nwhere z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\nare the representation of molecule and its corresponding literature. Zhao et al.\n[583] propose a unified Transformer architecture to jointly model molecule graph and the\ncorresponding bioassay description.\n11.3 Training strategies\nDespite the encouraging performance achieved by GNNs, the traditional supervised training scheme\nof GNNs faces a severe limitation: The scarcity of available molecules with desired properties.\nAlthough there are a large number of molecular graphs in public databases such as PubChem,\nlabeled molecules are hard to acquire due to the high cost of wet-lab experiments and quantum\nchemistry calculations. Directly training GNNs on such limited molecules in a supervised way\nis prone to over-fitting and lack of generalization. To address this issue, few-shot learning and\nself-supervised learning are widely used in molecular property prediction.\nFew-shot learning. Few-shot learning aims at generalizing to a task with a small labeled data\nset. The prediction of each property is treated as a single task. Metric-based and optimization-based\nfew-shot learning have been adopted for molecular property prediction. Metric-based few-shot\nlearning is similar to nearest neighbors and kernel density estimation, which learns a metric or\ndistance function over objects. IterRefLSTM [9] leverages matching network [455] as the few\u0002shot learning framework, calculating the similarity between support samples and query samples.\nOptimization-based few-shot learning optimizes a meta-learner for parameter initialization which\ncan be fast adapted to new tasks. Meta-MGNN [161] adopts MAML [120] to train a parameter\ninitialization to adapt to different tasks and use self-attentive task weights for each task. PAR [474]\nalso uses MAML framework and learns an adaptive relation graph among molecules for each task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 51\nSelf-supervised learning. Self-supervised learning can pre-train a GNN model with plenty of\nunlabeled molecular graphs and transfer it to specific molecular property prediction tasks. Self\u0002supervised learning contains generative methods and predictive methods. Predictive methods design\nprediction tasks to capture the intrinsic data features. Pre-GNN [181] exploits both node-level and\ngraph-level prediction tasks including context prediction, attribute masking, graph-level property\nprediction and structural similarity prediction. MGSSL [580] provides a motif-based generative\npre-training framework making topology prediction and motif generation iteratively. Contrastive\nmethods learn graph representations by pulling views from the same graph close and pushing\nviews from different graphs apart. Different views of the same graph are constructed by graph\naugmentation or leveraging the 1D SMILES and 3D structure. MolCLR [478] augments molecular\ngraphs by atom masking, bond deletion and subgraph removal and maximizes the agreement\nbetween the original molecular graph and augmented graphs. Fang et al. [110] uses a chemical\nknowledge graph to guide the graph augmentation. SMICLR [366] uses contrastive learning across\nSMILES and 2D molecular graphs. GeomGCL [268] leverages graph contrastive learning to capture\nthe geometry of the molecule across 2D and 3D views. Jiang et al. [201] and Fang et al. [111] integrate\nmolecule graphs with chemical knowledge graph and fuse the two modalities with contrastive\nlearning. Self-supervised learning can also be combined with few-shot learning to fully leverage\nthe hierarchical information in the training set [215].\n11.4 Summary\nThis section introduces graph representation learning in molecular property prediction and we\nprovide the summary as follows:\n\u2022 Techniques. For molecular property prediction, a molecule is represented as a graph whose\nnodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN, GAT, and GraphSAGE\nare adopted to learn the graph-level representation. The representations are then fed into a\nclassification or regression head for the molecular property prediction tasks. Many works\nguide the model structure design with medical domain knowledge including chemical bond\nfeatures, motif features, different modalities of molecular representation, chemical knowledge\ngraph and literature. Due to the scarcity of available molecules with desired properties,\nfew-shot learning and contrastive learning are used to train molecular property prediction\nmodels, so that the model can leverage the information in large unlabeled dataset and can be\nadapted to new tasks with a few examples.\n\u2022 Challenges and Limitations. Despite the great success of graph representation learning\nin molecular property prediction, the methods still have limitations: 1) Few-shot molecular\nproperty prediction are not fully explored. 2) Most methods depend on training with labeled\ndata, but neglect the chemical domain knowledge.\n\u2022 Future Works. In the future, we expect that: 1) More few-shot learning and zero-shot learning\nmethods are studied for molecular property prediction to solve the data scarcity problem. 2)\nHeterogeneous data can be fused for molecular property prediction. There are a large amount\nof heterogeneous data about molecules such as knowledge graphs, molecule descriptions\nand property descriptions. They can be considered to assist molecular property prediction. 3)\nChemical domain knowledge can be leveraged for the prediction model. For example, when\nwe perform affinity prediction, we can consider molecular dynamics knowledge.\n12 Molecular Generation\nMolecular generation is pivotal to drug discovery, where it serves a fundamental role in downstream\ntasks like molecular docking [341] and virtual screening [457]. The goal of molecular generation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n52 W. Ju, et al.\nis to produce chemical structures that satisfy a specific molecular profile, e.g., novelty, binding\naffinity, and SA scores. Traditional methods have relied on 1D string formats like SMILES [148] and\nSELFIES [240]. With the recent advances in graph representation learning, numerous graph-based\nmethods have also emerged, where molecular graph G can naturally embody both 2D topology and\n3D geometry. While recent literature reviews [99, 342] have covered the general topics of molecular\ndesign, this chapter is dedicated to the applications of graph representation learning in the molecular\ngeneration task. Molecular generation is intrinsically a de novo task, where molecular structures\nare generated from scratch to navigate and sample from the vast chemical space. Therefore, this\nchapter does not discuss tasks that restrict chemical structures a priori, such as docking [131, 427]\nand conformation generation [412, 607].\n12.1 Taxonomy for molecular featurization methods\nThis section categorizes the different methods to feature molecules. The taxonomy presented here\nis unique to the task of molecular generation, owing to the various modalities of molecular entities,\ncomplex interactions with other bio-molecular systems and formal knowledge from the laws of\nchemistry and physics.\n2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one thing, a\nmolecule can be unambiguously represented by its 2D topological graph G2D, where atoms are\nnodes and bonds are edges. G2D can be encoded by canonical MPNN models like GCN [230],\nGAT [452], and R-GCN [401], in ways similar to tasks like social networks and knowledge graphs. A\ntypical example of this line of work is GCPN [543], a graph convolutional policy network generating\nmolecules with desired properties such as synthetic accessibility and drug-likeness.\nFor another, the 3D conformation of a molecule can be accurately depicted by its 3D geometric\ngraph G3D, which incorporates 3D atom coordinates. In 3D-GNNs like SchNet [405] and Orb\u0002Net [371], G3D is organized into a \ud835\udc58-NN graph or a radius graph according to the Euclidean distance\nbetween atoms. It is justifiable to approximate G3D as a 3D extension to G2D, since covalent atoms\nare closest to each other in most cases. However, G3D can also find a more long-standing origin\nin the realm of computational chemistry [126], where both covalent and non-covalent atomistic\ninteractions are considered to optimize the potential surface and simulate molecular dynamics.\nTherefore, G3D more realistically represents the molecular geometry, which makes a good fit for\nprotein pocket binding and 3D-QSAR optimization [453].\nMolecules can rotate and translate, affecting their position in the 3D space. Therefore, it is ideal to\nencode these molecules with GNNs equivariant/invariant to roto-translations, which can be \u223c 103\ntimes more efficient than data augmentation [144]. Equivariant GNNs can be based on irreducible\nrepresentation [10, 24, 37, 130, 446], regular representation [121, 192], or scalarization [190, 212,\n232\u2013234, 292, 398, 404, 405, 445], which are explained in more detail in [165]. Recent works like\nGraphVF [430] and MolCode [579] have been incorporating G2D and G3D to accurately capture the\nrelationship between structure and properties in molecular design in a unified way.\nUnbounded vs. binding-based. Earlier works have aimed to generate unbounded molecules in\neither 2D or 3D space, striving to learn good molecular representations through this task. In the 2D\nscenario, GraphNVP [329] first introduces a flow-based model to learn an invertible transformation\nbetween the 2D chemical space and the latent space. GraphAF [413] further adopts an autoregressive\ngeneration scheme to check the valence of the generated atoms and bonds. In the 3D scenario,\nG-SchNet [142] first proposes to utilize G3D (instead of 3D density grids) as the generation backbone.\nIt encodes G3D via SchNet, and uses an auxiliary token to generate atoms on the discretized 3D\nspace autoregressively. G-SphereNet [316] uses symmetry-invariant representations in a spherical\ncoordinate system (SCS) to generate atoms in the continuous 3D space and preserve equivariance.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 53\nUnbounded models adopt certain techniques to optimize specific properties of the generated\nmolecules. GCPN and GraphAF use scores like logP, QED, and chemical validity to tune the model\nvia reinforcement learning. EDM [178] can generate 3D molecules with property \ud835\udc50 by re-training\nthe diffusion model with \ud835\udc50\u2019s feature vector concatenated to the E(n) equivariant dynamics function\n\ud835\udf50\u02c6\ud835\udc61 = \ud835\udf19 (\ud835\udc9b\ud835\udc61, [\ud835\udc61, \ud835\udc50]). cG-SchNet [143] adopts a conditioning network architecture to jointly target\nmultiple electronic properties during conditional generation without the need to re-train the model.\nRetMol [481] uses a retrieval-based model for controllable generation.\nOn the other hand, binding-based methods generate drug-like molecules (aka. ligands) according\nto the binding site (aka. binding pocket) of a protein receptor. Drawing inspirations from the lock\u0002and-key model for enzyme action [122], works like LiGAN [380] and DESERT [302] uses 3D\ndensity grids to fit the density surface between the ligand and the receptor, encoded by 3D-CNNs.\nMeanwhile, a growing amount of literature has adopted G3D for representing ligand and receptor\nmolecules, because G3D more accurately depicts molecular structures and atomistic interactions\nboth within and between the ligand and the receptor. Representative works include 3D-SBDD [306],\nGraphBP [288], Pocket2Mol [361], and DiffSBDD [403]. GraphBP shares a similar workflow with G\u0002SphereNet, except that the receptor atoms are also incorporated into G3D to depict the 3D geometry\nat the binding pocket.\nAtom-based vs.fragment-based. Molecules are inherently hierarchical structures. At the atom\u0002istic level, molecules are represented by encoding atoms and bonds. At a coarser level, molecules\ncan also be represented as molecular fragments like functional groups or chemical sub-structures.\nBoth the composition and the geometry are fixed within a given fragment, e.g., the planar peptide\u0002bond (\u2013CO\u2013NH\u2013) structure. Fragment-based generation effectively reduces the degree of freedom\n(DOF) of chemical structures, and injects well-established knowledge about molecular patterns\nand reactivity. JT-VAE [207] decomposes 2D molecular graph G2D into a junction-tree structure\nT, which is further encoded via tree message-passing. DeepScaffold [270] expands the provided\nmolecular scaffold into 3D molecules. L-Net [272] adopts a graph U-Net architecture and devises\na custom three-level node clustering scheme for pooling and unpooling operations in molecular\ngraphs. A number of works have also emerged lately for fragment-based generation in the binding\u0002based setting, including FLAG [581] and FragDiff [360]. FLAG uses a regression-based approach to\nsequentially decide the type and torsion angle of the next fragment to be placed at the binding site,\nand finally optimizes the molecule conformation via a pseudo-force field. FragDiff also adopts a\nsequential generation process but uses a diffusion model to determine the type and pose of each\nfragment in one go.\n12.2 Generative methods for molecular graphs\nFor a molecular graph generation process, the model first learns a latent distribution \ud835\udc43 (\ud835\udc4d |G)\ncharacterizing the input molecular graphs. A new molecular graph G\u02c6 is then generated by sam\u0002pling and decoding from this learned distribution. Various models have been adopted to generate\nmolecular graphs, including generative adversarial network (GAN), variational auto-encoder (VAE),\nnormalizing flow (NF), diffusion model (DM), and autoregressive model (AR).\nGenerative adversarial network (GAN). GAN [149] is trained to discriminate real data \ud835\udc99\nfrom generated generated data \ud835\udc9b, with the training object formalized as\nmin\n\ud835\udc3a\nmax\n\ud835\udc37\nL (\ud835\udc37,\ud835\udc3a) = E\ud835\udc99\u223c\ud835\udc5ddata [log\ud835\udc37(\ud835\udc99)] + E\ud835\udc9b\u223c\ud835\udc5d (\ud835\udc9b) [log(1 \u2212 \ud835\udc37(\ud835\udc3a(\ud835\udc9b)))], (131)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n54 W. Ju, et al.\nwhere \ud835\udc3a(\u00b7) is the generator function and \ud835\udc37(\u00b7) is the discriminator function. For example, Mol\u0002GAN [82] encodes G2D with R-GCN, trains \ud835\udc37 and \ud835\udc3a with improved W-GAN [13], and uses rein\u0002forcement learning to generate attributed molecules, where the score function is assigned from\nRDKit [249] and chemical validity.\nVaraitional auto-encoder (VAE). In VAE [228], the decoder parameterizes the conditional\nlikelihood distribution \ud835\udc5d\ud835\udf03 (\ud835\udc99|\ud835\udc9b), and the encoder parameterizes an approximate posterior distribution\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99) \u2248 \ud835\udc5d\ud835\udf03 (\ud835\udc9b|\ud835\udc99). The model is optimized by the evidence lower bound (ELBO), consisting of the\nreconstruction loss term and the distance loss term:\nmax\n\ud835\udf03,\ud835\udf19\nL\ud835\udf03,\ud835\udf19 (\ud835\udc99) := E\ud835\udc9b\u223c\ud835\udc5e\ud835\udf19 (\u00b7 |\ud835\udc99)\n\u0014\nln \ud835\udc5d\ud835\udf03 (\ud835\udc99, \ud835\udc9b)\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99)\n\u0015\n= ln \ud835\udc5d\ud835\udf03 (\ud835\udc99) \u2212 \ud835\udc37KL \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc99) \u2225\ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc99)\n\u0001\n. (132)\nMaximizing ELBO is equivalent to simultaneously maximizing the log-likelihood of the observed\ndata, and minimizing the divergence of the approximate posterior \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc65) from the exact poste\u0002rior \ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc65). Representative works along this thread include JT-VAE [207], GraphVAE [419], and\nCGVAE [290] for the 2D generation task, and 3DMolNet [351] for the 3D generation task.\nAutoregressive model (AR). Autoregressive model is an umbrella definition for any model\nthat sequentially generates the components (atoms or fragments) of a molecule. ARs better capture\nthe interdependency within the molecular structure and allows for explicit valency check. For each\nstep in AR, the new component can be produced using different techniques:\n\u2022 Regression/classification, such is the case with 3D-SBDD [306], Pocket2Mol [361], etc.\n\u2022 Reinforcement learning, such is the case with L-Net [272], DeepLigBuilder [273], etc.\n\u2022 Probabilistic models like normalizing flow and diffusion.\nNormalizing flow (NF). Based on the change-of-variable theorem, NF [385] constructs an\ninvertible mapping \ud835\udc53 between a complex data distribution \ud835\udc99 \u223c \ud835\udc4b: and a normalized latent distribu\u0002tion \ud835\udc9b \u223c \ud835\udc4d. Unlike VAE, which has juxtaposed parameters for encoder and decoder, the flow model\nuses the same set of parameter for encoding and encoding: reverse flow \ud835\udc53\n\u22121\nfor generation, and\nforward flow \ud835\udc53 for training:\nmax\n\ud835\udc53\nlog \ud835\udc5d(\ud835\udc99) = log \ud835\udc5d\ud835\udc3e (\ud835\udc9b\ud835\udc3e) (133)\n= log \ud835\udc5d\ud835\udc3e\u22121 (\ud835\udc9b\ud835\udc3e\u22121) \u2212 log\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc3e (\ud835\udc9b\ud835\udc3e\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc3e\u22121\n\u0013\n(134)\n= . . . (135)\n= log \ud835\udc5d0 (\ud835\udc9b0) \u2212\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc56=1\nlog\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc56 (\ud835\udc9b\ud835\udc56\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc56\u22121\n\u0013\n, (136)\nwhere \ud835\udc53 = \ud835\udc53\ud835\udc3e \u25e6 \ud835\udc53\ud835\udc3e\u22121 \u25e6 ... \u25e6 \ud835\udc531 is a composite of \ud835\udc3e blocks of transformation. While GraphNVP [329]\ngenerates the molecular graph with NF in one go, following works tend to use the autoregressive\nflow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439].\nDiffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion\nsteps to slowly add random noise to data \ud835\udc990 \u223c \ud835\udc5e(\ud835\udc99):\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121) = N (\ud835\udc99\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc99\ud835\udc61\u22121, \ud835\udefd\ud835\udc61 \ud835\udc70), (137)\n\ud835\udc5e(\ud835\udc991:\ud835\udc47 |\ud835\udc990) =\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121). (138)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 55\nTable 11. Summary of molecular generation models.\nModel 2D/3D Binding\u0002based\nFragment\u0002basedGNN\nBackbone\nGenerative\nModel\nGCPN [543] 2D GCN [230] GAN\nMolGAN [82] 2D R-GCN [401] GAN\nDEFactor [16] 2D GCN GAN\nGraphVAE [419] 2D ECC [418] VAE\nMDVAE [100] 2D GGNN [274] VAE\nJT-VAE [207] 2D \u2713 MPNN [147] VAE\nCGVAE [290] 2D GGNN VAE\nDeepScaffold [270] 2D \u2713 GCN VAE\nGraphNVP [329] 2D R-GCN NF\nMoFlow [557] 2D R-GCN NF\nGraphAF [413] 2D R-GCN NF + AR\nGraphDF [318] 2D R-GCN NF + AR\nL-Net [272] 3D \u2713 g-U-Net [133] AR\nG-SchNet [142] 3D SchNet [405] AR\nGEN3D [388] 3D EGNN [398] AR\nG-SphereNet [316] 3D SphereNet [292] NF + AR\nEDM [178] 3D EGNN DM\nGCDM [347] 3D GCPNet [346] DM\n3D-SBDD [306] 3D \u2713 SchNet AR\nPocket2Mol [361] 3D \u2713 GVP [211] AR\nFLAG [581] 3D \u2713 \u2713 SchNet AR\nGraphBP [288] 3D \u2713 SchNet NF + AR\nSiamFlow [439] 3D \u2713 R-GCN NF\nDiffBP [282] 3D \u2713 EGNN DM\nDiffSBDD [403] 3D \u2713 EGNN DM\nTargetDiff [156] 3D \u2713 EGNN DM\nFragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR\nGraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR\nMolCode [579] 2D + 3D \u2713 EGNN NF + AR\nThey then learn to reverse the diffusion process to construct desired data samples from the noise:\n\ud835\udc5d\ud835\udf03 (\ud835\udc990:\ud835\udc47 ) = \ud835\udc5d(\ud835\udc99\ud835\udc47 )\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61), (139)\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61) = N (\ud835\udc99\ud835\udc61\u22121; \ud835\udf41\ud835\udf03(\ud835\udc99\ud835\udc61, \ud835\udc61), \ud835\udeba\ud835\udf03 (\ud835\udc99\ud835\udc61, \ud835\udc61)), (140)\nwhile the models are trained using a variational lower bound. Diffusion models have been applied\nto generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands\nin DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate\nmolecular fragments in autoregressive models, as is the case with FragDiff [360].\n12.3 Summary and prospects\nWe wrap up this chapter with Table 11, which profiles existing molecular generation models\naccording to their taxonomy for molecular featurization, the GNN backbone, and the generative\nmethod. This chapter covers the critical topics of molecular generation, which also elicit valuable\ninsights into the promising directions for future research. We summarize these important aspects\nas follows.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n56 W. Ju, et al.\nTechniques. Graph neural networks can be flexibly leveraged to encode molecular features\non different representation levels and across different problem settings. Canonical GNNs like\nGCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs,\nwhile 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular,\nthis 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the\nbinding protein receptor is considered alongside the ligand geometry per se. Fragment-based models\nlike JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure.\nVarious generative methods have also been effectively incorporated into the molecular setting,\nincluding generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive\nmodel (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to\ngenerate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating\nthe search for drug candidates.\nChallenges and Limitations. While there has been an abundant supply of unlabelled molecular\nstructural and geometric data [125, 193, 426], the number of labeled molecular data over certain\ncritical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the\nother hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of\nthe generated molecules, such as QED and Vina [103], rather than actual wet lab experiments.\nFuture Works. Besides the structural and geometric attributes described in this chapter, an\neven more extensive array of data can be applied to aid molecular generation, including chemical\nreactions and medical ontology. These data can be organized into a heterogeneous knowledge\ngraph to aid the extraction of high-quality molecular representations. Furthermore, high through\u0002put experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and\ndruggability of the generated molecules in the wet lab. Concrete case studies, such as the design of\npotential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights\ninto leveraging these molecular generative models to facilitate the design and fabrication of potent\nand applicable drug molecules in the pharmaceutical industry.\nIntegrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations\noffers a promising new direction in molecular generation. Recent studies like those by [196] and\n[160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLM\u0002based approaches in this domain, including those by [338] and [18], predominantly utilize textual\nSMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The\nemerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image,\nand text, which could more comprehensively capture the intricacies of molecular structures. This\napproach marks a significant shift towards utilizing graph-based information alongside traditional\ntext, enhancing the capability of LLMs in molecular generation. Such advances suggest that future\nresearch should focus more on exploiting the synergy between graph-based molecular representa\u0002tions and the evolving landscape of LLMs to address complex challenges in chemistry and material\nsciences.\n13 Recommender Systems\nThe use of graph representation learning in recommender systems has been drawing increasing\nattention as one of the key strategies for addressing the issue of information overload. With their\nstrong ability to capture high-order connectivity between graph nodes, deep graph representation\nlearning has been shown to be beneficial in enhancing recommendation performance across a\nvariety of recommendation scenarios.\nTypical recommender systems take the observed interactions between users and items and\ntheir fixed features as input, and are intended for making proper predictions on which items a\nspecific user is probably interested in. To formulate, given an user set U, an item set I and the\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 57\nTable 12. Summary of graph models for recommender systems.\nModel Recommendation Task Graph Structure Graph Encoder Representation\nGC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer\nNGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation\nMMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer\nLightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling\nDGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling\nCAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling\nSR-GNN [496] Session-based Transition Graph GGNN Soft-Attention\nGC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention\nFGNN [377] Session-based Session Graph GAT Last-Layer\nGAG [378] Session-based Session Graph GCN Self-Attention\nGCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling\nHyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention\nDHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer\nMBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention\nHCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer\nH3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer\nSTHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling\ninteraction matrix between users and items \ud835\udc4b \u2208 {0, 1}\n|U|\u00d7|I|\n, where \ud835\udc4b\ud835\udc62,\ud835\udc63 indicates there is an\nobserved interaction between user \ud835\udc62 and item \ud835\udc56. The target of GNNs on recommender systems is to\nlearn representations \u210e\ud835\udc62, \u210e\ud835\udc56 \u2208 R\n\ud835\udc51\nfor given \ud835\udc62 and \ud835\udc56. The preference score can further be calculated\nby a similarity function:\n\ud835\udc65\u02c6\ud835\udc62,\ud835\udc56 = \ud835\udc53 (\u210e\ud835\udc62, \u210e\ud835\udc56), (141)\nwhere \ud835\udc53 (\u00b7, \u00b7) is the similarity function, e.g. inner product, cosine similarity, multi-layer perceptrons\nthat takes the representation of \ud835\udc62 and \ud835\udc56 and calculate the preference score \ud835\udc65\u02c6\ud835\udc62,\ud835\udc56.\nWhen it comes to adapting graph representation learning in recommender systems, a key step is\nto construct graph-structured data from the interaction set \ud835\udc4b. Generally, a graph is represented\nas G = {V, E} where V, E denotes the set of vertices and edges respectively. According to the\nconstruction of G, we can categorize the existing works as follows into three parts which are\nintroduced in the following subsections. A summary is provided in Table 12.\n13.1 User-Item Bipartite Graph\n13.1.1 Graph Construction A undirected bipartite graph where the vertex set V = U \u222a I and the\nundirected edge set E = {(\ud835\udc62,\ud835\udc56)|\ud835\udc62 \u2208 U \u2227\ud835\udc56 \u2208 I}. Under this case the graph adjacency can be directly\nobtained from the interaction matrix, thus the optimization target on the user-item bipartite graph\nis equivalent to collaborative filtering tasks such as MF [239] and SVD++ [238].\nThere have been plenty of previous works that applied GNNs on the constructed user-item bipar\u0002tite graphs. GC-MC [27] firstly applies graph convolution networks to user-item recommendation\nand optimizes a graph autoencoder (GAE) to reconstruct interactions between users and items.\nNGCF [470] introduces the concept of Collaborative Filtering (CF) into graph-based recommen\u0002dations by modeling the affinity between neighboring nodes on the interaction graph. MMGCN\n[485] extends the graph-based recommendation to multi-modal scenarios by constructing different\nsubgraphs for each modal. LightGCN [169] improves NGCF by removing the non-linear activation\nfunctions and simplifying the message function. With the development of disentangled representa\u0002tion learning, there are works like DGCF [473] that introduce disentangled graph representation\nlearning to represent users and items from multiple disentangled perspectives. Additionally, having\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n58 W. Ju, et al.\nrealized the limitation of the existing message-passing scheme in capturing collaborative signals,\nCAGCN [480] proposes Common Interacted Ratio (CIR) as a recommendation-oriented topological\nmetric for GNN-based recommender models.\n13.1.2 Graph Propagation Scheme A common practice is to follow the traditional message-passing\nnetworks (MPNNs) and design the graph propagation method accordingly. GC-MC adopts vanilla\nGCNs to encode the user-item bipartite graph. NGCF enhances GCNs by considering the affinity\nbetween users and items. The message function of NGCF from node \ud835\udc57 to \ud835\udc56 is formulated as:\n(\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc57 = \u221a 1\n|N\ud835\udc56| |N\ud835\udc57|\n(\ud835\udc4a1\ud835\udc52\ud835\udc57 +\ud835\udc4a2 (\ud835\udc52\ud835\udc56 \u2299 \ud835\udc52\ud835\udc57))\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc56 = \ud835\udc4a1\ud835\udc52\ud835\udc56\n, (142)\nwhere \ud835\udc4a1,\ud835\udc4a2 are trainable parameters, \ud835\udc52\ud835\udc56 represents \ud835\udc56\u2019s representation from previous layer. The\nmatrix form can be further provided by:\n\ud835\udc38\n(\ud835\udc59) = LeakyReLU( (L + \ud835\udc3c)\ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n1\n+ L\ud835\udc38\n(\ud835\udc59\u22121) \u2299 \ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n2\n), (143)\nwhere L represents the Laplacian matrix of the user-item graph. The element-wise product in Eq.\n143 represents the affinity between connected nodes, containing the collaborative signals from\ninteractions.\nHowever, the notable heaviness and burdensome calculation of NGCF\u2019s architecture hinder\nthe model from making faster recommendations on larger graphs. LightGCN solves this issue by\nproposing Light Graph Convolution (LGC), which simplifies the convolution operation with:\n\ud835\udc52\n(\ud835\udc59+1)\n\ud835\udc56\n=\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n1\n\u221a\ufe01\n|N\ud835\udc56||N\ud835\udc57|\n\ud835\udc52\n(\ud835\udc59)\n\ud835\udc57\n. (144)\nWhen an interaction takes place, e.g. a user clicks a particular item, there could be multiple\nintentions behind the observed interaction. Thus it is necessary to consider the various disentangled\nintentions among users and items. DGCF proposes the cross-intent embedding propagation scheme\non the graph, inspired by the dynamic routing algorithm of capsule networks [394]. To formulate,\nthe propagation process maintains a set of routing logits \u02dc\ud835\udc46\ud835\udc58 (\ud835\udc62,\ud835\udc56) for each user \ud835\udc62. The weighted sum\naggregator to get the representation of \ud835\udc62 can be defined as:\n\ud835\udc62\n\ud835\udc61\n\ud835\udc58\n=\n\u2211\ufe01\n\ud835\udc56\u2208N\ud835\udc62\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) \u00b7 \ud835\udc56\n0\n\ud835\udc58\n(145)\nfor \ud835\udc61-th iteration, where L\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) denotes the Laplacian matrix of \ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56), formulated as:\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) =\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n\u221a\ufe03\n[\n\u00cd\n\ud835\udc56\n\u2032\u2208N\ud835\udc62\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56\u2032)] \u00b7 [\u00cd\n\ud835\udc62\n\u2032\u2208N\ud835\udc56\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62\n\u2032\n,\ud835\udc56)]\n. (146)\n13.1.3 Node Representations After the graph propagation module outputs node-level representa\u0002tions, there are multiple methods to leverage node representations for recommendation tasks. A\nplain solution is to apply a readout function on layer outputs like the concatenation operation used\nby NGCF:\n\ud835\udc52\n\u2217 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc52(0)\n, ..., \ud835\udc52 (\ud835\udc3f)) = \ud835\udc52\n(0)\n\u2225...\u2225\ud835\udc52\n(\ud835\udc3f)\n. (147)\nHowever, the readout function among layers would neglect the relationship between the target\nitem and the current user. A general solution is to use the attention mechanism [451] to reweight\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 59\nand aggregate the node representations. SR-GNN adapts soft-attention mechanism to model the\nitem-item relationship:\n\ud835\udefc\ud835\udc56 = q\n\ud835\udc47\n\ud835\udf0e(\ud835\udc4a1\ud835\udc52\ud835\udc61 +\ud835\udc4a2\ud835\udc52\ud835\udc56 + \ud835\udc50),\n\ud835\udc60\ud835\udc54 =\n\ud835\udc5b\u2211\ufe01\u22121\n\ud835\udc56=1\n\ud835\udefc\ud835\udc56\ud835\udc52\ud835\udc56,\n(148)\nwhere q, \ud835\udc4a1, \ud835\udc4a2 are trainable matrices.\nSome methods focus on exploiting information from multiple graph structures. A common\npractice is contrastive learning, which maximizes the mutual information between hidden repre\u0002sentations from several views. HCCF leverage InfoNCE loss as the estimator of mutual information,\ngiven a pair of representation \ud835\udc67\ud835\udc56, \u0393\ud835\udc56 for node \ud835\udc56, controlled by temperature parameter \ud835\udf0f:\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc41\ud835\udc36\ud835\udc38 (\ud835\udc56) = \u2212 log exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56))/\ud835\udf0f\n\u00cd\n\ud835\udc56\n\u2032\u2260\ud835\udc56 exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56\n\u2032 ))/\ud835\udf0f\n. (149)\nBesides InfoNCE, there exist several other ways to combine node representations from different\nviews. For instance, MBHT applies an attention mechanism to fuse multiple semantics, DisenPOI\nadapts bayesian personalized ranking loss (BPR) [384] as a soft estimator for contrastive learning,\nand KBGNN applies pair-wise similarities to ensure the consistency from two views.\n13.2 Transition Graph\n13.2.1 Transition Graph Construction Since sequence-based recommendation (SR) is one of the\nfundamental problems in recommender systems, some researches focus on modeling the sequential\ninformation with GNNs. A commonly applied way is to construct transition graphs based on each\ngiven sequence according to the clicking sequence by a user. To formulate, given a user \ud835\udc62\u2019s clicking\nsequence \ud835\udc60\ud835\udc62 = [\ud835\udc56\ud835\udc62,1,\ud835\udc56\ud835\udc62,2, ...,\ud835\udc56\ud835\udc62,\ud835\udc5b] containing \ud835\udc5b items, noting that there could be duplicated items, the\nsequential graph is constructed via G\ud835\udc60 = {SET(\ud835\udc60\ud835\udc62), E}, where \u2200\n\ud835\udc56\ud835\udc57,\ud835\udc56\ud835\udc58\n\u2208 E indicates there exists\na successive transition from \ud835\udc56\ud835\udc57 to \ud835\udc56\ud835\udc58 . Since G\ud835\udc60 are directed graphs, a widely used way to depict\ngraph connectivity is by building the connection matrix \ud835\udc34\ud835\udc60 \u2208 R\n\ud835\udc5b\u00d72\ud835\udc5b\n. \ud835\udc34\ud835\udc60is the combination of two\nadjacency matrices \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n], which denotes the normalized node degrees of incoming\nand outgoing edges in the session graph respectively.\nThe proposed transition graphs that obtain user behavior patterns have been demonstrated\nimportant to session-based recommendations [263, 291]. SR-GNN and GC-SAN [496, 516] propose\nto leverage transition graphs and apply attention-based GNNs to capture the sequential information\nfor session-based recommendation. FGNN [377] formulates the recommendation within a session\nas a graph classification problem to predict the next item for an anonymous user. GAG [378] and\nGCE-GNN [482] further extend the model to capture global embeddings among multiple session\ngraphs.\n13.2.2 Session Graph Propagation Since the session graphs are directed item graphs, there have\nbeen multiple session graph propagation methods to obtain node representations on session graphs.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n60 W. Ju, et al.\nSR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information\nfrom a given session graph adjacency \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n] and item embedding set {\ud835\udc52\ud835\udc56 }:\n\ud835\udc4e\ud835\udc61 = \ud835\udc34\ud835\udc60 [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121]\n\ud835\udc47\ud835\udc3b + \ud835\udc4f, (150)\n\ud835\udc67\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc67\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc67\ud835\udc52\ud835\udc61\u22121), (151)\n\ud835\udc5f\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc5f\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc61\u22121), (152)\n\ud835\udc52\u02dc\ud835\udc61 = tanh(\ud835\udc4a\ud835\udc5c\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5c (\ud835\udc5f\ud835\udc61 \u2299 \ud835\udc52\ud835\udc61\u22121)), (153)\n\ud835\udc52\ud835\udc61 = (1 \u2212 \ud835\udc67\ud835\udc61) \u2299 \ud835\udc52\ud835\udc61\u22121 + \ud835\udc67\ud835\udc61\ud835\udc52\u02dc\ud835\udc61, (154)\nwhere \ud835\udc4a s and \ud835\udc48 s are trainable parameters. GC-SAN extend GGNN by calculating initial state \ud835\udc4e\ud835\udc61\nseparately to better exploit transition information:\n\ud835\udc4e\ud835\udc61 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc56\ud835\udc5b)\n), \ud835\udc34(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n)). (155)\n13.3 HyperGraph\n13.3.1 Hypergraph Topology Construction Motivated by the idea of modeling hyper-structures\nand high-order correlation among nodes, hypergraphs [119] are proposed as extensions of the\ncommonly used graph structures. For graph-based recommender systems, a common practice is\nto construct hyper structures among the original user-item bipartite graphs. To be specific, an\nincidence matrix of a graph with vertex set V is presented as a binary matrix \ud835\udc3b \u2208 {0, 1}\n|V |\u00d7 | E |\n,\nwhere E represents the set of hyperedges. Each entry \u210e(\ud835\udc63, \ud835\udc52) of \ud835\udc3b depicts the connectivity between\nvertex \ud835\udc63 and hyperedge \ud835\udc52:\n\u210e(\ud835\udc63, \ud835\udc52) =\n(\n1 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2208 \ud835\udc52\n0 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2209 \ud835\udc52\n. (156)\nGiven the formulation of hypergraphs, the degrees of vertices and hyperedges of \ud835\udc3b can then be\ndefined with two diagonal matrices \ud835\udc37\ud835\udc63 \u2208 N\n|V |\u00d7 |V | and \ud835\udc37\ud835\udc52 \u2208 N| E |\u00d7 | E |, where\n\ud835\udc37\ud835\udc63 (\ud835\udc56;\ud835\udc56) =\n\u2211\ufe01\n\ud835\udc52\u2208 E\n\u210e(\ud835\udc63\ud835\udc56, \ud835\udc52), \ud835\udc37\ud835\udc52 (\ud835\udc57; \ud835\udc57) =\n\u2211\ufe01\n\ud835\udc63\u2208V\n\u210e(\ud835\udc63, \ud835\udc52\ud835\udc57). (157)\nThe development of Hypergraph Neural Networks (HGNNs) [119, 188, 598] have shown to\nbe capable of capturing the high-order connectivity between nodes. HyperRec [463] firstly at\u0002tempts to leverage hypergraph structures for sequential recommendation by connecting items\nwith hyperedges according to the interactions with users during different time periods. DHCF\n[198] proposes to construct hypergraphs for users and items respectively based on certain rules, to\nexplicitly capture the collaborative similarities via HGNNs. MBHT [532] combines hypergraphs\nwith a low-rank self-attention mechanism to capture the dynamic heterogeneous relationships\nbetween users and items. HCCF [505] uses the contrastive information between hypergraph and\ninteraction graph to enhance the recommendation performance. To extend the model\u2019s ability to\nmulti-domain categories of items, H3Trans [523] incorporates two hyperedge-based modules and\nleverages hierarchical hypergraph propagation to transfer from domains. STHGCN [524] formulates\na spatio-temporal hypergraph structure for POI recommendation.\n13.3.2 Hyper Graph Message Passing With the development of HGNNs, previous works have\nproposed different variants of HGNN to better exploit hypergraph structures. A classic high-order\nhyper convolution process on a fixed hypergraph G = {V, E} with hyper adjacency \ud835\udc3b is given by:\n\ud835\udc54 \u2605\ud835\udc4b = \ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc3b\ud835\udc37\u22121\n\ud835\udc52 \ud835\udc3b\n\ud835\udc47\ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc4b\u0398, (158)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 61\nwhere \ud835\udc37\ud835\udc63, \ud835\udc37\ud835\udc52 are degree matrices of nodes and hyperedges, \u0398 denotes the convolution kernel. For\nhyper adjacency matrix \ud835\udc3b, DHCF refers to a rule-based hyperstructure via k-order reachable rule,\nwhere nodes in the same hyperedge group are k-order reachable to each other:\n\ud835\udc34\n\ud835\udc58\n\ud835\udc62 = min(1, power(\ud835\udc34 \u00b7 \ud835\udc34\n\ud835\udc47\n, \ud835\udc58)), (159)\nwhere \ud835\udc34 denotes the graph adjacency matrix. By considering the situations where \ud835\udc58 = 1, 2, the\nmatrix formulation of the hyper connectivity of users and items is calculated with:\n(\n\ud835\udc3b\ud835\udc62 = \ud835\udc34\u2225 (\ud835\udc34(\ud835\udc34\n\ud835\udc47\ud835\udc34))\n\ud835\udc3b\ud835\udc56 = \ud835\udc34\n\ud835\udc47\n\u2225 (\ud835\udc34\n\ud835\udc47\n(\ud835\udc34\ud835\udc34\ud835\udc47))\n, (160)\nwhich depicts the dual hypergraphs for users and items.\nHCCF proposes to construct a learnable hypergraph to depict the global dependencies between\nnodes on the interaction graph. To be specific, the hyperstructure is factorized with two low-rank\nembedding matrices to achieve model efficiency:\n\ud835\udc3b\ud835\udc62 = \ud835\udc38\ud835\udc62 \u00b7\ud835\udc4a\ud835\udc62, \ud835\udc3b\ud835\udc63 = \ud835\udc38\ud835\udc63 \u00b7\ud835\udc4a\ud835\udc63 . (161)\n13.4 Other Graphs\nSince there are a variety of recommendation scenarios, several tailored designed graph structures\nhave been proposed accordingly, to better exploit the domain information from different scenarios.\nFor instance, CKE [564] and MKR [462] introduce Knowledge graphs to enhance graph recommen\u0002dation. GSTN [484], KBGNN [219], DisenPOI [373] and Diff-POI [374] propose to build geographical\ngraphs based on the distance between Point-of-Interests (POIs) to better model the locality of users\u2019\nvisiting patterns. TGSRec [109] and DisenCTR [475] empower the user-item interaction graphs with\ntemporal sampling between layers to obtain sequential information from static bipartite graphs.\n13.5 Summary\nThis section introduces the application of different kinds of graph neural networks in recommender\nsystems and can be summarized as follows:\n\u2022 Graph Constructions. There are multiple options for constructing graph-structured data\nfor a variety of recommendation tasks. For instance, the user-item bipartite graphs reveal\nthe high-order collaborative similarity between users and items, and the transition graph\nis suitable for encoding sequential information in clicking history. These diversified graph\nstructures provide different views for node representation learning on users and items, and\ncan be further used for downstream ranking tasks.\n\u2022 Challenges and Limitations. Though the superiority of graph-structured data and GNNs\nagainst traditional methods has been widely illustrated, there are still challenges unsolved.\nFor example, the computational cost of graph methods is normally expensive and thus\nunacceptable in real-world applications. The data sparsity and cold-started issue in graph\nrecommendation remains to be explored as well.\n\u2022 Future Works. In the future, an efficient solution for applying GNNs in recommendation\ntasks is expected. There are also some attempts [109, 372, 475] on incorporating temporal\ninformation in graph representation learning for sequential recommendation tasks.\n14 Traffic Analysis\nIntelligent Transportation Systems (ITS) are essential for safe, reliable, and efficient transportation\nin smart cities, serving the daily commuting and traveling needs of millions of people. To support\nITS, advanced modeling and analysis techniques are necessary, and Graph Neural Networks (GNNs)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n62 W. Ju, et al.\nare a promising tool for traffic analysis. GNNs can effectively model spatial correlations, making\nthem well-suited for analyzing complex transportation networks. As such, GNNs have garnered\nsignificant interest in the traffic domain for their ability to provide insights into traffic patterns and\nbehaviors [260].\nIn this section, we first conclude the main GNN research directions in the traffic domain, and\nthen we summarize the typical graph construction processes in different traffic scenes and datasets.\nFinally, we list the classical GNN workflows for dealing with tasks in traffic networks. A summary\nis provided in Table 13.\n14.1 Research Directions in Traffic Domain\nWe summarize main GNN research directions in the traffic domain as follows,\n\u2022 Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role in ITS [90, 381],\nwhich involves leveraging spatial-temporal data collected by various sensors to gain insights\ninto future traffic patterns and behaviors. Classic methods, like autoregressive integrated\nmoving average (ARIMA) [36], support vector machine (SVM) [171] and recurrent neural\nnetworks (RNN) [76] can only model time series separately without considering their spatial\nconnections. To address this issue, graph neural networks (GNNs) have emerged as a powerful\napproach for traffic forecasting due to their strong ability of modeling complex graph\u0002structured correlations [40, 202, 277, 353, 383, 506, 592].\n\u2022 Trajectory Prediction. Trajectory prediction is a crucial task in various applications, such\nas autonomous driving and traffic surveillance, which aims to forecast future positions of\nagents in the traffic scene. However, accurately predicting trajectories can be challenging, as\nthe behavior of an agent is influenced not only by its own motion but also by interactions\nwith surrounding objects. To address this challenge, Graph Neural Networks (GNNs) have\nemerged as a promising tool for modeling complex interactions in trajectory prediction\n[44, 345, 432, 600]. By representing the scene as a graph, where each node corresponds to an\nagent and the edges capture interactions between them, GNNs can effectively capture spatial\ndependencies and interactions between agents. This makes GNNs well-suited for predicting\ntrajectories that accurately capture the behavior of agents in complex traffic scenes.\n\u2022 Traffic Anomaly Detection. Anomaly detection is an essential support for ITS. There are\nlots of traffic anomalies in daily transportation systems, for example, traffic accidents, extreme\nweather and unexpected situations. Handling these traffic anomalies timely can improve the\nservice quality of public transportation. The main trouble of traffic anomaly detection is the\nhighly twisted spatial-temporal characteristics of traffic data. The criteria and influence of\ntraffic anomaly vary among locations and times. GNNs have been introduced and achieved\nsuccess in this domain [66, 85, 86, 565].\n\u2022 Others. Traffic demand prediction targets at estimating the future number of traveling at\nsome location. It is of vital and practical significance in the resource scheduling for ITS. By\nusing GNNs, the spatial dependencies of demands can be revealed [530, 535]. What is more,\nurban vehicle emission analysis is also considered in recent work, which is closely related to\nenvironmental protection and gains increasing researcher attention [521].\n14.2 Traffic Graph Construction\n14.2.1 Traffic Graph The traffic network is represented as a graph G = (\ud835\udc49 , \ud835\udc38, \ud835\udc34), where \ud835\udc49 is the\nset of \ud835\udc41 traffic nodes, \ud835\udc38 is the set of edges, and \ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is an adjacency matrix representing the\nconnectivity of \ud835\udc41 nodes. In the traffic domain, \ud835\udc49 usually represents a set of physical nodes, like\ntraffic stations or traffic sensors. The features of nodes typically depend on the specific task. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 63\nTable 13. Summary of graph models for traffic analysis.\nModels Tasks Adjcency matrices GNN types Temporal modules\nSTGCN[545] Traffic Flow Forecasting Fixed Matrix GCN TCN\nDCRNN[275] Traffic Flow Forecasting Fixed Matrix ChebNet RNN\nAGCRN [19] Traffic Flow Forecasting Dynamic Matrix GCN GRU\nASTGCN [159] Traffic Flow Forecasting Fixed Matrix GAT Attention&TCN\nGraphWaveNet [500] Traffic Flow Forecasting Dynamic Matrix GCN Gated-TCN\nSTSGCN [422] Traffic Flow Forecasting Dynamic Matrix GCN Cropping\nLSGCN [187] Traffic Flow Forecasting Fixed Matrix GAT GLU\nGAC-Net [424] Traffic Flow Forecasting Fixed Matrix GAT Gated-TCN\nSTGODE [112] Traffic Flow Forecasting Fixed Matrix Graph ODE TCN\nSTG-NCDE [70] Traffic Flow Forecasting Dynamic Matrix GCN NCDE\nDDGCRN [488] Traffic Flow Forecasting Dynamic Matrix GAT RNN\nMS-ASTN [467] OD Flow Forecasting OD Matrix GCN LSTM\nSocial-STGCNN [345] Trajectory Prediction Fixed Matrix GCN TXP-CNN\nRSBG [432] Trajectory Prediction Dynamic Matrix GCN LSTM\nATG [555] Trajectory Prediction Fixed Matrix GODE NODE\nSTGAN [86] Anomaly Detection Fixed Matrix GCN GRU\nDMVST-VGNN [206] Traffic Demand Prediction Fixed Matrix GAT GLU\nDST-GNN [185] Traffic Demand Prediction Dynamic Matrix GCN Transformer\nTC-SGC [355] Traffic Speed Prediction Fixed Matrix GCN GRU\ntraffic flow forecasting as an example. The features are the traffic flows, i.e., the historical time\nseries of nodes. The traffic flow can be represented as a flow matrix \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc47\n, where \ud835\udc41 is the\nnumber of traffic nodes and \ud835\udc47 is the length of historical series, and \ud835\udc4b\ud835\udc5b\ud835\udc61 denotes the traffic flow of\nnode \ud835\udc5b at time \ud835\udc61. The goal of traffic flow forecasting is to learn a mapping function \ud835\udc53 to predict the\ntraffic flow during future \ud835\udc47\n\u2032\nsteps given the historical \ud835\udc47 step information, which can be formulated\nas follows:\n\u0002\n\ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +1, \ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61; G\n\u0003 \ud835\udc53\n\u2212\u2192 \u0002\ud835\udc4b:,\ud835\udc61+1, \ud835\udc4b:,\ud835\udc61+2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61+\ud835\udc47\n\u2032\n\u0003\n. (162)\n14.2.2 Graph Construction Constructing a graph to describe the interactions among traffic nodes,\ni.e., the design of the adjacency matrix \ud835\udc34, is the key part of traffic analysis. The mainstream designs\ncan be divided into two categories, fixed matrix and dynamic matrix.\nFixed matrix. Lots of works assume that the correlations among traffic nodes are fixed and\nconstant over time, and they design a fixed and pre-defined adjacency matrix to capture the spatial\ncorrelation. Here we list several common choices of fixed adjacency matrix.\nThe connectivity matrix is the most natural construction way. It relies on the support of\nroad map data. The element of the connectivity matrix is defined as 1 if two nodes are physically\nconnected and 0 otherwise. This binary format is convenient to construct and easy to interpret.\nThe distance-based matrix is also a common choice, which shows the connection between two\nnodes more precisely. The elements of the matrix are defined as the function of distance between\ntwo nodes (driving distance or geographical distance). A typical way is to use the threshold Gaussian\nfunction as follows,\n\ud835\udc34\ud835\udc56\ud835\udc57 =\n(\nexp(\u2212\ud835\udc51\n2\n\ud835\udc56 \ud835\udc57\n\ud835\udf0e\n2 ), \ud835\udc51\ud835\udc56\ud835\udc57  \ud835\udf16\n, (163)\nwhere \ud835\udc51\ud835\udc56\ud835\udc57 is the distance between node \ud835\udc56 and \ud835\udc57, and \ud835\udf0e and \ud835\udf16 are two hyperparameters to control the\ndistribution and the sparsity of the matrix.\nAnother kind of fixed adjacency matrix is the similarity-based matrix. In fact, a similarity\nmatrix is not an adjacency matrix to some extent. It is constructed according to the similarity of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n64 W. Ju, et al.\ntwo nodes, which means the neighbors in the similarity graph may be far away in the real world.\nThere are various similarity metrics. For example, many works measure the similarity of two nodes\nby their functionality, e.g., the distribution of surrounding points of interest (POIs). The assumption\nbehind this is that nodes that share similar functionality may share similar traffic patterns. We\ncan also define the similarity through the historical flow patterns. To compute the similarity of\ntwo-time series, a common practice is to use Dynamic Time Wrapping (DTW) algorithm [350],\nwhich is superior to other metrics due to its sensitivity to shape similarity rather than point-wise\nsimilarity. Specifically, given two time series \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc5b) and \ud835\udc4c = (\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc5b), DTW is\na dynamic programming algorithm defined as\n\ud835\udc37(\ud835\udc56, \ud835\udc57) = \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) + min (\ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57), \ud835\udc37(\ud835\udc56, \ud835\udc57 \u2212 1), \ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57 \u2212 1)) , (164)\nwhere \ud835\udc37(\ud835\udc56, \ud835\udc57) represents the shortest distance between subseries \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc56) and \ud835\udc4c =\n(\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc57), and\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) is some distance metric like absolute distance. As a result,\ud835\udc37\ud835\udc47\ud835\udc4a (\ud835\udc4b, \ud835\udc4c) =\n\ud835\udc37(\ud835\udc5b, \ud835\udc5b) is set as the final distance between \ud835\udc4b and \ud835\udc4c, which better reflects the similarity of the\ntwo-time series compared to the Euclidean distance.\nDynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect complete\ninformation of spatial correlations. The dynamic adaptive matrix is proposed to solve the issue.\nThe dynamic matrix is learned from input data automatically. To achieve the best prediction\nperformance, the dynamic matrix will manage to infer the hidden correlations among nodes, more\nthan those physical connections.\nA typical practice is learning adjacency matrix from node embeddings [19]. Let \ud835\udc38\ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc51 be a\nlearnable node embedding dictionary, where each row of \ud835\udc38\ud835\udc34 represents the embedding of a node,\n\ud835\udc41 and \ud835\udc51 denote the number of nodes and the dimension of embeddings respectively. The graph\nadjacency matrix is defined as the similarities among node embeddings,\n\ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 = \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \u0010\n\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc38\ud835\udc34 \u00b7 \ud835\udc38\n\ud835\udc47\n\ud835\udc34\n)\n\u0011\n, (165)\nwhere \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 function is to perform row-normalization, and \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 is the Laplacian matrix.\n14.3 Typical GNN Frameworks in Traffic Domain\nSpatial Temporal Graph Convolution Network (STGCN) [545]. STGCN is a pioneering work in the\nspatial-temporal GNN domain. It utilizes graph convolution to capture spatial features, and deploys\na gated causal convolution to extract temporal patterns. Specifically, the graph convolution and\ntemporal convolution are defined as follows,\n\u0398 \u2217G \ud835\udc65 = \ud835\udf03 (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc65 = \ud835\udf03 (\ud835\udc37\u02dc \u2212\n1\n2\ud835\udc34\u02dc\ud835\udc37\u02dc \u2212\n1\n2 )\ud835\udc65, (166)\n\u0393 \u2217T \ud835\udc66 = \ud835\udc43 \u2299 \ud835\udf0e(\ud835\udc44), (167)\nwhere \u0398 is the parameter of graph convolution, \ud835\udc43 and \ud835\udc44 are the outputs of a 1-d convolution\nalong the temporal dimension. The sigmoid gate \ud835\udf0e(\ud835\udc44) controls how the states of \ud835\udc43 are relevant\nfor discovering hidden temporal patterns. In order to fuse features from both spatial and temporal\ndimension, the spatial convolution layer and the temporal convolution layer are combined to\nconstruct a spatial temporal block to jointly deal with graph-structured time series, and more blocks\ncan be stacked to achieve a more scalable and complex model.\nDiffusion Convolutional Recurrent Neural Network (DCRNN) [275]. DCRNN is a representative\nsolution combining graph convolution networks with recurrent neural networks. It captures spatial\ndependencies by bidirectional random walks on the graph. The diffusion convolution operation on\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 65\na graph is defined as:\n\ud835\udc4b \u2217G \ud835\udc53\ud835\udf03 =\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc58=0\n\u0010\n\ud835\udf03\ud835\udc58,1 (\ud835\udc37\n\u22121\n\ud835\udc42 \ud835\udc34)\n\ud835\udc58 + \ud835\udf03\ud835\udc58,2 (\ud835\udc37\u22121\n\ud835\udc3c \ud835\udc34)\n\ud835\udc58\n\u0011\n\ud835\udc4b, (168)\nwhere \ud835\udf03 are parameters for the convolution filter, and \ud835\udc37\n\u22121\n\ud835\udc42\n\ud835\udc34, \ud835\udc37\u22121\n\ud835\udc3c\n\ud835\udc34 represent the bidirectional\ndiffusion processes respectively. In term of temporal dependency, DCRNN utilizes Gated Recurrent\nUnits (GRU), and replace the linear transformation in the GRU with the diffusion convolution as\nfollows,\n\ud835\udc5f\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc5f \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc5f), (169)\n\ud835\udc62\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc62 \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc62), (170)\n\ud835\udc36\n(\ud835\udc61) = tanh(\u0398\ud835\udc36 \u2217G [\ud835\udc4b(\ud835\udc61)\n, (\ud835\udc5f\n(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121)\n] + \ud835\udc4f\ud835\udc50 ), (171)\n\ud835\udc3b\n(\ud835\udc61) = \ud835\udc62(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121) + (1 \u2212 \ud835\udc62(\ud835\udc61)\n) \u2299 \ud835\udc36\n(\ud835\udc61)\n, (172)\nwhere \ud835\udc4b\n(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61) denote the input and output at time \ud835\udc61, \ud835\udc5f\n(\ud835\udc61)\n, \ud835\udc62(\ud835\udc61)are the reset and update gates\nrespectively, and \u0398\ud835\udc5f, \u0398\ud835\udc62, \u0398\ud835\udc36 are parameters of convolution filters. Moreover, DCRNN employs a\nsequence-to-sequence architecture to predict future series. Both the encoder and the decoder are\nconstructed with diffusion convolutional recurrent layers. The historical time series are fed into\nthe encoder and the predictions are generated by the decoder. The scheduled sampling technique is\nutilized to solve the discrepancy problem between training and test distribution.\nAdaptive Graph Convolutional Recurrent Network (AGCRN) [19]. The focuses of AGCRN are\ntwo-fold. On the one hand, it argues that the temporal patterns are diversified and thus parameter\u0002sharing for each node is inferior; on the other hand, it proposes that the pre-defined graph may be\nintuitive and incomplete for the specific prediction task. To mitigate the two issues, it designs a\nNode Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic\nseries, and a Data Adaptive Graph Generation (DAGG) module to infer the hidden correlations\namong nodes from data and to generate the graph during training. Specifically, the NAPL module\nis defined as follows,\n\ud835\udc4d = (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc4b \ud835\udc38G\ud835\udc4aG + \ud835\udc38G\ud835\udc4f G, (173)\nwhere \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc36 is the input feature, \ud835\udc38G \u2208 R\ud835\udc41 \u00d7\ud835\udc51\nis a node embedding dictionary, \ud835\udc51 is the\nembedding dimension (\ud835\udc51",
    "openalex_id": "https://openalex.org/W4392203343",
    "title": "A Comprehensive Survey on Deep Graph Representation Learning",
    "publication_date": "2024-02-27",
    "cited_by_count": 55,
    "topics": "Graph Neural Network Models and Applications, Statistical Mechanics of Complex Networks, Recommender System Technologies",
    "keywords": "Feature learning, Representation Learning, Knowledge Graph Embedding, Signal Processing on Graphs, Network Embedding, Deep Learning, Graph embedding",
    "concepts": "Computer science, Deep learning, Feature learning, Artificial intelligence, Graph, Theoretical computer science, Categorization, Embedding, Graph embedding, Machine learning",
    "pdf_urls_by_priority": [
      "https://arxiv.org/pdf/2304.05055"
    ],
    "text_type": "full_text",
    "successful_pdf_url": "https://arxiv.org/pdf/2304.05055",
    "referenced_works": [
      "https://openalex.org/W103666676",
      "https://openalex.org/W1510073064",
      "https://openalex.org/W1520469672",
      "https://openalex.org/W1614298861",
      "https://openalex.org/W1662382123",
      "https://openalex.org/W1816257748",
      "https://openalex.org/W1888005072",
      "https://openalex.org/W1924770834",
      "https://openalex.org/W1954735160",
      "https://openalex.org/W1959608418",
      "https://openalex.org/W1991252559",
      "https://openalex.org/W1992787746",
      "https://openalex.org/W1993046136",
      "https://openalex.org/W1994389483",
      "https://openalex.org/W1996058270",
      "https://openalex.org/W2006698588",
      "https://openalex.org/W2008056655",
      "https://openalex.org/W2009233867",
      "https://openalex.org/W2022638422",
      "https://openalex.org/W2027482274",
      "https://openalex.org/W2042123098",
      "https://openalex.org/W2053186076",
      "https://openalex.org/W2054141820",
      "https://openalex.org/W2056609785",
      "https://openalex.org/W2062340319",
      "https://openalex.org/W2076498053",
      "https://openalex.org/W2090891622",
      "https://openalex.org/W2095932468",
      "https://openalex.org/W2100495367",
      "https://openalex.org/W2101491865",
      "https://openalex.org/W2110242546",
      "https://openalex.org/W2113052721",
      "https://openalex.org/W2114704115",
      "https://openalex.org/W2115627867",
      "https://openalex.org/W2116341502",
      "https://openalex.org/W2121406124",
      "https://openalex.org/W2128332575",
      "https://openalex.org/W2140310134",
      "https://openalex.org/W2142498761",
      "https://openalex.org/W2142535891",
      "https://openalex.org/W2145658888",
      "https://openalex.org/W2148950790",
      "https://openalex.org/W2152184085",
      "https://openalex.org/W2152630148",
      "https://openalex.org/W2152825437",
      "https://openalex.org/W2153959628",
      "https://openalex.org/W2156718197",
      "https://openalex.org/W2158787690",
      "https://openalex.org/W2170057991",
      "https://openalex.org/W2184148260",
      "https://openalex.org/W2262123273",
      "https://openalex.org/W2319902168",
      "https://openalex.org/W2338678442",
      "https://openalex.org/W2387462954",
      "https://openalex.org/W2393319904",
      "https://openalex.org/W2415243320",
      "https://openalex.org/W2461620095",
      "https://openalex.org/W2481151430",
      "https://openalex.org/W2488133945",
      "https://openalex.org/W2493343568",
      "https://openalex.org/W2509893387",
      "https://openalex.org/W2520633135",
      "https://openalex.org/W2529996553",
      "https://openalex.org/W2531327146",
      "https://openalex.org/W2550925836",
      "https://openalex.org/W2551706664",
      "https://openalex.org/W2565684601",
      "https://openalex.org/W2594183968",
      "https://openalex.org/W2594899909",
      "https://openalex.org/W2602753196",
      "https://openalex.org/W2604738573",
      "https://openalex.org/W2606202972",
      "https://openalex.org/W2612872092",
      "https://openalex.org/W2618530766",
      "https://openalex.org/W2624407581",
      "https://openalex.org/W2700550412",
      "https://openalex.org/W2735246657",
      "https://openalex.org/W2743104969",
      "https://openalex.org/W2743930630",
      "https://openalex.org/W2749279690",
      "https://openalex.org/W2754490690",
      "https://openalex.org/W2767094836",
      "https://openalex.org/W2767404761",
      "https://openalex.org/W2772486182",
      "https://openalex.org/W2773515559",
      "https://openalex.org/W2788134583",
      "https://openalex.org/W2788775653",
      "https://openalex.org/W2788919350",
      "https://openalex.org/W2793544332",
      "https://openalex.org/W2798621783",
      "https://openalex.org/W2800415562",
      "https://openalex.org/W2803526748",
      "https://openalex.org/W2809279178",
      "https://openalex.org/W2809307135",
      "https://openalex.org/W2809435178",
      "https://openalex.org/W2884209963",
      "https://openalex.org/W2888164077",
      "https://openalex.org/W2888192920",
      "https://openalex.org/W2889337896",
      "https://openalex.org/W2892880750",
      "https://openalex.org/W2895744665",
      "https://openalex.org/W2895884529",
      "https://openalex.org/W2896202861",
      "https://openalex.org/W2896457183",
      "https://openalex.org/W2897862648",
      "https://openalex.org/W2897978524",
      "https://openalex.org/W2901454299",
      "https://openalex.org/W2903871660",
      "https://openalex.org/W2905432015",
      "https://openalex.org/W2907230994",
      "https://openalex.org/W2907492528",
      "https://openalex.org/W2911286998",
      "https://openalex.org/W2912323206",
      "https://openalex.org/W2912351665",
      "https://openalex.org/W2913015533",
      "https://openalex.org/W2913350752",
      "https://openalex.org/W2913825337",
      "https://openalex.org/W2914989158",
      "https://openalex.org/W2916446912",
      "https://openalex.org/W2944538680",
      "https://openalex.org/W2948684689",
      "https://openalex.org/W2948729509",
      "https://openalex.org/W2949103145",
      "https://openalex.org/W2949208225",
      "https://openalex.org/W2949243165",
      "https://openalex.org/W2949865801",
      "https://openalex.org/W2950898568",
      "https://openalex.org/W2951659295",
      "https://openalex.org/W2951970475",
      "https://openalex.org/W2959300817",
      "https://openalex.org/W2962756421",
      "https://openalex.org/W2962810718",
      "https://openalex.org/W2963017945",
      "https://openalex.org/W2963066159",
      "https://openalex.org/W2963084622",
      "https://openalex.org/W2963224980",
      "https://openalex.org/W2963241951",
      "https://openalex.org/W2963341924",
      "https://openalex.org/W2963358464",
      "https://openalex.org/W2963410212",
      "https://openalex.org/W2963456618",
      "https://openalex.org/W2963521729",
      "https://openalex.org/W2963639956",
      "https://openalex.org/W2963664410",
      "https://openalex.org/W2963703618",
      "https://openalex.org/W2963726920",
      "https://openalex.org/W2963919031",
      "https://openalex.org/W2964015378",
      "https://openalex.org/W2964044287",
      "https://openalex.org/W2964051675",
      "https://openalex.org/W2964113829",
      "https://openalex.org/W2964321699",
      "https://openalex.org/W2964568038",
      "https://openalex.org/W2964583308",
      "https://openalex.org/W2964926209",
      "https://openalex.org/W2965341826",
      "https://openalex.org/W2965857891",
      "https://openalex.org/W2966357564",
      "https://openalex.org/W2966683369",
      "https://openalex.org/W2966841471",
      "https://openalex.org/W2971126534",
      "https://openalex.org/W2971220558",
      "https://openalex.org/W2979750740",
      "https://openalex.org/W2979845147",
      "https://openalex.org/W2981536126",
      "https://openalex.org/W2981790137",
      "https://openalex.org/W2982108874",
      "https://openalex.org/W2982880755",
      "https://openalex.org/W2986423110",
      "https://openalex.org/W2986466936",
      "https://openalex.org/W2986515219",
      "https://openalex.org/W2988115728",
      "https://openalex.org/W2989285747",
      "https://openalex.org/W2992586577",
      "https://openalex.org/W2992613109",
      "https://openalex.org/W2994860160",
      "https://openalex.org/W2996635575",
      "https://openalex.org/W2996847713",
      "https://openalex.org/W2996910665",
      "https://openalex.org/W2997128522",
      "https://openalex.org/W2997574889",
      "https://openalex.org/W2997785591",
      "https://openalex.org/W2997997679",
      "https://openalex.org/W2998004401",
      "https://openalex.org/W2998122931",
      "https://openalex.org/W2998496395",
      "https://openalex.org/W2998604091",
      "https://openalex.org/W2998702685",
      "https://openalex.org/W3000301417",
      "https://openalex.org/W3000478925",
      "https://openalex.org/W3000577518",
      "https://openalex.org/W3000716014",
      "https://openalex.org/W3005552578",
      "https://openalex.org/W3007488165",
      "https://openalex.org/W3008194092",
      "https://openalex.org/W3011358689",
      "https://openalex.org/W3011667710",
      "https://openalex.org/W3012123536",
      "https://openalex.org/W3012871709",
      "https://openalex.org/W3013107657",
      "https://openalex.org/W3013888836",
      "https://openalex.org/W3016124664",
      "https://openalex.org/W3016427665",
      "https://openalex.org/W3025863369",
      "https://openalex.org/W3026887460",
      "https://openalex.org/W3031353169",
      "https://openalex.org/W3033039844",
      "https://openalex.org/W3033706928",
      "https://openalex.org/W3034231628",
      "https://openalex.org/W3034329572",
      "https://openalex.org/W3035060554",
      "https://openalex.org/W3035096461",
      "https://openalex.org/W3035237749",
      "https://openalex.org/W3035285524",
      "https://openalex.org/W3035523484",
      "https://openalex.org/W3035580605",
      "https://openalex.org/W3035649237",
      "https://openalex.org/W3035664258",
      "https://openalex.org/W3035666843",
      "https://openalex.org/W3035702572",
      "https://openalex.org/W3035740499",
      "https://openalex.org/W3036106327",
      "https://openalex.org/W3036167779",
      "https://openalex.org/W3036974265",
      "https://openalex.org/W3038719422",
      "https://openalex.org/W3038981236",
      "https://openalex.org/W3042918615",
      "https://openalex.org/W3044189835",
      "https://openalex.org/W3045200674",
      "https://openalex.org/W3045662942",
      "https://openalex.org/W3045928028",
      "https://openalex.org/W3046470859",
      "https://openalex.org/W3048817558",
      "https://openalex.org/W3080566854",
      "https://openalex.org/W3080997787",
      "https://openalex.org/W3081203761",
      "https://openalex.org/W3081325717",
      "https://openalex.org/W3081836708",
      "https://openalex.org/W3082154031",
      "https://openalex.org/W3082411326",
      "https://openalex.org/W3087318471",
      "https://openalex.org/W3092339997",
      "https://openalex.org/W3092462694",
      "https://openalex.org/W3093218977",
      "https://openalex.org/W3093687066",
      "https://openalex.org/W3094231942",
      "https://openalex.org/W3094500523",
      "https://openalex.org/W3095448863",
      "https://openalex.org/W3096831136",
      "https://openalex.org/W3098465726",
      "https://openalex.org/W3098797593",
      "https://openalex.org/W3099152386",
      "https://openalex.org/W3099414221",
      "https://openalex.org/W3100078588",
      "https://openalex.org/W3100278010",
      "https://openalex.org/W3100324210",
      "https://openalex.org/W3101707147",
      "https://openalex.org/W3102554291",
      "https://openalex.org/W3103523530",
      "https://openalex.org/W3103720336",
      "https://openalex.org/W3103736477",
      "https://openalex.org/W3104097132",
      "https://openalex.org/W3104644561",
      "https://openalex.org/W3104667978",
      "https://openalex.org/W3105259638",
      "https://openalex.org/W3105423481",
      "https://openalex.org/W3108202858",
      "https://openalex.org/W3108433857",
      "https://openalex.org/W3110901318",
      "https://openalex.org/W3111430045",
      "https://openalex.org/W3113177135",
      "https://openalex.org/W3114613321",
      "https://openalex.org/W3116239416",
      "https://openalex.org/W3117178429",
      "https://openalex.org/W3120567415",
      "https://openalex.org/W3122934853",
      "https://openalex.org/W3123909522",
      "https://openalex.org/W3124962940",
      "https://openalex.org/W3128443161",
      "https://openalex.org/W3129850062",
      "https://openalex.org/W3133780103",
      "https://openalex.org/W3134509497",
      "https://openalex.org/W3135205495",
      "https://openalex.org/W3135389928",
      "https://openalex.org/W3136999308",
      "https://openalex.org/W3137385578",
      "https://openalex.org/W3137928916",
      "https://openalex.org/W3138516171",
      "https://openalex.org/W3148711710",
      "https://openalex.org/W3151900735",
      "https://openalex.org/W3152893301",
      "https://openalex.org/W3154503084",
      "https://openalex.org/W3155056342",
      "https://openalex.org/W3155322940",
      "https://openalex.org/W3155577228",
      "https://openalex.org/W3156642753",
      "https://openalex.org/W3157039246",
      "https://openalex.org/W3157999218",
      "https://openalex.org/W3158827677",
      "https://openalex.org/W3160021293",
      "https://openalex.org/W3163426640",
      "https://openalex.org/W3164446335",
      "https://openalex.org/W3165171933",
      "https://openalex.org/W3165369424",
      "https://openalex.org/W3165924303",
      "https://openalex.org/W3167334189",
      "https://openalex.org/W3168436232",
      "https://openalex.org/W3169168872",
      "https://openalex.org/W3169450514",
      "https://openalex.org/W3169933688",
      "https://openalex.org/W3171581326",
      "https://openalex.org/W3171764584",
      "https://openalex.org/W3174163042",
      "https://openalex.org/W3174174150",
      "https://openalex.org/W3174823757",
      "https://openalex.org/W3175925542",
      "https://openalex.org/W3175971420",
      "https://openalex.org/W3176393519",
      "https://openalex.org/W3176806965",
      "https://openalex.org/W3176890989",
      "https://openalex.org/W3181414820",
      "https://openalex.org/W3184127157",
      "https://openalex.org/W3187985423",
      "https://openalex.org/W3190664711",
      "https://openalex.org/W3191962800",
      "https://openalex.org/W3192448376",
      "https://openalex.org/W3193553875",
      "https://openalex.org/W3194668998",
      "https://openalex.org/W3200806939",
      "https://openalex.org/W3201058350",
      "https://openalex.org/W3201249640",
      "https://openalex.org/W3204651332",
      "https://openalex.org/W3205227354",
      "https://openalex.org/W3206171352",
      "https://openalex.org/W3208638341",
      "https://openalex.org/W3209048663",
      "https://openalex.org/W3209056694",
      "https://openalex.org/W3209451568",
      "https://openalex.org/W3209764902",
      "https://openalex.org/W3210482950",
      "https://openalex.org/W3210611486",
      "https://openalex.org/W3210987203",
      "https://openalex.org/W3211394146",
      "https://openalex.org/W3211477647",
      "https://openalex.org/W3211973371",
      "https://openalex.org/W3213940558",
      "https://openalex.org/W3214642103",
      "https://openalex.org/W3214674106",
      "https://openalex.org/W3214872094",
      "https://openalex.org/W3215452784",
      "https://openalex.org/W4200635484",
      "https://openalex.org/W4205247958",
      "https://openalex.org/W4206174637",
      "https://openalex.org/W4206357214",
      "https://openalex.org/W4206445139",
      "https://openalex.org/W4206776774",
      "https://openalex.org/W4212805305",
      "https://openalex.org/W4213052788",
      "https://openalex.org/W4213147383",
      "https://openalex.org/W4213457653",
      "https://openalex.org/W4214868967",
      "https://openalex.org/W4220742022",
      "https://openalex.org/W4220933119",
      "https://openalex.org/W4221023051",
      "https://openalex.org/W4221138292",
      "https://openalex.org/W4221149947",
      "https://openalex.org/W4221155201",
      "https://openalex.org/W4221157965",
      "https://openalex.org/W4224309748",
      "https://openalex.org/W4224311348",
      "https://openalex.org/W4224311800",
      "https://openalex.org/W4224983022",
      "https://openalex.org/W4225090121",
      "https://openalex.org/W4225338086",
      "https://openalex.org/W4225405705",
      "https://openalex.org/W4225512856",
      "https://openalex.org/W4225596872",
      "https://openalex.org/W4225977739",
      "https://openalex.org/W4226058932",
      "https://openalex.org/W4226060238",
      "https://openalex.org/W4226208698",
      "https://openalex.org/W4229053887",
      "https://openalex.org/W4234842379",
      "https://openalex.org/W4239789016",
      "https://openalex.org/W4240185200",
      "https://openalex.org/W4240592325",
      "https://openalex.org/W4243799827",
      "https://openalex.org/W4246587917",
      "https://openalex.org/W4255866863",
      "https://openalex.org/W4280535976",
      "https://openalex.org/W4281387042",
      "https://openalex.org/W4281563651",
      "https://openalex.org/W4282913028",
      "https://openalex.org/W4282943426",
      "https://openalex.org/W4283121576",
      "https://openalex.org/W4283218438",
      "https://openalex.org/W4283462727",
      "https://openalex.org/W4283798273",
      "https://openalex.org/W4283810298",
      "https://openalex.org/W4283817628",
      "https://openalex.org/W4284666445",
      "https://openalex.org/W4284698122",
      "https://openalex.org/W4285428788",
      "https://openalex.org/W4286588524",
      "https://openalex.org/W4286795917",
      "https://openalex.org/W4286893581",
      "https://openalex.org/W4287123803",
      "https://openalex.org/W4287325738",
      "https://openalex.org/W4287780403",
      "https://openalex.org/W4287863694",
      "https://openalex.org/W4287991183",
      "https://openalex.org/W4287998109",
      "https://openalex.org/W4288052590",
      "https://openalex.org/W4288088467",
      "https://openalex.org/W4288346884",
      "https://openalex.org/W4289389616",
      "https://openalex.org/W4289533979",
      "https://openalex.org/W4289537189",
      "https://openalex.org/W4290648792",
      "https://openalex.org/W4290875097",
      "https://openalex.org/W4290877962",
      "https://openalex.org/W4293112739",
      "https://openalex.org/W4293370878",
      "https://openalex.org/W4293821372",
      "https://openalex.org/W4294170691",
      "https://openalex.org/W4294435970",
      "https://openalex.org/W4294558607",
      "https://openalex.org/W4295097398",
      "https://openalex.org/W4295728955",
      "https://openalex.org/W4295846611",
      "https://openalex.org/W4296047560",
      "https://openalex.org/W4296143727",
      "https://openalex.org/W4296185888",
      "https://openalex.org/W4297510052",
      "https://openalex.org/W4297733535",
      "https://openalex.org/W4297791874",
      "https://openalex.org/W4297946153",
      "https://openalex.org/W4297951436",
      "https://openalex.org/W4297999768",
      "https://openalex.org/W4298052734",
      "https://openalex.org/W4298312696",
      "https://openalex.org/W4301329292",
      "https://openalex.org/W4304097971",
      "https://openalex.org/W4306887124",
      "https://openalex.org/W4307416138",
      "https://openalex.org/W4308505492",
      "https://openalex.org/W4309635196",
      "https://openalex.org/W4309801650",
      "https://openalex.org/W4310012576",
      "https://openalex.org/W4311216457",
      "https://openalex.org/W4312126067",
      "https://openalex.org/W4312689497",
      "https://openalex.org/W4313201684",
      "https://openalex.org/W4315708854",
      "https://openalex.org/W4316495377",
      "https://openalex.org/W4317951161",
      "https://openalex.org/W4318150241",
      "https://openalex.org/W4318347779",
      "https://openalex.org/W4318540750",
      "https://openalex.org/W4318812521",
      "https://openalex.org/W4320814985",
      "https://openalex.org/W4321227311",
      "https://openalex.org/W4321367323",
      "https://openalex.org/W4321479940",
      "https://openalex.org/W4321480027",
      "https://openalex.org/W4321480031",
      "https://openalex.org/W4322614756",
      "https://openalex.org/W4322824839",
      "https://openalex.org/W4323650483",
      "https://openalex.org/W4327525152",
      "https://openalex.org/W4361247736",
      "https://openalex.org/W4362682267",
      "https://openalex.org/W4362714312",
      "https://openalex.org/W4366001157",
      "https://openalex.org/W4366198975",
      "https://openalex.org/W4366975604",
      "https://openalex.org/W4366986925",
      "https://openalex.org/W4367047082",
      "https://openalex.org/W4367047244",
      "https://openalex.org/W4367047306",
      "https://openalex.org/W4367060955",
      "https://openalex.org/W4367595602",
      "https://openalex.org/W4376121360",
      "https://openalex.org/W4378696994",
      "https://openalex.org/W4378766933",
      "https://openalex.org/W4378909195",
      "https://openalex.org/W4378976562",
      "https://openalex.org/W4379185762",
      "https://openalex.org/W4379506768",
      "https://openalex.org/W4380091056",
      "https://openalex.org/W4381573056",
      "https://openalex.org/W4381679608",
      "https://openalex.org/W4382202967",
      "https://openalex.org/W4382239955",
      "https://openalex.org/W4382240004",
      "https://openalex.org/W4382317956",
      "https://openalex.org/W4382468395",
      "https://openalex.org/W4383604692",
      "https://openalex.org/W4384895066",
      "https://openalex.org/W4384915287",
      "https://openalex.org/W4385245566",
      "https://openalex.org/W4385270450",
      "https://openalex.org/W4385568380",
      "https://openalex.org/W4386445678",
      "https://openalex.org/W4386494629",
      "https://openalex.org/W4386587901",
      "https://openalex.org/W4386620158",
      "https://openalex.org/W4386711666",
      "https://openalex.org/W4386746103",
      "https://openalex.org/W4387092606",
      "https://openalex.org/W4387969028",
      "https://openalex.org/W4388185880",
      "https://openalex.org/W4388537645",
      "https://openalex.org/W4388684696",
      "https://openalex.org/W4390534655",
      "https://openalex.org/W4390572965",
      "https://openalex.org/W4391345293",
      "https://openalex.org/W4391561379",
      "https://openalex.org/W637153065"
    ],
    "openalex_rank": 2,
    "num_tokens": 42218,
    "url": "https://arxiv.org/pdf/2304.05055",
    "best_oa_location_pdf_url": "https://arxiv.org/pdf/2304.05055",
    "question_generated": "What are the key advancements in deep graph representation learning, what challenges do researchers currently face in this field, and what future directions could enhance the effectiveness of these methods in various applications?",
    "pipeline_answer": "**Key Advancements in Deep Graph Representation Learning:**  \n- Recent developments include various graph neural network architectures, such as Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), which enhance the ability to capture complex relationships in graph-structured data [S6962542142].  \n\n**Current Challenges:**  \n- Researchers face challenges such as the need for scalable methods that can efficiently process large graphs [S1364293428].  \n- There is also difficulty in learning from sparse or noisy data, which affects model performance [S1364293428].  \n- Existing models have limitations in handling dynamic graphs, which poses a significant hurdle [S1364293428].  \n\n**Future Directions:**  \n- Future enhancements may involve integrating self-supervised learning techniques to improve model robustness and adaptability [S9248989287].  \n- Exploring heterogeneous graph structures is another promising direction that could enhance the effectiveness of these methods [S9248989287].  \n\n**Applications of Graph Representation Learning:**  \n- Graph representation learning has shown significant promise in various applications, including social network analysis, molecular property prediction, and recommendation systems, demonstrating its versatility across different domains [S6564751247].",
    "pipeline_references": {
      "S6962542142": {
        "id": "S6962542142",
        "text": "Recent advancements in deep graph representation learning include the development of various graph neural network architectures, such as Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), which enhance the ability to capture complex relationships in graph-structured data.",
        "children": [
          {
            "id": "E5304604864",
            "text": "A Comprehensive Survey on Deep Graph Representation Learning WEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University, China ZIYUE QIAO, The Hong Kong University of Science and Technology, China YIFANG QIN and JIANHAO SHEN, Peking University, China FANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA JUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China YIFAN WANG, University of International Business and Economics, China XIAO LUO\u2217, University of California, Los Angeles, USA MING ZHANG\u2217, Peking University, China Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations. \u2217Corresponding authors. Authors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn; Zequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871; Ziyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453; Yifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang Sun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei Yang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking University, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics, Beijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang, mzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2024 Association for Computing Machinery. 0004-5411/2024/2-ART $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. arXiv:2304.05055v3 [cs.LG] 28 Feb 2024 2 W. Ju, et al. Additional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network, Survey ACM Reference Format: Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024. A Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Graphs have recently emerged as a powerful tool for representing a variety of structured and complex data, including social networks, traffic networks, information systems, knowledge graphs, protein-protein interaction networks, and physical interaction networks. As a kind of general form of data organization, graph structures are capable of naturally expressing the intrinsic relationship of these data, and thus can characterize plenty of non-Euclidean structures that are crucial in a variety of disciplines and domains due to their flexible adaptability. For example, to encode a social network as a graph, nodes on the graph are used to represent individual users, and edges are used to represent the relationship between two individuals, such as friends. In the field of biology, nodes can be used to represent proteins, and edges can be used to represent biological interactions between various proteins, such as the dynamic interactions between proteins. Thus, by analyzing and mining the graph-structured data, we can understand the deep meaning hidden behind the data, and further discover valuable knowledge, so as to benefit society and human beings. In the last decade years, a wide range of machine learning algorithms have been developed for graph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410] usually break down graphs into different atomic substructures and then use kernel functions to measure the similarity between all pairs of them. Although graph kernels could provide a perspective on modeling graph topology, these approaches often generate substructures or feature representations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer from high computational complexity, and therefore have weak scalability and subpar performance. In the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have everincreasing emerged, which attempt to encode the structural information of the graph (usually a high-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to preserve the topology information and attribute information in the embedding space as much as possible, so that the learned graph embeddings can be naturally integrated into traditional machine learning algorithms. Compared to previous works which use feature engineering in the pre-processing phase to extract graph structural features, current graph embedding algorithms are conducted in a data-driven way leveraging machine learning algorithms (such as neural networks) to encode the structural information of the graph. Specifically, existing graph embedding methods can be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354] that factorize the matrix to learn node embedding which preserves the graph property; (ii) deep learning based methods [155, 362, 443, 460] that apply deep learning techniques specifically designed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either maximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these methods typically depend on shallow architectures, and fail to exploit the potential and capacity of deep neural networks, resulting in sub-optimal representation quality and learning performance. Inspired by the recent remarkable success of deep neural networks, a range of deep learning algorithms has been developed for graph-structured data learning. The core of these methods is to generate effective node and graph representations using graph neural networks (GNNs), followed by a goal-oriented learning paradigm. In this way, the derived representations can be adaptively J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 3 coupled with a variety of downstream tasks and applications. Following this line of thought, in this paper, we propose a new taxonomy to classify the existing graph representation learning algorithms, i.e., graph neural network architectures, learning paradigms, and various promising applications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the studies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer. For the learning paradigms, we explore three advanced types namely supervised/semi-supervised learning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate the effectiveness of the learned graph representations, we provide several promising applications to build tight connections between representation learning and downstream tasks, such as social analysis, molecular property prediction and generation, recommender systems, and traffic analysis. Last but not least, we present some perspectives for thought and suggest challenging directions that deserve further study in the future. Differences between this survey and existing ones. Up to now, there exist some other overview papers focusing on different perspectives of graph representation learning[17, 50, 53..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E3890607053",
            "text": "..naturally integrated into traditional machine learning algorithms. Compared to previous works which use feature engineering in the pre-processing phase to extract graph structural features, current graph embedding algorithms are conducted in a data-driven way leveraging machine learning algorithms (such as neural networks) to encode the structural information of the graph. Specifically, existing graph embedding methods can be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354] that factorize the matrix to learn node embedding which preserves the graph property; (ii) deep learning based methods [155, 362, 443, 460] that apply deep learning techniques specifically designed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either maximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these methods typically depend on shallow architectures, and fail to exploit the potential and capacity of deep neural networks, resulting in sub-optimal representation quality and learning performance. Inspired by the recent remarkable success of deep neural networks, a range of deep learning algorithms has been developed for graph-structured data learning. The core of these methods is to generate effective node and graph representations using graph neural networks (GNNs), followed by a goal-oriented learning paradigm. In this way, the derived representations can be adaptively J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 3 coupled with a variety of downstream tasks and applications. Following this line of thought, in this paper, we propose a new taxonomy to classify the existing graph representation learning algorithms, i.e., graph neural network architectures, learning paradigms, and various promising applications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the studies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer. For the learning paradigms, we explore three advanced types namely supervised/semi-supervised learning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate the effectiveness of the learned graph representations, we provide several promising applications to build tight connections between representation learning and downstream tasks, such as social analysis, molecular property prediction and generation, recommender systems, and traffic analysis. Last but not least, we present some perspectives for thought and suggest challenging directions that deserve further study in the future. Differences between this survey and existing ones. Up to now, there exist some other overview papers focusing on different perspectives of graph representation learning[17, 50, 53, 57, 227, 499, 502, 577, 601, 603] that are closely related to ours. However, there are very few comprehensive reviews have summarized deep graph representation learning simultaneously from the perspective of diverse GNN architectures and corresponding up-to-date learning paradigms. Therefore, we here clearly state their distinctions from our survey as follows. There have been several surveys on classic graph embedding[42, 151], these works categorize graph embedding methods based on different training objectives. Wang et al. [468] goes further and provides a comprehensive review of existing heterogeneous graph embedding approaches. With the rapid development of deep learning, there are a handful of surveys along this line. For example, Wu et al. [499] and Zhang et al. [577] mainly focus on several classical and representative GNN architectures without exploring deep graph representation learning from a view of the most recent advanced learning paradigms such as graph self-supervised learning and graph structure learning. Xia et al. [502] and Chami et al. [50] jointly summarize the studies of graph embeddings and GNNs. Zhou et al. [601] explores different types of computational modules for GNNs. One recent survey under review [227] categorizes the existing works in graph representation learning from both static and dynamic graphs. However, these taxonomies emphasize the basic GNN methods but pay insufficient attention to the learning paradigms, and provide few discussions of the most promising applications, such as recommender systems as well as molecular property prediction and generation. To the best of our knowledge, the most relevant survey published formally is [603], which presents a review of GNN architectures and roughly discusses the corresponding applications. Nevertheless, this survey merely covers methods up to the year of 2020, missing the latest developments in the past three years. Therefore, it is highly desired to summarize the representative GNN methods, the most recent advanced learning paradigms, and promising applications into one unified and comprehensive framework. Moreover, we strongly believe this survey with a new taxonomy of literature and more than 600 studies will strengthen future research on deep graph representation learning. Contribution of this survey. The goal of this survey is to systematically review the literature on the advances of deep graph representation learning and discuss further directions. It aims to help the researchers and practitioners who are interested in this area, and support them in understanding the panorama and the latest developments of deep graph representation learning. The key contributions of this survey are summarized as follows: \u2022 Systematic Taxonomy. We propose a systematic taxonomy to organize the existing deep graph representation learning approaches based on the ways of GNN architectures and the most recent advanced learning paradigms via providing some representative branches of J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 4 W. Ju, et al. Graph Self-Supervised Learning Semi-Supervised Learning on Graphs Graph Structure Learning Learning Paradigms Graph-Related Applications Molecular Generation Molecular Property Prediction Social Analysis Recommender Systems Tra c Analysis Future Directions Graph Neural Network Architectures Graph Kernel Neural Networks Graph Pooling Graph Convolutions Graph Transformer Graph Representations Graph Data Optimized Graph Representations Fig. 1. The architecture of this paper. methods. Moreover, several promising applications are presented to illustrate the superiority and potential of graph representation learning. \u2022 Comprehensive Review. For each branch of this survey, we review the essential components and provide detailed descriptions of representative algorithms, and systematically summarize the characteristics to make the overview comparison. \u2022 Future Directions. Based on the properties of existing deep graph representation learning algorithms, we discuss the limitations and challenges of current methods and propose the potential as well as promising research directions deserving of future investigations. 2 Background In this section, we first briefly introduce some definitions in deep graph representation learning that need to be clarified, and then we explain the reasons why we need graph representation learning. 2.1 Problem Definition Definition: Graph. Given a graph G = (V , E, X), where V = {v1, \u00b7 \u00b7 \u00b7 , v|V | } is the set of nodes, E = {e1, \u00b7 \u00b7 \u00b7 , e|V | } is the set of edges, and the edge e = (vi, vj) \u2208 E represent the connection relationship between nodes vi and vjin the graph. X \u2208 R |V |\u00d7M is the node feature matrix with M being the dimension of each node feature. The adjacency matrix of a graph can be defined as A \u2208 R |V |\u00d7 |V | , where Aij = 1 if (vi, vj) \u2208 E, otherwise Aij = 0. The adjacency matrix can be regarded as the structural representation of the graph-structured data, in which each row of the adjacency matrix A represents the connection relationship between the corresponding node of the row and all other nodes, which can be regarded as a discrete representation of the node. However, in real-life circumstances, the adjacency matrix A corresponding to G is a highly sparse matrix, and if A is used directly as node representations, it will be seriously affected by impractical storage demands and computational overhead. The storage space of the adjacency matrix A is |V |\u00d7 |V |, which is usually unacceptable when the total number of nodes grows to the order of millions. At the same time, the value of most dimensions in the node representation is 0. The sparsity will make subsequent machine learning tasks very difficult. Graph representation learning is a bridge between the original input data and the task objectives in the graph. The fundamental idea of the graph representation learning algorithm is first to learn the embedded representations of nodes or the entire graph from the input graph structure data and then apply these embedded representations to downstream related tasks, such as node classification, graph classification, link prediction, community detection, and visualization, etc. Specifically, it aims to learn low-dimensional, dense distributed embedding representations for nodes in the graph. Formally, the goal of graph representation learning is to learn its embedding vector representation J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 5 Table 1. Summary of traditional graph embedding methods. Type Method Similarity measure Loss function (L) Matrix Factorization LLE [390] general |zi \u2212 \u00cd j \u2208Ni Wijzj | 2 LE [11] general Z T LZ,s.t.ZTDZ = I GF [4] Ai,j |Wi,j \u2212 \u27e8zi, zj\u27e9|2 GraRep [46] Ai,j, A2 i,j, ..., Ak i,j |Wi,j \u2212 \u27e8zi , zj\u27e9|2 HOPE [354] general |Wi,j \u2212 \u27e8zi, zj\u27e9|2 Random Walk DeepWalk [362] p(vi|vi) \u2212Aij log\u27e8zi, zj\u27e9 Node2vec [155] p(vi|vi) (biased) \u2212Aij log\u27e8zi, zj\u27e9 HARP [59] p(vi|vi) (biased) \u2212Aij log\u27e8zi, zj\u27e9 LINE [443] Two-order Similarities Corresponding Loss Non-GNN Deep SDNE [460] Two-order Proximities Corresponding Loss DNGR [47] Two-order Proximities Corresponding Loss Rv \u2208 R d for each node v \u2208 V , where the dimension d of the vector is much smaller than the total number of nodes |V | in the graph. 2.2 Traditional Graph Embedding Traditional graph embedding learning methods, as part of dimensionality..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      },
      "S9248989287": {
        "id": "S9248989287",
        "text": "Future directions for enhancing deep graph representation learning include the integration of self-supervised learning techniques and the exploration of heterogeneous graph structures to improve model robustness and adaptability.",
        "children": [
          {
            "id": "E5304604864",
            "text": "A Comprehensive Survey on Deep Graph Representation Learning WEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University, China ZIYUE QIAO, The Hong Kong University of Science and Technology, China YIFANG QIN and JIANHAO SHEN, Peking University, China FANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA JUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China YIFAN WANG, University of International Business and Economics, China XIAO LUO\u2217, University of California, Los Angeles, USA MING ZHANG\u2217, Peking University, China Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations. \u2217Corresponding authors. Authors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn; Zequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871; Ziyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453; Yifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang Sun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei Yang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking University, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics, Beijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang, mzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2024 Association for Computing Machinery. 0004-5411/2024/2-ART $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. arXiv:2304.05055v3 [cs.LG] 28 Feb 2024 2 W. Ju, et al. Additional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network, Survey ACM Reference Format: Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024. A Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Graphs have recently emerged as a powerful tool for representing a variety of structured and complex data, including social networks, traffic networks, information systems, knowledge graphs, protein-protein interaction networks, and physical interaction networks. As a kind of general form of data organization, graph structures are capable of naturally expressing the intrinsic relationship of these data, and thus can characterize plenty of non-Euclidean structures that are crucial in a variety of disciplines and domains due to their flexible adaptability. For example, to encode a social network as a graph, nodes on the graph are used to represent individual users, and edges are used to represent the relationship between two individuals, such as friends. In the field of biology, nodes can be used to represent proteins, and edges can be used to represent biological interactions between various proteins, such as the dynamic interactions between proteins. Thus, by analyzing and mining the graph-structured data, we can understand the deep meaning hidden behind the data, and further discover valuable knowledge, so as to benefit society and human beings. In the last decade years, a wide range of machine learning algorithms have been developed for graph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410] usually break down graphs into different atomic substructures and then use kernel functions to measure the similarity between all pairs of them. Although graph kernels could provide a perspective on modeling graph topology, these approaches often generate substructures or feature representations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer from high computational complexity, and therefore have weak scalability and subpar performance. In the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have everincreasing emerged, which attempt to encode the structural information of the graph (usually a high-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to preserve the topology information and attribute information in the embedding space as much as possible, so that the learned graph embeddings can be naturally integrated into traditional machine learning algorithms. Compared to previous works which use feature engineering in the pre-processing phase to extract graph structural features, current graph embedding algorithms are conducted in a data-driven way leveraging machine learning algorithms (such as neural networks) to encode the structural information of the graph. Specifically, existing graph embedding methods can be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354] that factorize the matrix to learn node embedding which preserves the graph property; (ii) deep learning based methods [155, 362, 443, 460] that apply deep learning techniques specifically designed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either maximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these methods typically depend on shallow architectures, and fail to exploit the potential and capacity of deep neural networks, resulting in sub-optimal representation quality and learning performance. Inspired by the recent remarkable success of deep neural networks, a range of deep learning algorithms has been developed for graph-structured data learning. The core of these methods is to generate effective node and graph representations using graph neural networks (GNNs), followed by a goal-oriented learning paradigm. In this way, the derived representations can be adaptively J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 3 coupled with a variety of downstream tasks and applications. Following this line of thought, in this paper, we propose a new taxonomy to classify the existing graph representation learning algorithms, i.e., graph neural network architectures, learning paradigms, and various promising applications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the studies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer. For the learning paradigms, we explore three advanced types namely supervised/semi-supervised learning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate the effectiveness of the learned graph representations, we provide several promising applications to build tight connections between representation learning and downstream tasks, such as social analysis, molecular property prediction and generation, recommender systems, and traffic analysis. Last but not least, we present some perspectives for thought and suggest challenging directions that deserve further study in the future. Differences between this survey and existing ones. Up to now, there exist some other overview papers focusing on different perspectives of graph representation learning[17, 50, 53..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E5615187824",
            "text": "..works tend to use the autoregressive flow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439]. Diffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion steps to slowly add random noise to data x0 \u223c q(x): q(xt|xt\u22121) = N (xt; \u221a 1 \u2212 \u03b2txt\u22121, \u03b2t I), (137) q(x1:T |x0) = \u00d6 T t=1 q(xt|xt\u22121). (138) J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 55 Table 11. Summary of molecular generation models. Model 2D/3D Bindingbased FragmentbasedGNN Backbone Generative Model GCPN [543] 2D GCN [230] GAN MolGAN [82] 2D R-GCN [401] GAN DEFactor [16] 2D GCN GAN GraphVAE [419] 2D ECC [418] VAE MDVAE [100] 2D GGNN [274] VAE JT-VAE [207] 2D \u2713 MPNN [147] VAE CGVAE [290] 2D GGNN VAE DeepScaffold [270] 2D \u2713 GCN VAE GraphNVP [329] 2D R-GCN NF MoFlow [557] 2D R-GCN NF GraphAF [413] 2D R-GCN NF + AR GraphDF [318] 2D R-GCN NF + AR L-Net [272] 3D \u2713 g-U-Net [133] AR G-SchNet [142] 3D SchNet [405] AR GEN3D [388] 3D EGNN [398] AR G-SphereNet [316] 3D SphereNet [292] NF + AR EDM [178] 3D EGNN DM GCDM [347] 3D GCPNet [346] DM 3D-SBDD [306] 3D \u2713 SchNet AR Pocket2Mol [361] 3D \u2713 GVP [211] AR FLAG [581] 3D \u2713 \u2713 SchNet AR GraphBP [288] 3D \u2713 SchNet NF + AR SiamFlow [439] 3D \u2713 R-GCN NF DiffBP [282] 3D \u2713 EGNN DM DiffSBDD [403] 3D \u2713 EGNN DM TargetDiff [156] 3D \u2713 EGNN DM FragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR GraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR MolCode [579] 2D + 3D \u2713 EGNN NF + AR They then learn to reverse the diffusion process to construct desired data samples from the noise: p\u03b8 (x0:T ) = p(xT ) \u00d6 T t=1 p\u03b8 (xt\u22121 |xt), (139) p\u03b8 (xt\u22121 |xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3\u03b8 (xt, t)), (140) while the models are trained using a variational lower bound. Diffusion models have been applied to generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands in DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate molecular fragments in autoregressive models, as is the case with FragDiff [360]. 12.3 Summary and prospects We wrap up this chapter with Table 11, which profiles existing molecular generation models according to their taxonomy for molecular featurization, the GNN backbone, and the generative method. This chapter covers the critical topics of molecular generation, which also elicit valuable insights into the promising directions for future research. We summarize these important aspects as follows. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 56 W. Ju, et al. Techniques. Graph neural networks can be flexibly leveraged to encode molecular features on different representation levels and across different problem settings. Canonical GNNs like GCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular, this 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the binding protein receptor is considered alongside the ligand geometry per se. Fragment-based models like JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure. Various generative methods have also been effectively incorporated into the molecular setting, including generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to generate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating the search for drug candidates. Challenges and Limitations. While there has been an abundant supply of unlabelled molecular structural and geometric data [125, 193, 426], the number of labeled molecular data over certain critical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the other hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of the generated molecules, such as QED and Vina [103], rather than actual wet lab experiments. Future Works. Besides the structural and geometric attributes described in this chapter, an even more extensive array of data can be applied to aid molecular generation, including chemical reactions and medical ontology. These data can be organized into a heterogeneous knowledge graph to aid the extraction of high-quality molecular representations. Furthermore, high throughput experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and druggability of the generated molecules in the wet lab. Concrete case studies, such as the design of potential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights into leveraging these molecular generative models to facilitate the design and fabrication of potent and applicable drug molecules in the pharmaceutical industry. Integrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations offers a promising new direction in molecular generation. Recent studies like those by [196] and [160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLMbased approaches in this domain, including those by [338] and [18], predominantly utilize textual SMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The emerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image, and text, which could more comprehensively capture the intricacies of molecular structures. This approach marks a significant shift towards utilizing graph-based information alongside traditional text, enhancing the capability of LLMs in molecular generation. Such advances suggest that future research should focus more on exploiting the synergy between graph-based molecular representations and the evolving landscape of LLMs to address complex challenges in chemistry and material sciences. 13 Recommender Systems The use of graph representation learning in recommender systems has been drawing increasing attention as one of the key strategies for addressing the issue of information overload. With their strong ability to capture high-order connectivity between graph nodes, deep graph representation learning has been shown to be beneficial in enhancing recommendation performance across a variety of recommendation scenarios. Typical recommender systems take the observed interactions between users and items and their fixed features as input, and are intended for making proper predictions on which items a specific user is probably interested in. To formulate, given an user set U, an item set I and the J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 57 Table 12. Summary of graph models for recommender systems. Model Recommendation Task Graph Structure Graph Encoder Representation GC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer NGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation MMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer LightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling DGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling CAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling SR-GNN [496] Session-based Transition Graph GGNN Soft-Attention GC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention FGNN [377] Session-based Session Graph GAT Last-Layer GAG [378] Session-based Session Graph GCN Self-Attention GCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling HyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention DHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer MBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention HCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer H3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer STHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling interaction matrix between users and items X \u2208 {0, 1} |U|\u00d7|I| , where Xu,v indicates there is an observed interaction between user u and item i. The target of GNNs on recommender systems is to learn representations hu, hi \u2208 R d for given u and i. The preference score can..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      },
      "S1364293428": {
        "id": "S1364293428",
        "text": "Challenges in deep graph representation learning include the need for scalable methods that can efficiently process large graphs, the difficulty of learning from sparse or noisy data, and the limitations of existing models in handling dynamic graphs.",
        "children": [
          {
            "id": "E5304604864",
            "text": "A Comprehensive Survey on Deep Graph Representation Learning WEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University, China ZIYUE QIAO, The Hong Kong University of Science and Technology, China YIFANG QIN and JIANHAO SHEN, Peking University, China FANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA JUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China YIFAN WANG, University of International Business and Economics, China XIAO LUO\u2217, University of California, Los Angeles, USA MING ZHANG\u2217, Peking University, China Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations. \u2217Corresponding authors. Authors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn; Zequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871; Ziyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453; Yifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang Sun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei Yang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking University, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics, Beijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang, mzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2024 Association for Computing Machinery. 0004-5411/2024/2-ART $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. arXiv:2304.05055v3 [cs.LG] 28 Feb 2024 2 W. Ju, et al. Additional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network, Survey ACM Reference Format: Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024. A Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Graphs have recently emerged as a powerful tool for representing a variety of structured and complex data, including social networks, traffic networks, information systems, knowledge graphs, protein-protein interaction networks, and physical interaction networks. As a kind of general form of data organization, graph structures are capable of naturally expressing the intrinsic relationship of these data, and thus can characterize plenty of non-Euclidean structures that are crucial in a variety of disciplines and domains due to their flexible adaptability. For example, to encode a social network as a graph, nodes on the graph are used to represent individual users, and edges are used to represent the relationship between two individuals, such as friends. In the field of biology, nodes can be used to represent proteins, and edges can be used to represent biological interactions between various proteins, such as the dynamic interactions between proteins. Thus, by analyzing and mining the graph-structured data, we can understand the deep meaning hidden behind the data, and further discover valuable knowledge, so as to benefit society and human beings. In the last decade years, a wide range of machine learning algorithms have been developed for graph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410] usually break down graphs into different atomic substructures and then use kernel functions to measure the similarity between all pairs of them. Although graph kernels could provide a perspective on modeling graph topology, these approaches often generate substructures or feature representations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer from high computational complexity, and therefore have weak scalability and subpar performance. In the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have everincreasing emerged, which attempt to encode the structural information of the graph (usually a high-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to preserve the topology information and attribute information in the embedding space as much as possible, so that the learned graph embeddings can be naturally integrated into traditional machine learning algorithms. Compared to previous works which use feature engineering in the pre-processing phase to extract graph structural features, current graph embedding algorithms are conducted in a data-driven way leveraging machine learning algorithms (such as neural networks) to encode the structural information of the graph. Specifically, existing graph embedding methods can be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354] that factorize the matrix to learn node embedding which preserves the graph property; (ii) deep learning based methods [155, 362, 443, 460] that apply deep learning techniques specifically designed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either maximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these methods typically depend on shallow architectures, and fail to exploit the potential and capacity of deep neural networks, resulting in sub-optimal representation quality and learning performance. Inspired by the recent remarkable success of deep neural networks, a range of deep learning algorithms has been developed for graph-structured data learning. The core of these methods is to generate effective node and graph representations using graph neural networks (GNNs), followed by a goal-oriented learning paradigm. In this way, the derived representations can be adaptively J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 3 coupled with a variety of downstream tasks and applications. Following this line of thought, in this paper, we propose a new taxonomy to classify the existing graph representation learning algorithms, i.e., graph neural network architectures, learning paradigms, and various promising applications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the studies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer. For the learning paradigms, we explore three advanced types namely supervised/semi-supervised learning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate the effectiveness of the learned graph representations, we provide several promising applications to build tight connections between representation learning and downstream tasks, such as social analysis, molecular property prediction and generation, recommender systems, and traffic analysis. Last but not least, we present some perspectives for thought and suggest challenging directions that deserve further study in the future. Differences between this survey and existing ones. Up to now, there exist some other overview papers focusing on different perspectives of graph representation learning[17, 50, 53..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E3251525896",
            "text": "Fang Li\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nYi Nian\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nZenan Sun\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nCui Tao cui.tao@uth.tmc.edu\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nDrCui Tao\nAdvancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions\n1 Corresponding author: (Main text word count: 4,471, 31,518 characters with space, excluding Title page, Summary, Tables, Acknowledgements, and References; \u2264 34,000 characters including spaces) 2(4, \u2264 5) Graph representation learningbiomedicinegraph neural networkknowledge graphnode embedding\n250, \u2264 250 words)Objectives: Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.Methods:We conducted a comprehensive search of multiple databases, including PubMed, Web of Science, IEEE Xplore, and Google Scholar, to collect relevant publications from the past two years (2021)(2022). The studies selected for review were based on their relevance to the topic and the publication quality.Results: A total of 78 articles were included in our analysis. We identified three main categories of GRL methods and summarized their methodological foundations and notable models. In terms of GRL applications, we focused on two main topics: drug and disease. We analyzed the study frameworks and achievements of the prominent research. Based on the current state-of-the-art, we also discussed the challenges and future directions.Conclusions: GRL methods applied in the biomedical field demonstrated several key characteristics, including the utilization of attention mechanisms to prioritize relevant features, a growing emphasis on model interpretability, and the combination of various techniques to improve model performance. However, there are challenges needed to be addressed, including mitigating model bias, accommodating the heterogeneity of large-scale knowledge graphs, and improving the availability of high-quality graph data. To fully leverage the potential of GRL, future efforts should prioritize these areas of research.3\napplications in healthcare, including drug repurposing [12], disease risk prediction [13], and protein-protein interaction (PPI) prediction [14], and can be used to generate new hypotheses that ultimately translate into clinically actionable outcomes.\nOver the past two decades, machine learning (ML), specifically deep learning (DL), has been successful in vast healthcare scenarios, such as medical imaging and diagnostics [15], drug discovery [16], health insurance and fraud detection [17]. However, these techniques were mainly designed to process the Euclidean data, such as sequential electronic health records (EHRs), text, and image, and may not handle the non-Euclidean graph data directly [4] The distinct among the Euclidean and non-Euclidean data is the underlying geometry used to represent the data: the Euclidean geometry deals with flat, two-dimensional spaces, while the non-Euclidean geometry studies curved surfaces [18]. The key challenge in utilizing graph data in ML models is finding a way to represent graph structure that is easy for the models to learn. [19] Graph representation learning (GRL), which embeds raw graph data into a low-dimensional space while preserving graph topology and node properties [2], can make graph data more amenable to ML.\nAs the frontier of graph algorithms, GRL has attracted significant interest from diverse fields, including computer science and biomedicine. Despite the remarkable success of GRL, it still faces many challenges, ranging from the theoretical understanding of methods to scalability and interpretability in real systems, and from the soundness of methodology to empirical performance in applications [20].\nIn this survey, we review the recent progress of GRL in biomedicine, focusing on both its methods and applications. We also identify the key challenges that GRL faces and discuss the potential opportunities for future research. Our goal is to provide insight into the direction of future efforts in this active and fast-growing research area.\nMaterials and Methods\nThis survey aimed to review the latest development in GRL research in the healthcare field during the past two years (2021)(2022). To ensure that the analysis was in-depth and up-to-date, we conducted a thorough search for relevant articles in multiple databases, including PubMed, Web of Science, IEEE Xplore, Google Scholar, and arXiv. The study selection criteria focused on topic relevance and publication quality, with preference given to high-impact factor journals, top-tier conferences, and articles with large number of citations. By employing this rigorous approach, we included 78 most representative articles, encompassing both original research and reviews.\nThe selected studies were of high quality, ensuring a robust understanding of the latest advancements in GRL in the biomedical field.\nAdvances in GRL Methods\nOver the last decade, GRL has emerged as a critical and pervasive research area, greatly improving the efficiency and flexibility of representation learning [19]. There are two settings for graph learning patterns: transductive learning and inductive learning (or reasoning) [20,21].\nTransductive learning involves observing all data, both training and test (with unknown labels), during training. The model learns from the observed training data and predicts the labels of the test data. On the other hand, inductive learning is more like the traditional supervised learning, where the model encounters only the training data when developing and then the learned model is applied on the test data which it has never seen before. Transductive learning can generate node embeddings for existing nodes or suggest new relations (edges) in a fixed graph, while the inductive learning has the generalizability to the new graphs.\nIn this section, we will explore three fundamental categories of GRL methods, based on the classification defined in a few studies [2,22,23]. Table 1 presents the principles, characteristics and applicable tasks of these three GRL categories. Additionally, Table 2 lists some notable GRL models that have been developed in recent years.\nShallow Node Embedding\nThe purpose of the shallow node embedding is to project nodes onto a latent space, which is a multi-dimensional vector space learned by a model based on the input data. This latent space serves as a summary of the local graph structure, and the original relations in the graph are then represented by the topological relationships of the embedded representation. Node embeddings are characterized by an encoding and decoding process [22], where the encoder maps each node to the latent embedding space, serveing as an embedding lookup table, while the decoder reconstructs a graph statistic for a pair of embedded nodes. The optimization of encoder and decoder is intended to minimize the loss between the decoded statistic and some node-based similarity metrics. Notable shallow node embedding methods include DeepWalk [24], node2vec [25], struc2vec [26], and LINE [27].\nShallow node embedding methods are relatively simple to implement and interpret.\nHowever, their transductive nature [28] makes them less suitable for inductive reasoning, where the graph structure may change or not be pre-defined [29]. Moreover, the shallow embedding methods only consider the topological structure of graph as input and generate the embedding of nodes or edges, without taking into account any associated node and edge attributes.\nGraph Neural Networks\nGraph neural networks (GNNs) are neural networks designed to operate on graph data. [20] By learning compact representations of graph elements, their attributes, and supervised labels (if any) [23], GNNs surpass shallow node embeddings in their ability to perform inductive reasoning and capture higher-order and nonlinear patterns through multi-hop propagation within several layers of neural message passing [30]. In contrast, shallow node embedding methods only generate node representations that can be combined with ML models to perform downstream tasks, while GNNs fuse both graph topology and attributes to perform end-to-end graph tasks. [2] Additionally, GNNs can optimize supervised signals and graph structure simultaneously, whereas shallow embeddings require a two-stage approach to achieve the same. [23] Convolutional neural network (CNN) are among the most popular DL models used in the computer vision applications [31], and have shown exceptional performance in tasks such as object detection [32] and image analysis [33,34]. Although CNN are traditionally used for structured Euclidean data, such as image pixels or text sequences, the concept of convolution to learn local connections can be adapted to non-Euclidean graphs using spectral and spatial approaches [4]. In the spectral approach, graph inform",
            "url": "https://arxiv.org/pdf/2306.10456.pdf",
            "openalex_id": ""
          }
        ]
      },
      "S6564751247": {
        "id": "S6564751247",
        "text": "Graph representation learning has shown significant promise in various applications, including social network analysis, molecular property prediction, and recommendation systems, demonstrating its versatility across different domains.",
        "children": [
          {
            "id": "E5304604864",
            "text": "A Comprehensive Survey on Deep Graph Representation Learning WEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University, China ZIYUE QIAO, The Hong Kong University of Science and Technology, China YIFANG QIN and JIANHAO SHEN, Peking University, China FANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA JUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China YIFAN WANG, University of International Business and Economics, China XIAO LUO\u2217, University of California, Los Angeles, USA MING ZHANG\u2217, Peking University, China Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations. \u2217Corresponding authors. Authors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn; Zequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871; Ziyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453; Yifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang Sun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei Yang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking University, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics, Beijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang, mzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2024 Association for Computing Machinery. 0004-5411/2024/2-ART $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. arXiv:2304.05055v3 [cs.LG] 28 Feb 2024 2 W. Ju, et al. Additional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network, Survey ACM Reference Format: Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024. A Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Graphs have recently emerged as a powerful tool for representing a variety of structured and complex data, including social networks, traffic networks, information systems, knowledge graphs, protein-protein interaction networks, and physical interaction networks. As a kind of general form of data organization, graph structures are capable of naturally expressing the intrinsic relationship of these data, and thus can characterize plenty of non-Euclidean structures that are crucial in a variety of disciplines and domains due to their flexible adaptability. For example, to encode a social network as a graph, nodes on the graph are used to represent individual users, and edges are used to represent the relationship between two individuals, such as friends. In the field of biology, nodes can be used to represent proteins, and edges can be used to represent biological interactions between various proteins, such as the dynamic interactions between proteins. Thus, by analyzing and mining the graph-structured data, we can understand the deep meaning hidden behind the data, and further discover valuable knowledge, so as to benefit society and human beings. In the last decade years, a wide range of machine learning algorithms have been developed for graph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410] usually break down graphs into different atomic substructures and then use kernel functions to measure the similarity between all pairs of them. Although graph kernels could provide a perspective on modeling graph topology, these approaches often generate substructures or feature representations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer from high computational complexity, and therefore have weak scalability and subpar performance. In the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have everincreasing emerged, which attempt to encode the structural information of the graph (usually a high-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to preserve the topology information and attribute information in the embedding space as much as possible, so that the learned graph embeddings can be naturally integrated into traditional machine learning algorithms. Compared to previous works which use feature engineering in the pre-processing phase to extract graph structural features, current graph embedding algorithms are conducted in a data-driven way leveraging machine learning algorithms (such as neural networks) to encode the structural information of the graph. Specifically, existing graph embedding methods can be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354] that factorize the matrix to learn node embedding which preserves the graph property; (ii) deep learning based methods [155, 362, 443, 460] that apply deep learning techniques specifically designed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either maximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these methods typically depend on shallow architectures, and fail to exploit the potential and capacity of deep neural networks, resulting in sub-optimal representation quality and learning performance. Inspired by the recent remarkable success of deep neural networks, a range of deep learning algorithms has been developed for graph-structured data learning. The core of these methods is to generate effective node and graph representations using graph neural networks (GNNs), followed by a goal-oriented learning paradigm. In this way, the derived representations can be adaptively J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 3 coupled with a variety of downstream tasks and applications. Following this line of thought, in this paper, we propose a new taxonomy to classify the existing graph representation learning algorithms, i.e., graph neural network architectures, learning paradigms, and various promising applications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the studies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer. For the learning paradigms, we explore three advanced types namely supervised/semi-supervised learning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate the effectiveness of the learned graph representations, we provide several promising applications to build tight connections between representation learning and downstream tasks, such as social analysis, molecular property prediction and generation, recommender systems, and traffic analysis. Last but not least, we present some perspectives for thought and suggest challenging directions that deserve further study in the future. Differences between this survey and existing ones. Up to now, there exist some other overview papers focusing on different perspectives of graph representation learning[17, 50, 53..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E3251525896",
            "text": "Fang Li\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nYi Nian\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nZenan Sun\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nCui Tao cui.tao@uth.tmc.edu\nSchool of Biomedical Informatics\nSchool of Biomedical Informatics\nUniversity of Texas Health Science Center at Houston\nHoustonTXUSA\nUniversity of Texas Health Science Center at Houston\nSuite 6007000, 77030Fannin, HoustonTX\nDrCui Tao\nAdvancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions\n1 Corresponding author: (Main text word count: 4,471, 31,518 characters with space, excluding Title page, Summary, Tables, Acknowledgements, and References; \u2264 34,000 characters including spaces) 2(4, \u2264 5) Graph representation learningbiomedicinegraph neural networkknowledge graphnode embedding\n250, \u2264 250 words)Objectives: Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.Methods:We conducted a comprehensive search of multiple databases, including PubMed, Web of Science, IEEE Xplore, and Google Scholar, to collect relevant publications from the past two years (2021)(2022). The studies selected for review were based on their relevance to the topic and the publication quality.Results: A total of 78 articles were included in our analysis. We identified three main categories of GRL methods and summarized their methodological foundations and notable models. In terms of GRL applications, we focused on two main topics: drug and disease. We analyzed the study frameworks and achievements of the prominent research. Based on the current state-of-the-art, we also discussed the challenges and future directions.Conclusions: GRL methods applied in the biomedical field demonstrated several key characteristics, including the utilization of attention mechanisms to prioritize relevant features, a growing emphasis on model interpretability, and the combination of various techniques to improve model performance. However, there are challenges needed to be addressed, including mitigating model bias, accommodating the heterogeneity of large-scale knowledge graphs, and improving the availability of high-quality graph data. To fully leverage the potential of GRL, future efforts should prioritize these areas of research.3\napplications in healthcare, including drug repurposing [12], disease risk prediction [13], and protein-protein interaction (PPI) prediction [14], and can be used to generate new hypotheses that ultimately translate into clinically actionable outcomes.\nOver the past two decades, machine learning (ML), specifically deep learning (DL), has been successful in vast healthcare scenarios, such as medical imaging and diagnostics [15], drug discovery [16], health insurance and fraud detection [17]. However, these techniques were mainly designed to process the Euclidean data, such as sequential electronic health records (EHRs), text, and image, and may not handle the non-Euclidean graph data directly [4] The distinct among the Euclidean and non-Euclidean data is the underlying geometry used to represent the data: the Euclidean geometry deals with flat, two-dimensional spaces, while the non-Euclidean geometry studies curved surfaces [18]. The key challenge in utilizing graph data in ML models is finding a way to represent graph structure that is easy for the models to learn. [19] Graph representation learning (GRL), which embeds raw graph data into a low-dimensional space while preserving graph topology and node properties [2], can make graph data more amenable to ML.\nAs the frontier of graph algorithms, GRL has attracted significant interest from diverse fields, including computer science and biomedicine. Despite the remarkable success of GRL, it still faces many challenges, ranging from the theoretical understanding of methods to scalability and interpretability in real systems, and from the soundness of methodology to empirical performance in applications [20].\nIn this survey, we review the recent progress of GRL in biomedicine, focusing on both its methods and applications. We also identify the key challenges that GRL faces and discuss the potential opportunities for future research. Our goal is to provide insight into the direction of future efforts in this active and fast-growing research area.\nMaterials and Methods\nThis survey aimed to review the latest development in GRL research in the healthcare field during the past two years (2021)(2022). To ensure that the analysis was in-depth and up-to-date, we conducted a thorough search for relevant articles in multiple databases, including PubMed, Web of Science, IEEE Xplore, Google Scholar, and arXiv. The study selection criteria focused on topic relevance and publication quality, with preference given to high-impact factor journals, top-tier conferences, and articles with large number of citations. By employing this rigorous approach, we included 78 most representative articles, encompassing both original research and reviews.\nThe selected studies were of high quality, ensuring a robust understanding of the latest advancements in GRL in the biomedical field.\nAdvances in GRL Methods\nOver the last decade, GRL has emerged as a critical and pervasive research area, greatly improving the efficiency and flexibility of representation learning [19]. There are two settings for graph learning patterns: transductive learning and inductive learning (or reasoning) [20,21].\nTransductive learning involves observing all data, both training and test (with unknown labels), during training. The model learns from the observed training data and predicts the labels of the test data. On the other hand, inductive learning is more like the traditional supervised learning, where the model encounters only the training data when developing and then the learned model is applied on the test data which it has never seen before. Transductive learning can generate node embeddings for existing nodes or suggest new relations (edges) in a fixed graph, while the inductive learning has the generalizability to the new graphs.\nIn this section, we will explore three fundamental categories of GRL methods, based on the classification defined in a few studies [2,22,23]. Table 1 presents the principles, characteristics and applicable tasks of these three GRL categories. Additionally, Table 2 lists some notable GRL models that have been developed in recent years.\nShallow Node Embedding\nThe purpose of the shallow node embedding is to project nodes onto a latent space, which is a multi-dimensional vector space learned by a model based on the input data. This latent space serves as a summary of the local graph structure, and the original relations in the graph are then represented by the topological relationships of the embedded representation. Node embeddings are characterized by an encoding and decoding process [22], where the encoder maps each node to the latent embedding space, serveing as an embedding lookup table, while the decoder reconstructs a graph statistic for a pair of embedded nodes. The optimization of encoder and decoder is intended to minimize the loss between the decoded statistic and some node-based similarity metrics. Notable shallow node embedding methods include DeepWalk [24], node2vec [25], struc2vec [26], and LINE [27].\nShallow node embedding methods are relatively simple to implement and interpret.\nHowever, their transductive nature [28] makes them less suitable for inductive reasoning, where the graph structure may change or not be pre-defined [29]. Moreover, the shallow embedding methods only consider the topological structure of graph as input and generate the embedding of nodes or edges, without taking into account any associated node and edge attributes.\nGraph Neural Networks\nGraph neural networks (GNNs) are neural networks designed to operate on graph data. [20] By learning compact representations of graph elements, their attributes, and supervised labels (if any) [23], GNNs surpass shallow node embeddings in their ability to perform inductive reasoning and capture higher-order and nonlinear patterns through multi-hop propagation within several layers of neural message passing [30]. In contrast, shallow node embedding methods only generate node representations that can be combined with ML models to perform downstream tasks, while GNNs fuse both graph topology and attributes to perform end-to-end graph tasks. [2] Additionally, GNNs can optimize supervised signals and graph structure simultaneously, whereas shallow embeddings require a two-stage approach to achieve the same. [23] Convolutional neural network (CNN) are among the most popular DL models used in the computer vision applications [31], and have shown exceptional performance in tasks such as object detection [32] and image analysis [33,34]. Although CNN are traditionally used for structured Euclidean data, such as image pixels or text sequences, the concept of convolution to learn local connections can be adapted to non-Euclidean graphs using spectral and spatial approaches [4]. In the spectral approach, graph inform",
            "url": "https://arxiv.org/pdf/2306.10456.pdf",
            "openalex_id": ""
          }
        ]
      }
    }
  },
  {
    "id": "https://openalex.org/W4388017359",
    "text": "1\nEnd-to-End Speech Recognition: A Survey\nRohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE,\nRalf Schluter, \u00a8 Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE\nAbstract\u2014In the last decade of automatic speech recognition\n(ASR) research, the introduction of deep learning has brought\nconsiderable reductions in word error rate of more than 50%\nrelative, compared to modeling without deep learning. In the\nwake of this transition, a number of all-neural ASR architectures\nhave been introduced. These so-called end-to-end (E2E) models\nprovide highly integrated, completely neural ASR models, which\nrely strongly on general machine learning knowledge, learn more\nconsistently from data, with lower dependence on ASR domain\u0002specific experience. The success and enthusiastic adoption of deep\nlearning, accompanied by more generic model architectures has\nled to E2E models now becoming the prominent ASR approach.\nThe goal of this survey is to provide a taxonomy of E2E ASR\nmodels and corresponding improvements, and to discuss their\nproperties and their relationship to classical hidden Markov\nmodel (HMM) based ASR architectures. All relevant aspects\nof E2E ASR are covered in this work: modeling, training,\ndecoding, and external language model integration, discussions of\nperformance and deployment opportunities, as well as an outlook\ninto potential future developments.\nIndex Terms\u2014end-to-end, automatic speech recognition.\nI. INTRODUCTION\nThe classical1statistical architecture decomposes an auto\u0002matic speech recognition (ASR) system into four main compo\u0002nents: acoustic feature extraction from speech audio signals,\nacoustic modeling, language modeling and search based on\nBayes\u2019 decision rule [1], [2], [3]. Classical acoustic modeling\nis based on hidden Markov models (HMMs) to account for\nspeaking rate variation. Within the classical approach, deep\nlearning has been introduced into acoustic and language mod\u0002eling. In acoustic modeling, deep learning has replaced Gaus\u0002sian mixture distributions (hybrid HMM [4], [5]) or augmented\nthe acoustic feature set (e.g., non-linear discriminant/tandem\napproach [6], [7]). In language modeling, deep learning has re\u0002placed count-based approaches [8], [9], [10]. However, in these\nearly attempts at introducing deep learning, the classical ASR\narchitecture was unmodified. Classical state-of-the-art ASR\nsystems today are composed of many separate components and\nknowledge sources: especially speech signal preprocessing;\nmethods for robustness with respect to recording conditions;\nphoneme inventories and pronunciation lexica; phonetic clus\u0002tering; handling of out-of-vocabulary words; various methods\nfor adaptation/normalization; elaborate training schedules with\ndifferent objectives including sequence discriminative training,\netc. The potential of deep learning, on the other hand, initiated\nsuccessful approaches to integrate formerly separate modeling\nsteps, e.g., by integrating speech signal pre-processing and\nfeature extraction into acoustic modeling [11], [12].\n1 The term \u201cclassical\u201d here refers to the former, long-term, state-of-the-art\nASR architecture based on the decomposition into acoustic and language\nmodel, and with acoustic modeling based on hidden Markov models.\nMore consequently, the introduction of deep learning to\nASR also initiated research to replace classical ASR archi\u0002tectures based on hidden Markov models (HMM) with more\nintegrated joint neural network model structures [13], [14],\n[15], [16]. These ventures might be seen as trading specific\nspeech processing models for more generic machine learning\napproaches to sequence-to-sequence processing \u2013 akin to how\nstatistical approaches to natural language processing have\ncome to replace more linguistically oriented models. For these\nall-neural approaches recently the term end-to-end (E2E) [14],\n[17], [18], [19] has been established. Therefore, first of all\nan attempt to define the term end-to-end in the context of\nASR is due in this survey. According to the Cambridge\nDictionary, the adjective \u201cend-to-end\u201d is defined as: \u201cinclud\u0002ing all the stages of a process\u201d [20]. We therefore propose\nthe following definition of end-to-end ASR: an integrated\nASR model that enables joint training from scratch; avoids\nseparately obtained knowledge sources; and, provides single\u0002pass recognition consistent with the objective to optimize the\ntask-specific evaluation measure, i.e., usually label (word,\ncharacter, subword, etc.) error rate. While this definition\nsuffices for the present discussion, we note that such an\nidealized definition hides many nuances involved in the term\nE2E and lacks distinctiveness; we elaborate on some of these\nnuances in Sec. II to discuss the various connotations of the\nterm E2E in the context of ASR.\nWhat are potential benefits of E2E approaches to ASR?\nThe primary objective when developing an ASR systems is to\nminimize the expected word error rate; secondary objectives\nare to reduce time and memory complexity of the resulting\ndecoder, and \u2013 assuming a constrained development budget \u2013\ngenericity, and ease of modeling. First of all, an integrated\nASR system, defined in terms of a single neural network\nstructure supports genericity of modeling and may allow for\nfaster development cycles when building ASR systems for\nnew languages or domains. Similarly, ASR models defined\nby a single neural network structure may become more \u2018lean\u2019\ncompared to classical modeling, with a simpler decoding\nprocess, obviating the need to integrate separate models. The\nresulting reduction in memory footprint and power consump\u0002tion supports embedded ASR applications [21], [22]. Further\u0002more, end-to-end joint training may help to avoid spurious\noptima from intermediate training stages. Avoiding secondary\nknowledge sources like pronunciation lexica may be helpful\nfor languages/domains where such resources are not easily\navailable. Also, secondary knowledge sources may themselves\nbe erroneous; avoiding these may improve models trained\ndirectly from data, provided that sufficient amounts of task\u0002specific training data are available.\nWith the current surge of interest in E2E ASR models and an\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nincreasing diversity of corresponding work, the authors of this\nreview think it is time to provide an overview of this rapidly\nevolving domain of research. The goal of this survey is to\nprovide an in-depth overview of the current state of research\non E2E ASR systems, covering all relevant aspects of E2E\nASR, with a contrastive discussion of the different E2E and\nclassical ASR architectures.\nThis survey of E2E speech recognition is structured as fol\u0002lows. Sec. II discusses the nuances in the term E2E as it applies\nto ASR. Sec. III describes the historical evolution of E2E\nspeech recognition, with specific focus on the input-output\nalignment and an overview of prominent E2E ASR models.\nSec. IV discusses improvements of the basic E2E models,\nincluding E2E model combination, training loss functions,\ncontext, encoder/decoder structures and endpointing. Sec. V\nprovides an overview of E2E ASR model training. Decoding\nalgorithms for the different E2E approaches are discussed\nin Sec. VI. Sec. VII discusses the role and integration of\n(separate) language models in E2E ASR. Sec. VIII reviews\nexperimental comparisons of the different E2E as well as\nclassical ASR approaches. Sec. IX provides an overview of\napplications of E2E ASR. Sec. X investigates future directions\nof E2E research in ASR, before concluding in Sec. XI. Finally,\nwe note that this survey paper also includes comparative\ndiscussions between novel E2E models and classical HMM\u0002based ASR approaches in terms of various aspects; most\nsections end with a summarization of the relationship between\nE2E models and HMM-based ASR approaches in relation to\nthe topics covered within the respective sections.\nII. DISTINCTIVENESS OF THE TERM E2E\nAs noted in Sec. I the term E2E provides an idealized\ndefinition of ASR systems, and can benefit from a more\ndetailed discussion based on the following perspectives.\na) Joint Modeling: In terms of ASR, the E2E property\ncan be interpreted as considering all components of an ASR\nsystem jointly as a single computational graph. Even more so,\nthe common understanding of E2E in ASR is that of a single\njoint modeling approach that does not necessarily distinguish\nseparate components, which may also mean dropping the\nclassical separation of ASR into an acoustic model and a\nlanguage model. However, in practice E2E ASR systems are\noften combined with external language models trained on text\u0002only data, which weakens the end-to-end nature of the system\nto some extent.\nb) Joint Training: In terms of model training, E2E can\nbe interpreted as estimating all parameters, of all components\nof a model jointly using a single objective function that is\nconsistent with the task at hand, which in case of ASR means\nminimizing the expected word error rate2. However, the term\nlacks distinctiveness here, as classical and/or modular ASR\nmodel architectures also support joint training with a single\nobjective.\n2 Note that this does not necessarily require Bayes Risk training, as standard\ntraining criteria like cross entropy, maximum mutual information and max\u0002imum likelihood in case of classical ASR models asymptotically guarantee\noptimal performance in the sense of Bayes decision rule, also [23], [24].\nc) Training from Scratch: The E2E property can also be\ninterpreted with respect to the training process itself, by re\u0002quiring training from scratch, avoiding external knowledge like\nprior alignments or initial models pre-trained using different\ncriteria or knowledge sources. However, note that pre-training\nand fine-tuning strategies are also relevant, if the model has\nexplicit modularity, including self-supervised learning [25] or\njoint training of front-end and speech recognition models [26].\nEspecially in case of limited amounts of target task training\ndata, utilizing large pretrained models is important to obtain\nperformant E2E ASR systems.\nd) Avoiding Secondary Knowledge Sources: For ASR,\nstandard secondary knowledge sources are pronunciation lex\u0002ica and phoneme sets, as well as phonetic clustering, which\nin classical state-of-the-art ASR systems usually is based on\nclassification and regression trees (CART) [27]. Secondary\nknowledge sources and separately trained components may\nintroduce errors, might be inconsistent with the overall training\nobjective and/or may generate additional cost. Therefore, in\nan E2E approach, these would be avoided. Standard joint\ntraining of an E2E model requires using a single kind of\ntraining data, which in case of ASR would be transcribed\nspeech audio data. However, in ASR often even larger amounts\nof text-only data, as well as optional untranscribed speech\naudio are available. One of the challenges of E2E modeling\ntherefore is how to take advantage of text-only and audio-only\ndata jointly without introducing secondary (pretrained) models\nand/or training objectives [28], [29].\ne) Direct Vocabulary Modeling: Avoiding pronunciation\nlexica and corresponding subword units leave E2E recognition\nvocabularies to be derived from whole word or character\nrepresentations. Whole word models [30], according to Zipf\u2019s\nlaw [31], would require unrealistically high amounts of tran\u0002scribed training data for large vocabularies, which might not\nbe attainable for many tasks. On the other hand, methods\nto generate subword vocabularies based on characters, like\nthe currently popular byte pair encoding (BPE) approach\n[32], might be seen as secondary approaches outside the E2E\nobjective, even more so if acoustic data is considered for\nsubword derivation [33], [34], [35], [36].\nf) Generic Modeling: Finally, E2E modeling also re\u0002quires genericity of the underlying modeling: task-specific\nconstraints are learned completely from data, in contrast to\ntask-specific knowledge which influences the modeling of\nthe system architecture in the first place. For example, the\nmonotonicity constraint in ASR may be learned completely\nfrom data in an end-to-end fashion (e.g., in attention-based\napproaches [16]), or it may directly be implemented, as in\nclassical HMM structures. However, model constraints may\nbe considered by way of regularization in E2E ASR model\ntraining, and can thus provide an alternative way to introduce\ntask-specific knowledge.\ng) Single-Pass Search: In terms of the recognition/search\nproblem, the E2E property can be interpreted as integrating all\ncomponents (models, knowledge sources) of an ASR system\nbefore coming to a decision. This is in line with Bayes\u2019\ndecision rule, which exactly requires a single global decision\nintegrating all available knowledge sources, which is supported\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nby both classical ASR models as well as E2E models. On\nthe other hand, multipass search is not only exploited by\nclassical ASR models, but also by E2E ASR models, the\nmost prominent case here being (external) language model\nrescoring.\nAll in all, we need to conclude that a) \u201cE2E\u201d does not\nprovide a clear distinction between classical and novel, so\u0002called E2E models, and b) the E2E property often is weakened\nin practice, leaving the term as a more general, idealized\nperspective on ASR modeling.\nIII. A TAXONOMY OF E2E MODELS IN ASR\nBefore we derive a taxonomy of E2E ASR modeling\napproaches, we first introduce our notation. We denote the\ninput speech utterance as X, which we assume has been pa\u0002rameterized into D-dimensional acoustic frames (e.g., log-mel\nfeatures) of length T\n\u2032\n: X = (x1, \u00b7 \u00b7 \u00b7 , xT\u2032 ), where xt \u2208 R\nD.\nWe denote the corresponding word sequences as C, which can\nbe decomposed into a suitable sequence of labels of length L:\nC = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is\nagnostic to the specific representation used for decomposing\nthe word sequence into labels; popular choices include char\u0002acters, words, or sub-word sequences (e.g., BPE [32], word\u0002pieces [37]).\nASR may be viewed as a sequence classification problem\nwhich maps a variable length input, X, into an output,\nC, of unknown length. Following Bayes\u2019 decision rule, any\nstatistical approach to ASR must determine how to model the\nword sequence posterior probability, P(C|X). Thus, a natural\ntaxonomy of E2E ASR modeling can be based on the various\nstrategies for modeling this word sequence posterior: i.e., how\nthe alignment problem between input and output sequence is\nhandled; and, how sequence modeling is decomposed to the\nlevel of individual input vectors xt\n\u2032 and/or output labels cl\n.\nWe find that it is useful to distinguish implicit and explicit\nmodeling approaches, based on the modeling of the sequence\u0002to-sequence alignment:\na) Explicit Alignment Modeling: does not necessarily\nrefer to the determination of a single unique alignment, but\ninstead introduces an explicit alignment modeled as a latent\nvariable, A:\nP(C|X) = X\nA\nP(C, A|X)\nb) Implicit Alignment Modeling: does not introduce a\nlatent alignment variable, but models the label sequence pos\u0002terior P(C|X) directly.\nExplicit alignment modeling approaches can mainly be\ndistinguished by their choice of latent variable; these can be\nencoded in terms of valid emission paths in corresponding\nfinite state automata (FSA) [38] which relate the input and\noutput sequences \u2013 the approach taken in our article. Typically,\nlatent variables in explicit alignment modeling in transducer\nE2E models introduce extensions to the output label set\nwith different forms of continuation labels (including, but not\nlimited to so-called blank labels).3\n3 For example, these extensions may also include explicit duration variables,\nleading to segmental models [39]. Such models can be rewritten into equiv\u0002alent transducer models [40], and vice-versa.\nA. Encoder and Decoder Modules\nIrrespective of the alignment modeling approach, following\nthe notation introduced in [41], it is useful to view all E2E\nASR models as being composed of an encoder module and\na decoder module. The encoder module, denoted H(X),\nmaps an input acoustic frame sequence, X, of length T\n\u2032\ninto a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of\nlength T (typically T \u2264 T\n\u2032\n). Note that the encoder output is\nindependent of the hypothesized label sequence. The decoder\nmodule models the label sequence posterior on top of the\nencoder output:\nP(C|X) = P\nC\nH(X)\n\u0001\nThus, we may distinguish different approaches based upon\nhow the output label sequence distribution (including potential\nlatent variables resulting from the alignment modeling) are de\u0002composed into individual label (and alignment) contributions;\nthese may occur per output label position, per encoder frame\nposition, or combinations thereof:\nP\nC[, A]\nH(X)\n\u0001\n=\nY\nL\ni=1\nP\nci[, ai]\nc\ni\u22121\n1\n[, ai\u22121\n1\n], vi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X))\u0001\nwhere the notation mi\u22121\n1\ncorresponds to the sequence\nof i \u2212 1 previous instances of the variables m; and,\nvi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X)) denotes a context-vector that provides\nthe connection between encoder output, H(X), and the la\u0002bel output position, i. In general the context vector may\ndepend on the label context (and possibly the latent vari\u0002able context, for explicit alignment modeling approaches).\nApart from the underlying alignment model and corresponding\noutput label decomposition, decoder modules differ in terms\nof the assumptions on their label context c\ni\u22121\n1\n(and their\nlatent variable context a\ni\u22121\n1\n), which correspond to different\nconditional independence assumptions, and by their access to\nthe encoder output. For example, the local posterior may only\ndepend on a single encoder frame output (i.e., with the context\nvector being reduced to a single encoder frame\u2019s output):\nvi\nc\ni\u22121\n1\n, H(X)\n\u0001\n= hti(X). As we shall see in detail in the\nfollowing sections, the simplest case of an encoder frame\u0002level decomposition (with L = T, and ti = i) corresponds to\nCTC [13]; AED models [16] and their variants maintain the\nfull dependency of the context vector.\nFinally, different E2E models can also be distinguished by\nthe specific modeling choices that are involved in the design\nof the neural network used to implement the encoder and the\ndecoder. These might involve feed-forward neural networks,\nconvolutional neural networks, recurrent neural networks (ei\u0002ther uni-directional or bi-directional) [42], attention [43],\nand various combinations thereof (e.g., transformers [44] or\nconformers [45]). These modeling choices and corresponding\ntraining methods can be applied across E2E ASR models and\ntherefore do not enter the taxonomy of E2E ASR models\ndiscussed here. However, specific choices will be discussed as\npart of the exemplary E2E ASR models presented in Sec. VIII\nand Sec. IX and.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nB. Explicit Alignment Modeling Approaches\nEarly E2E modeling approaches modeled alignments explic\u0002itly through a latent variable, which is marginalized out (pos\u0002sibly, approximately) during training and inference. Examples\nof this family of approaches include connectionist temporal\nclassification (CTC) [13], the recurrent neural network trans\u0002ducer (RNN-T) [14], the recurrent neural aligner (RNA) [46],\nand the hybrid auto-regressive transducer [47] (HAT). As\nwill be discussed in subsequent sections, the latter modeling\napproaches in this family represent increasingly sophisticated\nmodeling of alignments, with fewer independence assumptions\nand are thus increasingly powerful. A common feature of all\nexplicit alignment models discussed in this section is that\nthey introduce an additional blank symbol, denoted \u27e8b\u27e9, and\ndefine an output probability distribution over symbols in the\nset Cb = C \u222a {\u27e8b\u27e9}. The interpretation of the \u27e8b\u27e9 symbol\nvaries slightly between each of these models, as we discuss in\ngreater details below. For now, it suffices to say that given\na specific training example, (X, C), each of these models\ndefines a set of valid alignments, denoted by A(T ,C), and\ndefine the conditional distribution P(C|X) by marginalizing\nover all valid alignment sequences:\nP(C|X) = X\nA\nP(C|A, H(X))P(A|H(X))\n=\nX\nA\u2208A(T =|H(X)|,C)\nP(A|H(X)) (1)\nwhere, by definition P(C|A, H(X)) = 1 if and only if A \u2208\nA(T ,C) and 0 otherwise.4 We discuss the specific formulations\nof each of these models in the subsequent sections.\n1) Connectionist Temporal Classification (CTC): Connec\u0002tionist Temporal Classification (CTC) was proposed by Graves\net al. [13] as a technique for mapping a sequence of input\ntokens to a corresponding sequence of output tokens. CTC ex\u0002plicitly models alignments between the encoder output, H(X),\nand the label sequence, C, by introducing a special \u201cblank\u201d la\u0002bel, denoted by \u27e8b\u27e9: Cb = C \u222a {\u27e8b\u27e9}. An alignment, A \u2208 C\u2217\nb\n, is\nthus a sequence of labels in C or \u27e8b\u27e9.\n5 Given a specific training\nexample, (X, C), we denote the set of all valid alignments,\nACTC\n(X,C) = {A = (a1, a2, . . . , aT )}, such that each at \u2208 Cb\nwith the additional constraint that A is identical to C after first\ncollapsing consecutive identical labels, and then removing all\nblank symbols. For example, if T = 10, and C = (s, e, e),\nthen A = (s,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ACTC\n(X,C)\n, as\nillustrated in Figure 1. As can be seen in this example, repeated\nlabels in the output can be represented by intervening blanks.\nFollowing Eq. (1), CTC defines the posterior probability\nof the label sequence C conditioned on the input, X, by\n4 This is equivalent to the assumption that the mapping from an alignment\nA to a label sequence C is unique, by definition. 5 S\n\u2217 denotes a Kleene\nclosure: the set of all possible sequences composed of tokens in the set S.\nTime\ns\ne\ne\n \ns\ne\ne\ns\ne\ne\ne\nFig. 1. Example alignment sequence for a CTC model with the target\nsequence C = (s, e, e) (right), alongside a (non-deterministic) finite state\nautomaton (FSA) [38] (left) representing the set of all valid alignment paths.\nEncoder H(X)\nSoftmax\nFig. 2. A representation of the CTC model consisting of an encoder which\nmaps the input speech into a higher-level representation, and a softmax layer\nwhich predicts frame-level probabilities over the set of output labels and blank.\nmarginalizing over all possible CTC alignments as:\nPCTC(C|X) = X\nA\u2208ACTC\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, \u00b7 \u00b7 \u00b7 , a1, H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|ht) (2)\nCritically, as can be seen in Eq. (2), CTC makes a strong\nindependence assumption that the model\u2019s output at time t is\nconditionally independent of the outputs at other timesteps,\ngiven the local encoder output at time t.\nThus, a CTC model consists of a neural network that\nmodels the distribution P(at|X), at each step as shown in\nFigure 2. The encoder is connected to a softmax layer with\n|Cb| targets representing the individual probabilities in Eq. (2):\nP(at = c|X) = P(at = c|H(X)), which comprises the\ndecoder module for CTC. Thus, at each step, t, the model\nconsumes a single encoded frame ht and outputs a distribution\nover the labels; in other words, the model \u201coutputs\u201d a single\nlabel either blank, \u27e8b\u27e9, or one of the targets in C.\n2) Recurrent Neural Network Transducer (RNN-T): The\nRecurrent Neural Network Transducer (RNN-T) [14], [48] was\nproposed by Graves as an improvement over the basic CTC\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 3. An RNN-T Model [14], [48] consists of an encoder which transforms\nthe input speech frames into a high-level representation, and a prediction\u0002network which models the sequence of non-blank labels that have been\noutput previously. The prediction network output, pit\n, represents the output\nafter producing the previous non-blank label sequence c1, . . . , cit\n. The\njoint network produces a probability distribution over the output symbols\n(augmented with blank) given the prediction network state and a specific\nencoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 4. Example alignment sequence (right) for an RNN-T model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs. The FSA (left) represents the set of all valid RNN-T\nalignment paths.\nmodel [13], by removing some of the conditional indepen\u0002dence assumptions that we discussed previously. The RNN\u0002T model, which is depicted in Figure 3, is best understood\nby contrasting it against the CTC model. As with CTC, the\nRNN-T model augments the output symbols with the blank\nsymbol, and thus defines a distribution over label sequences\nin Cb. Similarly, as with CTC, the model consists of an encoder\nwhich processes the input acoustic frames X to generate the\nencoded representation H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ).\nUnlike CTC, however, the blank symbol in RNN-T has a\nslightly different interpretation; for each input encoder frame,\nht, the RNN-T model outputs a sequence of zero or more\nsymbols in C which are terminated by a single blank symbol.\nThus, we may define the set of all valid alignment se\u0002quences in RNN-T as: ARNNT\n(X,C) = {A = (a1, a2, \u00b7 \u00b7 \u00b7 , aT +L)},\nthe set of all sequences of T + L symbols in C\n\u2217\nb\n, which\nare identical to C after removing all blanks. Finally, for a\ngiven output position \u03c4 , let i\u03c4 denote the number of non\u0002blank labels in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121). Thus, the\nnumber of blanks in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121) is\n\u03c4 \u2212 i\u03c4 \u2212 1. For example, if T = 7, and C = (s, e, e),\nthen A = (\u27e8b\u27e9, s,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ARNNT\n(X,C)\n.\nNote that, unlike the CTC model, repeated labels in the output\nrequire no special treatment as illustrated in Figure 4, where,\ni1 = i2 = 0;i3 = i4 = 1;i10 = 3; etc.\nWe may then define the posterior probability P(C|X) as\nbefore:\nPRNNT(C|X) = X\nA\u2208ARNNT\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |a\u03c4\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |ci\u03c4\n, ci\u03c4 \u22121, . . . , c0, h\u03c4\u2212i\u03c4\n)\n(3)\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |pi\u03c4, h\u03c4\u2212i\u03c4)\nwhere, P = (p1, \u00b7 \u00b7 \u00b7 , pL) represents the output of the predic\u0002tion network depicted in Figure 3 which summarizes the se\u0002quence of previously predicted non-blank labels, implemented\nas another neural network: pj = NN(\u00b7|c0, . . . , cj\u22121), where\nc0 is a special start-of-sentence label, \u27e8sos\u27e9. Thus, as can be\nseen in Eq. (2), RNN-T reduces some of the independence\nassumptions in CTC since the output at time t is conditionally\ndependent on the sequence of previous non-blank predictions,\nbut is independent of the specific choice of alignment (i.e.,\nthe choice of the frames at which the non-blank tokens were\nemitted).\nOur presentation of RNN-T alignments considers the\n\u201ccanonical\u201d case. In principle, however, the model can encode\nthe same set of conditional independence assumptions in\nRNN-T (i.e., the model structure), while considering alter\u0002native alignment structures as in the work of [49]. In their\nwork, Moritz et al., represent valid frame-level alignments as\nan arbitrary graph. This formulation, for example, allows for\nthe use of \u201cCTC-like\u201d alignments in the RNN-T model (i.e.,\noutputting a single label \u2013 blank, or non-blank \u2013 at each frame)\nwhile conditioning on the set of previous non-blank symbols\nas in the RNN-T model.\n3) Recurrent Neural Aligner (RNA): The recurrent neural\naligner (RNA) was proposed by Sak et al. [46]. The RNA\nmodel generalizes the RNN-T model by removing one of its\nconditional independence assumptions. The model, depicted\nin Figure 5, is best understood by considering how it differs\nfrom the RNN-T model. As with CTC and RNN-T, the RNA\nmodel defines a probability distribution over blank augmented\nlabels in the set Cb, where \u27e8b\u27e9 has the same semantics\nas in the CTC model: at each frame the model can only\noutput a single label \u2013 either blank, or non-blank \u2013 before\nadvancing to the next frame; unlike CTC (but as in RNN\u0002T) the model only outputs a single instance of each non\u0002blank label. More specifically, the set of valid alignments,\nARNA\n(X,C) = (a1, \u00b7 \u00b7 \u00b7 , aT ), in the RNA model consist of length T\nsequences in C\n\u2217\nb with exactly T \u2212L blank symbols, and which\nare identical to C after removing all blanks. Thus, the blank\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 5. An RNA Model [46] resembles the RNN-T model [14], [48] in\nterms of the model structure. However, this model is only permitted to output\na single label \u2013 either blank, or non-blank \u2013 in a single frame. Unlike\nRNN-T, the prediction network state in the RNA model, qt\u22121, depends on\nthe entire alignment sequence at\u22121, . . . , a1. The joint network produces a\nprobability distribution over the output symbols (augmented with blank) given\nthe prediction network state and a specific encoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 6. Example alignment sequence (right) for an RNA model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs; diagonal transitions correspond to outputting a non-blank\nsymbol. The FSA (left) represents the set of valid alignments for the RNA\nmodel. Although the FSA is identical to the corresponding FSA for RNN-T\nin Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases.\nsymbol has a different interpretation in RNA and the RNN\u0002T models: in RNN-T, outputting a blank symbol advances\nthe model to the next frame; in RNA, however, the model\nadvances to the next frame after outputting a single blank or\nnon-blank label. Restricting the model to output a single non\u0002blank label at each frame improves computational efficiency\nand simplifies the decoding process, by limiting the number\nof model expansions at each frame (in constrast to RNN-T\ndecoding). For example, if T = 8, and C = (s, e, e), then\nA = (\u27e8b\u27e9, s,\u27e8b\u27e9, e,\u27e8b\u27e9,\u27e8b\u27e9, e,\u27e8b\u27e9) \u2208 ARNA\n(X,C)\nas illustrated\nin Figure 6.\nThe RNA posterior probability, P(C|X), is defined as:\nPRNA(C|X) = X\nA\u2208ARNA\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|qt\u22121, ht) (4)\nwhere, as before it denotes the number of non-blank sym\u0002bols in the partial alignment sequence (a1, . . . , at\u22121), and\nqt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu\u0002ral network which summarizes the entire partial alignment\nsequence, where NN(\u00b7) represents a suitable neural network\n(an LSTM in [46]). Thus, RNA removes the one remaining\nconditional independence assumption of the RNN-T model,\nby conditioning on the sequence of previous non-blank labels\nas well as the alignment that generated them. However, this\ncomes at a cost: the exact computation of the log-likelihood in\nEq. (3) (and corresponding gradients) is intractable. Instead,\nRNA makes two simplifying assumption to ensure tractable\ntraining: by assuming that the model can only output a single\nlabel at each frame; and utilizing a straight-through estimator\nfor the alignment [50]. The latter constraint \u2013 allowing only a\nsingle label (blank or non-blank) at each frame \u2013 has also been\nexplored in the context of the monotonic RNN-T model [51].\nFinally, we note that the work in [52] further generalizes\nthe RNA model by employing two RNNs when defining the\nstate: a slow RNN (which corresponds to the sequence of\npreviously predicted non-blank labels), and a fast RNN (which\nalso conditions on the frames at which the non-blank labels\nwere output).\nC. Implicit Alignment Modeling Approaches\nOne of the main benefits of the explicit alignment ap\u0002proaches such as CTC, RNN-T, or RNA is that they result in\nASR models that are easily amenable to frame-synchronous\ndecoding6In this section, we discuss the attention-based\nencoder-decoder (AED) models (also known as, listen-attend\u0002and-spell (LAS)) [15], [16], [53], which employs the attention\nmechanism [43] to implicitly identify and model the portions\nof the input acoustics which are relevant to each output\nunit. These models were first popularized in the context of\nmachine translation [54]. Unlike explicit alignment modeling\napproaches, attention-based encoder-decoder models use an\nattention mechanism [43] to learn a correspondence between\nthe entire acoustic sequence and the individual labels. Such\nmodels support label-synchronous decoding, meaning that\nduring inference, each hypothesis in the beam is expanded\nby 1 label.\nIn the explicit alignment approaches presented in Sec\u0002tion III-B, during inference, the model continues to output\nsymbols until it has processed the final frame at which point\nthe decoding process is complete; similarly, during training,\nthe forward-backward algorithm aligns over all possible align\u0002ment sequences. Since an AED model processes the entire\nacoustic sequence at once, the model needs a mechanism\nby which it can indicate that it is done emitting all output\nsymbols. This is achieved by augmenting the set of outputs\nwith an end-of-sentence symbol, \u27e8eos\u27e9, so that the output\nvocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus,\nthe AED model, depicted in Figure 7, consists of an en\u0002coder network \u2013 which encodes the input acoustic frame\n6 By frame-synchronous decoding, we refer to the ability of the model to\nproduce output label for each input frame of speech. Models such as CTC,\nRNN-T, or RNA, support frame-synchronous decoding.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nSoftmax\nDecoder\nAttention\nEncoder H(X)\nFig. 7. An attention-based encoder decoder (AED) model [15], [16], [53].\nThe output distribution is conditioned on the decoder state, si (which\nsummarizes the previously decoded symbols), and the context vector, vi\n(which summarizes the encoder output based on the decoder state). In the\nseminal work of Chan et al., [16], for example, this is accomplished by\nconcatenating the two vectors, as denoted by the L symbol in the figure.\nsequence, X = (x1, . . . , xT\u2032 ), into a higher-level representa\u0002tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder\nwhich defines the probability distribution over the set of output\nsymbols, Ceos. Thus, given a paired training example, (X, C),\nwe denote by Ce = (c1, . . . , cL,\u27e8eos\u27e9), the ground-truth\nsymbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9\nsymbol. AED models compute the conditional probability of\nthe output sequence augmented with the \u27e8eos\u27e9 symbol as:\nP(Ce|X) = P(Ce|H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, vi)\n=\nL\nY\n+1\ni=1\nP(ci|si, vi) (5)\nwhere, vi corresponds to a context vector, which summarizes\nthe relevant portions of the encoder output, H(X), given\nthe sequence of previous predictions ci\u22121, . . . , c0; and, si\ncorresponds to the corresponding decoder state after outputting\nthe sequence of previous symbols, which is produced by\nupdating the decoder state based on the previous context vector\nand output label:\nsi = Decoder(vi\u22121, si\u22121, ci\u22121)\nThe symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol\nwhich serves as the first input to the attention-based decoder\nbefore it has produced any outputs. As can be seen in Eq. (5),\nan important benefit of AED models over models such as\nCTC or RNN-T is that they do not make any independence\nassumptions between model outputs and the input acoustics,\nTime\ns\ne\ne\nFig. 8. Unlike models such as RNN-T or CTC, AED models do not have\nexplicit alignment. However, it is possible to interpret the attention weights\n\u03b1t,i for a particular output symbol ci as an alignment weight which is\nrepresented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this\nrepresentation, the size of the circle and the darkness level are proportional\nto the corresponding attention weights; thus the total probability mass is the\nsame for each row. As illustrated above, the first few frames correspond to\nthe first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019:\nc3 = e.\nand are thus more general than the implicit alignment models,\nwhile being considerably easier to train and implement since\nwe do not have to explicitly marginalize over all possible\nalignment sequences. However, this comes at a cost: previ\u0002ously generated context vectors (which are analogous to the\ndecoded partial alignment in explicit alignment models) are\nnot revised as the decoding proceeds. Stated another way,\nwhile the encoder processing H(X) might be bi-directional,\nthe decoding process in AED models reveals a left-right\nasymmetry [55].\n1) Computing the Context Vector in AED Models: As\nwe mentioned before, the context vector, vi, is computed\nby employing the attention mechanism [43]. The central\nidea behind these approaches is to define a state vector si\nwhich corresponds to the state of the model after outputting\nc1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then\ndefines a score between the model state after outputting\ni \u2212 1 previous symbols, and each of the encoded frames in\nH(X). These scores can then be normalized using the softmax\nfunction to define a set of weights corresponding to each ht\nas:\n\u03b1t,i =\nexp {atten(ht, si)}\nPT\nt\n\u2032=1 exp {atten(ht\n\u2032 , si)}\nIntuitively, the weight \u03b1t,i represents the relevance of a\nparticular encoded frame ht when outputting the next symbol\nci, after the model has already output the symbols c1, . . . , ci\u22121,\nas illustrated in Figure 8. The context vector summarizes the\nencoder output based on the computed attention weights:\nvi =\nX\nt\n\u03b1t,iht\nA number of possible attention mechanisms have been\nexplored in the literature: the most common forms are called\n\u2018content-based attention\u2019, which include dot-product atten\u0002tion [16] and additive attention [43]. The content-based atten\u0002tion computes the attention score atten(ht, si) based on the\nrelevance between ht and si. However, the score does not\nconsider location information, i.e., it is determined by only the\ncontent, independent of the position. This can lead to incorrect\nattention weights with a large discrepancy against the previous\nsteps. Thus, location-based attention atten(si,fi,j ) has been\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nproposed [15], where fi,j is a convolutional feature vector\nextracted from \u03b1i\u22121, the attention weights in the previous step.\nThe hybrid attention, i.e., a combination of the content- and\nlocation-based attentions, has also been investigated in [15],\nshowing a higher accuracy than the separate ones. Besides,\nother location-based methods use a Gaussian (mixture) model\nestimated with sito obtain attention weights [56], [57].\nTransformer model [44] uses only content-based dot-product\nattention, but also takes location information into account\nthrough positional encoding. Apart from the specific choice\nof the attention mechanism, a common technique to improve\nperformance involves the use of multiple independent attention\nheads \u2013 v\n1\ni\n, . . . , v\nK\ni\n\u2013 which are then concatenated together\nto obtain the final context vector vi =\n\u0002\nv\n1\ni\n; . . . ; v\nK\ni\n\u0003\n, in\nthe so-called multi-head attention approach [44], or indeed\nby stacking together multiple attention-based layers in the\ntransformer decoder presented by Vaswani et al. [44].\nD. From Implicit to Explicit Alignment Modeling\nAED models, which make no conditional independence\nassumptions, are extremely powerful, often outperforming\nexplicit alignment E2E approaches such as CTC, or RNN\u0002T [41]. However, these models also have some significant\ndisadvantages, most notably that the models are typically non\u0002streaming: i.e., the models must process all acoustic frames\nbefore they can generate any output hypotheses. A somewhat\nrelated issue is that the models are extremely sensitive to\nthe length of the acoustic sequences, which requires special\nprocessing to be able to decode long-form audio [58]. There\nis a body of work that lies in between these two extremes:\nmodels such as the neural transducer [59], or those based on\nmonotonic alignments [60] and its variants (e.g., monotonic\nchunkwise alignments (MoChA) [61], monotonic infinite look\u0002back (MILK) [62] etc.) use an explicit alignment model, while\nalso utilizing an attention mechanism that allows the model\nto examine local acoustics in order to refine predictions. In\nother words, this corresponds to a class of streaming AED\nmodels. Generally speaking, these models are motivated by\nthe observation that speech (unlike tasks such as machine\ntranslation) exhibits a \u2018local\u2019 relationship between the encoded\nframes (assuming that the encoder is uni-directional) and\nthe output units; thus, unlike the general AED model which\ncomputes the context vector, vi, as a sum over all input\nframes ht, the various proposed models constrain this sum\nto be computed over a subset of frames to allow for streaming\ndecoding. In the context of our presentation, it is easiest to\nthink of these models as consisting of an underlying alignment\n(whether known or unknown) which can be used to perform\nstreaming inference.\nThe Neural Transducer (NT) [59] explicitly partitions the in\u0002put encoder frames into T W non-overlapping chunks of length\nW: HW\n1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; HW\nT W = [hT W +1, . . . , hT W W ],\nwhere T W =\n\u0006\nT\nW\n\u0007\n, and ht = 0 if t > T. Unlike the AED\nmodel which examines all encoded frames when computing\nthe context vector, the NT model is restricted to process\na single chunk at a time; the model only advances to the\nnext chunk when it outputs a special end-of-chunk symbol\n(analogous to \u27e8eos\u27e9 in the AED model); inference in the model\nterminates when the model has output the end-of-chunk sym\u0002bol in the final chunk HW\nT W . If the alignments of the ground\u0002truth output sequence, C, with respect to the W-length chunks\nare unknown, then it is possible to train the system by using a\nrough initial alignment where symbols are distributed equally\namong the T W chunks, followed by iterative refinement by\ncomputing the most likely output alignments given the current\nmodel parameters [59] similar to forced-alignments in HMM\u0002based systems. An alternate approach [63] consists of using a\nseparate system (e.g., a classical hybrid system) to get initial\nalignments (e.g., word-level alignments), which can be used\nto assign sub-word units to the individual chunks.\nAn alternative approach, proposed by Raffel et al. [60],\nmodifies the vanilla AED model by explicitly introducing an\nalignment module which scans the encoder frames, H(X),\nfrom left-to-right to identify whether the current frame should\nbe used to emit any outputs (modeled as a Bernoulli random\nvariable). If a frame, \u03c4 , is selected, then the model produces\nan output based on the local encoder frame, h\u03c4 . The process\nis then repeated starting from the currently selected frame,\nthus allowing multiple outputs to be generated at the same\nframe. This results in a model with hard monotonic alignments\nbetween the input speech and the output labels since the\nmodels are constrained to generate outputs in a streaming fash\u0002ion. A Monotonic Chunkwise Attention (MoChA) model [61]\nimproves upon the work of Raffel et al., by allowing the model\nto generate the next output using a context vector computed\nusing attention over a local window of frames to the left of the\nselected frame \u03c4 : h\u03c4\u2212W+1, . . . , h\u03c4 . Thus, the MoChA model\nconsists of a two-level process \u2013 identifying frames where\noutput should be produced following [60], followed by an\nAED model over frames to the left of the selected frame. A\nrefinement to the MoChA model, proposed by Arivazhagan et\nal. [62] \u2013 the monotonic infinite lookback (MILK) attention\nmodel \u2013 computes the context vector over all frames to the\nleft of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step.\nAnother two-fold approach to enable streaming operation is\npresented in [64] under the term of triggered attention, where\na CTC-network is used to trigger, i.e. control the activation\nof an AED model with a limited decoder delay. We also\ndirect interested readers to studies of various attention variants:\nMerboldt et al. [65] compare a number of local monotonic\nattention variants; Zeyer et al. [66] discuss segmental attention\nvariants; Zeyer et al. [67] study the related decoding and the\nrelevance of segment length modeling, leading to improved\ngeneralization towards long sequences. Segmental attention\nmodels are related to transducer models [68]. However, seg\u0002mental E2E ASR models are not limited to be realized based\non the attention mechanism and may not only be related to a\ndirect HMM [39], but have also been shown to be equivalent\nto neural transducer modeling [40], thus even providing a clear\nrelation between duration modeling and blank probabilities.\nRelationship to Classical ASR\nIn classical ASR models, these frame-level alignments can\nbe modeled with HMMs while using generative GMMs or\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nneural networks to model the output distribution of acous\u0002tic frames; frame-level alignments to train neural network\nacoustic models may be obtained by force-alignment from a\nbase GMM-HMM systems, but direct sequence training not\nrequiring initial alignments is also possible [69].\nE2E models introduce alternative alignment modeling ap\u0002proaches to ASR. While the attention mechanism provides\na qualitatively novel approach to map acoustic observation\nsequences to label sequences, transducer approaches [13], [14],\n[46], [70] handle the alignment problem in a way that can\nbe interpreted to be similar to HMMs with a specific model\ntopology, including marginalization over alignments [71], [72],\n[73]. CTC models can also be employed in an HMM-like fash\u0002ion during decoding [74]. Moreover, transducer approaches are\nequivalent to segmental models/direct HMM [40].\nAnother prominent feature of E2E systems besides the\nalignment property is their direct character-level modeling\navoiding phoneme-based modeling and pronunciation lexica\n[19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82],\nwith some even heading for whole-word modeling [76], [30].\nHowever, character-level modeling also is viable with classical\nhybrid HMM architectures [83] and has been shown to work\neven with standard HMM models w/o neural networks [84].\nIV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E\nMODELS\nIn this section, we describe various algorithmic changes\nto vanilla E2E models which are critical in order to obtain\nimproved performance over classical ASR systems. First, we\ndescribe various ways of combining different complementary\nE2E models to improve performance. Next, we introduce\nways to incorporate context into these models to improve\nperformance on rare proper noun entities. We then describe\nimproved encoder and decoder architectures that take better\nadvantage of the many cores on specialized architectures such\nas tensor processing units (TPUs) [85]. Finally, we discuss how\nto improve the latency of the model through an integrated E2E\nendpointer.\nA. Combinations of Models\nDifferent end-to-end models are complementary, and there\nhave been numerous attempts at combining these methods.\nFor example, Watanabe et al. [86] find that attention-based\nmodels perform poorly on long or noisy utterances, mainly\nbecause the model has too much flexibility in predicting\nalignments when presented with the entire input utterance.\nIn contrast, models such as CTC \u2013 which have left-to-right\nconstraints during decoding \u2013 perform much better in these\ncases. They propose to employ a multi-task learning strategy\nwith both CTC and attention-based losses, which provides a\n5\u201314% relative improvement in word error rate over attention\u0002based models on Wall Street Journal (WSJ) and Chime tasks.\nPang et al. [87] explore combining the benefits of RNN-T\nand AED. Specifically, RNN-T decodes utterances in a left\u0002to-right fashion, which works well for long utterances. On\nthe other hand, since AED sees the entire utterance, it often\nshows improvements for utterances where surrounding context\nis needed to predict the current word, e.g., \"one dollar\nand fifty cents\" \u2192 $1.50. To combine RNN-T and\nAED, the authors propose to produce a first-pass result with\nRNN-T, that is then rescored with AED in the second-pass.\nTo reduce computation, the authors share the encoder between\nRNN-T and AED. The authors find that RNN-T + AED\nprovides a 17\u201322% relative improvement in word error rate\nover RNN-T alone on a voice search task. Other flavors\nof streaming 1st-pass following by attention-based 2nd-pass\nrescoring, such as deliberation [88], have also been explored.\nOne of the issues with such rescoring approaches is that any\npotential improvements are limited to the lattice produced by\nthe 1st-pass system. To address this, methods which run a\n2nd-pass beam search have also been explored, particularly in\nthe context of streaming ASR \u2013 e.g. cascaded encoder [89],\nY-architecture [90] and Universal ASR [91].\nB. Incorporating Context\nContextual biasing to a specific domain, including a user\u2019s\nsong names, app names and contact names, is an impor\u0002tant component of any production-level automatic speech\nrecognition (ASR) system. Contextual biasing is particularly\nchallenging in E2E models because these models typically\nretain only a small list of candidates during beam search, and\ntend to perform poorly when recognizing words that are seen\ninfrequently during training (typically named entities), which\nis the main source of biasing phrases. There have been a few\napproaches in the literature to incorporate context.\nOne approach, known as shallow-fusion contextual bias\u0002ing [92], constructs a stand-alone weighted finite state trans\u0002ducer (FST) representing the biasing phrases. The scores from\nthe biasing FST are interpolated with the scores of the E2E\nmodel during beam search, with special care taken to ensure\nwe do not over- or under-bias phrases. An alternate approach\nproposes to inject biasing phrases into the model in an all\u0002neural fashion. For example, Pundak et al. [93] represent a\nset of biasing phrases by embedding vectors. These vectors\nare fed as additional input to an attention-based model, which\ncan then choose to attend to the phrases and hence boost the\nchances of predicting the phrases. Kim and Metze [94] propose\nto bias towards dialog context. In addition, Bruguier et al. [95]\nextend [93], by leveraging phonemic pronunciations for the\nbiasing phrases when constructing phrase embeddings. Finally,\nDelcroix et al. [96] use an utterance-wise context vector like an\ni-vector computed by a pooling across frame-by-frame hidden\nstate vectors obtained from a sub network (this sub-network\nis called a sequence-summary network).\nC. Encoder and Decoder Structure\nThere have been improvements to encoder architectures\nof E2E models over time. The first end-to-end models used\nlong short-term memory recurrent neural networks (LSTMs),\nfor both the encoder and decoder. The main drawback of\nthese sequential models is that each frame depends on the\ncomputation from the previous frame, and therefore multiple\nframes cannot be batched in parallel.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nWith the improvement of hardware, specifically on-device\nEdge Tensor Processing Units (TPUs), with thousands of\ncores, architectures that can better take advantage of the\nhardware, have been explored. Such architectures include\nconvolution-based architectures, such as ContextNet [97]. The\nuse of self-attention to replace the sequential recurrence\nin LSTMs was explored in Transformers for ASR [98],\n[99], [100]. Finally, combining self-attention with convolution,\nknown as Conformer [45], or multi-layer perceptron [101],\nwas also explored. Both Transformer and Conformer have\nshown competitive performance to LSTMs on many tasks\n[102], [103].\nOn the decoder side, research for transducer models has\nshown that a large LSTM decoder can be replaced with a sim\u0002ple embedding lookup table, that attends to only a few previous\ntokens from the model [47], [104], [105], [106], [107]. This\ndemonstrates that most of the power of the E2E model is in\nthe encoder, which has been a consistent theme of both E2E\nas well as classical hybrid HMM models. However, improved\ndecoder modeling may also be effective depending on the\nspecific downstream task. Research has shown that extended\ndecoder architectures enable pre-training and adaptation of the\ndecoder using extensive text-only data, leading to accuracy\ngains [108], [109]. For example, one approach separates RNN\u0002T\u2019s prediction network into separate blank and vocabulary\nprediction (LM) components, where the LM component can\nbe trained with text data [108]. In addition, in line with the\ngrowing interest in large language models in recent years,\nresearch has also begun on solving multiple tasks, including\nspeech recognition, using only an auto-regressive, GPT-style\ndecoder [110], [111].\nD. Integrated Endpointing\nAn important characteristic of streaming speech recognition\nsystems is that they must endpoint quickly, so that the ASR\nresult can be finalized and sent to the server for the appro\u0002priate action to be performed. Endpointing is typically done\nwith an external voice-activity detector. Since endpointing is\nboth an acoustic and language model decision, recent works\nin streaming RNN-T models [112], [113] have investigated\npredicting a microphone closing token \u27e8eos\u27e9 at the end of the\nutterance \u2013 e.g., \u201cWhat\u2019s the weather \u27e8eos\u27e9\u201d. Following the\nnotation from Section III, this is done by including an \u27e8eos\u27e9\ntoken as part of the set of class labels C and encouraging\nthe model to predict this token to terminate decoding. These\nmodels have shown improved latency and WER trade-off\nby having the endpointing decision predicted as part of the\nmodel. Furthermore, [114], [115] explored using the CTC\nblank symbol for endpoint detection.\nV. TRAINING E2E MODELS\nIn general, training of E2E models follows deep learn\u0002ing schemes [116], [117], with specific consideration of the\nsequential structure and the latent alignment problem to be\nhandled in ASR. E2E ASR models may be trained end-to\u0002end, notwithstanding potential elaborate training schedules\nand extensive data augmentation. Part of the appeal of end\u0002to-end models is that they do not assume conditional in\u0002dependence between the input frames. Given a training set\nT = {(Xn, Cn)}\nN\nn=1, the training criterion L to be minimized\ncan be written as: L = \u2212\nPN\nn=1 log P(Cn|Xn) (which is\nequivalent to maximizing the total conditional log-likelihood).\nA. Alignment in Training\nE2E models such as RNN-T and CTC introduce an addi\u0002tional blank token \u27e8b\u27e9 for alignment. Therefore optimization\nimplies marginalizing across all alignments, as follows:\nLex = \u2212\nX\nN\nn=1\nX\nAn\nlog P(Cn, An|Xn)\nThis requires the forward-backward algorithm [118], [119] for\nefficient computation of the training criterion and its gradient,\nwith minor modifications for CTC, RNN-T, and RNA models,\nas well as classical (full-sum) hybrid ANN/HMMs correspond\u0002ing to the differences in alignments defined in each of these\nmodels. In comparison, AED models are based on implicit\nalignment modeling approaches, and the training criterion does\nnot have a latent variable A for explicit alignment as:\nLim = \u2212\nX\nN\nn=1\nlog P(Cn|Xn)\nWe refer the interested reader to the individual papers for\nfurther details on the training algorithms [13], [14], [15], [16],\n[46], [48], [53], [71], [120]. As shown in Section III-A, in both\nexplicit and implicit alignment cases, P(C|X) is factorized\nwith respect to input time t and output position i, respectively,\nand the factorized distribution is conditioned on the label\ncontext c\ni\u22121\n1\n, except for CTC. For example, in the AED case:\nlog P(C|X) = PL\ni=1 log P(ci\n|X, ci\u22121\n1\n). During training, we\nuse a teacher-forcing technique where the ground truth history\nis used as a label context.\nAs part of the training procedure, all E2E as well as classical\nhidden Markov models for ASR provide mechanisms to solve\nthe underlying sequence alignment problem - either explicitly\nvia corresponding latent variables, as in CTC, RNN-T or RNA,\nand also hybrid ANN/HMM, or implicitly, as in AED models.\nAlso, the distinction between speech and silence needs to be\nconsidered, which may be handled explicitly by introducing\nsilence as a latent label (hybrid ANN/HMM), or implicitly\nby not labeling silence at all, as currently is the standard in\nvirtually all E2E models.\nE2E models also may take advantage of hierarchical training\nschedules. These schedules may comprise several separate\ntraining passes and explicit, initially generated alignments that\nare kept fixed for some Viterbi-style [121], [122], [123] train\u0002ing epochs before re-enabling E2E-style full-sum training that\nmarginalizes over all possible alignments. Such an alternative\napproach is employed by Zeyer et al. [52], where an initial\nfull-sum RNN-T model is used to generate an alignment and\ncontinue with framewise cross-entropy training. This greatly\nsimplifies the training process by replacing the summation\nover all possible alignments in Eq. (4) by a single term cor\u0002responding to the alignment sequence generated. Recently, a\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nsimilar procedure has been introduced in [124] also employing\nE2E models, only. In this work, CTC is used to initialize\nthe training and to generate an initial alignment, followed by\nintermediate Viterbi-style RNN-T training and final full-sum\nfine tuning, which improved convergence compared to full\u0002sum-only training approaches.\nIt is interesting to note that in contrast to the RNN-T and\nRNA label-topologies, CTC does not require alignments with\nsingle label emissions per label position. However, training\nCTC models eventually does lead to single label emissions\nper hypothesized label. An analysis of this property of CTC\ntraining which is usually called peaky behavior can be found\nin [125] and references therein. Laptev et al. [126] even\nintroduces a CTC variant without non-blank loop transitions.\nB. Training with External Language Models\nE2E ASR models generally are normalized on sequence\nlevel. Therefore, sequence training with the maximum mutual\ninformation criterion [127] is the same as standard cross en\u0002tropy/conditional likelihood training. However, once external\nlanguage models are included in the training phase, sequence\nnormalization needs to be included explicitly, leading to MMI\nsequence discriminative training. This has been exploited as\na further approach to combine E2E models with external\nlanguage models trained on text-only data during the training\nphase itself [128], [129], [130].\nC. Minimum Word Error Rate Training\nSince the objective of speech recognition is to minimize\nword error rate (WER), there has been a growing number of\nresearch studies that incorporate this into the objective function\nby minimizing the model-based expectation of the number of\nword errors, as follows:\nLmwer =\nX\nN\nn=1\nX\nC\u2032\nn\nW(Cn, C\u2032\nn\n)P(C\n\u2032\nn\n|Xn)\nwhere W(Cn, C\u2032\nn\n) is the word error count in a hypothesis\nC\n\u2032\nn given a reference Cn, and n is an index which iterates over\nthe entire training set. These methods, known as sequence\nor discriminative training, have shown great improvements\nfor classical ASR [131], [132], [133], [134], [135], and have\nsince been explored in E2E models. Typically these losses\nare constructed by running in \u2018beam-search\u2019 mode rather than\nteacher-forcing mode, and construct a loss from the errors\nmade from the candidate hypotheses in the beam. Thus, this\ntype of training first requires training the model to optimize\nP(C|X) in order to initialize the model with a good set\nof parameters to run a beam search. However, also direct\napproaches have been introduced that avoid this separation\nto train discriminatively from scratch [69], [136].\nPapers that explore penalizing word errors include, Mini\u0002mum Word Error Rate (MWER) training [137], where the loss\nfunction is constructed such that the expected number of word\nerrors are minimized. Further work includes MWER for RNN\u0002T and self-attention-T [138], as well as MWER using prefix\nsearch instead of n-best [139]. Also, there have been studies\nthat consider MWER in terms of reinforcement learning [140],\n[141]. Optimal Completion Distillation (OCD) [81] proposes\nto minimize the total edit distance using an efficient dynamic\nprogramming algorithm. Finally, another body of research with\nsequence training introduce a separate external language model\nat training time [142], which can also be done efficiently via\napproximate lattice recombination [129] and also lattice-free\napproaches [130].\nD. Pretraining\nAll E2E models as well as classical hidden Markov models\nfor ASR provide holistic models that in principle enable train\u0002ing from scratch. However, many strategies exist to initialize\nand guide the training process to reach optimal performance\nand/or to obtain efficient convergence by applying pretrain\u0002ing and model growing [143], [144]. Supervised layer-wise\npretraining has been successfully applied for classical [5],\n[145], as well as attention-based ASR models [146], which can\nbe combined with intermediate sub-sampling schemes [147],\nand model growing [148]. Pretraining approaches utilizing\nuntranscribed audio, large-scale semi-supervised data and/or\nmultilingual data [149], [150], [151], [152], [153], [154],\n[155], [156], [157], [158], [159], [160] would deserve a\nself-contained survey and they are applicable for hybrid\nDNN/HMM and E2E approaches likewise \u2013 they will not be\nfurther discussed here.\nE. Training Schedules and Curricula\nDedicated training schedules have been developed to guide\nthe optimization process and as part of that reach proper\nalignment behavior explicitly or implicitly [52], [124], [147].\nMany approaches exist for learning rate control [161], [162]:\nNewBob [163], [164] and enhancements [162]; global ver\u0002sus parameter-wise learning rate control (exponential decay,\npower decay, etc.) [165]; learning rate warm-up [44]; warm\nrestarts/cosine annealing [166]; weight decay versus gradually\ndecreasing batch size [167]; fine-tuning [168] or population\u0002based training [169]; etc. For a survey of meta learning\ncf. [170].\nSequence learning approaches also consider curriculum\nlearning [171], [172], e.g., by considering short sequences\nfirst [173], [174]; interim increase of sub-sampling [147]\ninitially more sub-sampling; or, for multi-speaker ASR training\nsort mixed speech by SNR and start with speakers of balanced\nenergy and mixed gender [175].\nF. Optimization and Regularization\nOptimization usually is based on stochastic gradient descent\n[176], with momentum [177], [178], and a number of corre\u0002sponding adaptive approaches, most prominently Adam [179]\nand variants thereof [145], [179], [180].\nInvesting more training epochs seems to provide improve\u0002ments [52, Table 8], and also averaging over epochs has been\nreported to help [102]. For a discussion of the double descent\neffect and its relation to the amount of training data, label\nnoise and early stopping cf. [181].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nRegularization strongly contributes to training performance:\ne.g., L2 and weight decay [182], [166]; weight noise [183];\nadaptive mean L1/L2 [184]; gradient noise [185]; dropout\n[186], [187], [188], layer dropout [189], [190], [191]; dropcon\u0002nect [192]; zoneout [193]; smoothing of attention scores [15];\nlabel smoothing [194]; scheduled sampling [195]; auxiliary\nloss [194], [196]; variable backpropagation through time [197],\n[198]; mixup [199]; increased frame rate [180]; or, batch\nnormalization [200].\nG. Data Augmentation\nTraining of E2E ASR models also benefit from data aug\u0002mentation methods, which might also be viewed as regu\u0002larization methods. However, their diversity and impact on\nperformance justifies a separate overview.\nMost data augmentation methods perform data perturbation\nby exploiting certain dimensions of speech signal variation:\nspeed perturbation [201], [202], vocal tract length perturbation\n[201], [203], frequency axis distortion [201], sequence noise\ninjection [204], SpecAugment [205], or semantic mask [206].\nAlso, text-only data may be used to generate data using text\u0002to-speech (TTS) on feature [207] or signal level [208]. In a\ncomparison of the effect of TTS-based data augmentation on\ndifferent E2E ASR architectures in [208], AED seemed to be\nthe only architecture that appeared to benefit significantly from\nthe TTS data.\nIn a recent study [174] and corresponding follow-up work\n[180], many of the regularization and data augmentation\nmethods listed here have been exploited jointly leading to\nstate-of-the-art performance on the Switchboard task for a\nsingle-headed AED model.\nRelationship to Classical ASR\nE2E systems attempt to define ASR models that integrate\nall knowledge sources into a single global joint model that\ndoes not utilize secondary knowledge sources and avoids the\nclassical separation into acoustic and language models. These\nglobal joint models are completely trained from scratch using\na single global training criterion based on a single kind of\n(transcribed) training data and thus require less ASR domain\u0002specific knowledge provided sufficient amounts of training\ndata are available.\nWhile standard hybrid ANN/HMM training for ASR using\nframe-wise cross entropy already is discriminative, it is not\nyet sequence discriminative, requires prior alignments and\nalso lacks consideration of an (external) language model\nduring training. However, these potential shortcomings may\nbe remedied by using sequence discriminative training criteria\n[127] and lattice-free training approaches [69].\nIn contrast to strict E2E systems, the classical ASR ar\u0002chitecture includes the use of secondary knowledge sources\nbeyond the primary training data, i.e. (transcribed) speech\naudio for acoustic model training, and textual data for language\nmodel training. Most prominently, this includes the use of a\npronunciation lexicon and the definition of a phoneme set.\nSecondary resources like pronunciation lexica may be helpful\nin low-resource scenarios. However, their generation often is\ncostly and may even introduce errors, like pronunciations from\na lexicon not reflecting the actual pronunciations observed.\nTherefore, for large enough training resources, secondary\nknowledge sources might become obsolete [209], or even\nharmful, in case of erroneous information introduced [210],\n[211].\nClassical ASR models usually are trained successively, with\nknowledge derived from models trained earlier injected into\nlater training stages, e.g. in the form of HMM state alignments.\nHowever, such approaches from classical ASR might also\nbe interpreted as specific training schedules. Initializing deep\nlearning models using HMM alignments obtained from acous\u0002tic models based on mixtures of Gaussians may be interpreted\nin this way, with the Gaussian mixtures serving as an initial\nshallow model. In classical ASR, also approaches training\ndeep neural networks from scratch while avoiding interme\u0002diate training of Gaussians has been proposed [212], [213],\n[214], also in combination with character-level modeling [83].\nAnother step towards more integrated training of classical\nsystems has been to apply discriminative training criteria\navoiding intermediate (usually lattice-based) representations of\ncompeting word sequences [215], [69], [216], [217], [136].\nThe training of classical ASR systems usually applies sec\u0002ondary objectives to solve subtasks like phonetic clustering.\nThe classification and regression trees (CART) approach is\nused to cluster triphone HMM states [27], [218]. More re\u0002cent approaches proposed clustering within a neural network\nmodeling framework, while still retaining secondary clustering\nobjectives [219], [213]. However, also in E2E approaches\nsecondary objectives are used, most prominently for subword\ngeneration, e.g. via byte-pair encoding [32]. Also, available\npronunciation lexica can be utilized indirectly for assisting\nsubword generation for E2E systems [35], [36], which are\nshown to outperform byte-pair encoding. Within classical ASR\nsystems, phonetic clustering also can be avoided completely\nby modeling phonemes in context directly [220].\nIt is interesting to observe that specifically attention-based\nencoder-decoder models require larger numbers of training\nepochs to reach high performance, e.g. for a comparison\nof systems trained on Switchboard 300h cf. Table 5 in\n[221]. Also, attention-based encoder-decoder models have\nbeen shown to suffer from low training resources [222], [223],\nwhich can be improved by a number of approaches, including\nregularization techniques [174] as well as data augmentation\nusing SpecAugment [224] and text-to-speech (TTS) [29].\nSpecAugment also is shown to improve classical hybrid HMM\nmodels [225]. TTS on the other hand considerably improved\nattention-based encoder-decoder models trained on limited\nresources, but did not reach the performance of other E2E\napproaches or hybrid HMM models, which in turn were not\nconsiderably improved by TTS [208]. Multilingual approaches\nalso help improve ASR development for low resource tasks,\nagain both for classical [226], as well as for E2E systems\n[227], [228].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nVI. DECODING E2E MODELS\nThis section describes several decoding algorithms for end\u0002to-end speech recognition. The basic decoding algorithm of\nend-to-end ASR tries to estimate the most likely sequence C\u02c6\namong all possible sequences, as follows:\nC\u02c6 = arg max\nC\u2208U\u2217\nP(C|X)\nThe following section describes how to obtain the recognition\nresult C\u02c6.\nA. Greedy Search\nThe Greedy search algorithm is mainly used in CTC, which\nignores the dependency of the output labels as follows:\nA\u02c6 =\nY\nT\nt=1\n\u0012\narg max\nat\nP(at|X)\n\u0013\nwhere at is an alignment token introduced in Section III-B1.\nThe original character sequence is obtained by converting\nalignment token sequence A\u02c6 to the corresponding token se\u0002quence C\u02c6. The argmax operation can be performed in parallel\nover input frame t, yielding fast decoding [13], [229], although\nthe lack of the output dependency causes relatively poor\nperformance than the attention and RNN-T based methods in\ngeneral.\nCTC\u2019s fast decoding is further boosted with transformer\n[44], [98], [102] and its variants [45], [103] since their entire\ncomputation across the frames is parallelized [190], [230].\nFor example, the non-autoregressive models, including Im\u0002puter [231], Mask-CTC [230], Insertion-based modeling [232],\nContinuous integrate-and-fire (CIF) [233] and other variants\n[234], [235] have been actively studied as an alternative non\u0002autoregressive model to CTC. [235] shows that CTC greedy\nsearch and its variants achieve 0.06 real-time factor (RTF)7\nby using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz. The\npaper also shows that the degradation of the non-autoregressive\nmodels from the attention/RNN-T methods with beam search\nis not extremely large (19.7% with self-conditioned CTC [234]\nversus 18.5 and 18.9% with AED and RNN-T, respectively).\nThe greedy search algorithm is also used as approximate\ndecoding for both implicit and explicit alignment modeling\napproaches, including AED, RNA, CTC, and RNN-T, as\nfollows:\nc\u02c6i = arg max\nci\nP(ci|C\u02c6\n1:i\u22121, X) for i = 1, . . . , N\na\u02c6t = arg max\nat\nP(at|A\u02c6\n1:t\u22121, X) for t = 1, . . . , T\nThe greedy search algorithm does not consider alternate\nhypotheses in a sequence compared with the beam search\nalgorithm described below. However, it is known that the\ndegradation of the greedy search algorithm is not very large\n[16], [46], especially when the model is well trained in\nmatched conditions8.\n7 The ratio of the actual decoding time to the duration of the input speech.\n8 On the other hand, in the AED models, increasing the search space does\nnot consistently improve the speech recognition performance [77], [236] \u2013 a\nfact also observed in neural machine translation [237].\nB. Beam Search\nThe beam search algorithm is introduced to approximately\nconsider a subset of possible hypotheses C\u02dc among all possible\nhypotheses U\n\u2217 during decoding, i.e., C \u2282 U \u02dc \u2217\n. A predicted\noutput sequence C\u02c6 is selected among a hypothesis subset C\u02dc\ninstead of all possible hypotheses U\n\u2217\n, i.e.,\nC\u02c6 = arg max\nC\u2208C\u02dc\nP(C|X) (6)\nThe beam search algorithm is to find a set of possible hy\u0002potheses C\u02dc, which can include promising hypotheses efficiently\nby avoiding the combinatorial explosion encountered with all\npossible hypotheses U\n\u2217\n.\nThere are two major beam search categories: 1) frame\nsynchronous beam search and 2) label synchronous beam\nsearch. The major difference between them is whether it\nperforms hypothesis pruning for every input frame t or every\noutput token i. The following sections describe these two\nalgorithms in more detail.\nC. Label Synchronous Beam Search\nSuppose we have a set of partial hypotheses up to (i \u2212 1)th\ntoken C\u02dc\n1:i\u22121. A set of all possible partial hypotheses up to ith\ntoken C1:iis expanded from C\u02dc\n1:i\u22121 as follows:\nC1:i = {(C\u02dc\n1:i\u22121, ci = c)}c\u2208U (7)\nThe number of hypotheses |C1:i| would be |C\u02dc\n1:i\u22121| \u00d7 |U|, at\nmost. The beam search algorithm prunes the low probability\nscore hypotheses from C1:i and only keeps a certain number\n(beam size \u2206) of hypotheses at i among C1:i. This pruning\nstep is represented as follows:\nC\u02dc\n1:i = NBESTC1:i\u2208C1:i P(C1:i\n|X), where |C\u02dc\n1:i\n| = \u2206 (8)\nNote that NBEST(\u00b7) is an operation to extract top \u2206 hypothe\u0002ses in terms of the probability score P(C1:i\n|X) computed from\nan end-to-end neural network, or a fusion of multiple scores\ndescribed in Section VII-B.\nIn the label synchronous beam search, the length of the out\u0002put sequence (N) is unknown. Therefore, during this pruning\nprocess, we also add the hypothesis that reaches the end of an\nutterance (i.e., predict the end of sentence symbol \u27e8eos\u27e9) to a\nset of hypotheses C\u02dc in Eq. (6) as a promising hypothesis.\nThe label synchronous beam search does not explicitly\ndepend on the alignment information; thus, it is often used\nin implicit alignment modeling approaches, including AED.\nDue to this nature, sequence hypotheses of the same length\nmight cover a completely different number of encoder frames,\nunlike the frame synchronous beam search, as pointed out by\n[40]. As a result, we observe that the scores of very short and\nlong segment hypotheses often become the same range, and\nthe beam search wrongly selects such hypotheses. [86] shows\nan example of such extreme cases, resulting in large deletion\nand insertion errors for short and long-segment hypotheses,\nrespectively. Thus, the label synchronous beam search requires\nheuristics to limit the output sequence length to avoid ex\u0002tremely long/short output sequences. Usually, the minimum\nand maximum length thresholds are determined proportionally\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nto the input frame length |X| with tunable parameters \u03c1min and\n\u03c1max as Lmin = \u230a\u03c1min|X|\u230b, Lmax = \u230a\u03c1max|X|\u230b. Although these\nare quite intuitive ways to control the length of a hypothesis,\nthe minimum and maximum output lengths depend on the\ntoken unit or type of script in each language. Another heuristic\nis to provide an additional score related to the output length\nor attention weights \u2013 e.g., a length penalty, and a coverage\nterm [77], [238]. The end-point detection [239] is also used to\nestimate the hypothesis length automatically. [236] redefines\nthe implicit length model of the attention decoder to take into\naccount beam search, resulting in consistent behavior without\ndegradation for increasing beam sizes.\nNote that there are several studies on applying label syn\u0002chronous beam search to explicit alignment modeling ap\u0002proaches. For example, label synchronous beam search al\u0002gorithms for CTC are realized by marginalizing all possible\nalignments for each label hypothesis [13]. [240] extends\nCIF [233] to produce label-level encoder representation and\nrealizes label synchronous beam search in RNN-T.\nD. Frame Synchronous Beam Search\nIn contrast to the label synchronous case in Eq. (8), the\nframe synchronous beam search algorithm performs pruning\nat every input frame t, as follows:\nC\u02dc\n1:i(t) = NBESTC1;i(t) P(C1;i(t)\n|X), where |C\u02dc\n1:i(t)\n| = \u2206\nwhere C1;i(t)is an i(t)-length label sequence obtained from\nthe alignment A1:t, which is introduced in Sec. III-B.\nP(C1;i(t)|X) is obtained by summing up all possible align\u0002ments A1:t \u2208 A(X,C1;i(t))\n. Unlike the label synchronous beam\nsearch, frame synchronous beam search depends on explicit\nalignment A; thus, it is often used for explicit alignment\nmodeling approaches, including CTC, RNN-T, and RNA.\nC1:i(t)is an expanded partial hypotheses up to input frame\nt, similar to Eq. (7).\nCompared with the label synchronous algorithm, the frame\nsynchronous algorithm needs to handle additional output to\u0002ken transitions inside the beam search algorithm. The frame\nsynchronous algorithm can be easily extended in online and/or\nstreaming decoding, thanks to the explicit alignment informa\u0002tion with input frame and output token.\nClassical approaches to beam search for HMM, but also\nCTC and RNN-T variants, are based on weighted finite state\ntransducers (WFST) [38], [74], [241] or lexical prefix trees\n[106], [242], [243]. They are categorized as frame synchronous\nbeam search. These methods are often combined with an N\u0002gram language model or a full-context neural language model\n[244], [245]. RNN-T [14], [246] and CTC prefix search [247]\ncan deal with a neural language model by incorporating the\nlanguage model score in the label transition state. Interestingly,\ntriggered attention approaches [248], [249] allow us to use\nimplicit alignment modeling approaches, including AED, in\nframe-synchronous beam search together with CTC and neural\nLM, which applies on-the-fly rescoring to the hypotheses given\nby CTC prefix search using the AED and LM scores.\nE. Block-wise Decoding\nAnother beam search implementation uses a fixed-length\nblock unit for the input feature. In this block processing, we\ncan use the future context inside the block by using the non\u0002causal encoder network based on the BLSTM, output-delayed\nunidirectional LSTM, or transformer (and its variants). This\nfuture context information avoids the degradation of the fully\ncausal network. In this setup, the chunk size becomes the\ntrade-off of controlling latency and accuracy. This technique is\nused in both RNN-T [100], [250], [251] and AED [61], [252],\n[253], [254]. Block-wise processing is especially important for\nimplicit alignment modeling approaches, including AED, since\nit can provide block-wise monotonic alignment constraint\nbetween the input feature and output label, and realize block\u0002wise streaming decoding.\nF. Model Fusion during Decoding\nSimilar to the classical HMM-based beam search, we com\u0002bine various scores obtained from different modules, including\nthe main end-to-end ASR and LM scores.\n1) Synchronous Score Fusion: The most simple score fu\u0002sion is performed when the scores of multiple modules are\nsynchronized. In this case, we can simply add the multiple\nscores at each frame t or label i. The most well-known score\ncombination is LM shallow fusion.\nLM shallow fusion: As discussed in Sec. VII, various neural\nLMs can be integrated with end-to-end ASR. The most simple\nintegration is based on LM shallow fusion [255][256][257], as\ndiscussed in Sec. VII-B1, which (log-) linearly adds the LM\nscore Plm(C1:i) to E2E ASR scores P(C1:i|X) during beam\nsearch in Eq. (8) as follows:\nlog P(C1:i|X) \u2192 log P(C1:i|X) + \u03b3 log Plm(C1:i)\nwhere \u03b3 is a language model weight. Of course, we can\ncombine other scores, such as the length penalty and coverage\nterms, as discussed in Sec. VI-C.u\n2) Asynchronous Score Fusion: If we combine the frame\u0002dependent score functions, P(at|\u00b7), used in explicit alignment\nmodeling approaches, e.g., CTC, RNN-T, and label-dependent\nscore functions, P(ci|\u00b7), used implicit alignment modeling\napproaches, e.g., AED, language model, we have to deal with\nthe mismatch between the frame and label time indices t and\ni, respectively.\nIn the time-synchronous beam search, this fusion is per\u0002formed by incorporating the language model score in the\nlabel transition state [70], [22], [258]. [247] also combines\na word-based language model and token-based CTC model\nby incorporating the language model score triggered by the\nword delimiter (space) symbol.\nIn the label-synchronous beam search, we first compute\nthe label-dependent scores from the frame-dependent score\nfunction by marginalizing all possible alignments given a\nhypothesis label sequence. CTC/attention joint decoding [86]\nis a typical example, where the CTC score is computed\nby marginalizing all possible alignments based on the CTC\nforward algorithm [229]. This approach eliminates the wrong\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nalignment issues and difficulties of finding the correct end of\nsentences in the label-synchronous beam search [86].\nNote that the model fusion method during beam search can\nrealize simple one-pass decoding, while it limits the time unit\nof the models to be the same or it requires additional dynamic\nprogramming to adjust the different time units, especially for\nthe label-synchronous beam search. This dynamic program\u0002ming computation becomes significantly large when the length\nof the utterance becomes larger and requires some heuristics\nto reduce the computational cost [259].\nG. Lexical Constraint during Score Fusion\nClassically, we use a word-based language model to cap\u0002ture the contextual information with the word unit, and also\nconsider the word-based lexical constraint for ASR. However,\nend-to-end ASR often uses a letter or token unit and it causes\nfurther unit mismatch during beam search. As described in\nprevious sections, the classical approach of incorporating the\nlexical constraint from the token unit to the word unit is\nbased on a WFST. This method first makes a TLG transducer\ncomposed of the token (T), word lexicon (L), and word-based\nlanguage transducers (G) [74]. This TLG transducer has been\nused for both CTC [74] and attention-based [53] models.\nAnother approach used in the time synchronous beam search\nis to insert the word-based language model score triggered by\nthe word delimiter (space) symbol [75]. To synchronize the\nword-based language model with a character-based end-to-end\nASR, [260] combines the word and character-based LMs with\nthe prefix tree representation, while [239], [261] uses look\u0002ahead word probabilities to predict next characters instead of\nusing the character-based LM. The prefix tree representation\nis also used for the sub-word token unit case [262], [263].\nH. Multi-pass Fusion\nThe previous fusion methods are performed during the\nbeam search, which enables a one-pass algorithm. The popular\nalternative methods are based on multi-pass algorithms where\nwe do not care about the synchronization and perform n-best or\nlattice scoring by considering the entire context within an ut\u0002terance. [16] uses the N-best rescoring techniques to integrate\na word-based language model. [55] combines forward and\nbackward searches within a multi-pass decoding framework to\ncombine bidirectional LSTM decoder networks. Recently two\u0002pass algorithms of switching different end-to-end ASR systems\nhave been investigated, including RNN-T \u2192 AED [264]; CTC\n\u2192 AED [265], [266]. This aims to provide streamed output in\nthe first pass and re-scoring with AED in the second pass to\nrefine the previous output, thus satisfying a real-time interface\nrequirement while providing high recognition performance.\nIn addition to the N-best output in the above discussion,\nthere is a strong demand for generating a lattice output\nfor better multi-pass decoding thanks to richer hypothesis\ninformation in a lattice. The lattice output can also be used\nfor spoken term detection, spoken language understanding,\nand word posteriors. However, due to the lack of Markov\nassumptions, RNN-T and AED cannot merge the hypothesis\nand cannot generate a lattice straightforwardly, unlike the\nHMM-based or CTC systems. To tackle this issue, there are\nseveral studies of modifying these models by limiting the\noutput dependencies in the fixed length (i.e., finite-history)\n[47], [267], or keeping the original RNN-T structure but\nmerging the similar hypotheses during beam search [107].\nI. Vectorization across both Hypotheses and Utterances\nWe can accelerate the decoding process by vectorizing\nmultiple hypotheses during the beam search, where we replace\nthe score accumulation steps for each hypothesis with vector\u0002matrix operations for the vectorized hypotheses. This has been\nstudied in RNN-T [22], [258], [268] and attention-based [259]\nmodels. This modification leverages the parallel computing\ncapabilities of multi-core CPUs, GPUs and TPUs, resulting in\nsignificant speedups, while enabling multiple utterances to be\nprocessed simultaneously in a batch. Major deep neural net\u0002work and end-to-end ASR toolkits support this vectorization.\nFor example, Tensorflow9[269], and FAIRESEQ10 [270] pro\u0002vide a vectorized beam search interface for a generic sequence\nto sequence task, and it can be used for attention-based end-to\u0002end ASR. End-to-end ASR toolkits including ESPnet11 [259],\nESPRESSO12[261], LINGVO [271], and, RETURNN13 [272]\nalso support the vectorized beam search algorithm.\nRelationship to Classical ASR\nOne of the most prominent properties shared between E2E\nand classical statistical ASR systems is the use of a single\u0002pass decoding strategy, which integrates all knowledge sources\ninvolved (models, components), before coming to a final\ndecision [123]. This includes the use of full label context\ndependency both for E2E systems [229], [51], [77], [273],\n[174], [262], [274], [275], as well as classical systems via full\u0002context language models [276], [244], [245], [277]. In classical\nASR systems, even HMM alignment path summation may be\nretained in search [278]. Both E2E as well as classical ASR\nsystems employ beam search in decoding. However, compared\nto classical search approaches, E2E beam search usually is\nhighly simplified with very small beam sizes around 1 to\n100 [15], [16], [77], [147]. Very small beam sizes also partly\nmask a length bias exhibited by E2E attention-based encoder\u0002decoder models [279], [280], thus trading model errors against\nsearch errors [281]. An overview of approaches to handle the\nlength bias beyond using small beam sizes in ASR is presented\nin [236].\nMany classical ASR search paradigms are based on mul\u0002tipass approaches that successively generate search space\nrepresentations applying increasingly complex acoustic and/or\nlanguage models [282], [283], [243]. However, multipass\nstrategies also are employed using E2E models, which how\u0002ever softens the E2E concept. Decoder model combination is\npursued in a two-pass approach, while even retaining latency\nconstraints as in [87]. Further multipass approaches include\nE2E adaptation approaches [284], [285], [286], [287].\n9 https://www.tensorflow.org/api docs/python/tf/contrib/seq2seq/BeamSearchDecoder\n10 https://github.com/pytorch/fairseq/blob/master/fairseq/sequence\ngenerator.py 11 https://github.com/espnet/espnet\n12 https://github.com/freewym/espresso\n13 https://github.com/rwth-i6/returnn\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nVII. LM INTEGRATION\nThis section discusses language models (LMs) used for E2E\nASR. Hybrid ASR systems have long been using a pretrained\nLM [2], whereas most end-to-end (E2E) ASR systems employ\na single E2E model that includes a network component acting\nas an LM.14 For example, the prediction network of RNN\u0002T and the decoder network of AED models take on the role\nof a LM covering label back-histories. Therefore, E2E ASR\ndoes not seem to require external LMs. Nevertheless, many\nstudies have demonstrated that external LMs help improve the\nrecognition accuracy in E2E ASR.\nThere are presumably three reasons that E2E ASR still\nrequires an external LM:\na) Compensation for poor generalization: E2E models\nneed to learn a more complicated mapping function than\nclassical modular-based models such as acoustic models. Con\u0002sequently, E2E models tend to face overfitting problems if\nthe amount of training data is not sufficient. Pretrained LMs\npotentially compensate for the less generalized predictions\nmade by E2E models.\nb) Use of external text data: E2E models need to be\ntrained using paired speech and text data, while LMs can\nbe trained with only text data. Generally, text data can be\ncollected more easily than the paired data. The training speed\nof an LM is also faster than that of E2E models for the same\nnumber of sentences. Accordingly, the LM can be improved\nmore effectively with external text data, providing additional\nperformance gain to the ASR system.\nc) Domain adaptation: Domain adaptation helps im\u0002prove recognition accuracy when the E2E model is applied\nto a specific domain. However, domain adaptation of the E2E\nmodel requires a certain amount of paired data in the target\ndomain. Also, when multiple domains are assumed, it may be\ncostly to maintain multiple E2E models for the domains the\nsystem supports. If a pretrained LM for the target domain is\navailable, it may more easily improve recognition accuracy for\ndomain-specific words and speaking styles without updating\nthe E2E model.\nThis section reviews various types of LMs used for E2E\nASR and fusion techniques to integrate LMs into E2E models.\nA. Language Models\nThe LMs provide a prior probability distribution, P(C). If\nthe sentence, C, can be decomposed into a sequence of tokens\nsuch as characters, subwords, and single words, the probability\ndistribution can be computed based on the chain rule as:\nP(C) =\nL\nY\n+1\ni=1\nP(ci|c0:i\u22121)\nwhere ci denotes the i-th token of C, and c0:i\u22121 represents\ntoken sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and\ncL+1 = \u27e8eos\u27e9.\nMost LMs are designed to provide the conditional probabil\u0002ity P(ci\n|c0:i\u22121), i.e., they are modeled to predict the next token\n14 In the simplest case of a CTC model as in Fig. 2, the included LM\ncomponent however is limited to a label prior without label context.\ngiven a sequence of the preceding tokens. We briefly review\nsuch LMs focusing on the different techniques to represent\neach token, ci, and back-history, c0:i\u22121.\n1) N-gram LM: N-gram LMs have long been used for\nASR [2]. Early E2E systems in [53], [74], [77] also employed\nan N-gram LM. The N-gram models rely on the Markov\nassumption that the probability distribution of the next token\ndepends only on the previous N\u22121 tokens, i.e., P(ci|c0:i\u22121) \u2248\nP(ci|ci\u2212N+1:i\u22121), where N is typically 3 to 5 for word-based\nmodels and higher for sub-word and character-based models.\nThe maximum likelihood estimates of N-gram probabilities\nare determined based on the counts of N sequential tokens in\nthe training data set as:\nP(ci|ci\u2212N+1:i\u22121) = K(ci\u2212N+1, . . . , ci)\nP\nci\nK(ci\u2212N+1, . . . , ci)\nwhere, K(\u00b7) denotes the count of each token sequence. Since\nthe data size is finite, it is important to apply a smoothing\ntechnique to avoid estimating the probabilities based on zero or\nvery small counts for rare token sequences. Those techniques\ncompensate the N-gram probabilities with lower order models,\ne.g., (N \u2212 1)-gram models, according to the magnitude of\nthe count [288]. However, since the N-gram probabilities\nstill rely on the discrete representation of each token and the\nhistory, they suffer from data sparsity problems, leading to\npoor generalization.\nThe advantage of the N-gram models is their simplicity,\nalthough they underperform state-of-the-art neural LMs. In the\ntraining, the main step is to just count the N tuples in the data\nset, which is required only once. During decoding, the LM\nprobabilities can be obtained very quickly by table lookup or\ncan be attached to a decoding graph, e.g., WFST, in advance.\n2) FNN-LM: The feed-forward neural network (FNN) LM\nwas proposed in [9], which estimates N-gram probabilities\nusing a neural network. The network accepts N \u2212 1 tokens,\nand predicts the next token as:\nP(ci|ci\u2212N+1:i\u22121) = softmax(Wohi + bo)\nhi = tanh(Whei + bh)\nei = concat(E(ci\u2212N+1), . . . , E(ci\u22121))\nwhere Wo and Wh are weight matrices, and bo and bh are\nbias vectors. E(y) provides an embedding vector of c, and\nconcat(\u00b7) operation concatenates given vectors 15. This model\nfirst maps each input token to an embedding space, and then\nobtains hidden vector, hi, as a context vector representing the\nprevious N \u22121 tokens. Finally, it outputs the probability distri\u0002bution of the next token through the softmax layer. Although\nthis LM still relies on the Markov assumption, it outperforms\nclassical N-gram LMs described in the previous section.\nThe superior performance of FNN-LM is primarily due to\nthe distributed representation of each token and the history.\nThe LM learns to represent token/context vectors such that\nsemantically similar tokens/histories are placed close to each\nother in the embedding space. Since this representation has a\nbetter smoothing effect than the count-based one used for N\u0002gram LMs, FNN-LM can provide a better generalization than\n15 We omit the optional direct connection from the embedding layer to the\nsoftmax layer in [9] for simplicity.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nN-gram LMs for predicting the next token. Neural network\u0002based LMs basically utilize this type of representation.\n3) RNN-LM: A recurrent neural network (RNN) LM was\nintroduced to exploit longer contextual information over N \u2212\n1 previous tokens using recurrent connections [289]. Unlike\nFNN-LM, the hidden vector is computed as:\nhi = recurrence(ei, hi\u22121)\nei = E(ci\u22121)\nwhere, recurrence(ei, hi\u22121) represents a recursive function,\nwhich accepts previous hidden vector hi\u22121 with input ei, and\noutputs next hidden vector hi. In the case of simple (Elman\u0002type) RNN, the function can be computed as\nrecurrence(e, h) = tanh(Whe + Wrh + bh)\nwhere, Wr is a weight matrix for the recurrent connection,\nwhich is applied to the previous hidden vector h. This recurrent\nloop makes it possible to hold the history information in the\nhidden vector without limiting the history to N \u2212 1 tokens.\nHowever, the history information decays exponentially as\ntokens are processed with this recursion. Therefore, currently\nstacked LSTM layers are more widely used for the recurrent\nnetwork, which have separate internal memory cells and gating\nmechanisms to keep long-range history information [290].\nWith this mechanism, RNN-LMs outperform other N-gram\u0002based models in many tasks.\n4) ConvLM: Convolutional neural networks (ConvLM)\nhave also been applied to LMs [291], [292], [293]. ConvLM\n[292] replace the recurrent connections used in RNN-LMs\nwith gated temporal convolutions. The hidden vector is com\u0002puted as\nhi =h\n\u2032\ni \u2297 \u03c3(gi)\nh\n\u2032\ni =ei\u2212k+1:i \u2217 W + b\ngi =ei\u2212k+1:i \u2217 V + c\nwhere \u2297 is element-wise multiplication, \u2217 is a temporal\nconvolution operation, and k is the patch size. \u03c3(gi) represents\na gating function of convoluted activation h\n\u2032\ni\n, and is modeled\nas a sigmoid function. W and V are matrices for convolution\nand b and c are bias vectors. The convolution and gating blocks\nare typically stacked multiple times with residual connections.\nIn [293], a ConvLM with 14 blocks has been applied for E2E\nASR. Similar to FNN-LM, ConvLM allow us to use only a\nfixed history size, but they are more parameter efficient and\neasier to utilize longer histories than the FNN-LM by stacking\nthe layers. Thus, they achieve competitive performance to that\nof RNN-LMs [292], even with the finite history consisting\nof short tokens such as characters [294]. Moreover, they are\nhighly parallelizable and thus suitable for training the model\nwith a large training data set.\n5) Transformer LM: Transformer architecture [44] has been\napplied to LMs [295] and used for ASR [102], [296], where\nthe LMs are designed as a Transformer decoder without any\ninputs from other modules such as encoders. The hidden vector\nis computed as:\nhi = FFN(h\n\u2032\ni\n) + h\n\u2032\ni\nh\n\u2032\ni = MHA(ei\n, e1:i, e1:i) + ei\nwhere FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network\nand a multi-head attention module, respectively. The multi\u0002head attention and feed-forward blocks are typically stacked\nmultiple times, e.g., 6 times [102], to obtain the final hidden\nvector. The advantage of Transformer LMs is that they can\ntake all tokens in the history into account through the self\u0002attention mechanism without summarizing them into a fixed\u0002size memory like RNN-LMs. Thus, the long history can be\nfully considered with attention to predict the next token,\nachieving better performance than RNN-LMs. However, the\ncomputational complexity increases quadratically as the length\nof the sequence. Therefore, the history length is typically\nlimited to a fixed size or within every single sentence. To\novercome this limitation, Transformer-XL [297] reuses already\ncomputed activations, which includes information on farther\nprevious tokens, and the model is trained with a truncated\nback-propagation through time (BPTT) algorithm [298]. Com\u0002pressive Transformer [299] extends this approach to utilize\neven longer contextual information by incorporating a com\u0002pression step to keep older, but important, information in a\nfixed-size memory network.\nB. Fusion Approaches\nThere are several ways to incorporate an external LM into\nE2E ASR, called LM fusion. Their purpose is to improve the\nrecognition accuracy of E2E ASR by leveraging the benefits\nof the external LM described in the first part of this section.\nHowever, there can be a mismatch in the prediction between\nthe E2E model and the LM when trained on different data\nsets, and therefore the LM may not collaborate well with\nthe E2E model. Researchers have investigated various LM\nfusion approaches to reduce the mismatch between models\nin different situations.\n1) Shallow Fusion: Shallow fusion is the most popular\napproach to combine the pretrained E2E model and LM in\nthe inference time. As we described in Sec. VI-F, shallow\nfusion simply combines the E2E and LM scores by a log\u0002linear combination as\nScore(C|X) = log P(C|X) + \u03b3 log P(C) (9)\nwhere \u03b3 is a scaling factor for the LM [255][256][257]. The\nadvantage of this approach is that it is easy and effective when\nthere are no major mismatches between the source and target\ndomains.\n2) Deep Fusion: Deep fusion [300] is an approach to\ncombine an LM with an E2E model using a joint network.\nGiven a pretrained E2E model and an LM, all the network\nparameters are fine-tuned jointly so that the models collaborate\nbetter to improve the recognition accuracy, where the joint\nnetwork is used to combine the E2E and LM states through\na gating mechanism that controls the contribution of the LM\naccording to the current state.\n3) Cold fusion: Cold fusion [301] is another approach to\ncombine a pretrained LM like deep fusion, but the E2E model\nis learned while freezing the LM parameters. Since the E2E\nmodel is aware of the LM throughout training, it learns to use\nthe LM to reduce language specific information and capture\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nonly the relevant information to map the source to the target\nsequence. This mechanism reduces the role of LM in the E2E\nmodel and alleviates the language bias of the training data.\nAccordingly, the E2E model becomes more robust to domain\nmismatches between the training data and the target domain.\nUnlike deep fusion, cold fusion makes it possible to combine\nthe E2E model with a pretrained LM for the target domain,\nimproving the recognition accuracy. Component fusion [302]\nextends cold fusion to use a pretrained LM with transcriptions\nof the training data for the E2E model, more focusing on\nreducing the bias of the training data.\n4) Internal LM Estimation: There is another approach to\nreduce language bias in training data through shallow fusion.\nThe language bias is a problem when a big domain mismatch\nexists between the source domain (training data) and the target\ndomain (test data) because the E2E model scores are strongly\ndependent on the language priors in the source domain. To\nremove such a bias from the score, we can explicitly estimate\nthe LM that represents the language priors, called Internal LM,\nand subtract the LM score from the ASR score of Eq. (9):\nScore(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C)\nwhere subscripts \u03c6 and \u03c4 indicate the source and target\ndomains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub\u0002tracting the internal LM score corresponds to approximating\nacoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d\nP\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR\nscore can be seen as a classical hybrid ASR system. Ac\u0002cordingly, the subtracted E2E model score plays a role of\nacoustic model and makes it more domain independent in\nterms of language, achieving a higher recognition accuracy\nin combination with the external LM P\u03c4 (C).\nThe density ratio method [303] trains an internal LM using\nthe transcript of the training data. Hybrid autoregressive trans\u0002ducer (HAT) [47] extends RNN-T so that the model becomes\nthe internal LM when the encoder output is eliminated, i.e., set\nto zero. This approach simplifies the framework by utilizing\nthe prediction network as the internal LM, which avoids\ntraining an additional LM and using it in the inference time.\nIn the work of [304], an approach similar to HAT has been\nproposed where the internal LM is formulated on top of\nstandard RNN-T and attention-based encoder-decoder models,\nrespectively. In [128], several techniques to estimate internal\nLMs have been proposed for AED models, where an estimated\nbias vector is fed to the LM instead of a zero vector. The bias\nvector can be estimated by averaging encoder states or context\nvectors, or by a small LSTM predicting the context vector\nbased on the decoder label context, only. These techniques to\nestimate the internal LM were also evaluated for RNN-T in\n[305].\nC. Use of Large-scale Pretrained LMs\nIn recent years, LMs trained with large-scale text data are\navailable for different NLP tasks. BERT [306] and GPT-2\n[307] are representative models based on Transformer LMs.\nSuch LMs have also been applied to E2E ASR systems in\ndifferent ways, e.g., N-best rescoring [308] and dialog context\nembedding [309].\nRelationship to Classical ASR\nThe architecture of classical ASR systems provides a sepa\u0002ration between the acoustic model and the language model.\nIn contrast to this, E2E models avoid this separation and\ndefine a joint model. While this allows for training with a\nsingle objective, it limits training of the (implicit) prior to\nthe transcriptions of the audio training data. To exploit further\ntext-only training data, usually a separate LM is combined with\nE2E models, nonetheless. However, due to the implicit prior\nof E2E models, i.e. the internal language model, combination\nwith separate language models is not straightforward and\nrequires corresponding internal language model estimation\nand compensation approaches, e.g. [303], [47], [304], [128],\n[310]. At least from the recognition accuracy perspective, it\nremains unclear, if the clear separation of acoustic modeling\nand language modeling in the classical ASR architecture is a\ndisadvantage because of separate training objectives, or rather\nan advantage, since text-only training data may be used easily.\nAlso, the language model training objective, i.e. language\nmodel perplexity, is observed to correlate well with word error\nrate [311], [312], [313], [314]. Furthermore, discriminative\napproaches to language modeling [315] may be viewed as a\nstep towards joint modeling.\nVIII. OVERALL PERFORMANCE TRENDS OF E2E\nAPPROACHES IN COMMON BENCHMARKS\nThis section summarizes various techniques with the com\u0002mon ASR benchmarks based on switchboard (SWBD) [316]\nin Figure 9 and Librispeech [317] in Figure 10 to see the\ntrajectory of the techniques developed in end-to-end ASR. We\nchoose these two databases because they are widely used in\nspeech and machine learning communities and cover sponta\u0002neous (SWBD) and read speech (Librispeech) speaking styles.\nFigures 9 and 10 show that the performance improvement\nrelative to the initial works [147], [79] based on the E2E\nmodels is significant, and the error rates of all tasks become\nless than half of the original error rates!16\nAlthough the overall trends show that the ASR performance\nhas steadily improved over time, there are several remarkable\ngains. One significant gain observed in both benchmarks in the\nmiddle of 2019 comes from the data augmentation method\nrepresented by SpecAugment [205], [206], as discussed in\nSection V-G. The subsequent gains mostly come from the\nexploration of the new neural network architectures, including\ntransformer [102], [318], conformer [45], [103], and contextnet\n[97] on top of SpecAugment, as discussed in Section IV-C.\nSuch an exploration is also performed in language modeling\nto improve the ASR performance [296], [102]. The final gain\nobserved in the Librispeech benchmark in 2021 is based\non self-supervised learning [25], [319] and semi-supervised\nlearning [320], [321]. These techniques utilize a considerable\n16 For readers who want to know the latest update of these\nbenchmarks can also check https://github.com/syhw/wer are we and\nhttps://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\nAEDAEDAEDAED\nHMM\nAEDAEDAEDAED AEDAED\nWER (%)\n0\n10\n20\n30\n40\n50\n1/1/2017 1/1/2018 1/1/2019 1/1/2020 1/1/2021\nswb chm\nFig. 9. E2E ASR performance improvement in the switchboard task. AED AED\nAEDAEDAED\ntransducer ContextNet Transducer\nCTC\nHMM\nAED transducer\nWER (%)\ntransformer\n0\n5\n10\n15\n1/1/2019 7/1/2019 1/1/2020 7/1/2020 1/1/2021 7/1/2021\ntest_clean test_other\nFig. 10. E2E ASR performance improvement in the Librispeech task.\namount of unlabeled in-domain speech data (e.g., Libri-light\n60K hours [322]).\nRelationship to Classical ASR\nSpeech recognition research has always been pushed by\ninternational evaluation campaigns (e.g. as lead by NIST)\nand corresponding benchmark tasks. The competition between\nclassical and E2E approaches is nicely reflected in the widely\nused Librispeech [317] and Switchboard [316] tasks, showing\nthat E2E models gain momentum. As shown in Figure 10,\non Librispeech, the current best-published classical hybrid\nsystems range around 2.3% (test-clean) and 4.9% (test-other)\nword error rate [323], [222], while there already are a number\nof E2E systems providing similar performance [224], [205],\n[320], [206], with some E2E systems clearly outperforming\nformer state-of-the-art results with word error rates down to\n1.8% (test-clean) and 3.7% (test-other) [324] with similar\nresults reported in [45], [97]. Merging insights from classical\nHMM-based and monotonic RNN-T provided similarly well\nresults with a limited training budget [124]. Finally, when\ntrained on Switchboard 300h, the current best result, obtained\nwith an E2E system [180] is 5.4% compared to 6.6% word\nerror rate for the best hybrid system result [325] on the\nHUB5\u201900 Switchboard test set, in Figure 9\nIX. DEPLOYMENT OF E2E MODELS\nMany of the ideas discussed in this paper have been\nexplored by various industry research labs [326], [327], [328],\n[329], [330], [331], [265], inter alia. In this section, we\nreview the development of on-device production-level systems\nat Google as a typical case study for deployment.\nThe first streaming E2E model, deployed to production,\nwas launched in 2019 for the Pixel 4 smartphone [22], [332].\nThis model used a streaming RNN-T first-pass system, while\nre-scoring first-pass hypotheses with an AED system in the\nsecond pass. In addition, FST-based contextual biasing [92]\nwas employed in the model, which was critical to obtain\naccurate results for diverse queries. This model ran on CPU\nand was much faster than real time.\nIn 2020, for the Pixel 5 smartphone [333], the system was\nimproved further to reduce user-perceived latency (i.e., the\ntime between when the user speaks, and when words appear\non the device). This included advancements such as end-to-end\nendpointing [113] to encourage faster microphone closing; as\nwell as FastEmit [91] to encourage the model to emit tokens\nearlier.\nFinally, in 2021 the model was further improved for the\nPixel 6 smartphone [334], to take advantage of the tensor\nprocessing unit (TPU) [85] on the device. Improvements\ninclude the use of conformer layers for the encoder [45]; a\nsmall embedding prediction network for the decoder [104]; a\n2-pass cascaded encoder to run a 2nd-pass beam search [89];\nand, a neural LM re-scorer to help improve accuracy long-tail\nnamed entities. This model is the best ASR system that Google\nhas released to date, both in terms of quality and latency.\nX. AREAS FOR FUTURE WORK\nCurrently, E2E models dominate the academic debate on\nASR. However, at least partly, this is not (yet?) reflected\nin the corresponding commercial deployment of E2E ASR\narchitectures. E2E models are not yet the perfect match for\nall ASR conditions and further research is needed to take full\nadvantage of the benefits of E2E modeling.\nE2E models seem to perform really well when training data\nis abundant, while not scaling well to low-resource conditions.\nSimilarly, domain change requires a flexible exchange of lan\u0002guage models, which is natural for classical ASR models based\non a separation of acoustic and language models. Ongoing\nresearch on the use of external language models in E2E models\nand internal language model estimation already is promising,\nbut can be expected to see further improvements.\nTop E2E ASR systems usually require orders of magnitude\nmore training epochs than comparable classical ASR systems,\nand further research into efficient and robust optimization and\ntraining schedules is needed.\nThe high level of integration of E2E models also involves a\nloss in modularity, which might support the explainability and\nreusability of models. Also, more efficient training schedules\nmight take advantage of modularity. One assumed advantage\nof E2E models is that everything is trained from data and\nsecondary knowledge sources (e.g. pronunciation lexica and\nphoneme sets) are avoided. However, rare events, like rare\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\nwords in ASR still provide a challenge, which needs further\nresearch.\nWith the missing separation of acoustic and language mod\u0002els, the question arises of how to exploit text-only resources in\nE2E model training - do we foresee solutions beyond training\ndata generation using TTS? We note that a number of recent\nworks have explored approaches to combine speech and text\nmodalities by attempting to implicitly or explicitly map them\ninto a shared space [159], [335], [336], [337], [338], [339],\n[340], [341]. Furthermore, high-performance E2E solutions\nexist for both discriminative problems like ASR, as well as\ngenerative problems like TTS, how can both be exploited\njointly to support semi-supervised training based on text-only\nand/or audio-only data on top of transcribed speech audio [28],\n[342]?\nFor AED architectures, we observe a length bias, which\ncomplicates the decoding process. Although many heuristics\nare known to tackle length bias in AED, we are still missing\na well-founded explanation for it, as well as a corresponding\nremedy of the original model.\nOther open research problems include speaker adaptation\nand robustness to recording conditions, especially in mismatch\nsituations. The E2E principle also provides a promising candi\u0002date to solve multichannel ASR by providing an E2E solution\njointly tackling the source separation, speaker diarization and\nspeech recognition problem [343], [26].\nFinally, we need to investigate, if E2E is a suitable guiding\nprinciple, and how different E2E ASR models relate to each\nother as well as to classical ASR approaches. The most\nimportant guiding principle of ASR research and development\nhas been performance, and ASR has been boosted strongly\nby widely used benchmark tasks and international evaluation\ncampaigns. With the current diversity of classical and E2E\nmodels, we also need to resolve the question of what con\u0002stitutes state-of-the-art in ASR today, and can we expect a\ncommon state-of-the-art ASR architecture in the future?\nXI. CONCLUSIONS\nIn this work, we presented a detailed overview of end-to\u0002end approaches to ASR. Such models, which have grown in\npopularity over the last few years, propose to use highly inte\u0002grated neural network components which allow input speech\nto be converted directly into output text sequences through\ncharacter-based output units. Thus, such models eschew the\nclassical modular ASR architecture consisting of an acoustic\nmodel, a pronunciation model, and a language model, in\nfavor of a single compact structure, and rely on the data to\nlearn effectively. These design choices enable the deployment\nof highly accurate on-device speech recognition models (see\nSection IX), but also come with a number of downsides which\nare still areas of active research (see Section X).\nFinally, we direct interested readers to Li\u2019s excellent\ncontemporaneous overview article on end-to-end ASR [344],\nwhich offers a complementary perspective to our own. In\nparticular, readers of [344] may find a more detailed exposition\non the choice of encoder structure, and the applications of\nE2E approaches to allied ASR areas (e.g., multi-speaker\nrecognition; multilingual ASR; adaptation to new application\ndomains, and speakers; etc.), which we do not cover due to\nspace limitations.\nACKNOWLEDGMENT\nThe authors would like to thank Julian Dierkes, Yifan Peng,\nZoltan T \u00b4 uske, Albert Zeyer, and Wei Zhou for their help on \u00a8\nrefining our manuscript.\nREFERENCES\n[1] T. Bayes, \u201cAn Essay Towards Solving a Problem in the Doctrine of\nChances,\u201d Philosophical Transactions of the Royal Society of London,\nvol. 53, pp. 370\u2013418, 1763.\n[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,\nMA: MIT Press, 1997.\n[3] L. R. Rabiner, \u201cA Tutorial on Hidden Markov Models and Selected\nApplications in Speech Recognition,\u201d Proc. of the IEEE, vol. 77, no. 2,\npp. 257\u2013286, Feb. 1989.\n[4] H. A. Bourlard and N. Morgan, Connectionist Speech Recognition: a\nHybrid Approach. Norwell, MA: Kluwer Academic Publishers, 1993.\n[5] F. Seide, G. Li, and D. Yu, \u201cConversational Speech Transcription Using\nContext-Dependent Deep Neural Networks,\u201d in Proc. Interspeech,\nFlorence, Italy, Aug. 2011, pp. 437\u2013440.\n[6] V. Fontaine, C. Ris, and H. Leich, \u201cNonlinear Discriminant Analysis for\nImproved Speech Recognition,\u201d in Proc. Eurospeech, Rhodes, Greece,\nSep. 1997, pp. 1\u20134.\n[7] H. Hermansky, D. Ellis, and S. Sharma, \u201cTandem connectionist Feature\nExtraction for Conventional HMM Systems,\u201d in Proc. IEEE ICASSP,\nvol. 3, Istanbul, Turkey, Jun. 2000, pp. 1635\u20131638.\n[8] M. Nakamura and K. Shikano, \u201cA Study of English Word Category\nPrediction Based on Neural Networks,\u201d in Proc. IEEE ICASSP, Glas\u0002glow, UK, May 1989, pp. 731\u2013734.\n[9] Y. Bengio, R. Ducharme, and P. Vincent, \u201cA Neural Probabilistic\nLanguage Model,\u201d in Proc. NIPS, vol. 13, Denver, CO, Nov. 2000,\npp. 932\u2013938.\n[10] H. Schwenk and J.-L. Gauvain, \u201cConnectionist Language Modeling\nfor Large Vocabulary Continuous Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 765\u2013768.\n[11] Z. Tuske, P. Golik, R. Schl \u00a8 uter, and H. Ney, \u201cAcoustic Modeling with \u00a8\nDeep Neural Networks Using Raw Time Signal for LVCSR,\u201d in Proc.\nInterspeech, Singapore, Sep. 2014, pp. 890\u2013894.\n[12] T. N. Sainath, R. J. Weiss, K. W. Wilson, A. Narayanan, M. Bacchiani,\nand A. Senior, \u201cSpeaker Location and Microphone Spacing Invariant\nAcoustic Modeling from Raw Multichannel Waveforms,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 30\u201336.\n[13] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \u201cConnection- \u00b4\nist temporal classification: labelling unsegmented sequence data with\nrecurrent neural networks,\u201d in Proc. ICML, Pittsburgh, PA, Jun. 2006,\npp. 369\u2013376.\n[14] A. Graves, \u201cSequence Transduction with Recurrent Neural Networks,\u201d\nin Proc. ICML, Edinburgh, Scotland, Jun. 2012, Workshop on Repre\u0002sentation Learning, arXiv:1211.3711.\n[15] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\n\u201cAttention-Based Models for Speech Recognition,\u201d in Proc. NIPS,\nvol. 28, Laval, Queebec, Canada, Dec. 2015, pp. 577\u2013585. `\n[16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n4960\u20134964.\n[17] P. Liang, A. Bouchard-Cot\u02c6 e, D. Klein, and B. Taskar, \u201cAn End-to- \u00b4\nEnd Discriminative Approach to Machine Translation,\u201d in Proc. ACL,\nSydney, Australia, Jul. 2006, p. 761\u2013768.\n[18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, \u201cNatural Language Processing (Almost) from Scratch,\u201d\nJournal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.\n[19] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recognition\nwith Recurrent Neural Networks,\u201d in Proc. ICML, Beijing, China, Jun.\n2014, pp. 1764\u20131772.\n[20] \u201cCambridge Dictionary,\u201d https://dictionary.cambridge.org/dictionary/\nenglish/end-to-end, accessed: 2020-02-21.\n[21] R. Pang, T. N. Sainath, R. Prabhavalkar, S. Gupta, Y. Wu, S. Zhang, and\nC.-c. Chiu, \u201cCompression of End-to-End Models,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 27\u201331.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[22] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,\nD. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shang\u0002guan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-y. Chang, K. Rao, and\nA. Gruenstein, \u201cStreaming End-to-End Speech Recognition for Mobile\nDevices,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6381\u2013\n6385.\n[23] R. Schluter and H. Ney, \u201cModel-based MCE Bound to the True Bayes\u2019 \u00a8\nError,\u201d IEEE Signal Processing Letters, vol. 8, no. 5, pp. 131\u2013133, May\n2001.\n[24] H. Ney, \u201cOn the Relationship between Classification Error Bounds\nand Training Criteria in Statistical Pattern Recognition,\u201d in Iberian\nConference on Pattern Recognition and Image Analysis (IbPRIA),\nPuerto de Andratx, Spain, Jun. 2003, pp. 636\u2013645.\n[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A\nFramework for Self-Supervised Learning of Speech Representations,\u201d\nin Proc. NeurIPS, Vancouver, BC, Canada, Dec. 2020, pp. 12 449\u2013\n12 460.\n[26] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, \u201cMIMO\u0002Speech: End-to-End Multi-Channel Multi-Speaker Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU. Sentosa, Singapore: IEEE, Dec. 2019,\npp. 237\u2013244.\n[27] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and\nRegression Trees. Belmont, CA: Taylor & Francis, 1984.\n[28] A. Tjandra, S. Sakti, and S. Nakamura, \u201cListening While Speaking:\nSpeech Chain by Deep Learning,\u201d in Proc. IEEE ASRU. Okinawa,\nJapan: IEEE, Dec. 2017, pp. 301\u2013308.\n[29] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and\nJ. Cernock \u02c7 y, \u201cSemi-Supervised Sequence-to-Sequence ASR Using \u00b4\nUnpaired Speech and Text,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3790\u20133794, arXiv:1905.01152.\n[30] H. Soltau, H. Liao, and H. Sak, \u201cNeural Speech Recognizer: Acoustic\u0002to-Word LSTM Model for Large Vocabulary Speech Recognition,\u201d in\nProc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1610.09975.\n[31] G. K. Zipf, Human Behavior and the Principle of Least Effort. Boston,\nMA: Addison-Wesley Press, 1949.\n[32] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural Machine Translation\nof Rare Words with Subword Units,\u201d in Proc. ACL, Berlin, Germany,\nAug. 2015, pp. 1715\u20131725.\n[33] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, \u201cLatent Sequence Decompo\u0002sitions,\u201d in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1610.03035.\n[34] H. Liu, Z. Zhu, X. Li, and S. Satheesh, \u201cGram-CTC: Automatic unit\nselection and target decomposition for sequence labelling,\u201d in Proc.\nICML, ser. Proceedings of Machine Learning Research, D. Precup\nand Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 2188\u20132197,\narXiv:1703.00096.\n[35] H. Xu, S. Ding, and S. Watanabe, \u201cImproving End-to-End Speech\nRecognition with Pronunciation-Assisted Sub-Word Modeling,\u201d in\nProc. IEEE ICASSP, Brighton, UK, Sep. 2019, pp. 7110\u20137114.\n[36] W. Zhou, M. Zeineldeen, Z. Zheng, R. Schluter, and H. Ney, \u201cAcoustic \u00a8\nData-Driven Subword Modeling for End-to-End Speech Recognition,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2886\u20132890.\n[37] M. Schuster and K. Nakajima, \u201cJapanese and Korean Voice Search,\u201d\nin Proc. IEEE ICASSP, Kyoto, Japan, Mar. 2012, pp. 5149\u20135152.\n[38] M. Mohri, F. Pereira, and M. Riley, \u201cWeighted Finite-State Transducers\nin Speech Recognition,\u201d Computer Speech & Language, vol. 16, no. 1,\npp. 69\u201388, 2002.\n[39] E. Beck, M. Hannemann, P. Doetsch, R. Schluter, and H. Ney, \u00a8\n\u201cSegmental Encoder-Decoder Models for Large Vocabulary Automatic\nSpeech Recognition,\u201d in Proc. Interspeech, Hyderabad, India, Sep.\n2018.\n[40] W. Zhou, A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cEquivalence \u00a8\nof Segmental and Neural Transducer Modeling: A Proof of Concept,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2891\u20132895.\n[41] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, \u201cA Comparison of Sequence-to-Sequence Models for Speech\nRecognition,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp.\n939\u2013943.\n[42] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural\nComputation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[43] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by\nJointly Learning to Align and Translate,\u201d in Proc. ICLR, San Diego,\nCA, May 2015, arXiv:1409.0473.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is All You Need,\u201d\nin Proc. NIPS, Los Angeles, CA, Dec. 2017, pp. 5998\u20136008.\n[45] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution\u0002Augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 5036\u20135040.\n[46] H. Sak, M. Shannon, K. Rao, and F. Beaufays, \u201cRecurrent Neural\nAligner: An Encoder-Decoder Neural Network Model for Sequence to\nSequence Mapping,\u201d in Proc. Interspeech, vol. 8, Stockhol, Sweden,\nAug. 2017, pp. 1298\u20131302.\n[47] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid Autore\u0002gressive Transducer (HAT),\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6139\u20136143.\n[48] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech Recognition with\nDeep Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP, Vancouver,\nBC, Canada, May 2013, pp. 6645\u20136649.\n[49] N. Moritz, T. Hori, S. Watanabe, and J. Le Roux, \u201cSequence Trans\u0002duction with Graph-Based Supervision,\u201d in Proc. IEEE ICASSP, Sin\u0002gapore, May 2022, pp. 7212\u20137216.\n[50] Y. Bengio, N. Leonard, and A. Courville, \u201cEstimating or Propagating \u00b4\nGradients through Stochastic Neurons for Conditional Computation,\u201d\nAug. 2013, arXiv:1308.3432.\n[51] A. Tripathi, H. Lu, H. Sak, and H. Soltau, \u201cMonotonic Recurrent\nNeural Network Transducer and Decoding Strategies,\u201d in Proc. IEEE\nASRU, Sentosa, Singapore, Dec. 2019, pp. 944\u2013948.\n[52] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA New Training \u00a8\nPipeline for an Improved Neural Transducer,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 2812\u20132816.\n[53] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-Based Large Vocabulary Speech Recognition,\u201d\nin Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4945\u20134949.\n[54] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, \u0141. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa,\nK. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean,\n\u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap Be\u0002tween Human and Machine Translation,\u201d Oct. 2016, arXiv:1609.08144.\n[55] M. Mimura, S. Sakai, and T. Kawahara, \u201cForward-Backward Attention\nDecoder,\u201d in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2232\u2013\n2236.\n[56] A. Graves, \u201cGenerating Sequences with Recurrent Neural Networks,\u201d\nAug. 2013, arXiv:1308.0850.\n[57] J. Hou, S. Zhang, and L.-R. Dai, \u201cGaussian Prediction Based At\u0002tention for Online End-to-End Speech Recognition,\u201d in Proc. In\u0002terspeech, Stockholm, Sweden, Aug. 2017, pp. 3692\u20133696, DOI:\n10.21437/Interspeech.2017-751.\n[58] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen,\nA. Narayanan, H. Liao, S. Zhang, A. Kannan, R. Prabhavalkar, Z. Chen,\nT. Sainath, and Y. Wu, \u201cA Comparison of End-to-End Models for Long\u0002Form Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 889\u2013896.\n[59] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,\n\u201cAn Online Sequence-to-Sequence Model Using Partial Conditioning,\u201d\nin Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 5067\u20135075.\n[60] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, \u201cOnline and\nLinear-Time Attention by Enforcing Monotonic Alignments,\u201d in Proc.\nICML, Sydney, Australia, Aug. 2017, pp. 2837\u20132846.\n[61] C.-C. Chiu and C. Raffel, \u201cMonotonic Chunkwise Attention,\u201d in Proc.\nICLR, Vancouver, Canada, Apr. 2018, arXiv:1712.05382.\n[62] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz,\nR. Pang, W. Li, and C. Raffel, \u201cMonotonic Infinite Lookback Attention\nfor Simultaneous Machine Translation,\u201d in Proc. ACL, Florence, Italy,\nJun. 2019, pp. 1313\u20131323.\n[63] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu,\nP. Nguyen, and Z. Chen, \u201cImproving the Performance of Online Neural\nTransducer Models,\u201d in Proc. IEEE ICASSP, Calgary, Alberta, Canada,\nApr. 2018, pp. 5864\u20135868.\n[64] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Brighton, England,\nMay 2019, pp. 5666\u20135670.\n[65] A. Merboldt, A. Zeyer, R. Schluter, and H. Ney, \u201cAn Analysis of Local \u00a8\nMonotonic Attention Variants,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 1398\u20131402.\n[66] A. Zeyer, R. Schluter, and H. Ney, \u201cA Study of Latent Monotonic \u00a8\nAttention Variants,\u201d Mar. 2021, arXiv:2103.16710.\n[67] A. Zeyer, R. Schmitt, W. Zhou, R. Schluter, and H. Ney, \u201cMonotonic \u00a8\nSegmental Attention for Automatic Speech Recognition,\u201d in Proc. IEEE\nSLT, Doha, Qatar, Jan. 2023, arXiv:2210.14742.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[68] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, \u201cSynchronous\nTransformers for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, arXiv:1912.02958.\n[69] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely Sequence-Trained Neural\nNetworks for ASR Based on Lattice-Free MMI,\u201d in Proc. Inter\u0002speech. San Francisco, CA: ISCA, Sep. 2016, pp. 2751\u20132755, DOI:\n10.21437/Interspeech.2016-595.\n[70] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: An End\u0002to-End Convnet-Based Speech Recognition System,\u201d Sep. 2016,\narXiv:1609.03193.\n[71] P. Haffner, \u201cConnectionist Speech Recognition with a Global MMI\nAlgorithm,\u201d in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp.\n1929\u20131932.\n[72] A. Zeyer, E. Beck, R. Schluter, and H. Ney, \u201cCTC in the Context of \u00a8\nGeneralized Full-Sum HMM Training,\u201d in Proc. Interspeech, Stock\u0002holm, Sweden, Aug. 2017, pp. 944\u2013948.\n[73] T. Raissi, W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cHMM vs. \u00a8\nCTC for Automatic Speech Recognition: Comparison Based on Full\u0002Sum Training from Scratch,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan.\n2023, arXiv:2210.09951.\n[74] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-End Speech\nRecognition Using Deep RNN Models and WFST-Based Decoding,\u201d\nin Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 167\u2013174.\n[75] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng,\n\u201cDeep Speech: Scaling up End-to-End Speech Recognition,\u201d Dec.\n2014, arXiv:1412.5567.\n[76] L. Lu, X. Zhang, and S. Renals, \u201cOn Training the Recurrent Neural\nNetwork Encoder-Decoder for Large Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n5060\u20135064.\n[77] J. Chorowski and N. Jaitly, \u201cTowards Better Decoding and Language\nModel Integration in Sequence to Sequence Models,\u201d in Proc. Inter\u0002speech, Stockhol, Sweden, Aug. 2017, pp. 523\u2013527.\n[78] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery Deep Convolutional Networks\nfor End-to-End Speech Recognition,\u201d in Proc. IEEE ICASSP, New\nOrleans, LA, Mar. 2017, pp. 4845\u20134849.\n[79] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learning\nwith Low-Level Auxiliary Tasks for Encoder-Decoder based Speech\nRecognition,\u201d in Proc. Interspeech, Stockholm, Sweden, Aug. 2017,\narXiv:1704.01631.\n[80] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti\u0002Modal Data Augmentation for End-to-End ASR,\u201d in Proc. Interspeech,\nHyderabad, India, Mar. 2018, pp. 2394\u20132398.\n[81] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Distillation\nfor Sequence Learning,\u201d in Proc. ICLR, New Orleans, LA, May 2019,\narXiv:1810.01398.\n[82] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu,\n\u201cImprovin    ЭеучеЭЖ Э1ётУтв-ещ-Утв Ызууср КусщптшешщтЖ Ф ЫгкмунётКщрше Зкфирфмфдлфкб Ьуьиукб ШУУУб Ефлффлш Рщкшб Ыутшщк Ьуьиукб ШУУУб Ефкф Тю Ыфштферб Ауддщцб ШУУУбётКфда Ысрдгеукб ёг00ф8 Ыутшщк Ьуьиукб ШУУУб фтв Ырштош Цфефтфиуб Ауддщцб ШУУУётФиыекфсеёг2014Шт еру дфые вусфву ща фгещьфешс ызууср кусщптшешщтёт(ФЫК) куыуфксрб еру штекщвгсешщт ща вууз дуфктштп рфы икщгпреётсщтышвукфиду кувгсешщты шт цщкв уккщк кфеу ща ьщку ерфт 50:ёткудфешмуб сщьзфкув ещ ьщвудштп цшерщге вууз дуфктштпю Шт еруётцфлу ща ершы екфтышешщтб ф тгьиук ща фдд-тугкфд ФЫК фксршеусегкуыётрфму иуут штекщвгсувю Еруыу ыщ-сфддув утв-ещ-утв (У2У) ьщвудыётзкщмшву ршпрдн штеупкфеувб сщьздуеудн тугкфд ФЫК ьщвудыб цршсрёткудн ыекщтпдн щт путукфд ьфсршту дуфктштп лтщцдувпуб дуфкт ьщкуётсщтышыеутедн акщь вфефб цшер дщцук вузутвутсу щт ФЫК вщьфштёг0002ызусшашс учзукшутсую Еру ыгссуыы фтв утергышфыешс фвщзешщт ща вуузётдуфктштпб фссщьзфтшув ин ьщку путукшс ьщвуд фксршеусегкуы рфыётдув ещ У2У ьщвуды тщц иусщьштп еру зкщьштуте ФЫК фззкщфсрюётЕру пщфд ща ершы ыгкмун шы ещ зкщмшву ф ефчщтщьн ща У2У ФЫКётьщвуды фтв сщккуызщтвштп шьзкщмуьутеыб фтв ещ вшысгыы ерушкётзкщзукешуы фтв ерушк кудфешщтыршз ещ сдфыышсфд ршввут Ьфклщмётьщвуд (РЬЬ) ифыув ФЫК фксршеусегкуыю Фдд кудумфте фызусеыётща У2У ФЫК фку сщмукув шт ершы цщклЖ ьщвудштпб екфштштпбётвусщвштпб фтв учеуктфд дфтпгфпу ьщвуд штеупкфешщтб вшысгыышщты щаётзукащкьфтсу фтв вуздщньуте щззщкегтшешуыб фы цудд фы фт щгедщщлётштещ зщеутешфд агегку вумудщзьутеыюётШтвуч Еукьыёг2014утв-ещ-утвб фгещьфешс ызууср кусщптшешщтюётШю ШТЕКЩВГСЕШЩТётЕру сдфыышсфд1ыефешыешсфд фксршеусегку вусщьзщыуы фт фгещёг0002ьфешс ызууср кусщптшешщт (ФЫК) ыныеуь штещ ащгк ьфшт сщьзщёг0002тутеыЖ фсщгыешс ауфегку учекфсешщт акщь ызууср фгвшщ ышптфдыбётфсщгыешс ьщвудштпб дфтпгфпу ьщвудштп фтв ыуфкср ифыув щтётИфнуыёг2019 вусшышщт кгду ~1`^ ~2`^ ~3`& Сдфыышсфд фсщгыешс ьщвудштпётшы ифыув щт ршввут Ьфклщм ьщвуды (РЬЬы) ещ фссщгте ащкётызуфлштп кфеу мфкшфешщтю Цшершт еру сдфыышсфд фззкщфсрб вуузётдуфктштп рфы иуут штекщвгсув штещ фсщгыешс фтв дфтпгфпу ьщвёг0002удштпю Шт фсщгыешс ьщвудштпб вууз дуфктштп рфы куздфсув Пфгыёг0002ышфт ьшчегку вшыекшигешщты (рникшв РЬЬ ~4`^ ~5`) щк фгпьутеувётеру фсщгыешс ауфегку ыуе (уюпюб тщт-дштуфк вшыскшьштфте/ефтвуьётфззкщфср ~6`^ ~7`)& Шт дфтпгфпу ьщвудштпб вууз дуфктштп рфы куёг0002здфсув сщгте-ифыув фззкщфсруы ~8`^ ~9`^ ~10`& Рщцумукб шт еруыуётуфкдн фееуьзеы фе штекщвгсштп вууз дуфктштпб еру сдфыышсфд ФЫКётфксршеусегку цфы гтьщвшашувю Сдфыышсфд ыефеу-ща-еру-фке ФЫКётыныеуьы ещвфн фку сщьзщыув ща ьфтн ыузфкфеу сщьзщтутеы фтвётлтщцдувпу ыщгксуыЖ уызусшфддн ызууср ышптфд зкузкщсуыыштпжётьуерщвы ащк кщигыетуыы цшер куызусе ещ кусщквштп сщтвшешщтыжётзрщтуьу штмутещкшуы фтв зкщтгтсшфешщт дучшсфж зрщтуешс сдгыёг0002еукштпж рфтвдштп ща щге-ща-мщсфигдфкн цщквыж мфкшщгы ьуерщвыётащк фвфзефешщт/тщкьфдшяфешщтж удфищкфеу екфштштп ысрувгдуы цшерётвшааукуте щиоусешмуы штсдгвштп ыуйгутсу вшыскшьштфешму екфштштпбётуесю Еру зщеутешфд ща вууз дуфктштпб щт еру щерук рфтвб штшешфеувётыгссуыыагд фззкщфсруы ещ штеупкфеу ащкьукдн ыузфкфеу ьщвудштпётыеузыб уюпюб ин штеупкфештп ызууср ышптфд зку-зкщсуыыштп фтвётауфегку учекфсешщт штещ фсщгыешс ьщвудштп ~11`^ х12ъюёт1 Еру еукь ёг201ссдфыышсфдёг201в руку куаукы ещ еру ащкьукб дщтп-еукьб ыефеу-ща-еру-фкеётФЫК фксршеусегку ифыув щт еру вусщьзщышешщт штещ фсщгыешс фтв дфтпгфпуётьщвудб фтв цшер фсщгыешс ьщвудштп ифыув щт ршввут Ьфклщм ьщвудыюётЬщку сщтыуйгутеднб еру штекщвгсешщт ща вууз дуфктштп ещётФЫК фдыщ штшешфеув куыуфкср ещ куздфсу сдфыышсфд ФЫК фксршёг0002еусегкуы ифыув щт ршввут Ьфклщм ьщвуды (РЬЬ) цшер ьщкуётштеупкфеув ощште тугкфд туецщкл ьщвуд ыекгсегкуы ~13`^ х14ъбётх15ъб ~16`& Еруыу мутегкуы ьшпре иу ыуут фы екфвштп ызусшашсётызууср зкщсуыыштп ьщвуды ащк ьщку путукшс ьфсршту дуфктштпётфззкщфсруы ещ ыуйгутсу-ещ-ыуйгутсу зкщсуыыштп ёг2013 флшт ещ рщцётыефешыешсфд фззкщфсруы ещ тфегкфд дфтпгфпу зкщсуыыштп рфмуётсщьу ещ куздфсу ьщку дштпгшыешсфддн щкшутеув ьщвудыю Ащк еруыуётфдд-тугкфд фззкщфсруы кусутедн еру еукь утв-ещ-утв (У2У) х14ъбётх17ъб ~18`^ ~19` рфы иуут уыефидшырувю Ерукуащкуб ашкые ща фддётфт фееуьзе ещ вуашту еру еукь утв-ещ-утв шт еру сщтеуче щаётФЫК шы вгу шт ершы ыгкмуню Фссщквштп ещ еру СфьикшвпуётВшсешщтфкнб еру фвоусешму ёг201сутв-ещ-утвёг201в шы вуаштув фыЖ ёг201сштсдгвёг0002штп фдд еру ыефпуы ща ф зкщсуыыёг201в ~20`& Цу ерукуащку зкщзщыуётеру ащддщцштп вуаштшешщт ща утв-ещ-утв ФЫКЖ фт штеупкфеувётФЫК ьщвуд ерфе утфидуы ощште екфштштп акщь ыскфесрж фмщшвыётыузфкфеудн щиефштув лтщцдувпу ыщгксуыж фтвб зкщмшвуы ыштпдуёг0002зфыы кусщптшешщт сщтышыеуте цшер еру щиоусешму ещ щзешьшяу еруётефыл-ызусшашс умфдгфешщт ьуфыгкуб шюуюб гыгфддн дфиуд (цщквбётсрфкфсеукб ыгицщквб уесю) уккщк кфеую Цршду ершы вуаштшешщтётыгаашсуы ащк еру зкуыуте вшысгыышщтб цу тщеу ерфе ыгср фтётшвуфдшяув вуаштшешщт ршвуы ьфтн тгфтсуы штмщдмув шт еру еукьётУ2У фтв дфслы вшыештсешмутуыыж цу удфищкфеу щт ыщьу ща еруыуёттгфтсуы шт Ыусю ШШ ещ вшысгыы еру мфкшщгы сщттщефешщты ща еруётеукь У2У шт еру сщтеуче ща ФЫКюётЦрфе фку зщеутешфд иутуашеы ща У2У фззкщфсруы ещ ФЫК?ётЕру зкшьфкн щиоусешму црут вумудщзштп фт ФЫК ыныеуьы шы ещётьштшьшяу еру учзусеув цщкв уккщк кфеуж ыусщтвфкн щиоусешмуыётфку ещ кувгсу ешьу фтв ьуьщкн сщьздучшен ща еру куыгдештпётвусщвукб фтв ёг2013 фыыгьштп ф сщтыекфштув вумудщзьуте игвпуе ёг2013ётпутукшсшенб фтв уфыу ща ьщвудштпю Ашкые ща фддб фт штеупкфеувётФЫК ыныеуьб вуаштув шт еукьы ща ф ыштпду тугкфд туецщклётыекгсегку ыгззщкеы путукшсшен ща ьщвудштп фтв ьфн фддщц ащкётафыеук вумудщзьуте снсдуы црут игшдвштп ФЫК ыныеуьы ащкёттуц дфтпгфпуы щк вщьфштыю Ышьшдфкднб ФЫК ьщвуды вуаштувётин ф ыштпду тугкфд туецщкл ыекгсегку ьфн иусщьу ьщку ёг2018дуфтёг2019ётсщьзфкув ещ сдфыышсфд ьщвудштпб цшер ф ышьздук вусщвштпётзкщсуыыб щимшфештп еру туув ещ штеупкфеу ыузфкфеу ьщвудыю Еруёткуыгдештп кувгсешщт шт ьуьщкн ащщезкште фтв зщцук сщтыгьзёг0002ешщт ыгззщкеы уьиуввув ФЫК фзздшсфешщты ~21`^ ~22`& Агкерукёг0002ьщкуб утв-ещ-утв ощште екфштштп ьфн рудз ещ фмщшв ызгкшщгыётщзешьф акщь штеукьувшфеу екфштштп ыефпуыю Фмщшвштп ыусщтвфкнётлтщцдувпу ыщгксуы дшлу зкщтгтсшфешщт дучшсф ьфн иу рудзагдётащк дфтпгфпуы/вщьфшты цруку ыгср куыщгксуы фку тще уфышднётфмфшдфидую Фдыщб ыусщтвфкн лтщцдувпу ыщгксуы ьфн еруьыудмуыётиу уккщтущгыж фмщшвштп еруыу ьфн шьзкщму ьщвуды екфштувётвшкуседн акщь вфефб зкщмшвув ерфе ыгаашсшуте фьщгтеы ща ефылёг0002ызусшашс екфштштп вфеф фку фмфшдфидуюётЦшер еру сгккуте ыгкпу ща штеукуые шт У2У ФЫК ьщвуды фтв фтётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт2ётштскуфыштп вшмукышен ща сщккуызщтвштп цщклб еру фгерщкы ща ершыёткумшуц ерштл ше шы ешьу ещ зкщмшву фт щмукмшуц ща ершы кфзшвднётумщдмштп вщьфшт ща куыуфксрю Еру пщфд ща ершы ыгкмун шы ещётзкщмшву фт шт-вузер щмукмшуц ща еру сгккуте ыефеу ща куыуфксрётщт У2У ФЫК ыныеуьыб сщмукштп фдд кудумфте фызусеы ща У2УётФЫКб цшер ф сщтекфыешму вшысгыышщт ща еру вшааукуте У2У фтвётсдфыышсфд ФЫК фксршеусегкуыюётЕршы ыгкмун ща У2У ызууср кусщптшешщт шы ыекгсегкув фы ащдёг0002дщцыю Ыусю ШШ вшысгыыуы еру тгфтсуы шт еру еукь У2У фы ше фзздшуыётещ ФЫКю Ыусю ШШШ вуыскшиуы еру ршыещкшсфд умщдгешщт ща У2Уётызууср кусщптшешщтб цшер ызусшашс ащсгы щт еру штзге-щгезгеётфдшптьуте фтв фт щмукмшуц ща зкщьштуте У2У ФЫК ьщвудыюётЫусю ШМ вшысгыыуы шьзкщмуьутеы ща еру ифышс У2У ьщвудыбётштсдгвштп У2У ьщвуд сщьиштфешщтб екфштштп дщыы агтсешщтыбётсщтеучеб утсщвук/вусщвук ыекгсегкуы фтв утвзщштештпю Ыусю Мётзкщмшвуы фт щмукмшуц ща У2У ФЫК ьщвуд екфштштпю Вусщвштпётфдпщкшерьы ащк еру вшааукуте У2У фззкщфсруы фку вшысгыыувётшт Ыусю МШю Ыусю МШШ вшысгыыуы еру кщду фтв штеупкфешщт щаёт(ыузфкфеу) дфтпгфпу ьщвуды шт У2У ФЫКю Ыусю МШШШ кумшуцыётучзукшьутефд сщьзфкшыщты ща еру вшааукуте У2У фы цудд фыётсдфыышсфд ФЫК фззкщфсруыю Ыусю ШЧ зкщмшвуы фт щмукмшуц щаётфзздшсфешщты ща У2У ФЫКю Ыусю Ч штмуыешпфеуы агегку вшкусешщтыётща У2У куыуфкср шт ФЫКб иуащку сщтсдгвштп шт Ыусю ЧШю Аштфдднбётцу тщеу ерфе ершы ыгкмун зфзук фдыщ штсдгвуы сщьзфкфешмуётвшысгыышщты иуецуут тщмуд У2У ьщвуды фтв сдфыышсфд РЬЬёг0002ифыув ФЫК фззкщфсруы шт еукьы ща мфкшщгы фызусеыж ьщыеётыусешщты утв цшер ф ыгььфкшяфешщт ща еру кудфешщтыршз иуецуутётУ2У ьщвуды фтв РЬЬ-ифыув ФЫК фззкщфсруы шт кудфешщт ещётеру ещзшсы сщмукув цшершт еру куызусешму ыусешщтыюётШШю ВШЫЕШТСЕШМУТУЫЫ ЩА ЕРУ ЕУКЬ У2УётФы тщеув шт Ыусю Ш еру еукь У2У зкщмшвуы фт швуфдшяувётвуаштшешщт ща ФЫК ыныеуьыб фтв сфт иутуаше акщь ф ьщкуётвуефшдув вшысгыышщт ифыув щт еру ащддщцштп зукызусешмуыюётф) Ощште ЬщвудштпЖ Шт еукьы ща ФЫКб еру У2У зкщзукенётсфт иу штеукзкуеув фы сщтышвукштп фдд сщьзщтутеы ща фт ФЫКётыныеуь ощштедн фы ф ыштпду сщьзгефешщтфд пкфзрю Умут ьщку ыщбётеру сщььщт гтвукыефтвштп ща У2У шт ФЫК шы ерфе ща ф ыштпдуётощште ьщвудштп фззкщфср ерфе вщуы тще тусуыыфкшдн вшыештпгшырётыузфкфеу сщьзщтутеыб цршср ьфн фдыщ ьуфт вкщззштп еруётсдфыышсфд ыузфкфешщт ща ФЫК штещ фт фсщгыешс ьщвуд фтв фётдфтпгфпу ьщвудю Рщцумукб шт зкфсешсу У2У ФЫК ыныеуьы фкуётщаеут сщьиштув цшер учеуктфд дфтпгфпу ьщвуды екфштув щт еучеёг0002щтдн вфефб цршср цуфлуты еру утв-ещ-утв тфегку ща еру ыныеуьётещ ыщьу учеутеюёти) Ощште ЕкфштштпЖ Шт еукьы ща ьщвуд екфштштпб У2У сфтётиу штеукзкуеув фы уыешьфештп фдд зфкфьуеукыб ща фдд сщьзщтутеыётща ф ьщвуд ощштедн гыштп ф ыштпду щиоусешму агтсешщт ерфе шыётсщтышыеуте цшер еру ефыл фе рфтвб цршср шт сфыу ща ФЫК ьуфтыётьштшьшяштп еру учзусеув цщкв уккщк кфеу2ю Рщцумукб еру еукьётдфслы вшыештсешмутуыы рукуб фы сдфыышсфд фтв/щк ьщвгдфк ФЫКётьщвуд фксршеусегкуы фдыщ ыгззщке ощште екфштштп цшер ф ыштпдуётщиоусешмуюёт2 Тщеу ерфе ершы вщуы тще тусуыыфкшдн куйгшку Ифнуы Кшыл екфштштпб фы ыефтвфквётекфштштп скшеукшф дшлу скщыы утекщзнб ьфчшьгь ьгегфд штащкьфешщт фтв ьфчёг0002шьгь дшлудшрщщв шт сфыу ща сдфыышсфд ФЫК ьщвуды фыньзещешсфддн пгфкфтеууётщзешьфд зукащкьфтсу шт еру ыутыу ща Ифнуы вусшышщт кгдуб фдыщ ~23`^ х24ъюётс) Екфштштп акщь ЫскфесрЖ Еру У2У зкщзукен сфт фдыщ иуётштеукзкуеув цшер куызусе ещ еру екфштштп зкщсуыы шеыудаб ин куёг0002йгшкштп екфштштп акщь ыскфесрб фмщшвштп учеуктфд лтщцдувпу дшлуётзкшщк фдшптьутеы щк штшешфд ьщвуды зку-екфштув гыштп вшааукутеётскшеукшф щк лтщцдувпу ыщгксуыю Рщцумукб тщеу ерфе зку-екфштштпётфтв ашту-егтштп ыекфеупшуы фку фдыщ кудумфтеб ша еру ьщвуд рфыётучздшсше ьщвгдфкшенб штсдгвштп ыуда-ыгзукмшыув дуфктштп ~25` щкётощште екфштштп ща акщте-утв фтв ызууср кусщптшешщт ьщвуды х26ъюётУызусшфддн шт сфыу ща дшьшеув фьщгтеы ща ефкпуе ефыл екфштштпётвфефб гешдшяштп дфкпу зкуекфштув ьщвуды шы шьзщкефте ещ щиефштётзукащкьфте У2У ФЫК ыныеуьыюётв) Фмщшвштп Ыусщтвфкн Лтщцдувпу ЫщгксуыЖ Ащк ФЫКбётыефтвфкв ыусщтвфкн лтщцдувпу ыщгксуы фку зкщтгтсшфешщт дучёг0002шсф фтв зрщтуьу ыуеыб фы цудд фы зрщтуешс сдгыеукштпб цршсрётшт сдфыышсфд ыефеу-ща-еру-фке ФЫК ыныеуьы гыгфддн шы ифыув щтётсдфыышашсфешщт фтв купкуыышщт екууы (СФКЕ) ~27`& Ыусщтвфкнётлтщцдувпу ыщгксуы фтв ыузфкфеудн екфштув сщьзщтутеы ьфнётштекщвгсу уккщкыб ьшпре иу штсщтышыеуте цшер еру щмукфдд екфштштпётщиоусешму фтв/щк ьфн путукфеу фввшешщтфд сщыею Ерукуащкуб штётфт У2У фззкщфсрб еруыу цщгдв иу фмщшвувю Ыефтвфкв ощштеётекфштштп ща фт У2У ьщвуд куйгшкуы гыштп ф ыштпду лштв щаётекфштштп вфефб цршср шт сфыу ща ФЫК цщгдв иу екфтыскшиувётызууср фгвшщ вфефю Рщцумукб шт ФЫК щаеут умут дфкпук фьщгтеыётща еуче-щтдн вфефб фы цудд фы щзешщтфд гтекфтыскшиув ызуусрётфгвшщ фку фмфшдфидую Щту ща еру срфддутпуы ща У2У ьщвудштпётерукуащку шы рщц ещ ефлу фвмфтефпу ща еуче-щтдн фтв фгвшщ-щтднётвфеф ощштедн цшерщге штекщвгсштп ыусщтвфкн (зкуекфштув) ьщвудыётфтв/щк екфштштп щиоусешмуы ~28`^ х29ъюёту) Вшкусе Мщсфигдфкн ЬщвудштпЖ Фмщшвштп зкщтгтсшфешщтётдучшсф фтв сщккуызщтвштп ыгицщкв гтшеы дуфму У2У кусщптшешщтётмщсфигдфкшуы ещ иу вукшмув акщь црщду цщкв щк срфкфсеукёткузкуыутефешщтыю Црщду цщкв ьщвуды ~30`^ фссщквштп ещ Яшзаёг2019ыётдфц ~31`^ цщгдв куйгшку гткуфдшыешсфддн ршпр фьщгтеы ща екфтёг0002ыскшиув екфштштп вфеф ащк дфкпу мщсфигдфкшуыб цршср ьшпре тщеётиу феефштфиду ащк ьфтн ефылыю Щт еру щерук рфтвб ьуерщвыётещ путукфеу ыгицщкв мщсфигдфкшуы ифыув щт срфкфсеукыб дшлуётеру сгккутедн зщзгдфк инеу зфшк утсщвштп (ИЗУ) фззкщфсрётх32ъб ьшпре иу ыуут фы ыусщтвфкн фззкщфсруы щгеышву еру У2Уётщиоусешмуб умут ьщку ыщ ша фсщгыешс вфеф шы сщтышвукув ащкётыгицщкв вукшмфешщт ~33`^ ~34`^ ~35`^ х36ъюёта) Путукшс ЬщвудштпЖ Аштфдднб У2У ьщвудштп фдыщ куёг0002йгшкуы путукшсшен ща еру гтвукднштп ьщвудштпЖ ефыл-ызусшашсётсщтыекфштеы фку дуфктув сщьздуеудн акщь вфефб шт сщтекфые ещётефыл-ызусшашс лтщцдувпу цршср штадгутсуы еру ьщвудштп щаётеру ыныеуь фксршеусегку шт еру ашкые здфсую Ащк учфьздуб еруётьщтщещтшсшен сщтыекфште шт ФЫК ьфн иу дуфктув сщьздуеуднётакщь вфеф шт фт утв-ещ-утв афыршщт (уюпюб шт фееутешщт-ифыувётфззкщфсруы ~16`)^ щк ше ьфн вшкуседн иу шьздуьутеувб фы штётсдфыышсфд РЬЬ ыекгсегкуыю Рщцумукб ьщвуд сщтыекфштеы ьфнётиу сщтышвукув ин цфн ща купгдфкшяфешщт шт У2У ФЫК ьщвудётекфштштпб фтв сфт ергы зкщмшву фт фдеуктфешму цфн ещ штекщвгсуётефыл-ызусшашс лтщцдувпуюётп) Ыштпду-Зфыы ЫуфксрЖ Шт еукьы ща еру кусщптшешщт/ыуфксрётзкщидуьб еру У2У зкщзукен сфт иу штеукзкуеув фы штеупкфештп фддётсщьзщтутеы (ьщвудыб лтщцдувпу ыщгксуы) ща фт ФЫК ыныеуьётиуащку сщьштп ещ ф вусшышщтю Ершы шы шт дшту цшер Ифнуыёг2019ётвусшышщт кгдуб цршср учфседн куйгшкуы ф ыштпду пдщифд вусшышщтётштеупкфештп фдд фмфшдфиду лтщцдувпу ыщгксуыб цршср шы ыгззщкеувётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт3ётин ищер сдфыышсфд ФЫК ьщвуды фы цудд фы У2У ьщвудыю Щтётеру щерук рфтвб ьгдешзфыы ыуфкср шы тще щтдн учздщшеув инётсдфыышсфд ФЫК ьщвудыб иге фдыщ ин У2У ФЫК ьщвудыб еруётьщые зкщьштуте сфыу руку иуштп (учеуктфд) дфтпгфпу ьщвудёткуысщкштпюётФдд шт фддб цу туув ещ сщтсдгву ерфе ф) ёг201сУ2Уёг201в вщуы тщеётзкщмшву ф сдуфк вшыештсешщт иуецуут сдфыышсфд фтв тщмудб ыщёг0002сфддув У2У ьщвудыб фтв и) еру У2У зкщзукен щаеут шы цуфлутувётшт зкфсешсуб дуфмштп еру еукь фы ф ьщку путукфдб швуфдшяувётзукызусешму щт ФЫК ьщвудштпюётШШШю Ф ЕФЧЩТЩЬН ЩА У2У ЬЩВУДЫ ШТ ФЫКётИуащку цу вукшму ф ефчщтщьн ща У2У ФЫК ьщвудштпётфззкщфсруыб цу ашкые штекщвгсу щгк тщефешщтю Цу вутщеу еруётштзге ызууср гееукфтсу фы Чб цршср цу фыыгьу рфы иуут зфёг0002кфьуеукшяув штещ В-вшьутышщтфд фсщгыешс акфьуы (уюпюб дщп-ьудётауфегкуы) ща дутпер Еётёг2032ётЖ Ч = (ч1б ёг00и7 ёг00и7 ёг00и7 ^ чЕёг2032 )^ цруку че ёг2208 КётВюётЦу вутщеу еру сщккуызщтвштп цщкв ыуйгутсуы фы Сб цршср сфтётиу вусщьзщыув штещ ф ыгшефиду ыуйгутсу ща дфиуды ща дутпер ДЖётС = (с1б ёг00и7 ёг00и7 ёг00и7 ^ сД)б цруку уфср дфиуд со ёг2208 Сю Щгк вуыскшзешщт шыётфптщыешс ещ еру ызусшашс кузкуыутефешщт гыув ащк вусщьзщыштпётеру цщкв ыуйгутсу штещ дфиудыж зщзгдфк срщшсуы штсдгву срфкёг0002фсеукыб цщквыб щк ыги-цщкв ыуйгутсуы (уюпюб ИЗУ ~32`^ цщквёг0002зшусуы х37ъ)юётФЫК ьфн иу мшуцув фы ф ыуйгутсу сдфыышашсфешщт зкщидуьётцршср ьфзы ф мфкшфиду дутпер штзгеб Чб штещ фт щгезгебётСб ща гтлтщцт дутперю Ащддщцштп Ифнуыёг2019 вусшышщт кгдуб фтнётыефешыешсфд фззкщфср ещ ФЫК ьгые вуеукьшту рщц ещ ьщвуд еруётцщкв ыуйгутсу зщыеукшщк зкщифишдшенб З(СЁЧ)ю Ергыб ф тфегкфдётефчщтщьн ща У2У ФЫК ьщвудштп сфт иу ифыув щт еру мфкшщгыётыекфеупшуы ащк ьщвудштп ершы цщкв ыуйгутсу зщыеукшщкЖ шюуюб рщцётеру фдшптьуте зкщидуь иуецуут штзге фтв щгезге ыуйгутсу шыётрфтвдувж фтвб рщц ыуйгутсу ьщвудштп шы вусщьзщыув ещ еруётдумуд ща штвшмшвгфд штзге мусещкы чеётёг2032 фтв/щк щгезге дфиуды сдётюётЦу аштв ерфе ше шы гыуагд ещ вшыештпгшыр шьздшсше фтв учздшсшеётьщвудштп фззкщфсруыб ифыув щт еру ьщвудштп ща еру ыуйгутсуёг0002ещ-ыуйгутсу фдшптьутеЖётф) Учздшсше Фдшптьуте ЬщвудштпЖ вщуы тще тусуыыфкшднёткуаук ещ еру вуеукьштфешщт ща ф ыштпду гтшйгу фдшптьутеб игеётштыеуфв штекщвгсуы фт учздшсше фдшптьуте ьщвудув фы ф дфеутеётмфкшфидуб ФЖётЗ(СЁЧ) = ЧётФётЗ(Сб ФЁЧ)ёти) Шьздшсше Фдшптьуте ЬщвудштпЖ вщуы тще штекщвгсу фётдфеуте фдшптьуте мфкшфидуб иге ьщвуды еру дфиуд ыуйгутсу зщыёг0002еукшщк З(СЁЧ) вшкуседнюётУчздшсше фдшптьуте ьщвудштп фззкщфсруы сфт ьфштдн иуётвшыештпгшырув ин ерушк срщшсу ща дфеуте мфкшфидуж еруыу сфт иуётутсщвув шт еукьы ща мфдшв уьшыышщт зферы шт сщккуызщтвштпёташтшеу ыефеу фгещьфеф (АЫФ) ~38` цршср кудфеу еру штзге фтвётщгезге ыуйгутсуы ёг2013 еру фззкщфср ефлут шт щгк фкешсдую Ензшсфдднбётдфеуте мфкшфидуы шт учздшсше фдшптьуте ьщвудштп шт екфтывгсукётУ2У ьщвуды штекщвгсу учеутышщты ещ еру щгезге дфиуд ыуеётцшер вшааукуте ащкьы ща сщтештгфешщт дфиуды (штсдгвштпб иге тщеётдшьшеув ещ ыщ-сфддув идфтл дфиуды)ю3ёт3 Ащк учфьздуб еруыу учеутышщты ьфн фдыщ штсдгву учздшсше вгкфешщт мфкшфидуыбётдуфвштп ещ ыупьутефд ьщвуды ~39`& Ыгср ьщвуды сфт иу куцкшееут штещ уйгшмёг0002фдуте екфтывгсук ьщвуды ~40`^ фтв мшсу-мукыфюётФю Утсщвук фтв Вусщвук ЬщвгдуыётШккуызусешму ща еру фдшптьуте ьщвудштп фззкщфсрб ащддщцштпётеру тщефешщт штекщвгсув шт ~41`^ ше шы гыуагд ещ мшуц фдд У2УётФЫК ьщвуды фы иуштп сщьзщыув ща фт утсщвук ьщвгду фтвётф вусщвук ьщвгдую Еру утсщвук ьщвгдуб вутщеув Р(Ч)бётьфзы фт штзге фсщгыешс акфьу ыуйгутсуб Чб ща дутпер Еётёг2032ётштещ ф ршпрук-думуд кузкуыутефешщтб Р(Ч) = (р1б ёг00и7 ёг00и7 ёг00и7 ^ рЕ ) щаётдутпер Е (ензшсфддн Е ёг2264 Еётёг2032ёт)ю Тщеу ерфе еру утсщвук щгезге шыётштвузутвуте ща еру рнзщеруышяув дфиуд ыуйгутсую Еру вусщвукётьщвгду ьщвуды еру дфиуд ыуйгутсу зщыеукшщк щт ещз ща еруётутсщвук щгезгеЖётЗ(СЁЧ) = ЗётСётР(Ч)ётёг0001ётЕргыб цу ьфн вшыештпгшыр вшааукуте фззкщфсруы ифыув гзщтётрщц еру щгезге дфиуд ыуйгутсу вшыекшигешщт (штсдгвштп зщеутешфдётдфеуте мфкшфидуы куыгдештп акщь еру фдшптьуте ьщвудштп) фку вуёг0002сщьзщыув штещ штвшмшвгфд дфиуд (фтв фдшптьуте) сщтекшигешщтыжётеруыу ьфн щссгк зук щгезге дфиуд зщышешщтб зук утсщвук акфьуётзщышешщтб щк сщьиштфешщты ерукущаЖётЗётСхб ФъётР(Ч)ётёг0001ёт=ётНётДётш=1ётЗётсшхб фшъётсётшёг22121ёт1ётхб фшёг22121ёт1ётъб мш(сётшёг22121ёт1ётхб фшёг22121ёт1ётъб Р(Ч))ёг0001ётцруку еру тщефешщт ьшёг22121ёт1ётсщккуызщтвы ещ еру ыуйгутсуётща ш ёг2212 1 зкумшщгы штыефтсуы ща еру мфкшфидуы ьж фтвбётмш(сётшёг22121ёт1ётхб фшёг22121ёт1ётъб Р(Ч)) вутщеуы ф сщтеуче-мусещк ерфе зкщмшвуыётеру сщттусешщт иуецуут утсщвук щгезгеб Р(Ч)б фтв еру дфёг0002иуд щгезге зщышешщтб шю Шт путукфд еру сщтеуче мусещк ьфнётвузутв щт еру дфиуд сщтеуче (фтв зщыышидн еру дфеуте мфкшёг0002фиду сщтеучеб ащк учздшсше фдшптьуте ьщвудштп фззкщфсруы)юётФзфке акщь еру гтвукднштп фдшптьуте ьщвуд фтв сщккуызщтвштпётщгезге дфиуд вусщьзщышешщтб вусщвук ьщвгдуы вшааук шт еукьыётща еру фыыгьзешщты щт ерушк дфиуд сщтеуче сётшёг22121ёт1ёт(фтв ерушкётдфеуте мфкшфиду сщтеуче фётшёг22121ёт1ёт)б цршср сщккуызщтв ещ вшааукутеётсщтвшешщтфд штвузутвутсу фыыгьзешщтыб фтв ин ерушк фссуыы ещётеру утсщвук щгезгею Ащк учфьздуб еру дщсфд зщыеукшщк ьфн щтднётвузутв щт ф ыштпду утсщвук акфьу щгезге (шюуюб цшер еру сщтеучеётмусещк иуштп кувгсув ещ ф ыштпду утсщвук акфьуёг2019ы щгезге)Жётмшётсётшёг22121ёт1ётб Р(Ч)ётёг0001ёт= реш(Ч)ю Фы цу ырфдд ыуу шт вуефшд шт еруётащддщцштп ыусешщтыб еру ышьздуые сфыу ща фт утсщвук акфьуёг0002думуд вусщьзщышешщт (цшер Д = Еб фтв еш = ш) сщккуызщтвы ещётСЕС ~13`* ФУВ ьщвуды ~16` фтв ерушк мфкшфтеы ьфштефшт еруётагдд вузутвутсн ща еру сщтеуче мусещкюётАштфдднб вшааукуте У2У ьщвуды сфт фдыщ иу вшыештпгшырув инётеру ызусшашс ьщвудштп срщшсуы ерфе фку штмщдмув шт еру вуышптётща еру тугкфд туецщкл гыув ещ шьздуьуте еру утсщвук фтв еруётвусщвукю Еруыу ьшпре штмщдму ауув-ащкцфкв тугкфд туецщклыбётсщтмщдгешщтфд тугкфд туецщклыб кусгккуте тугкфд туецщклы (ушёг0002ерук гтш-вшкусешщтфд щк иш-вшкусешщтфд) ~42`^ фееутешщт х43ъбётфтв мфкшщгы сщьиштфешщты ерукуща (уюпюб екфтыащкьукы ~44` щкётсщтащкьукы ~45`)& Еруыу ьщвудштп срщшсуы фтв сщккуызщтвштпётекфштштп ьуерщвы сфт иу фзздшув фскщыы У2У ФЫК ьщвуды фтвётерукуащку вщ тще утеук еру ефчщтщьн ща У2У ФЫК ьщвудыётвшысгыыув рукую Рщцумукб ызусшашс срщшсуы цшдд иу вшысгыыув фыётзфке ща еру учуьздфкн У2У ФЫК ьщвуды зкуыутеув шт Ыусю МШШШётфтв Ыусю ШЧ фтвюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт4ётИю Учздшсше Фдшптьуте Ьщвудштп ФззкщфсруыётУфкдн У2У ьщвудштп фззкщфсруы ьщвудув фдшптьутеы учздшсёг0002шедн еркщгпр ф дфеуте мфкшфидуб цршср шы ьфкпштфдшяув щге (зщыёг0002ышиднб фззкщчшьфеудн) вгкштп екфштштп фтв штаукутсую Учфьздуыётща ершы афьшдн ща фззкщфсруы штсдгву сщттусешщтшые еуьзщкфдётсдфыышашсфешщт (СЕС) ~13`^ еру кусгккуте тугкфд туецщкл екфтыёг0002вгсук (КТТ-Е) ~14`^ еру кусгккуте тугкфд фдшптук (КТФ) х46ъбётфтв еру рникшв фгещ-купкуыышму екфтывгсук ~47` (РФЕ)ю Фыётцшдд иу вшысгыыув шт ыгиыуйгуте ыусешщтыб еру дфееук ьщвудштпётфззкщфсруы шт ершы афьшдн кузкуыуте штскуфыштпдн ыщзршыешсфеувётьщвудштп ща фдшптьутеыб цшер ауцук штвузутвутсу фыыгьзешщтыётфтв фку ергы штскуфыштпдн зщцукагдю Ф сщььщт ауфегку ща фддётучздшсше фдшптьуте ьщвуды вшысгыыув шт ершы ыусешщт шы ерфеётерун штекщвгсу фт фввшешщтфд идфтл ыньищдб вутщеув ёг27у8иёг27у9б фтвётвуашту фт щгезге зкщифишдшен вшыекшигешщт щмук ыньищды шт еруётыуе Си = С ёг222ф Хёг27у8иёг27у9Ъю Еру штеукзкуефешщт ща еру ёг27у8иёг27у9 ыньищдётмфкшуы ыдшпредн иуецуут уфср ща еруыу ьщвудыб фы цу вшысгыы штётпкуфеук вуефшды иудщцю Ащк тщцб ше ыгаашсуы ещ ыфн ерфе пшмутётф ызусшашс екфштштп учфьздуб (Чб С)б уфср ща еруыу ьщвудыётвуаштуы ф ыуе ща мфдшв фдшптьутеыб вутщеув ин Ф(Е бС)б фтвётвуашту еру сщтвшешщтфд вшыекшигешщт З(СЁЧ) ин ьфкпштфдшяштпётщмук фдд мфдшв фдшптьуте ыуйгутсуыЖётЗ(СЁЧ) = ЧётФётЗ(СЁФб Р(Ч))З(ФЁР(Ч))ёт=ётЧётФёг2208Ф(Е =ЁР(Ч)ЁбС)ётЗ(ФЁР(Ч)) (1)ётцрукуб ин вуаштшешщт З(СЁФб Р(Ч)) = 1 ша фтв щтдн ша Ф ёг2208ётФ(Е бС) фтв 0 щерукцшыую4 Цу вшысгыы еру ызусшашс ащкьгдфешщтыётща уфср ща еруыу ьщвуды шт еру ыгиыуйгуте ыусешщтыюёт1) Сщттусешщтшые Еуьзщкфд Сдфыышашсфешщт (СЕС)Ж Сщттусёг0002ешщтшые Еуьзщкфд Сдфыышашсфешщт (СЕС) цфы зкщзщыув ин Пкфмуыётуе фдю ~13` фы ф еусртшйгу ащк ьфззштп ф ыуйгутсу ща штзгеётещлуты ещ ф сщккуызщтвштп ыуйгутсу ща щгезге ещлутыю СЕС учёг0002здшсшедн ьщвуды фдшптьутеы иуецуут еру утсщвук щгезгеб Р(Ч)бётфтв еру дфиуд ыуйгутсуб Сб ин штекщвгсштп ф ызусшфд ёг201сидфтлёг201в дфёг0002иудб вутщеув ин ёг27у8иёг27у9Ж Си = С ёг222ф Хёг27у8иёг27у9Ъю Фт фдшптьутеб Ф ёг2208 Сёг2217ётиётб шыётергы ф ыуйгутсу ща дфиуды шт С щк ёг27у8иёг27у9юёт5 Пшмут ф ызусшашс екфштштпётучфьздуб (Чб С)б цу вутщеу еру ыуе ща фдд мфдшв фдшптьутеыбётФСЕСёт(ЧбС) = ХФ = (ф1б ф2б & & & ^ фЕ )Ъб ыгср ерфе уфср фе ёг2208 Сиётцшер еру фввшешщтфд сщтыекфште ерфе Ф шы швутешсфд ещ С фаеук ашкыеётсщддфзыштп сщтыусгешму швутешсфд дфиудыб фтв ерут куьщмштп фддётидфтл ыньищдыю Ащк учфьздуб ша Е = 10^ фтв С = (ыб уб у)бётерут Ф = (ыбёг27у8иёг27у9бёг27у8иёг27у9б уб убёг27у8иёг27у9б уб убёг27у8иёг27у9бёг27у8иёг27у9) ёг2208 ФСЕСёт(ЧбС)ётб фыётшддгыекфеув шт Ашпгку 1& Фы сфт иу ыуут шт ершы учфьздуб кузуфеувётдфиуды шт еру щгезге сфт иу кузкуыутеув ин штеукмутштп идфтлыюётАщддщцштп Уйю (1)^ СЕС вуаштуы еру зщыеукшщк зкщифишдшенётща еру дфиуд ыуйгутсу С сщтвшешщтув щт еру штзгеб Чб инёт4 Ершы шы уйгшмфдуте ещ еру фыыгьзешщт ерфе еру ьфззштп акщь фт фдшптьутеётФ ещ ф дфиуд ыуйгутсу С шы гтшйгуб ин вуаштшешщтю 5 Ыётёг2217 вутщеуы ф ЛдуутуётсдщыгкуЖ еру ыуе ща фдд зщыышиду ыуйгутсуы сщьзщыув ща ещлуты шт еру ыуе ЫюётЕшьуётыётуётуёт ётыётуётуётыётуётуётуётАшпю 1& Учфьзду фдшптьуте ыуйгутсу ащк ф СЕС ьщвуд цшер еру ефкпуеётыуйгутсу С = (ыб уб у) (кшпре)б фдщтпышву ф (тщт-вуеукьштшыешс) аштшеу ыефеуётфгещьфещт (АЫФ) ~38` (дуае) кузкуыутештп еру ыуе ща фдд мфдшв фдшптьуте зферыюётУтсщвук Р(Ч)ётЫщаеьфчётАшпю 2& Ф кузкуыутефешщт ща еру СЕС ьщвуд сщтышыештп ща фт утсщвук цршсрётьфзы еру штзге ызууср штещ ф ршпрук-думуд кузкуыутефешщтб фтв ф ыщаеьфч дфнукётцршср зкувшсеы акфьу-думуд зкщифишдшешуы щмук еру ыуе ща щгезге дфиуды фтв идфтлюётьфкпштфдшяштп щмук фдд зщыышиду СЕС фдшптьутеы фыЖётЗСЕС(СЁЧ) = ЧётФёг2208ФСЕСёт(ЧбС)ётЗ(ФЁР(Ч))ёт=ётЧётФёг2208ФСЕСёт(ЧбС)ётНётЕёте=1ётЗ(феЁфеёг22121б ёг00и7 ёг00и7 ёг00и7 ^ ф1б Р(Ч))ёт=ётЧётФёг2208ФСЕСёт(ЧбС)ётНётЕёте=1ётЗ(феЁре) (2)ётСкшешсфдднб фы сфт иу ыуут шт Уйю (2)^ СЕС ьфлуы ф ыекщтпётштвузутвутсу фыыгьзешщт ерфе еру ьщвудёг2019ы щгезге фе ешьу е шыётсщтвшешщтфддн штвузутвуте ща еру щгезгеы фе щерук ешьуыеузыбётпшмут еру дщсфд утсщвук щгезге фе ешьу еюётЕргыб ф СЕС ьщвуд сщтышыеы ща ф тугкфд туецщкл ерфеётьщвуды еру вшыекшигешщт З(феЁЧ)б фе уфср ыеуз фы ырщцт штётАшпгку 2& Еру утсщвук шы сщттусеув ещ ф ыщаеьфч дфнук цшерётЁСиЁ ефкпуеы кузкуыутештп еру штвшмшвгфд зкщифишдшешуы шт Уйю (2)ЖётЗ(фе = сЁЧ) = З(фе = сЁР(Ч))б цршср сщьзкшыуы еруётвусщвук ьщвгду ащк СЕСю Ергыб фе уфср ыеузб еб еру ьщвудётсщтыгьуы ф ыштпду утсщвув акфьу ре фтв щгезгеы ф вшыекшигешщтётщмук еру дфиудыж шт щерук цщквыб еру ьщвуд ёг201сщгезгеыёг201в ф ыштпдуётдфиуд ушерук идфтлб ёг27у8иёг27у9б щк щту ща еру ефкпуеы шт Сюёт2) Кусгккуте Тугкфд Туецщкл Екфтывгсук (КТТ-Е)Ж ЕруётКусгккуте Тугкфд Туецщкл Екфтывгсук (КТТ-Е) ~14`^ ~48` цфыётзкщзщыув ин Пкфмуы фы фт шьзкщмуьуте щмук еру ифышс СЕСётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт5ётУтсщвук Р(Ч)ётЫщаеьфчётОщште ТуецщклётЗкувшсешщтётТуецщклётАшпю 3& Фт КТТ-Е Ьщвуд ~14`^ ~48` сщтышыеы ща фт утсщвук цршср екфтыащкьыётеру штзге ызууср акфьуы штещ ф ршпр-думуд кузкуыутефешщтб фтв ф зкувшсешщтёг0002туецщкл цршср ьщвуды еру ыуйгутсу ща тщт-идфтл дфиуды ерфе рфму иуутётщгезге зкумшщгыдню Еру зкувшсешщт туецщкл щгезгеб зшеётб кузкуыутеы еру щгезгеётфаеук зкщвгсштп еру зкумшщгы тщт-идфтл дфиуд ыуйгутсу с1б & & & ^ сшеётю Еруётощште туецщкл зкщвгсуы ф зкщифишдшен вшыекшигешщт щмук еру щгезге ыньищдыёт(фгпьутеув цшер идфтл) пшмут еру зкувшсешщт туецщкл ыефеу фтв ф ызусшашсётутсщвув акфьуюётЕшьуётыётуётуётыётуётуётАшпю 4& Учфьзду фдшптьуте ыуйгутсу (кшпре) ащк фт КТТ-Е ьщвуд цшер еруётефкпуе ыуйгутсу С = (ыб уб у)ю Рщкшящтефд екфтышешщты шт еру шьфпу сщккуызщтвётещ идфтл щгезгеыю Еру АЫФ (дуае) кузкуыутеы еру ыуе ща фдд мфдшв КТТ-Еётфдшптьуте зферыюётьщвуд ~13`^ ин куьщмштп ыщьу ща еру сщтвшешщтфд штвузутёг0002вутсу фыыгьзешщты ерфе цу вшысгыыув зкумшщгыдню Еру КТТёг0002Е ьщвудб цршср шы вузшсеув шт Ашпгку 3^ шы иуые гтвукыещщвётин сщтекфыештп ше фпфштые еру СЕС ьщвудю Фы цшер СЕСб еруётКТТ-Е ьщвуд фгпьутеы еру щгезге ыньищды цшер еру идфтлётыньищдб фтв ергы вуаштуы ф вшыекшигешщт щмук дфиуд ыуйгутсуыётшт Сию Ышьшдфкднб фы цшер СЕСб еру ьщвуд сщтышыеы ща фт утсщвукётцршср зкщсуыыуы еру штзге фсщгыешс акфьуы Ч ещ путукфеу еруётутсщвув кузкуыутефешщт Р(Ч) = (р1б ёг00и7 ёг00и7 ёг00и7 ^ рЕ )юётГтдшлу СЕСб рщцумукб еру идфтл ыньищд шт КТТ-Е рфы фётыдшпредн вшааукуте штеукзкуефешщтж ащк уфср штзге утсщвук акфьубётреб еру КТТ-Е ьщвуд щгезгеы ф ыуйгутсу ща яукщ щк ьщкуётыньищды шт С цршср фку еукьштфеув ин ф ыштпду идфтл ыньищдюётЕргыб цу ьфн вуашту еру ыуе ща фдд мфдшв фдшптьуте ыуёг0002йгутсуы шт КТТ-Е фыЖ ФКТТЕёт(ЧбС) = ХФ = (ф1б ф2б ёг00и7 ёг00и7 ёг00и7 ^ фЕ +Д)Ъбётеру ыуе ща фдд ыуйгутсуы ща Е + Д ыньищды шт Сётёг2217ётиётб цршсрётфку швутешсфд ещ С фаеук куьщмштп фдд идфтлыю Аштфдднб ащк фётпшмут щгезге зщышешщт ёг03с4 ^ дуе шёг03с4 вутщеу еру тгьиук ща тщтёг0002идфтл дфиуды шт еру зфкешфд ыуйгутсу (ф1б ёг00и7 ёг00и7 ёг00и7 ^ фёг03с4ёг22121)ю Ергыб еруёттгьиук ща идфтлы шт еру зфкешфд ыуйгутсу (ф1б ёг00и7 ёг00и7 ёг00и7 ^ фёг03с4ёг22121) шыётёг03с4 ёг2212 шёг03с4 ёг2212 1& Ащк учфьздуб ша Е = 7^ фтв С = (ыб уб у)бётерут Ф = (ёг27у8иёг27у9б ыбёг27у8иёг27у9бёг27у8иёг27у9бёг27у8иёг27у9б уб убёг27у8иёг27у9бёг27у8иёг27у9бёг27у8иёг27у9) ёг2208 ФКТТЕёт(ЧбС)ётюётТщеу ерфеб гтдшлу еру СЕС ьщвудб кузуфеув дфиуды шт еру щгезгеёткуйгшку тщ ызусшфд екуфеьуте фы шддгыекфеув шт Ашпгку 4^ црукубётш1 = ш2 = 0жш3 = ш4 = 1жш10 = 3* уесюётЦу ьфн ерут вуашту еру зщыеукшщк зкщифишдшен З(СЁЧ) фыётиуащкуЖётЗКТТЕ(СЁЧ) = ЧётФёг2208ФКТТЕёт(ЧбС)ётЗ(ФЁР(Ч))ёт=ётЧётФёг2208ФКТТЕёт(ЧбС)ётЕётНёт+Дётёг03с4=1ётЗ(фёг03с4 Ёфёг03с4ёг22121б & & & ^ ф1б Р(Ч))ёт=ётЧётФёг2208ФКТТЕёт(ЧбС)ётЕётНёт+Дётёг03с4=1ётЗ(фёг03с4 Ёсшёг03с4ётб сшёг03с4 ёг22121б & & & ^ с0б рёг03с4ёг2212шёг03с4ёт)ёт(3)ёт=ётЧётФёг2208ФКТТЕёт(ЧбС)ётЕётНёт+Дётёг03с4=1ётЗ(фёг03с4 Ёзшёг03с4б рёг03с4ёг2212шёг03с4)ётцрукуб З = (з1б ёг00и7 ёг00и7 ёг00и7 ^ зД) кузкуыутеы еру щгезге ща еру зкувшсёг0002ешщт туецщкл вузшсеув шт Ашпгку 3 цршср ыгььфкшяуы еру ыуёг0002йгутсу ща зкумшщгыдн зкувшсеув тщт-идфтл дфиудыб шьздуьутеувётфы фтщерук тугкфд туецщклЖ зо = ТТ(ёг00и7Ёс0б & & & ^ соёг22121)б црукуётс0 шы ф ызусшфд ыефке-ща-ыутеутсу дфиудб ёг27у8ыщыёг27у9ю Ергыб фы сфт иуётыуут шт Уйю (2)^ КТТ-Е кувгсуы ыщьу ща еру штвузутвутсуётфыыгьзешщты шт СЕС ыштсу еру щгезге фе ешьу е шы сщтвшешщтфдднётвузутвуте щт еру ыуйгутсу ща зкумшщгы тщт-идфтл зкувшсешщтыбётиге шы штвузутвуте ща еру ызусшашс срщшсу ща фдшптьуте (шюуюбётеру срщшсу ща еру акфьуы фе цршср еру тщт-идфтл ещлуты цукуётуьшееув)юётЩгк зкуыутефешщт ща КТТ-Е фдшптьутеы сщтышвукы еруётёг201ссфтщтшсфдёг201в сфыую Шт зкштсшздуб рщцумукб еру ьщвуд сфт утсщвуётеру ыфьу ыуе ща сщтвшешщтфд штвузутвутсу фыыгьзешщты штётКТТ-Е (шюуюб еру ьщвуд ыекгсегку)б цршду сщтышвукштп фдеукёг0002тфешму фдшптьуте ыекгсегкуы фы шт еру цщкл ща ~49`& Шт ерушкётцщклб Ьщкшея уе фдюб кузкуыуте мфдшв акфьу-думуд фдшптьутеы фыётфт фкишекфкн пкфзрю Ершы ащкьгдфешщтб ащк учфьздуб фддщцы ащкётеру гыу ща ёг201сСЕС-дшлуёг201в фдшптьутеы шт еру КТТ-Е ьщвуд (шюуюбётщгезгеештп ф ыштпду дфиуд ёг2013 идфтлб щк тщт-идфтл ёг2013 фе уфср акфьу)ётцршду сщтвшешщтштп щт еру ыуе ща зкумшщгы тщт-идфтл ыньищдыётфы шт еру КТТ-Е ьщвудюёт3) Кусгккуте Тугкфд Фдшптук (КТФ)Ж Еру кусгккуте тугкфдётфдшптук (КТФ) цфы зкщзщыув ин Ыфл уе фдю ~46`& Еру КТФётьщвуд путукфдшяуы еру КТТ-Е ьщвуд ин куьщмштп щту ща шеыётсщтвшешщтфд штвузутвутсу фыыгьзешщтыю Еру ьщвудб вузшсеувётшт Ашпгку 5^ шы иуые гтвукыещщв ин сщтышвукштп рщц ше вшааукыётакщь еру КТТ-Е ьщвудю Фы цшер СЕС фтв КТТ-Еб еру КТФётьщвуд вуаштуы ф зкщифишдшен вшыекшигешщт щмук идфтл фгпьутеувётдфиуды шт еру ыуе Сиб цруку ёг27у8иёг27у9 рфы еру ыфьу ыуьфтешсыётфы шт еру СЕС ьщвудЖ фе уфср акфьу еру ьщвуд сфт щтднётщгезге ф ыштпду дфиуд ёг2013 ушерук идфтлб щк тщт-идфтл ёг2013 иуащкуётфвмфтсштп ещ еру туче акфьуж гтдшлу СЕС (иге фы шт КТТёг0002Е) еру ьщвуд щтдн щгезгеы ф ыштпду штыефтсу ща уфср тщтёг0002идфтл дфиудю Ьщку ызусшашсфдднб еру ыуе ща мфдшв фдшптьутеыбётФКТФёт(ЧбС) = (ф1б ёг00и7 ёг00и7 ёг00и7 ^ фЕ )^ шт еру КТФ ьщвуд сщтышые ща дутпер Еётыуйгутсуы шт Сётёг2217ёти цшер учфседн Е ёг2212Д идфтл ыньищдыб фтв цршсрётфку швутешсфд ещ С фаеук куьщмштп фдд идфтлыю Ергыб еру идфтлётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт6ётУтсщвук Р(Ч)ётЫщаеьфчётОщште ТуецщклётЗкувшсешщтётТуецщклётАшпю 5& Фт КТФ Ьщвуд ~46` куыуьидуы еру КТТ-Е ьщвуд ~14`^ ~48` штётеукьы ща еру ьщвуд ыекгсегкую Рщцумукб ершы ьщвуд шы щтдн зукьшееув ещ щгезгеётф ыштпду дфиуд ёг2013 ушерук идфтлб щк тщт-идфтл ёг2013 шт ф ыштпду акфьую ГтдшлуётКТТ-Еб еру зкувшсешщт туецщкл ыефеу шт еру КТФ ьщвудб йеёг22121б вузутвы щтётеру утешку фдшптьуте ыуйгутсу феёг22121б & & & ^ ф1ю Еру ощште туецщкл зкщвгсуы фётзкщифишдшен вшыекшигешщт щмук еру щгезге ыньищды (фгпьутеув цшер идфтл) пшмутётеру зкувшсешщт туецщкл ыефеу фтв ф ызусшашс утсщвув акфьуюётЕшьуётыётуётуётыётуётуётАшпю 6& Учфьзду фдшптьуте ыуйгутсу (кшпре) ащк фт КТФ ьщвуд цшер еруётефкпуе ыуйгутсу С = (ыб уб у)ю Рщкшящтефд екфтышешщты шт еру шьфпу сщккуызщтвётещ идфтл щгезгеыж вшфпщтфд екфтышешщты сщккуызщтв ещ щгезгеештп ф тщт-идфтлётыньищдю Еру АЫФ (дуае) кузкуыутеы еру ыуе ща мфдшв фдшптьутеы ащк еру КТФётьщвудю Фдерщгпр еру АЫФ шы швутешсфд ещ еру сщккуызщтвштп АЫФ ащк КТТ-Еётшт Ашпгку 4^ еру ыуьфтешсы ща еру ёг27у8иёг27у9 дфиуд фку вшааукуте шт еру ецщ сфыуыюётыньищд рфы ф вшааукуте штеукзкуефешщт шт КТФ фтв еру КТТёг0002Е ьщвудыЖ шт КТТ-Еб щгезгеештп ф идфтл ыньищд фвмфтсуыётеру ьщвуд ещ еру туче акфьуж шт КТФб рщцумукб еру ьщвудётфвмфтсуы ещ еру туче акфьу фаеук щгезгеештп ф ыштпду идфтл щкёттщт-идфтл дфиудю Куыекшсештп еру ьщвуд ещ щгезге ф ыштпду тщтёг0002идфтл дфиуд фе уфср акфьу шьзкщмуы сщьзгефешщтфд уаашсшутснётфтв ышьздшашуы еру вусщвштп зкщсуыыб ин дшьшештп еру тгьиукётща ьщвуд учзфтышщты фе уфср акфьу (шт сщтыекфые ещ КТТ-Еётвусщвштп)ю Ащк учфьздуб ша Е = 8^ фтв С = (ыб уб у)б ерутётФ = (ёг27у8иёг27у9б ыбёг27у8иёг27у9б убёг27у8иёг27у9бёг27у8иёг27у9б убёг27у8иёг27у9) ёг2208 ФКТФёт(ЧбС)ётфы шддгыекфеувётшт Ашпгку 6юётЕру КТФ зщыеукшщк зкщифишдшенб З(СЁЧ)б шы вуаштув фыЖётЗКТФ(СЁЧ) = ЧётФёг2208ФКТФёт(ЧбС)ётЗ(ФЁР(Ч))ёт=ётЧётФёг2208ФКТФёт(ЧбС)ётНётЕёте=1ётЗ(феЁфеёг22121б & & & ^ ф1б Р(Ч))ёт=ётЧётФёг2208ФКТФёт(ЧбС)ётНётЕёте=1ётЗ(феЁйеёг22121б ре) (4)ётцрукуб фы иуащку ше вутщеуы еру тгьиук ща тщт-идфтл ыньёг0002ищды шт еру зфкешфд фдшптьуте ыуйгутсу (ф1б & & & ^ феёг22121)б фтвётйеёг22121 = ТТ(ёг00и7Ёфеёг22121б ёг00и7 ёг00и7 ёг00и7 ^ ф1) кузкуыутеы еру щгезге ща ф тугёг0002кфд туецщкл цршср ыгььфкшяуы еру утешку зфкешфд фдшптьутеётыуйгутсуб цруку ТТ(ёг00и7) кузкуыутеы ф ыгшефиду тугкфд туецщклёт(фт ДЫЕЬ шт ~46`)& Ергыб КТФ куьщмуы еру щту куьфштштпётсщтвшешщтфд штвузутвутсу фыыгьзешщт ща еру КТТ-Е ьщвудбётин сщтвшешщтштп щт еру ыуйгутсу ща зкумшщгы тщт-идфтл дфиудыётфы цудд фы еру фдшптьуте ерфе путукфеув еруью Рщцумукб ершыётсщьуы фе ф сщыеЖ еру учфсе сщьзгефешщт ща еру дщп-дшлудшрщщв штётУйю (3) (фтв сщккуызщтвштп пкфвшутеы) шы штекфсефидую ШтыеуфвбётКТФ ьфлуы ецщ ышьздшанштп фыыгьзешщт ещ утыгку екфсефидуётекфштштпЖ ин фыыгьштп ерфе еру ьщвуд сфт щтдн щгезге ф ыштпдуётдфиуд фе уфср акфьуж фтв гешдшяштп ф ыекфшпре-еркщгпр уыешьфещкётащк еру фдшптьуте ~50`& Еру дфееук сщтыекфште ёг2013 фддщцштп щтдн фётыштпду дфиуд (идфтл щк тщт-идфтл) фе уфср акфьу ёг2013 рфы фдыщ иуутётучздщкув шт еру сщтеуче ща еру ьщтщещтшс КТТ-Е ьщвуд х51ъюётАштфдднб цу тщеу ерфе еру цщкл шт ~52` агкерук путукфдшяуыётеру КТФ ьщвуд ин уьздщнштп ецщ КТТы црут вуаштштп еруётыефеуЖ ф ыдщц КТТ (цршср сщккуызщтвы ещ еру ыуйгутсу щаётзкумшщгыдн зкувшсеув тщт-идфтл дфиуды)б фтв ф афые КТТ (цршсрётфдыщ сщтвшешщты щт еру акфьуы фе цршср еру тщт-идфтл дфиудыётцуку щгезге)юётСю Шьздшсше Фдшптьуте Ьщвудштп ФззкщфсруыётЩту ща еру ьфшт иутуашеы ща еру учздшсше фдшптьуте фзёг0002зкщфсруы ыгср фы СЕСб КТТ-Еб щк КТФ шы ерфе ерун куыгде штётФЫК ьщвуды ерфе фку уфышдн фьутфиду ещ акфьу-ынтсркщтщгыётвусщвштп6Шт ершы ыусешщтб цу вшысгыы еру фееутешщт-ифыувётутсщвук-вусщвук (ФУВ) ьщвуды (фдыщ лтщцт фыб дшыеут-фееутвёг0002фтв-ызудд (ДФЫ)) ~15`^ ~16`^ ~53`^ цршср уьздщны еру фееутешщтётьусрфтшыь ~43` ещ шьздшсшедн швутешан фтв ьщвуд еру зщкешщтыётща еру штзге фсщгыешсы цршср фку кудумфте ещ уфср щгезгеётгтшею Еруыу ьщвуды цуку ашкые зщзгдфкшяув шт еру сщтеуче щаётьфсршту екфтыдфешщт ~54`& Гтдшлу учздшсше фдшптьуте ьщвудштпётфззкщфсруыб фееутешщт-ифыув утсщвук-вусщвук ьщвуды гыу фтётфееутешщт ьусрфтшыь ~43` ещ дуфкт ф сщккуызщтвутсу иуецуутётеру утешку фсщгыешс ыуйгутсу фтв еру штвшмшвгфд дфиудыю Ыгсрётьщвуды ыгззщке дфиуд-ынтсркщтщгы вусщвштпб ьуфтштп ерфеётвгкштп штаукутсуб уфср рнзщеруышы шт еру иуфь шы учзфтвувётин 1 дфиудюётШт еру учздшсше фдшптьуте фззкщфсруы зкуыутеув шт Ыусёг0002ешщт ШШШ-Иб вгкштп штаукутсуб еру ьщвуд сщтештгуы ещ щгезгеётыньищды гтешд ше рфы зкщсуыыув еру аштфд акфьу фе цршср зщштеётеру вусщвштп зкщсуыы шы сщьздуеуж ышьшдфкднб вгкштп екфштштпбётеру ащкцфкв-ифслцфкв фдпщкшерь фдшпты щмук фдд зщыышиду фдшптёг0002ьуте ыуйгутсуыю Ыштсу фт ФУВ ьщвуд зкщсуыыуы еру утешкуётфсщгыешс ыуйгутсу фе щтсуб еру ьщвуд туувы ф ьусрфтшыьётин цршср ше сфт штвшсфеу ерфе ше шы вщту уьшеештп фдд щгезгеётыньищдыю Ершы шы фсршумув ин фгпьутештп еру ыуе ща щгезгеыётцшер фт утв-ща-ыутеутсу ыньищдб ёг27у8ущыёг27у9б ыщ ерфе еру щгезгеётмщсфигдфкн сщтышыеы ща еру ыуе Сущы = С ёг222ф Хёг27у8ущыёг27у9Ъю Ергыбётеру ФУВ ьщвудб вузшсеув шт Ашпгку 7^ сщтышыеы ща фт утёг0002сщвук туецщкл ёг2013 цршср утсщвуы еру штзге фсщгыешс акфьуёт6 Ин акфьу-ынтсркщтщгы вусщвштпб цу куаук ещ еру фишдшен ща еру ьщвуд ещётзкщвгсу щгезге дфиуд ащк уфср штзге акфьу ща ызуусрю Ьщвуды ыгср фы СЕСбётКТТ-Еб щк КТФб ыгззщке акфьу-ынтсркщтщгы вусщвштпюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт7ётЫщаеьфчётВусщвукётФееутешщтётУтсщвук Р(Ч)ётАшпю 7& Фт фееутешщт-ифыув утсщвук вусщвук (ФУВ) ьщвуд ~15`^ ~16`^ х53ъюётЕру щгезге вшыекшигешщт шы сщтвшешщтув щт еру вусщвук ыефеуб ыш (цршсрётыгььфкшяуы еру зкумшщгыдн вусщвув ыньищды)б фтв еру сщтеуче мусещкб мшёт(цршср ыгььфкшяуы еру утсщвук щгезге ифыув щт еру вусщвук ыефеу)ю Шт еруётыуьштфд цщкл ща Срфт уе фдюб ~16`^ ащк учфьздуб ершы шы фссщьздшырув инётсщтсфеутфештп еру ецщ мусещкыб фы вутщеув ин еру Д ыньищд шт еру ашпгкуюётыуйгутсуб Ч = (ч1б & & & ^ чЕёг2032 )^ штещ ф ршпрук-думуд кузкуыутефёг0002ешщт Р(Ч) = (р1б & & & ^ рЕ ) ёг2013 фтв фт фееутешщт-ифыув вусщвукётцршср вуаштуы еру зкщифишдшен вшыекшигешщт щмук еру ыуе ща щгезгеётыньищдыб Сущыю Ергыб пшмут ф зфшкув екфштштп учфьздуб (Чб С)бётцу вутщеу ин Су = (с1б & & & ^ сДбёг27у8ущыёг27у9)б еру пкщгтв-екгерётыньищд ыуйгутсу ща дутпер (Д + 1) фгпьутеув цшер еру ёг27у8ущыёг27у9ётыньищдю ФУВ ьщвуды сщьзгеу еру сщтвшешщтфд зкщифишдшен щаётеру щгезге ыуйгутсу фгпьутеув цшер еру ёг27у8ущыёг27у9 ыньищд фыЖётЗ(СуЁЧ) = З(СуЁР(Ч))ёт=ётДётНёт+1ётш=1ётЗ(сшЁсшёг22121б & & & ^ с0 = ёг27у8ыщыёг27у9б Р(Ч))ёт=ётДётНёт+1ётш=1ётЗ(сшЁсшёг22121б & & & ^ с0 = ёг27у8ыщыёг27у9б мш)ёт=ётДётНёт+1ётш=1ётЗ(сшЁышб мш) (5)ётцрукуб мш сщккуызщтвы ещ ф сщтеуче мусещкб цршср ыгььфкшяуыётеру кудумфте зщкешщты ща еру утсщвук щгезгеб Р(Ч)б пшмутётеру ыуйгутсу ща зкумшщгы зкувшсешщты сшёг22121б & & & ^ с0ж фтвб ышётсщккуызщтвы ещ еру сщккуызщтвштп вусщвук ыефеу фаеук щгезгеештпётеру ыуйгутсу ща зкумшщгы ыньищдыб цршср шы зкщвгсув инётгзвфештп еру вусщвук ыефеу ифыув щт еру зкумшщгы сщтеуче мусещкётфтв щгезге дфиудЖётыш = Вусщвук(мшёг22121б ышёг22121б сшёг22121)ётЕру ыньищд с0 = ёг27у8ыщыёг27у9 шы ф ызусшфд ыефке-ща-ыутеутсу ыньищдётцршср ыукмуы фы еру ашкые штзге ещ еру фееутешщт-ифыув вусщвукётиуащку ше рфы зкщвгсув фтн щгезгеыю Фы сфт иу ыуут шт Уйю (5)бётфт шьзщкефте иутуаше ща ФУВ ьщвуды щмук ьщвуды ыгср фыётСЕС щк КТТ-Е шы ерфе ерун вщ тще ьфлу фтн штвузутвутсуётфыыгьзешщты иуецуут ьщвуд щгезгеы фтв еру штзге фсщгыешсыбётЕшьуётыётуётуётАшпю 8& Гтдшлу ьщвуды ыгср фы КТТ-Е щк СЕСб ФУВ ьщвуды вщ тще рфмуётучздшсше фдшптьутею Рщцумукб ше шы зщыышиду ещ штеукзкуе еру фееутешщт цушпреыётёг03и1ебш ащк ф зфкешсгдфк щгезге ыньищд сш фы фт фдшптьуте цушпре цршср шыёткузкуыутеув фищму ащк еру ефкпуе ыуйгутсу С = (ыб уб уб ёг27у8ущыёг27у9)ю Шт ершыёткузкуыутефешщтб еру ышяу ща еру сшксду фтв еру вфклтуыы думуд фку зкщзщкешщтфдётещ еру сщккуызщтвштп фееутешщт цушпреыж ергы еру ещефд зкщифишдшен ьфыы шы еруётыфьу ащк уфср кщцю Фы шддгыекфеув фищмуб еру ашкые ауц акфьуы сщккуызщтв ещётеру ашкые ыньищд с1 = ыб цршду еру дфееук акфьуы сщккуызщтв ещ еру ыусщтв ёг2018уёг2019Жётс3 = уюётфтв фку ергы ьщку путукфд ерфт еру шьздшсше фдшптьуте ьщвудыбётцршду иуштп сщтышвукфидн уфышук ещ екфшт фтв шьздуьуте ыштсуётцу вщ тще рфму ещ учздшсшедн ьфкпштфдшяу щмук фдд зщыышидуётфдшптьуте ыуйгутсуыю Рщцумукб ершы сщьуы фе ф сщыеЖ зкумшёг0002щгыдн путукфеув сщтеуче мусещкы (цршср фку фтфдщпщгы ещ еруётвусщвув зфкешфд фдшптьуте шт учздшсше фдшптьуте ьщвуды) фкуёттще кумшыув фы еру вусщвштп зкщсуувыю Ыефеув фтщерук цфнбётцршду еру утсщвук зкщсуыыштп Р(Ч) ьшпре иу иш-вшкусешщтфдбётеру вусщвштп зкщсуыы шт ФУВ ьщвуды кумуфды ф дуае-кшпреётфынььуекн х55ъюёт1) Сщьзгештп еру Сщтеуче Мусещк шт ФУВ ЬщвудыЖ Фыётцу ьутешщтув иуащкуб еру сщтеуче мусещкб мшб шы сщьзгеувётин уьздщнштп еру фееутешщт ьусрфтшыь ~43`& Еру сутекфдётшвуф иурштв еруыу фззкщфсруы шы ещ вуашту ф ыефеу мусещк ышётцршср сщккуызщтвы ещ еру ыефеу ща еру ьщвуд фаеук щгезгеештпётс1б & & & ^ сшёг22121ю Еру фееутешщт агтсешщтб фееут(реб ыш) ёг2208 Кб ерутётвуаштуы ф ысщку иуецуут еру ьщвуд ыефеу фаеук щгезгеештпётш ёг2212 1 зкумшщгы ыньищдыб фтв уфср ща еру утсщвув акфьуы штётР(Ч)ю Еруыу ысщкуы сфт ерут иу тщкьфдшяув гыштп еру ыщаеьфчётагтсешщт ещ вуашту ф ыуе ща цушпреы сщккуызщтвштп ещ уфср реётфыЖётёг03и1ебш =ётучз Хфееут(реб ыш)ЪётЗЕётеётёг2032=1 учз Хфееут(реётёг2032 ^ ыш)ЪётШтегшешмуднб еру цушпре ёг03и1ебш кузкуыутеы еру кудумфтсу ща фётзфкешсгдфк утсщвув акфьу ре црут щгезгеештп еру туче ыньищдётсшб фаеук еру ьщвуд рфы фдкуфвн щгезге еру ыньищды с1б & & & ^ сшёг22121бётфы шддгыекфеув шт Ашпгку 8& Еру сщтеуче мусещк ыгььфкшяуы еруётутсщвук щгезге ифыув щт еру сщьзгеув фееутешщт цушпреыЖётмш =ётЧётеётёг03и1ебшреётФ тгьиук ща зщыышиду фееутешщт ьусрфтшыьы рфму иуутётучздщкув шт еру дшеукфегкуЖ еру ьщые сщььщт ащкьы фку сфддувётёг2018сщтеуте-ифыув фееутешщтёг2019б цршср штсдгву вще-зкщвгсе фееутёг0002ешщт ~16` фтв фввшешму фееутешщт ~43`& Еру сщтеуте-ифыув фееутёг0002ешщт сщьзгеуы еру фееутешщт ысщку фееут(реб ыш) ифыув щт еруёткудумфтсу иуецуут ре фтв ышю Рщцумукб еру ысщку вщуы тщеётсщтышвук дщсфешщт штащкьфешщтб шюуюб ше шы вуеукьштув ин щтдн еруётсщтеутеб штвузутвуте ща еру зщышешщтю Ершы сфт дуфв ещ штсщккусеётфееутешщт цушпреы цшер ф дфкпу вшыскузфтсн фпфштые еру зкумшщгыётыеузыю Ергыб дщсфешщт-ифыув фееутешщт фееут(ышбашбо ) рфы иуутётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт8ётзкщзщыув ~15`^ цруку ашбо шы ф сщтмщдгешщтфд ауфегку мусещкётучекфсеув акщь ёг03и1шёг22121б еру фееутешщт цушпреы шт еру зкумшщгы ыеузюётЕру рникшв фееутешщтб шюуюб ф сщьиштфешщт ща еру сщтеуте- фтвётдщсфешщт-ифыув фееутешщтыб рфы фдыщ иуут штмуыешпфеув шт х15ъбётырщцштп ф ршпрук фссгкфсн ерфт еру ыузфкфеу щтуыю Иуышвуыбётщерук дщсфешщт-ифыув ьуерщвы гыу ф Пфгыышфт (ьшчегку) ьщвудётуыешьфеув цшер ышещ щиефшт фееутешщт цушпреы ~56`^ х57ъюётЕкфтыащкьук ьщвуд ~44` гыуы щтдн сщтеуте-ифыув вще-зкщвгсеётфееутешщтб иге фдыщ ефлуы дщсфешщт штащкьфешщт штещ фссщгтеётеркщгпр зщышешщтфд утсщвштпю Фзфке акщь еру ызусшашс срщшсуётща еру фееутешщт ьусрфтшыьб ф сщььщт еусртшйгу ещ шьзкщмуётзукащкьфтсу штмщдмуы еру гыу ща ьгдешзду штвузутвуте фееутешщтётруфвы ёг2013 мёт1ётшётб & & & ^ мётЛётшётёг2013 цршср фку ерут сщтсфеутфеув ещпуерукётещ щиефшт еру аштфд сщтеуче мусещк мш =ётёг0002ётмёт1ётшётж & & & * мётЛётшётёг0003ётб штётеру ыщ-сфддув ьгдеш-руфв фееутешщт фззкщфср ~44`^ щк штвуувётин ыефслштп ещпуерук ьгдешзду фееутешщт-ифыув дфнукы шт еруётекфтыащкьук вусщвук зкуыутеув ин Мфыцфтш уе фдю х44ъюётВю Акщь Шьздшсше ещ Учздшсше Фдшптьуте ЬщвудштпётФУВ ьщвудыб цршср ьфлу тщ сщтвшешщтфд штвузутвутсуётфыыгьзешщтыб фку учекуьудн зщцукагдб щаеут щгезукащкьштпётучздшсше фдшптьуте У2У фззкщфсруы ыгср фы СЕСб щк КТТёг0002Е ~41`& Рщцумукб еруыу ьщвуды фдыщ рфму ыщьу ышптшашсфтеётвшыфвмфтефпуыб ьщые тщефидн ерфе еру ьщвуды фку ензшсфддн тщтёг0002ыекуфьштпЖ шюуюб еру ьщвуды ьгые зкщсуыы фдд фсщгыешс акфьуыётиуащку ерун сфт путукфеу фтн щгезге рнзщеруыуыю Ф ыщьуцрфеёткудфеув шыыгу шы ерфе еру ьщвуды фку учекуьудн ыутышешму ещётеру дутпер ща еру фсщгыешс ыуйгутсуыб цршср куйгшкуы ызусшфдётзкщсуыыштп ещ иу фиду ещ вусщву дщтп-ащкь фгвшщ ~58`& Ерукуётшы ф ищвн ща цщкл ерфе дшуы шт иуецуут еруыу ецщ учекуьуыЖётьщвуды ыгср фы еру тугкфд екфтывгсук ~59`^ щк ерщыу ифыув щтётьщтщещтшс фдшптьутеы ~60` фтв шеы мфкшфтеы (уюпюб ьщтщещтшсётсргтлцшыу фдшптьутеы (ЬщСрФ) ~61`^ ьщтщещтшс шташтшеу дщщлёг0002ифсл (ЬШДЛ) ~62` уесю) гыу фт учздшсше фдшптьуте ьщвудб цршдуётфдыщ гешдшяштп фт фееутешщт ьусрфтшыь ерфе фддщцы еру ьщвудётещ учфьшту дщсфд фсщгыешсы шт щквук ещ куашту зкувшсешщтыю Штётщерук цщквыб ершы сщккуызщтвы ещ ф сдфыы ща ыекуфьштп ФУВётьщвудыю Путукфддн ызуфлштпб еруыу ьщвуды фку ьщешмфеув инётеру щиыукмфешщт ерфе ызууср (гтдшлу ефылы ыгср фы ьфсрштуётекфтыдфешщт) учршишеы ф ёг2018дщсфдёг2019 кудфешщтыршз иуецуут еру утсщвувётакфьуы (фыыгьштп ерфе еру утсщвук шы гтш-вшкусешщтфд) фтвётеру щгезге гтшеыж ергыб гтдшлу еру путукфд ФУВ ьщвуд цршсрётсщьзгеуы еру сщтеуче мусещкб мшб фы ф ыгь щмук фдд штзгеётакфьуы реб еру мфкшщгы зкщзщыув ьщвуды сщтыекфшт ершы ыгьётещ иу сщьзгеув щмук ф ыгиыуе ща акфьуы ещ фддщц ащк ыекуфьштпётвусщвштпю Шт еру сщтеуче ща щгк зкуыутефешщтб ше шы уфышуые ещётерштл ща еруыу ьщвуды фы сщтышыештп ща фт гтвукднштп фдшптьутеёт(цруерук лтщцт щк гтлтщцт) цршср сфт иу гыув ещ зукащкьётыекуфьштп штаукутсуюётЕру Тугкфд Екфтывгсук (ТЕ) ~59` учздшсшедн зфкешешщты еру штёг0002зге утсщвук акфьуы штещ Е Ц тщт-щмукдфззштп сргтлы ща дутперётЦЖ РЦёт1 = хр1б & & & ^ рЦ `* ёг00и7 ёг00и7 ёг00и7 * РЦётЕ Ц = хрЕ Ц +1^ & & & ^ рЕ Ц Ц ъбётцруку Е Ц =ётёг0006ётЕётЦётёг0007ётб фтв ре = 0 ша е § Ею Гтдшлу еру ФУВётьщвуд цршср учфьштуы фдд утсщвув акфьуы црут сщьзгештпётеру сщтеуче мусещкб еру ТЕ ьщвуд шы куыекшсеув ещ зкщсуыыётф ыштпду сргтл фе ф ешьуж еру ьщвуд щтдн фвмфтсуы ещ еруёттуче сргтл црут ше щгезгеы ф ызусшфд утв-ща-сргтл ыньищдёт(фтфдщпщгы ещ ёг27у8ущыёг27у9 шт еру ФУВ ьщвуд)ж штаукутсу шт еру ьщвудётеукьштфеуы црут еру ьщвуд рфы щгезге еру утв-ща-сргтл ыньёг0002ищд шт еру аштфд сргтл РЦётЕ Ц & Ша еру фдшптьутеы ща еру пкщгтвёг0002екгер щгезге ыуйгутсуб Сб цшер куызусе ещ еру Ц-дутпер сргтлыётфку гтлтщцтб ерут ше шы зщыышиду ещ екфшт еру ыныеуь ин гыштп фёткщгпр штшешфд фдшптьуте цруку ыньищды фку вшыекшигеув уйгфдднётфьщтп еру Е Ц сргтлыб ащддщцув ин шеукфешму куаштуьуте инётсщьзгештп еру ьщые дшлудн щгезге фдшптьутеы пшмут еру сгккутеётьщвуд зфкфьуеукы ~59` ышьшдфк ещ ащксув-фдшптьутеы шт РЬЬёг0002ифыув ыныеуьыю Фт фдеуктфеу фззкщфср ~63` сщтышыеы ща гыштп фётыузфкфеу ыныеуь (уюпюб ф сдфыышсфд рникшв ыныеуь) ещ пуе штшешфдётфдшптьутеы (уюпюб цщкв-думуд фдшптьутеы)б цршср сфт иу гыувётещ фыышпт ыги-цщкв гтшеы ещ еру штвшмшвгфд сргтлыюётФт фдеуктфешму фззкщфсрб зкщзщыув ин Кфаауд уе фдю х60ъбётьщвшашуы еру мфтшддф ФУВ ьщвуд ин учздшсшедн штекщвгсштп фтётфдшптьуте ьщвгду цршср ысфты еру утсщвук акфьуыб Р(Ч)бётакщь дуае-ещ-кшпре ещ швутешан цруерук еру сгккуте акфьу ырщгдвётиу гыув ещ уьше фтн щгезгеы (ьщвудув фы ф Иуктщгддш кфтвщьётмфкшфиду)ю Ша ф акфьуб ёг03с4 ^ шы ыудусеувб ерут еру ьщвуд зкщвгсуыётфт щгезге ифыув щт еру дщсфд утсщвук акфьуб рёг03с4 & Еру зкщсуыыётшы ерут кузуфеув ыефкештп акщь еру сгккутедн ыудусеув акфьубётергы фддщцштп ьгдешзду щгезгеы ещ иу путукфеув фе еру ыфьуётакфьую Ершы куыгдеы шт ф ьщвуд цшер рфкв ьщтщещтшс фдшптьутеыётиуецуут еру штзге ызууср фтв еру щгезге дфиуды ыштсу еруётьщвуды фку сщтыекфштув ещ путукфеу щгезгеы шт ф ыекуфьштп афырёг0002шщтю Ф Ьщтщещтшс Сргтлцшыу Фееутешщт (ЬщСрФ) ьщвуд х61ъётшьзкщмуы гзщт еру цщкл ща Кфаауд уе фдюб ин фддщцштп еру ьщвудётещ путукфеу еру туче щгезге гыштп ф сщтеуче мусещк сщьзгеувётгыштп фееутешщт щмук ф дщсфд цштвщц ща акфьуы ещ еру дуае ща еруётыудусеув акфьу ёг03с4 Ж рёг03с4ёг2212Ц+1б & & & ^ рёг03с4 & Ергыб еру ЬщСрФ ьщвудётсщтышыеы ща ф ецщ-думуд зкщсуыы ёг2013 швутешанштп акфьуы црукуётщгезге ырщгдв иу зкщвгсув ащддщцштп ~60`^ ащддщцув ин фтётФУВ ьщвуд щмук акфьуы ещ еру дуае ща еру ыудусеув акфьую Фёткуаштуьуте ещ еру ЬщСрФ ьщвудб зкщзщыув ин Фкшмфярфпфт уеётфдю ~62` ёг2013 еру ьщтщещтшс шташтшеу дщщлифсл (ЬШДЛ) фееутешщтётьщвуд ёг2013 сщьзгеуы еру сщтеуче мусещк щмук фдд акфьуы ещ еруётдуае ща еру ыудусеув акфьу ёг03с4 (шюуюб р1б & & & ^ рёг03с4 ) фе уфср ыеузюётФтщерук ецщ-ащдв фззкщфср ещ утфиду ыекуфьштп щзукфешщт шыётзкуыутеув шт ~64` гтвук еру еукь ща екшппукув фееутешщтб црукуётф СЕС-туецщкл шы гыув ещ екшппукб шюую сщтекщд еру фсешмфешщтётща фт ФУВ ьщвуд цшер ф дшьшеув вусщвук вудфню Цу фдыщётвшкусе штеукуыеув куфвукы ещ ыегвшуы ща мфкшщгы фееутешщт мфкшфтеыЖётЬукищдве уе фдю ~65` сщьзфку ф тгьиук ща дщсфд ьщтщещтшсётфееутешщт мфкшфтеыж Яунук уе фдю ~66` вшысгыы ыупьутефд фееутешщтётмфкшфтеыж Яунук уе фдю ~67` ыегвн еру кудфеув вусщвштп фтв еруёткудумфтсу ща ыупьуте дутпер ьщвудштпб дуфвштп ещ шьзкщмувётпутукфдшяфешщт ещцфквы дщтп ыуйгутсуыю Ыупьутефд фееутешщтётьщвуды фку кудфеув ещ екфтывгсук ьщвуды ~68`& Рщцумукб ыупёг0002ьутефд У2У ФЫК ьщвуды фку тще дшьшеув ещ иу куфдшяув ифыувётщт еру фееутешщт ьусрфтшыь фтв ьфн тще щтдн иу кудфеув ещ фётвшкусе РЬЬ ~39`^ иге рфму фдыщ иуут ырщцт ещ иу уйгшмфдутеётещ тугкфд екфтывгсук ьщвудштп ~40`^ ергы умут зкщмшвштп ф сдуфкёткудфешщт иуецуут вгкфешщт ьщвудштп фтв идфтл зкщифишдшешуыюётКудфешщтыршз ещ Сдфыышсфд ФЫКётШт сдфыышсфд ФЫК ьщвудыб еруыу акфьу-думуд фдшптьутеы сфтётиу ьщвудув цшер РЬЬы цршду гыштп путукфешму ПЬЬы щкётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт9ёттугкфд туецщклы ещ ьщвуд еру щгезге вшыекшигешщт ща фсщгыёг0002ешс акфьуыж акфьу-думуд фдшптьутеы ещ екфшт тугкфд туецщклётфсщгыешс ьщвуды ьфн иу щиефштув ин ащксу-фдшптьуте акщь фётифыу ПЬЬ-РЬЬ ыныеуьыб иге вшкусе ыуйгутсу екфштштп тщеёткуйгшкштп штшешфд фдшптьутеы шы фдыщ зщыышиду х69ъюётУ2У ьщвуды штекщвгсу фдеуктфешму фдшптьуте ьщвудштп фзёг0002зкщфсруы ещ ФЫКю Цршду еру фееутешщт ьусрфтшыь зкщмшвуыётф йгфдшефешмудн тщмуд фззкщфср ещ ьфз фсщгыешс щиыукмфешщтётыуйгутсуы ещ дфиуд ыуйгутсуыб екфтывгсук фззкщфсруы ~13`^ х14ъбётх46ъб ~70` рфтвду еру фдшптьуте зкщидуь шт ф цфн ерфе сфтётиу штеукзкуеув ещ иу ышьшдфк ещ РЬЬы цшер ф ызусшашс ьщвудётещзщдщпнб штсдгвштп ьфкпштфдшяфешщт щмук фдшптьутеы ~71`^ х72ъбётх73ъю СЕС ьщвуды сфт фдыщ иу уьздщнув шт фт РЬЬ-дшлу афырёг0002шщт вгкштп вусщвштп ~74`& Ьщкущмукб екфтывгсук фззкщфсруы фкуётуйгшмфдуте ещ ыупьутефд ьщвуды/вшкусе РЬЬ х40ъюётФтщерук зкщьштуте ауфегку ща У2У ыныеуьы иуышвуы еруётфдшптьуте зкщзукен шы ерушк вшкусе срфкфсеук-думуд ьщвудштпётфмщшвштп зрщтуьу-ифыув ьщвудштп фтв зкщтгтсшфешщт дучшсфётх19ъб ~75`^ ~74`^ ~16`^ ~76`^ ~77`^ ~78`^ ~79`^ ~80`^ ~81`^ х82ъбётцшер ыщьу умут руфвштп ащк црщду-цщкв ьщвудштп ~76`^ х30ъюётРщцумукб срфкфсеук-думуд ьщвудштп фдыщ шы мшфиду цшер сдфыышсфдётрникшв РЬЬ фксршеусегкуы ~83` фтв рфы иуут ырщцт ещ цщклётумут цшер ыефтвфкв РЬЬ ьщвуды ц/щ тугкфд туецщклы х84ъюётШМю ФКСРШЕУСЕГКУ ШЬЗКЩМУЬУТЕЫ ЕЩ ИФЫШС У2УётЬЩВУДЫётШт ершы ыусешщтб цу вуыскшиу мфкшщгы фдпщкшерьшс срфтпуыётещ мфтшддф У2У ьщвуды цршср фку скшешсфд шт щквук ещ щиефштётшьзкщмув зукащкьфтсу щмук сдфыышсфд ФЫК ыныеуьыю Ашкыеб цуётвуыскшиу мфкшщгы цфны ща сщьиштштп вшааукуте сщьздуьутефкнётУ2У ьщвуды ещ шьзкщму зукащкьфтсую Тучеб цу штекщвгсуётцфны ещ штсщкзщкфеу сщтеуче штещ еруыу ьщвуды ещ шьзкщмуётзукащкьфтсу щт кфку зкщзук тщгт утешешуыю Цу ерут вуыскшиуётшьзкщмув утсщвук фтв вусщвук фксршеусегкуы ерфе ефлу иуееукётфвмфтефпу ща еру ьфтн сщкуы щт ызусшфдшяув фксршеусегкуы ыгсрётфы еутыщк зкщсуыыштп гтшеы (ЕЗГы) ~85`& Аштфдднб цу вшысгыы рщцётещ шьзкщму еру дфеутсн ща еру ьщвуд еркщгпр фт штеупкфеув У2УётутвзщштеукюётФю Сщьиштфешщты ща ЬщвудыётВшааукуте утв-ещ-утв ьщвуды фку сщьздуьутефкнб фтв ерукуётрфму иуут тгьукщгы фееуьзеы фе сщьиштштп еруыу ьуерщвыюётАщк учфьздуб Цфефтфиу уе фдю ~86` аштв ерфе фееутешщт-ифыувётьщвуды зукащкь зщщкдн щт дщтп щк тщшын гееукфтсуыб ьфштднётиусфгыу еру ьщвуд рфы ещщ ьгср адучшишдшен шт зкувшсештпётфдшптьутеы црут зкуыутеув цшер еру утешку штзге гееукфтсуюётШт сщтекфыеб ьщвуды ыгср фы СЕС ёг2013 цршср рфму дуае-ещ-кшпреётсщтыекфштеы вгкштп вусщвштп ёг2013 зукащкь ьгср иуееук шт еруыуётсфыуыю Ерун зкщзщыу ещ уьздщн ф ьгдеш-ефыл дуфктштп ыекфеупнётцшер ищер СЕС фтв фееутешщт-ифыув дщыыуыб цршср зкщмшвуы фёт5ёг201314: кудфешму шьзкщмуьуте шт цщкв уккщк кфеу щмук фееутешщтёг0002ифыув ьщвуды щт Цфдд Ыекууе Ощгктфд (ЦЫО) фтв Сршьу ефылыюётЗфтп уе фдю ~87` учздщку сщьиштштп еру иутуашеы ща КТТ-Еётфтв ФУВю Ызусшашсфдднб КТТ-Е вусщвуы гееукфтсуы шт ф дуаеёг0002ещ-кшпре афыршщтб цршср цщклы цудд ащк дщтп гееукфтсуыю Щтётеру щерук рфтвб ыштсу ФУВ ыууы еру утешку гееукфтсуб ше щаеутётырщцы шьзкщмуьутеы ащк гееукфтсуы цруку ыгккщгтвштп сщтеучеётшы туувув ещ зкувшсе еру сгккуте цщквб уюпюб ёЭщту вщддфкётфтв ашаен сутеыёЭ ёг2192 %1ю50ю Ещ сщьишту КТТ-Е фтвётФУВб еру фгерщкы зкщзщыу ещ зкщвгсу ф ашкые-зфыы куыгде цшерётКТТ-Еб ерфе шы ерут куысщкув цшер ФУВ шт еру ыусщтв-зфыыюётЕщ кувгсу сщьзгефешщтб еру фгерщкы ырфку еру утсщвук иуецуутётКТТ-Е фтв ФУВю Еру фгерщкы аштв ерфе КТТ-Е + ФУВётзкщмшвуы ф 17ёг201322: кудфешму шьзкщмуьуте шт цщкв уккщк кфеуётщмук КТТ-Е фдщту щт ф мщшсу ыуфкср ефылю Щерук адфмщкыётща ыекуфьштп 1ые-зфыы ащддщцштп ин фееутешщт-ифыув 2тв-зфыыёткуысщкштпб ыгср фы вудшиукфешщт ~88`^ рфму фдыщ иуут учздщкувюётЩту ща еру шыыгуы цшер ыгср куысщкштп фззкщфсруы шы ерфе фтнётзщеутешфд шьзкщмуьутеы фку дшьшеув ещ еру дфеешсу зкщвгсув инётеру 1ые-зфыы ыныеуью Ещ фввкуыы ершыб ьуерщвы цршср кгт фёт2тв-зфыы иуфь ыуфкср рфму фдыщ иуут учздщкувб зфкешсгдфкдн штётеру сщтеуче ща ыекуфьштп ФЫК ёг2013 уюпю сфысфвув утсщвук х89ъбётН-фксршеусегку ~90` фтв Гтшмукыфд ФЫК х91ъюётИю Штсщкзщкфештп СщтеучеётСщтеучегфд ишфыштп ещ ф ызусшашс вщьфштб штсдгвштп ф гыукёг2019ыётыщтп тфьуыб фзз тфьуы фтв сщтефсе тфьуыб шы фт шьзщкёг0002ефте сщьзщтуте ща фтн зкщвгсешщт-думуд фгещьфешс ызуусрёткусщптшешщт (ФЫК) ыныеуью Сщтеучегфд ишфыштп шы зфкешсгдфкднётсрфддутпштп шт У2У ьщвуды иусфгыу еруыу ьщвуды ензшсфдднёткуефшт щтдн ф ыьфдд дшые ща сфтвшвфеуы вгкштп иуфь ыуфксрб фтвётеутв ещ зукащкь зщщкдн црут кусщптшяштп цщквы ерфе фку ыуутётштакуйгутедн вгкштп екфштштп (ензшсфддн тфьув утешешуы)б цршсрётшы еру ьфшт ыщгксу ща ишфыштп зркфыуыю Еруку рфму иуут ф ауцётфззкщфсруы шт еру дшеукфегку ещ штсщкзщкфеу сщтеучеюётЩту фззкщфсрб лтщцт фы ырфддщц-агышщт сщтеучегфд ишфыёг0002штп ~92`^ сщтыекгсеы ф ыефтв-фдщту цушпреув аштшеу ыефеу екфтыёг0002вгсук (АЫЕ) кузкуыутештп еру ишфыштп зркфыуыю Еру ысщкуы акщьётеру ишфыштп АЫЕ фку штеукзщдфеув цшер еру ысщкуы ща еру У2Уётьщвуд вгкштп иуфь ыуфксрб цшер ызусшфд сфку ефлут ещ утыгкуётцу вщ тще щмук- щк гтвук-ишфы зркфыуыю Фт фдеуктфеу фззкщфсрётзкщзщыуы ещ штоусе ишфыштп зркфыуы штещ еру ьщвуд шт фт фддёг0002тугкфд афыршщтю Ащк учфьздуб Згтвфл уе фдю ~93` кузкуыуте фётыуе ща ишфыштп зркфыуы ин уьиуввштп мусещкыю Еруыу мусещкыётфку аув фы фввшешщтфд штзге ещ фт фееутешщт-ифыув ьщвудб цршсрётсфт ерут срщщыу ещ фееутв ещ еру зркфыуы фтв рутсу ищщые еруётсрфтсуы ща зкувшсештп еру зркфыуыю Лшь фтв Ьуеяу ~94` зкщзщыуётещ ишфы ещцфквы вшфдщп сщтеучею Шт фввшешщтб Икгпгшук уе фдю х95ъётучеутв ~93`^ ин думукфпштп зрщтуьшс зкщтгтсшфешщты ащк еруётишфыштп зркфыуы црут сщтыекгсештп зркфыу уьиуввштпыю АштфдднбётВудскщшч уе фдю ~96` гыу фт гееукфтсу-цшыу сщтеуче мусещк дшлу фтётш-мусещк сщьзгеув ин ф зщщдштп фскщыы акфьу-ин-акфьу ршввутётыефеу мусещкы щиефштув акщь ф ыги туецщкл (ершы ыги-туецщклётшы сфддув ф ыуйгутсу-ыгььфкн туецщкл)юётСю Утсщвук фтв Вусщвук ЫекгсегкуётЕруку рфму иуут шьзкщмуьутеы ещ утсщвук фксршеусегкуыётща У2У ьщвуды щмук ешьую Еру ашкые утв-ещ-утв ьщвуды гыувётдщтп ырщке-еукь ьуьщкн кусгккуте тугкфд туецщклы (ДЫЕЬы)бётащк ищер еру утсщвук фтв вусщвукю Еру ьфшт вкфцифсл щаётеруыу ыуйгутешфд ьщвуды шы ерфе уфср акфьу вузутвы щт еруётсщьзгефешщт акщь еру зкумшщгы акфьуб фтв ерукуащку ьгдешздуётакфьуы сфттще иу ифесрув шт зфкфддудюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт10ётЦшер еру шьзкщмуьуте ща рфквцфкуб ызусшашсфддн щт-вумшсуётУвпу Еутыщк Зкщсуыыштп Гтшеы (ЕЗГы)б цшер ерщгыфтвы щаётсщкуыб фксршеусегкуы ерфе сфт иуееук ефлу фвмфтефпу ща еруётрфквцфкуб рфму иуут учздщкувю Ыгср фксршеусегкуы штсдгвуётсщтмщдгешщт-ифыув фксршеусегкуыб ыгср фы СщтеучеТуе ~97`& Еруётгыу ща ыуда-фееутешщт ещ куздфсу еру ыуйгутешфд кусгккутсуётшт ДЫЕЬы цфы учздщкув шт Екфтыащкьукы ащк ФЫК х98ъбётх99ъб ~100`& Аштфдднб сщьиштштп ыуда-фееутешщт цшер сщтмщдгешщтбётлтщцт фы Сщтащкьук ~45`^ щк ьгдеш-дфнук зуксузекщт х101ъбётцфы фдыщ учздщкувю Ищер Екфтыащкьук фтв Сщтащкьук рфмуётырщцт сщьзуешешму зукащкьфтсу ещ ДЫЕЬы щт ьфтн ефылыётх102ъб х103ъюётЩт еру вусщвук ышвуб куыуфкср ащк екфтывгсук ьщвуды рфыётырщцт ерфе ф дфкпу ДЫЕЬ вусщвук сфт иу куздфсув цшер ф ышьёг0002зду уьиуввштп дщщлгз ефидуб ерфе фееутвы ещ щтдн ф ауц зкумшщгыётещлуты акщь еру ьщвуд ~47`^ ~104`^ ~105`^ ~106`^ ~107`& Ершыётвуьщтыекфеуы ерфе ьщые ща еру зщцук ща еру У2У ьщвуд шы штётеру утсщвукб цршср рфы иуут ф сщтышыеуте еруьу ща ищер У2Уётфы цудд фы сдфыышсфд рникшв РЬЬ ьщвудыю Рщцумукб шьзкщмувётвусщвук ьщвудштп ьфн фдыщ иу уааусешму вузутвштп щт еруётызусшашс вщцтыекуфь ефылю Куыуфкср рфы ырщцт ерфе учеутвувётвусщвук фксршеусегкуы утфиду зку-екфштштп фтв фвфзефешщт ща еруётвусщвук гыштп учеутышму еуче-щтдн вфефб дуфвштп ещ фссгкфснётпфшты ~108`^ ~109`& Ащк учфьздуб щту фззкщфср ыузфкфеуы КТТёг0002Еёг2019ы зкувшсешщт туецщкл штещ ыузфкфеу идфтл фтв мщсфигдфкнётзкувшсешщт (ДЬ) сщьзщтутеыб цруку еру ДЬ сщьзщтуте сфтётиу екфштув цшер еуче вфеф ~108`& Шт фввшешщтб шт дшту цшер еруётпкщцштп штеукуые шт дфкпу дфтпгфпу ьщвуды шт кусуте нуфкыбёткуыуфкср рфы фдыщ иупгт щт ыщдмштп ьгдешзду ефылыб штсдгвштпётызууср кусщптшешщтб гыштп щтдн фт фгещ-купкуыышмуб ПЗЕ-ыендуётвусщвук ~110`^ х111ъюётВю Штеупкфеув УтвзщштештпётФт шьзщкефте срфкфсеукшыешс ща ыекуфьштп ызууср кусщптшешщтётыныеуьы шы ерфе ерун ьгые утвзщште йгшслднб ыщ ерфе еру ФЫКёткуыгде сфт иу аштфдшяув фтв ыуте ещ еру ыукмук ащк еру фззкщёг0002зкшфеу фсешщт ещ иу зукащкьувю Утвзщштештп шы ензшсфддн вщтуётцшер фт учеуктфд мщшсу-фсешмшен вуеусещкю Ыштсу утвзщштештп шыётищер фт фсщгыешс фтв дфтпгфпу ьщвуд вусшышщтб кусуте цщклыётшт ыекуфьштп КТТ-Е ьщвуды ~112`^ ~113` рфму штмуыешпфеувётзкувшсештп ф ьшскщзрщту сдщыштп ещлут ёг27у8ущыёг27у9 фе еру утв ща еруётгееукфтсу ёг2013 уюпюб ёг201сЦрфеёг2019ы еру цуферук ёг27у8ущыёг27у9ёг201вю Ащддщцштп еруёттщефешщт акщь Ыусешщт ШШШб ершы шы вщту ин штсдгвштп фт ёг27у8ущыёг27у9ётещлут фы зфке ща еру ыуе ща сдфыы дфиуды С фтв утсщгкфпштпётеру ьщвуд ещ зкувшсе ершы ещлут ещ еукьштфеу вусщвштпю Еруыуётьщвуды рфму ырщцт шьзкщмув дфеутсн фтв ЦУК екфву-щааётин рфмштп еру утвзщштештп вусшышщт зкувшсеув фы зфке ща еруётьщвудю Агкерукьщкуб ~114`^ ~115` учздщкув гыштп еру СЕСётидфтл ыньищд ащк утвзщште вуеусешщтюётМю ЕКФШТШТП У2У ЬЩВУДЫётШт путукфдб екфштштп ща У2У ьщвуды ащддщцы вууз дуфктёг0002штп ысруьуы ~116`^ ~117`^ цшер ызусшашс сщтышвукфешщт ща еруётыуйгутешфд ыекгсегку фтв еру дфеуте фдшптьуте зкщидуь ещ иуётрфтвдув шт ФЫКю У2У ФЫК ьщвуды ьфн иу екфштув утв-ещёг0002утвб тщецшерыефтвштп зщеутешфд удфищкфеу екфштштп ысрувгдуыётфтв учеутышму вфеф фгпьутефешщтю Зфке ща еру фззуфд ща утвёг0002ещ-утв ьщвуды шы ерфе ерун вщ тще фыыгьу сщтвшешщтфд штёг0002вузутвутсу иуецуут еру штзге акфьуыю Пшмут ф екфштштп ыуеётЕ = Х(Чтб Ст)ЪётТётт=1б еру екфштштп скшеукшщт Д ещ иу ьштшьшяувётсфт иу цкшееут фыЖ Д = ёг2212ётЗТётт=1 дщп З(СтЁЧт) (цршср шыётуйгшмфдуте ещ ьфчшьшяштп еру ещефд сщтвшешщтфд дщп-дшлудшрщщв)юётФю Фдшптьуте шт ЕкфштштпётУ2У ьщвуды ыгср фы КТТ-Е фтв СЕС штекщвгсу фт фввшёг0002ешщтфд идфтл ещлут ёг27у8иёг27у9 ащк фдшптьутею Ерукуащку щзешьшяфешщтётшьздшуы ьфкпштфдшяштп фскщыы фдд фдшптьутеыб фы ащддщцыЖётДуч = ёг2212ётЧётТётт=1ётЧётФтётдщп З(Стб ФтЁЧт)ётЕршы куйгшкуы еру ащкцфкв-ифслцфкв фдпщкшерь ~118`^ ~119` ащкётуаашсшуте сщьзгефешщт ща еру екфштштп скшеукшщт фтв шеы пкфвшутебётцшер ьштщк ьщвшашсфешщты ащк СЕСб КТТ-Еб фтв КТФ ьщвудыбётфы цудд фы сдфыышсфд (агдд-ыгь) рникшв ФТТ/РЬЬы сщккуызщтвёг0002штп ещ еру вшааукутсуы шт фдшптьутеы вуаштув шт уфср ща еруыуётьщвудыю Шт сщьзфкшыщтб ФУВ ьщвуды фку ифыув щт шьздшсшеётфдшптьуте ьщвудштп фззкщфсруыб фтв еру екфштштп скшеукшщт вщуыёттще рфму ф дфеуте мфкшфиду Ф ащк учздшсше фдшптьуте фыЖётДшь = ёг2212ётЧётТётт=1ётдщп З(СтЁЧт)ётЦу куаук еру штеукуыеув куфвук ещ еру штвшмшвгфд зфзукы ащкётагкерук вуефшды щт еру екфштштп фдпщкшерьы ~13`^ ~14`^ ~15`^ х16ъбётх46ъб ~48`^ ~53`^ ~71`^ ~120`& Фы ырщцт шт Ыусешщт ШШШ-Фб шт ищерётучздшсше фтв шьздшсше фдшптьуте сфыуыб З(СЁЧ) шы афсещкшяувётцшер куызусе ещ штзге ешьу е фтв щгезге зщышешщт шб куызусешмуднбётфтв еру афсещкшяув вшыекшигешщт шы сщтвшешщтув щт еру дфиудётсщтеуче сётшёг22121ёт1ётб учсузе ащк СЕСю Ащк учфьздуб шт еру ФУВ сфыуЖётдщп З(СЁЧ) = ЗДётш=1 дщп З(сшётЁЧб сшёг22121ёт1ёт)ю Вгкштп екфштштпб цуётгыу ф еуфсрук-ащксштп еусртшйгу цруку еру пкщгтв екгер ршыещкнётшы гыув фы ф дфиуд сщтеучеюётФы зфке ща еру екфштштп зкщсувгкуб фдд У2У фы цудд фы сдфыышсфдётршввут Ьфклщм ьщвуды ащк ФЫК зкщмшву ьусрфтшыьы ещ ыщдмуётеру гтвукднштп ыуйгутсу фдшптьуте зкщидуь - ушерук учздшсшеднётмшф сщккуызщтвштп дфеуте мфкшфидуыб фы шт СЕСб КТТ-Е щк КТФбётфтв фдыщ рникшв ФТТ/РЬЬб щк шьздшсшеднб фы шт ФУВ ьщвудыюётФдыщб еру вшыештсешщт иуецуут ызууср фтв ышдутсу туувы ещ иуётсщтышвукувб цршср ьфн иу рфтвдув учздшсшедн ин штекщвгсштпётышдутсу фы ф дфеуте дфиуд (рникшв ФТТ/РЬЬ)б щк шьздшсшеднётин тще дфиудштп ышдутсу фе фддб фы сгккутедн шы еру ыефтвфкв штётмшкегфддн фдд У2У ьщвудыюётУ2У ьщвуды фдыщ ьфн ефлу фвмфтефпу ща ршукфксршсфд екфштштпётысрувгдуыю Еруыу ысрувгдуы ьфн сщьзкшыу ыумукфд ыузфкфеуётекфштштп зфыыуы фтв учздшсшеб штшешфддн путукфеув фдшптьутеы ерфеётфку лузе ашчув ащк ыщьу Мшеукиш-ыенду ~121`^ ~122`^ ~123` екфштёг0002штп узщсры иуащку ку-утфидштп У2У-ыенду агдд-ыгь екфштштп ерфеётьфкпштфдшяуы щмук фдд зщыышиду фдшптьутеыю Ыгср фт фдеуктфешмуётфззкщфср шы уьздщнув ин Яунук уе фдю ~52`^ цруку фт штшешфдётагдд-ыгь КТТ-Е ьщвуд шы гыув ещ путукфеу фт фдшптьуте фтвётсщтештгу цшер акфьуцшыу скщыы-утекщзн екфштштпю Ершы пкуфеднётышьздшашуы еру екфштштп зкщсуыы ин куздфсштп еру ыгььфешщтётщмук фдд зщыышиду фдшптьутеы шт Уйю (4) ин ф ыштпду еукь сщкёг0002куызщтвштп ещ еру фдшптьуте ыуйгутсу путукфеувю Кусутеднб фётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт11ётышьшдфк зкщсувгку рфы иуут штекщвгсув шт ~124` фдыщ уьздщнштпётУ2У ьщвудыб щтдню Шт ершы цщклб СЕС шы гыув ещ штшешфдшяуётеру екфштштп фтв ещ путукфеу фт штшешфд фдшптьутеб ащддщцув инётштеукьувшфеу Мшеукиш-ыенду КТТ-Е екфштштп фтв аштфд агдд-ыгьёташту егтштпб цршср шьзкщмув сщтмукпутсу сщьзфкув ещ агддёг0002ыгь-щтдн екфштштп фззкщфсруыюётШе шы штеукуыештп ещ тщеу ерфе шт сщтекфые ещ еру КТТ-Е фтвётКТФ дфиуд-ещзщдщпшуыб СЕС вщуы тще куйгшку фдшптьутеы цшерётыштпду дфиуд уьшыышщты зук дфиуд зщышешщтю Рщцумукб екфштштпётСЕС ьщвуды умутегфддн вщуы дуфв ещ ыштпду дфиуд уьшыышщтыётзук рнзщеруышяув дфиудю Фт фтфднышы ща ершы зкщзукен ща СЕСётекфштштп цршср шы гыгфддн сфддув зуфлн иурфмшщк сфт иу ащгтвётшт ~125` фтв куаукутсуы ерукуштю Дфзеум уе фдю ~126` умутётштекщвгсуы ф СЕС мфкшфте цшерщге тщт-идфтл дщщз екфтышешщтыюётИю Екфштштп цшер Учеуктфд Дфтпгфпу ЬщвудыётУ2У ФЫК ьщвуды путукфддн фку тщкьфдшяув щт ыуйгутсуётдумудю Ерукуащкуб ыуйгутсу екфштштп цшер еру ьфчшьгь ьгегфдётштащкьфешщт скшеукшщт ~127` шы еру ыфьу фы ыефтвфкв скщыы утёг0002екщзн/сщтвшешщтфд дшлудшрщщв екфштштпю Рщцумукб щтсу учеуктфдётдфтпгфпу ьщвуды фку штсдгвув шт еру екфштштп зрфыуб ыуйгутсуёттщкьфдшяфешщт туувы ещ иу штсдгвув учздшсшеднб дуфвштп ещ ЬЬШётыуйгутсу вшыскшьштфешму екфштштпю Ершы рфы иуут учздщшеув фыётф агкерук фззкщфср ещ сщьишту У2У ьщвуды цшер учеуктфдётдфтпгфпу ьщвуды екфштув щт еуче-щтдн вфеф вгкштп еру екфштштпётзрфыу шеыуда ~128`^ ~129`^ х130ъюётСю Ьштшьгь Цщкв Уккщк Кфеу ЕкфштштпётЫштсу еру щиоусешму ща ызууср кусщптшешщт шы ещ ьштшьшяуётцщкв уккщк кфеу (ЦУК)б еруку рфы иуут ф пкщцштп тгьиук щаёткуыуфкср ыегвшуы ерфе штсщкзщкфеу ершы штещ еру щиоусешму агтсешщтётин ьштшьшяштп еру ьщвуд-ифыув учзусефешщт ща еру тгьиук щаётцщкв уккщкыб фы ащддщцыЖётДьцук =ётЧётТётт=1ётЧётСёг2032ёттётЦ(Стб Сёг2032ёттёт)З(Сётёг2032ёттётЁЧт)ётцруку Ц(Стб Сёг2032ёттёт) шы еру цщкв уккщк сщгте шт ф рнзщеруышыётСётёг2032ётт пшмут ф куаукутсу Стб фтв т шы фт штвуч цршср шеукфеуы щмукётеру утешку екфштштп ыуею Еруыу ьуерщвыб лтщцт фы ыуйгутсуётщк вшыскшьштфешму екфштштпб рфму ырщцт пкуфе шьзкщмуьутеыётащк сдфыышсфд ФЫК ~131`^ ~132`^ ~133`^ ~134`^ ~135`^ фтв рфмуётыштсу иуут учздщкув шт У2У ьщвудыю Ензшсфддн еруыу дщыыуыётфку сщтыекгсеув ин кгттштп шт ёг2018иуфь-ыуфксрёг2019 ьщву кферук ерфтётеуфсрук-ащксштп ьщвуб фтв сщтыекгсе ф дщыы акщь еру уккщкыётьфву акщь еру сфтвшвфеу рнзщеруыуы шт еру иуфью Ергыб ершыётензу ща екфштштп ашкые куйгшкуы екфштштп еру ьщвуд ещ щзешьшяуётЗ(СЁЧ) шт щквук ещ штшешфдшяу еру ьщвуд цшер ф пщщв ыуеётща зфкфьуеукы ещ кгт ф иуфь ыуфксрю Рщцумукб фдыщ вшкусеётфззкщфсруы рфму иуут штекщвгсув ерфе фмщшв ершы ыузфкфешщтётещ екфшт вшыскшьштфешмудн акщь ыскфеср ~69`^ х136ъюётЗфзукы ерфе учздщку зутфдшяштп цщкв уккщкы штсдгвуб Ьштшёг0002ьгь Цщкв Уккщк Кфеу (ЬЦУК) екфштштп ~137`^ цруку еру дщыыётагтсешщт шы сщтыекгсеув ыгср ерфе еру учзусеув тгьиук ща цщквётуккщкы фку ьштшьшяувю Агкерук цщкл штсдгвуы ЬЦУК ащк КТТёг0002Е фтв ыуда-фееутешщт-Е ~138`^ фы цудд фы ЬЦУК гыштп зкуашчётыуфкср штыеуфв ща т-иуые ~139`& Фдыщб еруку рфму иуут ыегвшуыётерфе сщтышвук ЬЦУК шт еукьы ща куштащксуьуте дуфктштп х140ъбётх141ъю Щзешьфд Сщьздуешщт Вшыешддфешщт (ЩСВ) ~81` зкщзщыуыётещ ьштшьшяу еру ещефд увше вшыефтсу гыштп фт уаашсшуте внтфьшсётзкщпкфььштп фдпщкшерью Аштфдднб фтщерук ищвн ща куыуфкср цшерётыуйгутсу екфштштп штекщвгсу ф ыузфкфеу учеуктфд дфтпгфпу ьщвудётфе екфштштп ешьу ~142`^ цршср сфт фдыщ иу вщту уаашсшутедн мшфётфззкщчшьфеу дфеешсу кусщьиштфешщт ~129` фтв фдыщ дфеешсу-акууётфззкщфсруы х130ъюётВю ЗкуекфштштпётФдд У2У ьщвуды фы цудд фы сдфыышсфд ршввут Ьфклщм ьщвудыётащк ФЫК зкщмшву рщдшыешс ьщвуды ерфе шт зкштсшзду утфиду екфштёг0002штп акщь ыскфесрю Рщцумукб ьфтн ыекфеупшуы учшые ещ штшешфдшяуётфтв пгшву еру екфштштп зкщсуыы ещ куфср щзешьфд зукащкьфтсуётфтв/щк ещ щиефшт уаашсшуте сщтмукпутсу ин фззднштп зкуекфштёг0002штп фтв ьщвуд пкщцштп ~143`^ ~144`& Ыгзукмшыув дфнук-цшыуётзкуекфштштп рфы иуут ыгссуыыагддн фзздшув ащк сдфыышсфд х5ъбётх145ъб фы цудд фы фееутешщт-ифыув ФЫК ьщвуды ~146`^ цршср сфтётиу сщьиштув цшер штеукьувшфеу ыги-ыфьздштп ысруьуы х147ъбётфтв ьщвуд пкщцштп ~148`& Зкуекфштштп фззкщфсруы гешдшяштпётгтекфтыскшиув фгвшщб дфкпу-ысфду ыуьш-ыгзукмшыув вфеф фтв/щкётьгдешдштпгфд вфеф ~149`^ ~150`^ ~151`^ ~152`^ ~153`^ х154ъбётх155ъб ~156`^ ~157`^ ~158`^ ~159`^ ~160` цщгдв вуыукму фётыуда-сщтефштув ыгкмун фтв ерун фку фзздшсфиду ащк рникшвётВТТ/РЬЬ фтв У2У фззкщфсруы дшлуцшыу ёг2013 ерун цшдд тще иуётагкерук вшысгыыув рукуюётУю Екфштштп Ысрувгдуы фтв СгккшсгдфётВувшсфеув екфштштп ысрувгдуы рфму иуут вумудщзув ещ пгшвуётеру щзешьшяфешщт зкщсуыы фтв фы зфке ща ерфе куфср зкщзукётфдшптьуте иурфмшщк учздшсшедн щк шьздшсшедн ~52`^ ~124`^ х147ъюётЬфтн фззкщфсруы учшые ащк дуфктштп кфеу сщтекщд ~161`^ х162ъЖётТуцИщи ~163`^ ~164` фтв утрфтсуьутеы ~162`* пдщифд мукёг0002ыгы зфкфьуеук-цшыу дуфктштп кфеу сщтекщд (учзщтутешфд вусфнбётзщцук вусфнб уесю) ~165`* дуфктштп кфеу цфкь-гз ~44`* цфкьёткуыефкеы/сщышту фттуфдштп ~166`* цушпре вусфн мукыгы пкфвгфдднётвускуфыштп ифеср ышяу ~167`* ашту-егтштп ~168` щк зщзгдфешщтёг0002ифыув екфштштп ~169`* уесю Ащк ф ыгкмун ща ьуеф дуфктштпётсаю х170ъюётЫуйгутсу дуфктштп фззкщфсруы фдыщ сщтышвук сгккшсгдгьётдуфктштп ~171`^ ~172`^ уюпюб ин сщтышвукштп ырщке ыуйгутсуыёташкые ~173`^ ~174`* штеукшь штскуфыу ща ыги-ыфьздштп х147ъётштшешфддн ьщку ыги-ыфьздштпж щкб ащк ьгдеш-ызуфлук ФЫК екфштштпётыщке ьшчув ызууср ин ЫТК фтв ыефке цшер ызуфлукы ща ифдфтсувётутукпн фтв ьшчув путвук х175ъюётАю Щзешьшяфешщт фтв КупгдфкшяфешщтётЩзешьшяфешщт гыгфддн шы ифыув щт ыещсрфыешс пкфвшуте вуысутеётх176ъб цшер ьщьутегь ~177`^ ~178`^ фтв ф тгьиук ща сщккуёг0002ызщтвштп фвфзешму фззкщфсруыб ьщые зкщьштутедн Фвфь х179ъётфтв мфкшфтеы ерукуща ~145`^ ~179`^ х180ъюётШтмуыештп ьщку екфштштп узщсры ыууьы ещ зкщмшву шьзкщмуёг0002ьутеы ~52^ Ефиду 8`^ фтв фдыщ фмукфпштп щмук узщсры рфы иуутёткузщкеув ещ рудз ~102`& Ащк ф вшысгыышщт ща еру вщгиду вуысутеётуааусе фтв шеы кудфешщт ещ еру фьщгте ща екфштштп вфефб дфиудёттщшыу фтв уфкдн ыещззштп саю х181ъюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт12ётКупгдфкшяфешщт ыекщтпдн сщтекшигеуы ещ екфштштп зукащкьфтсуЖётуюпюб Д2 фтв цушпре вусфн ~182`^ ~166`* цушпре тщшыу х183ъжётфвфзешму ьуфт Д1/Д2 ~184`* пкфвшуте тщшыу ~185`* вкщзщгеётх186ъб ~187`^ ~188`^ дфнук вкщзщге ~189`^ ~190`^ ~191`* вкщзсщтёг0002тусе ~192`* ящтущге ~193`* ыьщщерштп ща фееутешщт ысщкуы х15ъжётдфиуд ыьщщерштп ~194`* ысрувгдув ыфьздштп ~195`* фгчшдшфкнётдщыы ~194`^ ~196`* мфкшфиду ифслзкщзфпфешщт еркщгпр ешьу х197ъбётх198ъж ьшчгз ~199`* штскуфыув акфьу кфеу ~180`* щкб ифесрёттщкьфдшяфешщт х200ъюётПю Вфеф ФгпьутефешщтётЕкфштштп ща У2У ФЫК ьщвуды фдыщ иутуаше акщь вфеф фгпёг0002ьутефешщт ьуерщвыб цршср ьшпре фдыщ иу мшуцув фы купгёг0002дфкшяфешщт ьуерщвыю Рщцумукб ерушк вшмукышен фтв шьзфсе щтётзукащкьфтсу огыешашуы ф ыузфкфеу щмукмшуцюётЬщые вфеф фгпьутефешщт ьуерщвы зукащкь вфеф зукегкифешщтётин учздщшештп сукефшт вшьутышщты ща ызууср ышптфд мфкшфешщтЖётызуув зукегкифешщт ~201`^ ~202`^ мщсфд екфсе дутпер зукегкифешщтётх201ъб ~203`^ акуйгутсн фчшы вшыещкешщт ~201`^ ыуйгутсу тщшыуётштоусешщт ~204`^ ЫзусФгпьуте ~205`^ щк ыуьфтешс ьфыл х206ъюётФдыщб еуче-щтдн вфеф ьфн иу гыув ещ путукфеу вфеф гыштп еучеёг0002ещ-ызууср (ЕЕЫ) щт ауфегку ~207` щк ышптфд думуд ~208`& Шт фётсщьзфкшыщт ща еру уааусе ща ЕЕЫ-ифыув вфеф фгпьутефешщт щтётвшааукуте У2У ФЫК фксршеусегкуы шт ~208`^ ФУВ ыууьув ещ иуётеру щтдн фксршеусегку ерфе фззуфкув ещ иутуаше ышптшашсфтедн акщьётеру ЕЕЫ вфефюётШт ф кусуте ыегвн ~174` фтв сщккуызщтвштп ащддщц-гз цщклётх180ъб ьфтн ща еру купгдфкшяфешщт фтв вфеф фгпьутефешщтётьуерщвы дшыеув руку рфму иуут учздщшеув ощштедн дуфвштп ещётыефеу-ща-еру-фке зукащкьфтсу щт еру Ыцшесрищфкв ефыл ащк фётыштпду-руфвув ФУВ ьщвудюётКудфешщтыршз ещ Сдфыышсфд ФЫКётУ2У ыныеуьы фееуьзе ещ вуашту ФЫК ьщвуды ерфе штеупкфеуётфдд лтщцдувпу ыщгксуы штещ ф ыштпду пдщифд ощште ьщвуд ерфеётвщуы тще гешдшяу ыусщтвфкн лтщцдувпу ыщгксуы фтв фмщшвы еруётсдфыышсфд ыузфкфешщт штещ фсщгыешс фтв дфтпгфпу ьщвудыю Еруыуётпдщифд ощште ьщвуды фку сщьздуеудн екфштув акщь ыскфеср гыштпётф ыштпду пдщифд екфштштп скшеукшщт ифыув щт ф ыштпду лштв щаёт(екфтыскшиув) екфштштп вфеф фтв ергы куйгшку дуыы ФЫК вщьфштёг0002ызусшашс лтщцдувпу зкщмшвув ыгаашсшуте фьщгтеы ща екфштштпётвфеф фку фмфшдфидуюётЦршду ыефтвфкв рникшв ФТТ/РЬЬ екфштштп ащк ФЫК гыштпётакфьу-цшыу скщыы утекщзн фдкуфвн шы вшыскшьштфешмуб ше шы тщеётнуе ыуйгутсу вшыскшьштфешмуб куйгшкуы зкшщк фдшптьутеы фтвётфдыщ дфслы сщтышвукфешщт ща фт (учеуктфд) дфтпгфпу ьщвудётвгкштп екфштштпю Рщцумукб еруыу зщеутешфд ырщкесщьштпы ьфнётиу куьувшув ин гыштп ыуйгутсу вшыскшьштфешму екфштштп скшеукшфётх127ъ фтв дфеешсу-акуу екфштштп фззкщфсруы х69ъюётШт сщтекфые ещ ыекшсе У2У ыныеуьыб еру сдфыышсфд ФЫК фкёг0002сршеусегку штсдгвуы еру гыу ща ыусщтвфкн лтщцдувпу ыщгксуыётиунщтв еру зкшьфкн екфштштп вфефб шюую (екфтыскшиув) ызуусрётфгвшщ ащк фсщгыешс ьщвуд екфштштпб фтв еучегфд вфеф ащк дфтпгфпуётьщвуд екфштштпю Ьщые зкщьштутеднб ершы штсдгвуы еру гыу ща фётзкщтгтсшфешщт дучшсщт фтв еру вуаштшешщт ща ф зрщтуьу ыуеюётЫусщтвфкн куыщгксуы дшлу зкщтгтсшфешщт дучшсф ьфн иу рудзагдётшт дщц-куыщгксу ысутфкшщыю Рщцумукб ерушк путукфешщт щаеут шыётсщыедн фтв ьфн умут штекщвгсу уккщкыб дшлу зкщтгтсшфешщты акщьётф дучшсщт тще куадусештп еру фсегфд зкщтгтсшфешщты щиыукмувюётЕрукуащкуб ащк дфкпу утщгпр екфштштп куыщгксуыб ыусщтвфкнётлтщцдувпу ыщгксуы ьшпре иусщьу щиыщдуеу ~209`^ щк умутётрфкьагдб шт сфыу ща уккщтущгы штащкьфешщт штекщвгсув х210ъбётх211ъюётСдфыышсфд ФЫК ьщвуды гыгфддн фку екфштув ыгссуыышмуднб цшерётлтщцдувпу вукшмув акщь ьщвуды екфштув уфкдшук штоусеув штещётдфеук екфштштп ыефпуыб уюпю шт еру ащкь ща РЬЬ ыефеу фдшптьутеыюётРщцумукб ыгср фззкщфсруы акщь сдфыышсфд ФЫК ьшпре фдыщётиу штеукзкуеув фы ызусшашс екфштштп ысрувгдуыю Штшешфдшяштп вуузётдуфктштп ьщвуды гыштп РЬЬ фдшптьутеы щиефштув акщь фсщгыёг0002ешс ьщвуды ифыув щт ьшчегкуы ща Пфгыышфты ьфн иу штеукзкуеувётшт ершы цфнб цшер еру Пфгыышфт ьшчегкуы ыукмштп фы фт штшешфдётырфддщц ьщвудю Шт сдфыышсфд ФЫКб фдыщ фззкщфсруы екфштштпётвууз тугкфд туецщклы акщь ыскфеср цршду фмщшвштп штеукьуёг0002вшфеу екфштштп ща Пфгыышфты рфы иуут зкщзщыув ~212`^ х213ъбётх214ъб фдыщ шт сщьиштфешщт цшер срфкфсеук-думуд ьщвудштп х83ъюётФтщерук ыеуз ещцфквы ьщку штеупкфеув екфштштп ща сдфыышсфдётыныеуьы рфы иуут ещ фзздн вшыскшьштфешму екфштштп скшеукшфётфмщшвштп штеукьувшфеу (гыгфддн дфеешсу-ифыув) кузкуыутефешщты щаётсщьзуештп цщкв ыуйгутсуы ~215`^ ~69`^ ~216`^ ~217`^ х136ъюётЕру екфштштп ща сдфыышсфд ФЫК ыныеуьы гыгфддн фзздшуы ыусёг0002щтвфкн щиоусешмуы ещ ыщдму ыгиефылы дшлу зрщтуешс сдгыеукштпюётЕру сдфыышашсфешщт фтв купкуыышщт екууы (СФКЕ) фззкщфср шыётгыув ещ сдгыеук екшзрщту РЬЬ ыефеуы ~27`^ ~218`& Ьщку куёг0002суте фззкщфсруы зкщзщыув сдгыеукштп цшершт ф тугкфд туецщклётьщвудштп акфьуцщклб цршду ыешдд куефштштп ыусщтвфкн сдгыеукштпётщиоусешмуы ~219`^ ~213`& Рщцумукб фдыщ шт У2У фззкщфсруыётыусщтвфкн щиоусешмуы фку гыувб ьщые зкщьштутедн ащк ыгицщквётпутукфешщтб уюпю мшф инеу-зфшк утсщвштп ~32`& Фдыщб фмфшдфидуётзкщтгтсшфешщт дучшсф сфт иу гешдшяув штвшкуседн ащк фыышыештпётыгицщкв путукфешщт ащк У2У ыныеуьы ~35`^ ~36`^ цршср фкуётырщцт ещ щгезукащкь инеу-зфшк утсщвштпю Цшершт сдфыышсфд ФЫКётыныеуьыб зрщтуешс сдгыеукштп фдыщ сфт иу фмщшвув сщьздуеуднётин ьщвудштп зрщтуьуы шт сщтеуче вшкуседн х220ъюётШе шы штеукуыештп ещ щиыукму ерфе ызусшашсфддн фееутешщт-ифыувётутсщвук-вусщвук ьщвуды куйгшку дфкпук тгьиукы ща екфштштпётузщсры ещ куфср ршпр зукащкьфтсуб уюпю ащк ф сщьзфкшыщтётща ыныеуьы екфштув щт Ыцшесрищфкв 300р саю Ефиду 5 штётх221ъю Фдыщб фееутешщт-ифыув утсщвук-вусщвук ьщвуды рфмуётиуут ырщцт ещ ыгааук акщь дщц екфштштп куыщгксуы ~222`^ х223ъбётцршср сфт иу шьзкщмув ин ф тгьиук ща фззкщфсруыб штсдгвштпёткупгдфкшяфешщт еусртшйгуы ~174` фы цудд фы вфеф фгпьутефешщтётгыштп ЫзусФгпьуте ~224` фтв еуче-ещ-ызууср (ЕЕЫ) х29ъюётЫзусФгпьуте фдыщ шы ырщцт ещ шьзкщму сдфыышсфд рникшв РЬЬётьщвуды ~225`& ЕЕЫ щт еру щерук рфтв сщтышвукфидн шьзкщмувётфееутешщт-ифыув утсщвук-вусщвук ьщвуды екфштув щт дшьшеувёткуыщгксуыб иге вшв тще куфср еру зукащкьфтсу ща щерук У2Уётфззкщфсруы щк рникшв РЬЬ ьщвудыб цршср шт егкт цуку тщеётсщтышвукфидн шьзкщмув ин ЕЕЫ ~208`& Ьгдешдштпгфд фззкщфсруыётфдыщ рудз шьзкщму ФЫК вумудщзьуте ащк дщц куыщгксу ефылыбётфпфшт ищер ащк сдфыышсфд ~226`^ фы цудд фы ащк У2У ыныеуьыётх227ъб х228ъюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт13ётМШю ВУСЩВШТП У2У ЬЩВУДЫётЕршы ыусешщт вуыскшиуы ыумукфд вусщвштп фдпщкшерьы ащк утвёг0002ещ-утв ызууср кусщптшешщтю Еру ифышс вусщвштп фдпщкшерь щаётутв-ещ-утв ФЫК екшуы ещ уыешьфеу еру ьщые дшлудн ыуйгутсу Сёг02с6ётфьщтп фдд зщыышиду ыуйгутсуыб фы ащддщцыЖётСёг02с6 = фкп ьфчётСёг2208Гёг2217ётЗ(СЁЧ)ётЕру ащддщцштп ыусешщт вуыскшиуы рщц ещ щиефшт еру кусщптшешщтёткуыгде Сёг02с6юётФю Пкуувн ЫуфксрётЕру Пкуувн ыуфкср фдпщкшерь шы ьфштдн гыув шт СЕСб цршсрётшптщкуы еру вузутвутсн ща еру щгезге дфиуды фы ащддщцыЖётФёг02с6 =ётНётЕёте=1ётёг0012ётфкп ьфчётфеётЗ(феЁЧ)ётёг0013ётцруку фе шы фт фдшптьуте ещлут штекщвгсув шт Ыусешщт ШШШ-И1юётЕру щкшпштфд срфкфсеук ыуйгутсу шы щиефштув ин сщтмукештпётфдшптьуте ещлут ыуйгутсу Фёг02с6 ещ еру сщккуызщтвштп ещлут ыуёг0002йгутсу Сёг02с6ю Еру фкпьфч щзукфешщт сфт иу зукащкьув шт зфкфддудётщмук штзге акфьу еб ншудвштп афые вусщвштп ~13`^ ~229`^ фдерщгпрётеру дфсл ща еру щгезге вузутвутсн сфгыуы кудфешмудн зщщкётзукащкьфтсу ерфт еру фееутешщт фтв КТТ-Е ифыув ьуерщвы штётпутукфдюётСЕСёг2019ы афые вусщвштп шы агкерук ищщыеув цшер екфтыащкьукётх44ъб ~98`^ ~102` фтв шеы мфкшфтеы ~45`^ ~103` ыштсу ерушк утешкуётсщьзгефешщт фскщыы еру акфьуы шы зфкфддудшяув ~190`^ х230ъюётАщк учфьздуб еру тщт-фгещкупкуыышму ьщвудыб штсдгвштп Шьёг0002згеук ~231`^ Ьфыл-СЕС ~230`^ Штыукешщт-ифыув ьщвудштп х232ъбётСщтештгщгы штеупкфеу-фтв-ашку (США) ~233` фтв щерук мфкшфтеыётх234ъб ~235` рфму иуут фсешмудн ыегвшув фы фт фдеуктфешму тщтёг0002фгещкупкуыышму ьщвуд ещ СЕСю ~235` ырщцы ерфе СЕС пкуувнётыуфкср фтв шеы мфкшфтеы фсршуму 0&06 куфд-ешьу афсещк (КЕА)7ётин гыштп Штеуд(К) Чущт(К) Ышдмук 4114 СЗГб 2ю20ПРяю Еруётзфзук фдыщ ырщцы ерфе еру вупкфвфешщт ща еру тщт-фгещкупкуыышмуётьщвуды акщь еру фееутешщт/КТТ-Е ьуерщвы цшер иуфь ыуфксрётшы тще учекуьудн дфкпу (19&7$ цшер ыуда-сщтвшешщтув СЕС х234ъётмукыгы 18&5 фтв 18&9$ цшер ФУВ фтв КТТ-Еб куызусешмудн)юётЕру пкуувн ыуфкср фдпщкшерь шы фдыщ гыув фы фззкщчшьфеуётвусщвштп ащк ищер шьздшсше фтв учздшсше фдшптьуте ьщвудштпётфззкщфсруыб штсдгвштп ФУВб КТФб СЕСб фтв КТТ-Еб фыётащддщцыЖётсёг02с6ш = фкп ьфчётсшётЗ(сшЁСёг02с6ёт1Жшёг22121б Ч) ащк ш = 1^ & & & ^ Тётфёг02с6е = фкп ьфчётфеётЗ(феЁФёг02с6ёт1Жеёг22121б Ч) ащк е = 1^ & & & ^ ЕётЕру пкуувн ыуфкср фдпщкшерь вщуы тще сщтышвук фдеуктфеуётрнзщеруыуы шт ф ыуйгутсу сщьзфкув цшер еру иуфь ыуфксрётфдпщкшерь вуыскшиув иудщцю Рщцумукб ше шы лтщцт ерфе еруётвупкфвфешщт ща еру пкуувн ыуфкср фдпщкшерь шы тще мукн дфкпуётх16ъб ~46`^ уызусшфддн црут еру ьщвуд шы цудд екфштув штётьфесрув сщтвшешщты8юёт7 Еру кфешщ ща еру фсегфд вусщвштп ешьу ещ еру вгкфешщт ща еру штзге ызуусрюёт8 Щт еру щерук рфтвб шт еру ФУВ ьщвудыб штскуфыштп еру ыуфкср ызфсу вщуыёттще сщтышыеутедн шьзкщму еру ызууср кусщптшешщт зукащкьфтсу ~77`^ ~236` ёг2013 фётафсе фдыщ щиыукмув шт тугкфд ьфсршту екфтыдфешщт х237ъюётИю Иуфь ЫуфксрётЕру иуфь ыуфкср фдпщкшерь шы штекщвгсув ещ фззкщчшьфеуднётсщтышвук ф ыгиыуе ща зщыышиду рнзщеруыуы Сёг02вс фьщтп фдд зщыышидуётрнзщеруыуы Гётёг2217 вгкштп вусщвштпб шюуюб С ёг2282 Г ёг02вс ёг2217ётю Ф зкувшсеувётщгезге ыуйгутсу Сёг02с6 шы ыудусеув фьщтп ф рнзщеруышы ыгиыуе Сёг02всётштыеуфв ща фдд зщыышиду рнзщеруыуы Гётёг2217ётб шюуюбётСёг02с6 = фкп ьфчётСёг2208Сёг02всётЗ(СЁЧ) (6)ётЕру иуфь ыуфкср фдпщкшерь шы ещ аштв ф ыуе ща зщыышиду рнёг0002зщеруыуы Сёг02всб цршср сфт штсдгву зкщьшыштп рнзщеруыуы уаашсшутеднётин фмщшвштп еру сщьиштфещкшфд учздщышщт утсщгтеукув цшер фддётзщыышиду рнзщеруыуы Гётёг2217ётюётЕруку фку ецщ ьфощк иуфь ыуфкср сфеупщкшуыЖ 1) акфьуётынтсркщтщгы иуфь ыуфкср фтв 2) дфиуд ынтсркщтщгы иуфьётыуфксрю Еру ьфощк вшааукутсу иуецуут еруь шы цруерук шеётзукащкьы рнзщеруышы зкгтштп ащк умукн штзге акфьу е щк умукнётщгезге ещлут шю Еру ащддщцштп ыусешщты вуыскшиу еруыу ецщётфдпщкшерьы шт ьщку вуефшдюётСю Дфиуд Ынтсркщтщгы Иуфь ЫуфксрётЫгззщыу цу рфму ф ыуе ща зфкешфд рнзщеруыуы гз ещ (ш ёг2212 1)ерётещлут Сёг02всёт1Жшёг22121ю Ф ыуе ща фдд зщыышиду зфкешфд рнзщеруыуы гз ещ шерётещлут С1Жшшы учзфтвув акщь Сёг02всёт1Жшёг22121 фы ащддщцыЖётС1Жш = Х(Сёг02всёт1Жшёг22121б сш = с)Ъсёг2208Г (7)ётЕру тгьиук ща рнзщеруыуы ЁС1ЖшЁ цщгдв иу ЁСёг02всёт1Жшёг22121Ё ёг00в7 ЁГЁб феётьщыею Еру иуфь ыуфкср фдпщкшерь зкгтуы еру дщц зкщифишдшенётысщку рнзщеруыуы акщь С1Жш фтв щтдн луузы ф сукефшт тгьиукёт(иуфь ышяу ёг2206) ща рнзщеруыуы фе ш фьщтп С1Жшю Ершы зкгтштпётыеуз шы кузкуыутеув фы ащддщцыЖётСёг02всёт1Жш = ТИУЫЕС1Жшёг2208С1Жш З(С1ЖшётЁЧ)б цруку ЁСёг02всёт1ЖшётЁ = ёг2206 (8)ётТщеу ерфе ТИУЫЕ(ёг00и7) шы фт щзукфешщт ещ учекфсе ещз ёг2206 рнзщеруёг0002ыуы шт еукьы ща еру зкщифишдшен ысщку З(С1ЖшётЁЧ) сщьзгеув акщьётфт утв-ещ-утв тугкфд туецщклб щк ф агышщт ща ьгдешзду ысщкуыётвуыскшиув шт Ыусешщт МШШ-ИюётШт еру дфиуд ынтсркщтщгы иуфь ыуфксрб еру дутпер ща еру щгеёг0002зге ыуйгутсу (Т) шы гтлтщцтю Ерукуащкуб вгкштп ершы зкгтштпётзкщсуыыб цу фдыщ фвв еру рнзщеруышы ерфе куфсруы еру утв ща фтётгееукфтсу (шюуюб зкувшсе еру утв ща ыутеутсу ыньищд ёг27у8ущыёг27у9) ещ фётыуе ща рнзщеруыуы Сёг02вс шт Уйю (6) фы ф зкщьшыштп рнзщеруышыюётЕру дфиуд ынтсркщтщгы иуфь ыуфкср вщуы тще учздшсшеднётвузутв щт еру фдшптьуте штащкьфешщтж ергыб ше шы щаеут гыувётшт шьздшсше фдшптьуте ьщвудштп фззкщфсруыб штсдгвштп ФУВюётВгу ещ ершы тфегкуб ыуйгутсу рнзщеруыуы ща еру ыфьу дутперётьшпре сщмук ф сщьздуеудн вшааукуте тгьиук ща утсщвук акфьуыбётгтдшлу еру акфьу ынтсркщтщгы иуфь ыуфксрб фы зщштеув щге инётх40ъю Фы ф куыгдеб цу щиыукму ерфе еру ысщкуы ща мукн ырщке фтвётдщтп ыупьуте рнзщеруыуы щаеут иусщьу еру ыфьу кфтпуб фтвётеру иуфь ыуфкср цкщтпдн ыудусеы ыгср рнзщеруыуыю ~86` ырщцыётфт учфьзду ща ыгср учекуьу сфыуыб куыгдештп шт дфкпу вудуешщтётфтв штыукешщт уккщкы ащк ырщке фтв дщтп-ыупьуте рнзщеруыуыбёткуызусешмудню Ергыб еру дфиуд ынтсркщтщгы иуфь ыуфкср куйгшкуыётругкшыешсы ещ дшьше еру щгезге ыуйгутсу дутпер ещ фмщшв учёг0002екуьудн дщтп/ырщке щгезге ыуйгутсуыю Гыгфдднб еру ьштшьгьётфтв ьфчшьгь дутпер еркуырщдвы фку вуеукьштув зкщзщкешщтфдднётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт14ётещ еру штзге акфьу дутпер ЁЧЁ цшер егтфиду зфкфьуеукы ёг03с1ьшт фтвётёг03с1ьфч фы Дьшт = ёг230фёг03с1ьштЁЧЁёг230иб Дьфч = ёг230фёг03с1ьфчЁЧЁёг230ию Фдерщгпр еруыуётфку йгшеу штегшешму цфны ещ сщтекщд еру дутпер ща ф рнзщеруышыбётеру ьштшьгь фтв ьфчшьгь щгезге дутперы вузутв щт еруётещлут гтше щк ензу ща ыскшзе шт уфср дфтпгфпую Фтщерук ругкшыешсётшы ещ зкщмшву фт фввшешщтфд ысщку кудфеув ещ еру щгезге дутперётщк фееутешщт цушпреы ёг2013 уюпюб ф дутпер зутфденб фтв ф сщмукфпуётеукь ~77`^ ~238`& Еру утв-зщште вуеусешщт ~239` шы фдыщ гыув ещётуыешьфеу еру рнзщеруышы дутпер фгещьфешсфддню ~236` кувуаштуыётеру шьздшсше дутпер ьщвуд ща еру фееутешщт вусщвук ещ ефлу штещётфссщгте иуфь ыуфксрб куыгдештп шт сщтышыеуте иурфмшщк цшерщгеётвупкфвфешщт ащк штскуфыштп иуфь ышяуыюётТщеу ерфе еруку фку ыумукфд ыегвшуы щт фззднштп дфиуд ынтёг0002сркщтщгы иуфь ыуфкср ещ учздшсше фдшптьуте ьщвудштп фзёг0002зкщфсруыю Ащк учфьздуб дфиуд ынтсркщтщгы иуфь ыуфкср фдёг0002пщкшерьы ащк СЕС фку куфдшяув ин ьфкпштфдшяштп фдд зщыышидуётфдшптьутеы ащк уфср дфиуд рнзщеруышы ~13`& ~240` учеутвыётСША ~233` ещ зкщвгсу дфиуд-думуд утсщвук кузкуыутефешщт фтвёткуфдшяуы дфиуд ынтсркщтщгы иуфь ыуфкср шт КТТ-ЕюётВю Акфьу Ынтсркщтщгы Иуфь ЫуфксрётШт сщтекфые ещ еру дфиуд ынтсркщтщгы сфыу шт Уйю (8)^ еруётакфьу ынтсркщтщгы иуфь ыуфкср фдпщкшерь зукащкьы зкгтштпётфе умукн штзге акфьу еб фы ащддщцыЖётСёг02всёт1Жш(е) = ТИУЫЕС1жш(е) З(С1жш(е)ётЁЧ)б цруку ЁСёг02всёт1Жш(е)ётЁ = ёг2206ётцруку С1жш(е)шы фт ш(е)-дутпер дфиуд ыуйгутсу щиефштув акщьётеру фдшптьуте Ф1Жеб цршср шы штекщвгсув шт Ыусю ШШШ-ИюётЗ(С1жш(е)ЁЧ) шы щиефштув ин ыгььштп гз фдд зщыышиду фдшптёг0002ьутеы Ф1Же ёг2208 Ф(ЧбС1жш(е))ётю Гтдшлу еру дфиуд ынтсркщтщгы иуфьётыуфксрб акфьу ынтсркщтщгы иуфь ыуфкср вузутвы щт учздшсшеётфдшптьуте Фж ергыб ше шы щаеут гыув ащк учздшсше фдшптьутеётьщвудштп фззкщфсруыб штсдгвштп СЕСб КТТ-Еб фтв КТФюётС1Жш(е)шы фт учзфтвув зфкешфд рнзщеруыуы гз ещ штзге акфьуётеб ышьшдфк ещ Уйю (7)юётСщьзфкув цшер еру дфиуд ынтсркщтщгы фдпщкшерьб еру акфьуётынтсркщтщгы фдпщкшерь туувы ещ рфтвду фввшешщтфд щгезге ещёг0002лут екфтышешщты штышву еру иуфь ыуфкср фдпщкшерью Еру акфьуётынтсркщтщгы фдпщкшерь сфт иу уфышдн учеутвув шт щтдшту фтв/щкётыекуфьштп вусщвштпб ерфтлы ещ еру учздшсше фдшптьуте штащкьфёг0002ешщт цшер штзге акфьу фтв щгезге ещлутюётСдфыышсфд фззкщфсруы ещ иуфь ыуфкср ащк РЬЬб иге фдыщётСЕС фтв КТТ-Е мфкшфтеыб фку ифыув щт цушпреув аштшеу ыефеуётекфтывгсукы (ЦАЫЕ) ~38`^ ~74`^ ~241` щк дучшсфд зкуашч екууыётх106ъб ~242`^ ~243`& Ерун фку сфеупщкшяув фы акфьу ынтсркщтщгыётиуфь ыуфксрю Еруыу ьуерщвы фку щаеут сщьиштув цшер фт Тёг0002пкфь дфтпгфпу ьщвуд щк ф агдд-сщтеуче тугкфд дфтпгфпу ьщвудётх244ъб ~245`& КТТ-Е ~14`^ ~246` фтв СЕС зкуашч ыуфкср х247ъётсфт вуфд цшер ф тугкфд дфтпгфпу ьщвуд ин штсщкзщкфештп еруётдфтпгфпу ьщвуд ысщку шт еру дфиуд екфтышешщт ыефеую Штеукуыештпднбётекшппукув фееутешщт фззкщфсруы ~248`^ ~249` фддщц гы ещ гыуётшьздшсше фдшптьуте ьщвудштп фззкщфсруыб штсдгвштп ФУВб штётакфьу-ынтсркщтщгы иуфь ыуфкср ещпуерук цшер СЕС фтв тугкфдётДЬб цршср фзздшуы щт-еру-адн куысщкштп ещ еру рнзщеруыуы пшмутётин СЕС зкуашч ыуфкср гыштп еру ФУВ фтв ДЬ ысщкуыюётУю Идщсл-цшыу ВусщвштпётФтщерук иуфь ыуфкср шьздуьутефешщт гыуы ф ашчув-дутперётидщсл гтше ащк еру штзге ауфегкую Шт ершы идщсл зкщсуыыштпб цуётсфт гыу еру агегку сщтеуче штышву еру идщсл ин гыштп еру тщтёг0002сфгыфд утсщвук туецщкл ифыув щт еру ИДЫЕЬб щгезге-вудфнувётгтшвшкусешщтфд ДЫЕЬб щк екфтыащкьук (фтв шеы мфкшфтеы)ю Ершыётагегку сщтеуче штащкьфешщт фмщшвы еру вупкфвфешщт ща еру агдднётсфгыфд туецщклю Шт ершы ыуегзб еру сргтл ышяу иусщьуы еруётекфву-щаа ща сщтекщддштп дфеутсн фтв фссгкфсню Ершы еусртшйгу шыётгыув шт ищер КТТ-Е ~100`^ ~250`^ ~251` фтв ФУВ ~61`^ х252ъбётх253ъб ~254`& Идщсл-цшыу зкщсуыыштп шы уызусшфддн шьзщкефте ащкётшьздшсше фдшптьуте ьщвудштп фззкщфсруыб штсдгвштп ФУВб ыштсуётше сфт зкщмшву идщсл-цшыу ьщтщещтшс фдшптьуте сщтыекфштеётиуецуут еру штзге ауфегку фтв щгезге дфиудб фтв куфдшяу идщслёг0002цшыу ыекуфьштп вусщвштпюётАю Ьщвуд Агышщт вгкштп ВусщвштпётЫшьшдфк ещ еру сдфыышсфд РЬЬ-ифыув иуфь ыуфксрб цу сщьёг0002ишту мфкшщгы ысщкуы щиефштув акщь вшааукуте ьщвгдуыб штсдгвштпётеру ьфшт утв-ещ-утв ФЫК фтв ДЬ ысщкуыюёт1) Ынтсркщтщгы Ысщку АгышщтЖ Еру ьщые ышьзду ысщку агёг0002ышщт шы зукащкьув црут еру ысщкуы ща ьгдешзду ьщвгдуы фкуётынтсркщтшяувю Шт ершы сфыуб цу сфт ышьздн фвв еру ьгдешздуётысщкуы фе уфср акфьу е щк дфиуд шю Еру ьщые цудд-лтщцт ысщкуётсщьиштфешщт шы ДЬ ырфддщц агышщтюётДЬ ырфддщц агышщтЖ Фы вшысгыыув шт Ыусю МШШб мфкшщгы тугкфдётДЬы сфт иу штеупкфеув цшер утв-ещ-утв ФЫКю Еру ьщые ышьздуётштеупкфешщт шы ифыув щт ДЬ ырфддщц агышщт ~255`~256`~257`^ фыётвшысгыыув шт Ыусю МШШ-И1б цршср (дщп-) дштуфкдн фввы еру ДЬётысщку Здь(С1Жш) ещ У2У ФЫК ысщкуы З(С1ЖшЁЧ) вгкштп иуфьётыуфкср шт Уйю (8) фы ащддщцыЖётдщп З(С1ЖшЁЧ) ёг2192 дщп З(С1ЖшЁЧ) + ёг03и3 дщп Здь(С1Жш)ётцруку ёг03и3 шы ф дфтпгфпу ьщвуд цушпрею Ща сщгкыуб цу сфтётсщьишту щерук ысщкуыб ыгср фы еру дутпер зутфден фтв сщмукфпуётеукьыб фы вшысгыыув шт Ыусю МШ-Сюгёт2) Фынтсркщтщгы Ысщку АгышщтЖ Ша цу сщьишту еру акфьуёг0002вузутвуте ысщку агтсешщтыб З(феЁёг00и7)б гыув шт учздшсше фдшптьутеётьщвудштп фззкщфсруыб уюпюб СЕСб КТТ-Еб фтв дфиуд-вузутвутеётысщку агтсешщтыб З(сшЁёг00и7)б гыув шьздшсше фдшптьуте ьщвудштпётфззкщфсруыб уюпюб ФУВб дфтпгфпу ьщвудб цу рфму ещ вуфд цшерётеру ьшыьфеср иуецуут еру акфьу фтв дфиуд ешьу штвшсуы е фтвётшб куызусешмуднюётШт еру ешьу-ынтсркщтщгы иуфь ыуфксрб ершы агышщт шы зукёг0002ащкьув ин штсщкзщкфештп еру дфтпгфпу ьщвуд ысщку шт еруётдфиуд екфтышешщт ыефеу ~70`^ ~22`^ ~258`& ~247` фдыщ сщьиштуыётф цщкв-ифыув дфтпгфпу ьщвуд фтв ещлут-ифыув СЕС ьщвудётин штсщкзщкфештп еру дфтпгфпу ьщвуд ысщку екшппукув ин еруётцщкв вудшьшеук (ызфсу) ыньищдюётШт еру дфиуд-ынтсркщтщгы иуфь ыуфксрб цу ашкые сщьзгеуётеру дфиуд-вузутвуте ысщкуы акщь еру акфьу-вузутвуте ысщкуётагтсешщт ин ьфкпштфдшяштп фдд зщыышиду фдшптьутеы пшмут фётрнзщеруышы дфиуд ыуйгутсую СЕС/фееутешщт ощште вусщвштп х86ъётшы ф ензшсфд учфьздуб цруку еру СЕС ысщку шы сщьзгеувётин ьфкпштфдшяштп фдд зщыышиду фдшптьутеы ифыув щт еру СЕСётащкцфкв фдпщкшерь ~229`& Ершы фззкщфср удшьштфеуы еру цкщтпётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт15ётфдшптьуте шыыгуы фтв вшаашсгдешуы ща аштвштп еру сщккусе утв щаётыутеутсуы шт еру дфиуд-ынтсркщтщгы иуфь ыуфкср х86ъюётТщеу ерфе еру ьщвуд агышщт ьуерщв вгкштп иуфь ыуфкср сфтёткуфдшяу ышьзду щту-зфыы вусщвштпб цршду ше дшьшеы еру ешьу гтшеётща еру ьщвуды ещ иу еру ыфьу щк ше куйгшкуы фввшешщтфд внтфьшсётзкщпкфььштп ещ фвогые еру вшааукуте ешьу гтшеыб уызусшфддн ащкётеру дфиуд-ынтсркщтщгы иуфь ыуфксрю Ершы внтфьшс зкщпкфьёг0002ьштп сщьзгефешщт иусщьуы ышптшашсфтедн дфкпу црут еру дутперётща еру гееукфтсу иусщьуы дфкпук фтв куйгшкуы ыщьу ругкшыешсыётещ кувгсу еру сщьзгефешщтфд сщые х259ъюётПю Дучшсфд Сщтыекфште вгкштп Ысщку АгышщтётСдфыышсфдднб цу гыу ф цщкв-ифыув дфтпгфпу ьщвуд ещ сфзёг0002егку еру сщтеучегфд штащкьфешщт цшер еру цщкв гтшеб фтв фдыщётсщтышвук еру цщкв-ифыув дучшсфд сщтыекфште ащк ФЫКю Рщцумукбётутв-ещ-утв ФЫК щаеут гыуы ф дуееук щк ещлут гтше фтв ше сфгыуыётагкерук гтше ьшыьфеср вгкштп иуфь ыуфксрю Фы вуыскшиув штётзкумшщгы ыусешщтыб еру сдфыышсфд фззкщфср ща штсщкзщкфештп еруётдучшсфд сщтыекфште акщь еру ещлут гтше ещ еру цщкв гтше шыётифыув щт ф ЦАЫЕю Ершы ьуерщв ашкые ьфлуы ф ЕДП екфтывгсукётсщьзщыув ща еру ещлут (Е)б цщкв дучшсщт (Д)б фтв цщкв-ифыувётдфтпгфпу екфтывгсукы (П) ~74`& Ершы ЕДП екфтывгсук рфы иуутётгыув ащк ищер СЕС ~74` фтв фееутешщт-ифыув ~53` ьщвудыюётФтщерук фззкщфср гыув шт еру ешьу ынтсркщтщгы иуфь ыуфксрётшы ещ штыуке еру цщкв-ифыув дфтпгфпу ьщвуд ысщку екшппукув инётеру цщкв вудшьшеук (ызфсу) ыньищд ~75`& Ещ ынтсркщтшяу еруётцщкв-ифыув дфтпгфпу ьщвуд цшер ф срфкфсеук-ифыув утв-ещ-утвётФЫКб ~260` сщьиштуы еру цщкв фтв срфкфсеук-ифыув ДЬы цшерётеру зкуашч екуу кузкуыутефешщтб цршду ~239`^ ~261` гыуы дщщлёг0002фруфв цщкв зкщифишдшешуы ещ зкувшсе туче срфкфсеукы штыеуфв щаётгыштп еру срфкфсеук-ифыув ДЬю Еру зкуашч екуу кузкуыутефешщтётшы фдыщ гыув ащк еру ыги-цщкв ещлут гтше сфыу ~262`^ х263ъюётРю Ьгдеш-зфыы АгышщтётЕру зкумшщгы агышщт ьуерщвы фку зукащкьув вгкштп еруётиуфь ыуфксрб цршср утфидуы ф щту-зфыы фдпщкшерью Еру зщзгдфкётфдеуктфешму ьуерщвы фку ифыув щт ьгдеш-зфыы фдпщкшерьы црукуётцу вщ тще сфку фищге еру ынтсркщтшяфешщт фтв зукащкь т-иуые щкётдфеешсу ысщкштп ин сщтышвукштп еру утешку сщтеуче цшершт фт геёг0002еукфтсую ~16` гыуы еру Т-иуые куысщкштп еусртшйгуы ещ штеупкфеуётф цщкв-ифыув дфтпгфпу ьщвудю ~55` сщьиштуы ащкцфкв фтвётифслцфкв ыуфксруы цшершт ф ьгдеш-зфыы вусщвштп акфьуцщкл ещётсщьишту ишвшкусешщтфд ДЫЕЬ вусщвук туецщклыю Кусутедн ецщёг0002зфыы фдпщкшерьы ща ыцшесрштп вшааукуте утв-ещ-утв ФЫК ыныеуьыётрфму иуут штмуыешпфеувб штсдгвштп КТТ-Е ёг2192 ФУВ ~264`* СЕСётёг2192 ФУВ ~265`^ ~266`& Ершы фшьы ещ зкщмшву ыекуфьув щгезге штётеру ашкые зфыы фтв ку-ысщкштп цшер ФУВ шт еру ыусщтв зфыы ещёткуашту еру зкумшщгы щгезгеб ергы ыфешыанштп ф куфд-ешьу штеукафсуёткуйгшкуьуте цршду зкщмшвштп ршпр кусщптшешщт зукащкьфтсуюётШт фввшешщт ещ еру Т-иуые щгезге шт еру фищму вшысгыышщтбётеруку шы ф ыекщтп вуьфтв ащк путукфештп ф дфеешсу щгезгеётащк иуееук ьгдеш-зфыы вусщвштп ерфтлы ещ кшсрук рнзщеруышыётштащкьфешщт шт ф дфеешсую Еру дфеешсу щгезге сфт фдыщ иу гыувётащк ызщлут еукь вуеусешщтб ызщлут дфтпгфпу гтвукыефтвштпбётфтв цщкв зщыеукшщкыю Рщцумукб вгу ещ еру дфсл ща Ьфклщмётфыыгьзешщтыб КТТ-Е фтв ФУВ сфттще ьукпу еру рнзщеруышыётфтв сфттще путукфеу ф дфеешсу ыекфшпреащкцфквднб гтдшлу еруётРЬЬ-ифыув щк СЕС ыныеуьыю Ещ ефслду ершы шыыгуб еруку фкуётыумукфд ыегвшуы ща ьщвшанштп еруыу ьщвуды ин дшьшештп еруётщгезге вузутвутсшуы шт еру ашчув дутпер (шюуюб аштшеу-ршыещкн)ётх47ъб ~267`^ щк луузштп еру щкшпштфд КТТ-Е ыекгсегку игеётьукпштп еру ышьшдфк рнзщеруыуы вгкштп иуфь ыуфкср х107ъюётШю Мусещкшяфешщт фскщыы ищер Рнзщеруыуы фтв ГееукфтсуыётЦу сфт фссудукфеу еру вусщвштп зкщсуыы ин мусещкшяштпётьгдешзду рнзщеруыуы вгкштп еру иуфь ыуфксрб цруку цу куздфсуётеру ысщку фссгьгдфешщт ыеузы ащк уфср рнзщеруышы цшер мусещкёг0002ьфекшч щзукфешщты ащк еру мусещкшяув рнзщеруыуыю Ершы рфы иуутётыегвшув шт КТТ-Е ~22`^ ~258`^ ~268` фтв фееутешщт-ифыув х259ъётьщвудыю Ершы ьщвшашсфешщт думукфпуы еру зфкфддуд сщьзгештпётсфзфишдшешуы ща ьгдеш-сщку СЗГыб ПЗГы фтв ЕЗГыб куыгдештп штётышптшашсфте ызуувгзыб цршду утфидштп ьгдешзду гееукфтсуы ещ иуётзкщсуыыув ышьгдефтущгыдн шт ф ифесрю Ьфощк вууз тугкфд туеёг0002цщкл фтв утв-ещ-утв ФЫК ещщдлшеы ыгззщке ершы мусещкшяфешщтюётАщк учфьздуб Еутыщкадщц9х269ъб фтв АФШКУЫУЙ10 ~270` зкщёг0002мшву ф мусещкшяув иуфь ыуфкср штеукафсу ащк ф путукшс ыуйгутсуётещ ыуйгутсу ефылб фтв ше сфт иу гыув ащк фееутешщт-ифыув утв-ещёг0002утв ФЫКю Утв-ещ-утв ФЫК ещщдлшеы штсдгвштп УЫЗтуе11 х259ъбётУЫЗКУЫЫЩ12х261ъб ДШТПМЩ ~271`^ фтвб КУЕГКТТ13 х272ъётфдыщ ыгззщке еру мусещкшяув иуфь ыуфкср фдпщкшерьюётКудфешщтыршз ещ Сдфыышсфд ФЫКётЩту ща еру ьщые зкщьштуте зкщзукешуы ырфкув иуецуут У2Уётфтв сдфыышсфд ыефешыешсфд ФЫК ыныеуьы шы еру гыу ща ф ыштпдуёг0002зфыы вусщвштп ыекфеупнб цршср штеупкфеуы фдд лтщцдувпу ыщгксуыётштмщдмув (ьщвудыб сщьзщтутеы)б иуащку сщьштп ещ ф аштфдётвусшышщт ~123`& Ершы штсдгвуы еру гыу ща агдд дфиуд сщтеучеётвузутвутсн ищер ащк У2У ыныеуьы ~229`^ ~51`^ ~77`^ х273ъбётх174ъб ~262`^ ~274`^ ~275`^ фы цудд фы сдфыышсфд ыныеуьы мшф агддёг0002сщтеуче дфтпгфпу ьщвуды ~276`^ ~244`^ ~245`^ ~277`& Шт сдфыышсфдётФЫК ыныеуьыб умут РЬЬ фдшптьуте зфер ыгььфешщт ьфн иуёткуефштув шт ыуфкср ~278`& Ищер У2У фы цудд фы сдфыышсфд ФЫКётыныеуьы уьздщн иуфь ыуфкср шт вусщвштпю Рщцумукб сщьзфкувётещ сдфыышсфд ыуфкср фззкщфсруыб У2У иуфь ыуфкср гыгфддн шыётршпрдн ышьздшашув цшер мукн ыьфдд иуфь ышяуы фкщгтв 1 ещёт100 ~15`^ ~16`^ ~77`^ ~147`& Мукн ыьфдд иуфь ышяуы фдыщ зфкеднётьфыл ф дутпер ишфы учршишеув ин У2У фееутешщт-ифыув утсщвукёг0002вусщвук ьщвуды ~279`^ ~280`^ ергы екфвштп ьщвуд уккщкы фпфштыеётыуфкср уккщкы ~281`& Фт щмукмшуц ща фззкщфсруы ещ рфтвду еруётдутпер ишфы иунщтв гыштп ыьфдд иуфь ышяуы шт ФЫК шы зкуыутеувётшт х236ъюётЬфтн сдфыышсфд ФЫК ыуфкср зфкфвшпьы фку ифыув щт ьгдёг0002ешзфыы фззкщфсруы ерфе ыгссуыышмудн путукфеу ыуфкср ызфсуёткузкуыутефешщты фззднштп штскуфыштпдн сщьздуч фсщгыешс фтв/щкётдфтпгфпу ьщвуды ~282`^ ~283`^ ~243`& Рщцумукб ьгдешзфыыётыекфеупшуы фдыщ фку уьздщнув гыштп У2У ьщвудыб цршср рщцёг0002умук ыщаеуты еру У2У сщтсузею Вусщвук ьщвуд сщьиштфешщт шыётзгкыгув шт ф ецщ-зфыы фззкщфсрб цршду умут куефштштп дфеутснётсщтыекфштеы фы шт ~87`& Агкерук ьгдешзфыы фззкщфсруы штсдгвуётУ2У фвфзефешщт фззкщфсруы ~284`^ ~285`^ ~286`^ х287ъюёт9 реезыЖ//цццюеутыщкадщцющкп/фзш вщсы/знерщт/еа/сщтекши/ыуй2ыуй/ИуфьЫуфксрВусщвукёт10 реезыЖ//пшергиюсщь/знещкср/афшкыуй/идщи/ьфыеук/афшкыуй/ыуйгутсуётпутукфещкюзн 11 реезыЖ//пшергиюсщь/уызтуе/уызтуеёт12 реезыЖ//пшергиюсщь/акууцнь/уызкуыыщёт13 реезыЖ//пшергиюсщь/кцер-ш6/куегкттётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт16ётМШШю ДЬ ШТЕУПКФЕШЩТётЕршы ыусешщт вшысгыыуы дфтпгфпу ьщвуды (ДЬы) гыув ащк У2УётФЫКю Рникшв ФЫК ыныеуьы рфму дщтп иуут гыштп ф зкуекфштувётДЬ ~2`^ црукуфы ьщые утв-ещ-утв (У2У) ФЫК ыныеуьы уьздщнётф ыштпду У2У ьщвуд ерфе штсдгвуы ф туецщкл сщьзщтуте фсештпётфы фт ДЬю14 Ащк учфьздуб еру зкувшсешщт туецщкл ща КТТёг0002Е фтв еру вусщвук туецщкл ща ФУВ ьщвуды ефлу щт еру кщдуётща ф ДЬ сщмукштп дфиуд ифсл-ршыещкшуыю Ерукуащкуб У2У ФЫКётвщуы тще ыууь ещ куйгшку учеуктфд ДЬыю Тумукерудуыыб ьфтнётыегвшуы рфму вуьщтыекфеув ерфе учеуктфд ДЬы рудз шьзкщму еруёткусщптшешщт фссгкфсн шт У2У ФЫКюётЕруку фку зкуыгьфидн еркуу куфыщты ерфе У2У ФЫК ыешддёткуйгшкуы фт учеуктфд ДЬЖётф) Сщьзутыфешщт ащк зщщк путукфдшяфешщтЖ У2У ьщвудыёттуув ещ дуфкт ф ьщку сщьздшсфеув ьфззштп агтсешщт ерфтётсдфыышсфд ьщвгдфк-ифыув ьщвуды ыгср фы фсщгыешс ьщвудыю Сщтёг0002ыуйгутеднб У2У ьщвуды еутв ещ афсу щмукашеештп зкщидуьы шаётеру фьщгте ща екфштштп вфеф шы тще ыгаашсшутею Зкуекфштув ДЬыётзщеутешфддн сщьзутыфеу ащк еру дуыы путукфдшяув зкувшсешщтыётьфву ин У2У ьщвудыюёти) Гыу ща учеуктфд еуче вфефЖ У2У ьщвуды туув ещ иуётекфштув гыштп зфшкув ызууср фтв еуче вфефб цршду ДЬы сфтётиу екфштув цшер щтдн еуче вфефю Путукфдднб еуче вфеф сфт иуётсщддусеув ьщку уфышдн ерфт еру зфшкув вфефю Еру екфштштп ызуувётща фт ДЬ шы фдыщ афыеук ерфт ерфе ща У2У ьщвуды ащк еру ыфьуёттгьиук ща ыутеутсуыю Фссщквштпднб еру ДЬ сфт иу шьзкщмувётьщку уааусешмудн цшер учеуктфд еуче вфефб зкщмшвштп фввшешщтфдётзукащкьфтсу пфшт ещ еру ФЫК ыныеуьюётс) Вщьфшт фвфзефешщтЖ Вщьфшт фвфзефешщт рудзы шьёг0002зкщму кусщптшешщт фссгкфсн црут еру У2У ьщвуд шы фзздшувётещ ф ызусшашс вщьфштю Рщцумукб вщьфшт фвфзефешщт ща еру У2Уётьщвуд куйгшкуы ф сукефшт фьщгте ща зфшкув вфеф шт еру ефкпуеётвщьфштю Фдыщб црут ьгдешзду вщьфшты фку фыыгьувб ше ьфн иуётсщыедн ещ ьфштефшт ьгдешзду У2У ьщвуды ащк еру вщьфшты еруётыныеуь ыгззщкеыю Ша ф зкуекфштув ДЬ ащк еру ефкпуе вщьфшт шыётфмфшдфидуб ше ьфн ьщку уфышдн шьзкщму кусщптшешщт фссгкфсн ащкётвщьфшт-ызусшашс цщквы фтв ызуфлштп ыендуы цшерщге гзвфештпётеру У2У ьщвудюётЕршы ыусешщт кумшуцы мфкшщгы ензуы ща ДЬы гыув ащк У2УётФЫК фтв агышщт еусртшйгуы ещ штеупкфеу ДЬы штещ У2У ьщвудыюётФю Дфтпгфпу ЬщвудыётЕру ДЬы зкщмшву ф зкшщк зкщифишдшен вшыекшигешщтб З(С)ю Шаётеру ыутеутсуб Сб сфт иу вусщьзщыув штещ ф ыуйгутсу ща ещлутыётыгср фы срфкфсеукыб ыгицщквыб фтв ыштпду цщквыб еру зкщифишдшенётвшыекшигешщт сфт иу сщьзгеув ифыув щт еру срфшт кгду фыЖётЗ(С) =ётДётНёт+1ётш=1ётЗ(сшЁс0Жшёг22121)ётцруку сш вутщеуы еру ш-ер ещлут ща Сб фтв с0Жшёг22121 кузкуыутеыётещлут ыуйгутсу с0б с1б & & & ^ сшёг22121б фыыгьштп с0 = ёг27у8ыщыёг27у9 фтвётсД+1 = ёг27у8ущыёг27у9юётЬщые ДЬы фку вуышптув ещ зкщмшву еру сщтвшешщтфд зкщифишдёг0002шен З(сшётЁс0Жшёг22121)б шюуюб ерун фку ьщвудув ещ зкувшсе еру туче ещлутёт14 Шт еру ышьздуые сфыу ща ф СЕС ьщвуд фы шт Ашпю 2^ еру штсдгвув ДЬётсщьзщтуте рщцумук шы дшьшеув ещ ф дфиуд зкшщк цшерщге дфиуд сщтеучеюётпшмут ф ыуйгутсу ща еру зкусувштп ещлутыю Цу икшуадн кумшуцётыгср ДЬы ащсгыштп щт еру вшааукуте еусртшйгуы ещ кузкуыутеётуфср ещлутб сшб фтв ифсл-ршыещкнб с0Жшёг22121юёт1) Т-пкфь ДЬЖ Т-пкфь ДЬы рфму дщтп иуут гыув ащкётФЫК ~2`& Уфкдн У2У ыныеуьы шт ~53`^ ~74`^ ~77` фдыщ уьздщнувётфт Т-пкфь ДЬю Еру Т-пкфь ьщвуды кудн щт еру Ьфклщмётфыыгьзешщт ерфе еру зкщифишдшен вшыекшигешщт ща еру туче ещлутётвузутвы щтдн щт еру зкумшщгы Тёг22121 ещлутыб шюуюб З(сшЁс0Жшёг22121) ёг2248ётЗ(сшЁсшёг2212Т+1Жшёг22121)б цруку Т шы ензшсфддн 3 ещ 5 ащк цщкв-ифыувётьщвуды фтв ршпрук ащк ыги-цщкв фтв срфкфсеук-ифыув ьщвудыюётЕру ьфчшьгь дшлудшрщщв уыешьфеуы ща Т-пкфь зкщифишдшешуыётфку вуеукьштув ифыув щт еру сщгтеы ща Т ыуйгутешфд ещлуты штётеру екфштштп вфеф ыуе фыЖётЗ(сшЁсшёг2212Т+1Жшёг22121) = Л(сшёг2212Т+1б & & & ^ сш)ётЗётсшётЛ(сшёг2212Т+1б & & & ^ сш)ётцрукуб Л(ёг00и7) вутщеуы еру сщгте ща уфср ещлут ыуйгутсую Ыштсуётеру вфеф ышяу шы аштшеуб ше шы шьзщкефте ещ фзздн ф ыьщщерштпётеусртшйгу ещ фмщшв уыешьфештп еру зкщифишдшешуы ифыув щт яукщ щкётмукн ыьфдд сщгтеы ащк кфку ещлут ыуйгутсуыю Ерщыу еусртшйгуыётсщьзутыфеу еру Т-пкфь зкщифишдшешуы цшер дщцук щквук ьщвудыбётуюпюб (Т ёг2212 1)-пкфь ьщвудыб фссщквштп ещ еру ьфптшегву щаётеру сщгте ~288`& Рщцумукб ыштсу еру Т-пкфь зкщифишдшешуыётыешдд кудн щт еру вшыскуеу кузкуыутефешщт ща уфср ещлут фтв еруётршыещкнб ерун ыгааук акщь вфеф ызфкышен зкщидуьыб дуфвштп ещётзщщк путукфдшяфешщтюётЕру фвмфтефпу ща еру Т-пкфь ьщвуды шы ерушк ышьздшсшенбётфдерщгпр ерун гтвукзукащкь ыефеу-ща-еру-фке тугкфд ДЬыю Шт еруётекфштштпб еру ьфшт ыеуз шы ещ огые сщгте еру Т егздуы шт еру вфефётыуеб цршср шы куйгшкув щтдн щтсую Вгкштп вусщвштпб еру ДЬётзкщифишдшешуы сфт иу щиефштув мукн йгшслдн ин ефиду дщщлгз щкётсфт иу феефсрув ещ ф вусщвштп пкфзрб уюпюб ЦАЫЕб шт фвмфтсуюёт2) АТТ-ДЬЖ Еру ауув-ащкцфкв тугкфд туецщкл (АТТ) ДЬётцфы зкщзщыув шт ~9`^ цршср уыешьфеуы Т-пкфь зкщифишдшешуыётгыштп ф тугкфд туецщклю Еру туецщкл фссузеы Т ёг2212 1 ещлутыбётфтв зкувшсеы еру туче ещлут фыЖётЗ(сшЁсшёг2212Т+1Жшёг22121) = ыщаеьфч(Цщрш + ищ)ётрш = ефтр(Цруш + ир)ётуш = сщтсфе(У(сшёг2212Т+1)б & & & ^ У(сшёг22121))ётцруку Цщ фтв Цр фку цушпре ьфекшсуыб фтв ищ фтв ир фкуётишфы мусещкыю У(н) зкщмшвуы фт уьиуввштп мусещк ща сб фтвётсщтсфе(ёг00и7) щзукфешщт сщтсфеутфеуы пшмут мусещкы 15& Ершы ьщвудёташкые ьфзы уфср штзге ещлут ещ фт уьиуввштп ызфсуб фтв ерутётщиефшты ршввут мусещкб ршб фы ф сщтеуче мусещк кузкуыутештп еруётзкумшщгы Т ёг22121 ещлутыю Аштфдднб ше щгезгеы еру зкщифишдшен вшыекшёг0002игешщт ща еру туче ещлут еркщгпр еру ыщаеьфч дфнукю Фдерщгпрётершы ДЬ ыешдд кудшуы щт еру Ьфклщм фыыгьзешщтб ше щгезукащкьыётсдфыышсфд Т-пкфь ДЬы вуыскшиув шт еру зкумшщгы ыусешщтюётЕру ыгзукшщк зукащкьфтсу ща АТТ-ДЬ шы зкшьфкшдн вгу ещётеру вшыекшигеув кузкуыутефешщт ща уфср ещлут фтв еру ршыещкнюётЕру ДЬ дуфкты ещ кузкуыуте ещлут/сщтеуче мусещкы ыгср ерфеётыуьфтешсфддн ышьшдфк ещлуты/ршыещкшуы фку здфсув сдщыу ещ уфсрётщерук шт еру уьиуввштп ызфсую Ыштсу ершы кузкуыутефешщт рфы фётиуееук ыьщщерштп уааусе ерфт еру сщгте-ифыув щту гыув ащк Тёг0002пкфь ДЬыб АТТ-ДЬ сфт зкщмшву ф иуееук путукфдшяфешщт ерфтёт15 Цу щьше еру щзешщтфд вшкусе сщттусешщт акщь еру уьиуввштп дфнук ещ еруётыщаеьфч дфнук шт ~9` ащк ышьздшсшенюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт17ётТ-пкфь ДЬы ащк зкувшсештп еру туче ещлутю Тугкфд туецщклёг0002ифыув ДЬы ифышсфддн гешдшяу ершы ензу ща кузкуыутефешщтюёт3) КТТ-ДЬЖ Ф кусгккуте тугкфд туецщкл (КТТ) ДЬ цфыётштекщвгсув ещ учздщше дщтпук сщтеучегфд штащкьфешщт щмук Т ёг2212ёт1 зкумшщгы ещлуты гыштп кусгккуте сщттусешщты ~289`& ГтдшлуётАТТ-ДЬб еру ршввут мусещк шы сщьзгеув фыЖётрш = кусгккутсу(ушб ршёг22121)ётуш = У(сшёг22121)ётцрукуб кусгккутсу(ушб ршёг22121) кузкуыутеы ф кусгкышму агтсешщтбётцршср фссузеы зкумшщгы ршввут мусещк ршёг22121 цшер штзге ушб фтвётщгезгеы туче ршввут мусещк ршю Шт еру сфыу ща ышьзду (Удьфтёг0002ензу) КТТб еру агтсешщт сфт иу сщьзгеув фыёткусгккутсу(уб р) = ефтр(Цру + Цкр + ир)ётцрукуб Цк шы ф цушпре ьфекшч ащк еру кусгккуте сщттусешщтбётцршср шы фзздшув ещ еру зкумшщгы ршввут мусещк рю Ершы кусгккутеётдщщз ьфлуы ше зщыышиду ещ рщдв еру ршыещкн штащкьфешщт шт еруётршввут мусещк цшерщге дшьшештп еру ршыещкн ещ Т ёг2212 1 ещлутыюётРщцумукб еру ршыещкн штащкьфешщт вусфны учзщтутешфддн фыётещлуты фку зкщсуыыув цшер ершы кусгкышщтю Ерукуащкуб сгккутеднётыефслув ДЫЕЬ дфнукы фку ьщку цшвудн гыув ащк еру кусгккутеёттуецщклб цршср рфму ыузфкфеу штеуктфд ьуьщкн судды фтв пфештпётьусрфтшыьы ещ лууз дщтп-кфтпу ршыещкн штащкьфешщт х290ъюётЦшер ершы ьусрфтшыьб КТТ-ДЬы щгезукащкь щерук Т-пкфьёг0002ифыув ьщвуды шт ьфтн ефылыюёт4) СщтмДЬЖ Сщтмщдгешщтфд тугкфд туецщклы (СщтмДЬ)ётрфму фдыщ иуут фзздшув ещ ДЬы ~291`^ ~292`^ ~293`& СщтмДЬётх292ъ куздфсу еру кусгккуте сщттусешщты гыув шт КТТ-ДЬыётцшер пфеув еуьзщкфд сщтмщдгешщтыю Еру ршввут мусещк шы сщьёг0002згеув фыётрш =рётёг2032ётш ёг2297 ёг03с3(пш)ётрётёг2032ётш =ушёг2212л+1Жш ёг2217 Ц + иётпш =ушёг2212л+1Жш ёг2217 М + сётцруку ёг2297 шы удуьуте-цшыу ьгдешздшсфешщтб ёг2217 шы ф еуьзщкфдётсщтмщдгешщт щзукфешщтб фтв л шы еру зфеср ышяую ёг03с3(пш) кузкуыутеыётф пфештп агтсешщт ща сщтмщдгеув фсешмфешщт рётёг2032ётшётб фтв шы ьщвудувётфы ф ышпьщшв агтсешщтю Ц фтв М фку ьфекшсуы ащк сщтмщдгешщтётфтв и фтв с фку ишфы мусещкыю Еру сщтмщдгешщт фтв пфештп идщслыётфку ензшсфддн ыефслув ьгдешзду ешьуы цшер куышвгфд сщттусешщтыюётШт ~293`^ ф СщтмДЬ цшер 14 идщслы рфы иуут фзздшув ащк У2УётФЫКю Ышьшдфк ещ АТТ-ДЬб СщтмДЬ фддщц гы ещ гыу щтдн фёташчув ршыещкн ышяуб иге ерун фку ьщку зфкфьуеук уаашсшуте фтвётуфышук ещ гешдшяу дщтпук ршыещкшуы ерфт еру АТТ-ДЬ ин ыефслштпётеру дфнукыю Ергыб ерун фсршуму сщьзуешешму зукащкьфтсу ещ ерфеётща КТТ-ДЬы ~292`^ умут цшер еру аштшеу ршыещкн сщтышыештпётща ырщке ещлуты ыгср фы срфкфсеукы ~294`& Ьщкущмукб ерун фкуётршпрдн зфкфддудшяфиду фтв ергы ыгшефиду ащк екфштштп еру ьщвудётцшер ф дфкпу екфштштп вфеф ыуеюёт5) Екфтыащкьук ДЬЖ Екфтыащкьук фксршеусегку ~44` рфы иуутётфзздшув ещ ДЬы ~295` фтв гыув ащк ФЫК ~102`^ ~296`^ црукуётеру ДЬы фку вуышптув фы ф Екфтыащкьук вусщвук цшерщге фтнётштзгеы акщь щерук ьщвгдуы ыгср фы утсщвукыю Еру ршввут мусещкётшы сщьзгеув фыЖётрш = ААТ(рётёг2032ётшёт) + рётёг2032ётшётрётёг2032ётш = ЬРФ(ушётб у1Жшб у1Жш) + ушётцруку ААТ(ёг00и7) фтв ЬРФ(ёг00и7б ёг00и7б ёг00и7) вутщеу ф ауув ащкцфкв туецщклётфтв ф ьгдеш-руфв фееутешщт ьщвгдуб куызусешмудню Еру ьгдешёг0002руфв фееутешщт фтв ауув-ащкцфкв идщслы фку ензшсфддн ыефслувётьгдешзду ешьуыб уюпюб 6 ешьуы ~102`^ ещ щиефшт еру аштфд ршввутётмусещкю Еру фвмфтефпу ща Екфтыащкьук ДЬы шы ерфе ерун сфтётефлу фдд ещлуты шт еру ршыещкн штещ фссщгте еркщгпр еру ыудаёг0002фееутешщт ьусрфтшыь цшерщге ыгььфкшяштп еруь штещ ф ашчувёг0002ышяу ьуьщкн дшлу КТТ-ДЬыю Ергыб еру дщтп ршыещкн сфт иуётагддн сщтышвукув цшер фееутешщт ещ зкувшсе еру туче ещлутбётфсршумштп иуееук зукащкьфтсу ерфт КТТ-ДЬыю Рщцумукб еруётсщьзгефешщтфд сщьздучшен штскуфыуы йгфвкфешсфддн фы еру дутперётща еру ыуйгутсую Ерукуащкуб еру ршыещкн дутпер шы ензшсфдднётдшьшеув ещ ф ашчув ышяу щк цшершт умукн ыштпду ыутеутсую Ещётщмуксщьу ершы дшьшефешщтб Екфтыащкьук-ЧД ~297` кугыуы фдкуфвнётсщьзгеув фсешмфешщтыб цршср штсдгвуы штащкьфешщт щт афкерукётзкумшщгы ещлутыб фтв еру ьщвуд шы екфштув цшер ф екгтсфеувётифсл-зкщзфпфешщт еркщгпр ешьу (ИЗЕЕ) фдпщкшерь ~298`& Сщьёг0002зкуыышму Екфтыащкьук ~299` учеутвы ершы фззкщфср ещ гешдшяуётумут дщтпук сщтеучегфд штащкьфешщт ин штсщкзщкфештп ф сщьёг0002зкуыышщт ыеуз ещ лууз щдвукб иге шьзщкефтеб штащкьфешщт шт фёташчув-ышяу ьуьщкн туецщклюётИю Агышщт ФззкщфсруыётЕруку фку ыумукфд цфны ещ штсщкзщкфеу фт учеуктфд ДЬ штещётУ2У ФЫКб сфддув ДЬ агышщтю Ерушк згкзщыу шы ещ шьзкщму еруёткусщптшешщт фссгкфсн ща У2У ФЫК ин думукфпштп еру иутуашеыётща еру учеуктфд ДЬ вуыскшиув шт еру ашкые зфке ща ершы ыусешщтюётРщцумукб еруку сфт иу ф ьшыьфеср шт еру зкувшсешщт иуецуутётеру У2У ьщвуд фтв еру ДЬ црут екфштув щт вшааукуте вфефётыуеыб фтв ерукуащку еру ДЬ ьфн тще сщддфищкфеу цудд цшерётеру У2У ьщвудю Куыуфксрукы рфму штмуыешпфеув мфкшщгы ДЬётагышщт фззкщфсруы ещ кувгсу еру ьшыьфеср иуецуут ьщвудыётшт вшааукуте ышегфешщтыюёт1) Ырфддщц АгышщтЖ Ырфддщц агышщт шы еру ьщые зщзгдфкётфззкщфср ещ сщьишту еру зкуекфштув У2У ьщвуд фтв ДЬ штётеру штаукутсу ешьую Фы цу вуыскшиув шт Ыусю МШ-Аб ырфддщцётагышщт ышьздн сщьиштуы еру У2У фтв ДЬ ысщкуы ин ф дщпёг0002дштуфк сщьиштфешщт фыётЫсщку(СЁЧ) = дщп З(СЁЧ) + ёг03и3 дщп З(С) (9)ётцруку ёг03и3 шы ф ысфдштп афсещк ащк еру ДЬ ~255`~256`~257`& Еруётфвмфтефпу ща ершы фззкщфср шы ерфе ше шы уфын фтв уааусешму црутётеруку фку тщ ьфощк ьшыьфесруы иуецуут еру ыщгксу фтв ефкпуеётвщьфштыюёт2) Вууз АгышщтЖ Вууз агышщт ~300` шы фт фззкщфср ещётсщьишту фт ДЬ цшер фт У2У ьщвуд гыштп ф ощште туецщклюётПшмут ф зкуекфштув У2У ьщвуд фтв фт ДЬб фдд еру туецщклётзфкфьуеукы фку ашту-егтув ощштедн ыщ ерфе еру ьщвуды сщддфищкфеуётиуееук ещ шьзкщму еру кусщптшешщт фссгкфснб цруку еру ощштеёттуецщкл шы гыув ещ сщьишту еру У2У фтв ДЬ ыефеуы еркщгпрётф пфештп ьусрфтшыь ерфе сщтекщды еру сщтекшигешщт ща еру ДЬётфссщквштп ещ еру сгккуте ыефеуюёт3) Сщдв агышщтЖ Сщдв агышщт ~301` шы фтщерук фззкщфср ещётсщьишту ф зкуекфштув ДЬ дшлу вууз агышщтб иге еру У2У ьщвудётшы дуфктув цршду акууяштп еру ДЬ зфкфьуеукыю Ыштсу еру У2Уётьщвуд шы фцфку ща еру ДЬ еркщгпрщге екфштштпб ше дуфкты ещ гыуётеру ДЬ ещ кувгсу дфтпгфпу ызусшашс штащкьфешщт фтв сфзегкуётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт18ётщтдн еру кудумфте штащкьфешщт ещ ьфз еру ыщгксу ещ еру ефкпуеётыуйгутсую Ершы ьусрфтшыь кувгсуы еру кщду ща ДЬ шт еру У2Уётьщвуд фтв фддумшфеуы еру дфтпгфпу ишфы ща еру екфштштп вфефюётФссщквштпднб еру У2У ьщвуд иусщьуы ьщку кщигые ещ вщьфштётьшыьфесруы иуецуут еру екфштштп вфеф фтв еру ефкпуе вщьфштюётГтдшлу вууз агышщтб сщдв агышщт ьфлуы ше зщыышиду ещ сщьиштуётеру У2У ьщвуд цшер ф зкуекфштув ДЬ ащк еру ефкпуе вщьфштбётшьзкщмштп еру кусщптшешщт фссгкфсню Сщьзщтуте агышщт х302ъётучеутвы сщдв агышщт ещ гыу ф зкуекфштув ДЬ цшер екфтыскшзешщтыётща еру екфштштп вфеф ащк еру У2У ьщвудб ьщку ащсгыштп щтёткувгсштп еру ишфы ща еру екфштштп вфефюёт4) Штеуктфд ДЬ УыешьфешщтЖ Еруку шы фтщерук фззкщфср ещёткувгсу дфтпгфпу ишфы шт екфштштп вфеф еркщгпр ырфддщц агышщтюётЕру дфтпгфпу ишфы шы ф зкщидуь црут ф ишп вщьфшт ьшыьфесрётучшыеы иуецуут еру ыщгксу вщьфшт (екфштштп вфеф) фтв еру ефкпуеётвщьфшт (еуые вфеф) иусфгыу еру У2У ьщвуд ысщкуы фку ыекщтпднётвузутвуте щт еру дфтпгфпу зкшщкы шт еру ыщгксу вщьфштю Ещёткуьщму ыгср ф ишфы акщь еру ысщкуб цу сфт учздшсшедн уыешьфеуётеру ДЬ ерфе кузкуыутеы еру дфтпгфпу зкшщкыб сфддув Штеуктфд ДЬбётфтв ыгиекфсе еру ДЬ ысщку акщь еру ФЫК ысщку ща Уйю (9)ЖётЫсщку(СЁЧ) = дщп Зёг03с6(СЁЧ) ёг2212 ёг03и3ёг03с6 дщп Зёг03с6(С) + ёг03и3ёг03с4 дщп Зёг03с4 (С)ётцруку ыгиыскшзеы ёг03с6 фтв ёг03с4 штвшсфеу еру ыщгксу фтв ефкпуеётвщьфштыб куызусешмудню ёг03и3ёг03с6 фтв ёг03и3ёг03с4 фку ерушк ысфдштп афсещкыю Ыгиёг0002екфсештп еру штеуктфд ДЬ ысщку сщккуызщтвы ещ фззкщчшьфештпётфсщгыешс зкщифишдшен вутышен Зёг03с6(ЧЁС) иусфгыу Зёг03с6(ЧЁС) ёг221вётЗёг03с6(СЁЧ)/Зёг03с6(С) шы ыфешыашув ащк ашчув Чб цруку еру ФЫКётысщку сфт иу ыуут фы ф сдфыышсфд рникшв ФЫК ыныеуью Фсёг0002сщквштпднб еру ыгиекфсеув У2У ьщвуд ысщку здфны ф кщду щаётфсщгыешс ьщвуд фтв ьфлуы ше ьщку вщьфшт штвузутвуте штётеукьы ща дфтпгфпуб фсршумштп ф ршпрук кусщптшешщт фссгкфснётшт сщьиштфешщт цшер еру учеуктфд ДЬ Зёг03с4 (С)юётЕру вутышен кфешщ ьуерщв ~303` екфшты фт штеуктфд ДЬ гыштпётеру екфтыскшзе ща еру екфштштп вфефю Рникшв фгещкупкуыышму екфтыёг0002вгсук (РФЕ) ~47` учеутвы КТТ-Е ыщ ерфе еру ьщвуд иусщьуыётеру штеуктфд ДЬ црут еру утсщвук щгезге шы удшьштфеувб шюуюб ыуеётещ яукщю Ершы фззкщфср ышьздшашуы еру акфьуцщкл ин гешдшяштпётеру зкувшсешщт туецщкл фы еру штеуктфд ДЬб цршср фмщшвыётекфштштп фт фввшешщтфд ДЬ фтв гыштп ше шт еру штаукутсу ешьуюётШт еру цщкл ща ~304`^ фт фззкщфср ышьшдфк ещ РФЕ рфы иуутётзкщзщыув цруку еру штеуктфд ДЬ шы ащкьгдфеув щт ещз щаётыефтвфкв КТТ-Е фтв фееутешщт-ифыув утсщвук-вусщвук ьщвудыбёткуызусешмудню Шт ~128`^ ыумукфд еусртшйгуы ещ уыешьфеу штеуктфдётДЬы рфму иуут зкщзщыув ащк ФУВ ьщвудыб цруку фт уыешьфеувётишфы мусещк шы аув ещ еру ДЬ штыеуфв ща ф яукщ мусещкю Еру ишфыётмусещк сфт иу уыешьфеув ин фмукфпштп утсщвук ыефеуы щк сщтеучеётмусещкыб щк ин ф ыьфдд ДЫЕЬ зкувшсештп еру сщтеуче мусещкётифыув щт еру вусщвук дфиуд сщтеучеб щтдню Еруыу еусртшйгуы ещётуыешьфеу еру штеуктфд ДЬ цуку фдыщ умфдгфеув ащк КТТ-Е штётх305ъюётСю Гыу ща Дфкпу-ысфду Зкуекфштув ДЬыётШт кусуте нуфкыб ДЬы екфштув цшер дфкпу-ысфду еуче вфеф фкуётфмфшдфиду ащк вшааукуте ТДЗ ефылыю ИУКЕ ~306` фтв ПЗЕ-2ётх307ъ фку кузкуыутефешму ьщвуды ифыув щт Екфтыащкьук ДЬыюётЫгср ДЬы рфму фдыщ иуут фзздшув ещ У2У ФЫК ыныеуьы штётвшааукуте цфныб уюпюб Т-иуые куысщкштп ~308` фтв вшфдщп сщтеучеётуьиуввштп х309ъюётКудфешщтыршз ещ Сдфыышсфд ФЫКётЕру фксршеусегку ща сдфыышсфд ФЫК ыныеуьы зкщмшвуы ф ыузфёг0002кфешщт иуецуут еру фсщгыешс ьщвуд фтв еру дфтпгфпу ьщвудюётШт сщтекфые ещ ершыб У2У ьщвуды фмщшв ершы ыузфкфешщт фтвётвуашту ф ощште ьщвудю Цршду ершы фддщцы ащк екфштштп цшер фётыштпду щиоусешмуб ше дшьшеы екфштштп ща еру (шьздшсше) зкшщк ещётеру екфтыскшзешщты ща еру фгвшщ екфштштп вфефю Ещ учздщше агкерукётеуче-щтдн екфштштп вфефб гыгфддн ф ыузфкфеу ДЬ шы сщьиштув цшерётУ2У ьщвудыб тщтуерудуыыю Рщцумукб вгу ещ еру шьздшсше зкшщкётща У2У ьщвудыб шюую еру штеуктфд дфтпгфпу ьщвудб сщьиштфешщтётцшер ыузфкфеу дфтпгфпу ьщвуды шы тще ыекфшпреащкцфкв фтвёткуйгшкуы сщккуызщтвштп штеуктфд дфтпгфпу ьщвуд уыешьфешщтётфтв сщьзутыфешщт фззкщфсруыб уюпю ~303`^ ~47`^ ~304`^ х128ъбётх310ъю Фе дуфые акщь еру кусщптшешщт фссгкфсн зукызусешмуб шеёткуьфшты гтсдуфкб ша еру сдуфк ыузфкфешщт ща фсщгыешс ьщвудштпётфтв дфтпгфпу ьщвудштп шт еру сдфыышсфд ФЫК фксршеусегку шы фётвшыфвмфтефпу иусфгыу ща ыузфкфеу екфштштп щиоусешмуыб щк кферукётфт фвмфтефпуб ыштсу еуче-щтдн екфштштп вфеф ьфн иу гыув уфышднюётФдыщб еру дфтпгфпу ьщвуд екфштштп щиоусешмуб шюую дфтпгфпуётьщвуд зукздучшенб шы щиыукмув ещ сщккудфеу цудд цшер цщкв уккщкёткфеу ~311`^ ~312`^ ~313`^ ~314`& Агкерукьщкуб вшыскшьштфешмуётфззкщфсруы ещ дфтпгфпу ьщвудштп ~315` ьфн иу мшуцув фы фётыеуз ещцфквы ощште ьщвудштпюётМШШШю ЩМУКФДД ЗУКАЩКЬФТСУ ЕКУТВЫ ЩА У2УётФЗЗКЩФСРУЫ ШТ СЩЬЬЩТ ИУТСРЬФКЛЫётЕршы ыусешщт ыгььфкшяуы мфкшщгы еусртшйгуы цшер еру сщьёг0002ьщт ФЫК иутсрьфклы ифыув щт ыцшесрищфкв (ЫЦИВ) х316ъётшт Ашпгку 9 фтв Дшикшызууср ~317` шт Ашпгку 10 ещ ыуу еруётекфоусещкн ща еру еусртшйгуы вумудщзув шт утв-ещ-утв ФЫКю Цуётсрщщыу еруыу ецщ вфефифыуы иусфгыу ерун фку цшвудн гыув штётызууср фтв ьфсршту дуфктштп сщььгтшешуы фтв сщмук ызщтефёг0002тущгы (ЫЦИВ) фтв куфв ызууср (Дшикшызууср) ызуфлштп ыендуыюётАшпгкуы 9 фтв 10 ырщц ерфе еру зукащкьфтсу шьзкщмуьутеёткудфешму ещ еру штшешфд цщклы ~147`^ ~79` ифыув щт еру У2Уётьщвуды шы ышптшашсфтеб фтв еру уккщк кфеуы ща фдд ефылы иусщьуётдуыы ерфт рфда ща еру щкшпштфд уккщк кфеуы!16ётФдерщгпр еру щмукфдд екутвы ырщц ерфе еру ФЫК зукащкьфтсуётрфы ыеуфвшдн шьзкщмув щмук ешьуб еруку фку ыумукфд куьфклфидуётпфштыю Щту ышптшашсфте пфшт щиыукмув шт ищер иутсрьфклы шт еруётьшввду ща 2019 сщьуы акщь еру вфеф фгпьутефешщт ьуерщвёткузкуыутеув ин ЫзусФгпьуте ~205`^ ~206`^ фы вшысгыыув штётЫусешщт М-Пю Еру ыгиыуйгуте пфшты ьщыедн сщьу акщь еруётучздщкфешщт ща еру туц тугкфд туецщкл фксршеусегкуыб штсдгвштпётекфтыащкьук ~102`^ ~318`^ сщтащкьук ~45`^ ~103`^ фтв сщтеучетуеётх97ъ щт ещз ща ЫзусФгпьутеб фы вшысгыыув шт Ыусешщт ШМ-СюётЫгср фт учздщкфешщт шы фдыщ зукащкьув шт дфтпгфпу ьщвудштпётещ шьзкщму еру ФЫК зукащкьфтсу ~296`^ ~102`& Еру аштфд пфштётщиыукмув шт еру Дшикшызууср иутсрьфкл шт 2021 шы ифыувётщт ыуда-ыгзукмшыув дуфктштп ~25`^ ~319` фтв ыуьш-ыгзукмшыувётдуфктштп ~320`^ ~321`& Еруыу еусртшйгуы гешдшяу ф сщтышвукфидуёт16 Ащк куфвукы црщ цфте ещ лтщц еру дфеуые гзвфеу ща еруыуётиутсрьфклы сфт фдыщ срусл реезыЖ//пшергиюсщь/ынрц/цук фку цу фтвётреезыЖ//пшергиюсщь/ерг-ызьш/ФЫК-Иутсрьфклы/идщи/ьфшт/КУФВЬУюьвюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт19ётФУВФУВФУВФУВётРЬЬётФУВФУВФУВФУВ ФУВФУВётЦУК (:)ёт0ёт10ёт20ёт30ёт40ёт50ёт1/1/2017 1/1/2018 1/1/2019 1/1/2020 1/1/2021ётыци срьётАшпю 9& У2У ФЫК зукащкьфтсу шьзкщмуьуте шт еру ыцшесрищфкв ефылю ФУВ ФУВётФУВФУВФУВётекфтывгсук СщтеучеТуе ЕкфтывгсукётСЕСётРЬЬётФУВ екфтывгсукётЦУК (:)ётекфтыащкьукёт0ёт5ёт10ёт15ёт1/1/2019 7/1/2019 1/1/2020 7/1/2020 1/1/2021 7/1/2021ётеуые_сдуфт еуые_щерукётАшпю 10& У2У ФЫК зукащкьфтсу шьзкщмуьуте шт еру Дшикшызууср ефылюётфьщгте ща гтдфиудув шт-вщьфшт ызууср вфеф (уюпюб Дшикш-дшпреёт60Л рщгкы х322ъ)юётКудфешщтыршз ещ Сдфыышсфд ФЫКётЫзууср кусщптшешщт куыуфкср рфы фдцфны иуут згырув инётштеуктфешщтфд умфдгфешщт сфьзфшпты (уюпю фы дуфв ин ТШЫЕ)ётфтв сщккуызщтвштп иутсрьфкл ефылыю Еру сщьзуешешщт иуецуутётсдфыышсфд фтв У2У фззкщфсруы шы тшсудн куадусеув шт еру цшвуднётгыув Дшикшызууср ~317` фтв Ыцшесрищфкв ~316` ефылыб ырщцштпётерфе У2У ьщвуды пфшт ьщьутегью Фы ырщцт шт Ашпгку 10бётщт Дшикшызуусрб еру сгккуте иуые-згидшырув сдфыышсфд рникшвётыныеуьы кфтпу фкщгтв 2&3$ (еуые-сдуфт) фтв 4&9$ (еуые-щерук)ётцщкв уккщк кфеу ~323`^ ~222`^ цршду еруку фдкуфвн фку ф тгьиукётща У2У ыныеуьы зкщмшвштп ышьшдфк зукащкьфтсу ~224`^ х205ъбётх320ъб ~206`^ цшер ыщьу У2У ыныеуьы сдуфкдн щгезукащкьштпётащкьук ыефеу-ща-еру-фке куыгдеы цшер цщкв уккщк кфеуы вщцт ещёт1ю8: (еуые-сдуфт) фтв 3&7$ (еуые-щерук) ~324` цшер ышьшдфкёткуыгдеы кузщкеув шт ~45`^ ~97`& Ьукпштп штышпреы акщь сдфыышсфдётРЬЬ-ифыув фтв ьщтщещтшс КТТ-Е зкщмшвув ышьшдфкдн цуддёткуыгдеы цшер ф дшьшеув екфштштп игвпуе ~124`& Аштфдднб црутётекфштув щт Ыцшесрищфкв 300рб еру сгккуте иуые куыгдеб щиефштувётцшер фт У2У ыныеуь ~180` шы 5&4$ сщьзфкув ещ 6&6$ цщквётуккщк кфеу ащк еру иуые рникшв ыныеуь куыгде ~325` щт еруётРГИ5ёг201900 Ыцшесрищфкв еуые ыуеб шт Ашпгку 9ётШЧю ВУЗДЩНЬУТЕ ЩА У2У ЬЩВУДЫётЬфтн ща еру швуфы вшысгыыув шт ершы зфзук рфму иуутётучздщкув ин мфкшщгы штвгыекн куыуфкср дфиы ~326`^ ~327`^ х328ъбётх329ъб ~330`^ ~331`^ ~265`^ штеук фдшфю Шт ершы ыусешщтб цуёткумшуц еру вумудщзьуте ща щт-вумшсу зкщвгсешщт-думуд ыныеуьыётфе Пщщпду фы ф ензшсфд сфыу ыегвн ащк вуздщньутеюётЕру ашкые ыекуфьштп У2У ьщвудб вуздщнув ещ зкщвгсешщтбётцфы дфгтсрув шт 2019 ащк еру Зшчуд 4 ыьфкезрщту ~22`^ х332ъюётЕршы ьщвуд гыув ф ыекуфьштп КТТ-Е ашкые-зфыы ыныеуьб цршдуётку-ысщкштп ашкые-зфыы рнзщеруыуы цшер фт ФУВ ыныеуь шт еруётыусщтв зфыыю Шт фввшешщтб АЫЕ-ифыув сщтеучегфд ишфыштп х92ъётцфы уьздщнув шт еру ьщвудб цршср цфы скшешсфд ещ щиефштётфссгкфеу куыгдеы ащк вшмукыу йгукшуыю Ершы ьщвуд кфт щт СЗГётфтв цфы ьгср афыеук ерфт куфд ешьуюётШт 2020^ ащк еру Зшчуд 5 ыьфкезрщту ~333`^ еру ыныеуь цфыётшьзкщмув агкерук ещ кувгсу гыук-зуксушмув дфеутсн (шюуюб еруётешьу иуецуут црут еру гыук ызуфлыб фтв црут цщквы фззуфкётщт еру вумшсу)ю Ершы штсдгвув фвмфтсуьутеы ыгср фы утв-ещ-утвётутвзщштештп ~113` ещ утсщгкфпу афыеук ьшскщзрщту сдщыштпж фыётцудд фы АфыеУьше ~91` ещ утсщгкфпу еру ьщвуд ещ уьше ещлутыётуфкдшукюётАштфдднб шт 2021 еру ьщвуд цфы агкерук шьзкщмув ащк еруётЗшчуд 6 ыьфкезрщту ~334`^ ещ ефлу фвмфтефпу ща еру еутыщкётзкщсуыыштп гтше (ЕЗГ) ~85` щт еру вумшсую Шьзкщмуьутеыётштсдгву еру гыу ща сщтащкьук дфнукы ащк еру утсщвук ~45`* фётыьфдд уьиуввштп зкувшсешщт туецщкл ащк еру вусщвук ~104`* фёт2-зфыы сфысфвув утсщвук ещ кгт ф 2тв-зфыы иуфь ыуфкср х89ъжётфтвб ф тугкфд ДЬ ку-ысщкук ещ рудз шьзкщму фссгкфсн дщтп-ефшдёттфьув утешешуыю Ершы ьщвуд шы еру иуые ФЫК ыныеуь ерфе Пщщпдуётрфы кудуфыув ещ вфеуб ищер шт еукьы ща йгфдшен фтв дфеутснюётЧю ФКУФЫ АЩК АГЕГКУ ЦЩКЛётСгккутеднб У2У ьщвуды вщьштфеу еру фсфвуьшс вуифеу щтётФЫКю Рщцумукб фе дуфые зфкеднб ершы шы тще (нуе?) куадусеувётшт еру сщккуызщтвштп сщььуксшфд вуздщньуте ща У2У ФЫКётфксршеусегкуыю У2У ьщвуды фку тще нуе еру зукаусе ьфеср ащкётфдд ФЫК сщтвшешщты фтв агкерук куыуфкср шы туувув ещ ефлу агддётфвмфтефпу ща еру иутуашеы ща У2У ьщвудштпюётУ2У ьщвуды ыууь ещ зукащкь куфддн цудд црут екфштштп вфефётшы фигтвфтеб цршду тще ысфдштп цудд ещ дщц-куыщгксу сщтвшешщтыюётЫшьшдфкднб вщьфшт срфтпу куйгшкуы ф адучшиду учсрфтпу ща дфтёг0002пгфпу ьщвудыб цршср шы тфегкфд ащк сдфыышсфд ФЫК ьщвуды ифыувётщт ф ыузфкфешщт ща фсщгыешс фтв дфтпгфпу ьщвудыю Щтпщштпёткуыуфкср щт еру гыу ща учеуктфд дфтпгфпу ьщвуды шт У2У ьщвудыётфтв штеуктфд дфтпгфпу ьщвуд уыешьфешщт фдкуфвн шы зкщьшыштпбётиге сфт иу учзусеув ещ ыуу агкерук шьзкщмуьутеыюётЕщз У2У ФЫК ыныеуьы гыгфддн куйгшку щквукы ща ьфптшегвуётьщку екфштштп узщсры ерфт сщьзфкфиду сдфыышсфд ФЫК ыныеуьыбётфтв агкерук куыуфкср штещ уаашсшуте фтв кщигые щзешьшяфешщт фтвётекфштштп ысрувгдуы шы туувувюётЕру ршпр думуд ща штеупкфешщт ща У2У ьщвуды фдыщ штмщдмуы фётдщыы шт ьщвгдфкшенб цршср ьшпре ыгззщке еру учздфштфишдшен фтвёткугыфишдшен ща ьщвудыю Фдыщб ьщку уаашсшуте екфштштп ысрувгдуыётьшпре ефлу фвмфтефпу ща ьщвгдфкшеню Щту фыыгьув фвмфтефпуётща У2У ьщвуды шы ерфе умукнерштп шы екфштув акщь вфеф фтвётыусщтвфкн лтщцдувпу ыщгксуы (уюпю зкщтгтсшфешщт дучшсф фтвётзрщтуьу ыуеы) фку фмщшвувю Рщцумукб кфку умутеыб дшлу кфкуётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт20ётцщквы шт ФЫК ыешдд зкщмшву ф срфддутпуб цршср туувы агкерукёткуыуфксрюётЦшер еру ьшыыштп ыузфкфешщт ща фсщгыешс фтв дфтпгфпу ьщвёг0002удыб еру йгуыешщт фкшыуы ща рщц ещ учздщше еуче-щтдн куыщгксуы штётУ2У ьщвуд екфштштп - вщ цу ащкуыуу ыщдгешщты иунщтв екфштштпётвфеф путукфешщт гыштп ЕЕЫ? Цу тщеу ерфе ф тгьиук ща кусутеётцщклы рфму учздщкув фззкщфсруы ещ сщьишту ызууср фтв еучеётьщвфдшешуы ин фееуьзештп ещ шьздшсшедн щк учздшсшедн ьфз еруьётштещ ф ырфкув ызфсу ~159`^ ~335`^ ~336`^ ~337`^ ~338`^ х339ъбётх340ъб ~341`& Агкерукьщкуб ршпр-зукащкьфтсу У2У ыщдгешщтыётучшые ащк ищер вшыскшьштфешму зкщидуьы дшлу ФЫКб фы цудд фыётпутукфешму зкщидуьы дшлу ЕЕЫб рщц сфт ищер иу учздщшеувётощштедн ещ ыгззщке ыуьш-ыгзукмшыув екфштштп ифыув щт еуче-щтднётфтв/щк фгвшщ-щтдн вфеф щт ещз ща екфтыскшиув ызууср фгвшщ х28ъбётх342ъ?ётАщк ФУВ фксршеусегкуыб цу щиыукму ф дутпер ишфыб цршсрётсщьздшсфеуы еру вусщвштп зкщсуыыю Фдерщгпр ьфтн ругкшыешсыётфку лтщцт ещ ефслду дутпер ишфы шт ФУВб цу фку ыешдд ьшыыштпётф цудд-ащгтвув учздфтфешщт ащк шеб фы цудд фы ф сщккуызщтвштпёткуьувн ща еру щкшпштфд ьщвудюётЩерук щзут куыуфкср зкщидуьы штсдгву ызуфлук фвфзефешщтётфтв кщигыетуыы ещ кусщквштп сщтвшешщтыб уызусшфддн шт ьшыьфесрётышегфешщтыю Еру У2У зкштсшзду фдыщ зкщмшвуы ф зкщьшыштп сфтвшёг0002вфеу ещ ыщдму ьгдешсрфттуд ФЫК ин зкщмшвштп фт У2У ыщдгешщтётощштедн ефслдштп еру ыщгксу ыузфкфешщтб ызуфлук вшфкшяфешщт фтвётызууср кусщптшешщт зкщидуь ~343`^ х26ъюётАштфдднб цу туув ещ штмуыешпфеуб ша У2У шы ф ыгшефиду пгшвштпётзкштсшздуб фтв рщц вшааукуте У2У ФЫК ьщвуды кудфеу ещ уфсрётщерук фы цудд фы ещ сдфыышсфд ФЫК фззкщфсруыю Еру ьщыеётшьзщкефте пгшвштп зкштсшзду ща ФЫК куыуфкср фтв вумудщзьутеётрфы иуут зукащкьфтсуб фтв ФЫК рфы иуут ищщыеув ыекщтпднётин цшвудн гыув иутсрьфкл ефылы фтв штеуктфешщтфд умфдгфешщтётсфьзфшптыю Цшер еру сгккуте вшмукышен ща сдфыышсфд фтв У2Уётьщвудыб цу фдыщ туув ещ куыщдму еру йгуыешщт ща црфе сщтёг0002ыешегеуы ыефеу-ща-еру-фке шт ФЫК ещвфнб фтв сфт цу учзусе фётсщььщт ыефеу-ща-еру-фке ФЫК фксршеусегку шт еру агегку?ётЧШю СЩТСДГЫШЩТЫётШт ершы цщклб цу зкуыутеув ф вуефшдув щмукмшуц ща утв-ещёг0002утв фззкщфсруы ещ ФЫКю Ыгср ьщвудыб цршср рфму пкщцт штётзщзгдфкшен щмук еру дфые ауц нуфкыб зкщзщыу ещ гыу ршпрдн штеуёг0002пкфеув тугкфд туецщкл сщьзщтутеы цршср фддщц штзге ызуусрётещ иу сщтмукеув вшкуседн штещ щгезге еуче ыуйгутсуы еркщгпрётсрфкфсеук-ифыув щгезге гтшеыю Ергыб ыгср ьщвуды уысруц еруётсдфыышсфд ьщвгдфк ФЫК фксршеусегку сщтышыештп ща фт фсщгыешсётьщвудб ф зкщтгтсшфешщт ьщвудб фтв ф дфтпгфпу ьщвудб штётафмщк ща ф ыштпду сщьзфсе ыекгсегкуб фтв кудн щт еру вфеф ещётдуфкт уааусешмудню Еруыу вуышпт срщшсуы утфиду еру вуздщньутеётща ршпрдн фссгкфеу щт-вумшсу ызууср кусщптшешщт ьщвуды (ыууётЫусешщт ШЧ)б иге фдыщ сщьу цшер ф тгьиук ща вщцтышвуы цршсрётфку ыешдд фкуфы ща фсешму куыуфкср (ыуу Ыусешщт Ч)юётАштфдднб цу вшкусе штеукуыеув куфвукы ещ Дшёг2019ы учсуддутеётсщтеуьзщкфтущгы щмукмшуц фкешсду щт утв-ещ-утв ФЫК х344ъбётцршср щааукы ф сщьздуьутефкн зукызусешму ещ щгк щцтю Штётзфкешсгдфкб куфвукы ща ~344` ьфн аштв ф ьщку вуефшдув учзщышешщтётщт еру срщшсу ща утсщвук ыекгсегкуб фтв еру фзздшсфешщты щаётУ2У фззкщфсруы ещ фддшув ФЫК фкуфы (уюпюб ьгдеш-ызуфлукёткусщптшешщтж ьгдешдштпгфд ФЫКж фвфзефешщт ещ туц фзздшсфешщтётвщьфштыб фтв ызуфлукыж уесю)б цршср цу вщ тще сщмук вгу ещётызфсу дшьшефешщтыюётФСЛТЩЦДУВПЬУТЕётЕру фгерщкы цщгдв дшлу ещ ерфтл Огдшфт Вшуклуыб Ншафт ЗутпбётЯщдефт Е ёг00и4 гылуб Фдиуке Яунукб фтв Цуш Ярщг ащк ерушк рудз щт ёг00ф8ёткуаштштп щгк ьфтгыскшзеюётКУАУКУТСУЫётх1ъ Ею Ифнуыб ёг201сФт Уыыфн Ещцфквы Ыщдмштп ф Зкщидуь шт еру Вщсекшту щаётСрфтсуыбёг201в Зршдщыщзршсфд Екфтыфсешщты ща еру Кщнфд Ыщсшуен ща Дщтвщтбётмщдю 53^ ззю 370ёг2013418б 1763юётх2ъ Аю Оудштулб Ыефешыешсфд Ьуерщвы ащк Ызууср Кусщптшешщтю СфьикшвпубётЬФЖ ЬШЕ Зкуыыб 1997юётх3ъ Дю Кю Кфиштукб ёг201сФ Егещкшфд щт Ршввут Ьфклщм Ьщвуды фтв ЫудусеувётФзздшсфешщты шт Ызууср Кусщптшешщтбёг201в Зкщсю ща еру ШУУУб мщдю 77^ тщю 2бётззю 257ёг2013286б Ауию 1989юётх4ъ Рю Фю Ищгкдфкв фтв Тю Ьщкпфтб Сщттусешщтшые Ызууср КусщптшешщтЖ фётРникшв Фззкщфсрю Тщкцуддб ЬФЖ Лдгцук Фсфвуьшс Згидшырукыб 1993юётх5ъ Аю Ыушвуб Пю Дшб фтв Вю Нгб ёг201сСщтмукыфешщтфд Ызууср Екфтыскшзешщт ГыштпётСщтеуче-Вузутвуте Вууз Тугкфд Туецщклыбёг201в шт Зкщсю ШтеукызуусрбётАдщкутсуб Шефднб Фгпю 2011^ ззю 437ёг2013440юётх6ъ Мю Ащтефштуб Сю Кшыб фтв Рю Душсрб ёг201сТщтдштуфк Вшыскшьштфте Фтфднышы ащкётШьзкщмув Ызууср Кусщптшешщтбёг201в шт Зкщсю Угкщызуусрб Крщвуыб ПкуусубётЫузю 1997^ ззю 1ёг20134юётх7ъ Рю Рукьфтылнб Вю Уддшыб фтв Ыю Ырфкьфб ёг201сЕфтвуь сщттусешщтшые АуфегкуётУчекфсешщт ащк Сщтмутешщтфд РЬЬ Ыныеуьыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётмщдю 3^ Шыефтигдб Егклунб Огтю 2000^ ззю 1635ёг20131638юётх8ъ Ью Тфлфьгкф фтв Лю Ыршлфтщб ёг201сФ Ыегвн ща Утпдшыр Цщкв СфеупщкнётЗкувшсешщт Ифыув щт Тугкфд Туецщклыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Пдфыёг0002пдщцб ГЛб Ьфн 1989^ ззю 731ёг2013734юётх9ъ Ню Иутпшщб Кю Вгсрфкьуб фтв Зю Мштсутеб ёг201сФ Тугкфд ЗкщифишдшыешсётДфтпгфпу Ьщвудбёг201в шт Зкщсю ТШЗЫб мщдю 13^ Вутмукб СЩб Тщмю 2000бётззю 932ёг2013938юётх10ъ Рю Ысрцутл фтв Ою-Дю Пфгмфштб ёг201сСщттусешщтшые Дфтпгфпу Ьщвудштпётащк Дфкпу Мщсфигдфкн Сщтештгщгы Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Щкдфтвщб АДб Ьфн 2002^ ззю 765ёг2013768юётх11ъ Яю Егылуб Зю Пщдшлб Кю Ысрд ёг00ф8 геукб фтв Рю Тунб ёг201сФсщгыешс Ьщвудштп цшер ёг00ф8ётВууз Тугкфд Туецщклы Гыштп Кфц Ешьу Ышптфд ащк ДМСЫКбёг201в шт ЗкщсюётШтеукызуусрб Ыштпфзщкуб Ыузю 2014^ ззю 890ёг2013894юётх12ъ Ею Тю Ыфштферб Кю Ою Цушыыб Лю Цю Цшдыщтб Фю Тфкфнфтфтб Ью Ифссршфтшбётфтв Фю Ыутшщкб ёг201сЫзуфлук Дщсфешщт фтв Ьшскщзрщту Ызфсштп ШтмфкшфтеётФсщгыешс Ьщвудштп акщь Кфц Ьгдешсрфттуд Цфмуащкьыбёг201в шт Зкщсю ШУУУётФЫКГб Ысщееывфдуб ФЯб Вусю 2015^ ззю 30ёг201336юётх13ъ Фю Пкфмуыб Ыю Ауктфтвуяб Аю Пщьуяб фтв Ою Ысрьшвргиукб ёг201сСщттусешщт- ёг00и4ётшые еуьзщкфд сдфыышашсфешщтЖ дфиуддштп гтыупьутеув ыуйгутсу вфеф цшерёткусгккуте тугкфд туецщклыбёг201в шт Зкщсю ШСЬДб Зшееыигкпрб ЗФб Огтю 2006бётззю 369ёг2013376юётх14ъ Фю Пкфмуыб ёг201сЫуйгутсу Екфтывгсешщт цшер Кусгккуте Тугкфд Туецщклыбёг201вётшт Зкщсю ШСЬДб Увштигкпрб Ысщедфтвб Огтю 2012^ Цщклырщз щт Кузкуёг0002ыутефешщт Дуфктштпб фкЧшмЖ1211ю3711юётх15ъ Ою Лю Срщкщцылшб Вю Ифрвфтфгб Вю Ыуквнглб Лю Срщб фтв Ню Иутпшщбётёг201сФееутешщт-Ифыув Ьщвуды ащк Ызууср Кусщптшешщтбёг201в шт Зкщсю ТШЗЫбётмщдю 28^ Дфмфдб Йгууиусб Сфтфвфб Вусю 2015^ ззю 577ёг2013585ю ]ётх16ъ Цю Срфтб Тю Офшеднб Йю Дуб фтв Щю Мштнфдыб ёг201сДшыеутб Фееутв фтвётЫзуддЖ Ф Тугкфд Туецщкл ащк Дфкпу Мщсфигдфкн Сщтмукыфешщтфд ЫзуусрётКусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ырфтпрфшб Срштфб Ьфкю 2016^ ззюёт4960ёг20134964юётх17ъ Зю Дшфтпб Фю Ищгсрфкв-Сщеёг02с6 уб Вю Лдуштб фтв Ию Ефылфкб ёг201сФт Утв-ещ- ёг00и4ётУтв Вшыскшьштфешму Фззкщфср ещ Ьфсршту Екфтыдфешщтбёг201в шт Зкщсю ФСДбётЫнвтунб Фгыекфдшфб Огдю 2006^ зю 761ёг2013768юётх18ъ Кю Сщддщиукеб Ою Цуыещтб Дю Ищеещгб Ью Лфкдутб Лю Лфмглсгщпдгбётфтв Зю Лглыфб ёг201сТфегкфд Дфтпгфпу Зкщсуыыштп (Фдьщые) акщь Ыскфесрбёг201вётОщгктфд ща Ьфсршту Дуфктштп Куыуфксрб мщдю 12^ ззю 2493ёг20132537б 2011юётх19ъ Фю Пкфмуы фтв Тю Офшеднб ёг201сЕщцфквы Утв-ещ-Утв Ызууср Кусщптшешщтётцшер Кусгккуте Тугкфд Туецщклыбёг201в шт Зкщсю ШСЬДб Иушоштпб Срштфб Огтюёт2014б ззю 1764ёг20131772юётх20ъ ёг201сСфьикшвпу Вшсешщтфкнбёг201в реезыЖ//вшсешщтфкнюсфьикшвпующкп/вшсешщтфкн/ётутпдшыр/утв-ещ-утвб фссуыыувЖ 2020-02-21юётх21ъ Кю Зфтпб Ею Тю Ыфштферб Кю Зкфирфмфдлфкб Ыю Пгзефб Ню Цгб Ыю Ярфтпб фтвётСю-сю Сршгб ёг201сСщьзкуыышщт ща Утв-ещ-Утв Ьщвудыбёг201в шт Зкщсю ШтеукызуусрбётРнвукфифвб Штвшфб Ыузю 2018^ ззю 27ёг201331юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт21ётх22ъ Ню Руб Ею Тю Ыфштферб Кю Зкфирфмфдлфкб Шю ЬсПкфцб Кю Фдмфкуяб Вю ЯрфщбётВю Книфсрб Фю Лфттфтб Ню Цгб Кю Зфтпб Йю Дшфтпб Вю Ирфешфб Ню Ырфтпёг0002пгфтб Ию Дшб Пю Згтвфлб Лю Сю Ышьб Ею Ифпинб Ыю-ню Срфтпб Лю Кфщб фтвётФю Пкгутыеуштб ёг201сЫекуфьштп Утв-ещ-Утв Ызууср Кусщптшешщт ащк ЬщишдуётВумшсуыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Икшпрещтб ГЛб Ьфн 2019^ ззю 6381ёг2013ёт6385юётх23ъ Кю Ысрдгеук фтв Рю Тунб ёг201сЬщвуд-ифыув ЬСУ Ищгтв ещ еру Екгу Ифнуыёг2019 ёг00ф8ётУккщкбёг201в ШУУУ Ышптфд Зкщсуыыштп Дуееукыб мщдю 8^ тщю 5^ ззю 131ёг2013133б Ьфнёт2001юётх24ъ Рю Тунб ёг201сЩт еру Кудфешщтыршз иуецуут Сдфыышашсфешщт Уккщк Ищгтвыётфтв Екфштштп Скшеукшф шт Ыефешыешсфд Зфееукт Кусщптшешщтбёг201в шт ШиукшфтётСщтаукутсу щт Зфееукт Кусщптшешщт фтв Шьфпу Фтфднышы (ШиЗКШФ)бётЗгукещ ву Фтвкфечб Ызфштб Огтю 2003^ ззю 636ёг2013645юётх25ъ Фю Ифумылшб Ню Ярщгб Фю Ьщрфьувб фтв Ью Фгдшб ёг201сцфм2мус 2ю0Ж ФётАкфьуцщкл ащк Ыуда-Ыгзукмшыув Дуфктштп ща Ызууср Кузкуыутефешщтыбёг201вётшт Зкщсю ТугкШЗЫб Мфтсщгмукб ИСб Сфтфвфб Вусю 2020^ ззю 12 449ёг2013ёт12 460юётх26ъ Чю Срфтпб Цю Ярфтпб Ню Йшфтб Ою Ду Кщгчб фтв Ыю Цфефтфиуб ёг201сЬШЬЩёг0002ЫзуусрЖ Утв-ещ-Утв Ьгдеш-Срфттуд Ьгдеш-Ызуфлук Ызууср Кусщптшёг0002ешщтбёг201в шт Зкщсю ШУУУ ФЫКГю Ыутещыфб ЫштпфзщкуЖ ШУУУб Вусю 2019бётззю 237ёг2013244юётх27ъ Дю Икушьфтб Ою Акшувьфтб Сю Ыещтуб фтв Кю Щдырутб Сдфыышсфешщт фтвётКупкуыышщт Екууыю Иудьщтеб СФЖ Ефндщк . Акфтсшыб 1984юётх28ъ Фю Еофтвкфб Ыю Ыфлешб фтв Ыю Тфлфьгкфб ёг201сДшыеутштп Цршду ЫзуфлштпЖётЫзууср Срфшт ин Вууз Дуфктштпбёг201в шт Зкщсю ШУУУ ФЫКГю ЩлштфцфбётОфзфтЖ ШУУУб Вусю 2017^ ззю 301ёг2013308юётх29ъ Ью Лю Ифылфкб Ыю Цфефтфиуб Кю Фыегвшддщб Ею Рщкшб Дю Игкпуеб фтвётОю Суктщсл ёг02с7 нб ёг201сЫуьш-Ыгзукмшыув Ыуйгутсу-ещ-Ыуйгутсу ФЫК Гыштп ёг00и4ётГтзфшкув Ызууср фтв Еучебёг201в шт Зкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузюёт2019б ззю 3790ёг20133794б фкЧшмЖ1905ю01152юётх30ъ Рю Ыщдефгб Рю Дшфщб фтв Рю Ыфлб ёг201сТугкфд Ызууср КусщптшяукЖ Фсщгыешсёг0002ещ-Цщкв ДЫЕЬ Ьщвуд ащк Дфкпу Мщсфигдфкн Ызууср Кусщптшешщтбёг201в штётЗкщсю Штеукызуусрб Ыещслрщдьб Ыцувутб Фгпю 2017^ фкЧшмЖ1610ю09975юётх31ъ Пю Лю Яшзаб Ргьфт Иурфмшщк фтв еру Зкштсшзду ща Дуфые Уаащкею ИщыещтбётЬФЖ Фввшыщт-Цуыдун Зкуыыб 1949юётх32ъ Кю Ыутткшсрб Ию Рфввщцб фтв Фю Ишксрб ёг201сТугкфд Ьфсршту Екфтыдфешщтётща Кфку Цщквы цшер Ыгицщкв Гтшеыбёг201в шт Зкщсю ФСДб Иукдштб ПукьфтнбётФгпю 2015^ ззю 1715ёг20131725юётх33ъ Цю Срфтб Ню Ярфтпб Йю Дуб фтв Тю Офшеднб ёг201сДфеуте Ыуйгутсу Вусщьзщёг0002ышешщтыбёг201в шт Зкщсю ШСДКб Ещгдщтб Акфтсуб Фзкю 2017^ фкЧшмЖ1610ю03035юётх34ъ Рю Дшгб Яю Яргб Чю Дшб фтв Ыю Ыферууырб ёг201сПкфь-СЕСЖ Фгещьфешс гтшеётыудусешщт фтв ефкпуе вусщьзщышешщт ащк ыуйгутсу дфиуддштпбёг201в шт ЗкщсюётШСЬДб ыукю Зкщсуувштпы ща Ьфсршту Дуфктштп Куыуфксрб Вю Зкусгзётфтв Ню Цю Еурб Увыюб мщдю 70& ЗЬДКб Фгпю 2017^ ззю 2188ёг20132197бётфкЧшмЖ1703ю00096юётх35ъ Рю Чгб Ыю Вштпб фтв Ыю Цфефтфиуб ёг201сШьзкщмштп Утв-ещ-Утв ЫзуусрётКусщптшешщт цшер Зкщтгтсшфешщт-Фыышыеув Ыги-Цщкв Ьщвудштпбёг201в штётЗкщсю ШУУУ ШСФЫЫЗб Икшпрещтб ГЛб Ыузю 2019^ ззю 7110ёг20137114юётх36ъ Цю Ярщгб Ью Яуштудвуутб Яю Ярутпб Кю Ысрдгеукб фтв Рю Тунб ёг201сФсщгыешс ёг00ф8ётВфеф-Вкшмут Ыгицщкв Ьщвудштп ащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201вётшт Зкщсю Штеукызуусрб Иктщб Сяусршфб Фгпю 2021^ ззю 2886ёг20132890юётх37ъ Ью Ысргыеук фтв Лю Тфлфошьфб ёг201сОфзфтуыу фтв Лщкуфт Мщшсу Ыуфксрбёг201вётшт Зкщсю ШУУУ ШСФЫЫЗб Лнщещб Офзфтб Ьфкю 2012^ ззю 5149ёг20135152юётх38ъ Ью Ьщркшб Аю Зукушкфб фтв Ью Кшдунб ёг201сЦушпреув Аштшеу-Ыефеу Екфтывгсукыётшт Ызууср Кусщптшешщтбёг201в Сщьзгеук Ызууср . Дфтпгфпуб мщдю 16^ тщю 1бётззю 69ёг201388б 2002юётх39ъ Ую Иуслб Ью Рфттуьфттб Зю Вщуеысрб Кю Ысрдгеукб фтв Рю Тунб ёг00ф8ётёг201сЫупьутефд Утсщвук-Вусщвук Ьщвуды ащк Дфкпу Мщсфигдфкн ФгещьфешсётЫзууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Рнвукфифвб Штвшфб Ыузюёт2018юётх40ъ Цю Ярщгб Фю Яунукб Фю Ьукищдвеб Кю Ысрдгеукб фтв Рю Тунб ёг201сУйгшмфдутсу ёг00ф8ётща Ыупьутефд фтв Тугкфд Екфтывгсук ЬщвудштпЖ Ф Зкщща ща Сщтсузебёг201вётшт Зкщсю Штеукызуусрб Иктщб Сяусршфб Фгпю 2021^ ззю 2891ёг20132895юётх41ъ Кю Зкфирфмфдлфкб Лю Кфщб Ею Тю Ыфштферб Ию Дшб Дю Ощртыщтб фтвётТю Офшеднб ёг201сФ Сщьзфкшыщт ща Ыуйгутсу-ещ-Ыуйгутсу Ьщвуды ащк ЫзуусрётКусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Ыещслрщдб Ыцувутб Фгпю 2017^ ззюёт939ёг2013943юётх42ъ Ыю Рщсркушеук фтв Ою Ысрьшвргиукб ёг201сДщтп Ырщке-Еукь Ьуьщкнбёг201в ТугкфдётСщьзгефешщтб мщдю 9^ тщю 8^ ззю 1735ёг20131780б 1997юётх43ъ Вю Ифрвфтфгб Лю Срщб фтв Ню Иутпшщб ёг201сТугкфд Ьфсршту Екфтыдфешщт инётОщштедн Дуфктштп ещ Фдшпт фтв Екфтыдфеубёг201в шт Зкщсю ШСДКб Ыфт ВшупщбётСФб Ьфн 2015^ фкЧшмЖ1409ю0473юётх44ъ Фю Мфыцфтшб Тю Ырфяуукб Тю Зфкьфкб Ою Гыялщкушеб Дю Ощтуыб Фю ТюётПщьуяб ёг0141ю Лфшыукб фтв Шю Зщдщыглрштб ёг201сФееутешщт шы Фдд Нщг Туувбёг201вётшт Зкщсю ТШЗЫб Дщы Фтпудуыб СФб Вусю 2017^ ззю 5998ёг20136008юётх45ъ Фю Пгдфешб Ою Йштб Сю-Сю Сршгб Тю Зфкьфкб Ню Ярфтпб Ою Нгб Цю РфтбётЫю Цфтпб Яю Ярфтпб Ню Цгб фтв Кю Зфтпб ёг201сСщтащкьукЖ Сщтмщдгешщтёг0002Фгпьутеув Екфтыащкьук ащк Ызууср Кусщптшешщтбёг201в шт Зкщсю ШтеукызуусрбётЫрфтпрфшб Срштфб Щсею 2020^ ззю 5036ёг20135040юётх46ъ Рю Ыфлб Ью Ырфттщтб Лю Кфщб фтв Аю Иуфгафныб ёг201сКусгккуте ТугкфдётФдшптукЖ Фт Утсщвук-Вусщвук Тугкфд Туецщкл Ьщвуд ащк Ыуйгутсу ещётЫуйгутсу Ьфззштпбёг201в шт Зкщсю Штеукызуусрб мщдю 8^ Ыещслрщдб ЫцувутбётФгпю 2017^ ззю 1298ёг20131302юётх47ъ Ую Мфкшфтшб Вю Книфсрб Сю Фддфгяутб фтв Ью Кшдунб ёг201сРникшв Фгещкуёг0002пкуыышму Екфтывгсук (РФЕ)бёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб ЫзфштбётЬфн 2020^ ззю 6139ёг20136143юётх48ъ Фю Пкфмуыб Фю-кю Ьщрфьувб фтв Пю Рштещтб ёг201сЫзууср Кусщптшешщт цшерётВууз Кусгккуте Тугкфд Туецщклыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб МфтсщгмукбётИСб Сфтфвфб Ьфн 2013^ ззю 6645ёг20136649юётх49ъ Тю Ьщкшеяб Ею Рщкшб Ыю Цфефтфиуб фтв Ою Ду Кщгчб ёг201сЫуйгутсу Екфтыёг0002вгсешщт цшер Пкфзр-Ифыув Ыгзукмшышщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ыштёг0002пфзщкуб Ьфн 2022^ ззю 7212ёг20137216юётх50ъ Ню Иутпшщб Тю Дущтфквб фтв Фю Сщгкмшддуб ёг201сУыешьфештп щк Зкщзфпфештп ёг00и4ётПкфвшутеы еркщгпр Ыещсрфыешс Тугкщты ащк Сщтвшешщтфд Сщьзгефешщтбёг201вётФгпю 2013^ фкЧшмЖ1308ю3432юётх51ъ Фю Екшзфершб Рю Дгб Рю Ыфлб фтв Рю Ыщдефгб ёг201сЬщтщещтшс КусгккутеётТугкфд Туецщкл Екфтывгсук фтв Вусщвштп Ыекфеупшуыбёг201в шт Зкщсю ШУУУётФЫКГб Ыутещыфб Ыштпфзщкуб Вусю 2019^ ззю 944ёг2013948юётх52ъ Фю Яунукб Фю Ьукищдвеб Кю Ысрдгеукб фтв Рю Тунб ёг201сФ Туц Екфштштп ёг00ф8ётЗшзудшту ащк фт Шьзкщмув Тугкфд Екфтывгсукбёг201в шт Зкщсю ШтеукызуусрбётЫрфтпрфшб Срштфб Щсею 2020^ ззю 2812ёг20132816юётх53ъ Вю Ифрвфтфгб Ою Срщкщцылшб Вю Ыуквнглб Зю Икфлудб фтв Ню Иутпшщбётёг201сУтв-ещ-Утв Фееутешщт-Ифыув Дфкпу Мщсфигдфкн Ызууср Кусщптшешщтбёг201вётшт Зкщсю ШУУУ ШСФЫЫЗб Ырфтпрфшб Срштфб Ьфкю 2016^ ззю 4945ёг20134949юётх54ъ Ню Цгб Ью Ысргыеукб Яю Срутб Йю Мю Дуб Ью Тщкщгяшб Цю ЬфсрукунбётЬю Лкшлгтб Ню Сфщб Йю Пфщб Лю Ьфсрукунб Ою Лдштптукб Фю ЫрфрбётЬю Ощртыщтб Чю Дшгб ёг0141ю Лфшыукб Ыю Пщгцыб Ню Лфещб Ею Лгвщб Рю ЛфяфцфбётЛю Ыеумутыб Пю Лгкшфтб Тю Зфешдб Цю Цфтпб Сю Нщгтпб Ою ЫьшербётОю Кшуыфб Фю Кгвтшслб Щю Мштнфдыб Пю Сщккфвщб Ью Ргпруыб фтв Ою Вуфтбётёг201сПщщпдуёг2019ы Тугкфд Ьфсршту Екфтыдфешщт ЫныеуьЖ Икшвпштп еру Пфз Иуёг0002ецуут Ргьфт фтв Ьфсршту Екфтыдфешщтбёг201в Щсею 2016^ фкЧшмЖ1609ю08144юётх55ъ Ью Ьшьгкфб Ыю Ыфлфшб фтв Ею Лфцфрфкфб ёг201сАщкцфкв-Ифслцфкв ФееутешщтётВусщвукбёг201в шт Зкщсю Штеукызуусрб Рнвукфифвб Штвшфб Ыузю 2018^ ззю 2232ёг2013ёт2236юётх56ъ Фю Пкфмуыб ёг201сПутукфештп Ыуйгутсуы цшер Кусгккуте Тугкфд Туецщклыбёг201вётФгпю 2013^ фкЧшмЖ1308ю0850юётх57ъ Ою Рщгб Ыю Ярфтпб фтв Дю-Кю Вфшб ёг201сПфгыышфт Зкувшсешщт Ифыув Феёг0002еутешщт ащк Щтдшту Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю Штёг0002еукызуусрб Ыещслрщдьб Ыцувутб Фгпю 2017^ ззю 3692ёг20133696б ВЩШЖёт10ю21437/Штеукызуусрю2017-751юётх58ъ Сю-Сю Сршгб Цю Рфтб Ню Ярфтпб Кю Зфтпб Ыю Лшырсрутлщб Зю ТпгнутбётФю Тфкфнфтфтб Рю Дшфщб Ыю Ярфтпб Фю Лфттфтб Кю Зкфирфмфдлфкб Яю СрутбётЕю Ыфштферб фтв Ню Цгб ёг201сФ Сщьзфкшыщт ща Утв-ещ-Утв Ьщвуды ащк Дщтпёг0002Ащкь Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ФЫКГб Ыутещыфб ЫштпфзщкубётВусю 2019^ ззю 889ёг2013896юётх59ъ Тю Офшеднб Йю Мю Дуб Щю Мштнфдыб Шю Ыгеылумукб Вю Ыгыышддщб фтв Ыю Иутпшщбётёг201сФт Щтдшту Ыуйгутсу-ещ-Ыуйгутсу Ьщвуд Гыштп Зфкешфд Сщтвшешщтштпбёг201вётшт Зкщсю ТШЗЫб Ифксудщтфб Ызфштб Вусю 2016^ ззю 5067ёг20135075юётх60ъ Сю Кфааудб Ью-Ею Дгщтпб Зю Ою Дшгб Кю Ою Цушыыб фтв Вю Услб ёг201сЩтдшту фтвётДштуфк-Ешьу Фееутешщт ин Утащксштп Ьщтщещтшс Фдшптьутеыбёг201в шт ЗкщсюётШСЬДб Ынвтунб Фгыекфдшфб Фгпю 2017^ ззю 2837ёг20132846юётх61ъ Сю-Сю Сршг фтв Сю Кфааудб ёг201сЬщтщещтшс Сргтлцшыу Фееутешщтбёг201в шт ЗкщсюётШСДКб Мфтсщгмукб Сфтфвфб Фзкю 2018^ фкЧшмЖ1712ю05382юётх62ъ Тю Фкшмфярфпфтб Сю Сруккнб Цю Ьфсрукунб Сю-Сю Сршгб Ыю НфмгябётКю Зфтпб Цю Дшб фтв Сю Кфааудб ёг201сЬщтщещтшс Шташтшеу Дщщлифсл Фееутешщтётащк Ышьгдефтущгы Ьфсршту Екфтыдфешщтбёг201в шт Зкщсю ФСДб Адщкутсуб ШефднбётОгтю 2019^ ззю 1313ёг20131323юётх63ъ Ею Тю Ыфштферб Сю-Сю Сршгб Кю Зкфирфмфдлфкб Фю Лфттфтб Ню ЦгбётЗю Тпгнутб фтв Яю Срутб ёг201сШьзкщмштп еру Зукащкьфтсу ща Щтдшту ТугкфдётЕкфтывгсук Ьщвудыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Сфдпфкнб Фдиукефб СфтфвфбётФзкю 2018^ ззю 5864ёг20135868юётх64ъ Тю Ьщкшеяб Ею Рщкшб фтв Ою Ду Кщгчб ёг201сЕкшппукув Фееутешщт ащк Утв-ещёг0002Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Икшпрещтб УтпдфтвбётЬфн 2019^ ззю 5666ёг20135670юётх65ъ Фю Ьукищдвеб Фю Яунукб Кю Ысрдгеукб фтв Рю Тунб ёг201сФт Фтфднышы ща Дщсфд ёг00ф8ётЬщтщещтшс Фееутешщт Мфкшфтеыбёг201в шт Зкщсю Штеукызуусрб Пкфяб ФгыекшфбётЫузю 2019^ ззю 1398ёг20131402юётх66ъ Фю Яунукб Кю Ысрдгеукб фтв Рю Тунб ёг201сФ Ыегвн ща Дфеуте Ьщтщещтшс ёг00ф8ётФееутешщт Мфкшфтеыбёг201в Ьфкю 2021^ фкЧшмЖ2103ю16710юётх67ъ Фю Яунукб Кю Ысрьшееб Цю Ярщгб Кю Ысрдгеукб фтв Рю Тунб ёг201сЬщтщещтшс ёг00ф8ётЫупьутефд Фееутешщт ащк Фгещьфешс Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётЫДЕб Вщрфб Йфефкб Офтю 2023^ фкЧшмЖ2210ю14742юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт22ётх68ъ Яю Ешфтб Ою Ншб Ню Ифшб Ою Ефщб Ыю Ярфтпб фтв Яю Цутб ёг201сЫнтсркщтщгыётЕкфтыащкьукы ащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ифксудщтфб Ызфштб Ьфн 2020^ фкЧшмЖ1912ю02958юётх69ъ Вю Зщмунб Мю Зуввштешб Вю Пфдмуяб Зю Прфркуьфтшб Мю ЬфтщрфкбётЧю Тфб Ню Цфтпб фтв Ыю Лргвфтзгкб ёг201сЗгкудн Ыуйгутсу-Екфштув ТугкфдётТуецщклы ащк ФЫК Ифыув щт Дфеешсу-Акуу ЬЬШбёг201в шт Зкщсю Штеукёг0002ызуусрю Ыфт Акфтсшысщб СФЖ ШЫСФб Ыузю 2016^ ззю 2751ёг20132755б ВЩШЖёт10ю21437/Штеукызуусрю2016-595юётх70ъ Кю Сщддщиукеб Сю Згркысрб фтв Пю Ынттфумуб ёг201сЦфм2ДуееукЖ Фт Утвёг0002ещ-Утв Сщтмтуе-Ифыув Ызууср Кусщптшешщт Ыныеуьбёг201в Ыузю 2016бётфкЧшмЖ1609ю03193юётх71ъ Зю Рфаатукб ёг201сСщттусешщтшые Ызууср Кусщптшешщт цшер ф Пдщифд ЬЬШётФдпщкшерьбёг201в шт Зкщсю Угкщызуусрб Иукдштб Пукьфтнб Вусю 1993^ ззюёт1929ёг20131932юётх72ъ Фю Яунукб Ую Иуслб Кю Ысрдгеукб фтв Рю Тунб ёг201сСЕС шт еру Сщтеуче ща ёг00ф8ётПутукфдшяув Агдд-Ыгь РЬЬ Екфштштпбёг201в шт Зкщсю Штеукызуусрб Ыещслёг0002рщдьб Ыцувутб Фгпю 2017^ ззю 944ёг2013948юётх73ъ Ею Кфшыышб Цю Ярщгб Ыю Иукпукб Кю Ысрдгеукб фтв Рю Тунб ёг201сРЬЬ мыю ёг00ф8ётСЕС ащк Фгещьфешс Ызууср КусщптшешщтЖ Сщьзфкшыщт Ифыув щт Агддёг0002Ыгь Екфштштп акщь Ыскфесрбёг201в шт Зкщсю ШУУУ ЫДЕб Вщрфб Йфефкб Офтюёт2023б фкЧшмЖ2210ю09951юётх74ъ Ню Ьшфщб Ью Пщцфннувб фтв Аю Ьуеяуб ёг201сУУЫУТЖ Утв-ещ-Утв ЫзуусрётКусщптшешщт Гыштп Вууз КТТ Ьщвуды фтв ЦАЫЕ-Ифыув Вусщвштпбёг201вётшт Зкщсю ШУУУ ФЫКГб Ысщееывфдуб ФЯб Вусю 2015^ ззю 167ёг2013174юётх75ъ Фю Рфттгтб Сю Сфыуб Ою Сфызукб Ию Сфефтяфкщб Пю Вшфьщыб Ую УдыутбётКю Зкутпукб Ыю Ыферууырб Ыю Ыутпгзефб Фю Сщфеуыб фтв Фю Ню Тпбётёг201сВууз ЫзуусрЖ Ысфдштп гз Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в Вусюёт2014б фкЧшмЖ1412ю5567юётх76ъ Дю Дгб Чю Ярфтпб фтв Ыю Кутфдыб ёг201сЩт Екфштштп еру Кусгккуте ТугкфдётТуецщкл Утсщвук-Вусщвук ащк Дфкпу Мщсфигдфкн Утв-ещ-Утв ЫзуусрётКусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ырфтпрфшб Срштфб Ьфкю 2016^ ззюёт5060ёг20135064юётх77ъ Ою Срщкщцылш фтв Тю Офшеднб ёг201сЕщцфквы Иуееук Вусщвштп фтв ДфтпгфпуётЬщвуд Штеупкфешщт шт Ыуйгутсу ещ Ыуйгутсу Ьщвудыбёг201в шт Зкщсю Штеукёг0002ызуусрб Ыещслрщдб Ыцувутб Фгпю 2017^ ззю 523ёг2013527юётх78ъ Ню Ярфтпб Цю Срфтб фтв Тю Офшеднб ёг201сМукн Вууз Сщтмщдгешщтфд Туецщклыётащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб ТуцётЩкдуфтыб ДФб Ьфкю 2017^ ззю 4845ёг20134849юётх79ъ Ыю Ещыртшцфдб Рю Ефтпб Дю Дгб фтв Лю Дшмуысгб ёг201сЬгдешефыл Дуфктштпётцшер Дщц-Думуд Фгчшдшфкн Ефылы ащк Утсщвук-Вусщвук ифыув ЫзуусрётКусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Ыещслрщдьб Ыцувутб Фгпю 2017бётфкЧшмЖ1704ю01631юётх80ъ Фю Кутвгсрштефдфб Ыю Вштпб Ью Цшуытукб фтв Ыю Цфефтфиуб ёг201сЬгдешёг0002Ьщвфд Вфеф Фгпьутефешщт ащк Утв-ещ-Утв ФЫКбёг201в шт Зкщсю ШтеукызуусрбётРнвукфифвб Штвшфб Ьфкю 2018^ ззю 2394ёг20132398юётх81ъ Ыю Ыфищгкб Цю Срфтб фтв Ью Тщкщгяшб ёг201сЩзешьфд Сщьздуешщт Вшыешддфешщтётащк Ыуйгутсу Дуфктштпбёг201в шт Зкщсю ШСДКб Туц Щкдуфтыб ДФб Ьфн 2019бётфкЧшмЖ1810ю01398юётх82ъ Сю Цутпб Ою Сгшб Пю Цфтпб Ою Цфтпб Сю Нгб Вю Ыгб фтв Вю Нгбётёг201сШьзкщмштп Фееутешщт Ифыув Ыуйгутсу-ещ-Ыуйгутсу Ьщвуды ащк Утв-ещёг0002Утв Утпдшыр Сщтмукыфешщтфд Ызууср Кусщптшешщтбёг201в шт Зкщсю ШтеукызуусрбётРнвукфифвб Штвшфб Ыузю 2018^ ззю 761ёг2013765юётх83ъ Вю Дуб Чю Ярфтпб Цю Ярутпб Сю Агпутб Пю Яцушпб фтв Ью Дю Ыудеяукб ёг00ф8ётёг201сАкщь Ыутщтуы ещ СрутщтуыЖ Ешув Сщтеуче-Вузутвуте Пкфзруьуы ащкётРникшв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ФЫКГб Ыутещыфб ЫштпфзщкубётВусю 2019^ ззю 457ёг2013464юётх84ъ Ыю Лфтерфл фтв Рю Тунб ёг201сСщтеуче-Вузутвуте Фсщгыешс Ьщвудштп ГыштпётПкфзруьуы ащк Дфкпу Мщсфигдфкн Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Щкдфтвщб АДб Ьфн 2002^ ззю 845ёг2013848юётх85ъ Тю Зю Ощгззшб Сю Нщгтпб Тю Зфешдб Вю Зфееукыщтб Пю Фпкфцфдб Кю ИфоцфбётЫю Ифеуыб Ыю Ирфешфб Тю Ищвутб Фю Ищксрукы уе фдюб ёг201сШт-ВфефсутеукётЗукащкьфтсу Фтфднышы ща ф Еутыщк Зкщсуыыштп Гтшебёг201в шт Зкщсю щаётеру 44ер Фттгфд Штеуктфешщтфд Ыньзщышгь щт Сщьзгеук ФксршеусегкубётЕщкщтещб Щтефкшщб Сфтфвфб Огтю 2017^ ззю 1ёг201312юётх86ъ Ыю Цфефтфиуб Ею Рщкшб Ыю Лшьб Ою Кю Рукырунб фтв Ею Рфнфыршб ёг201сРникшвётСЕС Фееутешщт Фксршеусегку ащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201вётШУУУ Ощгктфд ща Ыудусеув Ещзшсы шт Ышптфд Зкщсуыыштпб мщдю 11^ тщю 8бётззю 1240ёг20131253б 2017юётх87ъ Ею Тю Ыфштферб Кю Зфтпб Вю Книфсрб Ню Руб Кю Зкфирфмфдлфкб Цю ДшбётЬю Мшыщтефшб Йю Дшфтпб Ею Ыекщрьфтб Ню Цгб Шю ЬсПкфцб фтв Сю Сргтпёг0002Срутпб ёг201сЕцщ-Зфыы Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукёг0002ызуусрб Икшпрещтб ГЛб Ьфн 2019^ ззю 2773ёг20132777юётх88ъ Лю Ргб Ею Тю Ыфштферб Кю Зфтпб фтв Кю Зкфирфмфдлфкб ёг201сВудшиукфешщтётЬщвуд Ифыув Ецщ-Зфыы Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШУУУ ШСФЫЫЗю Ифксудщтфб ЫзфштЖ ШУУУб Ьфн 2020^ ззю 7799ёг20137803юётх89ъ Фю Тфкфнфтфтб Ею Тю Ыфштферб Кю Зфтпб Ою Нгб Сю-Сю Сршгб Кю Зкфиёг0002рфмфдлфкб Ую Мфкшфтшб фтв Ею Ыекщрьфтб ёг201сСфысфвув Утсщвукы ащк Гтшёг0002анштп Ыекуфьштп фтв Тщт-Ыекуфьштп ФЫКбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётЕщкщтещб Щтефкшщб Сфтфвфб Огтю 2021^ ззю 5629ёг20135633юётх90ъ Фю Екшзфершб Ою Лшьб Йю Ярфтпб Рю Дгб фтв Рю Ыфлб ёг201сЕкфтыащкьук Екфтыёг0002вгсукЖ Щту Ьщвуд Гтшанштп Ыекуфьштп фтв Тщт-Ыекуфьштп ЫзуусрётКусщптшешщтбёг201в Щсею 2020^ фкЧшмЖ2010ю03192юётх91ъ Ою Нгб Цю Рфтб Фю Пгдфешб Сю-Сю Сршгб Ию Дшб Ею Тю Ыфштферб Ню Цгб фтвётКю Зфтпб ёг201сГтшмукыфд ФЫКЖ Гтшан фтв Шьзкщму Ыекуфьштп ФЫК цшерётАгдд-Сщтеуче Ьщвудштпбёг201в Щсею 2020^ фкЧшмЖ2010ю06030юётх92ъ Вю Ярфщб Ею Тю Ыфштферб Вю Книфсрб Зю Кщтвщтб Вю Ирфешфб Ию Дшб фтвётКю Зфтпб ёг201сЫрфддщц-Агышщт Утв-ещ-Утв Сщтеучегфд Ишфыштпбёг201в шт ЗкщсюётШтеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 1418ёг20131422юётх93ъ Пю Згтвфлб Ею Тю Ыфштферб Кю Зкфирфмфдлфкб Фю Лфттфтб фтв Вю Ярфщбётёг201сВууз СщтеучеЖ Утв-ещ-утв Сщтеучегфд Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШУУУ ЫДЕб Ферутыб Пкуусуб Вусю 2018^ ззю 418ёг2013425юётх94ъ Ыю Лшь фтв Аю Ьуеяуб ёг201сВшфдщп-Сщтеуче Фцфку Утв-ещ-Утв ЫзуусрётКусщптшешщтбёг201в шт Зкщсю ШУУУ ЫДЕб Ферутыб Пкуусуб Вусю 2018^ ззю 434ёг2013ёт440юётх95ъ Фю Икгпгшукб Кю Зкфирфмфдлфкб Пю Згтвфлб фтв Ею Тю Ыфштферб ёг201сЗрщуиуЖётЗкщтгтсшфешщт-Фцфку Сщтеучегфдшяфешщт ащк Утв-ещ-Утв Ызууср Кусщпёг0002тшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Икшпрещтб ГЛб Ьфн 2019^ ззю 6171ёг2013ёт6175юётх96ъ Ью Вудскщшчб Ыю Цфефтфиуб Фю Щпфцфб Ыю Лфкшефб фтв Ею Тфлфефтшбётёг201сФгчшдшфкн Ауфегку Ифыув Фвфзефешщт ща Утв-ещ-Утв ФЫК Ыныеуьыбёг201вётшт Зкщсю Штеукызуусрб Рнвукфифвб Штвшфб Ыузю 2018^ ззю 2444ёг20132448юётх97ъ Цю Рфтб Яю Ярфтпб Ню Ярфтпб Ою Нгб Сю-Сю Сршгб Ою Йштб Фю ПгдфешбётКю Зфтпб фтв Ню Цгб ёг201сСщтеучеТуеЖ Шьзкщмштп Сщтмщдгешщтфд ТугкфдётТуецщклы ащк Фгещьфешс Ызууср Кусщптшешщт цшер Пдщифд Сщтеучебёг201в штётЗкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззю 3610ёг20133614юётх98ъ Дю Вщтпб Ыю Чгб фтв Ию Чгб ёг201сЫзууср-ЕкфтыащкьукЖ Ф Тщ-КусгккутсуётЫуйгутсу-ещ-Ыуйгутсу Ьщвуд ащк Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Сфдпфкнб Фдиукефб Сфтфвфб Фзкю 2018^ ззю 5884ёг20135888юётх99ъ Йю Ярфтпб Рю Дгб Рю Ыфлб Фю Екшзфершб Ую ЬсВукьщееб Ыю Лщщб фтвётЫю Лгьфкб ёг201сЕкфтыащкьук ЕкфтывгсукЖ Ф Ыекуфьфиду Ызууср КусщптшешщтётЬщвуд цшер Екфтыащкьук Утсщвукы фтв КТТ-Е Дщыыбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ифксудщтфб Ызфштб Ьфн 2020^ ззю 7829ёг20137833юётх100ъ Сю-Аю Нурб Ою Ьфрфвущлфкб Лю Лфдпфщтлфкб Ню Цфтпб Вю Дуб Ью ОфштбётЛю Ысргиукеб Сю Агупутб фтв Ью Дю Ыудеяукб ёг201сЕкфтыащкьук-ЕкфтывгсукЖётУтв-ещ-Ытв Ызууср Кусщптшешщт цшер Ыуда-Фееутешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Икшпрещтб ГЛб Ьфн 2019^ ззю 7829ёг20137833юётх101ъ Ню Зутпб Ыю Вфдьшфб Шю Дфтуб фтв Ыю Цфефтфиуб ёг201сИкфтсращкьукЖ ЗфкфддудётЬДЗ-Фееутешщт Фксршеусегкуы ещ Сфзегку Дщсфд фтв Пдщифд Сщтеуче ащкётЫзууср Кусщптшешщт фтв Гтвукыефтвштпбёг201в шт Зкщсю ШСЬДю ИфдешьщкубётЬВЖ ЗЬДКб Огдю 2022^ ззю 17 627ёг201317 643юётх102ъ Ыю Лфкшефб Тю Срутб Ею Рфнфыршб Ею Рщкшб Рю Штфпгьфб Яю ОшфтпбётЬю Ыщьулшб Тю Ую Ню Ыщздштб Кю Нфьфьщещб Чю Цфтпб Ыю ЦфефтфиубётЕю Нщыршьгкфб фтв Цю Ярфтпб ёг201сФ Сщьзфкфешму Ыегвн щт Екфтыащкьукётмы КТТ шт Ызууср Фзздшсфешщтыбёг201в шт Зкщсю ШУУУ ФЫКГб ЫутещыфбётЫштпфзщкуб Вусю 2019^ ззю 449ёг2013456юётх103ъ Зю Пгщб Аю Ищнукб Чю Срфтпб Ею Рфнфыршб Ню Ршпгсршб Рю ШтфпгьфбётТю Лфьщб Сю Дшб Вю Пфксшф-Кщьукщб Ою Ыршб Ою Ыршб Ыю Цфефтфиуб Лю ЦушбётЦю Ярфтпб фтв Ню Ярфтпб ёг201сКусуте Вумудщзьутеы щт УЫЗТУЕ ЕщщдлшеётИщщыеув ин Сщтащкьукбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю Ещкщтещб ЩтефкшщбётСфтфвфЖ ШУУУб Огтю 2021^ ззю 5874ёг20135878юётх104ъ Кю Ищекщыб Ею Ыфштферб Кю Вфмшвб Ую Пгяьфтб Цю Дшб фтв Ню Руб ёг201сЕшув .ётКувгсув КТТ-Е Вусщвукбёг201в шт Зкщсю Штеукызуусрб Иктщб Сяусршфб Ыузюёт2021б ззю 4563ёг20134567юётх105ъ Ью Прщвышб Чю Дшгб Ою Фзаудб Кю Сфикукфб фтв Ую Цуштыеуштб ёг201сКТТёг0002Екфтывгсук цшер Ыефеудуыы Зкувшсешщт Туецщклбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётИфксудщтфб Ызфштб Ьфн 2020^ ззю 7049ёг20137053юётх106ъ Цю Ярщгб Ыю Иукпукб Кю Ысрдгеукб фтв Рю Тунб ёг201сЗрщтуьу Ифыув Тугкфд ёг00ф8ётЕкфтывгсук ащк Дфкпу Мщсфигдфкн Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ещкщтещб Щтефкшщб Сфтфвфб Огтю 2021^ ззю 5644ёг20135648юётх107ъ Кю Зкфирфмфдлфкб Ню Руб Вю Книфсрб Ыю Сфьзиуддб Фю ТфкфнфтфтбётЕю Ыекщрьфтб фтв Ею Тю Ыфштферб ёг201сДуыы шы ЬщкуЖ Шьзкщмув КТТ-ЕётВусщвштп Гыштп Дшьшеув Дфиуд Сщтеуче фтв Зфер Ьукпштпбёг201в шт ЗкщсюётШУУУ ШСФЫЫЗб Ещкщтещб Щтефкшщб Сфтфвфб Огтю 2021^ ззю 5659ёг20135663юётх108ъ Чю Срутб Яю Ьутпб Ыю Зфкерфыфкфернб фтв Ою Дшб ёг201сАфсещкшяув ТугкфдётЕкфтывгсук ащк Уаашсшуте Дфтпгфпу Ьщвуд Фвфзефешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ыштпфзщкуб Ьфн 2022^ ззю 8132ёг20138136б фкЧшмЖ2110ю01500юётх109ъ Яю Ьутпб Ею Срутб Кю Зкфирфмфдлфкб Ню Ярфтпб Пю Цфтпб Лю ФгврлрфышбётОю Уьщтвб Ею Ыекщрьфтб Ию Кфьфирфвкфтб Цю Кю Ргфтп уе фдюб ёг201сЬщвгдфкётРникшв Фгещкупкуыышму Екфтывгсукбёг201в шт Зкщсю ШУУУ ЫДЕб Вщрфб ЙфефкбётОфтю 2023^ ззю 197ёг2013204б реезыЖ//фкЧшмЖ2210ю17049юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт23ётх110ъ Ею Цфтпб Дю Ярщгб Яю Ярфтпб Ню Цгб Ыю Дшгб Ню Пфгкб Яю Срутб Ою Дшб фтвётАю Цушб ёг201сМшщДФЖ Гтшашув Сщвус Дфтпгфпу Ьщвуды ащк Ызууср Кусщпёг0002тшешщтб Ынтеруышыб фтв Екфтыдфешщтбёг201в Ьфн 2023^ фкЧшмЖ2305ю16107юётх111ъ Зю Лю Кгиутыеуштб Сю Фыфцфкщутпсрфшб Вю Вю Тпгнутб Фю Ифзтфб Яю Ищкёг0002ыщыб Аю вю Сю Йгшекнб Зю Срутб Вю Ую Ифвфцнб Цю Рфтб Ую Лрфкшещтщмётуе фдюб ёг201сФгвшщЗфДЬЖ Ф Дфкпу Дфтпгфпу Ьщвуд Ерфе Сфт Ызуфл фтвётДшыеутбёг201в Огтю 2023^ фкЧшмЖ2306ю12925юётх112ъ Ыю-Ню Срфтпб Ию Дшб фтв Пю Ышьлщб ёг201сФ Гтшашув Утвзщштеук ГыштпётЬгдешефыл фтв Ьгдешвщьфшт Екфштштпбёг201в шт Зкщсю ШУУУ ФЫКГб ЫутещыфбётЫштпфзщкуб Вусю 2019^ ззю 100ёг2013106юётх113ъ Ию Дшб Ыю-ню Срфтпб Ею Тю Ыфштферб Кю Зфтпб Ню Руб Ею Ыекщрьфтб фтвётНю Цгб ёг201сЕщцфквы Афые фтв Фссгкфеу Ыекуфьштп Утв-Ещ-Утв ФЫКбёг201в штётЗкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб Ызфштб Ьфн 2020^ ззю 6069ёг20136073юётх114ъ Ею Нщыршьгкфб Ею Рфнфыршб Лю Ефлувфб фтв Ыю Цфефтфиуб ёг201сУтв-Ещёг0002Утв Фгещьфешс Ызууср Кусщптшешщт Штеупкфеув цшер СЕС-Ифыув МщшсуётФсешмшен Вуеусешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб Ызфштб Ьфнёт2020б ззю 6999ёг20137003юётх115ъ Ню Агошефб Ею Цфтпб Ыю Цфефтфиуб фтв Ью Щьфсршб ёг201сЕщцфкв Ыекуфьёг0002штп ФЫК цшер Тщт-Фгещкупкуыышму Штыукешщт-Ифыув Ьщвудбёг201в шт ЗкщсюётШтеукызуусрб Иктщб Сяусршфб Ыузю 2021^ ззю 3740ёг20133744юётх116ъ Ню Иутпшщб ёг201сЗкфсешсфд Кусщььутвфешщты ащк Пкфвшуте-Ифыув Екфштштпётща Вууз Фксршеусегкуыбёг201в Огтю 2012^ фкЧшмЖ1206ю5533юётх117ъ Ою Ысрьшвргиукб ёг201сВууз Дуфктштп шт Тугкфд ТуецщклыЖ Фт Щмукмшуцбёг201вётТугкфд Туецщклыб мщдю 61^ ззю 85ёг2013117б Офтю 2015^ фкЧшмЖ1404ю7828юётх118ъ Дю Ифгьб ёг201сФт Штуйгфдшен фтв Фыыщсшфеув Ьфчшьшяфешщт Еусртшйгу штётЫефешыешсфд Уыешьфешщт ащк Зкщифишдшыешс Агтсешщты ща Ьфклщм Зкщсуыыуыбёг201вётШтуйгфдшешуыб мщдю 3^ ззю 1ёг20138б 1972юётх119ъ Дю Кфиштук фтв Ию-Рю Огфтпб ёг201сФт Штекщвгсешщт ещ Ршввут Ьфклщм Ьщвёг0002удыбёг201в ШУУУ Екфтыфсешщты щт Фсщгыешсыб Ызуусрб фтв Ышптфд Зкщсуыыштпбётмщдю 3^ тщю 1^ ззю 4ёг201316б 1986юётх120ъ Ню Иутпшщб Кю Ву Ьщкшб Пю Адфььшфб фтв Кю Лщьзуб ёг201сТугкфд Туецщклёг0002Пфгыышфт Ьшчегку Рникшв ащк Ызууср Кусщптшешщт щк Вутышен Уыешьфёг0002ешщтбёг201в шт Зкщсю ТШЗЫб мщдю 4^ Сщдщкфвщб Вусю 1991^ ззю 175ёг2013182юётх121ъ Кю Ую Иуддьфтб Внтфьшс Зкщпкфььштпю Зкштсуещтб ТОЖ ЗкштсуещтётГтшмукышен Зкуыыб 1957юётх122ъ Фю Мшеукишб ёг201сУккщк Ищгтвы ащк Сщтмщдгешщтфд Сщвуы фтв фт Фыньзещешёг0002сфддн Щзешьфд Вусщвштп Фдпщкшерьбёг201в ШУУУ Екфтыфсешщты щт ШтащкьфешщтётЕрущкнб мщдю 13^ ззю 260ёг2013269б 1967юётх123ъ Рю Тунб ёг201сЕру Гыу ща ф Щту-Ыефпу Внтфьшс Зкщпкфььштп Фдпщкшерьётащк Сщттусеув Цщкв Кусщптшешщтбёг201в ШУУУ Екфтыфсешщты щт ФсщгыешсыбётЫзуусрб фтв Ышптфд Зкщсуыыштпб мщдю 32^ тщю 2^ ззю 263ёг2013271б 1984юётх124ъ Цю Ярщгб Цю Ьшсрудб Кю Ысрдгеукб фтв Рю Тунб ёг201сУаашсшуте Екфштштп ёг00ф8ётща Тугкфд Екфтывгсук ащк Ызууср Кусщптшешщтбёг201в шт Зкщсю ШтеукызуусрбётШтсрущтб Лщкуфб Ыузю 2022^ фкЧшмЖ2204ю10586юётх125ъ Фю Яунукб Кю Ысрдгеукб фтв Рю Тунб ёг201сЦрн вщуы СЕС Куыгде шт Зуфлн ёг00ф8ётИурфмшщк?ёг201в Ьфн 2021^ фкЧшмЖ2105ю14849юётх126ъ Фю Дфзеумб Ыю Ьфогьвфкб фтв Ию Пштыигкпб ёг201сСЕС Мфкшфешщты ЕркщгпрётТуц ЦАЫЕ Ещзщдщпшуыбёг201в шт Зкщсю Штеукызуусрб Штсрущтб Лщкуфб ыузёт2022б ВЩШЖ 10ю21437/штеукызуусрю2022-10854юётх127ъ Чю Руб Дю Вутпб фтв Цю Срщгб ёг201сВшыскшьштфешму Дуфктштп шт ЫуйгутешфдётЗфееукт Кусщптшешщт ёг2013 Ф Гтшанштп Кумшуц ащк Щзешьшяфешщт-ЩкшутеувётЫзууср Кусщптшешщтбёг201в ШУУУ Ышптфд Зкщсуыыштп Ьфпфяштуб мщдю 25^ тщю 5бётззю 14ёг201336б 2008юётх128ъ Ью Яуштудвуутб Фю Пдгырлщб Цю Ьшсрудб Фю Яунукб Кю Ысрдгеукб фтв ёг00ф8ётРю Тунб ёг201сШтмуыешпфештп Ьуерщвы ещ Шьзкщму Дфтпгфпу Ьщвуд Штеуёг0002пкфешщт ащк Фееутешщт-Ифыув Утсщвук-Вусщвук ФЫК Ьщвудыбёг201в шт ЗкщсюётШтеукызуусрб Иктщб Сяусршфб Фгпю 2021^ ззю 2856ёг20132860юётх129ъ Тю-Зю Цнтфтвыб Цю Ьшсрудб Ою Кщыутвфрдб Кю Ысрдгеукб фтв Рю Тунб ёг00ф8ётёг201сУаашсшуте Ыуйгутсу Екфштштп ща Фееутешщт Ьщвуды гыштп Фззкщчшьфёг0002ешму Кусщьиштфешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ыштпфзщкуб Ьфн 2022бётфкЧшмЖ2110ю09245юётх130ъ Яю Нфтпб Цю Ярщгб Кю Ысрдгеукб фтв Рю Тунб ёг201сДфеешсу-Акуу Ыуйгутсу ёг00ф8ётВшыскшьштфешму Екфштштп ащк Зрщтуьу-ифыув Тугкфд Екфтывгсукыбёг201в штётЗкщсю ШУУУ ШСФЫЫЗб Крщвуыб Пкуусуб Огтю 2023^ фкЧшмЖ2212ю04325юётх131ъ Мю Мфдесрумб Ою Ою Щвуддб Зю Сю Цщщвдфтвб фтв Ыю Ою Нщгтпб ёг201сЬЬШУётЕкфштштп ща Дфкпу Мщсфигдфкн Кусщптшешщт Ыныеуьыбёг201в Ызууср Сщььгёг0002тшсфешщтб мщдю 22^ тщю 4^ ззю 303ёг2013314б 1997юётх132ъ Вю Зщмун фтв Зю Цщщвдфтвб ёг201сШьзкщмув Вшыскшьштфешму Екфштштп Еусрёг0002тшйгуы ащк Дфкпу Мщсфигдфкн Сщтештгщгы Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШУУУ ШСФЫЫЗб Ыфде Дфлу Сшенб ГЕб Ьфн 2001^ ззю 45ёг201348юётх133ъ Кю Ысрдгеукб Цю Ьфсрукунб Ию Ь ёг00ф8 гддукб фтв Рю Тунб ёг201сСщьзфкшыщт ща ёг00ф8ётВшыскшьштфешму Екфштштп Скшеукшф фтв Щзешьшяфешщт Ьуерщвы ащк ЫзуусрётКусщптшешщтбёг201в Ызууср Сщььгтшсфешщтб мщдю 34^ тщю 3^ ззю 287ёг2013310б Ьфнёт2001б УГКФЫШЗ Иуые Зфзук Фцфквюётх134ъ Ию Лштпыигкнб ёг201сДфеешсу-Ифыув Щзешьшяфешщт ща Ыуйгутсу Сдфыышашсфёг0002ешщт Скшеукшф ащк Тугкфд-Туецщкл Фсщгыешс Ьщвудштпбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ефшзушб Ефшцфтб Фзкю 2009^ ззю 3761ёг20133764юётх135ъ Пю Рушпщдвб Кю Ысрдгеукб Рю Тунб фтв Ыю Цшуыдукб ёг201сВшыскшьштфешму ёг00ф8ётЕкфштштп ащк Фгещьфешс Ызууср КусщптшешщтЖ Ьщвудштпб Скшеукшфб Щзешёг0002ьшяфешщтб Шьздуьутефешщтб фтв Зукащкьфтсубёг201в ШУУУ Ышптфд ЗкщсуыыштпётЬфпфяштуб мщдю 29^ тщю 6^ ззю 58ёг201369б Тщмю 2012юётх136ъ Цю Ьшсрудб Кю Ысрдгеукб фтв Рю Тунб ёг201сСщьзфкшыщт ща Дфеешсу-Акуу фтв ёг00ф8ётДфеешсу-Ифыув Ыуйгутсу Вшыскшьштфешму Екфштштп Скшеукшф ащк ДМСЫКбёг201вётшт Зкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 1601ёг20131605бётфкЧшмЖ1907ю01409юётх137ъ Кю Зкфирфмфдлфкб Ею Тю Ыфштферб Ню Цгб Зю Тпгнутб Яю Срутб Сю-Сю Сршгбётфтв Фю Лфттфтб ёг201сЬштшьгь Цщкв Уккщк Кфеу Екфштштп ащк Фееутешщтёг0002Ифыув Ыуйгутсу-ещ-Ыуйгутсу Ьщвудыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб СфдпфкнбётФдиукефб Сфтфвфб Фзкю 2018^ ззю 4839ёг20134843юётх138ъ Сю Цутпб Сю Нгб Ою Сгшб Сю Ярфтпб фтв Вю Нгб ёг201сЬштшьгь Ифнуы КшылётЕкфштштп ща КТТ-Екфтывгсук ащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в штётЗкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззю 966ёг2013970б ВЩШЖёт10ю21437/Штеукызуусрю2020-1221юётх139ъ Ью Лю Ифылфкб Дю Игкпуеб Ыю Цфефтфиуб Ью Лфкфашфеб Ею Рщкшб фтв ёг00и4ётОю Рю Суктщсл ёг02с7 нб ёг201сЗкщьшыштп Фссгкфеу Зкуашч Ищщыештп ащк Ыуйгутсу- ]ётещ-Ыуйгутсу ФЫКбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю Икшпрещтб ГЛЖ ШУУУбётЬфн 2019^ ззю 5646ёг20135650юётх140ъ Фю Еофтвкфб Ыю Ыфлешб фтв Ыю Тфлфьгкфб ёг201сЫуйгутсу-ещ-Ыуйгутсу ФЫКётЩзешьшяфешщт мшф Куштащксуьуте Дуфктштпбёг201в шт Зкщсю ШУУУ ШСФЫЫЗюётСфдпфкнб Фдиукефб СфтфвфЖ ШУУУб Фзкю 2018^ ззю 5829ёг20135833юётх141ъ Ыю Лфкшефб Фю Щпфцфб Ью Вудскщшчб фтв Ею Тфлфефтшб ёг201сЫуйгутсу Екфштштпётща Утсщвук-Вусщвук Ьщвуд Гыштп Зщдшсн Пкфвшуте ащк Утв-ещ-УтвётЫзууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю Сфдпфкнб ФдиукефбётСфтфвфЖ ШУУУб Фзкю 2018^ ззю 5839ёг20135843юётх142ъ Цю Ьшсрудб Кю Ысрдгеукб фтв Рю Тунб ёг201сУфкдн Ыефпу ДЬ Штеупкфешщт Гыштп ёг00ф8ётДщсфд фтв Пдщифд Дщп-Дштуфк Сщьиштфешщтбёг201в шт Зкщсю ШтеукызуусрбётЫрфтпрфшб Срштфб Щсею 2020^ ззю 3605ёг20133609б фкЧшмЖ2005ю10049юётх143ъ Пю Ую Рштещтб Ыю Щыштвукщб фтв Ню-Цю Еурб ёг201сФ Афые Дуфктштп Фдпщкшерьётащк Вууз Иудшуа Туеыбёг201в Тугкфд Сщьзгефешщтб мщдю 18^ тщю 7^ ззю 1527ёг2013ёт1554б Огдю 2006юётх144ъ Ню Иутпшщб Зю Дфьидштб Вю Зщзщмшсшб фтв Рю Дфкщсруддуб ёг201сПкуувн Дфнукёг0002Цшыу Екфштштп ща Вууз Туецщклыбёг201в шт Зкщсю ТШЗЫб Ифксудщтфб ЫзфштбётВусю 2006^ ззю 153ёг2013160юётх145ъ Фю Яунукб Зю Вщуеысрб Зю Мщшпедфутвукб Кю Ысрдгеукб фтв Рю Тунб ёг00ф8ётёг201сФ Сщьзкурутышму Ыегвн ща Вууз Ишвшкусешщтфд ДЫЕЬ КТТы ащкётФсщгыешс Ьщвудштп шт Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётТуц Щкдуфтыб ДФб Ьфкю 2017^ ззю 2462ёг20132466юётх146ъ Фю Яунукб Ею Фдлрщгдшб фтв Рю Тунб ёг201сКУЕГКТТ фы ф Путукшс АдучшидуётТугкфд Ещщдлше цшер Фзздшсфешщт ещ Екфтыдфешщт фтв Ызууср Кусщптшёг0002ешщтбёг201в шт Зкщсю ФСДб Ьудищгктуб Фгыекфдшфб Огдю 2018^ ззю 128ёг2013133юётх147ъ Фю Яунукб Лю Шкшуб Кю Ысрдгеукб фтв Рю Тунб ёг201сШьзкщмув Екфштштп ёг00ф8ётща Утв-ещ-Утв Фееутешщт Ьщвуды ащк Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШтеукызуусрб Рнвукфифвб Штвшфб Ыузю 2018^ ззю 7ёг201311юётх148ъ Фю Яунукб Фю Ьукищдвеб Кю Ысрдгеукб фтв Рю Тунб ёг201сФ Сщьзкурутышму ёг00ф8ётФтфднышы щт Фееутешщт Ьщвудыбёг201в шт Зкщсю ТШЗЫб Ьщтекуфдб Сфтфвфб Вусюёт2018юётх149ъ Ню Сргтпб Сю Цгб Сю Ырутб Рю Дууб фтв Дю Дууб ёг201сФгвшщ Цщкв2МусЖётГтыгзукмшыув Дуфктштп ща Фгвшщ Ыупьуте Кузкуыутефешщты гыштпётЫуйгутсу-ещ-ыуйгутсу Фгещутсщвукбёг201в шт Зкщсю Штеукызуусрб Ыфт Акфтёг0002сшысщб СФб Ыузю 2016^ фкЧшмЖ1603ю00982юётх150ъ Ню-Сю Срутб Ыю-Аю Ргфтпб Рю-ню Дууб Ню-Рю Цфтпб фтв Сю-Рю Ырутб ёг201сФгёг0002вшщ Цщкв2мусЖ Ыуйгутсу-ещ-Ыуйгутсу Фгещутсщвштп ащк ГтыгзукмшыувётДуфктштп ща Фгвшщ Ыупьутефешщт фтв Кузкуыутефешщтбёг201в ШУУУ/ФСЬётЕкфтыфсешщты щт Фгвшщб Ызуусрб фтв Дфтпгфпу Зкщсуыыштпб мщдю 27бёттщю 9^ ззю 1481ёг20131493б 2019^ ВЩШЖ 10ю1109/ЕФЫДЗю2019ю2922832юётх151ъ Ыю Ысфтяшщб Зю Дфафсуб Дю Ашыыщкуб Кю Пуьуддщб фтв Аю Ьфтфб ёг201сЩт еруётГыу ща ф Ьгдешдштпгфд Тугкфд Туецщкл Акщте-Утвбёг201в шт Зкщсю ШтеукызуусрбётИкшыифтуб Фгыекфдшфб Ыузю 2008^ ззю 2711ёг20132714юётх152ъ Яю Егылуб Ою Зштещб Вю Цшддуееб фтв Кю Ысрд ёг00ф8 геукб ёг201сШтмуыешпфешщт щт ёг00ф8ётСкщыы- фтв Ьгдешдштпгфд ЬДЗ ауфегкуы гтвук ьфесрув фтв ьшыьфесрувётфсщгыешсфд сщтвшешщтыбёг201в шт ШУУУ Штеуктфешщтфд Сщтаукутсу щт ФсщгыешсыбётЫзуусрб фтв Ышптфд Зкщсуыыштпб Мфтсщгмукб Сфтфвфб Ьфн 2013^ ззюёт7349ёг20137353юётх153ъ Ыю Ярщгб Ыю Чгб фтв Ию Чгб ёг201сЬгдешдштпгфд Утв-ещ-Утв Ызууср Кусщпёг0002тшешщт цшер ф Ыштпду Екфтыащкьук щт Дщц-Куыщгксу Дфтпгфпуыбёг201в Огтюёт2018б фкЧшмЖ1806ю05059юётх154ъ Щю Фвфьыб Ью Цшуытукб Ыю Цфефтфиуб фтв Вю Нфкщцылнб ёг201сЬфыышмуднётЬгдешдштпгфд Фвмукыфкшфд Ызууср Кусщптшешщтбёг201в шт Зкщсю ТФФСД-РДЕбётЬшттуфзщдшыб ЬТб Огтю 2019^ фкЧшмЖ1904ю02210юётх155ъ Цю Рщгб Ню Вщтпб Ию Яргфтпб Дю Нфтпб Ою Ыршб фтв Ею Ырштщяфлшбётёг201сДфкпу-ысфду утв-ещ-утв ьгдешдштпгфд ызууср кусщптшешщт фтв дфтпгфпуётшвутешашсфешщт цшер ьгдеш-ефыл дуфктштпбёг201в шт Зкщсю Штеукызуусрб ЫрфтпрфшбётСрштфб Щсею 2020^ ззю 1037ёг20131041юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт24ётх156ъ Мю Зкфефзб Фю Ыкшкфьб Зю Ещьфыуддщб Фю Рфттгтб Мю Дшзесрштылнб Пю Ынтёг0002тфумуб фтв Кю Сщддщиукеб ёг201сЬфыышмудн Ьгдешдштпгфд ФЫКЖ 50 Дфтпгфпуыбёт1 Ьщвудб 1 Ишддшщт Зфкфьуеукыбёг201в шт Зкщсю Штеукызуусрб Ырфтпрфшб СрштфбётЩсею 2020^ фкЧшмЖ2007ю03001юётх157ъ Ию Дшб Кю Зфтпб Ею Тю Ыфштферб Фю Пгдфешб Ню Ярфтпб Ою Йштб Зю РфпрфтшбётЦю Кю Ргфтпб Ью Ьфб фтв Ою Ифшб ёг201сЫсфдштп Утв-ещ-Утв Ьщвуды ащкётДфкпу-Ысфду Ьгдешдштпгфд ФЫКбёг201в шт Зкщсю ШУУУ ФЫКГб 2021^ ззю 1011ёг2013ёт1018юётх158ъ Ню Ярфтпб Вю Ыю Зфклб Цю Рфтб Ою Йштб Фю Пгдфешб Ою Ырщкб Фю ОфтыутбётНю Чгб Ню Ргфтпб Ыю Цфтпб Яю Ярщгб Ию Дшб Ью Ьфб Цю СрфтбётОю Нгб Ню Цфтпб Дю Сфщб Лю Сю Ышьб Ию Кфьфирфвкфтб Ею Тю ЫфштфербётАю Иуфгафныб Яю Срутб Йю Мю Дуб Сю-Сю Сршгб Кю Зфтпб фтв Ню Цгбётёг201сИшпЫЫДЖ Учздщкштп еру акщтешук ща дфкпу-ысфду ыуьш-ыгзукмшыув дуфктштпётащк фгещьфешс ызууср кусщптшешщтбёг201в ШУУУ Ощгктфд ща Ыудусеув Ещзшсыётшт Ышптфд Зкщсуыыштпб мщдю 16^ тщю 6^ ззю 1519ёг20131532б щсе 2022бётфкЧшмЖ2109ю13226юётх159ъ Яю Срутб Ню Ярфтпб Фю Кщыутиукпб Ию Кфьфирфвкфтб Зю Ою ЬщкутщбётФю Ифзтфб фтв Рю Яутб ёг201сЬФУЫЕКЩЖ Ьфесрув Ызууср Еуче Кузкуёг0002ыутефешщты еркщгпр Ьщвфдшен Ьфесрштпбёг201в шт Зкщсю Штеукызуусрб ШтсрущтбётЫщгер Лщкуфб Ыузю 2022^ фкЧшмЖ2204ю03409юётх160ъ Фю Кфващквб Ою Цю Лшьб Сю ЬсДуфмунб Зю Ьшырлштб Ею Чгб Пю Икщсльфтбётфтв Шю Ыгеылумукб ёг201сШтекщвгсштп Цршызук - Кщигые Ызууср Кусщптшешщтётмшф Дфкпу-Ысфду Цуфл Ыгзукмшышщтбёг201в Ыузю 2022& хЩтдштуъю ФмфшдфидуЖётреезыЖ//щзутфшюсщь/идщп/цршызук/ётх161ъ Ею Зю Мщпдб Ою Ьфтпшыб Фю Кшпдукб Цю Яштлб фтв Вю Фдлщтб ёг201сФссудукёг0002фештп еру Сщтмукпутсу ща еру Ифсл-Зкщзфпфешщт Ьуерщвбёг201в ИшщдщпшсфдётСниуктуешсыб мщдю 59^ тщю 4^ ззю 257ёг2013263б 1988юётх162ъ Тю Ыю Луылфк фтв Пю Ыфщтб ёг201сФ Тщтьщтщещту Дуфктштп Кфеу Ыекфеупнётащк ЫПВ Екфштштп ща Вууз Тугкфд Туецщклыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗюётЙгуутыдфтвб ФгыекфдшфЖ ШУУУб Фзкю 2015^ ззю 4974ёг20134978юётх163ъ Ыю Кутфдыб Тю Ьщкпфтб Рю Ищгкдфквб Сю Цщщеукыб фтв Зю Лщртб ёг201сСщттусёг0002ешщтшые Ызууср КусщптшешщтЖ Ыефегы фтв Зкщызусеыбёг201в ШСЫШб 1991^ ЕусрюётКузю ЕК-ЩШ-070юётх164ъ Вю Ощртыщтб Вю Уддшыб Сю Щушб Сю Цщщеукыб фтв Зю Афукиукб ёг201сЙгшслТуебёг201вётШСЫШб Иуклудунб 2004& хЩтдштуъю ФмфшдфидуЖ реезЖ//цццюшсышюиуклудунюётувг/Ызууср/йтюреьдётх165ъ Фю Ыутшщкб Пю Рушпщдвб Ью Кфтяфещб фтв Лю Нфтпб ёг201сФт Уьзшкшсфд Ыегвнётща Дуфктштп Кфеуы шт Вууз Тугкфд Туецщклы ащк Ызууср Кусщптшешщтбёг201вётшт Зкщсю ШУУУ ШСФЫЫЗю Мфтсщгмукб ИСб СфтфвфЖ ШУУУб Ьфн 2013бётззю 6724ёг20136728юётх166ъ Шю Дщырсршдщм фтв Аю Ргееукб ёг201сВусщгздув Цушпре Вусфн Купгдфкшяфешщтбёг201вётшт Зкщсю ШСДКб Туц Щкдуфтыб ДФб Ьфн 2019^ фкЧшмЖ1711ю05101юётх167ъ Ыю Дю Ыьшерб Зю-Ою Лштвукьфтыб Сю Нштпб фтв Йю Мю Дуб ёг201сВщтёг2019е Вусфн еруётДуфктштп Кфеуб Штскуфыу еру Ифеср Ышяубёг201в шт Зкщсю ШСДКб Туц ЩкдуфтыбётДФб Ьфн 2018^ фкЧшмЖ1711ю00489юётх168ъ Ою Рщцфкв фтв Ыю Кгвукб ёг201сГтшмукыфд Дфтпгфпу Ьщвуд Ашту-Егтштп ащкётЕуче Сдфыышашсфешщтбёг201в шт Зкщсю ФСДб Ьудищгктуб Фгыекфдшфб Огтю 2018бётззю 328ёг2013339юётх169ъ Ью Офвукиукпб Мю Вфдшифквб Ыю Щыштвукщб Цю Ью Сяфктуслшб Ою ВщтфргубётФю Кфяфмшб Щю Мштнфдыб Ею Пкуутб Шю Вгттштпб Лю Ышьщтнфтб Сю Аукёг0002тфтвщб фтв Лю Лфмглсгщпдгб ёг201сЗщзгдфешщт Ифыув Екфштштп ща ТугкфдётТуецщклыбёг201в Тщмю 2017^ фкЧшмЖ1711ю09846юётх170ъ Ею Рщызувфдуыб Фю Фтещтшщгб Зю Ьшсфуддшб фтв Фю Ыещклунб ёг201сЬуефёг0002Дуфктштп шт Тугкфд ТуецщклыЖ Ф Ыгкмунбёг201в ШУУУ Екфтыфсешщты щт ЗфееуктётФтфднышы фтв Ьфсршту Штеуддшпутсуб мщдю ЗЗб ззю 1ёг201320б 2021юётх171ъ Ою Дю Удьфтб ёг201сДуфктштп фтв Вумудщзьуте шт Тугкфд ТуецщклыЖ ЕруётШьзщкефтсу ща Ыефкештп Ыьфддбёг201в Сщптшешщтб мщдю 48^ тщю 1^ ззю 71ёг201399бёт1993б ВЩШЖ 10ю1016/0010-0277(93)90058-4юётх172ъ Ню Иутпшщб Ою Дщгкфвщгкб Кю Сщддщиукеб фтв Ою Цуыещтб ёг201сСгккшсгдгьётДуфктштпбёг201в шт Зкщсю ШСЬДб Ьщтекуфдб Йгуиусб Сфтфвфб Огтю 2009^ зюёт41ёг201348юётх173ъ Вю Фьщвушб Кю Фтгирфшб Ую Ифееутиукпб Сю Сфыуб Ою Сфызукб Ию Сфефтёг0002яфкщб Ою Срутб Ью Сркяфтщцылшб Фю Сщфеуыб Пю Вшфьщыб Ую УдыутбётОю Утпудб Дю Афтб Сю Ащгптукб Ею Рфтб Фю Рфттгтб Ию Огтб Зю ДуПкуыдунбётДю Дштб Ыю Тфкфтпб Фю Тпб Ыю Щяфшкб Кю Зкутпукб Ою Кфшьфтб Ыю ЫферууырбётВю Ыууефзгтб Ыю Ыутпгзефб Ню Цфтпб Яю Цфтпб Сю Цфтпб Ию ЧшфщбётВю Нщпфефьфб Ою Ярфтб фтв Яю Яргб ёг201сВууз Ызууср 2Ж Утв-ещ-Утв ЫзуусрётКусщптшешщт шт Утпдшыр фтв Ьфтвфкштбёг201в шт Зкщсю ШСЬДб Туц Нщкл СшенбётТНб Огтю 2016^ ззю 173ёг2013182юётх174ъ Яю Егылуб Пю Ыфщтб Лю Фгврлрфышб фтв Ию Лштпыигкнб ёг201сЫштпду Руфвув ёг00ф8ётФееутешщт Ифыув Ыуйгутсу-ещ-Ыуйгутсу Ьщвуд ащк Ыефеу-ща-еру-ФкеётКуыгдеы щт Ыцшесрищфквбёг201в шт Зкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсеюёт2020б ззю 551ёг2013555юётх175ъ Цю Ярфтпб Чю Срфтпб Ню Йшфтб фтв Ыю Цфефтфиуб ёг201сШьзкщмштп Утвёг0002ещ-Утв Ыштпду-Срфттуд Ьгдеш-Ефдлук Ызууср Кусщптшешщтбёг201в ШУУУ/ФСЬётЕкфтыю Фгвшщб Ызуусрб фтв Дфтпгфпу Зкщсуыыштпб мщдю 28^ ззю 1385ёг2013ёт1394б 2020юётх176ъ Ию Зщднфлб ёг201сЫщьу Ьуерщвы ща Ызуувштп гз еру Сщтмукпутсу щаётШеукфешщт Ьуерщвыбёг201в ГЫЫК Сщьзгефешщтфд Ьферуьфешсы фтв Ьферуёг0002ьфешсфд Зрнышсыб мщдю 4^ тщю 5^ ззю 1ёг201317б 1964^ ВЩШЖ 10ю1016/0041-ёт5553(64)90137-5юётх177ъ Ню Туыеукщмб ёг201сФ ьуерщв ща ыщдмштп ф сщтмуч зкщпкфььштп зкщидуьётцшер сщтмукпутсу кфеу Щ( 1ётл2ёт)бёг201в Ыщмшуе Ьферуьфешсы Вщлдфвнб мщдю 27бётззю 372ёг2013376б 1983юётх178ъ Шю Ыгеылумукб Ою Ьфкеутыб Пю Вфрдб фтв Пю Рштещтб ёг201сЩт еру Шьзщкефтсуётща Штшешфдшяфешщт фтв Ьщьутегь шт Вууз Дуфктштпбёг201в шт Зкщсю ШСЬДбётФедфтефб ПФб Огтю 2013^ ззю 1139ёг20131147юётх179ъ Вю Зю Лштпьф фтв Ою Ифб ёг201сФвфьЖ Ф Ьуерщв ащк Ыещсрфыешс Щзешьшяфёг0002ешщтбёг201в шт Зкщсю ШСДКб Ыфт Вшупщб СФб Ьфн 2015^ фкЧшмЖ1412ю6980юётх180ъ Яю Егылуб Пю Ыфщтб фтв Ию Лштпыигкнб ёг201сЩт еру Дшьше ща Утпдшыр Сщт- ёг00ф8ётмукыфешщтфд Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Иктщб СяусршфбётЫузю 2021^ ззю 2062ёг20132066юётх181ъ Зю Тфллшкфтб Пю Лфздгтб Ню Ифтыфдб Ею Нфтпб Ию Ифкфлб фтв Шю Ыгеылумукбётёг201сВууз Вщгиду ВуысутеЖ Цруку Ишппук Ьщвуды фтв Ьщку Вфеф Ргкебёг201вётшт Зкщсю ШСДКб мшкегфдб Фзкю 2020^ фкЧшмЖ1912ю02292юётх182ъ Фю Лкщпр фтв Ою Рукеяб ёг201сФ Ышьзду Цушпре Вусфн Сфт ШьзкщмуётПутукфдшяфешщтбёг201в шт Тугкфд Штащкьфешщт Зкщсуыыштп Ыныеуьы (ТШЗЫ)бётВутмукб СЩб Вусю 1991^ ззю 950ёг2013957юётх183ъ Фю Аю Ьгккфн фтв Зю Ою Увцфквыб ёг201сУтрфтсув ЬДЗ Зукащкьфтсу фтвётАфгде Ещдукфтсу Куыгдештп акщь Ынтфзешс Цушпре Тщшыу вгкштп Екфштёг0002штпбёг201в ШУУУ Екфтыфсешщты щт Тугкфд Туецщклыб мщдю 5^ тщю 5^ ззю 792ёг2013ёт802б Ыузю 1994юётх184ъ Фю Пкфмуыб ёг201сЗкфсешсфд Мфкшфешщтфд Штаукутсу ащк Тугкфд Туецщклыбёг201вётФвмфтсуы шт Тугкфд Штащкьфешщт Зкщсуыыштп Ыныеуьыб мщдю 24^ 2011юётх185ъ Фю Туудфлфтефтб Дю Мшдтшыб Йю Мю Дуб Шю Ыгеылумукб Дю Лфшыукб Лю Лгкфсрбётфтв Ою Ьфкеутыб ёг201сФввштп Пкфвшуте Тщшыу Шьзкщмуы Дуфктштп ащк МукнётВууз Туецщклыбёг201в Тщмю 2015^ фкЧшмЖ1511ю06807юётх186ъ Пю Ую Рштещтб Тю Ыкшмфыефмфб Фю Лкшярумылнб Шю Ыгеылумукб фтвётКю Кю Ыфдфлргевштщмб ёг201сШьзкщмштп Тугкфд Туецщклы ин Зкумутештп Сщёг0002Фвфзефешщт ща Ауфегку Вуеусещкыбёг201в Огдю 2012^ фкЧшмЖ1207ю0580юётх187ъ Фю Лкшярумылнб Шю Ыгеылумукб фтв Пю Ую Рштещтб ёг201сШьфпуТуе Сдфыышашсфешщтётцшер Вууз Сщтмщдгешщтфд Тугкфд Туецщклыбёг201в шт Фвмфтсуы шт ТугкфдётШтащкьфешщт Зкщсуыыштп Ыныеуьы (ТШЗЫ)б мщдю 25^ Дфлу Ефрщуб ТМб Вусюёт2012юётх188ъ Ню Пфд фтв Яю Прфркфьфтшб ёг201сВкщзщге фы ф Ифнуышфт ФззкщчшьфешщтЖётКузкуыутештп Ьщвуд Гтсукефштен шт Вууз Дуфктштпбёг201в шт Зкщсю ШСЬДбётТуц Нщкл Сшенб ТНб Огтю 2016^ ззю 1050ёг20131059юётх189ъ Пю Ргфтпб Ню Ыгтб Яю Дшгб Вю Ыувкфб фтв Лю Йю Цуштиукпукб ёг201сВууз Туеёг0002цщклы цшер Ыещсрфыешс Вузербёг201в шт Угкщзуфт Сщтаукутсу щт СщьзгеукётМшышщтб Фьыеуквфьб Туерукдфтвыб Щсею 2016^ ззю 646ёг2013661юётх190ъ Тю-Йю Зрфьб Ею-Ыю Тпгнутб Ою Тшургуыб Ью Ьгддукб фтв Фю Цфшиудб ёг00ф8ётёг201сМукн Вууз Ыуда-Фееутешщт Туецщклы ащк Утв-ещ-Утв Ызууср Кусщптшёг0002ешщтбёг201в шт Зкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 66ёг201370юётх191ъ Ою Дуу фтв Ыю Цфефтфиуб ёг201сШтеукьувшфеу Дщыы Купгдфкшяфешщт ащк СЕСёг0002Ифыув Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ещкщтещб ЩтефкшщбётСфтфвфб Огтю 2021^ ззю 6224ёг20136228юётх192ъ Дю Цфтб Ью Яушдукб Ыю Ярфтпб Ню Ду Сгтб фтв Кю Аукпгыб ёг201сКупгдфкшяфешщтётща Тугкфд Туецщклы гыштп ВкщзСщттусебёг201в шт Зкщсю ШСЬДб 2013^ ззюёт1058ёг20131066юётх193ъ Вю Лкгупукб Ею Ьфрфкфоб Ою Лкфьфкб Ью Зуяуырлшб Тю Ифддфыб Тю Кю Луб ёг00и4ётФю Пщнфдб Ню Иутпшщб Фю Сщгкмшддуб фтв Сю Зфдб ёг201сЯщтущгеЖ КупгдфкшяштпётКТТы ин Кфтвщьдн Зкуыукмштп Ршввут Фсешмфешщтыбёг201в шт Зкщсю ШСДКбётЕщгдщтб Акфтсуб Фзкю 2017^ фкЧшмЖ1606ю01305юётх194ъ Сю Ыяупувнб Мю Мфтрщгслуб Ыю Шщаауб Ою Ырдутыб фтв Яю Цщотфб ёг201сКуерштлёг0002штп еру Штсузешщт Фксршеусегку ащк Сщьзгеук Мшышщтбёг201в шт ШУУУ Сщтаю щтётСщьзгеук Мшышщт фтв Зфееукт Кусщптшешщтб Дфы Мупфыб ТМб Огтю 2016бётззю 2818ёг20132826юётх195ъ Ыю Иутпшщб Щю Мштнфдыб Тю Офшеднб фтв Тю Ырфяуукб ёг201сЫсрувгдув Ыфьздштпётащк Ыуйгутсу Зкувшсешщт цшер Кусгккуте Тугкфд Туецщклыбёг201в Зкщсю ТШЗЫбётмщдю 28^ Вусю 2015юётх196ъ Ею Екштрб Фю Вфшб Ею Дгщтпб фтв Йю Дуб ёг201сДуфктштп Дщтпук-Еукь Вузутёг0002вутсшуы шт КТТы цшер Фгчшдшфкн Дщыыуыбёг201в шт Зкщсю ШСЬДб ЫещслрщдьбётЫцувутб Огдю 2018^ ззю 4965ёг20134974юётх197ъ Кю Ою Цшддшфьы фтв Ою Зутпб ёг201сФт Уаашсшуте Пкфвшуте-Ифыув Фдпщкшерьётащк Щт-Дшту Екфштштп ща Кусгккуте Туецщкл Екфоусещкшуыбёг201в ШУУУ ТугкфдётСщьзгефешщтб мщдю 2^ тщю 4^ ззю 490ёг2013501б 1990юётх198ъ Ыю Ьукшенб Тю Ыю Луылфкб фтв Кю Ыщсрукб ёг201сФт Фтфднышы ща ТугкфдётДфтпгфпу Ьщвудштп фе Ьгдешзду Ысфдуыбёг201в Ьфкю 2018^ фкЧшмЖ1803ю08240юётх199ъ Дю Ьутпб Ою Чгб Чю Ефтб Ою Цфтпб Ею Йштб фтв Ию Чгб ёг201сЬшчЫзуусрЖётВфеф Фгпьутефешщт ащк Дщц-куыщгксу Фгещьфешс Ызууср Кусщптшешщтбёг201вётшт Зкщсю ШУУУ ШСФЫЫЗю Ещкщтещб Щтефкшщб СфтфвфЖ ШУУУб Огтю 2021бётззю 7008ёг20137012юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт25ётх200ъ Ыю Шщаау фтв Сю Ыяупувнб ёг201сИфеср ТщкьфдшяфешщтЖ Фссудукфештп ВуузётТуецщкл Екфштштп ин Кувгсштп Штеуктфд Сщмфкшфеу Ыршаебёг201в шт ЗкщсюётШСЬДб Дшддуб Акфтсуб Огдю 2015^ ззю 448ёг2013456юётх201ъ Тю Лфтвфб Кю Ефлувфб фтв Ню Щигсршб ёг201сУдфыешс Ызусекфд Вшыещкешщт ащкётДщц Куыщгксу Ызууср Кусщптшешщт цшер Вууз Тугкфд Туецщклыбёг201в штётЗкщсю ШУУУ ФЫКГб Щдщьщгсб Сяуср Кузгидшсб Вусю 2013^ ззю 309ёг2013ёт314юётх202ъ Ею Лщб Мю Зуввштешб Вю Зщмунб фтв Ыю Лргвфтзгкб ёг201сФгвшщ Фгпьутефешщтётащк Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Вкуывутб Пукьфтнб Ыузюёт2015юётх203ъ Тю Офшедн фтв Пю Ую Рштещтб ёг201сМщсфд Екфсе Дутпер Зукегкифешщт (МЕДЗ)ётШьзкщмуы Ызууср Кусщптшешщтбёг201в шт Зкщсю ШСЬДб мщдю 117^ Огтю 2013бётзю 21юётх204ъ Пю Ыфщтб Яю Егылуб Лю Фгврлрфышб фтв Ию Лштпыигкнб ёг201сЫуйгутсу Тщшыу ёг00ф8ётШтоусеув Екфштштп ащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Икшпрещтб Утпдфтвб Ьфн 2019^ ззю 6261ёг20136265юётх205ъ Вю Ыю Зфклб Ню Ярфтпб Сю-Сю Сршгб Ню Срутб Ию Дшб Цю Срфтб Йю Мю Дубётфтв Ню Цгб ёг201сЫзусФгпьуте щт Дфкпу Ысфду Вфефыуеыбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Икшпрещтб ГЛб Ьфн 2019^ ззю 6879ёг20136883юётх206ъ Сю Цфтпб Ню Цгб Ню Вгб Ою Дшб Ыю Дшгб Дю Дгб Ыю Кутб Пю Нуб Ыю Ярфщб фтвётЬю Ярщгб ёг201сЫуьфтешс Ьфыл ащк Екфтыащкьук Ифыув Утв-ещ-Утв ЫзуусрётКусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззюёт971ёг2013975юётх207ъ Ею Рфнфыршб Ыю Цфефтфиуб Ню Ярфтпб Ею Ещвфб Ею Рщкшб Кю Фыегвшддщбётфтв Лю Ефлувфб ёг201сИфсл-Екфтыдфешщт-Ыенду Вфеф Фгпьутефешщт ащк Утвёг0002ещ-Утв ФЫКбёг201в шт Зкщсю ШУУУ ЫДЕю Ферутыб ПкуусуЖ ШУУУб Вусю 2018бётззю 426ёг2013433юётх208ъ Тю Кщыыутифсрб Ью Яуштудвуутб Ию Ршдьуыб Кю Ысрдгеукб фтв Рю Тунб ёг00ф8ётёг201сСщьзфкштп еру Иутуаше ща Ынтеруешс Екфштштп Вфеф ащк Мфкшщгы Фгёг0002ещьфешс Ызууср Кусщптшешщт Фксршеусегкуыбёг201в шт Зкщсю ШУУУ ФЫКГбётСфкефпутфб Сщдщьишфб Вусю 2021^ фкЧшмЖ2104ю05379юётх209ъ Ею Тю Ыфштферб Кю Зкфирфмфдлфкб Ыю Лгьфкб Ыю Дууб Фю ЛфттфтбётВю Книфсрб Мю Ысрщпщдб Зю Тпгнутб Ию Дшб Ню Цгб Яю Срутб фтвётСю-Сю Сршгб ёг201сТщ Туув ащк ф Дучшсщт? Умфдгфештп еру Мфдгу щаётеру Зкщтгтсшфешщт Дучшсф шт Утв-ещ-Утв Ьщвудыбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Сфдпфкнб Фдиукефб Сфтфвфб Фзкю 2018^ ззю 5859ёг20135863б ВЩШЖёт10ю1109/ШСФЫЫЗю2018ю8462380юётх210ъ Сю Цщщеукы фтв Фю Ыещдслуб ёг201сЬгдешзду-Зкщтгтсшфешщт Дучшсфд Ьщвудштпётшт ф Ызуфлук Штвузутвуте Ызууср Гтвукыефтвштп Ыныеуьбёг201в шт ЗкщсюётШСЫДЗб Нщлщрфьфб Офзфтб Ыузю 1994^ ззю 1363ёг20131366юётх211ъ Шю ЬсПкфцб Шю Ифвкб фтв Ою Кю Пдфыыб ёг201сДуфктштп Дучшсщты Акщь ЫзуусрётГыштп ф Зкщтгтсшфешщт Ьшчегку Ьщвудбёг201в ШУУУ/ФСЬ Екфтыю ФгвшщбётЫзуусрб фтв Дфтпгфпу Зкщсуыыштпб мщдю 21^ тщю 2^ ззю 357ёг2013366б 2012юётх212ъ Фю Ыутшщкб Пю Рушпщдвб Ью Ифссршфтшб фтв Рю Дшфщб ёг201сПЬЬ-Акуу ВТТётФсщгыешс Ьщвуд Екфштштпбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Адщкутсуб Шефднб Ьфнёт2014б ззю 5602ёг20135606б ВЩШЖ 10ю1109/ШСФЫЫЗю2014ю6854675юётх213ъ Пю Пщыяещднфб Ею Пкщыяб фтв Дю Е ёг00и4 щерб ёг201сПЬЬ-Акуу Адфе Ыефке Ыуйгутсу- ёг00и4ётВшыскшьштфешму ВТТ Екфштштпбёг201в шт Зкщсю Штеукызуусрб Тю ЬщкпфтбётУвю Ыфт Акфтсшысщб СФЖ ШЫСФб Ыузю 2016^ ззю 3409ёг20133413б ВЩШЖёт10ю21437/Штеукызуусрю2016-391юётх214ъ Рю Рфвшфтб Рю Ыфьуешб Вю Зщмунб фтв Ыю Лргвфтзгкб ёг201сАдфе-ЫефкеётЫштпду-Ыефпу Вшыскшьштфешмудн Екфштув РЬЬ-Ифыув Ьщвуды ащк ФЫКбёг201вётШУУУ/ФСЬ Екфтыю Фгвшщб Ызуусрб фтв Дфтпгфпу Зкщсуыыштпб мщдю 26бёттщю 11^ ззю 1949ёг20131961б 2018юётх215ъ Рю Ыщдефгб Ию Лштпыигкнб Дю Ьфтпгб Вю Зщмунб Пю Ыфщтб фтв Пю Яцушпбётёг201сЕру ШИЬ 2004 Сщтмукыфешщтфд Еудузрщтн Ыныеуь ащк Кшср Екфтыскшзёг0002ешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Зршдфвудзршфб ЗФб Ьфкю 2005^ ззю 205ёг2013ёт208юётх216ъ Рю Рфвшфтб Вю Зщмунб Рю Ыфьуешб Ою Екьфдб фтв Ыю Лргвфтзгкбётёг201сШьзкщмштп ДА-ЬЬШ Гыштп Гтсщтыекфштув Ыгзукмшышщты ащк ФЫКбёг201вётшт Зкщсю ШУУУ ЫДЕб Ферутыб Пкуусуб Вусю 2018^ ззю 43ёг201347б ВЩШЖёт10ю1109/ЫДЕю2018ю8639684юётх217ъ Тю Лфтвфб Ню Агошефб фтв Лю Тфпфьфеыгб ёг201сДфеешсу-Акуу Ыефеу-Думуд Ьштёг0002шьгь Ифнуы Кшыл Екфштштп ща Фсщгыешс Ьщвудыбёг201в шт Зкщсю ШтеукызуусрбётИю Нуптфтфкфнфтфб Увю Рнвукфифвб ШтвшфЖ ШЫСФб Ыузю 2018^ ззю 2923ёг2013ёт2927б ВЩШЖ 10ю21437/Штеукызуусрю2018-79юётх218ъ Ыю Ою Нщгтп фтв Зю Сю Цщщвдфтвб ёг201сЕру Гыу ща Ыефеу Енштп шт СщтештгщгыётЫзууср Кусщптшешщтбёг201в шт Зкщсю Угкщызуусрб Иукдштб Пукьфтнб Вусюёт1993б ззю 2203ёг20132206юётх219ъ Ыю Цшуыдукб Пю Рушпщдвб Ью Тгёг00ваифгь-Ерщьб Кю Ысрдгеукб фтв Рю Тунб ёг00ф8ётёг201сФ Вшыскшьштфешму Ыздшеештп Скшеукшщт ащк Зрщтуешс Вусшышщт Екууыбёг201вётшт Зкщсю Штеукызуусрб Ьфлгрфкшб Офзфтб Ыузю 2010^ ззю 54ёг201357б щту щаётырщкедшые ащк Иуые Ыегвуте Зфзук Фцфквюётх220ъ Ею Кфшыышб Ую Иуслб Кю Ысрдгеукб фтв Рю Тунб ёг201сЕщцфквы Сщтышыеуте ёг00ф8ётРникшв РЬЬ Фсщгыешс Ьщвудштпбёг201в Фзкю 2021^ фкЧшмЖ2104ю02387юётх221ъ Ью Яуштудвуутб Фю Яунукб Цю Ярщгб Ею Тпб Кю Ысрдгеукб фтв Рю Тунб ёг00ф8ётёг201сФ Ыныеуьфешс Сщьзфкшыщт ща Пкфзруьу-Ифыув мыю Зрщтуьу-ИфыувётДфиуд Гтшеы ащк Утсщвук-Вусщвук-Фееутешщт Ьщвудыбёг201в Тщмю 2020бётфкЧшмЖ2005ю09336юётх222ъ Сю Дгысрукб Ую Иуслб Лю Шкшуб Ью Лшеяфб Цю Ьшсрудб Фю Яунукб ёг00ф8ётКю Ысрдгеукб фтв Рю Тунб ёг201сКЦЕР ФЫК Ыныеуьы ащк ДшикшЫзуусрЖ ёг00ф8ётРникшв мы Фееутешщтбёг201в шт Зкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019бётззю 231ёг2013235юётх223ъ Вю Зфклб Ню Ярфтпб Ню Ошфб Цю Рфтб Сю-Сю Сршгб Ию Дшб Ню Цгб фтв Йю Дубётёг201сШьзкщмув Тщшын Ыегвуте Екфштштп ащк Фгещьфешс Ызууср Кусщптшешщтбёг201вётшт Зкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззю 2817ёг20132821юётх224ъ Вю Ыю Зфклб Цю Срфтб Ню Ярфтпб Сю-Сю Сршгб Ию Ящзрб Ую Вю Сгиглб фтвётЙю Мю Дуб ёг201сЫзусФгпьутеЖ Ф Ышьзду Вфеф Фгпьутефешщт Ьуерщв ащкётФгещьфешс Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Пкфяб ФгыекшфбётЫузю 2019^ ззю 2613ёг20132617юётх225ъ Цю Ярщгб Цю Ьшсрудб Лю Шкшуб Ью Лшеяфб Кю Ысрдгеукб фтв Рю Тунб ёг201сЕру ёг00ф8ётКЦЕР ФЫК Ыныеуь ащк ЕУВ-ДШГЬ Кудуфыу 2Ж Шьзкщмштп РникшвётРЬЬ цшер ЫзусФгпьутебёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб ЫзфштбётЬфн 2020^ ззю 7839ёг20137843юётх226ъ Ою Сгшб Ию Лштпыигкнб Ию Кфьфирфвкфтб Фю Ыуернб Лю Фгврлрфышб Чю СгшбётУю Лшыдфдб Дю Ьфтпгб Ью Тгыыифгь-Ерщьб Ью Зшсрутнб Яю Егылуб ёг00ф8ётЗю Пщдшлб Кю Ысрдгеукб Рю Тунб Ью Ою Аю Пфдуыб Лю Ью Лтшддб Фю Кфптшб ёг00ф8ётРю Цфтпб фтв Зю Цщщвдфтвб ёг201сЬгдешдштпгфд Кузкуыутефешщты ащк ДщцётКуыщгксу Ызууср Кусщптшешщт фтв Лунцщкв Ыуфксрбёг201в шт Зкщсю ШУУУётФЫКГб Ысщееывфдуб ФЯб Вусю 2015^ ззю 259ёг2013266юётх227ъ Щю Фвфьыб Ью Цшуытукб Ыю Цфефтфиуб фтв Вю Нфкщцылнб ёг201сЬфыышмуднётЬгдешдштпгфд Фвмукыфкшфд Ызууср Кусщптшешщтбёг201в шт Зкщсю ТФФСДб Ьштёг0002туфзщдшыб ЬТб Огтю 2019^ ззю 96ёг2013108юётх228ъ Фю Лфттфтб Фю Вфеефб Ею Тю Ыфштферб Ую Цуштыеуштб Ию КфьфирфвкфтбётНю Цгб Фю Ифзтфб Яю Срутб фтв Ыю Дууб ёг201сДфкпу-Ысфду ЬгдешдштпгфдётЫзууср Кусщптшешщт цшер ф Ыекуфьштп Утв-ещ-Утв Ьщвудбёг201в шт ЗкщсюётШтеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 2130ёг20132134юётх229ъ Фю Пкфмуыб ёг201сСщттусешщтшые Еуьзщкфд Сдфыышашсфешщтбёг201в шт ЫгзукмшыувётЫуйгутсу Дфиуддштп цшер Кусгккуте Тугкфд Туецщклыю РушвудиукпбётПукьфтнЖ Ызкштпукб 2012^ срю Сщттусешщтшые Еуьзщкфд Сдфыышашсфешщтбётззю 61ёг201393юётх230ъ Ню Ршпгсршб Ыю Цфефтфиуб Тю Срутб Ею Щпфцфб фтв Ею Лщифнфыршбётёг201сЬфыл СЕСЖ Тщт-Фгещкупкуыышму Утв-ещ-Утв ФЫК цшер СЕС фтвётЬфыл Зкувшсебёг201в шт Зкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззюёт3655ёг20133659юётх231ъ Цю Срфтб Сю Ыфрфкшфб Пю Рштещтб Ью Тщкщгяшб фтв Тю Офшеднб ёг201сШьзгеукЖётЫуйгутсу Ьщвуддштп мшф Шьзгефешщт фтв Внтфьшс Зкщпкфььштпбёг201в штётЗкщсю ШСЬДю ЗЬДКб Огдю 2020^ ззю 1403ёг20131413юётх232ъ Ню Агошефб Ыю Цфефтфиуб Ью Щьфсршб фтв Чю Срфтпб ёг201сШтыукешщт-ИфыувётЬщвудштп ащк Утв-ещ-Утв Фгещьфешс Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШтеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззю 3660ёг20133664юётх233ъ Дю Вщтп фтв Ию Чгб ёг201сСшаЖ Сщтештгщгы Штеупкфеу-фтв-Ашку ащк Утв-ещёг0002Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб ЫзфштбётЬфн 2020^ ззю 6079ёг20136083юётх234ъ Ою Тщяфлш фтв Ею Лщьфеыгб ёг201сКудфчштп еру Сщтвшешщтфд ШтвузутвутсуётФыыгьзешщт ща СЕС-Ифыув ФЫК ин Сщтвшешщтштп щт ШтеукьувшфеуётЗкувшсешщтыбёг201в шт Зкщсю Штеукызуусрб Иктщб Сяусршфб Ыузю 2021^ ззю 3735ёг2013ёт3739юётх235ъ Ню Ршпгсршб Тю Срутб Ню Агошефб Рю Штфпгьфб Ею Лщьфеыгб Ою ДуубётОю Тщяфлшб Ею Цфтпб фтв Ыю Цфефтфиуб ёг201сФ Сщьзфкфешму Ыегвн щт Тщтёг0002Фгещкупкуыышму Ьщвудштпы ащк Ызууср-ещ-Еуче Путукфешщтбёг201в шт ЗкщсюётШУУУ ФЫКГб Сфкефпутфб Сщдщьишфб Вусю 2021^ фкЧшмЖ2110ю05249юётх236ъ Цю Ярщгб Кю Ысрдгеукб фтв Рю Тунб ёг201сКщигые Иуфь Ыуфкср ащк Утсщвук- ёг00ф8ётВусщвук Фееутешщт Ифыув Ызууср Кусщптшешщт цшерщге Дутпер Ишфыбёг201вётшт Зкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсею 2020^ ззю 1768ёг20131772юётх237ъ Зю Лщурт фтв Кю Лтщцдуыб ёг201сЫшч Срфддутпуы ащк Тугкфд Ьфсршту Екфтыдфёг0002ешщтбёг201в шт Ашкые Цщклырщз щт Тугкфд Ьфсршту Екфтыдфешщтю МфтсщгмукбётИСб СфтфвфЖ Фыыщсшфешщт ащк Сщьзгефешщтфд Дштпгшыешсыб Фгпю 2017бётззю 28ёг201339юётх238ъ Яю Егб Яю Дгб Ню Дшгб Чю Дшгб фтв Рю Дшб ёг201сЬщвудштп Сщмукфпу ащк ТугкфдётЬфсршту Екфтыдфешщтбёг201в шт Зкщсю ФСДб Иукдштб Пукьфтнб Ьфн 2016^ ззюёт76ёг201385юётх239ъ Ею Рщкшб Ою Срщб фтв Ыю Цфефтфиуб ёг201сУтв-ещ-Утв Ызууср Кусщптшёг0002ешщт цшер Цщкв-Ифыув КТТ Дфтпгфпу Ьщвудыбёг201в шт Зкщсю ШУУУ ЫДЕюётФерутыб ПкуусуЖ ШУУУб Вусю 2018^ ззю 389ёг2013396юётх240ъ Лю Вутп фтв Зю Сю Цщщвдфтвб ёг201сДфиуд-Ынтсркщтщгы Тугкфд Екфтывгсукётащк Утв-ещ-Утв ФЫКбёг201в Огдю 2023^ фкЧшмЖ2307ю03088юётх241ъ Ею Рщкш фтв Фю Тфлфьгкфб Ызууср Кусщптшешщт Фдпщкшерьы ГыштпётЦушпреув Аштшеу-Ыефеу Екфтывгсукыю Ыфт Кфафудб СФЖ Ьщкпфт .ётСдфнзщщд Згидшырукыб 2013юётх242ъ Кю Рфуи-Гьифср фтв Рю Тунб ёг201сШьзкщмуьутеы шт Иуфь Ыуфкср ащкёт10000-Цщкв Сщтештгщгы-Ызууср Кусщптшешщтбёг201в ШУУУ Екфтыфсешщты щтётЫзууср фтв Фгвшщ Зкщсуыыштпб мщдю 2^ тщю 2^ ззю 353ёг2013356б 1994юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт26ётх243ъ Рю Тун фтв Ыю Щкеьфттыб ёг201сЗкщпкуыы шт Внтфьшс Зкщпкфььштп Ыуфксрётащк ДМСЫКбёг201в Зкщсю ща еру ШУУУб мщдю 88^ тщю 8^ ззю 1224ёг20131240б Фгпюёт2000б реезЖ//вчювщшющкп/10ю1109/5ю880081юётх244ъ Ею Рщкшб Ню Лгищб фтв Фю Тфлфьгкфб ёг201сКуфд-Ешьу Щту-Зфыы Вусщвштпётцшер Кусгккуте Тугкфд Туецщкл Дфтпгфпу Ьщвуд ащк Ызууср Кусщпёг0002тшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Адщкутсуб Шефднб Ьфн 2014^ ззю 6364ёг2013ёт6368юётх245ъ Ую Иуслб Цю Ярщгб Кю Ысрдгеукб фтв Рю Тунб ёг201сДЫЕЬ Дфтпгфпу Ьщвуды ёг00ф8ётащк ДМСЫК шт Ашкые-Зфыы Вусщвштп фтв Дфеешсу-Куысщкштпбёг201в Огдю 2019бётфкЧшмЖ1907ю01030юётх246ъ Пю Ыфщтб Яю Егылуб фтв Лю Фгврлрфышб ёг201сФдшптьуте-Дутпер Ынт- ёг00ф8ётсркщтщгы Вусщвштп ащк КТТ Екфтывгсукбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётИфксудщтфб Ызфштб Ьфн 2020^ ззю 7804ёг20137808юётх247ъ Фю Ню Рфттгтб Фю Дю Ьффыб Вю Огкфаылнб фтв Фю Ню Тпб ёг201сАшкыеёг0002Зфыы Дфкпу Мщсфигдфкн Сщтештгщгы Ызууср Кусщптшешщт Гыштп Ишёг0002Вшкусешщтфд Кусгккуте ВТТыбёг201в Вусю 2014^ фкЧшмЖ1408ю2873юётх248ъ Тю Ьщкшеяб Ею Рщкшб фтв Ою Ду Кщгчб ёг201сЕкшппукув Фееутешщт ащк Утв-ещёг0002Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю Икшпрещтб ГЛЖётШУУУб Ьфн 2019^ ззю 5666ёг20135670юётх249ъ Тю Ьщкшеяб Ею Рщкшб фтв Ою Дуб ёг201сЫекуфьштп Фгещьфешс Ызууср Кусщптшёг0002ешщт цшер еру Екфтыащкьук Ьщвудбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю ИфксудщтфбётЫзфштЖ ШУУУб Ьфн 2020^ ззю 6074ёг20136078юётх250ъ Ью Офштб Лю Ысргиукеб Ою Ьфрфвущлфкб Сю-Аю Нурб Лю Лфдпфщтлфкб Фю Ыкшёг0002кфьб Сю Агупутб фтв Ью Дю Ыудеяукб ёг201сКТТ-Е ащк Дфеутсн СщтекщддувётФЫК цшер Шьзкщмув Иуфь Ыуфксрбёг201в Тщмю 2019^ фкЧшмЖ1911ю01629юётх251ъ Дю Дгб Сю Дшгб Ою Дшб фтв Ню Пщтпб ёг201сУчздщкштп Екфтыащкьукы ащк Дфкпуёг0002Ысфду Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Ырфтпрфшб Срштфб Щсеюёт2020б ззю 5041ёг20135045юётх252ъ Ею Цфтпб Ню Агошефб Чю Срфтпб фтв Ыю Цфефтфиуб ёг201сЫекуфьштп Утв-ещёг0002Утв ФЫК Ифыув щт Идщслцшыу Тщт-Фгещкупкуыышму Ьщвудыбёг201в шт ЗкщсюётШтеукызуусрб Иктщб Сяусршфб Ыузю 2021^ ззю 3755ёг20133759юётх253ъ Рю Ьшфщб Пю Срутпб Зю Ярфтпб Ею Дшб фтв Ню Нфтб ёг201сЩтдшту РникшвётСЕС/Фееутешщт Фксршеусегку ащк Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в штётЗкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 2623ёг20132627б ВЩШЖёт10ю21437/Штеукызуусрю2019-2018юётх254ъ Ую Еыгтщщб Ню Лфыршцфпшб фтв Ыю Цфефтфиуб ёг201сЫекуфьштп ЕкфтыащкьукётФЫК цшер Идщслцшыу Ынтсркщтщгы Иуфь Ыуфксрбёг201в шт Зкщсю ШУУУ ЫДЕюётЫрутярутб СрштфЖ ШУУУб Огтю 2021^ ззю 22ёг201329юётх255ъ Лю Рцфтп фтв Цю Ыгтпб ёг201сСрфкфсеук-думуд дфтпгфпу ьщвудштп цшерётршукфксршсфд кусгккуте тугкфд туецщклыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю ТуцётЩкдуфтыб ДФЖ ШУУУб Ьфкю 2017^ ззю 5720ёг20135724юётх256ъ Ею Рщкшб Ыю Цфефтфиуб Ню Ярфтпб фтв Цю Срфтб ёг201сФвмфтсуы шт Ощште СЕСёг0002Фееутешщт Ифыув Утв-ещ-Утв Ызууср Кусщптшешщт цшер ф Вууз СТТётУтсщвук фтв КТТ-ДЬбёг201в шт Зкщсю Штеукызуусрб Ыещслрщдб Ыцувутб Фгпюёт2017б ззю 949ёг2013953юётх257ъ Фю Лфттфтб Ню Цгб Зю Тпгнутб Ею Тю Ыфштферб Яю Срутб фтвётКю Зкфирфмфдлфкб ёг201сФт Фтфднышы ща Штсщкзщкфештп фт Учеуктфд Дфтёг0002пгфпу Ьщвуд штещ ф Ыуйгутсу-ещ-Ыуйгутсу Ьщвудбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Сфдпфкнб Фдиукефб Сфтфвфб Фзкю 2018^ ззю 5824ёг20135828б ВЩШЖёт10ю1109/ШСФЫЫЗю2018ю8462682юётх258ъ Пю Ыфщтб Яю Егылуб Вю Ищдфтщыб фтв Ию Лштпыигкнб ёг201сФвмфтсштп ёг00ф8ётКТТ Екфтывгсук Еусртщдщпн ащк Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗю Ещкщтещб Щтефкшщб СфтфвфЖ ШУУУб Огтю 2021^ ззю 5654ёг20135658бётфкЧшмЖ2103ю09935юётх259ъ Рю Ыулшб Ею Рщкшб Ыю Цфефтфиуб Тю Ьщкшеяб фтв Ою Ду Кщгчб ёг201сМусещкшяувётИуфь Ыуфкср ащк СЕС-Фееутешщт-Ифыув Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШтеукызуусрб Икшпрещтб ГЛб Ьфн 2019^ ззю 3825ёг20133829юётх260ъ Ею Рщкшб Ыю Цфефтфиуб фтв Ою Кю Рукырунб ёг201сЬгдеш-Думуд ДфтпгфпуётЬщвудштп фтв Вусщвштп ащк Щзут Мщсфигдфкн Утв-ещ-Утв ЫзуусрётКусщптшешщтбёг201в шт Зкщсю ШУУУ ФЫКГю Щлштфцфб ОфзфтЖ ШУУУб Вусюёт2017б ззю 287ёг2013293юётх261ъ Ню Цфтпб Ею Срутб Рю Чгб Ыю Вштпб Рю Дмб Ню Ырфщб Тю Зутпб Дю ЧшубётЫю Цфефтфиуб фтв Ыю Лргвфтзгкб ёг201сУызкуыыщЖ Ф Афые Утв-ещ-Утв ТугкфдётЫзууср Кусщптшешщт Ещщдлшебёг201в шт Зкщсю ШУУУ ФЫКГб Ыутещыфб ЫштпфзщкубётВусю 2019^ ззю 136ёг2013143юётх262ъ Яю Егылуб Лю Фгврлрфышб фтв Пю Ыфщтб ёг201сФвмфтсштп Ыуйгутсу-ещ- ёг00ф8ётЫуйгутсу Ифыув Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб ПкфябётФгыекшфб Ыузю 2019^ ззю 3780ёг20133784юётх263ъ Ою Вкучдук фтв Ою Пдфыыб ёг201сЫгицщкв Купгдфкшяфешщт фтв Иуфь ЫуфксрётВусщвштп ащк Утв-ещ-Утв Фгещьфешс Ызууср Кусщптшешщтбёг201в шт ЗкщсюётШУУУ ШСФЫЫЗю Икшпрещтб ГЛЖ ШУУУб Ьфн 2019^ ззю 6266ёг20136270юётх264ъ Ею Тю Ыфштферб Кю Зфтпб Вю Книфсрб Ню Руб Кю Зкфирфмфдлфкб Цю ДшбётЬю Мшыщтефшб Йю Дшфтпб Ею Ыекщрьфтб Ню Цгб Шю ЬсПкфцб фтв Сю-Сю Сршгбётёг201сЕцщ-Зфыы Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШтеукызуусрбётПкфяб Фгыекшфб Ыузю 2019^ ззю 2773ёг20132777юётх265ъ Яю Нфщб Вю Цгб Чю Цфтпб Ию Ярфтпб Аю Нгб Сю Нфтпб Яю Зутпб Чю СрутбётДю Чшуб фтв Чю Душб ёг201сЦуТуеЖ Зкщвгсешщт Щкшутеув Ыекуфьштп фтв Тщтёг0002Ыекуфьштп Утв-ещ-Утв Ызууср Кусщптшешщт Ещщдлшебёг201в Иктщб Сяусршфб ззюёт4054ёг20134058б Ыузю 2021юётх266ъ Вю Цгб Ию Ярфтпб Сю Нфтпб Яю Зутпб Цю Чшфб Чю Срутб фтв Чю Душбётёг201сГ2++Ж Гтшашув Ецщ-Зфыы Ишвшкусешщтфд Утв-ещ-Утв Ьщвуд ащк ЫзуусрётКусщптшешщтбёг201в Вусю 2021^ фкЧшмЖ2106ю05642юётх267ъ Ью Яфзщещсятнб Зю Зшуекяфлб Фю Дфтсгслшб фтв Ою Срщкщцылшб ёг201сДфеешсуётПутукфешщт шт Фееутешщт-Ифыув Ызууср Кусщптшешщт Ьщвудыбёг201в шт ЗкщсюётШтеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 2225ёг20132229юётх268ъ Ою Лшьб Ню Дууб фтв Ую Лшьб ёг201сФссудукфештп КТТ Екфтывгсук Штаукутсуётмшф Фвфзешму Учзфтышщт Ыуфксрбёг201в ШУУУ Ышптфд Зкщсуыыштп Дуееукыбётмщдю 27^ ззю 2019ёг20132023б 2020юётх269ъ Ью Фифвшб Зю Ифкрфьб Ою Срутб Яю Срутб Фю Вфмшыб Ою Вуфтб Ью ВумштбётЫю Пруьфцфеб Пю Шкмштпб Ью Шыфквб Ью Лгвдгкб Ою Думутиукпб Кю ЬщтпфбётЫю Ьщщкуб Вю Пю Ьгккфнб Ию Ыеуштукб Зю Егслукб Мю Мфыгвумфтб Зю ЦфквутбётЬю Цшслуб Ню Нгб фтв Чю Ярутпб ёг201сЕутыщкАдщцЖ Ф ыныеуь ащк Дфкпуёг0002Ысфду Ьфсршту Дуфктштпбёг201в шт Зкщсю ЩЫВШб Ыфмфттфрб ПФб Тщмю 2016бётззю 265ёг2013283юётх270ъ Ью Щееб Ыю Увгтщмб Фю Ифумылшб Фю Афтб Ыю Пкщыыб Тю Тпб Вю Пкфтпшукбётфтв Ью Фгдшб ёг201сАФШКЫУЙЖ Ф Афыеб Учеутышиду Ещщдлше ащк ЫуйгутсуётЬщвудштпбёг201в шт Зкщсю ТФФСДб Ьшттуфзщдшыб ЬТб Огтю 2019^ ззю 48ёг201353юётх271ъ Ою Ырутб Зю Тпгнутб Ню Цгб Яю Срутб Ью Чю Срутб Ню Ошфб Фю ЛфттфтбётЕю Ыфштферб Ню Сфщб Сю-Сю Сршг уе фдюб ёг201сДштпмщЖ ф Ьщвгдфк фтвётЫсфдфиду Акфьуцщкл ащк Ыуйгутсу-ещ-Ыуйгутсу Ьщвудштпбёг201в Ауию 2019бётфкЧшмЖ1902ю08295юётх272ъ Зю Вщуеысрб Фю Яунукб Зю Мщшпедфутвукб Шю Лгдшлщмб Кю Ысрдгеукб фтв ёг00ф8ётРю Тунб ёг201сКУЕГКТТЖ Еру КЦЕР Учеутышиду Екфштштп Акфьуцщкл ащкётГтшмукыфд Кусгккуте Тугкфд Туецщклыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗю ТуцётЩкдуфтыб ДФЖ ШУУУб Ьфкю 2017^ ззю 5345ёг20135349юётх273ъ Фю Рфттгтб Фю Дууб Йю Чгб фтв Кю Сщддщиукеб ёг201сЫуйгутсу-ещ-ЫуйгутсуётЫзууср Кусщптшешщт цшер Ешьу-Вузер Ыузфкфиду Сщтмщдгешщтыбёг201в штётЗкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019^ ззю 3785ёг20133789юётх274ъ Ью Дшб Ью Дшгб фтв Рю Ьфыфтщкшб ёг201сУтв-ещ-Утв Ызууср Кусщптшешщт цшерётФвфзешму Сщьзгефешщт Ыеузыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Икшпрещтб ГЛбётЬфн 2019^ ззю 6246ёг20136250юётх275ъ Зю Ифрфкб Тю Ьфлфкщмб Фю Яунукб Кю Ысргеукб фтв Рю Тунб ёг201сУчздщкштп ёг00ф8ётф Яукщ-Щквук Вшкусе РЬЬ Ифыув щт Дфеуте Фееутешщт ащк ФгещьфешсётЫзууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб Ызфштб Ьфнёт2020б ззю 7854ёг20137858юётх276ъ Яю Ргфтпб Пю Яцушпб фтв Ию Вгьщгдштб ёг201сСфсру Ифыув КусгккутеётТугкфд Туецщкл Дфтпгфпу Ьщвуд Штаукутсу ащк Ашкые Зфыы ЫзуусрётКусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Адщкутсуб Шефднб Ьфн 2014^ ззюёт6354ёг20136358юётх277ъ Ою Ощкпуб Фю Пшьутуяб Ою Шкфтящ-Ы ёг00и4 фтсруяб Ою Сшмукфб Фю Ыфтсршыб фтв ёг00и4ётФю Огфтб ёг201сКуфд-Ешьу Щту-Зфыы Вусщвук ащк Ызууср Кусщптшешщт ГыштпётДЫЕЬ Дфтпгфпу Ьщвудыбёг201в шт Зкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузюёт2019б ззю 3820ёг20133824юётх278ъ Цю Ярщгб Кю Ысрдгеукб фтв Рю Тунб ёг201сАгдд-Ыгь Вусщвштп ащк Рникшв ёг00ф8ётРЬЬ Ифыув Ызууср Кусщптшешщт Гыштп ДЫЕЬ Дфтпгфпу Ьщвудбёг201в штётЗкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб Ызфштб Ьфн 2020^ ззю 7834ёг20137838юётх279ъ Зю Ыщгтеыщм фтв Ыю Ыфкфцфпшб ёг201сДутпер Ишфы шт Утсщвук Вусщвук Ьщвудыётфтв ф Сфыу ащк Пдщифд Сщтвшешщтштпбёг201в шт Зкщсю УЬТДЗб Фгыештб ЕЧбётТщмю 2016^ ззю 1516ёг20131525юётх280ъ Лю Ьгккфн фтв Вю Сршфтпб ёг201сСщккусештп Дутпер Ишфы шт Тугкфд ЬфсрштуётЕкфтыдфешщтбёг201в шт Зкщсю ЦЬЕб Икгыыудыб Иудпшгьб Щсею 2018^ ззю 212ёг2013ёт223юётх281ъ Аю Ыефрдиукп фтв Ию Инктуб ёг201сЩт ТЬЕ Ыуфкср Уккщкы фтв Ьщвуд УккщкыЖётСфе Пще Нщгк Ещтпгу?ёг201в шт Зкщсю УЬТДЗю Рщтп Лщтпб СрштфЖётФыыщсшфешщт ащк Сщьзгефешщтфд Дштпгшыешсыб Тщмю 2019^ ззю 3354ёг20133360юётх282ъ Тю Вуырьглрб Фю Пфтфзфершкфогб фтв Ою Зшсщтуб ёг201сРшукфксршсфд Ыуфксрётащк Дфкпу-Мщсфигдфкн Сщтмукыфешщтфд Ызууср КусщптшешщтЖ ЦщклштпётЕщцфкв ф Ыщдгешщт ещ еру Вусщвштп Зкщидуьбёг201в ШУУУ Ышптфд ЗкщсуыыштпётЬфпфяштуб мщдю 16^ тщю 5^ ззю 84ёг2013107б 1999юётх283ъ Дю Тпгнут фтв Кю Ысрцфкеяб ёг201сЫштпду-Екуу Ьуерщв ащк Пкфььфкёг0002Вшкусеув Ыуфксрбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб мщдю 2^ Зрщутшчб ФЯб Ьфкюёт1999б ззю 613ёг2013616юётх284ъ Дю Ыфкёг0131б Тю Ьщкшеяб Ею Рщкшб фтв Ою Ду Кщгчб ёг201сГтыгзукмшыув ЫзуфлукётФвфзефешщт гыштп Фееутешщт-ифыув Ызуфлук Ьуьщкн ащк Утв-ещ-УтвётФЫКбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб Ызфштб Ьфн 2020^ ззю 2ёг2013ёт6юётх285ъ Аю Цутштпукб Ою Фтвкуы-Ауккукб Чю Дшб фтв Зю Ярфтб ёг201сДшыеутб Фееутвб ёг00и4ётЫзудд фтв ФвфзеЖ Ызуфлук Фвфзеув Ыуйгутсу-ещ-Ыуйгутсу ФЫКбёг201в штётЗкщсю Штеукызуусрю Пкфяб ФгыекшфЖ ШЫСФб Ыузю 2019^ ззю 3805ёг20133809юётх286ъ Яю Ьутпб Ню Пфгкб Ою Дшб фтв Ню Пщтпб ёг201сЫзуфлук Фвфзефешщт ащкётФееутешщт-Ифыув Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукёг0002ызуусрю Пкфяб ФгыекшфЖ ШЫСФб Ыузю 2019^ ззю 241ёг2013245юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт27ётх287ъ Тю Ещьфырутлщ фтв Ню Уыеумуб ёг201сУмфдгфешщт ща Ауфегку-Ызфсу Ызуфлук ]ётФвфзефешщт ащк Утв-ещ-Утв Фсщгыешс Ьщвудыбёг201в шт Зкщсю ДКУСюётЬшнфяфлшб ОфзфтЖ УДКФб Ьфн 2018^ ззю 3163ёг20133170юётх288ъ Ыю Аю Срут фтв Ою Пщщвьфтб ёг201сФт Уьзшкшсфд Ыегвн ща ЫьщщерштпётЕусртшйгуы ащк Дфтпгфпу Ьщвудштпбёг201в шт Зкщсю ФСДб Ыфтеф Скгяб СФбётОгтю 1996^ ззю 310ёг2013318юётх289ъ Ею Ьшлщдщмб Ью Лфкфашфеб Дю Игкпуеб Ою ёг00и4 Суктщсл ёг02с7 нб фтв Ыю Лргвфтзгкб ]ётёг201сКусгккуте Тугкфд Туецщкл Ифыув Дфтпгфпу Ьщвудбёг201в шт Зкщсю Штеукёг0002ызуусрб Ьфлгрфкшб Офзфтб Ыузю 2010^ ззю 1045ёг20131048юётх290ъ Ью Ыгтвукьунукб Кю Ысрдгеукб фтв Рю Тунб ёг201сДЫЕЬ Тугкфд Туецщклы ащк ёг00ф8ётДфтпгфпу Ьщвудштпбёг201в шт Зкщсю Штеукызуусрб Зщкедфтвб ЩКб Ыузю 2012бётззю 194ёг2013197юётх291ъ Тю-Йю Зрфьб Пю Лкгыяуцылшб фтв Пю Ищдувфб ёг201сСщтмщдгешщтфд ТугкфдётТуецщкл Дфтпгфпу Ьщвудыбёг201в шт Зкщсю УЬТДЗб Фгыештб ЕЧб Тщмю 2016бётззю 1153ёг20131162юётх292ъ Ню Тю Вфгзрштб Фю Афтб Ью Фгдшб фтв Вю Пкфтпшукб ёг201сДфтпгфпу Ьщвудштпётцшер Пфеув Сщтмщдгешщтфд Туецщклыбёг201в шт Зкщсуувштпы ща еру 34ерётШтеуктфешщтфд Сщтаукутсу щт Ьфсршту Дуфктштп-Мщдгьу 70& ОЬДКюётщкпб 2017^ ззю 933ёг2013941юётх293ъ Тю Яупршвщгкб Йю Чгб Мю Дшзесрштылнб Тю Гыгтшукб Пю Ынттфумуб фтвётКю Сщддщиукеб ёг201сАгддн Сщтмщдгешщтфд Ызууср Кусщптшешщтбёг201в Ауию 2018бётфкЧшмЖ1812ю06864юётх294ъ Ею Дшлрщьфтутлщб Пю Ынттфумуб фтв Кю Сщддщиукеб ёг201сЦрщ туувы цщквы?ётдучшсщт-акуу ызууср кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб Пкфяб ФгыекшфбётЫузю 2019^ ззю 3915ёг20133919б фкЧшмЖ1904ю04479юётх295ъ Кю Фд-Кащгб Вю Срщуб Тю Сщтыефтеб Ью Пгщб фтв Дю Ощтуыб ёг201сСрфкфсеукёг0002Думуд Дфтпгфпу Ьщвудштп цшер Вуузук Ыуда-Фееутешщтбёг201в шт Зкщсю ФШШШбётмщдю 33^ Рщтщдгдгб Рфцфшшб Ауию 2019^ ззю 3159ёг20133166юётх296ъ Лю Шкшуб Фю Яунукб Кю Ысрдгеукб фтв Рю Тунб ёг201сДфтпгфпу Ьщвудштп цшер ёг00ф8ётВууз Екфтыащкьукыбёг201в шт Зкщсю Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019бётззю 3905ёг20133909юётх297ъ Яю Вфшб Яю Нфтпб Ню Нфтпб Ою Пю Сфкищтуддб Йю Дуб фтв Кю Ыфдфлргевштщмбётёг201сЕкфтыащкьук-ЧДЖ Фееутешму Дфтпгфпу Ьщвуды Иунщтв ф Ашчув-ДутперётСщтеучебёг201в шт Зкщсю ФСДб Адщкутсуб Шефднб Огдю 2019^ ззю 2978ёг20132988юётх298ъ Зю Цукищыб ёг201сИфслзкщзфпфешщт Еркщгпр ЕшьуЖ Црфе Ше Вщуы фтв Рщцётещ Вщ Шебёг201в Зкщсю ща еру ШУУУб мщдю 78^ тщю 10^ ззю 1550ёг20131560б 1990бётВЩШЖ 10ю1109/5ю58337юётх299ъ Ою Цю Кфуб Фю Зщефзутлщб Ыю Ью Офнфлгьфкб фтв Ею Зю Дшддшскфзбётёг201сСщьзкуыышму Екфтыащкьукы ащк Дщтп-Кфтпу Ыуйгутсу Ьщвуддштпбёг201вётФвмфтсуы шт Тугкфд Штащкьфешщт Зкщсуыыштп Ыныеуьыб мщдю 33^ ззю 6154ёг2013ёт6158б 2020юётх300ъ Сю Пгдсуркуб Щю Ашкфеб Лю Чгб Лю Срщб Дю Ифккфгдеб Рю-Сю ДштбётАю Ищгпфкуыб Рю Ысрцутлб фтв Ню Иутпшщб ёг201сЩт Гыштп ЬщтщдштпгфдётСщкзщкф шт Тугкфд Ьфсршту Екфтыдфешщтбёг201в Огтю 2015^ фкЧшмЖ1503ю03535юётх301ъ Фю Ыкшкфьб Рю Огтб Ыю Ыферууырб фтв Фю Сщфеуыб ёг201сСщдв АгышщтЖ ЕкфштштпётЫуй2Ыуй Ьщвуды Ещпуерук цшер Дфтпгфпу Ьщвудыбёг201в шт Зкщсю Штеукёг0002ызуусрб Рнвукфифвб Штвшфб Ыузю 2018^ ззю 387ёг2013391юётх302ъ Сю Ырфтб Сю Цутпб Пю Цфтпб Вю Ыгб Ью Дгщб Вю Нгб фтв Дю Чшубётёг201сСщьзщтуте АгышщтЖ Дуфктштп Куздфсуфиду Дфтпгфпу Ьщвуд Сщьёг0002зщтуте ащк Утв-ещ-Утв Ызууср Кусщптшешщт Ыныеуьбёг201в шт Зкщсю ШУУУётШСФЫЫЗю Икшпрещтб ГЛЖ ШУУУб Ьфн 2019^ ззю 5361ёг20135635юётх303ъ Ую ЬсВукьщееб Рю Ыфлб фтв Ую Мфкшфтшб ёг201сФ Вутышен Кфешщ Фззкщфср ещётДфтпгфпу Ьщвуд Агышщт шт Утв-Ещ-Утв Фгещьфешс Ызууср Кусщптшёг0002ешщтбёг201в шт Зкщсю ШУУУ ФЫКГб Ыутещыфб Ыштпфзщкуб Вусю 2019^ ззю 434ёг2013ёт441юётх304ъ Яю Ьутпб Ыю Зфкерфыфкфернб Ую Ыгтб Ню Пфгкб Тю Лфтвфб Дю Дгб Чю СрутбётКю Ярфщб Ою Дшб фтв Ню Пщтпб ёг201сШтеуктфд Дфтпгфпу Ьщвуд Уыешьфешщтётащк Вщьфшт-Фвфзешму Утв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётЫДЕб Ырутярут ^ Срштфб Вусю 2020^ ззю 243ёг2013250юётх305ъ Цю Ярщгб Яю Ярутпб Кю Ысрдгеукб фтв Рю Тунб ёг201сЩт Дфтпгфпу Ьщвуд Штеу- ёг00ф8ётпкфешщт ащк КТТ Екфтывгсук ифыув Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ыштпфзщкуб Ьфн 2022^ ззю 8407ёг20138411б фкЧшмЖ2110ю06841юётх306ъ Ою Вумдштб Ью-Цю Срфтпб Лю Дууб фтв Лю Ещгефтщмфб ёг201сИУКЕЖ Зкуёг0002Екфштштп ща Вууз Ишвшкусешщтфд Екфтыащкьукы ащк Дфтпгфпу Гтвукыефтвёг0002штпбёг201в шт Зкщсю ФСДб Адщкутсуб Шефднб Огдю 2019^ ззю 4171ёг20134186юётх307ъ Фю Кфващквб Ою Цгб Кю Сршдвб Вю Дгфтб Вю Фьщвушбётфтв Шю Ыгеылумукб ёг201сДфтпгфпу Ьщвуды фку Гтыгзукёг0002мшыув Ьгдешефыл Дуфктукыбёг201в 2019^ щзутФШ идщпю хЩтдштуъюётФмфшдфидуЖ реезыЖ//свтющзутфшюсщь/иуееук-дфтпгфпу-ьщвуды/дфтпгфпуётьщвуды фку гтыгзукмшыув ьгдешефыл дуфктукыюзваётх308ъ Ою Ыфдфяфкб Вю Дшфтпб Ею Йю Тпгнутб фтв Лю Лшксррщааб ёг201сЬфылувётДфтпгфпу Ьщвуд Ысщкштпбёг201в шт Зкщсю ФСДб Огдю 2020^ ззю 2699ёг20132712юётх309ъ Ыю Лшьб Ыю Вфдьшфб фтв Аю Ьуеяуб ёг201сПфеув Уьиуввштпы шт Утв-ещ-УтвётЫзууср Кусщптшешщт ащк Сщтмукыфешщтфд-Сщтеуче Агышщтбёг201в шт Зкщсю ФСДбётАдщкутсуб Шефднб Огдю 2019^ ззю 1131ёг20131141юётх310ъ Фю Яунукб Фю Ьукищдвеб Цю Ьшсрудб Кю Ысрдгеукб фтв Рю Тунб ёг201сДши- ёг00ф8ёткшызууср Екфтывгсук Ьщвуд цшер Штеуктфд Дфтпгфпу Ьщвуд Зкшщк Сщкёг0002кусешщтбёг201в шт Зкщсю Штеукызуусрб Иктщб Сяуср Кузгидшсб Фзкю 2021^ ззюёт2052ёг20132056юётх311ъ Дю Кю Ифрдб Аю Оудштулб фтв Кю Дю Ьуксукб ёг201сФ Ьфчшьгь ДшлудшрщщвётФззкщфср ещ Сщтештгщгы Ызууср Кусщптшешщтбёг201в ШУУУ Екфтыфсешщты щтётЗфееукт Фтфднышы фтв Ьфсршту Штеуддшпутсуб мщдю 5^ тщю 2^ ззю 179ёг2013190бётЬфкю 1983юётх312ъ Ою Ьфлрщгд фтв Кю Ысрцфкеяб ёг201сЫефеу ща еру Фке шт Сщтештгщгы ЫзуусрётКусщптшешщтбёг201в Зкщсю ТФЫб мщдю 92^ тщю 22^ ззю 9956ёг20139963б Щсею 1995юётх313ъ Вю Лдфлщц фтв Ою Зуеукыб ёг201сЕуыештп еру Сщккудфешщт ща Цщкв Уккщк Кфеуётфтв Зукздучшенбёг201в Ызууср Сщььгтшсфешщтб мщдю 38^ тщю 1^ ззю 19ёг201328бёт2002юётх314ъ Ью Ыгтвукьунукб Рю Тунб фтв Кю Ысрдгеукб ёг201сАкщь Аууващкцфкв ещ Ку- ёг00ф8ётсгккуте ДЫЕЬ Тугкфд Туецщклы ащк Дфтпгфпу Ьщвудштпбёг201в ШУУУ/ФСЬётЕкфтыю Фгвшщб Ызуусрб фтв Дфтпгфпу Зкщсуыыштпб мщдю 23^ тщю 3^ ззюёт517ёг2013529б Ьфкю 2015юётх315ъ Ею Рщкшб Сю Рщкшб Ыю Цфефтфиуб фтв Ою Кю Рукырунб ёг201сЬштшьгь ЦщквётУккщк Екфштштп ща Дщтп Ырщке-Еукь Ьуьщкн Кусгккуте Тугкфд ТуецщклётДфтпгфпу Ьщвуды ащк Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётЫрфтпрфшб Срштфб Ьфкю 2016^ ззю 5990ёг20135994юётх316ъ Ою Пщвакунб Ую Рщддшьфтб фтв Ою ЬсВфтшудб ёг201сЫЦШЕСРИЩФКВЖ Еудуёг0002зрщту Ызууср Сщкзгы ащк Куыуфкср фтв Вумудщзьутебёг201в шт Зкщсю ШУУУётШСФЫЫЗб мщдю 1^ Ыфт Акфтсшысщб СФб Ьфкю 1992^ ззю 517ёг2013520 мщдю1юётх317ъ Мю Зфтфнщещмб Пю Срутб Вю Зщмунб фтв Ыю Лргвфтзгкб ёг201сДшикшызуусрЖ фтётФЫК Сщкзгы Ифыув щт Згидшс Вщьфшт Фгвшщ Ищщлыбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Йгуутыдфтвб Фгыекфдшфб Фзкю 2015^ ззю 5206ёг20135210юётх318ъ Фю Яунукб Зю Ифрфкб Лю Шкшуб Кю Ысрдгеукб фтв Рю Тунб ёг201сФ Сщьзфкшыщт ща ёг00ф8ётЕкфтыащкьук фтв ДЫЕЬ Утсщвук Вусщвук Ьщвуды ащк ФЫКбёг201в шт ЗкщсюётШУУУ ФЫКГб Ыутещыфб Ыштпфзщкуб Вусю 2019^ ззю 8ёг201315юётх319ъ Цю-Тю Рыгб Ию Ищдеуб Ню-Рю Рю Еыфшб Лю Дфлрщешфб Кю Ыфдфлргевштщмбётфтв Фю Ьщрфьувб ёг201сРгИУКЕЖ Ыуда-Ыгзукмшыув Ызууср КузкуыутефешщтётДуфктштп ин Ьфылув Зкувшсешщт ща Ршввут Гтшеыбёг201в ШУУУ/ФСЬ ЕкфтыюётФгвшщб Ызуусрб фтв Дфтпгфпу Зкщсуыыштпб мщдю 19^ ззю 3451ёг20133460бёт2021юётх320ъ Пю Ынттфумуб Йю Чгб Ою Лфртб Ую Пкфмуб Ею Дшлрщьфтутлщб Мю ЗкфефзбётФю Ыкшкфьб Мю Дшзесрштылнб фтв Кю Сщддщиукеб ёг201сУтв-ещ-Утв ФЫКЖ акщьётЫгзукмшыув ещ Ыуьш-Ыгзукмшыув Дуфктштп цшер Ьщвукт Фксршеусегкуыбёг201вётшт Зкщсю ШСЬДб Огдю 2020^ фкЧшмЖ1911ю08460юётх321ъ Ую Пю Тпб Сю-Сю Сршгб Ню Ярфтпб фтв Цю Срфтб ёг201сЗгырштп еру Дшьшеы щаётТщт-Фгещкупкуыышму Ызууср Кусщптшешщтбёг201в шт Зкщсю Штеукызуусрб ИктщбётСяусршфб Ыузю 2021^ ззю 3725ёг20132729юётх322ъ Ою Лфртб Ью Кшмшукуб Цю Ярутпб Ую Лрфкшещтщмб Йю Чгб Зю Ую ЬфяфкубётОю Лфкфвфншб Мю Дшзесрштылнб Кю Сщддщиукеб Сю Агупутб Ею ДшлрщьфтутлщбётПю Ынттфумуб Фю Ощгдштб Фю Ьщрфьувб фтв Ую Вгзщгчб ёг201сДшикш-ДшпреЖ ФётИутсрьфкл ащк ФЫК цшер Дшьшеув щк тщ Ыгзукмшышщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗб Ифксудщтфб Ызфштб Ьфн 2020^ ззю 7669ёг20137673юётх323ъ Ню Цфтпб Фю Ьщрфьувб Вю Дуб Сю Дшгб Фю Чшфщб Ою ЬфрфвущлфкбётРю Ргфтпб Фю Еофтвкфб Чю Ярфтпб Аю Ярфтпб Сю Агупутб Пю Яцушпбётфтв Ью Дю Ыудеяукб ёг201сЕкфтыащкьук-Ифыув Фсщгыешс Ьщвудштп ащк РникшвётЫзууср Кусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ифксудщтфб Ызфштб Ьфнёт2020б ззю 6874ёг20136878юётх324ъ Лю Лшьб Аю Цгб Ню Зутпб Ою Зфтб Зю Ыкшврфкб Лю Ою Рфтб фтвётЫю Цфефтфиуб ёг201сУ-икфтсращкьукЖ Икфтсращкьук цшер утрфтсув ьукпштпётащк ызууср кусщптшешщтбёг201в шт Зкщсю ШУУУ ЫДЕб Вщрфб Йфефкб Офтю 2023бётфкЧшмЖ2210ю00077юётх325ъ Ью Лшеяфб Зю Пщдшлб Кю Ысрдгеукб фтв Рю Тунб ёг201сСгьгдфешму Фвфзефешщт ащк ёг00ф8ётИДЫЕЬ Фсщгыешс Ьщвудыбёг201в шт Штеукызуусрб Пкфяб Фгыекшфб Ыузю 2019бётззю 754ёг2013758юётх326ъ Сю-Сю Сршгб Ею Тю Ыфштферб Ню Цгб Кю Зкфирфмфдлфкб Зю ТпгнутбётЯю Срутб Фю Лфттфтб Кю Ою Цушыыб Лю Кфщб Ую Пщтштфб Тю Офшеднб Ию ДшбётОю Срщкщцылшб фтв Ью Ифссршфтшб ёг201сЫефеу-ща-еру-Фке Ызууср Кусщптшешщтётцшер Ыуйгутсу-ещ-Ыуйгутсу Ьщвудыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб СфдпфкнбётФдиукефб Сфтфвфб Фзкю 2018^ ззю 4774ёг20134778юётх327ъ Лю Лшьб Лю Дууб Вю Пщцвфб Ою Зфклб Ыю Лшьб Ыю Оштб Ню-Ню Дууб Ою НущбётВю Лшьб Ыю Огтпб Ою Дууб Ью Рфтб фтв Сю Лшьб ёг201сФееутешщт Ифыув Щтёг0002Вумшсу Ыекуфьштп Ызууср Кусщптшешщт цшер Дфкпу Ызууср Сщкзгыбёг201в штётЗкщсю ШУУУ ФЫКГб Ыутещыфб Ыштпфзщкуб Вусю 2019^ ззю 956ёг2013963юётх328ъ Ою Дшб Кю Ярфщб Яю Ьутпб Ню Дшгб Цю Цушб Ыю Зфкерфыфкфернб Мю ЬфяфдщмбётЯю Цфтпб Дю Руб Ыю Ярфщ уе фдюб ёг201сВумудщзштп КТТ-Е Ьщвуды ЫгкзфыыштпётРшпр-Зукащкьфтсу Рникшв Ьщвуды цшер Сгыещьшяфешщт Сфзфишдшенбёг201в штётЗкщсю Штеукызуусрб Ырфтпрфшб Срштф (мшкегфд)б Щсею 2020^ ззю 3590ёг2013ёт3594б фкЧшмЖ2007ю15188юётх329ъ Кю Рышфщб Вю Сфтб Ею Тпб Кю Екфмфвшб фтв Фю Прщырфдб ёг201сЩтдштуётФгещьфешс Ызууср Кусщптшешщт цшер Дшыеутб Фееутв фтв Ызудд Ьщвудбёг201вётШУУУ Ышптфд Зкщсуыыштп Дуееукыб мщдю 27^ ззю 1889ёг20131893б 2020юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт28ётх330ъ Ню Ыршб Ню Цфтпб Сю Цгб Сю-Аю Нурб Ою Срфтб Аю Ярфтпб Вю Дуб фтвётЬю Ыудеяукб ёг201сУьащкьукЖ Уаашсшуте Ьуьщкн Екфтыащкьук ифыув ФсщгыешсётЬщвуд ащк Дщц Дфеутсн Ыекуфьштп Ызууср Кусщптшешщтбёг201в шт Зкщсю ШУУУётШСФЫЫЗю Ещкщтещб Щтефкшщб СфтфвфЖ ШУУУб Огтю 2021^ ззю 6783ёг20136787юётх331ъ Чю Срутб Ню Цгб Яю Цфтпб Ыю Дшгб фтв Ою Дшб ёг201сВумудщзштп Куфд-ЕшьуётЫекуфьштп Екфтыащкьук Екфтывгсук ащк Ызууср Кусщптшешщт щт Дфкпуёг0002Ысфду Вфефыуебёг201в шт Зкщсю ШУУУ ШСФЫЫЗю Ещкщтещб Щтефкшщб СфтфвфЖётШУУУб Огтю 2021^ ззю 5904ёг20135908юётх332ъ Ею Тю Ыфштферб Ню Руб Ию Дшб Фю Тфкфнфтфтб Кю Зфтпб Фю ИкгпгшукбётЫю-ню Срфтпб Цю Дшб Кю Фдмфкуяб Яю Срутб Сю-Сю Сршгб Вю ПфксшфбётФю Пкгутыеуштб Лю Ргб Ью Оштб Фю Лфттфтб Йю Дшфтпб Шю ЬсПкфцбётСю Зуныукб Кю Зкфирфмфдлфкб Пю Згтвфлб Вю Книфсрб Ню ЫрфтппгфтбётНю Ыруерб Ею Ыекщрьфтб Ью Мшыщтефшб Ню Цгб Ню Ярфтпб фтв Вю Ярфщбётёг201сФ Ыекуфьштп Щт-Вумшсу Утв-Ещ-Утв Ьщвуд Ыгкзфыыштп Ыукмук-ЫшвуётСщтмутешщтфд Ьщвуд Йгфдшен фтв Дфеутснбёг201в шт Зкщсю ШУУУ ШСФЫЫЗбётИфксудщтфб Ызфштб ьфн 2020^ ззю 6059ёг20136063юётх333ъ Ию Дшб Фю Пгдфешб Ою Нгб Ею Тю Ыфштферб Сю-Сю Сршгб Фю ТфкфнфтфтбётЫю-Ню Срфтпб Кю Зфтпб Ню Руб Ою Йштб Цю Рфтб Йю Дшфтпб Ню ЯрфтпбётЕю Ыекщрьфтб фтв Ню Цгб ёг201сФ Иуееук фтв Афыеук Утв-ещ-Утв Ьщвуд ащкётЫекуфьштп ФЫКбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ещкщтещб Щтефкшщб СфтфвфбётОгтю 2021^ ззю 5634ёг20135638юётх334ъ Ею Тю Ыфштферб Ню Руб Фю Тфкфнфтфтб Кю Ищекщыб Кю Зфтпб Вю КнифсрбётСю Фддфгяутб Ую Мфкшфтшб Ою Йштб Йю-Тю Ду-Еруб Ыю-Ню Срфтпб Ию ДшбётФю Пгдфешб Ою Нгб Сю-Сю Сршгб Вю Сфыушкщб Цю Дшб Йю Дшфтпб фтвётЗю Кщтвщтб ёг201сФт Уаашсшуте Ыекуфьштп Тщт-Кусгккуте Щт-Вумшсу Утвёг0002ещ-Утв Ьщвуд цшер Шьзкщмуьутеы ещ Кфку-Цщкв Ьщвудштпбёг201в шт ЗкщсюётШтеукызуусрб Иктщб Сяусршфб Ыузю 2021^ ззю 1777ёг20131781юётх335ъ Фю Ифзтфб Ню-Фю Сргтпб Тю Цгб ^ Фю Пгдфешб Ню Ошфб Ою Рю СдфклбётЬю Ощртыщтб Ою Кшуыфб Фю Сщттуфгб фтв Ню Ярфтпб ёг201сЫДФЬЖ Ф ГтшашувётУтсщвук ащк Ызууср фтв Дфтпгфпу Ьщвудштп мшф Ызууср-Еуче ОщштеётЗку-Екфштштпбёг201в Щсею 2021^ фкЧшмЖ2110ю10329юётх336ъ Фю Ифзтфб Сю Сруккнб Ню Ярфтпб Ню Ошфб Ью Ощртыщтб Ню СрутпбётЫю Лрфтгофб Ою Кшуыфб фтв Фю Сщттуфгб ёг201сьЫДФЬЖ Ьфыышмудн Ьгдёг0002ешдштпгфд Ощште Зку-Екфштштп ащк Ызууср фтв Еучебёг201в Ауию 2022бётфкЧшмЖ2202ю01374юётх337ъ Ню Ефтпб Рю Пщтпб Тю Вщтпб Сю Цфпб Цю Рыгб Ою Пгб Фю Ифумылшб Чю ДшбётФю Ьщрфьувб Ью Фгдшб фтв Ою Зштщб ёг201сГтшашув Ызууср-Еуче Зку-екфштштпётащк Ызууср Екфтыдфешщт фтв Кусщптшешщтбёг201в шт Зкщсю ФСДб Вгидштб ШкудфтвбётЬфн 2022^ ззю 1488ёг20131499б фкЧшмЖ2204ю05409юётх338ъ Ню-Фю Сргтпб Сю Яргб фтв Ью Яутпб ёг201сЫЗДФЕЖ Ызууср-Дфтпгфпу ОщштеётЗку-Екфштштп ащк Ызщлут Дфтпгфпу Гтвукыефтвштпбёг201в шт Зкщсю ТФФСДбётОгтю 2021^ ззю 1897ёг20131907б фкЧшмЖ2010ю02295юётх339ъ Ою Фщб Кю Цфтпб Дю Ярщгб Сю Цфтпб Ыю Кутб Ню Цгб Ыю Дшгб Ею ЛщбётЙю Дшб Ню Ярфтпб Яю Цушб Ню Йшфтб Ою Дшб фтв Аю Цушб ёг201сЫзуусрЕ5ЖётГтшашув-Ьщвфд Утсщвук-Вусщвук Зку-Екфштштп ащк Ызщлут ДфтпгфпуётЗкщсуыыштпбёг201в шт Зкщсю ФСДб Вгидштб Шкудфтвб Ьфн 2022^ ззю 5723ёг20135738бётфкЧшмЖ2110ю07205юётх340ъ Ыю Ерщьфыб Рю Ою Лгщб Ию Лштпыигкнб фтв Пю Ыфщтб ёг201сЕщцфквы Кувгсштпётеру Туув ащк Ызууср Екфштштп Вфеф ещ Игшдв Ызщлут Дфтпгфпу Гтвукёг0002ыефтвштп Ыныеуьыбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Ыштпфзщкуб Ьфн 2022^ ззюёт7932ёг20137936б фкЧшмЖ2203ю00006юётх341ъ Ею Тю Ыфштферб Кю Зкфирфмфдлфкб Фю Ифзтфб Ню Ярфтпб Яю Ргщб Яю СрутбётИю Дшб Цю Цфтпб фтв Ею Ыекщрьфтб ёг201сОЩШЫЕЖ Ф ощште ызууср фтв еучеётыекуфьштп ьщвуд ащк ФЫКбёг201в шт Зкщсю ШУУУ ЫДЕб Вщрфб Йфефкб Офтю 2023бётфкЧшмЖ2210ю07353юётх342ъ Ею Рщкшб Кю Фыегвшддщб Ею Рфнфыршб Ню Ярфтпб Ыю Цфефтфиуб фтвётОю Ду Кщгчб ёг201сСнсду-Сщтышыеутсн Екфштштп ащк Утв-ещ-Утв ЫзуусрётКусщптшешщтбёг201в шт Зкщсю ШУУУ ШСФЫЫЗб Икшпрещтб ГЛб Ьфн 2019^ ззюёт6271ёг20136275юётх343ъ Ею Щсршфшб Ыю Цфефтфиуб Ею Рщкшб фтв Ою Кю Рукырунб ёг201сЬгдешсрфттудётУтв-ещ-Утв Ызууср Кусщптшешщтбёг201в шт Зкщсю ШСЬДю Ынвтунб ФгыекфдшфЖётЗЬДКб Фгпю 2017^ ззю 2632ёг20132641юётх344ъ Ою Дшб ёг201сКусуте Фвмфтсуы шт Утв-ещ-Утв Фгещьфешс Ызууср Кусщптшёг0002ешщтбёг201в ФЗЫШЗФ Екфтыю щт Ышптфд фтв Штащкьфешщт Зкщсуыыштпб мщдю 11бёттщю 1^ Тщмю 2021^ ВЩШЖ 10&1561/116&00000050^ фкЧшмЖ2111ю01690юётЗДФСУётЗРЩЕЩётРУКУётКщрше Зкфирфмфдлфк Кщрше Зкфирфмфдлфк кусушмувётршы ЗрВ шт Сщьзгеук Ысшутсу фтв Утпштуукштп акщьётЕру Щршщ Ыефеу Гтшмукышенб ГЫФб шт 2013& Ащддщцёг0002штп ршы ЗрВб Кщрше ощштув еру Ызууср Еусртщдщпшуыётпкщгз фе Пщщпду цруку ру шы сгккутедн ф Ыефаа Куёг0002ыуфкср Ысшутешыею Фе Пщщпдуб ршы куыуфкср рфы ащсгыувётзкшьфкшдн щт вумудщзштп сщьзфсе фсщгыешс ьщвудыётцршср сфт кгт уаашсшутедн щт ьщишду вумшсуыб фтв щтётвумудщзштп шьзкщмув утв-ещ-утв фгещьфешс ызуусрёткусщптшешщт ыныеуьыю Кщрше рфы сщ-фгерщкув щмук 50ёткуаукуув зфзукыб цршср рфму кусушмув ецщ иуые зфзукётфцфквы (ФЫКГ 2017* ШСФЫЫЗ 2018)& Ру сгккутедн ыукмуы фы ф ьуьиук ща еруётШУУУ Ызууср фтв Дфтпгфпу Зкщсуыыштп Еусртшсфд Сщььшееуу (2018ёг20132024)бётфтв фы фт Фыыщсшфеу Увшещк ща еру ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызуусрбётфтв Дфтпгфпу ЗкщсуыыштпюётЗДФСУётЗРЩЕЩётРУКУётЕфлффлш Рщкш кусушмув ршы ЗрВ вупкуу шт ыныеуьётфтв штащкьфешщт утпштуукштп акщь Нфьфпфеф Гтшёг0002мукышенб Нщтуяфцфб Офзфтб шт 1999& Акщь 1999 ещёт2015б ру рфв иуут утпфпув шт куыуфксруы щт ызуусрёткусщптшешщт фтв ызщлут дфтпгфпу зкщсуыыштп фе Снёг0002иук Ызфсу Дфищкфещкшуы фтв Сщььгтшсфешщт ЫсшутсуётДфищкфещкшуы шт Тшззщт Еудупкфзр фтв Еудузрщтуёт(ТЕЕ) Сщкзщкфешщтб Офзфтю Акщь 2015 ещ 2021бётру цфы ф Ыутшщк Зкштсшзфд Куыуфкср Ысшутешые феётЬшеыгишырш Удусекшс Куыуфкср Дфищкфещкшуы (ЬУКД)бётГЫФю Ру шы сгккутедн ф Ьфсршту Дуфктштп Куёг0002ыуфксрук фе Фзздую Ршы куыуфкср штеукуыеы штсдгву фгещьфешс ызууср кусщптшешщтбётызщлут дфтпгфпу гтвукыефтвштпб фтв дфтпгфпу ьщвудштпю Ру ыукмув фы фётьуьиук ща еру ШУУУ Ызууср фтв Дфтпгфпу Зкщсуыыштп Еусртшсфд Сщььшееууёт(2020ёг20132022)юётЗДФСУётЗРЩЕЩётРУКУётЕфкф Ыфштфер кусушмув рук ЗрВ шт Удусекшсфд Утпшёг0002туукштп фтв Сщьзгеук Ысшутсу акщь ЬШЕ шт 2009юётЕру ьфшт ащсгы ща рук ЗрВ цщкл цфы шт фсщгыешсётьщвудштп ащк тщшыу кщигые ызууср кусщптшешщтю Фаеукётрук ЗрВб ыру ызуте 5 нуфкы фе еру Ызууср фтвётДфтпгфпу Фдпщкшерьы пкщгз фе ШИЬ ЕюОю Цфеыщт Куёг0002ыуфкср Сутеукб иуащку ощштштп Пщщпду Куыуфксрю Ыруётрфы ыукмув фы ф Зкщпкфь Срфшк ащк ШСДК шт 2017 фтвёт2018ю Фдыщб ыру рфы сщ-щкпфтшяув тгьукщгы ызусшфдётыуыышщты фтв цщклырщзыб штсдгвштп Штеукызууср 2010бётШСЬД 2013^ Штеукызууср 2016 фтв ШСЬД 2017& Штётфввшешщтб ыру шы ф ьуьиук ща еру ШУУУ Ызууср фтв Дфтпгфпу ЗкщсуыыштпётЕусртшсфд Сщььшееуу (ЫДЕС) фы цудд фы еру Фыыщсшфеу Увшещк ащк ШУУУ/ФСЬётЕкфтыфсешщты щт Фгвшщб Ызуусрб фтв Дфтпгфпу ЗкщсуыыштпюётЗДФСУётЗРЩЕЩётРУКУётКфда Ысрдгеук ёг00ф8 Кфда Ысрдгеук кусушмув ршы Вкюкукютфею ёг00ф8ётвупкуу шт Сщьзгеук Ысшутсу шт 2000 фтв рфишдшефеувётшт Сщьзгеук Ысшутсу шт 2019^ ищер фе КЦЕР ФфсрутётГтшмукышеню Шт Ьфн 1996^ Кфда Ысрдгеук ощштув еру ёг00ф8ётСщьзгеук Ысшутсу Вузфкеьуте фе КЦЕР ФфсрутётГтшмукышенб цруку ру сгккутедн шы Дусегкук фтвётФсфвуьшс Вшкусещкб дуфвштп еру Фгещьфешс ЫзуусрётКусщптшешщт Пкщгз фе еру Срфшк Сщьзгеук Ысшутсуёт6 ёг2013 Ьфсршту Дуфктштп фтв Ргьфт Дфтпгфпу Еусрёг0002тщдщпню Шт 2019^ Кфда фдыщ ощштув ФззЕул ПьиРётФфсрут фы Ыутшщк Куыуфксрукю Ршы куыуфкср штеукуыеыётсщмук ыуйгутсу сдфыышашсфешщтб ызусшашсфддн фдд фызусеы ща фгещьфешс ызуусрёткусщптшешщтб вусшышщт ерущкнб ыещсрфыешс ьщвудштпб фтв ышптфд фтфднышыю Кфдаётыукмув фы Ыгиоусе Увшещк ащк Ызууср Сщььгтшсфешщт (2013-2019)юётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/ёт29ётЗДФСУётЗРЩЕЩётРУКУётЫрштош Цфефтфиу шы фт Фыыщсшфеу Зкщауыыщк феётСфктупшу Ьуддщт Гтшмукышенб Зшееыигкпрб ЗФю Ру куёг0002сушмув ршы ИюЫюб ЬюЫюб фтв ЗрюВю (Вкю Утпю) вупкууыётакщь Цфыувф Гтшмукышенб Ещлнщб Офзфтю Ру цфы фёткуыуфкср ысшутешые фе ТЕЕ Сщььгтшсфешщт ЫсшутсуётДфищкфещкшуыб Лнщещб Офзфтб акщь 2001 ещ 2011^ фётмшышештп ысрщдфк фе Пущкпшф штыешегеу ща еусртщдщпнбётФедфтефб ПФб шт 2009^ фтв ф ыутшщк зкштсшзфд куыуфксрётысшутешые фе Ьшеыгишырш Удусекшс Куыуфкср Дфищкфещёг0002кшуы (ЬУКД)б Сфьикшвпуб ЬФ ГЫФ акщь 2012 ещёт2017ю Иуащку Сфктупшу Ьуддщт Гтшмукышенб ру цфыётфт фыыщсшфеу куыуфкср зкщауыыщк фе Ощрты Рщзлшты Гтшмукышенб ИфдешьщкубётЬВб ГЫФб акщь 2017 ещ 2020& Ршы куыуфкср штеукуыеы штсдгву фгещьфешсётызууср кусщптшешщтб ызууср утрфтсуьутеб ызщлут дфтпгфпу гтвукыефтвштпб фтвётьфсршту дуфктштп ащк ызууср фтв дфтпгфпу зкщсуыыштпю Ру шы фт ШУУУ фтвётШЫСФ АуддщцюётЕршы фкешсду рфы иуут фссузеув ащк згидшсфешщт шт ШУУУ/ФСЬ Екфтыфсешщты щт Фгвшщб Ызууср фтв Дфтпгфпу Зкщсуыыштпю Ершы шы еру фгерщкэы мукышщт цршср рфы тще иуут агддн увшеув фтвётсщтеуте ьфн срфтпу зкшщк ещ аштфд згидшсфешщтю Сшефешщт штащкьфешщтЖ ВЩШ 10ю1109/ЕФЫДЗю2023ю3328283ётЕршы цщкл шы дшсутыув гтвук ф Скуфешму Сщььщты Феекшигешщт 4&0 Дшсутыую Ащк ьщку штащкьфешщтб ыуу реезыЖ//скуфешмусщььщтыющкп/дшсутыуы/ин/4ю0/Эб
g Attention Based Sequence-to-Sequence Models for End-to\u0002End English Conversational Speech Recognition,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 761\u2013765.\n[83] D. Le, X. Zhang, W. Zheng, C. Fugen, G. Zweig, and M. L. Seltzer, \u00a8\n\u201cFrom Senones to Chenones: Tied Context-Dependent Graphemes for\nHybrid Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 457\u2013464.\n[84] S. Kanthak and H. Ney, \u201cContext-Dependent Acoustic Modeling Using\nGraphemes for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 845\u2013848.\n[85] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., \u201cIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit,\u201d in Proc. of\nthe 44th Annual International Symposium on Computer Architecture,\nToronto, Ontario, Canada, Jun. 2017, pp. 1\u201312.\n[86] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, \u201cHybrid\nCTC Attention Architecture for End-to-End Speech Recognition,\u201d\nIEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8,\npp. 1240\u20131253, 2017.\n[87] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C. Chung\u0002Cheng, \u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech, Brighton, UK, May 2019, pp. 2773\u20132777.\n[88] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, \u201cDeliberation\nModel Based Two-Pass End-to-End Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 7799\u20137803.\n[89] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab\u0002havalkar, E. Variani, and T. Strohman, \u201cCascaded Encoders for Uni\u0002fying Streaming and Non-Streaming ASR,\u201d in Proc. IEEE ICASSP,\nToronto, Ontario, Canada, Jun. 2021, pp. 5629\u20135633.\n[90] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, \u201cTransformer Trans\u0002ducer: One Model Unifying Streaming and Non-Streaming Speech\nRecognition,\u201d Oct. 2020, arXiv:2010.03192.\n[91] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu, and\nR. Pang, \u201cUniversal ASR: Unify and Improve Streaming ASR with\nFull-Context Modeling,\u201d Oct. 2020, arXiv:2010.06030.\n[92] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, and\nR. Pang, \u201cShallow-Fusion End-to-End Contextual Biasing,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 1418\u20131422.\n[93] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao,\n\u201cDeep Context: End-to-end Contextual Speech Recognition,\u201d in Proc.\nIEEE SLT, Athens, Greece, Dec. 2018, pp. 418\u2013425.\n[94] S. Kim and F. Metze, \u201cDialog-Context Aware End-to-End Speech\nRecognition,\u201d in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 434\u2013\n440.\n[95] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, \u201cPhoebe:\nPronunciation-Aware Contextualization for End-to-End Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6171\u2013\n6175.\n[96] M. Delcroix, S. Watanabe, A. Ogawa, S. Karita, and T. Nakatani,\n\u201cAuxiliary Feature Based Adaptation of End-to-End ASR Systems,\u201d\nin Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2444\u20132448.\n[97] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextNet: Improving Convolutional Neural\nNetworks for Automatic Speech Recognition with Global Context,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 3610\u20133614.\n[98] L. Dong, S. Xu, and B. Xu, \u201cSpeech-Transformer: A No-Recurrence\nSequence-to-Sequence Model for Speech Recognition,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5884\u20135888.\n[99] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer Transducer: A Streamable Speech Recognition\nModel with Transformer Encoders and RNN-T Loss,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7829\u20137833.\n[100] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain,\nK. Schubert, C. Fuegen, and M. L. Seltzer, \u201cTransformer-Transducer:\nEnd-to-Snd Speech Recognition with Self-Attention,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 7829\u20137833.\n[101] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, \u201cBranchformer: Parallel\nMLP-Attention Architectures to Capture Local and Global Context for\nSpeech Recognition and Understanding,\u201d in Proc. ICML. Baltimore,\nMD: PMLR, Jul. 2022, pp. 17 627\u201317 643.\n[102] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang, \u201cA Comparative Study on Transformer\nvs RNN in Speech Applications,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 449\u2013456.\n[103] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,\nN. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei,\nW. Zhang, and Y. Zhang, \u201cRecent Developments on ESPNET Toolkit\nBoosted by Conformer,\u201d in Proc. IEEE ICASSP. Toronto, Ontario,\nCanada: IEEE, Jun. 2021, pp. 5874\u20135878.\n[104] R. Botros, T. Sainath, R. David, E. Guzman, W. Li, and Y. He, \u201cTied &\nReduced RNN-T Decoder,\u201d in Proc. Interspeech, Brno, Czechia, Sep.\n2021, pp. 4563\u20134567.\n[105] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, \u201cRNN\u0002Transducer with Stateless Prediction Network,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7049\u20137053.\n[106] W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cPhoneme Based Neural \u00a8\nTransducer for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5644\u20135648.\n[107] R. Prabhavalkar, Y. He, D. Rybach, S. Campbell, A. Narayanan,\nT. Strohman, and T. N. Sainath, \u201cLess is More: Improved RNN-T\nDecoding Using Limited Label Context and Path Merging,\u201d in Proc.\nIEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5659\u20135663.\n[108] X. Chen, Z. Meng, S. Parthasarathy, and J. Li, \u201cFactorized Neural\nTransducer for Efficient Language Model Adaptation,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8132\u20138136, arXiv:2110.01500.\n[109] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi,\nJ. Emond, T. Strohman, B. Ramabhadran, W. R. Huang et al., \u201cModular\nHybrid Autoregressive Transducer,\u201d in Proc. IEEE SLT, Doha, Qatar,\nJan. 2023, pp. 197\u2013204, https://arXiv:2210.17049.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[110] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and\nF. Wei, \u201cVioLA: Unified Codec Language Models for Speech Recog\u0002nition, Synthesis, and Translation,\u201d May 2023, arXiv:2305.16107.\n[111] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor\u0002sos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov\net al., \u201cAudioPaLM: A Large Language Model That Can Speak and\nListen,\u201d Jun. 2023, arXiv:2306.12925.\n[112] S.-Y. Chang, B. Li, and G. Simko, \u201cA Unified Endpointer Using\nMultitask and Multidomain Training,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 100\u2013106.\n[113] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and\nY. Wu, \u201cTowards Fast and Accurate Streaming End-To-End ASR,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6069\u20136073.\n[114] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, \u201cEnd-To\u0002End Automatic Speech Recognition Integrated with CTC-Based Voice\nActivity Detection,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6999\u20137003.\n[115] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, \u201cToward Stream\u0002ing ASR with Non-Autoregressive Insertion-Based Model,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3740\u20133744.\n[116] Y. Bengio, \u201cPractical Recommendations for Gradient-Based Training\nof Deep Architectures,\u201d Jun. 2012, arXiv:1206.5533.\n[117] J. Schmidhuber, \u201cDeep Learning in Neural Networks: An Overview,\u201d\nNeural Networks, vol. 61, pp. 85\u2013117, Jan. 2015, arXiv:1404.7828.\n[118] L. Baum, \u201cAn Inequality and Associated Maximization Technique in\nStatistical Estimation for Probabilistic Functions of Markov Processes,\u201d\nInequalities, vol. 3, pp. 1\u20138, 1972.\n[119] L. Rabiner and B.-H. Juang, \u201cAn Introduction to Hidden Markov Mod\u0002els,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing,\nvol. 3, no. 1, pp. 4\u201316, 1986.\n[120] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, \u201cNeural Network\u0002Gaussian Mixture Hybrid for Speech Recognition or Density Estima\u0002tion,\u201d in Proc. NIPS, vol. 4, Colorado, Dec. 1991, pp. 175\u2013182.\n[121] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton\nUniversity Press, 1957.\n[122] A. Viterbi, \u201cError Bounds for Convolutional Codes and an Asymptoti\u0002cally Optimal Decoding Algorithm,\u201d IEEE Transactions on Information\nTheory, vol. 13, pp. 260\u2013269, 1967.\n[123] H. Ney, \u201cThe Use of a One-Stage Dynamic Programming Algorithm\nfor Connected Word Recognition,\u201d IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, vol. 32, no. 2, pp. 263\u2013271, 1984.\n[124] W. Zhou, W. Michel, R. Schluter, and H. Ney, \u201cEfficient Training \u00a8\nof Neural Transducer for Speech Recognition,\u201d in Proc. Interspeech,\nIncheon, Korea, Sep. 2022, arXiv:2204.10586.\n[125] A. Zeyer, R. Schluter, and H. Ney, \u201cWhy does CTC Result in Peaky \u00a8\nBehavior?\u201d May 2021, arXiv:2105.14849.\n[126] A. Laptev, S. Majumdar, and B. Ginsburg, \u201cCTC Variations Through\nNew WFST Topologies,\u201d in Proc. Interspeech, Incheon, Korea, sep\n2022, DOI: 10.21437/interspeech.2022-10854.\n[127] X. He, L. Deng, and W. Chou, \u201cDiscriminative Learning in Sequential\nPattern Recognition \u2013 A Unifying Review for Optimization-Oriented\nSpeech Recognition,\u201d IEEE Signal Processing Magazine, vol. 25, no. 5,\npp. 14\u201336, 2008.\n[128] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schluter, and \u00a8\nH. Ney, \u201cInvestigating Methods to Improve Language Model Inte\u0002gration for Attention-Based Encoder-Decoder ASR Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Aug. 2021, pp. 2856\u20132860.\n[129] N.-P. Wynands, W. Michel, J. Rosendahl, R. Schluter, and H. Ney, \u00a8\n\u201cEfficient Sequence Training of Attention Models using Approxima\u0002tive Recombination,\u201d in Proc. IEEE ICASSP, Singapore, May 2022,\narXiv:2110.09245.\n[130] Z. Yang, W. Zhou, R. Schluter, and H. Ney, \u201cLattice-Free Sequence \u00a8\nDiscriminative Training for Phoneme-based Neural Transducers,\u201d in\nProc. IEEE ICASSP, Rhodes, Greece, Jun. 2023, arXiv:2212.04325.\n[131] V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young, \u201cMMIE\nTraining of Large Vocabulary Recognition Systems,\u201d Speech Commu\u0002nication, vol. 22, no. 4, pp. 303\u2013314, 1997.\n[132] D. Povey and P. Woodland, \u201cImproved Discriminative Training Tech\u0002niques for Large Vocabulary Continuous Speech Recognition,\u201d in Proc.\nIEEE ICASSP, Salt Lake City, UT, May 2001, pp. 45\u201348.\n[133] R. Schluter, W. Macherey, B. M \u00a8 uller, and H. Ney, \u201cComparison of \u00a8\nDiscriminative Training Criteria and Optimization Methods for Speech\nRecognition,\u201d Speech Communication, vol. 34, no. 3, pp. 287\u2013310, May\n2001, EURASIP Best Paper Award.\n[134] B. Kingsbury, \u201cLattice-Based Optimization of Sequence Classifica\u0002tion Criteria for Neural-Network Acoustic Modeling,\u201d in Proc. IEEE\nICASSP, Taipei, Taiwan, Apr. 2009, pp. 3761\u20133764.\n[135] G. Heigold, R. Schluter, H. Ney, and S. Wiesler, \u201cDiscriminative \u00a8\nTraining for Automatic Speech Recognition: Modeling, Criteria, Opti\u0002mization, Implementation, and Performance,\u201d IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 58\u201369, Nov. 2012.\n[136] W. Michel, R. Schluter, and H. Ney, \u201cComparison of Lattice-Free and \u00a8\nLattice-Based Sequence Discriminative Training Criteria for LVCSR,\u201d\nin Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1601\u20131605,\narXiv:1907.01409.\n[137] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu,\nand A. Kannan, \u201cMinimum Word Error Rate Training for Attention\u0002Based Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4839\u20134843.\n[138] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, \u201cMinimum Bayes Risk\nTraining of RNN-Transducer for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 966\u2013970, DOI:\n10.21437/Interspeech.2020-1221.\n[139] M. K. Baskar, L. Burget, S. Watanabe, M. Karafiat, T. Hori, and \u00b4\nJ. H. Cernock \u02c7 y, \u201cPromising Accurate Prefix Boosting for Sequence- `\nto-Sequence ASR,\u201d in Proc. IEEE ICASSP. Brighton, UK: IEEE,\nMay 2019, pp. 5646\u20135650.\n[140] A. Tjandra, S. Sakti, and S. Nakamura, \u201cSequence-to-Sequence ASR\nOptimization via Reinforcement Learning,\u201d in Proc. IEEE ICASSP.\nCalgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5829\u20135833.\n[141] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, \u201cSequence Training\nof Encoder-Decoder Model Using Policy Gradient for End-to-End\nSpeech Recognition,\u201d in Proc. IEEE ICASSP. Calgary, Alberta,\nCanada: IEEE, Apr. 2018, pp. 5839\u20135843.\n[142] W. Michel, R. Schluter, and H. Ney, \u201cEarly Stage LM Integration Using \u00a8\nLocal and Global Log-Linear Combination,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 3605\u20133609, arXiv:2005.10049.\n[143] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA Fast Learning Algorithm\nfor Deep Belief Nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u2013\n1554, Jul. 2006.\n[144] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy Layer\u0002Wise Training of Deep Networks,\u201d in Proc. NIPS, Barcelona, Spain,\nDec. 2006, pp. 153\u2013160.\n[145] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schluter, and H. Ney, \u00a8\n\u201cA Comprehensive Study of Deep Bidirectional LSTM RNNs for\nAcoustic Modeling in Speech Recognition,\u201d in Proc. IEEE ICASSP,\nNew Orleans, LA, Mar. 2017, pp. 2462\u20132466.\n[146] A. Zeyer, T. Alkhouli, and H. Ney, \u201cRETURNN as a Generic Flexible\nNeural Toolkit with Application to Translation and Speech Recogni\u0002tion,\u201d in Proc. ACL, Melbourne, Australia, Jul. 2018, pp. 128\u2013133.\n[147] A. Zeyer, K. Irie, R. Schluter, and H. Ney, \u201cImproved Training \u00a8\nof End-to-End Attention Models for Speech Recognition,\u201d in Proc.\nInterspeech, Hyderabad, India, Sep. 2018, pp. 7\u201311.\n[148] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA Comprehensive \u00a8\nAnalysis on Attention Models,\u201d in Proc. NIPS, Montreal, Canada, Dec.\n2018.\n[149] Y. Chung, C. Wu, C. Shen, H. Lee, and L. Lee, \u201cAudio Word2Vec:\nUnsupervised Learning of Audio Segment Representations using\nSequence-to-sequence Autoencoder,\u201d in Proc. Interspeech, San Fran\u0002cisco, CA, Sep. 2016, arXiv:1603.00982.\n[150] Y.-C. Chen, S.-F. Huang, H.-y. Lee, Y.-H. Wang, and C.-H. Shen, \u201cAu\u0002dio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised\nLearning of Audio Segmentation and Representation,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 27,\nno. 9, pp. 1481\u20131493, 2019, DOI: 10.1109/TASLP.2019.2922832.\n[151] S. Scanzio, P. Laface, L. Fissore, R. Gemello, and F. Mana, \u201cOn the\nUse of a Multilingual Neural Network Front-End,\u201d in Proc. Interspeech,\nBrisbane, Australia, Sep. 2008, pp. 2711\u20132714.\n[152] Z. Tuske, J. Pinto, D. Willett, and R. Schl \u00a8 uter, \u201cInvestigation on \u00a8\nCross- and Multilingual MLP features under matched and mismatched\nacoustical conditions,\u201d in IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, Vancouver, Canada, May 2013, pp.\n7349\u20137353.\n[153] S. Zhou, S. Xu, and B. Xu, \u201cMultilingual End-to-End Speech Recog\u0002nition with a Single Transformer on Low-Resource Languages,\u201d Jun.\n2018, arXiv:1806.05059.\n[154] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL-HLT,\nMinneapolis, MN, Jun. 2019, arXiv:1904.02210.\n[155] W. Hou, Y. Dong, B. Zhuang, L. Yang, J. Shi, and T. Shinozaki,\n\u201cLarge-scale end-to-end multilingual speech recognition and language\nidentification with multi-task learning,\u201d in Proc. Interspeech, Shanghai,\nChina, Oct. 2020, pp. 1037\u20131041.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\n[156] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Syn\u0002naeve, and R. Collobert, \u201cMassively Multilingual ASR: 50 Languages,\n1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, Shanghai, China,\nOct. 2020, arXiv:2007.03001.\n[157] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani,\nW. R. Huang, M. Ma, and J. Bai, \u201cScaling End-to-End Models for\nLarge-Scale Multilingual ASR,\u201d in Proc. IEEE ASRU, 2021, pp. 1011\u2013\n1018.\n[158] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen,\nY. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li, M. Ma, W. Chan,\nJ. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath,\nF. Beaufays, Z. Chen, Q. V. Le, C.-C. Chiu, R. Pang, and Y. Wu,\n\u201cBigSSL: Exploring the frontier of large-scale semi-supervised learning\nfor automatic speech recognition,\u201d IEEE Journal of Selected Topics\nin Signal Processing, vol. 16, no. 6, pp. 1519\u20131532, oct 2022,\narXiv:2109.13226.\n[159] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Repre\u0002sentations through Modality Matching,\u201d in Proc. Interspeech, Incheon,\nSouth Korea, Sep. 2022, arXiv:2204.03409.\n[160] A. Radford, J. W. Kim, C. McLeavey, P. Mishkin, T. Xu, G. Brockman,\nand I. Sutskever, \u201cIntroducing Whisper - Robust Speech Recognition\nvia Large-Scale Weak Supervision,\u201d Sep. 2022. [Online]. Available:\nhttps://openai.com/blog/whisper/\n[161] T. P. Vogl, J. Mangis, A. Rigler, W. Zink, and D. Alkon, \u201cAcceler\u0002ating the Convergence of the Back-Propagation Method,\u201d Biological\nCybernetics, vol. 59, no. 4, pp. 257\u2013263, 1988.\n[162] N. S. Keskar and G. Saon, \u201cA Nonmonotone Learning Rate Strategy\nfor SGD Training of Deep Neural Networks,\u201d in Proc. IEEE ICASSP.\nQueensland, Australia: IEEE, Apr. 2015, pp. 4974\u20134978.\n[163] S. Renals, N. Morgan, H. Bourlard, C. Wooters, and P. Kohn, \u201cConnec\u0002tionist Speech Recognition: Status and Prospects,\u201d ICSI, 1991, Tech.\nRep. TR-OI-070.\n[164] D. Johnson, D. Ellis, C. Oei, C. Wooters, and P. Faerber, \u201cQuickNet,\u201d\nICSI, Berkeley, 2004. [Online]. Available: http://www.icsi.berkeley.\nedu/Speech/qn.html\n[165] A. Senior, G. Heigold, M. Ranzato, and K. Yang, \u201cAn Empirical Study\nof Learning Rates in Deep Neural Networks for Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Vancouver, BC, Canada: IEEE, May 2013,\npp. 6724\u20136728.\n[166] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d\nin Proc. ICLR, New Orleans, LA, May 2019, arXiv:1711.05101.\n[167] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, \u201cDon\u2019t Decay the\nLearning Rate, Increase the Batch Size,\u201d in Proc. ICLR, New Orleans,\nLA, May 2018, arXiv:1711.00489.\n[168] J. Howard and S. Ruder, \u201cUniversal Language Model Fine-Tuning for\nText Classification,\u201d in Proc. ACL, Melbourne, Australia, Jun. 2018,\npp. 328\u2013339.\n[169] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fer\u0002nando, and K. Kavukcuoglu, \u201cPopulation Based Training of Neural\nNetworks,\u201d Nov. 2017, arXiv:1711.09846.\n[170] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta\u0002Learning in Neural Networks: A Survey,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PP, pp. 1\u201320, 2021.\n[171] J. L. Elman, \u201cLearning and Development in Neural Networks: The\nImportance of Starting Small,\u201d Cognition, vol. 48, no. 1, pp. 71\u201399,\n1993, DOI: 10.1016/0010-0277(93)90058-4.\n[172] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum\nLearning,\u201d in Proc. ICML, Montreal, Quebec, Canada, Jun. 2009, p.\n41\u201348.\n[173] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan\u0002zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen,\nJ. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley,\nL. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh,\nD. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao,\nD. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep Speech 2: End-to-End Speech\nRecognition in English and Mandarin,\u201d in Proc. ICML, New York City,\nNY, Jun. 2016, pp. 173\u2013182.\n[174] Z. Tuske, G. Saon, K. Audhkhasi, and B. Kingsbury, \u201cSingle Headed \u00a8\nAttention Based Sequence-to-Sequence Model for State-of-the-Art\nResults on Switchboard,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 551\u2013555.\n[175] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, \u201cImproving End\u0002to-End Single-Channel Multi-Talker Speech Recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 28, pp. 1385\u2013\n1394, 2020.\n[176] B. Polyak, \u201cSome Methods of Speeding up the Convergence of\nIteration Methods,\u201d USSR Computational Mathematics and Mathe\u0002matical Physics, vol. 4, no. 5, pp. 1\u201317, 1964, DOI: 10.1016/0041-\n5553(64)90137-5.\n[177] Y. Nesterov, \u201cA method of solving a convex programming problem\nwith convergence rate O( 1\nk2\n),\u201d Soviet Mathematics Doklady, vol. 27,\npp. 372\u2013376, 1983.\n[178] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, \u201cOn the Importance\nof Initialization and Momentum in Deep Learning,\u201d in Proc. ICML,\nAtlanta, GA, Jun. 2013, pp. 1139\u20131147.\n[179] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimiza\u0002tion,\u201d in Proc. ICLR, San Diego, CA, May 2015, arXiv:1412.6980.\n[180] Z. Tuske, G. Saon, and B. Kingsbury, \u201cOn the Limit of English Con- \u00a8\nversational Speech Recognition,\u201d in Proc. Interspeech, Brno, Czechia,\nSep. 2021, pp. 2062\u20132066.\n[181] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever,\n\u201cDeep Double Descent: Where Bigger Models and More Data Hurt,\u201d\nin Proc. ICLR, virtual, Apr. 2020, arXiv:1912.02292.\n[182] A. Krogh and J. Hertz, \u201cA Simple Weight Decay Can Improve\nGeneralization,\u201d in Neural Information Processing Systems (NIPS),\nDenver, CO, Dec. 1991, pp. 950\u2013957.\n[183] A. F. Murray and P. J. Edwards, \u201cEnhanced MLP Performance and\nFault Tolerance Resulting from Synaptic Weight Noise during Train\u0002ing,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 5, pp. 792\u2013\n802, Sep. 1994.\n[184] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nAdvances in Neural Information Processing Systems, vol. 24, 2011.\n[185] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach,\nand J. Martens, \u201cAdding Gradient Noise Improves Learning for Very\nDeep Networks,\u201d Nov. 2015, arXiv:1511.06807.\n[186] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, \u201cImproving Neural Networks by Preventing Co\u0002Adaptation of Feature Detectors,\u201d Jul. 2012, arXiv:1207.0580.\n[187] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification\nwith Deep Convolutional Neural Networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), vol. 25, Lake Tahoe, NV, Dec.\n2012.\n[188] Y. Gal and Z. Ghahramani, \u201cDropout as a Bayesian Approximation:\nRepresenting Model Uncertainty in Deep Learning,\u201d in Proc. ICML,\nNew York City, NY, Jun. 2016, pp. 1050\u20131059.\n[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep Net\u0002works with Stochastic Depth,\u201d in European Conference on Computer\nVision, Amsterdam, Netherlands, Oct. 2016, pp. 646\u2013661.\n[190] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Muller, and A. Waibel, \u00a8\n\u201cVery Deep Self-Attention Networks for End-to-End Speech Recogni\u0002tion,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 66\u201370.\n[191] J. Lee and S. Watanabe, \u201cIntermediate Loss Regularization for CTC\u0002Based Speech Recognition,\u201d in Proc. IEEE ICASSP, Toronto, Ontario,\nCanada, Jun. 2021, pp. 6224\u20136228.\n[192] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \u201cRegularization\nof Neural Networks using DropConnect,\u201d in Proc. ICML, 2013, pp.\n1058\u20131066.\n[193] D. Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, \u00b4\nA. Goyal, Y. Bengio, A. Courville, and C. Pal, \u201cZoneout: Regularizing\nRNNs by Randomly Preserving Hidden Activations,\u201d in Proc. ICLR,\nToulon, France, Apr. 2017, arXiv:1606.01305.\n[194] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink\u0002ing the Inception Architecture for Computer Vision,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition, Las Vegas, NV, Jun. 2016,\npp. 2818\u20132826.\n[195] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled Sampling\nfor Sequence Prediction with Recurrent Neural Networks,\u201d Proc. NIPS,\nvol. 28, Dec. 2015.\n[196] T. Trinh, A. Dai, T. Luong, and Q. Le, \u201cLearning Longer-Term Depen\u0002dencies in RNNs with Auxiliary Losses,\u201d in Proc. ICML, Stockholm,\nSweden, Jul. 2018, pp. 4965\u20134974.\n[197] R. J. Williams and J. Peng, \u201cAn Efficient Gradient-Based Algorithm\nfor On-Line Training of Recurrent Network Trajectories,\u201d IEEE Neural\nComputation, vol. 2, no. 4, pp. 490\u2013501, 1990.\n[198] S. Merity, N. S. Keskar, and R. Socher, \u201cAn Analysis of Neural\nLanguage Modeling at Multiple Scales,\u201d Mar. 2018, arXiv:1803.08240.\n[199] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, \u201cMixSpeech:\nData Augmentation for Low-resource Automatic Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021,\npp. 7008\u20137012.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n25\n[200] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift,\u201d in Proc.\nICML, Lille, France, Jul. 2015, pp. 448\u2013456.\n[201] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic Spectral Distortion for\nLow Resource Speech Recognition with Deep Neural Networks,\u201d in\nProc. IEEE ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 309\u2013\n314.\n[202] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmentation\nfor Speech Recognition,\u201d in Proc. Interspeech, Dresden, Germany, Sep.\n2015.\n[203] N. Jaitly and G. E. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nImproves Speech Recognition,\u201d in Proc. ICML, vol. 117, Jun. 2013,\np. 21.\n[204] G. Saon, Z. Tuske, K. Audhkhasi, and B. Kingsbury, \u201cSequence Noise \u00a8\nInjected Training for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Brighton, England, May 2019, pp. 6261\u20136265.\n[205] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le,\nand Y. Wu, \u201cSpecAugment on Large Scale Datasets,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 6879\u20136883.\n[206] C. Wang, Y. Wu, Y. Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao, and\nM. Zhou, \u201cSemantic Mask for Transformer Based End-to-End Speech\nRecognition,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n971\u2013975.\n[207] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo,\nand K. Takeda, \u201cBack-Translation-Style Data Augmentation for End\u0002to-End ASR,\u201d in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018,\npp. 426\u2013433.\n[208] N. Rossenbach, M. Zeineldeen, B. Hilmes, R. Schluter, and H. Ney, \u00a8\n\u201cComparing the Benefit of Synthetic Training Data for Various Au\u0002tomatic Speech Recognition Architectures,\u201d in Proc. IEEE ASRU,\nCartagena, Colombia, Dec. 2021, arXiv:2104.05379.\n[209] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen, and\nC.-C. Chiu, \u201cNo Need for a Lexicon? Evaluating the Value of\nthe Pronunciation Lexica in End-to-End Models,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5859\u20135863, DOI:\n10.1109/ICASSP.2018.8462380.\n[210] C. Wooters and A. Stolcke, \u201cMultiple-Pronunciation Lexical Modeling\nin a Speaker Independent Speech Understanding System,\u201d in Proc.\nICSLP, Yokohama, Japan, Sep. 1994, pp. 1363\u20131366.\n[211] I. McGraw, I. Badr, and J. R. Glass, \u201cLearning Lexicons From Speech\nUsing a Pronunciation Mixture Model,\u201d IEEE/ACM Trans. Audio,\nSpeech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2012.\n[212] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGMM-Free DNN\nAcoustic Model Training,\u201d in Proc. IEEE ICASSP, Florence, Italy, May\n2014, pp. 5602\u20135606, DOI: 10.1109/ICASSP.2014.6854675.\n[213] G. Gosztolya, T. Grosz, and L. T \u00b4 oth, \u201cGMM-Free Flat Start Sequence- \u00b4\nDiscriminative DNN Training,\u201d in Proc. Interspeech, N. Morgan,\nEd. San Francisco, CA: ISCA, Sep. 2016, pp. 3409\u20133413, DOI:\n10.21437/Interspeech.2016-391.\n[214] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cFlat-Start\nSingle-Stage Discriminatively Trained HMM-Based Models for ASR,\u201d\nIEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 26,\nno. 11, pp. 1949\u20131961, 2018.\n[215] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig,\n\u201cThe IBM 2004 Conversational Telephony System for Rich Transcrip\u0002tion,\u201d in Proc. IEEE ICASSP, Philadelphia, PA, Mar. 2005, pp. 205\u2013\n208.\n[216] H. Hadian, D. Povey, H. Sameti, J. Trmal, and S. Khudanpur,\n\u201cImproving LF-MMI Using Unconstrained Supervisions for ASR,\u201d\nin Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 43\u201347, DOI:\n10.1109/SLT.2018.8639684.\n[217] N. Kanda, Y. Fujita, and K. Nagamatsu, \u201cLattice-Free State-Level Min\u0002imum Bayes Risk Training of Acoustic Models,\u201d in Proc. Interspeech,\nB. Yegnanarayana, Ed. Hyderabad, India: ISCA, Sep. 2018, pp. 2923\u2013\n2927, DOI: 10.21437/Interspeech.2018-79.\n[218] S. J. Young and P. C. Woodland, \u201cThe Use of State Tying in Continuous\nSpeech Recognition,\u201d in Proc. Eurospeech, Berlin, Germany, Dec.\n1993, pp. 2203\u20132206.\n[219] S. Wiesler, G. Heigold, M. Nu\u00dfbaum-Thom, R. Schluter, and H. Ney, \u00a8\n\u201cA Discriminative Splitting Criterion for Phonetic Decision Trees,\u201d\nin Proc. Interspeech, Makuhari, Japan, Sep. 2010, pp. 54\u201357, one of\nshortlist for Best Student Paper Award.\n[220] T. Raissi, E. Beck, R. Schluter, and H. Ney, \u201cTowards Consistent \u00a8\nHybrid HMM Acoustic Modeling,\u201d Apr. 2021, arXiv:2104.02387.\n[221] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schluter, and H. Ney, \u00a8\n\u201cA Systematic Comparison of Grapheme-Based vs. Phoneme-Based\nLabel Units for Encoder-Decoder-Attention Models,\u201d Nov. 2020,\narXiv:2005.09336.\n[222] C. Luscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, \u00a8\nR. Schluter, and H. Ney, \u201cRWTH ASR Systems for LibriSpeech: \u00a8\nHybrid vs Attention,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 231\u2013235.\n[223] D. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. Le,\n\u201cImproved Noisy Student Training for Automatic Speech Recognition,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2817\u20132821.\n[224] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and\nQ. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for\nAutomatic Speech Recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 2613\u20132617.\n[225] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schluter, and H. Ney, \u201cThe \u00a8\nRWTH ASR System for TED-LIUM Release 2: Improving Hybrid\nHMM with SpecAugment,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 7839\u20137843.\n[226] J. Cui, B. Kingsbury, B. Ramabhadran, A. Sethy, K. Audhkhasi, X. Cui,\nE. Kislal, L. Mangu, M. Nussbaum-Thom, M. Picheny, Z. Tuske, \u00a8\nP. Golik, R. Schluter, H. Ney, M. J. F. Gales, K. M. Knill, A. Ragni, \u00a8\nH. Wang, and P. Woodland, \u201cMultilingual Representations for Low\nResource Speech Recognition and Keyword Search,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 259\u2013266.\n[227] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL, Min\u0002neapolis, MN, Jun. 2019, pp. 96\u2013108.\n[228] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. Ramabhadran,\nY. Wu, A. Bapna, Z. Chen, and S. Lee, \u201cLarge-Scale Multilingual\nSpeech Recognition with a Streaming End-to-End Model,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2130\u20132134.\n[229] A. Graves, \u201cConnectionist Temporal Classification,\u201d in Supervised\nSequence Labelling with Recurrent Neural Networks. Heidelberg,\nGermany: Springer, 2012, ch. Connectionist Temporal Classification,\npp. 61\u201393.\n[230] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,\n\u201cMask CTC: Non-Autoregressive End-to-End ASR with CTC and\nMask Predict,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n3655\u20133659.\n[231] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, \u201cImputer:\nSequence Modelling via Imputation and Dynamic Programming,\u201d in\nProc. ICML. PMLR, Jul. 2020, pp. 1403\u20131413.\n[232] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, \u201cInsertion-Based\nModeling for End-to-End Automatic Speech Recognition,\u201d in Proc.\nInterspeech, Shanghai, China, Oct. 2020, pp. 3660\u20133664.\n[233] L. Dong and B. Xu, \u201cCif: Continuous Integrate-and-Fire for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6079\u20136083.\n[234] J. Nozaki and T. Komatsu, \u201cRelaxing the Conditional Independence\nAssumption of CTC-Based ASR by Conditioning on Intermediate\nPredictions,\u201d in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3735\u2013\n3739.\n[235] Y. Higuchi, N. Chen, Y. Fujita, H. Inaguma, T. Komatsu, J. Lee,\nJ. Nozaki, T. Wang, and S. Watanabe, \u201cA Comparative Study on Non\u0002Autoregressive Modelings for Speech-to-Text Generation,\u201d in Proc.\nIEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2110.05249.\n[236] W. Zhou, R. Schluter, and H. Ney, \u201cRobust Beam Search for Encoder- \u00a8\nDecoder Attention Based Speech Recognition without Length Bias,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1768\u20131772.\n[237] P. Koehn and R. Knowles, \u201cSix Challenges for Neural Machine Transla\u0002tion,\u201d in First Workshop on Neural Machine Translation. Vancouver,\nBC, Canada: Association for Computational Linguistics, Aug. 2017,\npp. 28\u201339.\n[238] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, \u201cModeling Coverage for Neural\nMachine Translation,\u201d in Proc. ACL, Berlin, Germany, May 2016, pp.\n76\u201385.\n[239] T. Hori, J. Cho, and S. Watanabe, \u201cEnd-to-End Speech Recogni\u0002tion with Word-Based RNN Language Models,\u201d in Proc. IEEE SLT.\nAthens, Greece: IEEE, Dec. 2018, pp. 389\u2013396.\n[240] K. Deng and P. C. Woodland, \u201cLabel-Synchronous Neural Transducer\nfor End-to-End ASR,\u201d Jul. 2023, arXiv:2307.03088.\n[241] T. Hori and A. Nakamura, Speech Recognition Algorithms Using\nWeighted Finite-State Transducers. San Rafael, CA: Morgan &\nClaypool Publishers, 2013.\n[242] R. Haeb-Umbach and H. Ney, \u201cImprovements in Beam Search for\n10000-Word Continuous-Speech Recognition,\u201d IEEE Transactions on\nSpeech and Audio Processing, vol. 2, no. 2, pp. 353\u2013356, 1994.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n26\n[243] H. Ney and S. Ortmanns, \u201cProgress in Dynamic Programming Search\nfor LVCSR,\u201d Proc. of the IEEE, vol. 88, no. 8, pp. 1224\u20131240, Aug.\n2000, http://dx.doi.org/10.1109/5.880081.\n[244] T. Hori, Y. Kubo, and A. Nakamura, \u201cReal-Time One-Pass Decoding\nwith Recurrent Neural Network Language Model for Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6364\u2013\n6368.\n[245] E. Beck, W. Zhou, R. Schluter, and H. Ney, \u201cLSTM Language Models \u00a8\nfor LVCSR in First-Pass Decoding and Lattice-Rescoring,\u201d Jul. 2019,\narXiv:1907.01030.\n[246] G. Saon, Z. Tuske, and K. Audhkhasi, \u201cAlignment-Length Syn- \u00a8\nchronous Decoding for RNN Transducer,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7804\u20137808.\n[247] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, \u201cFirst\u0002Pass Large Vocabulary Continuous Speech Recognition Using Bi\u0002Directional Recurrent DNNs,\u201d Dec. 2014, arXiv:1408.2873.\n[248] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP. Brighton, UK:\nIEEE, May 2019, pp. 5666\u20135670.\n[249] N. Moritz, T. Hori, and J. Le, \u201cStreaming Automatic Speech Recogni\u0002tion with the Transformer Model,\u201d in Proc. IEEE ICASSP. Barcelona,\nSpain: IEEE, May 2020, pp. 6074\u20136078.\n[250] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kalgaonkar, A. Sri\u0002ram, C. Fuegen, and M. L. Seltzer, \u201cRNN-T for Latency Controlled\nASR with Improved Beam Search,\u201d Nov. 2019, arXiv:1911.01629.\n[251] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring Transformers for Large\u0002Scale Speech Recognition,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 5041\u20135045.\n[252] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, \u201cStreaming End-to\u0002End ASR Based on Blockwise Non-Autoregressive Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3755\u20133759.\n[253] H. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, \u201cOnline Hybrid\nCTC/Attention Architecture for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 2623\u20132627, DOI:\n10.21437/Interspeech.2019-2018.\n[254] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming Transformer\nASR with Blockwise Synchronous Beam Search,\u201d in Proc. IEEE SLT.\nShenzhen, China: IEEE, Jun. 2021, pp. 22\u201329.\n[255] K. Hwang and W. Sung, \u201cCharacter-level language modeling with\nhierarchical recurrent neural networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5720\u20135724.\n[256] T. Hori, S. Watanabe, Y. Zhang, and W. Chan, \u201cAdvances in Joint CTC\u0002Attention Based End-to-End Speech Recognition with a Deep CNN\nEncoder and RNN-LM,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug.\n2017, pp. 949\u2013953.\n[257] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, \u201cAn Analysis of Incorporating an External Lan\u0002guage Model into a Sequence-to-Sequence Model,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5824\u20135828, DOI:\n10.1109/ICASSP.2018.8462682.\n[258] G. Saon, Z. Tuske, D. Bolanos, and B. Kingsbury, \u201cAdvancing \u00a8\nRNN Transducer Technology for Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5654\u20135658,\narXiv:2103.09935.\n[259] H. Seki, T. Hori, S. Watanabe, N. Moritz, and J. Le Roux, \u201cVectorized\nBeam Search for CTC-Attention-Based Speech Recognition,\u201d in Proc.\nInterspeech, Brighton, UK, May 2019, pp. 3825\u20133829.\n[260] T. Hori, S. Watanabe, and J. R. Hershey, \u201cMulti-Level Language\nModeling and Decoding for Open Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec.\n2017, pp. 287\u2013293.\n[261] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie,\nS. Watanabe, and S. Khudanpur, \u201cEspresso: A Fast End-to-End Neural\nSpeech Recognition Toolkit,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 136\u2013143.\n[262] Z. Tuske, K. Audhkhasi, and G. Saon, \u201cAdvancing Sequence-to- \u00a8\nSequence Based Speech Recognition,\u201d in Proc. Interspeech, Graz,\nAustria, Sep. 2019, pp. 3780\u20133784.\n[263] J. Drexler and J. Glass, \u201cSubword Regularization and Beam Search\nDecoding for End-to-End Automatic Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 6266\u20136270.\n[264] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu,\n\u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Interspeech,\nGraz, Austria, Sep. 2019, pp. 2773\u20132777.\n[265] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen,\nL. Xie, and X. Lei, \u201cWeNet: Production Oriented Streaming and Non\u0002Streaming End-to-End Speech Recognition Toolkit,\u201d Brno, Czechia, pp.\n4054\u20134058, Sep. 2021.\n[266] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei,\n\u201cU2++: Unified Two-Pass Bidirectional End-to-End Model for Speech\nRecognition,\u201d Dec. 2021, arXiv:2106.05642.\n[267] M. Zapotoczny, P. Pietrzak, A. Lancucki, and J. Chorowski, \u201cLattice\nGeneration in Attention-Based Speech Recognition Models,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2225\u20132229.\n[268] J. Kim, Y. Lee, and E. Kim, \u201cAccelerating RNN Transducer Inference\nvia Adaptive Expansion Search,\u201d IEEE Signal Processing Letters,\nvol. 27, pp. 2019\u20132023, 2020.\n[269] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: A system for Large\u0002Scale Machine Learning,\u201d in Proc. OSDI, Savannah, GA, Nov. 2016,\npp. 265\u2013283.\n[270] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,\nand M. Auli, \u201cFAIRSEQ: A Fast, Extensible Toolkit for Sequence\nModeling,\u201d in Proc. NAACL, Minneapolis, MN, Jun. 2019, pp. 48\u201353.\n[271] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan,\nT. Sainath, Y. Cao, C.-C. Chiu et al., \u201cLingvo: a Modular and\nScalable Framework for Sequence-to-Sequence Modeling,\u201d Feb. 2019,\narXiv:1902.08295.\n[272] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schluter, and \u00a8\nH. Ney, \u201cRETURNN: The RWTH Extensible Training Framework for\nUniversal Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5345\u20135349.\n[273] A. Hannun, A. Lee, Q. Xu, and R. Collobert, \u201cSequence-to-Sequence\nSpeech Recognition with Time-Depth Separable Convolutions,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 3785\u20133789.\n[274] M. Li, M. Liu, and H. Masanori, \u201cEnd-to-End Speech Recognition with\nAdaptive Computation Steps,\u201d in Proc. IEEE ICASSP, Brighton, UK,\nMay 2019, pp. 6246\u20136250.\n[275] P. Bahar, N. Makarov, A. Zeyer, R. Schuter, and H. Ney, \u201cExploring \u00a8\na Zero-Order Direct HMM Based on Latent Attention for Automatic\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 7854\u20137858.\n[276] Z. Huang, G. Zweig, and B. Dumoulin, \u201cCache Based Recurrent\nNeural Network Language Model Inference for First Pass Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp.\n6354\u20136358.\n[277] J. Jorge, A. Gimenez, J. Iranzo-S \u00b4 anchez, J. Civera, A. Sanchis, and \u00b4\nA. Juan, \u201cReal-Time One-Pass Decoder for Speech Recognition Using\nLSTM Language Models,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3820\u20133824.\n[278] W. Zhou, R. Schluter, and H. Ney, \u201cFull-Sum Decoding for Hybrid \u00a8\nHMM Based Speech Recognition Using LSTM Language Model,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7834\u20137838.\n[279] P. Sountsov and S. Sarawagi, \u201cLength Bias in Encoder Decoder Models\nand a Case for Global Conditioning,\u201d in Proc. EMNLP, Austin, TX,\nNov. 2016, pp. 1516\u20131525.\n[280] K. Murray and D. Chiang, \u201cCorrecting Length Bias in Neural Machine\nTranslation,\u201d in Proc. WMT, Brussels, Belgium, Oct. 2018, pp. 212\u2013\n223.\n[281] F. Stahlberg and B. Byrne, \u201cOn NMT Search Errors and Model Errors:\nCat Got Your Tongue?\u201d in Proc. EMNLP. Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 3354\u20133360.\n[282] N. Deshmukh, A. Ganapathiraju, and J. Picone, \u201cHierarchical Search\nfor Large-Vocabulary Conversational Speech Recognition: Working\nToward a Solution to the Decoding Problem,\u201d IEEE Signal Processing\nMagazine, vol. 16, no. 5, pp. 84\u2013107, 1999.\n[283] L. Nguyen and R. Schwartz, \u201cSingle-Tree Method for Grammar\u0002Directed Search,\u201d in Proc. IEEE ICASSP, vol. 2, Phoenix, AZ, Mar.\n1999, pp. 613\u2013616.\n[284] L. Sar\u0131, N. Moritz, T. Hori, and J. Le Roux, \u201cUnsupervised Speaker\nAdaptation using Attention-based Speaker Memory for End-to-End\nASR,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 2\u2013\n6.\n[285] F. Weninger, J. Andres-Ferrer, X. Li, and P. Zhan, \u201cListen, Attend, \u00b4\nSpell and Adapt: Speaker Adapted Sequence-to-Sequence ASR,\u201d in\nProc. Interspeech. Graz, Austria: ISCA, Sep. 2019, pp. 3805\u20133809.\n[286] Z. Meng, Y. Gaur, J. Li, and Y. Gong, \u201cSpeaker Adaptation for\nAttention-Based End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech. Graz, Austria: ISCA, Sep. 2019, pp. 241\u2013245.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n27\n[287] N. Tomashenko and Y. Esteve, \u201cEvaluation of Feature-Space Speaker `\nAdaptation for End-to-End Acoustic Models,\u201d in Proc. LREC.\nMiyazaki, Japan: ELRA, May 2018, pp. 3163\u20133170.\n[288] S. F. Chen and J. Goodman, \u201cAn Empirical Study of Smoothing\nTechniques for Language Modeling,\u201d in Proc. ACL, Santa Cruz, CA,\nJun. 1996, pp. 310\u2013318.\n[289] T. Mikolov, M. Karafiat, L. Burget, J. \u00b4 Cernock \u02c7 y, and S. Khudanpur, `\n\u201cRecurrent Neural Network Based Language Model,\u201d in Proc. Inter\u0002speech, Makuhari, Japan, Sep. 2010, pp. 1045\u20131048.\n[290] M. Sundermeyer, R. Schluter, and H. Ney, \u201cLSTM Neural Networks for \u00a8\nLanguage Modeling,\u201d in Proc. Interspeech, Portland, OR, Sep. 2012,\npp. 194\u2013197.\n[291] N.-Q. Pham, G. Kruszewski, and G. Boleda, \u201cConvolutional Neural\nNetwork Language Models,\u201d in Proc. EMNLP, Austin, TX, Nov. 2016,\npp. 1153\u20131162.\n[292] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage Modeling\nwith Gated Convolutional Networks,\u201d in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. JMLR.\norg, 2017, pp. 933\u2013941.\n[293] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and\nR. Collobert, \u201cFully Convolutional Speech Recognition,\u201d Feb. 2018,\narXiv:1812.06864.\n[294] T. Likhomanenko, G. Synnaeve, and R. Collobert, \u201cWho needs words?\nlexicon-free speech recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 3915\u20133919, arXiv:1904.04479.\n[295] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, \u201cCharacter\u0002Level Language Modeling with Deeper Self-Attention,\u201d in Proc. AIII,\nvol. 33, Honolulu, Hawaii, Feb. 2019, pp. 3159\u20133166.\n[296] K. Irie, A. Zeyer, R. Schluter, and H. Ney, \u201cLanguage Modeling with \u00a8\nDeep Transformers,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 3905\u20133909.\n[297] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov,\n\u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 2978\u20132988.\n[298] P. Werbos, \u201cBackpropagation Through Time: What It Does and How\nto Do It,\u201d Proc. of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990,\nDOI: 10.1109/5.58337.\n[299] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n\u201cCompressive Transformers for Long-Range Sequence Modelling,\u201d\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6154\u2013\n6158, 2020.\n[300] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn Using Monolingual\nCorpora in Neural Machine Translation,\u201d Jun. 2015, arXiv:1503.03535.\n[301] A. Sriram, H. Jun, S. Satheesh, and A. Coates, \u201cCold Fusion: Training\nSeq2Seq Models Together with Language Models,\u201d in Proc. Inter\u0002speech, Hyderabad, India, Sep. 2018, pp. 387\u2013391.\n[302] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n\u201cComponent Fusion: Learning Replaceable Language Model Com\u0002ponent for End-to-End Speech Recognition System,\u201d in Proc. IEEE\nICASSP. Brighton, UK: IEEE, May 2019, pp. 5361\u20135635.\n[303] E. McDermott, H. Sak, and E. Variani, \u201cA Density Ratio Approach to\nLanguage Model Fusion in End-To-End Automatic Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 434\u2013\n441.\n[304] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen,\nR. Zhao, J. Li, and Y. Gong, \u201cInternal Language Model Estimation\nfor Domain-Adaptive End-to-End Speech Recognition,\u201d in Proc. IEEE\nSLT, Shenzhen , China, Dec. 2020, pp. 243\u2013250.\n[305] W. Zhou, Z. Zheng, R. Schluter, and H. Ney, \u201cOn Language Model Inte- \u00a8\ngration for RNN Transducer based Speech Recognition,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8407\u20138411, arXiv:2110.06841.\n[306] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre\u0002Training of Deep Bidirectional Transformers for Language Understand\u0002ing,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 4171\u20134186.\n[307] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, \u201cLanguage Models are Unsuper\u0002vised Multitask Learners,\u201d 2019, openAI blog. [Online].\nAvailable: https://cdn.openai.com/better-language-models/language\nmodels are unsupervised multitask learners.pdf\n[308] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, \u201cMasked\nLanguage Model Scoring,\u201d in Proc. ACL, Jul. 2020, pp. 2699\u20132712.\n[309] S. Kim, S. Dalmia, and F. Metze, \u201cGated Embeddings in End-to-End\nSpeech Recognition for Conversational-Context Fusion,\u201d in Proc. ACL,\nFlorence, Italy, Jul. 2019, pp. 1131\u20131141.\n[310] A. Zeyer, A. Merboldt, W. Michel, R. Schluter, and H. Ney, \u201cLib- \u00a8\nrispeech Transducer Model with Internal Language Model Prior Cor\u0002rection,\u201d in Proc. Interspeech, Brno, Czech Republic, Apr. 2021, pp.\n2052\u20132056.\n[311] L. R. Bahl, F. Jelinek, and R. L. Mercer, \u201cA Maximum Likelihood\nApproach to Continuous Speech Recognition,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 5, no. 2, pp. 179\u2013190,\nMar. 1983.\n[312] J. Makhoul and R. Schwartz, \u201cState of the Art in Continuous Speech\nRecognition,\u201d Proc. NAS, vol. 92, no. 22, pp. 9956\u20139963, Oct. 1995.\n[313] D. Klakow and J. Peters, \u201cTesting the Correlation of Word Error Rate\nand Perplexity,\u201d Speech Communication, vol. 38, no. 1, pp. 19\u201328,\n2002.\n[314] M. Sundermeyer, H. Ney, and R. Schluter, \u201cFrom Feedforward to Re- \u00a8\ncurrent LSTM Neural Networks for Language Modeling,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 23, no. 3, pp.\n517\u2013529, Mar. 2015.\n[315] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, \u201cMinimum Word\nError Training of Long Short-Term Memory Recurrent Neural Network\nLanguage Models for Speech Recognition,\u201d in Proc. IEEE ICASSP,\nShanghai, China, Mar. 2016, pp. 5990\u20135994.\n[316] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Tele\u0002phone Speech Corpus for Research and Development,\u201d in Proc. IEEE\nICASSP, vol. 1, San Francisco, CA, Mar. 1992, pp. 517\u2013520 vol.1.\n[317] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an\nASR Corpus Based on Public Domain Audio Books,\u201d in Proc. IEEE\nICASSP, Queensland, Australia, Apr. 2015, pp. 5206\u20135210.\n[318] A. Zeyer, P. Bahar, K. Irie, R. Schluter, and H. Ney, \u201cA Comparison of \u00a8\nTransformer and LSTM Encoder Decoder Models for ASR,\u201d in Proc.\nIEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 8\u201315.\n[319] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,\nand A. Mohamed, \u201cHuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans.\nAudio, Speech, and Language Processing, vol. 19, pp. 3451\u20133460,\n2021.\n[320] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap,\nA. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-End ASR: from\nSupervised to Semi-Supervised Learning with Modern Architectures,\u201d\nin Proc. ICML, Jul. 2020, arXiv:1911.08460.\n[321] E. G. Ng, C.-C. Chiu, Y. Zhang, and W. Chan, \u201cPushing the Limits of\nNon-Autoregressive Speech Recognition,\u201d in Proc. Interspeech, Brno,\nCzechia, Sep. 2021, pp. 3725\u20132729.\n[322] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazare,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-Light: A\nBenchmark for ASR with Limited or no Supervision,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7669\u20137673.\n[323] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig,\nand M. L. Seltzer, \u201cTransformer-Based Acoustic Modeling for Hybrid\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6874\u20136878.\n[324] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and\nS. Watanabe, \u201cE-branchformer: Branchformer with enhanced merging\nfor speech recognition,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.00077.\n[325] M. Kitza, P. Golik, R. Schluter, and H. Ney, \u201cCumulative Adaptation for \u00a8\nBLSTM Acoustic Models,\u201d in Interspeech, Graz, Austria, Sep. 2019,\npp. 754\u2013758.\n[326] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. Li,\nJ. Chorowski, and M. Bacchiani, \u201cState-of-the-Art Speech Recognition\nwith Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4774\u20134778.\n[327] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo,\nD. Kim, S. Jung, J. Lee, M. Han, and C. Kim, \u201cAttention Based On\u0002Device Streaming Speech Recognition with Large Speech Corpus,\u201d in\nProc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 956\u2013963.\n[328] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov,\nZ. Wang, L. He, S. Zhao et al., \u201cDeveloping RNN-T Models Surpassing\nHigh-Performance Hybrid Models with Customization Capability,\u201d in\nProc. Interspeech, Shanghai, China (virtual), Oct. 2020, pp. 3590\u2013\n3594, arXiv:2007.15188.\n[329] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, \u201cOnline\nAutomatic Speech Recognition with Listen, Attend and Spell Model,\u201d\nIEEE Signal Processing Letters, vol. 27, pp. 1889\u20131893, 2020.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n28\n[330] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and\nM. Seltzer, \u201cEmformer: Efficient Memory Transformer based Acoustic\nModel for Low Latency Streaming Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 6783\u20136787.\n[331] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping Real-Time\nStreaming Transformer Transducer for Speech Recognition on Large\u0002Scale Dataset,\u201d in Proc. IEEE ICASSP. Toronto, Ontario, Canada:\nIEEE, Jun. 2021, pp. 5904\u20135908.\n[332] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, C.-C. Chiu, D. Garcia,\nA. Gruenstein, K. Hu, M. Jin, A. Kannan, Q. Liang, I. McGraw,\nC. Peyser, R. Prabhavalkar, G. Pundak, D. Rybach, Y. Shangguan,\nY. Sheth, T. Strohman, M. Visontai, Y. Wu, Y. Zhang, and D. Zhao,\n\u201cA Streaming On-Device End-To-End Model Surpassing Server-Side\nConventional Model Quality and Latency,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, may 2020, pp. 6059\u20136063.\n[333] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan,\nS.-Y. Chang, R. Pang, Y. He, J. Qin, W. Han, Q. Liang, Y. Zhang,\nT. Strohman, and Y. Wu, \u201cA Better and Faster End-to-End Model for\nStreaming ASR,\u201d in Proc. IEEE ICASSP, Toronto, Ontario, Canada,\nJun. 2021, pp. 5634\u20135638.\n[334] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach,\nC. Allauzen, E. Variani, J. Qin, Q.-N. Le-The, S.-Y. Chang, B. Li,\nA. Gulati, J. Yu, C.-C. Chiu, D. Caseiro, W. Li, Q. Liang, and\nP. Rondon, \u201cAn Efficient Streaming Non-Recurrent On-Device End\u0002to-End Model with Improvements to Rare-Word Modeling,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 1777\u20131781.\n[335] A. Bapna, Y.-A. Chung, N. Wu, , A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSLAM: A Unified\nEncoder for Speech and Language Modeling via Speech-Text Joint\nPre-Training,\u201d Oct. 2021, arXiv:2110.10329.\n[336] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng,\nS. Khanuja, J. Riesa, and A. Conneau, \u201cmSLAM: Massively Mul\u0002tilingual Joint Pre-Training for Speech and Text,\u201d Feb. 2022,\narXiv:2202.01374.\n[337] Y. Tang, H. Gong, N. Dong, C. Wag, W. Hsu, J. Gu, A. Baevski, X. Li,\nA. Mohamed, M. Auli, and J. Pino, \u201cUnified Speech-Text Pre-training\nfor Speech Translation and Recognition,\u201d in Proc. ACL, Dublin, Ireland,\nMay 2022, pp. 1488\u20131499, arXiv:2204.05409.\n[338] Y.-A. Chung, C. Zhu, and M. Zeng, \u201cSPLAT: Speech-Language Joint\nPre-Training for Spoken Language Understanding,\u201d in Proc. NAACL,\nJun. 2021, pp. 1897\u20131907, arXiv:2010.02295.\n[339] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko,\nQ. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, \u201cSpeechT5:\nUnified-Modal Encoder-Decoder Pre-Training for Spoken Language\nProcessing,\u201d in Proc. ACL, Dublin, Ireland, May 2022, pp. 5723\u20135738,\narXiv:2110.07205.\n[340] S. Thomas, H. J. Kuo, B. Kingsbury, and G. Saon, \u201cTowards Reducing\nthe Need for Speech Training Data to Build Spoken Language Under\u0002standing Systems,\u201d in Proc. IEEE ICASSP, Singapore, May 2022, pp.\n7932\u20137936, arXiv:2203.00006.\n[341] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen,\nB. Li, W. Wang, and T. Strohman, \u201cJOIST: A joint speech and text\nstreaming model for ASR,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.07353.\n[342] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and\nJ. Le Roux, \u201cCycle-Consistency Training for End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp.\n6271\u20136275.\n[343] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, \u201cMultichannel\nEnd-to-End Speech Recognition,\u201d in Proc. ICML. Sydney, Australia:\nPMLR, Aug. 2017, pp. 2632\u20132641.\n[344] J. Li, \u201cRecent Advances in End-to-End Automatic Speech Recogni\u0002tion,\u201d APSIPA Trans. on Signal and Information Processing, vol. 11,\nno. 1, Nov. 2021, DOI: 10.1561/116.00000050, arXiv:2111.01690.\nPLACE\nPHOTO\nHERE\nRohit Prabhavalkar Rohit Prabhavalkar received\nhis PhD in Computer Science and Engineering from\nThe Ohio State University, USA, in 2013. Follow\u0002ing his PhD, Rohit joined the Speech Technologies\ngroup at Google where he is currently a Staff Re\u0002search Scientist. At Google, his research has focused\nprimarily on developing compact acoustic models\nwhich can run efficiently on mobile devices, and on\ndeveloping improved end-to-end automatic speech\nrecognition systems. Rohit has co-authored over 50\nrefereed papers, which have received two best paper\nawards (ASRU 2017; ICASSP 2018). He currently serves as a member of the\nIEEE Speech and Language Processing Technical Committee (2018\u20132024),\nand as an Associate Editor of the IEEE/ACM Transactions on Audio, Speech,\nand Language Processing.\nPLACE\nPHOTO\nHERE\nTakaaki Hori received his PhD degree in system\nand information engineering from Yamagata Uni\u0002versity, Yonezawa, Japan, in 1999. From 1999 to\n2015, he had been engaged in researches on speech\nrecognition and spoken language processing at Cy\u0002ber Space Laboratories and Communication Science\nLaboratories in Nippon Telegraph and Telephone\n(NTT) Corporation, Japan. From 2015 to 2021,\nhe was a Senior Principal Research Scientist at\nMitsubishi Electric Research Laboratories (MERL),\nUSA. He is currently a Machine Learning Re\u0002searcher at Apple. His research interests include automatic speech recognition,\nspoken language understanding, and language modeling. He served as a\nmember of the IEEE Speech and Language Processing Technical Committee\n(2020\u20132022).\nPLACE\nPHOTO\nHERE\nTara Sainath received her PhD in Electrical Engi\u0002neering and Computer Science from MIT in 2009.\nThe main focus of her PhD work was in acoustic\nmodeling for noise robust speech recognition. After\nher PhD, she spent 5 years at the Speech and\nLanguage Algorithms group at IBM T.J. Watson Re\u0002search Center, before joining Google Research. She\nhas served as a Program Chair for ICLR in 2017 and\n2018. Also, she has co-organized numerous special\nsessions and workshops, including Interspeech 2010,\nICML 2013, Interspeech 2016 and ICML 2017. In\naddition, she is a member of the IEEE Speech and Language Processing\nTechnical Committee (SLTC) as well as the Associate Editor for IEEE/ACM\nTransactions on Audio, Speech, and Language Processing.\nPLACE\nPHOTO\nHERE\nRalf Schluter \u00a8 Ralf Schluter received his Dr.rer.nat. \u00a8\ndegree in Computer Science in 2000 and habilitated\nin Computer Science in 2019, both at RWTH Aachen\nUniversity. In May 1996, Ralf Schluter joined the \u00a8\nComputer Science Department at RWTH Aachen\nUniversity, where he currently is Lecturer and\nAcademic Director, leading the Automatic Speech\nRecognition Group at the Chair Computer Science\n6 \u2013 Machine Learning and Human Language Tech\u0002nology. In 2019, Ralf also joined AppTek GmbH\nAachen as Senior Researcher. His research interests\ncover sequence classification, specifically all aspects of automatic speech\nrecognition, decision theory, stochastic modeling, and signal analysis. Ralf\nserved as Subject Editor for Speech Communication (2013-2019).\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n29\nPLACE\nPHOTO\nHERE\nShinji Watanabe is an Associate Professor at\nCarnegie Mellon University, Pittsburgh, PA. He re\u0002ceived his B.S., M.S., and Ph.D. (Dr. Eng.) degrees\nfrom Waseda University, Tokyo, Japan. He was a\nresearch scientist at NTT Communication Science\nLaboratories, Kyoto, Japan, from 2001 to 2011, a\nvisiting scholar at Georgia institute of technology,\nAtlanta, GA, in 2009, and a senior principal research\nscientist at Mitsubishi Electric Research Laborato\u0002ries (MERL), Cambridge, MA USA from 2012 to\n2017. Before Carnegie Mellon University, he was\nan associate research professor at Johns Hopkins University, Baltimore,\nMD, USA, from 2017 to 2020. His research interests include automatic\nspeech recognition, speech enhancement, spoken language understanding, and\nmachine learning for speech and language processing. He is an IEEE and\nISCA Fellow.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
    "openalex_id": "https://openalex.org/W4388017359",
    "title": "End-to-End Speech Recognition: A Survey",
    "publication_date": "2024-01-01",
    "cited_by_count": 33,
    "topics": "Speech Recognition Technology, Statistical Machine Translation and Natural Language Processing, Audio Signal Classification and Analysis",
    "keywords": "End-to-End Speech Recognition, Automatic Speech Recognition, Acoustic Modeling, Environmental Sound Recognition, End-to-end principle, Deep Learning, Word error rate, Deep neural networks",
    "concepts": "Computer science, Hidden Markov model, Deep learning, Artificial neural network, Language model, Software deployment, Artificial intelligence, End-to-end principle, Speech recognition, Word error rate, Domain (mathematical analysis), Deep neural networks, Natural language processing, Machine learning, Mathematical analysis, Mathematics, Operating system",
    "pdf_urls_by_priority": [
      "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
      "https://arxiv.org/pdf/2303.03329"
    ],
    "text_type": "full_text",
    "successful_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "referenced_works": [
      "https://openalex.org/W108866686",
      "https://openalex.org/W1494198834",
      "https://openalex.org/W1501286448",
      "https://openalex.org/W1508165687",
      "https://openalex.org/W1553004968",
      "https://openalex.org/W1583239513",
      "https://openalex.org/W1587755118",
      "https://openalex.org/W1588735863",
      "https://openalex.org/W1710082047",
      "https://openalex.org/W179875071",
      "https://openalex.org/W1806891645",
      "https://openalex.org/W1904365287",
      "https://openalex.org/W1915251500",
      "https://openalex.org/W1922655562",
      "https://openalex.org/W1966812932",
      "https://openalex.org/W1975550806",
      "https://openalex.org/W1979136262",
      "https://openalex.org/W1985258458",
      "https://openalex.org/W1986184096",
      "https://openalex.org/W1988720110",
      "https://openalex.org/W1989674786",
      "https://openalex.org/W1991133427",
      "https://openalex.org/W2000200144",
      "https://openalex.org/W2001679125",
      "https://openalex.org/W2008554732",
      "https://openalex.org/W2014151772",
      "https://openalex.org/W2024539680",
      "https://openalex.org/W2033245860",
      "https://openalex.org/W2033565080",
      "https://openalex.org/W2046932483",
      "https://openalex.org/W2050526637",
      "https://openalex.org/W2056590938",
      "https://openalex.org/W2057653135",
      "https://openalex.org/W2064675550",
      "https://openalex.org/W206545267",
      "https://openalex.org/W2066378046",
      "https://openalex.org/W2078354939",
      "https://openalex.org/W2080213370",
      "https://openalex.org/W2091981305",
      "https://openalex.org/W2097927681",
      "https://openalex.org/W2100180150",
      "https://openalex.org/W2105482032",
      "https://openalex.org/W2105594594",
      "https://openalex.org/W2110798204",
      "https://openalex.org/W2114016253",
      "https://openalex.org/W2121879602",
      "https://openalex.org/W2125838338",
      "https://openalex.org/W2127095586",
      "https://openalex.org/W2127141656",
      "https://openalex.org/W2129545859",
      "https://openalex.org/W2131968858",
      "https://openalex.org/W2136617108",
      "https://openalex.org/W2136922672",
      "https://openalex.org/W2143564602",
      "https://openalex.org/W2143612262",
      "https://openalex.org/W2145249131",
      "https://openalex.org/W2150355110",
      "https://openalex.org/W2151058131",
      "https://openalex.org/W2151834591",
      "https://openalex.org/W2155368638",
      "https://openalex.org/W2157749010",
      "https://openalex.org/W2165712214",
      "https://openalex.org/W2166637769",
      "https://openalex.org/W2183341477",
      "https://openalex.org/W2242818861",
      "https://openalex.org/W2288217446",
      "https://openalex.org/W2291975472",
      "https://openalex.org/W2296073425",
      "https://openalex.org/W2327501763",
      "https://openalex.org/W2331143823",
      "https://openalex.org/W2394932179",
      "https://openalex.org/W2396464458",
      "https://openalex.org/W2402268235",
      "https://openalex.org/W2407080277",
      "https://openalex.org/W2408093180",
      "https://openalex.org/W2411921399",
      "https://openalex.org/W2471933213",
      "https://openalex.org/W2514741789",
      "https://openalex.org/W2516457973",
      "https://openalex.org/W2520160253",
      "https://openalex.org/W2525778437",
      "https://openalex.org/W2530876040",
      "https://openalex.org/W2545177271",
      "https://openalex.org/W2566563465",
      "https://openalex.org/W2577366047",
      "https://openalex.org/W2606722458",
      "https://openalex.org/W2608712415",
      "https://openalex.org/W2618530766",
      "https://openalex.org/W2627092829",
      "https://openalex.org/W2745439869",
      "https://openalex.org/W2746192915",
      "https://openalex.org/W2748816379",
      "https://openalex.org/W2750499125",
      "https://openalex.org/W2766219058",
      "https://openalex.org/W2787663903",
      "https://openalex.org/W2792376130",
      "https://openalex.org/W2799800213",
      "https://openalex.org/W2808640845",
      "https://openalex.org/W2808939837",
      "https://openalex.org/W2883586237",
      "https://openalex.org/W2886025712",
      "https://openalex.org/W2886180730",
      "https://openalex.org/W2886319145",
      "https://openalex.org/W2888779557",
      "https://openalex.org/W2888909726",
      "https://openalex.org/W2889129739",
      "https://openalex.org/W2889163603",
      "https://openalex.org/W2889187401",
      "https://openalex.org/W2889374926",
      "https://openalex.org/W2889504751",
      "https://openalex.org/W2892009249",
      "https://openalex.org/W2892124901",
      "https://openalex.org/W2899879954",
      "https://openalex.org/W2900209846",
      "https://openalex.org/W2904818793",
      "https://openalex.org/W2914018192",
      "https://openalex.org/W2915977493",
      "https://openalex.org/W2928941594",
      "https://openalex.org/W2933138175",
      "https://openalex.org/W2936123380",
      "https://openalex.org/W2936774411",
      "https://openalex.org/W2937402758",
      "https://openalex.org/W2937780860",
      "https://openalex.org/W2938348542",
      "https://openalex.org/W2939111082",
      "https://openalex.org/W2940180244",
      "https://openalex.org/W2943845043",
      "https://openalex.org/W2949975180",
      "https://openalex.org/W2951974815",
      "https://openalex.org/W2952992734",
      "https://openalex.org/W2953561564",
      "https://openalex.org/W2962699523",
      "https://openalex.org/W2962728618",
      "https://openalex.org/W2962742956",
      "https://openalex.org/W2962745521",
      "https://openalex.org/W2962760690",
      "https://openalex.org/W2962784628",
      "https://openalex.org/W2962824709",
      "https://openalex.org/W2962826786",
      "https://openalex.org/W2963022149",
      "https://openalex.org/W2963026768",
      "https://openalex.org/W2963088785",
      "https://openalex.org/W2963144852",
      "https://openalex.org/W2963211739",
      "https://openalex.org/W2963240019",
      "https://openalex.org/W2963260202",
      "https://openalex.org/W2963303028",
      "https://openalex.org/W2963382396",
      "https://openalex.org/W2963431393",
      "https://openalex.org/W2963506925",
      "https://openalex.org/W2963571336",
      "https://openalex.org/W2963739817",
      "https://openalex.org/W2963747784",
      "https://openalex.org/W2964012862",
      "https://openalex.org/W2964103964",
      "https://openalex.org/W2964107261",
      "https://openalex.org/W2964110616",
      "https://openalex.org/W2970692082",
      "https://openalex.org/W2971840980",
      "https://openalex.org/W2972451902",
      "https://openalex.org/W2972528057",
      "https://openalex.org/W2972621414",
      "https://openalex.org/W2972625221",
      "https://openalex.org/W2972630480",
      "https://openalex.org/W2972692349",
      "https://openalex.org/W2972780808",
      "https://openalex.org/W2972799770",
      "https://openalex.org/W2972837679",
      "https://openalex.org/W2972889948",
      "https://openalex.org/W2972953886",
      "https://openalex.org/W2972977747",
      "https://openalex.org/W2972995428",
      "https://openalex.org/W2973122799",
      "https://openalex.org/W2981857663",
      "https://openalex.org/W2987019345",
      "https://openalex.org/W2995181338",
      "https://openalex.org/W2997617958",
      "https://openalex.org/W3005302685",
      "https://openalex.org/W3007328579",
      "https://openalex.org/W3007528493",
      "https://openalex.org/W3008037978",
      "https://openalex.org/W3008174054",
      "https://openalex.org/W3008191852",
      "https://openalex.org/W3008284571",
      "https://openalex.org/W3008525923",
      "https://openalex.org/W3008762051",
      "https://openalex.org/W3008898571",
      "https://openalex.org/W3008912312",
      "https://openalex.org/W3011339933",
      "https://openalex.org/W3015190365",
      "https://openalex.org/W3015194534",
      "https://openalex.org/W3015369343",
      "https://openalex.org/W3015383801",
      "https://openalex.org/W3015501067",
      "https://openalex.org/W3015671919",
      "https://openalex.org/W3015686596",
      "https://openalex.org/W3015726069",
      "https://openalex.org/W3015927303",
      "https://openalex.org/W3015974384",
      "https://openalex.org/W3015995734",
      "https://openalex.org/W3016010032",
      "https://openalex.org/W3016053754",
      "https://openalex.org/W3016167541",
      "https://openalex.org/W3016234571",
      "https://openalex.org/W3017474798",
      "https://openalex.org/W3026041220",
      "https://openalex.org/W3028545098",
      "https://openalex.org/W3034775979",
      "https://openalex.org/W3092122846",
      "https://openalex.org/W3094667432",
      "https://openalex.org/W3094713728",
      "https://openalex.org/W3094957294",
      "https://openalex.org/W3095173472",
      "https://openalex.org/W3095189764",
      "https://openalex.org/W3095376166",
      "https://openalex.org/W3095697114",
      "https://openalex.org/W3096032230",
      "https://openalex.org/W3096160024",
      "https://openalex.org/W3096215352",
      "https://openalex.org/W3097747488",
      "https://openalex.org/W3097777922",
      "https://openalex.org/W3097882114",
      "https://openalex.org/W3097973766",
      "https://openalex.org/W3100910367",
      "https://openalex.org/W3103005696",
      "https://openalex.org/W3105532142",
      "https://openalex.org/W3147187328",
      "https://openalex.org/W3147414526",
      "https://openalex.org/W3148001440",
      "https://openalex.org/W3148654612",
      "https://openalex.org/W3151269043",
      "https://openalex.org/W3152221657",
      "https://openalex.org/W3160551958",
      "https://openalex.org/W3160766462",
      "https://openalex.org/W3161375121",
      "https://openalex.org/W3161873870",
      "https://openalex.org/W3162249256",
      "https://openalex.org/W3162665866",
      "https://openalex.org/W3163203022",
      "https://openalex.org/W3163300396",
      "https://openalex.org/W3163793923",
      "https://openalex.org/W3163839574",
      "https://openalex.org/W3163842339",
      "https://openalex.org/W3167895882",
      "https://openalex.org/W3170405627",
      "https://openalex.org/W3197140813",
      "https://openalex.org/W3197304116",
      "https://openalex.org/W3197478142",
      "https://openalex.org/W3197507772",
      "https://openalex.org/W3197976839",
      "https://openalex.org/W3197991202",
      "https://openalex.org/W3198116002",
      "https://openalex.org/W3198439131",
      "https://openalex.org/W3198442913",
      "https://openalex.org/W3198455051",
      "https://openalex.org/W3198654230",
      "https://openalex.org/W3202184514",
      "https://openalex.org/W3202419788",
      "https://openalex.org/W3204696009",
      "https://openalex.org/W3205201903",
      "https://openalex.org/W3205644108",
      "https://openalex.org/W3206573929",
      "https://openalex.org/W3206876927",
      "https://openalex.org/W3207222250",
      "https://openalex.org/W3209059054",
      "https://openalex.org/W3211040052",
      "https://openalex.org/W3211278025",
      "https://openalex.org/W4206410067",
      "https://openalex.org/W4210463634",
      "https://openalex.org/W4221155340",
      "https://openalex.org/W4223622550",
      "https://openalex.org/W4224518768",
      "https://openalex.org/W4225319488",
      "https://openalex.org/W4225334634",
      "https://openalex.org/W4226120743",
      "https://openalex.org/W4240908132",
      "https://openalex.org/W4288290348",
      "https://openalex.org/W4297781872",
      "https://openalex.org/W4299649720",
      "https://openalex.org/W4319862255",
      "https://openalex.org/W4319862408",
      "https://openalex.org/W4319862418",
      "https://openalex.org/W4319862474",
      "https://openalex.org/W4319862683",
      "https://openalex.org/W4372259859",
      "https://openalex.org/W4378501656",
      "https://openalex.org/W4381827575",
      "https://openalex.org/W4383605108",
      "https://openalex.org/W4385245566",
      "https://openalex.org/W4394662461",
      "https://openalex.org/W4473315",
      "https://openalex.org/W66978610",
      "https://openalex.org/W98857008"
    ],
    "openalex_rank": 5,
    "num_tokens": 51849,
    "url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "question_generated": "What are the key developments in end-to-end speech recognition systems, the challenges they face in terms of performance and deployment, and what future directions could enhance their effectiveness in various applications?",
    "pipeline_answer": "Key Developments in End-to-End Speech Recognition Systems:\n\n- Transition to Neural Architectures: End-to-end speech recognition systems have evolved from traditional hybrid models to fully neural architectures, resulting in significant improvements in accuracy and efficiency for automatic speech recognition tasks [S2087648615].\n\n- Deep Learning Advancements: The implementation of deep learning techniques has led to a more than 50% reduction in word error rates compared to previous models, showcasing substantial advancements in performance [S3597456458].\n\nChallenges in Performance and Deployment:\n\n- Robustness in Noisy Environments: One of the primary challenges faced by end-to-end speech recognition systems is their robustness in noisy environments, where traditional hybrid models tend to perform better [S3568884510].\n\n- Real-Time Processing Needs: There is a critical requirement for real-time processing capabilities in deployment, which poses challenges for current end-to-end systems [S3568884510].\n\nFuture Directions for Enhancement:\n\n- Hybrid Approaches: Future enhancements may involve hybrid models that combine the strengths of different architectures, such as Listen, Attend and Spell (LAS) and Recurrent Neural Network Transducer (RNNT), to improve both accuracy and real-time performance [S3683972091].\n\n- Advanced Signal Processing: The use of multi-microphone arrays and advanced signal processing techniques, including dereverberation and acoustic beamforming, is essential for enhancing the performance of far-field automatic speech recognition systems [S8713312544].\n\nIn summary, while end-to-end speech recognition systems have made significant strides in accuracy and efficiency, challenges remain in robustness and real-time processing. Future developments may benefit from hybrid approaches and advanced signal processing techniques.",
    "pipeline_references": {
      "S8713312544": {
        "id": "S8713312544",
        "text": "The use of multi-microphone arrays and advanced signal processing techniques, such as dereverberation and acoustic beamforming, is essential for improving the performance of far-field automatic speech recognition systems.",
        "children": [
          {
            "id": "E5511477866",
            "text": "Reinhold Haeb-Umbach\nJahn Heymann\nLukas Drude\nShinji Watanabe\nMarc Delcroix\nTomohiro Nakatani\nFar-Field Automatic Speech Recognition\n1Index Terms-Automatic speech recognitionspeech en- hancementdereverberationacoustic beamformingend-to- end speech recognition\nThe machine recognition of speech spoken at a distance from the microphones, known as far-field automatic speech recognition (ASR), has received a significant increase of attention in science and industry, which caused or was caused by an equally significant improvement in recognition accuracy. Meanwhile it has entered the consumer market with digital home assistants with a spoken language interface being its most prominent application. Speech recorded at a distance is affected by various acoustic distortions and, consequently, quite different processing pipelines have emerged compared to ASR for close-talk speech. A signal enhancement front-end for dereverberation, source separation and acoustic beamforming is employed to clean up the speech, and the back-end ASR engine is robustified by multi-condition training and adaptation. We will also describe the so-called end-to-end approach to ASR, which is a new promising architecture that has recently been extended to the far-field scenario. This tutorial article gives an account of the algorithms used to enable accurate speech recognition from a distance, and it will be seen that, although deep learning has a significant share in the technological breakthroughs, a clever combination with traditional signal processing can lead to surprisingly effective solutions.\nI. INTRODUCTION\nF AR-field, also called distant ASR is concerned with the machine recognition of speech spoken at a distance from the microphone. Such recording conditions are common for applications like voice-control of digital home assistants, the automatic transcription of meetings, human-to-robot communication, and several other more. In recent years far-field ASR has witnessed a great increase of attention in the speech research community. This popularity can be attributed to several factors. There is first the large gains in recognition performance enabled by Deep Learning (DL), which made the more challenging task of accurate far-field ASR come within reach. A second reason is the commercial success of speech enabled digital home assistants, which has become possible through progress in various fields, including signal processing, ASR and natural language processing (NLP). Finally, scientific challenges related to far-field noise and reverberation robust ASR, such as the REVERB challenge [1], the series of CHiME challenges [2]- [5], J. Heymann and L. Drude were in part supported by a Google Faculty Research Award. and the ASpIRE challenge [6] exposed the task to a wide research audience and met with a lot of publicity. Conversely, those challenges have also helped to get a clearer picture as to which techniques and algorithms are helpful for far-field ASR.\nThe reason why far-field ASR is more challenging than ASR of speech recorded by a close-talking microphone is the degraded signal quality. First, the speech signal is attenuated when propagating from the speaker to the microphones, resulting in low signal power and often also low Signal-to-Noise Ratio (SNR). Second, in an enclosure, such as the living or a meeting room, the source signal is repeatedly reflected by walls and objects in the room, resulting in multi-path propagation, which causes a temporal smearing of the source signal called reverberation, much like multi-path propagation does in wireless communications. Third, it is likely that the microphone will capture other interfering sounds, in addition to the desired speech signal, such as the television or HVAC equipment. These sources of acoustic interference can be diverse, hard to predict, and often nonstationary in nature and thus difficult to compensate for. All these factors have a detrimental impact on ASR recognition performance.\nGiven these signal degradations, it is not surprising that quite different processing pipelines have emerged compared to ASR for close-talk speech. There is, foremostly, the use of a microphone array instead of a single microphone for sound capture. This allows for multichannel speech enhancement, which has proven very successful in noisy reverberant environments. Second, the speech recognition engine is trained with data which represents the typical signal degradations the recognizer is exposed to in a far-field scenario. This robustifies the acoustic model (AM), which is the component of the recognizer which translates the speech signal into linguistic units. The following examples demonstrate the power of enhancement and acoustic modeling:\n\u2022 The REVERB challenge data consists of recordings of the text prompts of the Wall Street Journal (WSJ) data set, respoken and rerecorded in a far-field scenario with a distance of 2-3 m between speaker and microphone array [1]. The challenge baseline ASR system, defined in 2014, which operates on a single channel microphone signal, achieved a Word Error Rate (WER) of 49%. Using a strong AM based on DL, the WER could be reduced to 22.2% [7]- [9], while the addition of a multi-microphone frontend and strong dereverberation brought the error rate down to 6.14% [10].\n\u2022 The data set of the CHiME-3 challenge consists of recordings of the WSJ sentences in four different noise environments (bus, street, cafe, pedestrian area) [11]. The data was recorded using a tablet computer with six microphones mounted around the frame of the device. The baseline system reached a WER of 33%, while a robust back-end speech recognizer achieved 11.4% [12]. Finally, the multimicrophone front-end processing brought the error rate down to 2.7% [12].\n\u2022 CHiME-5/6 consists of recordings of casual conversations among friends during a dinner party. The spontaneous speech, reverberation, and the large portion of times where more than one speaker is speaking simultaneously results in a WER of barely below 80% achieved by the baseline system. Using a strong back-end, approximately 60% WER is achieved [13], while the addition of multi-microphone source separation and dereverberation results in a WER of 43.2% [13]. Improvements in both front-end and back-end resulted in 30.5% WER in the follow-up CHiME-6 campaign [14].\nThe progress in ASR brought about by DL is well documented in the literature [15]- [17]. In this contribution we therefore concentrate on those aspects of acoustic modeling that are typical of far-field ASR. But those aspects, although improving the error rate a lot, proved to be insufficient to cope with high reverberation, low SNR and concurrent speech, as is typical of far-field ASR. This is because common ASR feature representations are agnostic to phase (a.k.a. spatial) information and are vulnerable to reverberation, i.e., the temporal dispersion of the signal over multiple analysis frames, and because it is difficult for a single AM to decide which speech source to decode, if multiple are present. Therefore, front-end processing for cleaning up the signals has been developed, including techniques for acoustic beamforming [18,19], dereverberation [20,21], and source separation/extraction [22]. All of those have been shown to significantly improve speech recognition performance, as can be seen in the examples above. In the last years, neural networks (NNs) have challenged the traditional signal processing based solutions for speech enhancement [23]- [25], and achieved excellent performance on a number of tasks. However, those advances come at a price. The networks are notorious for their computational and memory demands, often require large sets of parallel data (clean and distorted version of the same utterance) for training, which have to be matched to the test scenario, and are \"black box\" systems, lacking interpretability by a human. In multi-channel Array Enhancement\nSec. III ASR Sec. IV In addition... The profit... scenarios, it is furthermore not obvious how to handle phase information. As a consequence researchers tried to combine the best of both worlds, i.e., to blend classic multi-channel signal processing with deep learning.\nThe purpose of this tutorial article is to describe the specific challenges of far-field ASR and how they are approached. We will discuss the general components of an ASR system only as much as is necessary to understand the modifications introduced in the far-field scenario. The organization of the paper is oriented along the processing pipeline of a typical far-field ASR as shown in Fig. 1. First, the signal is captured by an array of M microphones. The signal model, which describes the typical distortions encountered, is given in Section II. Although recently good single-channel dereverberation [21] and source separation techniques have been developed [24,26,27], the use of an array of microphones instead of a single one has the clear advantage that spatial information can be exploited, which often leads to a much more effective suppression of noise and competing audio sources, as well as to better dereverberation performance. Dereverberation, acoustic beamforming and source separation/extraction techniques will be discussed in Section III.\nOnce the signal is cleaned up it is forwarded to the ASR back-end, whose task it is to transcribe the audio in a machine readable form. In far-field ASR it is particularly important to make the acoustic model robust against remaining signal degradations. We will explain in Section IV how this can be achieved by socalled multi-style training and by adaptation techniques. Section V discusses end-to-end approaches to ASR. In this rather new approach, the recognizer consists of a monolithic neural network, which directly models the posterior distribution of linguistic units given the audio. This paradigm has recently been extended to the far-field scenario, as we explain in that section.\nWe c",
            "url": "https://arxiv.org/pdf/2009.09395.pdf",
            "openalex_id": ""
          },
          {
            "id": "E9426920602",
            "text": "Rashmi Makhijani\nUrmila Shrawankar\nDrV M Thakare\nProfHod\nAsst. Prof., CSE Deptt. GHRCE\nDeptt. S. G. B\nResearch Scholar\nGHRCE Nagpur\nNagpurIndia, India\nAmaravati University\nAmaravatiIndia\nOpportunities and Challenges in Automatic Speech Recognition\nSpeech Recognitionaccuracythroughputnoisy environmentrecognition latency\nAutomatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.\nIntroduction\nApplications in today's world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4]. Rather, parallel scalability (the ability for an application to efficiently utilize an increasing number of processing elements) is now required for software to obtain sustained performance improvements on successive generations of processors.\nAutomatic Speech Recognition (ASR) is an application that consistently exploits advances in computation capabilities. With the availability of a new generation of highly parallel single-chip computation platforms, ASR researchers are faced with the question of unlimited computing to make speech recognition better. The goal of the work reported here is to explore plausible approaches to improve ASR in three ways:\n1. Improve Accuracy: Account for noisy and reverberant environments in which current systems perform poorly, thereby increasing the range of scenarios where speech technology can be an effective solution.\n2. Improve Throughput: Allow batch processing of the speech recognition task to execute as efficiently as possible, thereby increasing the utility for multimedia search and retrieval.\n3. Improve Latency: Allow speech-based applications, such as speech-to-speech translation, to achieve real-time performance, where speech recognition is just one component of the application.\nThis paper discusses current work as well as opportunities and challenges in these areas with regard to parallelization from the point of view of speech researchers.\nImproving Accuracy\nSpeech recognition systems can be sufficiently accurate when trained with enough data having similar characteristics to the test conditions. However, there still remain many circumstances in which recognition accuracy is quite poor. These include moderately to seriously noisy or reverberant noise conditions, and any variability between training and recognition conditions with respect to channel and speaker characteristics (such as style, emotion, topic, accent, and language).\nOne approach that is both \"embarrassingly\" parallel and effective in improving ASR robustness is the socalled multistream approach. As has been shown for a number of years [5,6,15,11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks. And as noted in [23], recent results have demonstrated that a larger number of feature representations can be particularly effective in the case of noisy speech. In order to conduct research on a massively parallel front end, a large feature space is desired. One approach that found to be useful is to compute spectro-temporal features. These features correspond to the output of filters that are tuned to certain rates of change in the time and frequency dimensions.\nVarious approaches have been devised to combine and select the inherently large number of potential spectrotemporal features because processing them entirely is currently considered computationally intractable.\nCurrent Approach\nCurrent preferred approach to robust feature extraction is to generate many feature streams with different spectrotemporal properties. For instance, some streams might be more sensitive to speech that varies at a slow syllabic rate (e.g., 2 per second) and others might be more sensitive to signals that vary at a higher rate (such as 6 syllables per second). The streams are processed by neural networks (Multi-Layer Perceptrons, or MLPs) trained for discrimination between phones and generate estimates of posterior phone probability distributions.\nFor MLP-based feature streams, the most common combining techniques are: (1) appending all features to a single stream; (2) combining posterior distributions by a product rule, with or without scaling; (3) combining posterior distributions by an additive rule, with or without scaling; and (4) combining posterior distributions by another MLP, which may also use other features.\nCurrent best approach to combination is to train an additional Neural Network to generate combination weights by incorporating entropies from the streams as well as overall spectral information. A 28-stream system is used, including 16 streams from division of temporal modulation frequencies, 8 streams from division by spectral modulation frequencies, and 4 streams from a division by both [23]. Using this method, for the Numbers 95 corpus with the Aurora noises added [12] the average word error rate was 8.1%, reduced from 15.3% for MFCCs and first and second order time derivatives. While robustness to environmental acoustics is the main focus, four equally weighted streams, with quasi-tonotopically divided spectro-temporal features were used. The system yielded a 13.3% relative improvement on the baseline, lowering word error rate from 25.5% to 22.1%.\nFuture Directions\nIn the current approach, the same modulation filters are applied to the entire spectrum. Within this one feature stream, a pipe-and-filter parallel pattern can be used to distribute work across processing elements. Since the MLPs used within the stream depend on dense linear algebra, the wealth of methods to parallelize matrix operations can be exploited. The 28 streams can also be potentially expanded to hundreds or thousands of streams by applying the Gabor filters to different parts of the spectrum as separate streams using a map-reduce parallel pattern.\nThese techniques will be even more important to analyze speech from distant microphones at meetings, a task that naturally provides challenges due to noise and reverberation. Finally, there will be more parallelization considerations in combining the many stream methods with conventional approaches to noise robustness. As many stream feature combination naturally adapt to parallel computing architectures, the improvement will be significant.\nImproving Throughput\nBatch speech transcription can be \"embarrassingly parallel\" by distributing different speech utterances to different machines. However, there is significant value in improving compute efficiency, which is increasingly relevant in today's energy limited and form-factor limited devices and compute facilities. The many components of an ASR system can be partitioned into a feature extractor and an inference engine. The speech feature extractor collects feature vectors from input audio waveforms using a sequence of signal processing steps in a data flow framework. Many levels of parallelism can be exploited within a step, as well as across steps, as described in section 2.1. Thus feature extraction is highly scalable with respect to the parallel platform advances. However, parallelizing the inference engine requires surmounting significant challenges.\nThe inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network. In a typical recognition process, there are significant parallelization challenges in concurrently evaluating thousands of alternative interpretations of a speech utterance to find the most likely interpretation. The traversal is conducted over an irregular graph-based knowledge network and is controlled by a sequence of audio features known only at run time. Furthermore, the data working set changes dynamically during the traversal process and the algorithm requires frequent communication between concurrent tasks. These problem characteristics lead to unpredictable memory accesses and poor data locality and cause significant challenges in load balancing and efficient synchronization between processor cores. There have been many attempts to parallelize speech recognition on emerging platforms, leveraging both fine grained and coarse-grained concurrency in the application. Fine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20]. The implementation statically mapped a carefully partitioned recognition network onto the multiprocessors to minimize load imbalance. [14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture. [22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP. The Viterbi search was parallelized by statically partitioning a tree-lexical search network across cores. The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network. Prior works such as [10,7] leveraged many core processors and focused on speeding up the",
            "url": "https://arxiv.org/pdf/1305.2846.pdf",
            "openalex_id": ""
          }
        ]
      },
      "S3683972091": {
        "id": "S3683972091",
        "text": "Future directions for enhancing the effectiveness of end-to-end speech recognition systems may involve hybrid approaches that combine the strengths of different architectures, such as LAS and RNNT, to improve both accuracy and real-time performance.",
        "children": [
          {
            "id": "E9426920602",
            "text": "Rashmi Makhijani\nUrmila Shrawankar\nDrV M Thakare\nProfHod\nAsst. Prof., CSE Deptt. GHRCE\nDeptt. S. G. B\nResearch Scholar\nGHRCE Nagpur\nNagpurIndia, India\nAmaravati University\nAmaravatiIndia\nOpportunities and Challenges in Automatic Speech Recognition\nSpeech Recognitionaccuracythroughputnoisy environmentrecognition latency\nAutomatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.\nIntroduction\nApplications in today's world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4]. Rather, parallel scalability (the ability for an application to efficiently utilize an increasing number of processing elements) is now required for software to obtain sustained performance improvements on successive generations of processors.\nAutomatic Speech Recognition (ASR) is an application that consistently exploits advances in computation capabilities. With the availability of a new generation of highly parallel single-chip computation platforms, ASR researchers are faced with the question of unlimited computing to make speech recognition better. The goal of the work reported here is to explore plausible approaches to improve ASR in three ways:\n1. Improve Accuracy: Account for noisy and reverberant environments in which current systems perform poorly, thereby increasing the range of scenarios where speech technology can be an effective solution.\n2. Improve Throughput: Allow batch processing of the speech recognition task to execute as efficiently as possible, thereby increasing the utility for multimedia search and retrieval.\n3. Improve Latency: Allow speech-based applications, such as speech-to-speech translation, to achieve real-time performance, where speech recognition is just one component of the application.\nThis paper discusses current work as well as opportunities and challenges in these areas with regard to parallelization from the point of view of speech researchers.\nImproving Accuracy\nSpeech recognition systems can be sufficiently accurate when trained with enough data having similar characteristics to the test conditions. However, there still remain many circumstances in which recognition accuracy is quite poor. These include moderately to seriously noisy or reverberant noise conditions, and any variability between training and recognition conditions with respect to channel and speaker characteristics (such as style, emotion, topic, accent, and language).\nOne approach that is both \"embarrassingly\" parallel and effective in improving ASR robustness is the socalled multistream approach. As has been shown for a number of years [5,6,15,11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks. And as noted in [23], recent results have demonstrated that a larger number of feature representations can be particularly effective in the case of noisy speech. In order to conduct research on a massively parallel front end, a large feature space is desired. One approach that found to be useful is to compute spectro-temporal features. These features correspond to the output of filters that are tuned to certain rates of change in the time and frequency dimensions.\nVarious approaches have been devised to combine and select the inherently large number of potential spectrotemporal features because processing them entirely is currently considered computationally intractable.\nCurrent Approach\nCurrent preferred approach to robust feature extraction is to generate many feature streams with different spectrotemporal properties. For instance, some streams might be more sensitive to speech that varies at a slow syllabic rate (e.g., 2 per second) and others might be more sensitive to signals that vary at a higher rate (such as 6 syllables per second). The streams are processed by neural networks (Multi-Layer Perceptrons, or MLPs) trained for discrimination between phones and generate estimates of posterior phone probability distributions.\nFor MLP-based feature streams, the most common combining techniques are: (1) appending all features to a single stream; (2) combining posterior distributions by a product rule, with or without scaling; (3) combining posterior distributions by an additive rule, with or without scaling; and (4) combining posterior distributions by another MLP, which may also use other features.\nCurrent best approach to combination is to train an additional Neural Network to generate combination weights by incorporating entropies from the streams as well as overall spectral information. A 28-stream system is used, including 16 streams from division of temporal modulation frequencies, 8 streams from division by spectral modulation frequencies, and 4 streams from a division by both [23]. Using this method, for the Numbers 95 corpus with the Aurora noises added [12] the average word error rate was 8.1%, reduced from 15.3% for MFCCs and first and second order time derivatives. While robustness to environmental acoustics is the main focus, four equally weighted streams, with quasi-tonotopically divided spectro-temporal features were used. The system yielded a 13.3% relative improvement on the baseline, lowering word error rate from 25.5% to 22.1%.\nFuture Directions\nIn the current approach, the same modulation filters are applied to the entire spectrum. Within this one feature stream, a pipe-and-filter parallel pattern can be used to distribute work across processing elements. Since the MLPs used within the stream depend on dense linear algebra, the wealth of methods to parallelize matrix operations can be exploited. The 28 streams can also be potentially expanded to hundreds or thousands of streams by applying the Gabor filters to different parts of the spectrum as separate streams using a map-reduce parallel pattern.\nThese techniques will be even more important to analyze speech from distant microphones at meetings, a task that naturally provides challenges due to noise and reverberation. Finally, there will be more parallelization considerations in combining the many stream methods with conventional approaches to noise robustness. As many stream feature combination naturally adapt to parallel computing architectures, the improvement will be significant.\nImproving Throughput\nBatch speech transcription can be \"embarrassingly parallel\" by distributing different speech utterances to different machines. However, there is significant value in improving compute efficiency, which is increasingly relevant in today's energy limited and form-factor limited devices and compute facilities. The many components of an ASR system can be partitioned into a feature extractor and an inference engine. The speech feature extractor collects feature vectors from input audio waveforms using a sequence of signal processing steps in a data flow framework. Many levels of parallelism can be exploited within a step, as well as across steps, as described in section 2.1. Thus feature extraction is highly scalable with respect to the parallel platform advances. However, parallelizing the inference engine requires surmounting significant challenges.\nThe inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network. In a typical recognition process, there are significant parallelization challenges in concurrently evaluating thousands of alternative interpretations of a speech utterance to find the most likely interpretation. The traversal is conducted over an irregular graph-based knowledge network and is controlled by a sequence of audio features known only at run time. Furthermore, the data working set changes dynamically during the traversal process and the algorithm requires frequent communication between concurrent tasks. These problem characteristics lead to unpredictable memory accesses and poor data locality and cause significant challenges in load balancing and efficient synchronization between processor cores. There have been many attempts to parallelize speech recognition on emerging platforms, leveraging both fine grained and coarse-grained concurrency in the application. Fine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20]. The implementation statically mapped a carefully partitioned recognition network onto the multiprocessors to minimize load imbalance. [14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture. [22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP. The Viterbi search was parallelized by statically partitioning a tree-lexical search network across cores. The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network. Prior works such as [10,7] leveraged many core processors and focused on speeding up the",
            "url": "https://arxiv.org/pdf/1305.2846.pdf",
            "openalex_id": ""
          },
          {
            "id": "E2700463692",
            "text": "Electrical Engineering and Systems Science &gt; Audio and Speech Processing\n \n arXiv:2111.01690 (eess)\n[Submitted on 2 Nov 2021 (v1), last revised 2 Feb 2022 (this version, v2)]\n View PDF  \nAbstract:Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.\nSubmission history  From: Jinyu Li [view email]   [v1] \nTue, 2 Nov 2021 15:49:20 UTC (6,878 KB)\n[v2]\nWed, 2 Feb 2022 23:38:10 UTC (6,908 KB)\n \n \nCurrent browse context:  eess.AS\nexport BibTeX citation\n  Bookmark    \n  \n \n \nBibliographic and Citation Tools\n \nCode, Data and Media Associated with this Article\n \nDemos\n \nRecommenders and Search Tools\n \narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?  Learn more about arXivLabs .",
            "url": "https://arxiv.org/abs/2111.01690",
            "openalex_id": ""
          }
        ]
      },
      "S3568884510": {
        "id": "S3568884510",
        "text": "Challenges in deploying end-to-end speech recognition systems include their robustness in noisy environments and the need for real-time processing capabilities, which traditional hybrid models currently handle better.",
        "children": [
          {
            "id": "E9426920602",
            "text": "Rashmi Makhijani\nUrmila Shrawankar\nDrV M Thakare\nProfHod\nAsst. Prof., CSE Deptt. GHRCE\nDeptt. S. G. B\nResearch Scholar\nGHRCE Nagpur\nNagpurIndia, India\nAmaravati University\nAmaravatiIndia\nOpportunities and Challenges in Automatic Speech Recognition\nSpeech Recognitionaccuracythroughputnoisy environmentrecognition latency\nAutomatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.\nIntroduction\nApplications in today's world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4]. Rather, parallel scalability (the ability for an application to efficiently utilize an increasing number of processing elements) is now required for software to obtain sustained performance improvements on successive generations of processors.\nAutomatic Speech Recognition (ASR) is an application that consistently exploits advances in computation capabilities. With the availability of a new generation of highly parallel single-chip computation platforms, ASR researchers are faced with the question of unlimited computing to make speech recognition better. The goal of the work reported here is to explore plausible approaches to improve ASR in three ways:\n1. Improve Accuracy: Account for noisy and reverberant environments in which current systems perform poorly, thereby increasing the range of scenarios where speech technology can be an effective solution.\n2. Improve Throughput: Allow batch processing of the speech recognition task to execute as efficiently as possible, thereby increasing the utility for multimedia search and retrieval.\n3. Improve Latency: Allow speech-based applications, such as speech-to-speech translation, to achieve real-time performance, where speech recognition is just one component of the application.\nThis paper discusses current work as well as opportunities and challenges in these areas with regard to parallelization from the point of view of speech researchers.\nImproving Accuracy\nSpeech recognition systems can be sufficiently accurate when trained with enough data having similar characteristics to the test conditions. However, there still remain many circumstances in which recognition accuracy is quite poor. These include moderately to seriously noisy or reverberant noise conditions, and any variability between training and recognition conditions with respect to channel and speaker characteristics (such as style, emotion, topic, accent, and language).\nOne approach that is both \"embarrassingly\" parallel and effective in improving ASR robustness is the socalled multistream approach. As has been shown for a number of years [5,6,15,11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks. And as noted in [23], recent results have demonstrated that a larger number of feature representations can be particularly effective in the case of noisy speech. In order to conduct research on a massively parallel front end, a large feature space is desired. One approach that found to be useful is to compute spectro-temporal features. These features correspond to the output of filters that are tuned to certain rates of change in the time and frequency dimensions.\nVarious approaches have been devised to combine and select the inherently large number of potential spectrotemporal features because processing them entirely is currently considered computationally intractable.\nCurrent Approach\nCurrent preferred approach to robust feature extraction is to generate many feature streams with different spectrotemporal properties. For instance, some streams might be more sensitive to speech that varies at a slow syllabic rate (e.g., 2 per second) and others might be more sensitive to signals that vary at a higher rate (such as 6 syllables per second). The streams are processed by neural networks (Multi-Layer Perceptrons, or MLPs) trained for discrimination between phones and generate estimates of posterior phone probability distributions.\nFor MLP-based feature streams, the most common combining techniques are: (1) appending all features to a single stream; (2) combining posterior distributions by a product rule, with or without scaling; (3) combining posterior distributions by an additive rule, with or without scaling; and (4) combining posterior distributions by another MLP, which may also use other features.\nCurrent best approach to combination is to train an additional Neural Network to generate combination weights by incorporating entropies from the streams as well as overall spectral information. A 28-stream system is used, including 16 streams from division of temporal modulation frequencies, 8 streams from division by spectral modulation frequencies, and 4 streams from a division by both [23]. Using this method, for the Numbers 95 corpus with the Aurora noises added [12] the average word error rate was 8.1%, reduced from 15.3% for MFCCs and first and second order time derivatives. While robustness to environmental acoustics is the main focus, four equally weighted streams, with quasi-tonotopically divided spectro-temporal features were used. The system yielded a 13.3% relative improvement on the baseline, lowering word error rate from 25.5% to 22.1%.\nFuture Directions\nIn the current approach, the same modulation filters are applied to the entire spectrum. Within this one feature stream, a pipe-and-filter parallel pattern can be used to distribute work across processing elements. Since the MLPs used within the stream depend on dense linear algebra, the wealth of methods to parallelize matrix operations can be exploited. The 28 streams can also be potentially expanded to hundreds or thousands of streams by applying the Gabor filters to different parts of the spectrum as separate streams using a map-reduce parallel pattern.\nThese techniques will be even more important to analyze speech from distant microphones at meetings, a task that naturally provides challenges due to noise and reverberation. Finally, there will be more parallelization considerations in combining the many stream methods with conventional approaches to noise robustness. As many stream feature combination naturally adapt to parallel computing architectures, the improvement will be significant.\nImproving Throughput\nBatch speech transcription can be \"embarrassingly parallel\" by distributing different speech utterances to different machines. However, there is significant value in improving compute efficiency, which is increasingly relevant in today's energy limited and form-factor limited devices and compute facilities. The many components of an ASR system can be partitioned into a feature extractor and an inference engine. The speech feature extractor collects feature vectors from input audio waveforms using a sequence of signal processing steps in a data flow framework. Many levels of parallelism can be exploited within a step, as well as across steps, as described in section 2.1. Thus feature extraction is highly scalable with respect to the parallel platform advances. However, parallelizing the inference engine requires surmounting significant challenges.\nThe inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network. In a typical recognition process, there are significant parallelization challenges in concurrently evaluating thousands of alternative interpretations of a speech utterance to find the most likely interpretation. The traversal is conducted over an irregular graph-based knowledge network and is controlled by a sequence of audio features known only at run time. Furthermore, the data working set changes dynamically during the traversal process and the algorithm requires frequent communication between concurrent tasks. These problem characteristics lead to unpredictable memory accesses and poor data locality and cause significant challenges in load balancing and efficient synchronization between processor cores. There have been many attempts to parallelize speech recognition on emerging platforms, leveraging both fine grained and coarse-grained concurrency in the application. Fine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20]. The implementation statically mapped a carefully partitioned recognition network onto the multiprocessors to minimize load imbalance. [14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture. [22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP. The Viterbi search was parallelized by statically partitioning a tree-lexical search network across cores. The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network. Prior works such as [10,7] leveraged many core processors and focused on speeding up the",
            "url": "https://arxiv.org/pdf/1305.2846.pdf",
            "openalex_id": ""
          },
          {
            "id": "E0960993452",
            "text": "Member, IEEERohit Prabhavalkar\nSenior Member, IEEETakaaki Hori\nFellow, IEEETara N Sainath\nSenior Member, IEEERalf Schl\u00fcter\nFellow, IEEEShinji Watanabe\nEnd-to-End Speech Recognition: A Survey\n1Index Terms-end-to-end, automatic speech recognition\nIn the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domainspecific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.\nI. INTRODUCTION\nThe classical 1 statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule [1], [2]. Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation. Within the classical approach, deep learning has been introduced to acoustic and language modeling. In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM [3], [4]) or augmented the acoustic feature set (nonlinear disciminant/tandem approach [5], [6]). In language modeling, deep learning replaced count-based approaches [7], [8], [9]. However, when introducing deep learning, the classical ASR architecture was not yet touched. Classical stateof-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t. recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl. sequence discriminative training, etc. The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g. integrating speech signal preprocessing and feature extraction into acoustic modeling [10], [11].\nMore consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [12], [13], [14], [15]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [16], [17], [13], [18] has been established. However, it lacks distinction in many ways. Therefore, first of all an attempt to defining the term end-to-end in the context of ASR is due in this survey.\nAccording to the Cambridge Dictionary, the adjective \"endto-end\" is defined by: \"including all the stages of a process\" [19]. This can be regarded from a number of perspectives: a) Joint Modeling: In terms of ASR, the E2E property might be understood as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which also may mean dropping the classical separation of ASR into an acoustic model and a language model. b) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes' decision rule, which exactly requires a single global decision integrating all available knowledge sources. c) Joint Training: In terms of model training, E2E suggests estimating all parameters of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate. d) Training Data: Joint training of an integrated model implies using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data [20], [21]. e) Training from Scratch: The E2E property can also be interpreted for the training process itself, by requiring training from scratch avoiding external knowledge like prior alignments or initial models pre-trained using different criteria and/or knowledge sources. Note that pre-training and finetuning strategies are also important in some scenarios, if the model has explicit modularity, including self-supervised learn-ing [22] or joint training of front-end and speech recognition models [23].\nf) Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [24]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. g) Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units would limit E2E recognition vocabularies to be based on whole word or character models. Whole word models [25], according to Zipf's law [26], would require unrealisticly high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [27], might be seen as secondary approaches outside the E2E objective.\nh) Generic vs. Informed Modeling: Finally, E2E may also be seen in terms of the genericity of the underlying modeling: are task-specific constraints learned from data completely, or does task-specific knowledge enter modeling the system architecture in the first place? For example, the monotonicity constraint in ASR may be learned completely from data like in attention-based E2E approaches [15], or it may directly be implemented, as in classical HMM structures.\nOverall, end-to-end ASR therefore can be defined as an integrated ASR model that enables joint training and recognition consistently minimizing expected word error rate, avoiding separately obtained knowledge sources.\nHowever, what are potential benefits of E2E approaches to ASR? The primary objective when developing ASR systems is to minimize the expected word error rate. However, secondary objectives are to reduce time and memory complexity of the resulting decoder, and -assuming a constrained development budget -genericity and ease of modeling.\nFirst of all, defining an ASR system E2E in terms of a single neural network structure supports modeling genericity and may allow for faster development cycles when building ASR systems for new languages/domains. ASR models defined by a single neural network structure may become more lean compared to classical modeling, and the decoding process becomes simpler as it does not need to integrate separate models. The resulting reduction in memory footprint and power consumption supports embedded ASR applications [28].\nFurthermore, joint training E2E may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may itself be erroneous. Avoiding these supports improved models trained directly from data, provided that sufficient amounts of taskspecific training data are available.\nWith the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research w.r.t. E2E ASR systems, covering all relevant aspects of E2E ASR and including a contrastive discussion of the different E2E and classical ASR architectures.\nThis survey of E2E speech recognition is structured as follows. Sec. II describes the historical evolution of E2E speech recognition, with specific focus on the input/output alignment and an overview of currently prominent basic E2E ASR models. Sec. III discusses improvements of the basic E2E models, incl. E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. IV provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. V. Sec. VI discusses the role and integ",
            "url": "https://arxiv.org/pdf/2303.03329.pdf",
            "openalex_id": ""
          }
        ]
      },
      "S2087648615": {
        "id": "S2087648615",
        "text": "End-to-end speech recognition systems have transitioned from traditional hybrid models to fully neural architectures, significantly improving accuracy and efficiency in automatic speech recognition tasks.",
        "children": [
          {
            "id": "E0960993452",
            "text": "Member, IEEERohit Prabhavalkar\nSenior Member, IEEETakaaki Hori\nFellow, IEEETara N Sainath\nSenior Member, IEEERalf Schl\u00fcter\nFellow, IEEEShinji Watanabe\nEnd-to-End Speech Recognition: A Survey\n1Index Terms-end-to-end, automatic speech recognition\nIn the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domainspecific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.\nI. INTRODUCTION\nThe classical 1 statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule [1], [2]. Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation. Within the classical approach, deep learning has been introduced to acoustic and language modeling. In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM [3], [4]) or augmented the acoustic feature set (nonlinear disciminant/tandem approach [5], [6]). In language modeling, deep learning replaced count-based approaches [7], [8], [9]. However, when introducing deep learning, the classical ASR architecture was not yet touched. Classical stateof-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t. recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl. sequence discriminative training, etc. The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g. integrating speech signal preprocessing and feature extraction into acoustic modeling [10], [11].\nMore consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [12], [13], [14], [15]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [16], [17], [13], [18] has been established. However, it lacks distinction in many ways. Therefore, first of all an attempt to defining the term end-to-end in the context of ASR is due in this survey.\nAccording to the Cambridge Dictionary, the adjective \"endto-end\" is defined by: \"including all the stages of a process\" [19]. This can be regarded from a number of perspectives: a) Joint Modeling: In terms of ASR, the E2E property might be understood as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which also may mean dropping the classical separation of ASR into an acoustic model and a language model. b) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes' decision rule, which exactly requires a single global decision integrating all available knowledge sources. c) Joint Training: In terms of model training, E2E suggests estimating all parameters of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate. d) Training Data: Joint training of an integrated model implies using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data [20], [21]. e) Training from Scratch: The E2E property can also be interpreted for the training process itself, by requiring training from scratch avoiding external knowledge like prior alignments or initial models pre-trained using different criteria and/or knowledge sources. Note that pre-training and finetuning strategies are also important in some scenarios, if the model has explicit modularity, including self-supervised learn-ing [22] or joint training of front-end and speech recognition models [23].\nf) Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [24]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. g) Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units would limit E2E recognition vocabularies to be based on whole word or character models. Whole word models [25], according to Zipf's law [26], would require unrealisticly high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [27], might be seen as secondary approaches outside the E2E objective.\nh) Generic vs. Informed Modeling: Finally, E2E may also be seen in terms of the genericity of the underlying modeling: are task-specific constraints learned from data completely, or does task-specific knowledge enter modeling the system architecture in the first place? For example, the monotonicity constraint in ASR may be learned completely from data like in attention-based E2E approaches [15], or it may directly be implemented, as in classical HMM structures.\nOverall, end-to-end ASR therefore can be defined as an integrated ASR model that enables joint training and recognition consistently minimizing expected word error rate, avoiding separately obtained knowledge sources.\nHowever, what are potential benefits of E2E approaches to ASR? The primary objective when developing ASR systems is to minimize the expected word error rate. However, secondary objectives are to reduce time and memory complexity of the resulting decoder, and -assuming a constrained development budget -genericity and ease of modeling.\nFirst of all, defining an ASR system E2E in terms of a single neural network structure supports modeling genericity and may allow for faster development cycles when building ASR systems for new languages/domains. ASR models defined by a single neural network structure may become more lean compared to classical modeling, and the decoding process becomes simpler as it does not need to integrate separate models. The resulting reduction in memory footprint and power consumption supports embedded ASR applications [28].\nFurthermore, joint training E2E may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may itself be erroneous. Avoiding these supports improved models trained directly from data, provided that sufficient amounts of taskspecific training data are available.\nWith the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research w.r.t. E2E ASR systems, covering all relevant aspects of E2E ASR and including a contrastive discussion of the different E2E and classical ASR architectures.\nThis survey of E2E speech recognition is structured as follows. Sec. II describes the historical evolution of E2E speech recognition, with specific focus on the input/output alignment and an overview of currently prominent basic E2E ASR models. Sec. III discusses improvements of the basic E2E models, incl. E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. IV provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. V. Sec. VI discusses the role and integ",
            "url": "https://arxiv.org/pdf/2303.03329.pdf",
            "openalex_id": ""
          },
          {
            "id": "E9426920602",
            "text": "Rashmi Makhijani\nUrmila Shrawankar\nDrV M Thakare\nProfHod\nAsst. Prof., CSE Deptt. GHRCE\nDeptt. S. G. B\nResearch Scholar\nGHRCE Nagpur\nNagpurIndia, India\nAmaravati University\nAmaravatiIndia\nOpportunities and Challenges in Automatic Speech Recognition\nSpeech Recognitionaccuracythroughputnoisy environmentrecognition latency\nAutomatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.\nIntroduction\nApplications in today's world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4]. Rather, parallel scalability (the ability for an application to efficiently utilize an increasing number of processing elements) is now required for software to obtain sustained performance improvements on successive generations of processors.\nAutomatic Speech Recognition (ASR) is an application that consistently exploits advances in computation capabilities. With the availability of a new generation of highly parallel single-chip computation platforms, ASR researchers are faced with the question of unlimited computing to make speech recognition better. The goal of the work reported here is to explore plausible approaches to improve ASR in three ways:\n1. Improve Accuracy: Account for noisy and reverberant environments in which current systems perform poorly, thereby increasing the range of scenarios where speech technology can be an effective solution.\n2. Improve Throughput: Allow batch processing of the speech recognition task to execute as efficiently as possible, thereby increasing the utility for multimedia search and retrieval.\n3. Improve Latency: Allow speech-based applications, such as speech-to-speech translation, to achieve real-time performance, where speech recognition is just one component of the application.\nThis paper discusses current work as well as opportunities and challenges in these areas with regard to parallelization from the point of view of speech researchers.\nImproving Accuracy\nSpeech recognition systems can be sufficiently accurate when trained with enough data having similar characteristics to the test conditions. However, there still remain many circumstances in which recognition accuracy is quite poor. These include moderately to seriously noisy or reverberant noise conditions, and any variability between training and recognition conditions with respect to channel and speaker characteristics (such as style, emotion, topic, accent, and language).\nOne approach that is both \"embarrassingly\" parallel and effective in improving ASR robustness is the socalled multistream approach. As has been shown for a number of years [5,6,15,11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks. And as noted in [23], recent results have demonstrated that a larger number of feature representations can be particularly effective in the case of noisy speech. In order to conduct research on a massively parallel front end, a large feature space is desired. One approach that found to be useful is to compute spectro-temporal features. These features correspond to the output of filters that are tuned to certain rates of change in the time and frequency dimensions.\nVarious approaches have been devised to combine and select the inherently large number of potential spectrotemporal features because processing them entirely is currently considered computationally intractable.\nCurrent Approach\nCurrent preferred approach to robust feature extraction is to generate many feature streams with different spectrotemporal properties. For instance, some streams might be more sensitive to speech that varies at a slow syllabic rate (e.g., 2 per second) and others might be more sensitive to signals that vary at a higher rate (such as 6 syllables per second). The streams are processed by neural networks (Multi-Layer Perceptrons, or MLPs) trained for discrimination between phones and generate estimates of posterior phone probability distributions.\nFor MLP-based feature streams, the most common combining techniques are: (1) appending all features to a single stream; (2) combining posterior distributions by a product rule, with or without scaling; (3) combining posterior distributions by an additive rule, with or without scaling; and (4) combining posterior distributions by another MLP, which may also use other features.\nCurrent best approach to combination is to train an additional Neural Network to generate combination weights by incorporating entropies from the streams as well as overall spectral information. A 28-stream system is used, including 16 streams from division of temporal modulation frequencies, 8 streams from division by spectral modulation frequencies, and 4 streams from a division by both [23]. Using this method, for the Numbers 95 corpus with the Aurora noises added [12] the average word error rate was 8.1%, reduced from 15.3% for MFCCs and first and second order time derivatives. While robustness to environmental acoustics is the main focus, four equally weighted streams, with quasi-tonotopically divided spectro-temporal features were used. The system yielded a 13.3% relative improvement on the baseline, lowering word error rate from 25.5% to 22.1%.\nFuture Directions\nIn the current approach, the same modulation filters are applied to the entire spectrum. Within this one feature stream, a pipe-and-filter parallel pattern can be used to distribute work across processing elements. Since the MLPs used within the stream depend on dense linear algebra, the wealth of methods to parallelize matrix operations can be exploited. The 28 streams can also be potentially expanded to hundreds or thousands of streams by applying the Gabor filters to different parts of the spectrum as separate streams using a map-reduce parallel pattern.\nThese techniques will be even more important to analyze speech from distant microphones at meetings, a task that naturally provides challenges due to noise and reverberation. Finally, there will be more parallelization considerations in combining the many stream methods with conventional approaches to noise robustness. As many stream feature combination naturally adapt to parallel computing architectures, the improvement will be significant.\nImproving Throughput\nBatch speech transcription can be \"embarrassingly parallel\" by distributing different speech utterances to different machines. However, there is significant value in improving compute efficiency, which is increasingly relevant in today's energy limited and form-factor limited devices and compute facilities. The many components of an ASR system can be partitioned into a feature extractor and an inference engine. The speech feature extractor collects feature vectors from input audio waveforms using a sequence of signal processing steps in a data flow framework. Many levels of parallelism can be exploited within a step, as well as across steps, as described in section 2.1. Thus feature extraction is highly scalable with respect to the parallel platform advances. However, parallelizing the inference engine requires surmounting significant challenges.\nThe inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network. In a typical recognition process, there are significant parallelization challenges in concurrently evaluating thousands of alternative interpretations of a speech utterance to find the most likely interpretation. The traversal is conducted over an irregular graph-based knowledge network and is controlled by a sequence of audio features known only at run time. Furthermore, the data working set changes dynamically during the traversal process and the algorithm requires frequent communication between concurrent tasks. These problem characteristics lead to unpredictable memory accesses and poor data locality and cause significant challenges in load balancing and efficient synchronization between processor cores. There have been many attempts to parallelize speech recognition on emerging platforms, leveraging both fine grained and coarse-grained concurrency in the application. Fine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20]. The implementation statically mapped a carefully partitioned recognition network onto the multiprocessors to minimize load imbalance. [14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture. [22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP. The Viterbi search was parallelized by statically partitioning a tree-lexical search network across cores. The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network. Prior works such as [10,7] leveraged many core processors and focused on speeding up the",
            "url": "https://arxiv.org/pdf/1305.2846.pdf",
            "openalex_id": ""
          }
        ]
      },
      "S3597456458": {
        "id": "S3597456458",
        "text": "The introduction of deep learning techniques in end-to-end speech recognition has led to a reduction in word error rates by more than 50% compared to previous models, demonstrating substantial advancements in performance.",
        "children": [
          {
            "id": "E0960993452",
            "text": "Member, IEEERohit Prabhavalkar\nSenior Member, IEEETakaaki Hori\nFellow, IEEETara N Sainath\nSenior Member, IEEERalf Schl\u00fcter\nFellow, IEEEShinji Watanabe\nEnd-to-End Speech Recognition: A Survey\n1Index Terms-end-to-end, automatic speech recognition\nIn the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domainspecific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.\nI. INTRODUCTION\nThe classical 1 statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule [1], [2]. Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation. Within the classical approach, deep learning has been introduced to acoustic and language modeling. In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM [3], [4]) or augmented the acoustic feature set (nonlinear disciminant/tandem approach [5], [6]). In language modeling, deep learning replaced count-based approaches [7], [8], [9]. However, when introducing deep learning, the classical ASR architecture was not yet touched. Classical stateof-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t. recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl. sequence discriminative training, etc. The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g. integrating speech signal preprocessing and feature extraction into acoustic modeling [10], [11].\nMore consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [12], [13], [14], [15]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [16], [17], [13], [18] has been established. However, it lacks distinction in many ways. Therefore, first of all an attempt to defining the term end-to-end in the context of ASR is due in this survey.\nAccording to the Cambridge Dictionary, the adjective \"endto-end\" is defined by: \"including all the stages of a process\" [19]. This can be regarded from a number of perspectives: a) Joint Modeling: In terms of ASR, the E2E property might be understood as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which also may mean dropping the classical separation of ASR into an acoustic model and a language model. b) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes' decision rule, which exactly requires a single global decision integrating all available knowledge sources. c) Joint Training: In terms of model training, E2E suggests estimating all parameters of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate. d) Training Data: Joint training of an integrated model implies using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data [20], [21]. e) Training from Scratch: The E2E property can also be interpreted for the training process itself, by requiring training from scratch avoiding external knowledge like prior alignments or initial models pre-trained using different criteria and/or knowledge sources. Note that pre-training and finetuning strategies are also important in some scenarios, if the model has explicit modularity, including self-supervised learn-ing [22] or joint training of front-end and speech recognition models [23].\nf) Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [24]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. g) Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units would limit E2E recognition vocabularies to be based on whole word or character models. Whole word models [25], according to Zipf's law [26], would require unrealisticly high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [27], might be seen as secondary approaches outside the E2E objective.\nh) Generic vs. Informed Modeling: Finally, E2E may also be seen in terms of the genericity of the underlying modeling: are task-specific constraints learned from data completely, or does task-specific knowledge enter modeling the system architecture in the first place? For example, the monotonicity constraint in ASR may be learned completely from data like in attention-based E2E approaches [15], or it may directly be implemented, as in classical HMM structures.\nOverall, end-to-end ASR therefore can be defined as an integrated ASR model that enables joint training and recognition consistently minimizing expected word error rate, avoiding separately obtained knowledge sources.\nHowever, what are potential benefits of E2E approaches to ASR? The primary objective when developing ASR systems is to minimize the expected word error rate. However, secondary objectives are to reduce time and memory complexity of the resulting decoder, and -assuming a constrained development budget -genericity and ease of modeling.\nFirst of all, defining an ASR system E2E in terms of a single neural network structure supports modeling genericity and may allow for faster development cycles when building ASR systems for new languages/domains. ASR models defined by a single neural network structure may become more lean compared to classical modeling, and the decoding process becomes simpler as it does not need to integrate separate models. The resulting reduction in memory footprint and power consumption supports embedded ASR applications [28].\nFurthermore, joint training E2E may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may itself be erroneous. Avoiding these supports improved models trained directly from data, provided that sufficient amounts of taskspecific training data are available.\nWith the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research w.r.t. E2E ASR systems, covering all relevant aspects of E2E ASR and including a contrastive discussion of the different E2E and classical ASR architectures.\nThis survey of E2E speech recognition is structured as follows. Sec. II describes the historical evolution of E2E speech recognition, with specific focus on the input/output alignment and an overview of currently prominent basic E2E ASR models. Sec. III discusses improvements of the basic E2E models, incl. E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. IV provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. V. Sec. VI discusses the role and integ",
            "url": "https://arxiv.org/pdf/2303.03329.pdf",
            "openalex_id": ""
          },
          {
            "id": "E2700463692",
            "text": "Electrical Engineering and Systems Science &gt; Audio and Speech Processing\n \n arXiv:2111.01690 (eess)\n[Submitted on 2 Nov 2021 (v1), last revised 2 Feb 2022 (this version, v2)]\n View PDF  \nAbstract:Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.\nSubmission history  From: Jinyu Li [view email]   [v1] \nTue, 2 Nov 2021 15:49:20 UTC (6,878 KB)\n[v2]\nWed, 2 Feb 2022 23:38:10 UTC (6,908 KB)\n \n \nCurrent browse context:  eess.AS\nexport BibTeX citation\n  Bookmark    \n  \n \n \nBibliographic and Citation Tools\n \nCode, Data and Media Associated with this Article\n \nDemos\n \nRecommenders and Search Tools\n \narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?  Learn more about arXivLabs .",
            "url": "https://arxiv.org/abs/2111.01690",
            "openalex_id": ""
          }
        ]
      }
    }
  }
]