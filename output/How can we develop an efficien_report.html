
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>How can we develop an efficient and scalable method for performing k-NN search with cross-encoders that significantly reduces computational costs and resource demands while maintaining high recall accuracy in large-scale datasets?</title>
            <style>
                body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; }
                h1 { color: #333; }
                table { border-collapse: collapse; width: 100%; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
            </style>
        </head>
        <body>
            <h1>How can we develop an efficient and scalable method for performing k-NN search with cross-encoders that significantly reduces computational costs and resource demands while maintaining high recall accuracy in large-scale datasets?</h1>
            <h2>Latest Available Answer</h2>
            <p>The proposed sparse-matrix factorization approach for k-NN search with cross-encoders significantly reduces offline indexing costs compared to existing methods. <a href="#S6109812601">[S6109812601]</a><br>Cross-encoder models outperform dual-encoder models in estimating query-item relevance, leading to better recall in k-NN search. <a href="#S0579617533">[S0579617533]</a><br>The proposed k-NN search method achieves up to 54% improvement in recall over dual-encoder approaches while significantly speeding up indexing processes. <a href="#S3438308611">[S3438308611]</a><br>An iterative retrieval process that alternates between estimating and updating test query embeddings enhances search efficiency. <a href="#S8401159017">[S8401159017]</a><br>k-NN search is commonly applied in recommender systems across industries such as e-commerce, entertainment, and social media, utilizing item similarity for personalized recommendations. <a href="#S6239365205">[S6239365205]</a><br>Hybrid optimization techniques, such as combining genetic algorithms with local search strategies, can improve the efficiency of k-NN search processes. <a href="#S6122934702">[S6122934702]</a><br>Using hybrid search techniques that combine score normalization methods can significantly enhance the efficiency and recall accuracy of k-NN search. <a href="#S8613303667">[S8613303667]</a><br>Implementing distributed, in-memory k-NN optimization techniques can significantly improve the efficiency of k-NN search operations. <a href="#S2612945597">[S2612945597]</a><br>Cross-encoders provide high accuracy in k-NN search, significantly improving recall compared to dual-encoder models. <a href="#S2202772347">[S2202772347]</a><br>Dynamic datasets can negatively impact the efficiency of k-NN search methods, particularly in high-dimensional spaces where computational complexity increases. <a href="#S8802393801">[S8802393801]</a><br></p>
            <h2>Support Table</h2>
            
        <table>
            <tr>
                <th>ID</th>
                <th>Text</th>
                <th>Support</th>
            </tr>
        
            <tr id="S6109812601">
                <td>S6109812601</td>
                <td>{'evidence': ['E1863991955', 'E8732008330', 'E6911890933'], 'explanation': 'The proposed method significantly reduces offline indexing costs by constructing a sparse matrix containing cross-encoder scores, which allows for efficient fitting of an embedding space to approximate cross-encoder scores for k-NN search. This is supported by the statement that the proposed approach requires significantly less compute resources for indexing items compared to previous methods, as noted in the evidence.', 'text': 'The proposed sparse-matrix factorization approach for k-NN search with cross-encoders significantly reduces offline indexing costs compared to existing methods.', 'support_score': 0.95, 'id': 'S6109812601'}</td>
                <td><a href="#E1863991955">E1863991955</a>, <a href="#E8732008330">E8732008330</a>, <a href="#E6911890933">E6911890933</a></td>
            </tr>
            
            <tr id="S0579617533">
                <td>S0579617533</td>
                <td>{'evidence': ['E1863991955', 'E8406065438', 'E2718178979'], 'explanation': 'Cross-encoder models compute similarity by jointly encoding a query-item pair, which has been shown to perform better than dual-encoder models in estimating query-item relevance. This is evidenced by the statement that existing approaches using dual-encoders suffer from poor recall, particularly in new domains, while cross-encoders provide improved performance.', 'text': 'Cross-encoder models outperform dual-encoder models in estimating query-item relevance, leading to better recall in k-NN search.', 'support_score': 0.92, 'id': 'S0579617533'}</td>
                <td><a href="#E1863991955">E1863991955</a>, <a href="#E8406065438">E8406065438</a>, <a href="#E2718178979">E2718178979</a></td>
            </tr>
            
            <tr id="S3438308611">
                <td>S3438308611</td>
                <td>{'evidence': ['E1863991955', 'E2457977668', 'E5374683773'], 'explanation': 'The proposed k-NN search method improves recall by up to 5% for k=1 and 54% for k=100 over dual-encoder approaches, while achieving significant speedups in indexing. This is supported by the evidence that highlights the improvements in recall and the efficiency of the proposed method compared to existing techniques.', 'text': 'The proposed k-NN search method achieves up to 54% improvement in recall over dual-encoder approaches while significantly speeding up indexing processes.', 'support_score': 0.93, 'id': 'S3438308611'}</td>
                <td><a href="#E1863991955">E1863991955</a>, <a href="#E2457977668">E2457977668</a>, <a href="#E5374683773">E5374683773</a></td>
            </tr>
            
            <tr id="S8401159017">
                <td>S8401159017</td>
                <td>{'evidence': ['E1863991955', 'E9245915993', 'E2718178979'], 'explanation': 'The proposed method allows for retrieval over multiple rounds, alternating between estimating the test query embedding and using the updated embedding for further retrieval, which enhances the efficiency of the k-NN search process. This is directly supported by the evidence that describes the retrieval process and its iterative nature.', 'text': 'The proposed k-NN search method utilizes an iterative retrieval process that alternates between estimating and updating test query embeddings, enhancing search efficiency.', 'support_score': 0.9, 'id': 'S8401159017'}</td>
                <td><a href="#E1863991955">E1863991955</a>, <a href="#E9245915993">E9245915993</a>, <a href="#E2718178979">E2718178979</a></td>
            </tr>
            
            <tr id="S6239365205">
                <td>S6239365205</td>
                <td>{'evidence': ['E2315282298', 'E3386906366', 'E1887943394'], 'explanation': "The k-NN algorithm is widely used in recommender systems across various applications, such as e-commerce, entertainment, and social media, where it leverages the similarity between users or items to generate personalized recommendations. This is supported by the evidence stating that 'Recommender systems are widely used in various applications, such as e-commerce, entertainment, and social media, to provide personalized recommendations to users.'", 'text': 'k-NN search is commonly applied in recommender systems across industries such as e-commerce, entertainment, and social media, utilizing item similarity for personalized recommendations.', 'support_score': 0.95, 'id': 'S6239365205'}</td>
                <td><a href="#E2315282298">E2315282298</a>, <a href="#E3386906366">E3386906366</a>, <a href="#E1887943394">E1887943394</a></td>
            </tr>
            
            <tr id="S1998672025">
                <td>S1998672025</td>
                <td>{'evidence': ['E1887943394', 'E0622445425', 'E8401159017'], 'explanation': "The k-NN algorithm is utilized in various applications, including classification tasks, where it is effective in determining the category or class of a dataset based on the nearest neighbors. The evidence states that 'The k-nearest neighbors (k-NN) algorithm is a simple yet powerful tool used in various machine learning and data mining applications.'", 'text': 'k-NN is effectively used in classification tasks, determining the category or class of datasets based on the nearest neighbors.', 'support_score': 0.92, 'id': 'S1998672025'}</td>
                <td><a href="#E1887943394">E1887943394</a>, <a href="#E0622445425">E0622445425</a>, <a href="#E8401159017">E8401159017</a></td>
            </tr>
            
            <tr id="S2564745513">
                <td>S2564745513</td>
                <td>{'evidence': ['E2282404941', 'E9493578677', 'S0579617533'], 'explanation': "Cross-encoder models, which compute similarity by jointly encoding a query-item pair, outperform dual-encoder models in estimating query-item relevance, making them suitable for applications requiring high accuracy in k-NN search. The evidence states that 'Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance.'", 'text': 'Cross-encoders enhance k-NN search by providing better accuracy in estimating query-item relevance compared to dual-encoder models, making them suitable for high-accuracy applications.', 'support_score': 0.94, 'id': 'S2564745513'}</td>
                <td><a href="#E2282404941">E2282404941</a>, <a href="#E9493578677">E9493578677</a>, <a href="#S0579617533">S0579617533</a></td>
            </tr>
            
            <tr id="S6122934702">
                <td>S6122934702</td>
                <td>{'evidence': ['E8143009735', 'E5907416563', 'S0579617533'], 'explanation': 'The proposed hybrid optimization technique merges a genetic algorithm with a local search strategy based on the interior point method, demonstrating efficiency in solving constrained multi-objective problems, which can be analogous to optimizing k-NN search processes. This aligns with the statement that cross-encoder models outperform dual-encoder models in estimating query-item relevance, suggesting that hybrid techniques could similarly enhance k-NN efficiency.', 'text': 'Hybrid optimization techniques, such as combining genetic algorithms with local search strategies, can improve the efficiency of k-NN search processes.', 'support_score': 0.95, 'id': 'S6122934702'}</td>
                <td><a href="#E8143009735">E8143009735</a>, <a href="#E5907416563">E5907416563</a>, <a href="#S0579617533">S0579617533</a></td>
            </tr>
            
            <tr id="S8613303667">
                <td>S8613303667</td>
                <td>{'evidence': ['E8729522952', 'E5120392964', 'S3438308611'], 'explanation': 'The combination of min-max score normalization and arithmetic mean score combination in hybrid search has been shown to achieve better results in search relevance, which is crucial for k-NN search efficiency. This supports the previous statement that the proposed k-NN search method achieves significant improvements in recall, indicating that hybrid techniques can enhance search performance.', 'text': 'Using hybrid search techniques that combine score normalization methods can significantly enhance the efficiency and recall accuracy of k-NN search.', 'support_score': 0.92, 'id': 'S8613303667'}</td>
                <td><a href="#E8729522952">E8729522952</a>, <a href="#E5120392964">E5120392964</a>, <a href="#S3438308611">S3438308611</a></td>
            </tr>
            
            <tr id="S2612945597">
                <td>S2612945597</td>
                <td>{'evidence': ['E1894724338', 'E8770159117', 'S2612945597'], 'explanation': 'The evidence discusses three k-NN optimization techniques for distributed, in-memory indexing methods, which can improve the efficiency of k-NN search operations. This supports the statement regarding the implementation of distributed, in-memory k-NN optimization techniques.', 'text': 'Implementing distributed, in-memory k-NN optimization techniques can significantly improve the efficiency of k-NN search operations.', 'support_score': 0.93, 'id': 'S2612945597'}</td>
                <td><a href="#E1894724338">E1894724338</a>, <a href="#E8770159117">E8770159117</a>, <a href="#S2612945597">S2612945597</a></td>
            </tr>
            
            <tr id="S5658875878">
                <td>S5658875878</td>
                <td>{'evidence': ['E7769670926', 'E5657219903', 'S5658875878'], 'explanation': 'The evidence highlights the importance of optimizing hyperparameters and employing clustering strategies to enhance k-NN search efficiency, which directly supports the statement regarding the reduction of computational costs.', 'text': 'Optimizing hyperparameters and employing clustering strategies can enhance the efficiency of k-NN search, reducing computational costs.', 'support_score': 0.9, 'id': 'S5658875878'}</td>
                <td><a href="#E7769670926">E7769670926</a>, <a href="#E5657219903">E5657219903</a>, <a href="#S5658875878">S5658875878</a></td>
            </tr>
            
            <tr id="S2751262844">
                <td>S2751262844</td>
                <td>{'evidence': ['E9515268709', 'E4070961332', 'S0579617533'], 'explanation': 'The proposed k-NN search method achieves up to 54% improvement in recall for k=100 over dual-encoder approaches, as stated in the evidence from the Weaviate paper, which also highlights that cross-encoders outperform dual-encoders in estimating query-item relevance. This supports the claim that cross-encoders can enhance k-NN search efficiency and accuracy.', 'text': 'The proposed k-NN search method utilizing cross-encoders can achieve significant improvements in recall accuracy, with up to 54% enhancement over dual-encoder approaches for k=100.', 'support_score': 0.95, 'id': 'S2751262844'}</td>
                <td><a href="#E9515268709">E9515268709</a>, <a href="#E4070961332">E4070961332</a>, <a href="#S0579617533">S0579617533</a></td>
            </tr>
            
            <tr id="S7443473358">
                <td>S7443473358</td>
                <td>{'evidence': ['E1732591041', 'S8401159017'], 'explanation': 'The iterative retrieval process described in the evidence involves alternating between estimating and updating query embeddings, which enhances search efficiency, directly supporting the claim about improving k-NN search efficiency.', 'text': 'An iterative retrieval process that alternates between estimating and updating query embeddings enhances the efficiency of k-NN search using cross-encoders.', 'support_score': 0.92, 'id': 'S7443473358'}</td>
                <td><a href="#E1732591041">E1732591041</a>, <a href="#S8401159017">S8401159017</a></td>
            </tr>
            
            <tr id="S3064987850">
                <td>S3064987850</td>
                <td>{'evidence': ['E8904002854', 'E2221245109', 'S3438308611'], 'explanation': 'The evidence indicates that cross-encoders, while more accurate, are computationally intensive when used for every query-item pair. However, the proposed methods aim to align item embeddings with cross-encoder scores, achieving significant speedups in indexing while maintaining high recall accuracy, thus addressing the computational cost issue.', 'text': 'Cross-encoders provide high accuracy in k-NN search, but their computational intensity can be mitigated by aligning item embeddings with cross-encoder scores, achieving significant speedups in indexing.', 'support_score': 0.93, 'id': 'S3064987850'}</td>
                <td><a href="#E8904002854">E8904002854</a>, <a href="#E2221245109">E2221245109</a>, <a href="#S3438308611">S3438308611</a></td>
            </tr>
            
            <tr id="S6148643180">
                <td>S6148643180</td>
                <td>{'evidence': ['E5248050248', 'S6122934702'], 'explanation': 'The hybridization of Self-adaptive Particle Swarm Optimization (IDPSO) with the k-means algorithm, as proposed by Pacifico and Ludermir (2021), illustrates how combining different methodologies can enhance k-NN search efficiency. This aligns with the statement that hybrid optimization techniques can improve the efficiency of k-NN search processes.', 'text': 'Hybrid optimization techniques, such as combining Self-adaptive Particle Swarm Optimization with k-means, can enhance the efficiency of k-NN search processes.', 'support_score': 0.95, 'id': 'S6148643180'}</td>
                <td><a href="#E5248050248">E5248050248</a>, <a href="#S6122934702">S6122934702</a></td>
            </tr>
            
            <tr id="S7750795638">
                <td>S7750795638</td>
                <td>{'evidence': ['E3691484213', 'S8613303667'], 'explanation': 'The context mentions that hybrid search techniques combining keyword and neural search can significantly enhance search relevance and efficiency. This supports the statement that using hybrid search techniques can improve the efficiency and recall accuracy of k-NN search.', 'text': 'Using hybrid search techniques that combine keyword and neural search can significantly enhance the efficiency and recall accuracy of k-NN search.', 'support_score': 0.92, 'id': 'S7750795638'}</td>
                <td><a href="#E3691484213">E3691484213</a>, <a href="#S8613303667">S8613303667</a></td>
            </tr>
            
            <tr id="S6382555768">
                <td>S6382555768</td>
                <td>{'evidence': ['E9326371288', 'S3438308611'], 'explanation': 'The proposed k-NN search method achieves up to 54% improvement in recall for k=100 over dual-encoder approaches, as stated in the evidence, which supports the claim of significant recall enhancement in k-NN search using cross-encoders.', 'text': 'The proposed k-NN search method utilizing cross-encoders can achieve up to 54% improvement in recall accuracy over dual-encoder approaches.', 'support_score': 0.95, 'id': 'S6382555768'}</td>
                <td><a href="#E9326371288">E9326371288</a>, <a href="#S3438308611">S3438308611</a></td>
            </tr>
            
            <tr id="S2202772347">
                <td>S2202772347</td>
                <td>{'evidence': ['E3764887207', 'S0579617533'], 'explanation': 'The evidence indicates that cross-encoders outperform dual-encoders in estimating query-item relevance, which supports the claim that cross-encoders provide high accuracy in k-NN search.', 'text': 'Cross-encoders provide high accuracy in k-NN search, significantly improving recall compared to dual-encoder models.', 'support_score': 0.9, 'id': 'S2202772347'}</td>
                <td><a href="#E3764887207">E3764887207</a>, <a href="#S0579617533">S0579617533</a></td>
            </tr>
            
            <tr id="S3461616960">
                <td>S3461616960</td>
                <td>{'evidence': ['E1732591041', 'S6109812601'], 'explanation': 'The proposed sparse-matrix factorization method is mentioned as a way to efficiently compute latent query and item embeddings, which supports the claim of reducing computational costs in k-NN search.', 'text': 'The sparse-matrix factorization method proposed for k-NN search with cross-encoders significantly reduces computational costs compared to existing methods.', 'support_score': 0.93, 'id': 'S3461616960'}</td>
                <td><a href="#E1732591041">E1732591041</a>, <a href="#S6109812601">S6109812601</a></td>
            </tr>
            
            <tr id="S4446720078">
                <td>S4446720078</td>
                <td>{'evidence': ['E0980439480', 'E9728343128', 'S6122934702'], 'explanation': 'The evidence outlines an effective search algorithm for k-d trees that combines an optimal depth-first branch and bound (DFBB) strategy with unique path ordering and pruning methods, which can enhance k-NN search efficiency. This aligns with the statement that hybrid optimization techniques can improve k-NN search processes.', 'text': 'Hybrid optimization techniques, such as combining depth-first branch and bound strategies with path ordering and pruning, can significantly enhance the efficiency of k-NN search.', 'support_score': 0.95, 'id': 'S4446720078'}</td>
                <td><a href="#E0980439480">E0980439480</a>, <a href="#E9728343128">E9728343128</a>, <a href="#S6122934702">S6122934702</a></td>
            </tr>
            
            <tr id="S6589560863">
                <td>S6589560863</td>
                <td>{'evidence': ['E9325708777', 'E6575392910', 'S8613303667'], 'explanation': 'The evidence indicates that hybrid queries combining neural search and text search improve search relevance and efficiency. This supports the statement that hybrid search techniques can enhance k-NN search efficiency and recall accuracy.', 'text': 'Using hybrid search techniques that combine neural and text search can significantly enhance the efficiency and recall accuracy of k-NN search.', 'support_score': 0.92, 'id': 'S6589560863'}</td>
                <td><a href="#E9325708777">E9325708777</a>, <a href="#E6575392910">E6575392910</a>, <a href="#S8613303667">S8613303667</a></td>
            </tr>
            
            <tr id="S6143240688">
                <td>S6143240688</td>
                <td>{'evidence': ['E3788087638', 'S6122934702'], 'explanation': 'The proposed techniques for k-NN optimization include a density-based optimization technique that utilizes data distribution, which aligns with the statement that hybrid optimization techniques can improve k-NN search efficiency. This is supported by the evidence stating that these techniques are designed to speed up content-based image retrieval, which is a real-time application of k-NN search.', 'text': 'Hybrid optimization techniques, such as density-based optimization, can significantly enhance the efficiency of k-NN search processes in real-time applications.', 'support_score': 0.95, 'id': 'S6143240688'}</td>
                <td><a href="#E3788087638">E3788087638</a>, <a href="#S6122934702">S6122934702</a></td>
            </tr>
            
            <tr id="S9079017145">
                <td>S9079017145</td>
                <td>{'evidence': ['E9006351068', 'S6122934702'], 'explanation': 'The evidence discusses the robustness of evolutionary algorithms as global optimization techniques for solving large-scale problems, which supports the statement that hybrid optimization techniques can improve k-NN search efficiency. This aligns with the previous statement that hybrid optimization can enhance k-NN search processes.', 'text': 'Evolutionary algorithms, as part of hybrid optimization techniques, can improve the efficiency of k-NN search processes, particularly in large-scale datasets.', 'support_score': 0.92, 'id': 'S9079017145'}</td>
                <td><a href="#E9006351068">E9006351068</a>, <a href="#S6122934702">S6122934702</a></td>
            </tr>
            
            <tr id="S3723964869">
                <td>S3723964869</td>
                <td>{'evidence': ['E9006351068', 'S8613303667'], 'explanation': 'The evidence highlights the effectiveness of combining evolutionary algorithms with local search strategies, which supports the statement that hybrid search techniques can enhance the efficiency and recall accuracy of k-NN search. This is consistent with the previous statement regarding the benefits of hybrid techniques in optimizing search processes.', 'text': 'Combining evolutionary algorithms with local search strategies can significantly enhance the efficiency and recall accuracy of k-NN search in real-time scenarios.', 'support_score': 0.93, 'id': 'S3723964869'}</td>
                <td><a href="#E9006351068">E9006351068</a>, <a href="#S8613303667">S8613303667</a></td>
            </tr>
            
            <tr id="S0586523034">
                <td>S0586523034</td>
                <td>{'evidence': ['E3788087638', 'S2612945597'], 'explanation': 'The evidence presents distributed, in-memory k-NN optimization techniques that can improve search operations, supporting the statement that implementing such techniques can enhance the efficiency of k-NN search processes. This is also reflected in the previous statement about the benefits of distributed methods.', 'text': 'Implementing distributed, in-memory k-NN optimization techniques can significantly improve the efficiency of k-NN search operations in real-time applications.', 'support_score': 0.94, 'id': 'S0586523034'}</td>
                <td><a href="#E3788087638">E3788087638</a>, <a href="#S2612945597">S2612945597</a></td>
            </tr>
            
            <tr id="S4363472077">
                <td>S4363472077</td>
                <td>{'evidence': ['E9847712756', 'E7993569666', 'S2612945597'], 'explanation': 'The R-tree data structure is specifically designed to support nearest neighbor search in dynamic contexts, as it has efficient algorithms for insertions and deletions. This is supported by the evidence stating that R-trees can yield nearest neighbors efficiently, which is crucial for maintaining performance in dynamic datasets. Additionally, previous statements highlight that implementing distributed, in-memory k-NN optimization techniques can significantly improve efficiency, which aligns with the capabilities of R-trees in dynamic scenarios.', 'text': 'R-trees are effective for k-NN search in dynamic datasets due to their efficient algorithms for insertions and deletions, enhancing overall search efficiency.', 'support_score': 0.95, 'id': 'S4363472077'}</td>
                <td><a href="#E9847712756">E9847712756</a>, <a href="#E7993569666">E7993569666</a>, <a href="#S2612945597">S2612945597</a></td>
            </tr>
            
            <tr id="S8802393801">
                <td>S8802393801</td>
                <td>{'evidence': ['E4859495217', 'E1398576483', 'S2612945597'], 'explanation': "The performance of the k-NN algorithm deteriorates in high-dimensional data due to increased computational complexity, which is a significant concern when dealing with dynamic datasets. The evidence states that in high-dimensional spaces, the k-NN algorithm's accuracy and efficiency are hindered, which is relevant to understanding the impact of dynamic datasets on k-NN search methods. Previous statements emphasize the importance of hybrid optimization techniques to improve efficiency, which is critical in dynamic contexts.", 'text': 'Dynamic datasets can negatively impact the efficiency of k-NN search methods, particularly in high-dimensional spaces where computational complexity increases.', 'support_score': 0.92, 'id': 'S8802393801'}</td>
                <td><a href="#E4859495217">E4859495217</a>, <a href="#E1398576483">E1398576483</a>, <a href="#S2612945597">S2612945597</a></td>
            </tr>
            
            <tr id="S2988313683">
                <td>S2988313683</td>
                <td>{'evidence': ['E7993569666', 'E3276915217', 'S2612945597'], 'explanation': 'The average complexity of k-NN search methods can be significantly affected by the dimensionality of the data, as indicated by the evidence stating that the average complexity is O(log N) for randomly distributed points. This complexity can worsen in dynamic datasets, where frequent updates may lead to increased computational demands. Previous statements also highlight the need for hybrid optimization techniques to enhance efficiency, which is particularly relevant in dynamic contexts.', 'text': 'The efficiency of k-NN search methods in dynamic datasets is influenced by the average complexity of the algorithms, which can worsen with increased dimensionality and frequent updates.', 'support_score': 0.9, 'id': 'S2988313683'}</td>
                <td><a href="#E7993569666">E7993569666</a>, <a href="#E3276915217">E3276915217</a>, <a href="#S2612945597">S2612945597</a></td>
            </tr>
            
            <tr id="E1863991955">
                <td>E1863991955</td>
                <td>At test time, we compute the test ... cross-encoder scores. We perform extensive empirical analysis on two zero-shot retrieval benchmarks and show that our proposed approach provides significant improvement in test-time ... italic_k-NN search recall-vs-cost tradeoffs while still requiring significantly less compute resources for indexing ... At test time, we compute the test query embedding to approximate cross-encoder scores of the given test query for a small set of adaptively-chosen items, and perform retrieval with the approximate cross-encoder scores. We perform extensive empirical analysis on two zero-shot retrieval benchmarks and show that our proposed approach provides significant improvement in test-time ... italic_k-NN search recall-vs-cost tradeoffs while still requiring significantly less compute resources for indexing items from a target domain as compared to previous approaches. For instance, for a domain with 500 anchor/train queries and 10K items, it takes around 10 hours111On an Nvidia 2080ti GPU with 12 GB memory using batch size=50 to compute the dense query-item score matrix with a CE parameterized using bert-base (Yadav et al., 2022). By simple extrapolation, indexing 5 million items using 500 queries would take around 5000 GPU hours. In this paper, we propose a sparse-matrix factorization-based approach to improve the efficiency of fitting an embedding space to approximate the cross-encoder for ... italic_k-NN search. Our proposed approach significantly reduces the offline indexing cost as compared to existing approaches by constructing a sparse matrix containing cross-encoder scores between a set of training queries Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than using dot-product with embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform ... italic_k-NN search with cross-encoders by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall as DE generalizes poorly to new domains and the test-time retrieval with DE is decoupled from the CE. Another line of work explores model quantization (Nayak et al., 2019; Liu et al., 2021) and early-exit strategies (Xin et al., 2020a; b) to approximate the neural model while speeding up each forward pass through the model and reducing its memory footprint. It would be interesting to study if such data structures and approaches to speed up cross-encoder score computation can be combined with matrix factorization based approaches proposed in this work to further improve recall-vs-cost trade-offs for ... Similar to PRF-based methods in information retrieval (Rocchio Jr, 1971; Lavrenko & Croft, 2001), our proposed ... italic_k-NN search method Axn refines the test query representation using model-based feedback.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8732008330">
                <td>E8732008330</td>
                <td>Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than using dot-product with embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform ... italic_k-NN search with cross-encoders by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall as DE generalizes poorly to new domains and the test-time retrieval with DE is decoupled from the CE.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E6911890933">
                <td>E6911890933</td>
                <td>Things happening in deep learning: arxiv, twitter, reddit Cross-encoder (CE) models which compute similarity by jointly encoding aquery-item pair perform better than embedding-based models (dual-encoders) atestimating query-item relevance. Existing approaches perform k-NN search withCE by approximating the CE similarity with a vector embedding space fit eitherwith dual-encoders (DE) or CUR matrix factorization. DE-basedretrieve-and-rerank approaches suffer from poor recall on new domains and theretrieval with DE is decoupled from the CE. While CUR-based approaches can bemore accurate than the DE-based approach, they require a prohibitively largenumber of CE calls to compute item embeddings, thus making it impractical fordeployment at scale. In this paper, we address these shortcomings with ourproposed sparse-matrix factorization based method that efficiently computeslatent query and item embeddings to approximate CE scores and performs k-NNsearch with the approximate CE similarity. Our method produces a high-quality approximation while requiringonly a fraction of CE calls as compared to CUR-based methods, and allows forleveraging DE to initialize the embedding space while avoiding compute- andresource-intensive finetuning of DE via distillation. At test time, the itemembeddings remain fixed and retrieval occurs over rounds, alternating betweena) estimating the test query embedding by minimizing error in approximating CEscores of items retrieved thus far, and b) using the updated test queryembedding for retrieving more items. Our k-NN search method improves recall byup to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, ourindexing approach achieves a speedup of up to 100x over CUR-based and 5x overDE distillation methods, while matching or improving k-NN search recall overbaselines.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8406065438">
                <td>E8406065438</td>
                <td>Cross-encoder (CE) models which compute similarity by jointly encoding aquery-item pair perform better than embedding-based models (dual-encoders) atestimating query-item relevance. Existing approaches perform k-NN search withCE by approximating the CE similarity with a vector embedding space fit eitherwith dual-encoders (DE) or CUR matrix factorization. DE-basedretrieve-and-rerank approaches suffer from poor recall on new domains and theretrieval with DE is decoupled from the CE.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E2718178979">
                <td>E2718178979</td>
                <td>Additionally, ourindexing approach achieves a speedup of up to 100x over CUR-based and 5x overDE distillation methods, while matching or improving k-NN search recall overbaselines.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E2457977668">
                <td>E2457977668</td>
                <td>Another line of work explores model quantization (Nayak et al., 2019; Liu et al., 2021) and early-exit strategies (Xin et al., 2020a; b) to approximate the neural model while speeding up each forward pass through the model and reducing its memory footprint. It would be interesting to study if such data structures and approaches to speed up cross-encoder score computation can be combined with matrix factorization based approaches proposed in this work to further improve recall-vs-cost trade-offs for ... Similar to PRF-based methods in information retrieval (Rocchio Jr, 1971; Lavrenko & Croft, 2001), our proposed ... italic_k-NN search method Axn refines the test query representation using model-based feedback.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E5374683773">
                <td>E5374683773</td>
                <td>Our method produces a high-quality approximation while requiringonly a fraction of CE calls as compared to CUR-based methods, and allows forleveraging DE to initialize the embedding space while avoiding compute- andresource-intensive finetuning of DE via distillation. At test time, the itemembeddings remain fixed and retrieval occurs over rounds, alternating betweena) estimating the test query embedding by minimizing error in approximating CEscores of items retrieved thus far, and b) using the updated test queryembedding for retrieving more items. Our k-NN search method improves recall byup to 5% (k=1) and 54% (k=100) over DE-based approaches.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9245915993">
                <td>E9245915993</td>
                <td>For instance, for a domain with 500 anchor/train queries and 10K items, it takes around 10 hours111On an Nvidia 2080ti GPU with 12 GB memory using batch size=50 to compute the dense query-item score matrix with a CE parameterized using bert-base (Yadav et al., 2022). By simple extrapolation, indexing 5 million items using 500 queries would take around 5000 GPU hours. In this paper, we propose a sparse-matrix factorization-based approach to improve the efficiency of fitting an embedding space to approximate the cross-encoder for ... italic_k-NN search. Our proposed approach significantly reduces the offline indexing cost as compared to existing approaches by constructing a sparse matrix containing cross-encoder scores between a set of training queries</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E2315282298">
                <td>E2315282298</td>
                <td>K-Nearest Neighbours is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain Recommender systems are widely used in various applications, such as e-commerce, entertainment, and social media, to provide personalized recommendations to users. One common approach to building recommender systems is using the K-Nearest Neighbors (KNN) algorithm. This method leverages the similarity between users or items to generate recommendati ... Nearest Neighbors (NN) search is a fundamental task in many fields, including machine learning, data mining, and computer vision. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better. It is recommended to choose an odd value for k to avoid ties in classification. Cross-validation methods can help in selecting the best k value for the given dataset. The k-nearest neighbors (k-NN) algorithm is a simple yet powerful tool used in various machine learning and data mining applications. While k-NN is often applied to an entire dataset to classify or predict values for multiple points, there are scenarios where you may need to find the k-nearest neighbors for a single point. The knn function in R is a powerful tool for implementing the k-Nearest Neighbors (k-NN) algorithm, a simple and intuitive method for classification and regression tasks. The function is part of the class package, which provides functions for classification. Among its various parameters, the cl parameter plays a crucial role. This article will expl ... The K-Nearest Neighbors (KNN) algorithm is a simple and effective way to perform classification tasks. While KNN is straightforward in its application, visualizing its decision boundaries can significantly enhance understanding of its model behavior, especially in relation to how it classifies new points.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E3386906366">
                <td>E3386906366</td>
                <td>Recommender systems are widely used in various applications, such as e-commerce, entertainment, and social media, to provide personalized recommendations to users. One common approach to building recommender systems is using the K-Nearest Neighbors (KNN) algorithm. This method leverages the similarity between users or items to generate recommendati ... Nearest Neighbors (NN) search is a fundamental task in many fields, including machine learning, data mining, and computer vision.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1887943394">
                <td>E1887943394</td>
                <td>Learn more about one of the most popular and simplest classification and regression classifiers used in machine learning, the k-nearest neighbors algorithm. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset. ... To delve deeper, you can learn more about the k-NN algorithm by using Python and scikit-learn (also known as sklearn). The k-NN algorithm has been utilized within a variety of applications, largely within classification. Just like any machine learning algorithm, k-NN has its strengths and weaknesses. Depending on the project and application, it may or may not be the right choice. Euclidean distance (p=2): This is the most commonly used distance measure, and it is limited to real-valued vectors.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E0622445425">
                <td>E0622445425</td>
                <td>The k-NN algorithm has been utilized within a variety of applications, largely within classification.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8401159017">
                <td>E8401159017</td>
                <td>Snippet text not found for ID: E8401159017</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E2282404941">
                <td>E2282404941</td>
                <td>Things happening in deep learning: arxiv, twitter, reddit Cross-encoder (CE) models which compute similarity by jointly encoding aquery-item pair perform better than embedding-based models (dual-encoders) atestimating query-item relevance. Existing approaches perform k-NN search withCE by approximating the CE similarity with a vector embedding space fit eitherwith dual-encoders (DE) or CUR matrix factorization. At test time, the itemembeddings remain fixed and retrieval occurs over rounds, alternating betweena) estimating the test query embedding by minimizing error in approximating CEscores of items retrieved thus far, and b) using the updated test queryembedding for retrieving more items. Our k-NN search method improves recall byup to 5% (k=1) and 54% (k=100) over DE-based approaches. While CUR-based approaches can bemore accurate than the DE-based approach, they require a prohibitively largenumber of CE calls to compute item embeddings, thus making it impractical fordeployment at scale. In this paper, we address these shortcomings with ourproposed sparse-matrix factorization based method that efficiently computeslatent query and item embeddings to approximate CE scores and performs k-NNsearch with the approximate CE similarity. Additionally, ourindexing approach achieves a speedup of up to 100x over CUR-based and 5x overDE distillation methods, while matching or improving k-NN search recall overbaselines.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9493578677">
                <td>E9493578677</td>
                <td>ACL materials are Copyright © 1963–2024 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for ...</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8143009735">
                <td>E8143009735</td>
                <td>Evolutionary algorithms are robust and powerful global optimization techniques for solving large-scale problems that have many local optima. However, … This paper proposes a new hybrid optimization technique that merges a genetic algorithm with a local search strategy based on the interior point method. The efficiency of this hybrid approach is demonstrated by solving a constrained multi-objective mathematical test-case.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E5907416563">
                <td>E5907416563</td>
                <td>This paper proposes a new hybrid optimization technique that merges a genetic algorithm with a local search strategy based on the interior point method. The efficiency of this hybrid approach is demonstrated by solving a constrained multi-objective mathematical test-case.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8729522952">
                <td>E8729522952</td>
                <td>Improve search relevance with OpenSearch 2.10 when you tune search relevance by using hybrid search to combine and normalize query relevance scores. The combination of min-max score normalization and arithmetic_mean score combination achieves the best results, compared to other techniques. In most cases, increasing the value of k in the k-NN data type leads to better results up to a certain point, but after that, there is no increase in relevance. For the neural query, we generated text embeddings using neural search data ingestion. We used pretrained and fine-tuned transformers to generate embeddings and run search queries. For the HNSW algorithm in k-NN search, we used k = 100. Vamshi Vijay Nakkirtha is a software engineering manager working on the OpenSearch Project and Amazon OpenSearch Service. His primary interests include distributed systems. He is an active contributor to various plugins, like k-NN, GeoSpatial, and dashboard-maps. Each of these techniques has its advantages and disadvantages, so it is natural to try to combine them so that they complement each other. The naive approach to combination—an arithmetic combination of the scores returned by each system—doesn’t work: Different query types provide scores on different scales. For instance, a full-text match query score can be any positive number, while a knn or neural-search query score is typically between 0.0 and 1.0.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E5120392964">
                <td>E5120392964</td>
                <td>The combination of min-max score normalization and arithmetic_mean score combination achieves the best results, compared to other techniques. In most cases, increasing the value of k in the k-NN data type leads to better results up to a certain point, but after that, there is no increase in relevance.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E7566171373">
                <td>E7566171373</td>
                <td>In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query ... In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query optimization: a density-based optimization technique that performs k-NN optimization using data distribution; a cost-based optimization technique using query processing cost statistics; and a learning-based optimization technique using a deep learning model, based on query logs. In this paper, we used iDistance, which is a distance-based indexing method for efficient k-NN query processing. The authors of [18] used a deep learning technique based on a combination of CNNs to classify images, and used RNNs to analyze natural language queries. They also used CNNs to take advantage of the deep learning technology in image content classification. The RNN model helps users to make search queries more efficiently. The authors of [19] identified occupied and vacant parking lots using a hybrid deep learning model. This study proposes optimization techniques to determine the initial search range. Figure 1 shows the structure of k-NN query processing in the distributed, in-memory, high-dimensional index structure. As in our previous research, when the data were input, a distributed in-memory hybrid index, using k-d trees and iDistance, was built using Spark [17]. When processing a k-NN query, it was transformed into an optimized k-NN query by using the query location and k. In particular, we compared the performances of the existing methods, the hybrid indexing method proposed by our research team, and a method that applies the three optimization techniques used for hybrid indexing. Figure 14 shows the k-NN query processing time with the indexing method. The sequential search had the worst performance because it requires extensive time to compare the distance for all the data and because it manages the k-nearest list using additional comparison operations.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8770159117">
                <td>E8770159117</td>
                <td>In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query ...</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1643422460">
                <td>E1643422460</td>
                <td>For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, "random projections", "sketches" or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option. Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E2771503431">
                <td>E2771503431</td>
                <td>The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular approach is the use of evolutionary algorithms to optimize feature scaling.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9515268709">
                <td>E9515268709</td>
                <td>The quality of a reranked retreiver and the speed of a bi-encoder retreiver! Proposed k-NN search method can achieve up to 5% and 54% improvement in k-NN recall for k = 1 and 100 respectively over the widely-used DE-based retrieve-and-rerank approach. ... Check out the Quickstart tutorial, and begin building amazing apps with the free trial of Weaviate Cloud (WCD). ... Sign up for our bi-weekly newsletter to stay updated! By submitting, I agree to the Terms of Service and Privacy Policy. How do you get the retrieval quality of a cross-encoder/re-ranker and the efficiency of a bi-encoder? You can think of this as an efficient way to train an adaptor for the query vector that transforms the query vector in such a way that makes the similarity scores b/w query-documents more like the cross-encoder similarity scores. This new paper from DeepMind proposes Adaptive Cross-Encoder Nearest Neighbor Search, an alternative which approximates the re-ranker query-document similarities while still using a bi-encoder setup. Can use existing bi-encoder models to initialize the item and query embeddings · In an offline indexing step -> compute query/item embeddings to index a given set of items from a target domain making sure the similarity scores are similar to cross encoder scores</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E4070961332">
                <td>E4070961332</td>
                <td>However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take "forever" to perform the search.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E5742287286">
                <td>E5742287286</td>
                <td>Bi-Encoder models are not the only possible way of scoring pairs of data on similarity. A different strategy is using Cross-Encoders. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a data pair, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3).</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E8904002854">
                <td>E8904002854</td>
                <td>Semantic search overcomes the shortcomings of keyword-based search.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E2221245109">
                <td>E2221245109</td>
                <td>Learn about bi-encoder and cross-encoder machine learning models, and why combining them could improve the vector search experience. You hope to find some salmon, but your net also catches other kinds of fish that you are not interested in. Next, you sort the catch by keeping the salmon and throwing the other fish back into the sea. This sorting step is time-consuming and expensive, but very effective. ... Catching fish with the big net represents how Bi-Encoders work. Bi-Encoder models are not the only possible way of scoring pairs of data on similarity. A different strategy is using Cross-Encoders. Cross-Encoder models do not produce vector embeddings for data, but use a classification mechanism for data pairs instead. The input of the model always consists of a data pair, for example two sentences, and outputs a value between 0 and 1 indicating the similarity between these two sentences (Figure 3). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take "forever" to perform the search. Semantic search overcomes the shortcomings of keyword-based search.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E5248050248">
                <td>E5248050248</td>
                <td>Pacifico and Ludermir (2021) proposed a hybridization between Self-adaptive Particle Swarm Optimization (IDPSO) and the k-means algorithm, in which the IDPSO is used in the exploration phase, and the k-means is adopted in the exploitation phase of the algorithm. The self-adaptive scheme is employed so that the parameters for each individual of PSO may reflect the current state of the search promoted by the entire population. The approach also uses a crossover operator to improve the diversification of the PSO population, avoiding premature convergence. El-Shorbagy et al. (2019) proposed an enhanced Genetic Algorithm with a new mutation operator based on the k-means algorithm for cluster analysis.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E3691484213">
                <td>E3691484213</td>
                <td>This guide covers the different types of hybrid search queries supported by Elasticsearch, its limitations, optimizations, and more. Then, using some illustrative examples, we showed how Elasticsearch provides hybrid search support for sparse and dense vector spaces alike, using both Convex Combination and Reciprocal Rank Fusion as scoring and ranking methods. We also briefly introduced the Elastic Learned Sparse EncodeR model (ELSER), which is their first attempt at providing an out-of-domain sparse model built on a 30,000 tokens vocabulary. We will certainly publish new articles in the future to keep you informed as this model gets enhanced and as Elastic improves hybrid search support as announced during the ElasticON AI conference. Armed with all the vector search knowledge learned in the first article, the second article: How to Set Up Vector Search in OpenSearch, guided you through the meanders of how to set up vector search in OpenSearch using either the k-NN plugin or the new Neural Search plugin that was recently made generally available in 2.9. As mentioned in the previous article, Elasticsearch came late into the vector search game and only started supporting approximate nearest neighbors (ANN) search in February 2022 with the 8.0 release. In contrast, OpenDistro for Elasticsearch already released that feature in September 2019 via their k-NN plugin for Elasticsearch 7.2, shortly after Apache Lucene started to support dense vectors. The fourth part: How to Set Up Vector Search in Elasticsearch, was similar to the second one but focused on how to set up vector search and execute k-NN searches in Elasticsearch specifically.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1532856717">
                <td>E1532856717</td>
                <td>In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query ... In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query optimization: a density-based optimization technique that performs k-NN optimization using data distribution; a cost-based optimization technique using query processing cost statistics; and a learning-based optimization technique using a deep learning model, based on query logs. In this paper, we used iDistance, which is a distance-based indexing method for efficient k-NN query processing. The authors of [18] used a deep learning technique based on a combination of CNNs to classify images, and used RNNs to analyze natural language queries. They also used CNNs to take advantage of the deep learning technology in image content classification. The RNN model helps users to make search queries more efficiently. The authors of [19] identified occupied and vacant parking lots using a hybrid deep learning model. To evaluate the performance of k-NN queries, we set k to 100 and conducted the same 100 queries that we used in the range query evaluation. In particular, we compared the performances of the existing methods, the hybrid indexing method proposed by our research team, and a method that applies the three optimization techniques used for hybrid indexing. The authors of [26] proposed a fast CBIR system using Spark (CBIR-S), which targets large-scale images. It uses a memory-centric distributed storage system called Tachyon to enhance the write operation. It can speed up by using a parallel k-NN search method, which is based on the MapReduce model on Spark.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1805217149">
                <td>E1805217149</td>
                <td>The fourth part: How to Set Up Vector Search in Elasticsearch, was similar to the second one but focused on how to set up vector search and execute k-NN searches in Elasticsearch specifically.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9326371288">
                <td>E9326371288</td>
                <td>Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding ... Abstract:Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. View a PDF of the paper titled Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders, by Nishant Yadav and 4 other authors View PDF HTML (experimental) At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1732591041">
                <td>E1732591041</td>
                <td>While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E3764887207">
                <td>E3764887207</td>
                <td>At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E0980439480">
                <td>E0980439480</td>
                <td>PDF | While k-d trees have been widely studied and used, their theoretical advantages are often not realized due to ineffective search strategies and... | Find, read and cite all the research you need on ResearchGate In this paper we outline an effective search algorithm for k-d trees that combines an optimal depth-first branch and bound (DFBB) strategy with a unique method for path ordering and pruning. Our initial method was developed for improving nearest neighbor (NN) search, but has also proven effective for k-NN search and approximate k-NN classification. We outline an effective search algorithm for k-d trees that combines an optimal depth-first branch and bound (DFBB) strategy with a unique method for path ordering and pruning. This technique was developed for ... [Show full abstract] improving nearest neighbor (NN) search, but has also proven effective for k-NN and approximate k-NN queries.View full-text Epsilon revision for NN search is a simple task. Whenever a neighbor is discovered that is · closer to α than the current distance ε, this new distance becomes the new ε. By changing a · global ε, there can be a dynamic contraction of the clipping window as well. NN searching with k-d trees as early as 1977 [2]. More recently, Arya and Mount have presented · refined search tactics which have been especially effective for approximate searches [3]. The · refinements we present are effective for all of the search types outlined and provide the maximum ... Many authors have examined the best ways to construct k-d trees. We do not purport to · improve the construction method, but present our simple method for future comparisons.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9728343128">
                <td>E9728343128</td>
                <td>In this paper we outline an effective search algorithm for k-d trees that combines an optimal depth-first branch and bound (DFBB) strategy with a unique method for path ordering and pruning. Our initial method was developed for improving nearest neighbor (NN) search, but has also proven effective for k-NN search and approximate k-NN classification.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9325708777">
                <td>E9325708777</td>
                <td>Improve search relevance with OpenSearch 2.10 when you tune search relevance by using hybrid search to combine and normalize query relevance scores. Running benchmarks on the same datasets as in our earlier blog post allowed us to use the previous data as a baseline. We built the hybrid query as a combination of two queries: a neural search query and a text search match query. For the neural query, we generated text embeddings using neural search data ingestion. We used pretrained and fine-tuned transformers to generate embeddings and run search queries. For the HNSW algorithm in k-NN search, we used k = 100. We’ve seen that the following conclusions may be applied to most datasets: For semantic search, a hybrid query with normalization produces better results compared to neural search or text search alone. The combination of min-max score normalization and arithmetic_mean score combination achieves the best results, compared to other techniques. In most cases, increasing the value of k in the k-NN data type leads to better results up to a certain point, but after that, there is no increase in relevance. It’s possible to define a filter for each inner query individually, but it’s not optimal for a filter condition to be the same for all inner queries. Adding more benchmark results for larger datasets so we can provide recommendations on using hybrid search in various configurations. The following table provides further details of the test datasets used for benchmarking. The ABCs of semantic search in OpenSearch: Architectures, benchmarks, and combination strategies. Vamshi Vijay Nakkirtha is a software engineering manager working on the OpenSearch Project and Amazon OpenSearch Service. His primary interests include distributed systems. He is an active contributor to various plugins, like k-NN, GeoSpatial, and dashboard-maps.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E6575392910">
                <td>E6575392910</td>
                <td>We’ve seen that the following conclusions may be applied to most datasets: For semantic search, a hybrid query with normalization produces better results compared to neural search or text search alone. The combination of min-max score normalization and arithmetic_mean score combination achieves the best results, compared to other techniques. In most cases, increasing the value of k in the k-NN data type leads to better results up to a certain point, but after that, there is no increase in relevance.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1894724338">
                <td>E1894724338</td>
                <td>In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query ... In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query optimization: a density-based optimization technique that performs k-NN optimization using data distribution; a cost-based optimization technique using query processing cost statistics; and a learning-based optimization technique using a deep learning model, based on query logs. In this paper, we used iDistance, which is a distance-based indexing method for efficient k-NN query processing. The authors of [18] used a deep learning technique based on a combination of CNNs to classify images, and used RNNs to analyze natural language queries. They also used CNNs to take advantage of the deep learning technology in image content classification. The RNN model helps users to make search queries more efficiently. The authors of [19] identified occupied and vacant parking lots using a hybrid deep learning model. A hybrid distributed high-dimensional index was implemented to address the issue of k-d trees and iDistance. Combining the advantages of both indexes, the overall structure consists of a master/slave structure, in which the master is responsible for data distribution and for selecting slaves for query processing, which reduces the system load. The slave nodes process the query and conduct the data indexing. The k-NN algorithm has a wide range of uses because it finds k neighbors for a given value of k. Figure 1 shows the structure of k-NN query processing in the distributed, in-memory, high-dimensional index structure. As in our previous research, when the data were input, a distributed in-memory hybrid index, using k-d trees and iDistance, was built using Spark [17]. When processing a k-NN query, it was transformed into an optimized k-NN query by using the query location and k.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E7769670926">
                <td>E7769670926</td>
                <td>PDF | While k-d trees have been widely studied and used, their theoretical advantages are often not realized due to ineffective search strategies and... | Find, read and cite all the research you need on ResearchGate</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E5657219903">
                <td>E5657219903</td>
                <td>Epsilon revision for NN search is a simple task. Whenever a neighbor is discovered that is · closer to α than the current distance ε, this new distance becomes the new ε. By changing a · global ε, there can be a dynamic contraction of the clipping window as well.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E3788087638">
                <td>E3788087638</td>
                <td>In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query ... In this study, we propose three k-nearest neighbor (k-NN) optimization techniques for a distributed, in-memory-based, high-dimensional indexing method to speed up content-based image retrieval. The proposed techniques perform distributed, in-memory, high-dimensional indexing-based k-NN query optimization: a density-based optimization technique that performs k-NN optimization using data distribution; a cost-based optimization technique using query processing cost statistics; and a learning-based optimization technique using a deep learning model, based on query logs. Therefore, an optimization technique is needed to convert these k-NN queries into range queries. The authors of [24] proposed an optimization technique based on product quantization. They applied the optimization technique in different similarity search scenarios, such as brute-force, approximate, and compressed-domain. To evaluate the performance of k-NN queries, we set k to 100 and conducted the same 100 queries that we used in the range query evaluation. In particular, we compared the performances of the existing methods, the hybrid indexing method proposed by our research team, and a method that applies the three optimization techniques used for hybrid indexing. The existing techniques provide approximate results to speed up search performance [24]. Third, various k-NN query optimization techniques have been proposed. When the proposed techniques apply to the real world system, they have the advantage of being able to find optimal performance by replacing optimization techniques.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9006351068">
                <td>E9006351068</td>
                <td>Evolutionary algorithms are robust and powerful global optimization techniques for solving large-scale problems that have many local optima. However, …</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E9847712756">
                <td>E9847712756</td>
                <td>For constant dimension query time, ... in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree. R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances. In the case of general metric space, the branch-and-bound approach is known as the metric tree approach. Particular examples include vp-tree and BK-tree methods... For constant dimension query time, average complexity is O(log N) in the case of randomly distributed points, worst case complexity is O(kN^(1-1/k)) Alternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree. R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances. In the case of general metric space, the branch-and-bound approach is known as the metric tree approach. Particular examples include vp-tree and BK-tree methods. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the "best so far". This algorithm, sometimes referred to as the naive approach, has a running time of O(dN), where N is the cardinality of S and d is the dimensionality of S. A direct generalization of this problem is a k-NN search, where we need to find the k closest points. There are numerous variants of the NNS problem and the two most well-known are the k-nearest neighbor search and the ε-approximate nearest neighbor search.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E7993569666">
                <td>E7993569666</td>
                <td>For constant dimension query time, average complexity is O(log N) in the case of randomly distributed points, worst case complexity is O(kN^(1-1/k)) Alternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree. R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances. In the case of general metric space, the branch-and-bound approach is known as the metric tree approach. Particular examples include vp-tree and BK-tree methods.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E4859495217">
                <td>E4859495217</td>
                <td>A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions. In high-dimensional data, the performance of the k-nearest neighbor (k-NN) algorithm often deteriorates due to increased computational complexity and the breakdown of the assumption that similar points are proximate. These challenges hinder the algorithm’s accuracy and efficiency in high-dimensional spaces. While k-NN is often applied to an entire dataset to classify or predict values for multiple points, there are scenarios where you may need to find the k-nearest neighbors for a single point. The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning method employed to tackle classification and regression problems. Evelyn Fix and Joseph Hodges developed this algorithm in 1951, which was subsequently expanded by Thomas Cover. The article explores the fundamentals, workings, and implementation of the KNN algorithm. What is t ... The k-nearest neighbors (k-NN) algorithm is a simple yet powerful tool used in various machine learning and data mining applications. Increased Computational Complexity: With higher dimensionality, the computational cost of KNN increases significantly. The algorithm needs to compute distances in a high-dimensional space, which involves more calculations. This can make the KNN algorithm slower and less efficient, especially when dealing with large datasets.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E1398576483">
                <td>E1398576483</td>
                <td>In high-dimensional data, the performance of the k-nearest neighbor (k-NN) algorithm often deteriorates due to increased computational complexity and the breakdown of the assumption that similar points are proximate. These challenges hinder the algorithm’s accuracy and efficiency in high-dimensional spaces.</td>
                <td>External evidence</td>
            </tr>
            
            <tr id="E3276915217">
                <td>E3276915217</td>
                <td>Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office.</td>
                <td>External evidence</td>
            </tr>
            </table>
        </body>
        </html>
        