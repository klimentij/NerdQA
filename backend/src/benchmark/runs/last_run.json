[
  {
    "id": "https://openalex.org/W4392203343",
    "text": "A Comprehensive Survey on Deep Graph Representation\nLearning\nWEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University,\nChina\nZIYUE QIAO, The Hong Kong University of Science and Technology, China\nYIFANG QIN and JIANHAO SHEN, Peking University, China\nFANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA\nJUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China\nYIFAN WANG, University of International Business and Economics, China\nXIAO LUO\u2217, University of California, Los Angeles, USA\nMING ZHANG\u2217, Peking University, China\nGraph representation learning aims to effectively encode high-dimensional sparse graph-structured data into\nlow-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields,\nincluding machine learning and data mining. Classic graph embedding methods follow the basic idea that the\nembedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby\npreserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i)\ntraditional methods have limited model capacity which limits the learning performance; (ii) existing techniques\ntypically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii)\nrepresentation learning and downstream tasks are dependent on each other which should be jointly enhanced.\nWith the remarkable success of deep learning, deep graph representation learning has shown great potential\nand advantages over shallow (traditional) methods, there exist a large number of deep graph representation\nlearning techniques have been proposed in the past decade, especially graph neural networks. In this survey,\nwe conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a\nnew taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential\ncomponents of graph representation learning and categorize existing approaches by the ways of graph neural\nnetwork architectures and the most recent advanced learning paradigms. Moreover, this survey also provides\nthe practical and promising applications of deep graph representation learning. Last but not least, we state\nnew perspectives and suggest challenging directions which deserve further investigations in the future.\nCCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations.\n\u2217Corresponding authors.\nAuthors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn;\nZequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871;\nZiyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453;\nYifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang\nSun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei\nYang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking\nUniversity, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics,\nBeijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang,\nmzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Association for Computing Machinery.\n0004-5411/2024/2-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\narXiv:2304.05055v3 [cs.LG] 28 Feb 2024\n2 W. Ju, et al.\nAdditional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural\nNetwork, Survey\nACM Reference Format:\nWei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun,\nZhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024.\nA Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nGraphs have recently emerged as a powerful tool for representing a variety of structured and\ncomplex data, including social networks, traffic networks, information systems, knowledge graphs,\nprotein-protein interaction networks, and physical interaction networks. As a kind of general form\nof data organization, graph structures are capable of naturally expressing the intrinsic relationship\nof these data, and thus can characterize plenty of non-Euclidean structures that are crucial in\na variety of disciplines and domains due to their flexible adaptability. For example, to encode a\nsocial network as a graph, nodes on the graph are used to represent individual users, and edges are\nused to represent the relationship between two individuals, such as friends. In the field of biology,\nnodes can be used to represent proteins, and edges can be used to represent biological interactions\nbetween various proteins, such as the dynamic interactions between proteins. Thus, by analyzing\nand mining the graph-structured data, we can understand the deep meaning hidden behind the\ndata, and further discover valuable knowledge, so as to benefit society and human beings.\nIn the last decade years, a wide range of machine learning algorithms have been developed for\ngraph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410]\nusually break down graphs into different atomic substructures and then use kernel functions\nto measure the similarity between all pairs of them. Although graph kernels could provide a\nperspective on modeling graph topology, these approaches often generate substructures or feature\nrepresentations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer\nfrom high computational complexity, and therefore have weak scalability and subpar performance.\nIn the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have ever\u0002increasing emerged, which attempt to encode the structural information of the graph (usually a\nhigh-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to\npreserve the topology information and attribute information in the embedding space as much\nas possible, so that the learned graph embeddings can be naturally integrated into traditional\nmachine learning algorithms. Compared to previous works which use feature engineering in the\npre-processing phase to extract graph structural features, current graph embedding algorithms are\nconducted in a data-driven way leveraging machine learning algorithms (such as neural networks)\nto encode the structural information of the graph. Specifically, existing graph embedding methods\ncan be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354]\nthat factorize the matrix to learn node embedding which preserves the graph property; (ii) deep\nlearning based methods [155, 362, 443, 460] that apply deep learning techniques specifically de\u0002signed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either\nmaximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these\nmethods typically depend on shallow architectures, and fail to exploit the potential and capacity of\ndeep neural networks, resulting in sub-optimal representation quality and learning performance.\nInspired by the recent remarkable success of deep neural networks, a range of deep learning\nalgorithms has been developed for graph-structured data learning. The core of these methods is to\ngenerate effective node and graph representations using graph neural networks (GNNs), followed\nby a goal-oriented learning paradigm. In this way, the derived representations can be adaptively\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 3\ncoupled with a variety of downstream tasks and applications. Following this line of thought, in\nthis paper, we propose a new taxonomy to classify the existing graph representation learning\nalgorithms, i.e., graph neural network architectures, learning paradigms, and various promising\napplications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the\nstudies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer.\nFor the learning paradigms, we explore three advanced types namely supervised/semi-supervised\nlearning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate\nthe effectiveness of the learned graph representations, we provide several promising applications\nto build tight connections between representation learning and downstream tasks, such as social\nanalysis, molecular property prediction and generation, recommender systems, and traffic analysis.\nLast but not least, we present some perspectives for thought and suggest challenging directions\nthat deserve further study in the future.\nDifferences between this survey and existing ones. Up to now, there exist some other overview\npapers focusing on different perspectives of graph representation learning[17, 50, 53, 57, 227, 499,\n502, 577, 601, 603] that are closely related to ours. However, there are very few comprehensive\nreviews have summarized deep graph representation learning simultaneously from the perspective\nof diverse GNN architectures and corresponding up-to-date learning paradigms. Therefore, we\nhere clearly state their distinctions from our survey as follows. There have been several surveys\non classic graph embedding[42, 151], these works categorize graph embedding methods based on\ndifferent training objectives. Wang et al. [468] goes further and provides a comprehensive review of\nexisting heterogeneous graph embedding approaches. With the rapid development of deep learning,\nthere are a handful of surveys along this line. For example, Wu et al. [499] and Zhang et al. [577]\nmainly focus on several classical and representative GNN architectures without exploring deep\ngraph representation learning from a view of the most recent advanced learning paradigms such as\ngraph self-supervised learning and graph structure learning. Xia et al. [502] and Chami et al. [50]\njointly summarize the studies of graph embeddings and GNNs. Zhou et al. [601] explores different\ntypes of computational modules for GNNs. One recent survey under review [227] categorizes the\nexisting works in graph representation learning from both static and dynamic graphs. However,\nthese taxonomies emphasize the basic GNN methods but pay insufficient attention to the learning\nparadigms, and provide few discussions of the most promising applications, such as recommender\nsystems as well as molecular property prediction and generation. To the best of our knowledge, the\nmost relevant survey published formally is [603], which presents a review of GNN architectures\nand roughly discusses the corresponding applications. Nevertheless, this survey merely covers\nmethods up to the year of 2020, missing the latest developments in the past three years.\nTherefore, it is highly desired to summarize the representative GNN methods, the most recent\nadvanced learning paradigms, and promising applications into one unified and comprehensive\nframework. Moreover, we strongly believe this survey with a new taxonomy of literature and more\nthan 600 studies will strengthen future research on deep graph representation learning.\nContribution of this survey. The goal of this survey is to systematically review the literature\non the advances of deep graph representation learning and discuss further directions. It aims\nto help the researchers and practitioners who are interested in this area, and support them in\nunderstanding the panorama and the latest developments of deep graph representation learning.\nThe key contributions of this survey are summarized as follows:\n\u2022 Systematic Taxonomy. We propose a systematic taxonomy to organize the existing deep\ngraph representation learning approaches based on the ways of GNN architectures and the\nmost recent advanced learning paradigms via providing some representative branches of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n4 W. Ju, et al.\nGraph Self-Supervised Learning\nSemi-Supervised Learning on Graphs\nGraph Structure Learning\nLearning Paradigms\nGraph-Related Applications\nMolecular Generation\nMolecular Property Prediction\nSocial Analysis\nRecommender Systems\nTra c Analysis\nFuture Directions\nGraph Neural Network Architectures\nGraph Kernel Neural Networks\nGraph Pooling\nGraph Convolutions\nGraph Transformer\nGraph Representations\nGraph Data\nOptimized Graph Representations\nFig. 1. The architecture of this paper.\nmethods. Moreover, several promising applications are presented to illustrate the superiority\nand potential of graph representation learning.\n\u2022 Comprehensive Review. For each branch of this survey, we review the essential components\nand provide detailed descriptions of representative algorithms, and systematically summarize\nthe characteristics to make the overview comparison.\n\u2022 Future Directions. Based on the properties of existing deep graph representation learning\nalgorithms, we discuss the limitations and challenges of current methods and propose the\npotential as well as promising research directions deserving of future investigations.\n2 Background\nIn this section, we first briefly introduce some definitions in deep graph representation learning that\nneed to be clarified, and then we explain the reasons why we need graph representation learning.\n2.1 Problem Definition\nDefinition: Graph. Given a graph \ud835\udc3a = (\ud835\udc49 , \ud835\udc38, X), where \ud835\udc49 = {\ud835\udc631, \u00b7 \u00b7 \u00b7 , \ud835\udc63|\ud835\udc49 | } is the set of nodes,\n\ud835\udc38 = {\ud835\udc521, \u00b7 \u00b7 \u00b7 , \ud835\udc52|\ud835\udc49 | } is the set of edges, and the edge \ud835\udc52 = (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38 represent the connection\nrelationship between nodes \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57in the graph. X \u2208 R\n|\ud835\udc49 |\u00d7\ud835\udc40 is the node feature matrix with\n\ud835\udc40 being the dimension of each node feature. The adjacency matrix of a graph can be defined as\nA \u2208 R\n|\ud835\udc49 |\u00d7 |\ud835\udc49 |\n, where A\ud835\udc56\ud835\udc57 = 1 if (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) \u2208 \ud835\udc38, otherwise A\ud835\udc56\ud835\udc57 = 0.\nThe adjacency matrix can be regarded as the structural representation of the graph-structured\ndata, in which each row of the adjacency matrix A represents the connection relationship between\nthe corresponding node of the row and all other nodes, which can be regarded as a discrete repre\u0002sentation of the node. However, in real-life circumstances, the adjacency matrix A corresponding\nto \ud835\udc3a is a highly sparse matrix, and if A is used directly as node representations, it will be seriously\naffected by impractical storage demands and computational overhead. The storage space of the\nadjacency matrix A is |\ud835\udc49 |\u00d7 |\ud835\udc49 |, which is usually unacceptable when the total number of nodes grows\nto the order of millions. At the same time, the value of most dimensions in the node representation\nis 0. The sparsity will make subsequent machine learning tasks very difficult.\nGraph representation learning is a bridge between the original input data and the task objectives\nin the graph. The fundamental idea of the graph representation learning algorithm is first to learn\nthe embedded representations of nodes or the entire graph from the input graph structure data and\nthen apply these embedded representations to downstream related tasks, such as node classification,\ngraph classification, link prediction, community detection, and visualization, etc. Specifically, it\naims to learn low-dimensional, dense distributed embedding representations for nodes in the graph.\nFormally, the goal of graph representation learning is to learn its embedding vector representation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 5\nTable 1. Summary of traditional graph embedding methods.\nType Method Similarity measure Loss function (\ud835\udc3f)\nMatrix Factorization\nLLE [390] general |\ud835\udc67\ud835\udc56 \u2212\n\u00cd\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56 \ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57\n|\n2\nLE [11] general \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d,s.t.\ud835\udc4d\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c\nGF [4] \ud835\udc34\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nGraRep [46] \ud835\udc34\ud835\udc56,\ud835\udc57, \ud835\udc342\n\ud835\udc56,\ud835\udc57, ..., \ud835\udc34\ud835\udc58\n\ud835\udc56,\ud835\udc57 |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56\n, \ud835\udc67\ud835\udc57\u27e9|2\nHOPE [354] general |\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2\nRandom Walk\nDeepWalk [362] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nNode2vec [155] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nHARP [59] \ud835\udc5d(\ud835\udc63\ud835\udc56|\ud835\udc63\ud835\udc56) (biased) \u2212\ud835\udc34\ud835\udc56\ud835\udc57 log\u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9\nLINE [443] Two-order Similarities Corresponding Loss\nNon-GNN Deep SDNE [460] Two-order Proximities Corresponding Loss\nDNGR [47] Two-order Proximities Corresponding Loss\n\ud835\udc45\ud835\udc63 \u2208 R\n\ud835\udc51\nfor each node \ud835\udc63 \u2208 \ud835\udc49 , where the dimension \ud835\udc51 of the vector is much smaller than the total\nnumber of nodes |\ud835\udc49 | in the graph.\n2.2 Traditional Graph Embedding\nTraditional graph embedding learning methods, as part of dimensionality reduction techniques,\naimed to embed graph data into a lower-dimensional vector space with the idea that connected nodes\nin the graph should still be closer to each other in this lower-dimensional space, thereby preserving\nthe structural information between nodes in the graph. Influenced by classical dimensionality\nreduction techniques, early graph embedding methods are primarily inspired by classic matrix\nfactorization techniques [25] and multi-dimensional scaling [245]. The following three sections\ndescribe these methods in more detail, distinguishing among matrix factorization-based methods,\nrandom walks-based methods and other non-GNN deep methods. In Table 1, we summarize different\ncategories of traditional graph embedding methods.\n2.2.1 Matrix factorization-based methods Matrix factorization-based methods are the early en\u0002deavors in graph embedding learning. These approaches can be outlined in a two-step process.\nIn the initial step, a proximity-based matrix is constructed for the graph, where each element of\nthe matrix represents the proximity measure between two nodes in the graph. Subsequently, a\ndimensionality reduction technique is employed on this matrix in the second step to generate the\nnode embeddings.\nLocally Linear Embedding (LLE) [390]. LLE assumes that node representations are sampled from\nthe same manifold space, and any node in the graph and its neighboring nodes are located in a\nlocal region of that manifold space. Therefore, node representations can be obtained by linearly\ncombining them with their neighboring nodes. LLE first constructs a local reconstruction weight\nmatrix, \ud835\udc4a\ud835\udc56\ud835\udc57 , for nodes in the graph to linearly combine neighboring nodes. By computing the\ndistance between the linear combination and the central node, the problem is reduced to solving\nfor matrix eigenvalues to learn low-dimensional vector representations for nodes. The objective\nfunction is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56 \u2212\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56\n\ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc67\ud835\udc57|\n2\n, (1)\nwhere \ud835\udc67\ud835\udc56 represents the low-dimensional representation of the \ud835\udc56-th node, and \ud835\udc41\ud835\udc56is the set of\nneighboring nodes for the central node \ud835\udc56.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n6 W. Ju, et al.\nLaplacian Eigenmaps (LE) [11]. LE believes that nodes directly connected in graph data should\nbe kept as close as possible in the embedding space. Specifically, it achieves this by defining the\ndistance between connected nodes in the embedding space using the square of the Euclidean\ndistance. It transforms the final optimization objective into the computation of the Laplacian\nmatrix\u2019s eigenvectors. The objective function is computed as follows:\n\ud835\udf19 (\ud835\udc4d) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57\n|\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|\n2\ud835\udc4a\ud835\udc56\ud835\udc57 = \ud835\udc4d\ud835\udc47\n\ud835\udc3f\ud835\udc4d, s.t. \ud835\udc4d\n\ud835\udc47\ud835\udc37\ud835\udc4d = \ud835\udc3c, (2)\nwhere \ud835\udc4a\ud835\udc56\ud835\udc57 represents the connection weight between nodes \ud835\udc56 and \ud835\udc57 in the graph. After linear\ntransformation, the optimization of \ud835\udf19 (\ud835\udc4d) can be reformulated as \ud835\udc4d\n\ud835\udc47 \ud835\udc3f\ud835\udc4d, where \ud835\udc3f = \ud835\udc37 \u2212\ud835\udc4a is the\nconstructed graph Laplacian matrix, and \ud835\udc37 is a symmetric matrix.\nGraph Factorization (GF) [4]. The matrix eigenvector-based methods mentioned before consider\nthe similarity between nodes throughout the entire graph, which can result in excellent node\nfeature representations. However, with the ever-growing scale of real-world graph data, computing\nmatrix eigenvectors for large graphs can be computationally expensive and memory-intensive.\nGF introduces a graph embedding method with a time complexity of \ud835\udc42(|\ud835\udc38|) by factorizing the\nadjacency matrix of the graph. The objective function is as follows:\n\ud835\udf19 (\ud835\udc4d, \ud835\udf06) =\n1\n2\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57 \u2208\ud835\udc38\n|\ud835\udc4a\ud835\udc56,\ud835\udc57 \u2212 \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9|2+\n\ud835\udf06\n2\n\u2211\ufe01\n\ud835\udc56\n|\ud835\udc67\ud835\udc56|\n2\n, (3)\nwhere \ud835\udf06 is a regularization coefficient, and \u27e8\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\u27e9 represents the corresponding inner-product\noperation. Moreover, these inner-product methods also contain GraRep [46] and HOPE [354], which\nconsider higher-order and general node similarity respectively.\n2.2.2 Random walk-based methods Random walk-based methods have also attracted a lot of\nattention in graph embedding learning. The basic idea of these methods is to create random walks\namong nodes in the graph to capture its structural characteristics. Thus, nodes tend to have similar\nembedding if they co-occur on short random walks. Compared to fixed proximity measures in\ntraditional matrix factorization-based methods, these approaches use co-occurrence in a random\nwalk as a measure of node similarity, which is more flexible and has demonstrated promising\nperformance across various applications.\nDeepWalk [362]. DeepWalk analogizes nodes in a graph to words in text. It uses random walks\non the graph to generate numerous node sequences \ud835\udc46 = {\ud835\udc631, . . . , \ud835\udc63|\ud835\udc60 | }, treating these sequences as\nsentences, and then inputting them into the Word2vec [343], which aims to maximize the probability\nof node context given the target node \ud835\udc63\ud835\udc56. It can be written as:\n1\n|\ud835\udc46 |\n\u2211\ufe01\n|\ud835\udc46 |\n\ud835\udc56=1\n\u2211\ufe01\n\u2212\ud835\udc61 \u2264\ud835\udc57\u2264\ud835\udc61,\ud835\udc57\u22600\nlog \ud835\udc5d(\ud835\udc63\ud835\udc56+\ud835\udc57|\ud835\udc63\ud835\udc56), (4)\nwhere \ud835\udc61 is the context window size. Compared to matrix factorization-based methods, DeepWalk\nexhibits extremely low time complexity and is suitable for large-scale graph representation learning.\nHowever, DeepWalk only considers local information between nodes in the graph, making it\nchallenging to find the optimal random walk sampling sequences.\nNode2vec [155]. Based on DeepWalk, Node2vec utilizes parameters \ud835\udc5d and \ud835\udc5e to guide the random\nwalk. Parameter \ud835\udc5d allows the algorithm to revisit previously traversed nodes \ud835\udc61, with smaller\nvalues of \ud835\udc5d increasing the likelihood of returning to \ud835\udc61. Parameter \ud835\udc5e facilitates both inward and\noutward exploration; when \ud835\udc5e > 1, the algorithm tends to visit nodes closer to \ud835\udc61; while for \ud835\udc5e  1. And the first-order similarity can\nbe defined as:\n\ud835\udc3f2 =\n\u2211\ufe01\n(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57 ) \u2208\ud835\udc38\n\ud835\udc34\ud835\udc56\ud835\udc57 |\ud835\udc67\ud835\udc56 \u2212 \ud835\udc67\ud835\udc57|, (8)\nwhere \ud835\udc67\ud835\udc56is the learned representation of node \ud835\udc63\ud835\udc56.\nDeep Neural Graph Representations (DNGR) [47]. Similar to SDNE, DNGR utilizes pointwise\nmutual information between two nodes co-occurring in random walks instead of the adjacency\nmatrix values.\n2.3 Why study deep graph representation learning\nWith the rapid development of deep learning techniques, deep neural networks such as convolu\u0002tional neural networks and recurrent neural networks have made breakthroughs in the fields of\ncomputer vision, natural language processing, and speech recognition. They can well abstract the\nsemantic information of images, natural languages, and speeches. However, current deep learning\ntechniques fail to handle more complex and irregular graph-structured data. To effectively analyze\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n8 W. Ju, et al.\nand model this kind of non-Euclidean structure data, many graph representation learning algo\u0002rithms have emerged in recent years, including graph embedding and graph neural networks. At\npresent, compared with Euclidean-style data such as images, natural language, and speech, graph\u0002structured data is high-dimensional, complex, and irregular. Therefore, the graph representation\nlearning algorithm is a rather powerful tool for studying graph-structured data. To encode complex\ngraph-structured data, deep graph representation learning needs to meet several characteristics: (1)\ntopological properties: Graph representations need to capture the complex topological information\nof the graph, such as the relationship between nodes and nodes, and other substructure information,\nsuch as subgraphs, motif, etc; (2) feature attributes: It is necessary for graph representations to de\u0002scribe high-dimensional attribute features in the graph, including the attributes of nodes and edges\nthemselves; (3) scalability: Because different real graph data have different characteristics, graph\nrepresentation learning algorithms should be able to efficiently learn its embedding representation\non different graph structure data, making it universal and transferable.\n3 Graph Convolutions\nGraph convolutions have become the basic building blocks in many deep graph representation\nlearning algorithms and graph neural networks developed recently. In this section, we provide a\ncomprehensive review of graph convolutions, which generally fall into two categories: spectral\ngraph convolutions and spatial graph convolutions. Based on the solid mathematical foundations\nof Graph Signal Processing (GSP) [164, 396, 414], spectral graph convolutions seek to capture the\npatterns of the graph in the frequency domain. On the other hand, spatial graph convolutions\ninherit the idea of message passing from Recurrent Graph Neural Networks (RecGNNs), and they\ncompute node features by aggregating the features of their neighbors. Thus, the computation graph\nof a node is derived from the local graph structure around it, and the graph topology is naturally\nincorporated into the way node features are computed. In this section, we first introduce spectral\ngraph convolutions and then spatial graph convolutions, followed by a brief summary. In Table 2,\nwe summarize a number of graph convolutions proposed in recent years.\n3.1 Spectral Graph Convolutions\nWith the success of Convolutional Neural Networks (CNNs) in computer vision [244], efforts have\nbeen made to transfer the idea of convolution to the graph domain. However, this is not an easy task\nbecause of the non-Euclidean nature of graphical data. Graph signal processing (GSP) [164, 396, 414]\ndefines the Fourier Transform on graphs and thus provides a solid theoretical foundation of spectral\ngraph convolutions.\nIn graph signal processing, a graph signal refers to a set of scalars associated with every node\nin the graph, i.e. \ud835\udc53 (\ud835\udc63), \u2200\ud835\udc63 \u2208 \ud835\udc49 , and it can be written in the \ud835\udc5b-dimensional vector form x \u2208 R\n\ud835\udc5b\n,\nwhere \ud835\udc5b is the number of nodes in the graph. Another core concept of graph signal processing\nis the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian), defined as\nL = I \u2212 D\n\u22121/2AD\u22121/2\n, where I is the identity matrix, D is the degree matrix (i.e. a diagonal matrix\nD\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc57 A\ud835\udc56\ud835\udc57), and A is the adjacency matrix. In the typical setting of graph signal processing, the\ngraph \ud835\udc3a is undirected. Therefore, L is real symmetric and positive semi-definite. This guarantees\nthe eigen decomposition of the graph Laplacian: L = U\u039bU\ud835\udc47, where U = [u0, u1, ..., u\ud835\udc5b\u22121] is the\neigenvectors of the graph Laplacian and the diagonal elements of \u039b = diag(\ud835\udf060, \ud835\udf061, ..., \ud835\udf06\ud835\udc5b\u22121) are the\neigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal x is defined as x\u02dc = U\n\ud835\udc47 x,\nwhere x\u02dc is the graph frequencies of x. Correspondingly, the Inverse Graph Fourier Transform can\nbe written as x = Ux\u02dc.\nWith GFT and the Convolution Theorem, the graph convolution of a graph signal x and a filter\ng can be defined as g \u2217\ud835\udc3a x = U(U\n\ud835\udc47 g \u2299 U\ud835\udc47 x). To simplify this, let g\ud835\udf03 = diag(U\ud835\udc47 \ud835\udc54), the graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 9\nTable 2. Summary of graph convolution methods.\nMethod Category Aggregation Time Complexity\nSpectral CNN [39] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nHenaff et al. [172] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n3\n)\nChebNet [83] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGCN [230] Spectral / Spatial Weighted Average \ud835\udc42(\ud835\udc5a)\nCayleyNet [254] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGraphSAGE [163] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nGAT [452] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nDGCNN [477] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nLanzcosNet [280] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5b\n2\n)\nSGC [493] Spatial Graph Convolution Weighted Average \ud835\udc42(\ud835\udc5a)\nGWNN [512] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGIN [518] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nGraphAIR [179] Spatial Graph Convolution Sum \ud835\udc42(\ud835\udc5a)\nPNA [77] Spatial Graph Convolution Multiple \ud835\udc42(\ud835\udc5a)\nS\n2GC [606] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGNNML3 [21] Spatial / Spectral - \ud835\udc42(\ud835\udc5a)\nMSGNN [170] Spectral Graph Convolution - \ud835\udc42(\ud835\udc5a)\nEGC [437] Spatial Graph Convolution General \ud835\udc42(\ud835\udc5a)\nAPPNP [138] Spatial Graph Convolution (Approximate) Personalized Pagerank \ud835\udc42(\ud835\udc5a)\nGCNII [61] Spatial Graph Convolution - \ud835\udc42(\ud835\udc5a)\nGATv2 [38] Spatial Graph Convolution Attentive \ud835\udc42(\ud835\udc5a)\nconvolution can be written as:\ng \u2217\ud835\udc3a x = Ug\ud835\udf03U\n\ud835\udc47\nx, (9)\nwhich is the general form of most spectral graph convolutions. The key of spectral graph convolu\u0002tions is to parameterize and learn the filter g\ud835\udf03 .\nSpectral Convolutional Neural Network (Spectral CNN) [39] sets graph filter as a learnable diagonal\nmatrix W. The convolution operation can be written as y = UWU\ud835\udc47 x. In practice, multi-channel\nsignals and activation functions are common, and the graph convolution can be written as\nY:,\ud835\udc57 = \ud835\udf0e\nU\n\u2211\ufe01\ud835\udc50\ud835\udc56\ud835\udc5b\n\ud835\udc56=1\nW\ud835\udc56,\ud835\udc57U\n\ud835\udc47 X:,\ud835\udc56!\n, \ud835\udc57 = 1, 2, ..., \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61, (10)\nwhere \ud835\udc50\ud835\udc56\ud835\udc5b is the number of input channel, \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 is the number of output channel, X is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc56\ud835\udc5b\nmatrix representing the input signal, Y is a \ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix denoting the output signal, W\ud835\udc56,\ud835\udc57 is a\nparameterized diagonal matrix, and \ud835\udf0e(\u00b7) is the activation function. For mathematical convenience\nwe sometimes use single-channel versions of graph convolutions omitting activation functions,\nand the multi-channel versions are similar to Eq. 10.\nSpectral CNN has several limitations. Firstly, the filters are basis-dependent, which means that\nthey cannot be generalized across graphs. Secondly, the algorithm requires eigen decomposition,\nwhich is computationally expensive. Thirdly, it has no guarantee of spatial localization of filters.\nTo make filters spatially localized, Henaff et al. [172] propose to use a smooth spectral transfer\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n10 W. Ju, et al.\nfunction \u0398(\u039b) to parameterize the filter, and the convolution operation can be written as:\ny = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx. (11)\nChebyshev Spectral Convolutional Neural Network (ChebNet) [83] extends this idea by using\ntruncated Chebyshev polynomials to approximate the spectral transfer function. The Chebyshev\npolynomial is defined as \ud835\udc470 (\ud835\udc65) = 1, \ud835\udc471 (\ud835\udc65) = \ud835\udc65, \ud835\udc47\ud835\udc58 (\ud835\udc65) = 2\ud835\udc65\ud835\udc47\ud835\udc58\u22121 (\ud835\udc65) \u2212 \ud835\udc47\ud835\udc58\u22122 (\ud835\udc65), and the spectral\ntransfer function \ud835\udc39 (\u039b) is approximated to the order of \ud835\udc3e \u2212 1 as\n\ud835\udc39 (\u039b) =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc ), (12)\nwhere the model parameters \ud835\udf03\ud835\udc58, \ud835\udc58 \u2208 {0, 1, ..., \ud835\udc3e \u2212 1} are the Chebyshev coefficients, and \u039b\u02dc =\n2\u039b/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be written\nas:\ng \u2217\ud835\udc3a x = U\ud835\udc39 (\u039b)U\n\ud835\udc47\nx = U\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (\u039b\u02dc )U\n\ud835\udc47\nx =\n\ud835\udc3e\n\u2211\ufe01\u22121\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58\ud835\udc47\ud835\udc58 (L\u02dc)x, (13)\nwhere L\u02dc = 2L/\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 I.\nGraph Convolutional Network (GCN) [230] is proposed as the localized first-order approximation\nof ChebNet. Assuming \ud835\udc3e = 2 and \ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65 = 2, Eq. 13 can be simplified as:\ng \u2217\ud835\udc3a x = \ud835\udf030x + \ud835\udf031 (L \u2212 I)x = \ud835\udf030x \u2212 \ud835\udf031D\n\u22121/2AD\u22121/2\nx. (14)\nTo further constraint the number of parameters, we assume \ud835\udf03 = \ud835\udf030 = \u2212\ud835\udf031, which gives a simpler\nform of graph convolution:\ng \u2217G x = \ud835\udf03 (I + D\n\u22121/2AD\u22121/2\n)x. (15)\nAs I + D\n\u22121/2AD\u22121/2 now has the eigenvalues in the range of [0, 2] and repeatedly multiplying\nthis matrix can lead to numerical instabilities, GCN empirically proposes a renormalization trick to\nsolve this problem by using D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2instead, where A\u02dc = A + I and D\u02dc\n\ud835\udc56\ud835\udc56 =\n\u00cd\n\ud835\udc56 A\u02dc\n\ud835\udc56\ud835\udc57 .\nAllowing multi-channel signals and adding activation functions, the more common formula in\nliterature is:\nY = \ud835\udf0e( (D\u02dc \u22121/2A\u02dc D\u02dc \u22121/2)X\u0398), (16)\nwhere X, Y have the same shape as in Eq. 10 and \u0398 is a \ud835\udc50\ud835\udc56\ud835\udc5b \u00d7 \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc61 matrix as model\u2019s parameters.\nApart from the aforementioned methods, other spectral graph convolutions have been proposed.\nLevie et al. [254] propose CayleyNets that utilize Cayley Polynomials to equip the filters with\nthe ability to detect narrow frequency bands. Liao et al. [280] propose LanczosNets that employ\nthe Lanczos algorithm to construct a low-rank approximation of graph Laplacian to improve the\ncomputation efficiency of graph convolutions. The proposed model is able to efficiently utilize the\nmulti-scale information in the graph data. Instead of using Graph Fourier Transform, Xu et al. [512]\npropose a Graph Wavelet Neural Network (GWNN) that uses graph wavelet transform to avoid\nmatrix eigendecomposition. Moreover, graph wavelets are sparse and localized, which provides\ngood interpretations for the convolution operation. Zhu and Koniusz [606] derive a Simple Spectral\nGraph Convolution (S2GC) from a modified Markov Diffusion Kernel, which achieves a trade-off\nbetween low-pass and high-pass filter bands.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 11\n3.2 Spatial Graph Convolutions\nInspired by the convolution on Euclidean data (e.g. images and texts), which applies data trans\u0002formation on a small region, spatial graph convolutions compute the central node\u2019s feature via\ntransforming and aggregating its neighbors\u2019 features. In this way, the graph structure is naturally\nembedded in the computation graph of node features. Moreover, the idea of sending one node\u2019s\nfeature to another node is similar to the message passing used in recurrent graph neural networks.\nIn the following, we will introduce several seminal spatial graph convolutions as well as some\nrecently proposed promising methods.\nSpatial graph convolutions generally follow a three-step paradigm: message generation, feature\naggregation and feature update. This can be mathematically written as:\ny\ud835\udc56 = UPDATE x\ud835\udc56, AGGREGATE {MESSAGE x\ud835\udc56, x\ud835\udc57, e\ud835\udc56\ud835\udc57\u0001, \ud835\udc57 \u2208 N (\ud835\udc56)}\u0001\u0001 , (17)\nwhere x\ud835\udc56 and y\ud835\udc56is the input and output feature vector of node \ud835\udc56, e\ud835\udc56\ud835\udc57 is the feature vector of the edge\n(or more generally, the relationship) between node \ud835\udc56 and its neighbor node \ud835\udc57, and N (\ud835\udc56) denote the\nneighbor of node \ud835\udc56, which could be more generally defined.\nIn the previous subsection, we show the spectral interpretation of GCN [230]. The model also\nhas its spatial interpretation, which can be mathematically written as:\ny\ud835\udc56 = \u0398\n\ud835\udc47 \u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\u222a\ud835\udc56\n1\n\u221a\ufe03\n\u02c6\ud835\udc51\ud835\udc56\u02c6\ud835\udc51\ud835\udc57\nx\ud835\udc57, (18)\nwhere \u02c6\ud835\udc51\ud835\udc56 and \u02c6\ud835\udc51\ud835\udc57is the \ud835\udc56-th and \ud835\udc57-th row sums of A\u02c6 in Eq. 16. For each node, the model takes a\nweighted sum of its neighbors\u2019 features as well as its own features and applies a linear transformation\nto obtain the result. In practice, multiple GCN layers are often stacked together with non-linear\nfunctions after convolution to encode complex and hierarchical features. Nonetheless, Wu et al.\n[493] show that the model still achieves competitive results without non-linearity.\nAlthough GCN as well as other spectral graph convolutions achieve competitive results on a\nnumber of benchmarks, these methods assume the presence of all nodes in the graph and fall in the\ncategory of transductive learning. Hamilton et al. [163] propose GraphSAGE that performs graph\nconvolutions in inductive settings, when there are new nodes during inference (e.g. newcomers\nin the social network). For each node, the model samples its \ud835\udc3e-hop neighbors and uses \ud835\udc3e graph\nconvolutions to aggregate their features hierarchically. Furthermore, the use of sampling also\nreduces the computation when a node has too many neighbors.\nThe attention mechanism has been successfully used in natural language processing [451], com\u0002puter vision [295] and multi-modal tasks [62, 168, 552, 591]. Graph Attention Networks (GAT) [452]\nintroduces the idea of attention to graphs. The attention mechanism uses an adaptive, feature\u0002dependent weight (i.e. attention coefficient) to aggregate a set of features, which can be mathemati\u0002cally written as:\n\ud835\udefc\ud835\udc56,\ud835\udc57 =\nexp LeakyReLU a\n\ud835\udc47\n[\u0398x\ud835\udc56||\u0398x\ud835\udc57]\n\u0001\u0001\n\u00cd\n\ud835\udc58 \u2208N (\ud835\udc56)\u222a{\ud835\udc56 } exp\nLeakyReLU a\n\ud835\udc47 [\u0398x\ud835\udc56\n||\u0398x\ud835\udc57]\n\u0001\u0001 , (19)\nwhere \ud835\udefc\ud835\udc56,\ud835\udc57 is the attention coefficient, a and \u0398 are model parameters, and [\u00b7||\u00b7] means concatenation.\nAfter the \ud835\udefcs are obtained, the new features are computed as a weighted sum of input node features,\nwhich is:\ny\ud835\udc56 = \ud835\udefc\ud835\udc56,\ud835\udc56\u0398x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\n\ud835\udefc\ud835\udc56,\ud835\udc57\u0398x\ud835\udc57. (20)\nXu et al. [518] explore the representational limitations of graph neural networks. What they\ndiscover is that message passing networks like GCN [230] and GraphSAGE [163] are incapable of\ndistinguishing certain graph structures. To improve the representational power of graph neural\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n12 W. Ju, et al.\nnetworks, they propose the Graph Isomorphism Network (GIN) that gives an adjustable weight to\nthe central node feature, which can be mathematically written as:\ny\ud835\udc56 = MLP \u00a9\n\u00ad\n\u00ab\n(1 + \ud835\udf16)x\ud835\udc56 +\n\u2211\ufe01\n\ud835\udc57 \u2208N (\ud835\udc56)\nx\ud835\udc57\n\u00aa\n\u00ae\n\u00ac\n, (21)\nwhere \ud835\udf16 is a learnable parameter.\nMore recently, efforts have been made to improve the representational power of graph neural\nnetworks. For example, Hu et al. [179] propose GraphAIR that explicitly models the neighborhood\ninteraction to better capture complex non-linear features. Specifically, they use the Hadamard\nproduct between pairs of nodes in the neighborhood to model the quadratic terms of neighborhood\ninteraction. Balcilar et al. [21] propose GNNML3 that breaks the limits of the first-order Weisfeiler\u0002Lehman test (1-WL) and reaches the third-order WL test (3-WL) experimentally. They also show\nthat the Hadamard product is required for the model to have more representational power than the\nfirst-order Weisfeiler-Lehman test. Other elements in spatial graph convolutions are widely studied.\nFor example, Corso et al. [77] explore the aggregation operation in GNN and proposes Principal\nNeighbourhood Aggregation (PNA) that uses multiple aggregators with degree-scalers. Tailor et al.\n[437] explore the anisotropism and isotropism in the message passing process of graph neural\nnetworks, and proposes Efficient Graph Convolution (EGC) that achieves promising results with\nreduced memory consumption due to isotropism. In order to increase the size of the neighborhood\nof a node, Gasteiger et al. [138] propose personalized propagation of neural predictions (PPNP) and\nits approximation using power iteration (APPNP). To increase the depth of graph neural networks,\nChen et al. [61] propose GCNII that uses initial residual and identity mapping to mitigate the over\u0002smoothing problem. Brody et al. [38] propose GATv2 that uses dynamic attention and improves\nthe expressive power of GAT [452].\n3.3 Summary\nThis section introduces graph convolutions. We provide the summary as follows:\n\u2022 Techniques. Graph convolutions mainly fall into two types, i.e. spectral graph convolu\u0002tions and spatial graph convolutions. Spectral graph convolutions have solid mathematical\nfoundations of Graph Signal Processing and therefore their operations have theoretical in\u0002terpretations. Spatial graph convolutions are inspired by Recurrent Graph Neural Networks\nand their computation is simple and straightforward, as their computation graph is derived\nfrom the local graph structure. Generally, spatial graph convolutions are more common in\napplications.\n\u2022 Challenges and Limitations. Despite the great success of graph convolutions, their perfor\u0002mance is unsatisfactory in more complicated applications. On the one hand, the performance\nof graph convolutions relies heavily on the construction of the graph. Different constructions\nof the graph might result in different performances of graph convolutions. On the other\nhand, graph convolutions are prone to over-smoothing when constructing very deep neural\nnetworks.\n\u2022 Future Works. In the future, we expect that more powerful graph convolutions will be\ndeveloped to mitigate the problem of over-smoothing and we also hope that techniques and\nmethodologies in Graph Structure Learning (GSL) can help learn more meaningful graph\nstructure to benefit the performance of graph convolutions.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 13\n4 Graph Kernel Neural Networks\nGraph kernels (GKs) are historically the most widely used technique on graph analyzing and\nrepresentation tasks [137, 243, 423, 601]. However, traditional graph kernels rely on hand-crafted\npatterns or domain knowledge on specific tasks[242, 410]. Over the years, an amount of research has\nbeen conducted on graph kernel neural networks (GKNNs), which has yielded promising results.\nResearchers have explored various aspects of GKNNs, including their theoretical foundations,\nalgorithmic design, and practical applications. These efforts have led to the development of a wide\nrange of GKNN-based models and methods that can be used for graph analysis and representation\ntasks, such as node classification [113, 222, 298, 534], link prediction [54, 300, 497, 525], and graph\nclustering [243, 299].\nThe success of GKNNs can be attributed to their ability to leverage the strengths of both graph\nkernels and neural networks [221, 299, 497]. By using kernel functions to measure similarity\nbetween graphs, GKNNs can capture the structural properties of graphs, while the use of neural\nnetworks enables them to learn more complex and abstract representations of graphs [56, 558].\nThis combination of techniques allows GKNNs to achieve state-of-the-art performance on a wide\nrange of graph-related tasks [216, 243, 479].\nIn this section, we begin with introducing the most representative traditional graph kernels.\nThen we summarize the basic framework for combining GNNs and graph kernels. Finally, we\ncategorize the popular graph kernel Neural networks into several categories and compare their\ndifferences.\n4.1 Graph Kernels\nGraph kernels generally evaluate pairwise similarity between nodes or graphs by decomposing\nthem into basic structural units. Random walks [223], subtrees [409], shortest paths [32] and\ngraphlets [410] are representative categories.\nGiven two graphs \ud835\udc3a1 = (\ud835\udc491, \ud835\udc381, \ud835\udc4b1) and \ud835\udc3a2 = (\ud835\udc492, \ud835\udc382, \ud835\udc4b2), a graph kernel function \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2)\nmeasures the similarity between \ud835\udc3a1 and \ud835\udc3a2 through the following formula:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 \ud835\udc59\ud835\udc3a1(\ud835\udc621),\ud835\udc59\ud835\udc3a2(\ud835\udc622)\n\u0001\n, (22)\nwhere \ud835\udc59\ud835\udc3a (\ud835\udc62) denotes a set of local substructures centered at node \ud835\udc62 in graph \ud835\udc3a, and \ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 is a base\nkernel measuring the similarity between the two sets of substructures. For simplicity, we may\nrewrite Eq. 22 as:\n\ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52 (\ud835\udc621, \ud835\udc622), (23)\nthe uppercase letter \ud835\udc3e(\ud835\udc3a1,\ud835\udc3a2) is denoted as graph kernels, \ud835\udf05(\ud835\udc621, \ud835\udc622) is denoted as node kernels,\nand lowercase \ud835\udc58 (\ud835\udc65, \ud835\udc66) is denoted as general kernel functions.\nThe kernel mapping of a kernel \ud835\udf13 maps a data point into its corresponding Reproducing Kernel\nHilbert Space (RKHS) H. Specifically, given a kernel \ud835\udc58\u2217 (\u00b7, \u00b7), its kernel mapping\ud835\udf13\u2217 can be formalized\nas,\n\u2200\ud835\udc651, \ud835\udc652, \ud835\udc58\u2217 (\ud835\udc651, \ud835\udc652) = \u27e8\ud835\udf13\u2217 (\ud835\udc651),\ud835\udf13\u2217 (\ud835\udc652)\u27e9H\u2217, (24)\nwhere H\u2217 is the RKHS of \ud835\udc58\u2217 (\u00b7, \u00b7).\nWe introduce several representative and popular graph kernels below.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n14 W. Ju, et al.\nWalk and Path Kernels. A \ud835\udc59-walk kernel \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 compares all length \ud835\udc59 walks starting from each\nnode in two graphs \ud835\udc3a1,\ud835\udc3a2,\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc641\u2208W\ud835\udc59(\ud835\udc3a1,\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc642\u2208W\ud835\udc59(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc4b1 (\ud835\udc641), \ud835\udc4b2 (\ud835\udc642)),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc59)\n\ud835\udc64\ud835\udc4e\ud835\udc59\ud835\udc58 (\ud835\udc621, \ud835\udc622).\n(25)\nSubstituting W with P is able to get the \ud835\udc59-path kernel.\nSubtree Kernels. The WL subtree kernel is the most popular one in subtree kernels. It is a finite\u0002depth kernel variant of the 1-WL test. The WL subtree kernel with depth \ud835\udc59, \ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f compares all\nsubtrees with depth \u2264 \ud835\udc59 rooted at each node.\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622) =\n\u2211\ufe01\n\ud835\udc611\u2208 T\ud835\udc56(\ud835\udc3a1,\ud835\udc622 )\n\u2211\ufe01\n\ud835\udc612\u2208 T\ud835\udc56(\ud835\udc3a2,\ud835\udc622 )\n\ud835\udeff (\ud835\udc611, \ud835\udc612),\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc621\u2208\ud835\udc491\n\u2211\ufe01\n\ud835\udc622\u2208\ud835\udc492\n\ud835\udf05\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc621, \ud835\udc622),\n\ud835\udc3e\n(\ud835\udc59)\n\ud835\udc4a \ud835\udc3f (\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc59\n\ud835\udc56=0\n\ud835\udc3e\n(\ud835\udc56)\n\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc52 (\ud835\udc3a1,\ud835\udc3a2),\n(26)\nwhere \ud835\udc61 \u2208 T (\ud835\udc56)(\ud835\udc3a, \ud835\udc62) denotes a subtree of depth \ud835\udc56 rooted at \ud835\udc62 in \ud835\udc3a.\n4.2 General Framework of GKNNs\nIn this section, we summarize the general framework of GKNNs. For the first step, a kernel that\nmeasures similarities of heterogeneous features from heterogeneous nodes and edges (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc622) and\n(\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622) should be defined. Take the inner product of neighbor tensors as an example, its neighbor\nkernel is defined as follows,\n\ud835\udf05( (\ud835\udc621, \ud835\udc52\u00b7,\ud835\udc621), (\ud835\udc622, \ud835\udc52\u00b7,\ud835\udc622)) = \u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc621), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc622)\u27e9.\nBased on the neighbor kernel, a kernel with two \ud835\udc59-hop neighborhoods for central node \ud835\udc621 and \ud835\udc622\ncan be defined as \ud835\udc3e\n(\ud835\udc59)\n(\ud835\udc621, \ud835\udc622) =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \ud835\udc59 = 0\n\u27e8\ud835\udc53 (\ud835\udc621), \ud835\udc53 (\ud835\udc622)\u27e9 \u00b7 \u2211\ufe01\n\ud835\udc631\u2208\ud835\udc41 (\ud835\udc621 )\n\u2211\ufe01\n\ud835\udc632\u2208\ud835\udc41 (\ud835\udc622 )\n\ud835\udc3e\n(\ud835\udc59\u22121)\n(\ud835\udc631, \ud835\udc632) \u00b7 \u27e8\ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc631), \ud835\udc53 (\ud835\udc52\u00b7,\ud835\udc632\n)\u27e9 \ud835\udc59 > 0\n, (27)\nBy regarding the lower-hop kernel \ud835\udf05\n(\ud835\udc59\u22121)\n(\ud835\udc621, \ud835\udc622), as the inner product of the (\ud835\udc59 \u2212 1)-th hidden\nrepresentations of \ud835\udc621 and \ud835\udc622. Furthermore, by recursively applying the neighborhood kernel, the\n\ud835\udc59-hop graph kernel can be derived as\n\ud835\udc3e\n\ud835\udc59\n(\ud835\udc3a1,\ud835\udc3a2) =\n\u2211\ufe01\n\ud835\udc981\u2208W\ud835\udc59(\ud835\udc3a1 )\n\u2211\ufe01\n\ud835\udc982\u2208W\ud835\udc59(\ud835\udc3a2 )\n\u00d6\n\ud835\udc59\u22121\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n1\n), \ud835\udc53 (\ud835\udc98\n(\ud835\udc56)\n2\n)\u27e9 \u00d7\u00d6\n\ud835\udc59\u22122\n\ud835\udc56=0\n\u27e8\ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n1\n,\ud835\udc98\n(\ud835\udc56+1)\n1\n), \ud835\udc53 (\ud835\udc52\ud835\udc98\n(\ud835\udc56)\n2\n,\ud835\udc98\n(\ud835\udc56+1)\n2\n)\u27e9!,\n(28)\nwhere W\ud835\udc59(\ud835\udc3a) denotes the set of all walk sequences with length \ud835\udc59 in graph \ud835\udc3a, and \ud835\udc98\n(\ud835\udc56)\n1\ndenotes the\n\ud835\udc56-th node in sequence \ud835\udc981.\nAs shown in Eq. 24, kernel methods implicitly perform projections from original data spaces\nto their RKHS H. Hence, as GNNs also project nodes or graphs into vector spaces, connections\nhave been established between GKs and GNNs through the kernel mappings. And several works\nconducted research on the connections [253, 491], and found some foundation conclusions. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 15\nthe basic rule introduced in [253] as an example, the proposed graph kernel in Eq. 22 can be derived\nas the general formulas,\n\u210e\n(0)\n(\ud835\udc63) =\ud835\udc7e\n(0)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63),\n\u210e\n(\ud835\udc59)\n(\ud835\udc63) =\ud835\udc7e\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\ud835\udc53 (\ud835\udc63) \u2299 \u2211\ufe01\n\ud835\udc62\u2208\ud835\udc41 (\ud835\udc63)\n(\ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc49 (\ud835\udc63)\n\u210e\n(\ud835\udc59\u22121)\n(\ud835\udc62) \u2299 \ud835\udc7c\n(\ud835\udc59)\n\ud835\udc61\ud835\udc38 (\ud835\udc52\ud835\udc62,\ud835\udc63 )\n\ud835\udc53 (\ud835\udc52\ud835\udc62,\ud835\udc63 )), 1  1-WL\nGGT [102] \u2713 \u2713 structure only \u2713\nGTSA [241] \u2713 \u2713 \u2713 \u2713\nHGT [183] \u2713 \u2713\nG2SHGT [536] \u2713 \u2713 \u2713\nHINormer [335] \u2713 \u2713 \u2713\nGRUGT [41] \u2713 \u2713 \u2713\nGRIT [324] \u2713 \u2713 \u2713\nGraphormer-GD [560] \u2713 \u2713 \u2713\nGraphormer [540] \u2713 \u2713 \u2713 \u2713\nGSGT [191] \u2713 \u2713 \u2713\nTMDG [145] \u2713 \u2713 \u2713\nGraph-BERT [567] \u2713 \u2713 \u2713\nLRGT [498] \u2713 \u2713\nSAT [56] \u2713 \u2713 \u2713\n6.1 Transformer\nTransformer [451] was first applied to model machine translation, but two of the key mechanisms\nadopted in this work, attention operation and positional encoding, are highly compatible with the\ngraph modeling problem.\nTo be specific, we denote the input of attention layer in Transformer as X = [x0, x1, . . . , x\ud835\udc5b\u22121],\nx\ud835\udc56 \u2208 R\n\ud835\udc51\n, where \ud835\udc5b is the length of input sequence and \ud835\udc51 is the dimension of each input embedding\nx\ud835\udc56. Then the core operation of calculating new embedding x\u02c6\ud835\udc56 for each x\ud835\udc56in attention layer can be\nstreamlined as:\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57) = NORM\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\nQ\n\u210e\n(x\ud835\udc56)\nTK\u210e\n(x\ud835\udc58 )),\nx\n\u210e\n\ud835\udc56 =\n\u2211\ufe01\nx\ud835\udc57 \u2208X\ns\n\u210e\n(x\ud835\udc56, x\ud835\udc57)V\u210e(x\ud835\udc57),\nx\u02c6\ud835\udc56 = MERGE(x\n1\n\ud835\udc56\n, x\n2\n\ud835\udc56\n, . . . , x\n\ud835\udc3b\n\ud835\udc56\n),\n(57)\nwhere \u210e \u2208 {0, 1, . . . , \ud835\udc3b \u2212 1} represents the attention head number. Q\n\u210e\n, K\u210eand V\u210eare projection\nfunctions mapping a vector to the query space, key space and value space respectively. s\n\u210e\n(x\ud835\udc56\n, x\ud835\udc57) is\nscore function measuring the similarity between x\ud835\udc56 and x\ud835\udc57. NORM is the normalization operation\nensuring \u00cd\nx\ud835\udc57 \u2208X s\n\u210e\n(x\ud835\udc56, x\ud835\udc57) \u2261 1 to propel the stability of the output generated by a stack of attention\nlayers, it is usually performed as scaled softmax: NORM(\u00b7) = SoftMax(\u00b7/\u221a\ud835\udc51). And MERGE function\nis designed to combine the information extracted from multiple attention heads. Here, we omit\nfurther implementation details that do not affect our understanding of attention operation.\nThe attention process cannot encode the position information of each x\ud835\udc56, which is essential in\nmachine translation problems. So positional encoding is introduced to remedy this deficiency, and\nit\u2019s calculated as:\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57\n= sin(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), X\n\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc56,2\ud835\udc57+1\n= cos(\ud835\udc56/100002\ud835\udc57/\ud835\udc51), (58)\nwhere \ud835\udc56 is the position and \ud835\udc57 is the dimension. The positional encoding is added to the input before\nit is fed to the Transformer.\n6.2 Overview\nFrom the simplified process shown in Eq. 57, we can see that the core of the attention operation is\nto accomplish information transfer based on the similarity between the source and the target to be\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 25\nupdated. It\u2019s quite similar to the message-passing process on a fully-connected graph. However,\nthe direct application of this architecture to arbitrary graphs does not make use of structural\ninformation, so it may lead to poor performance when graph topology is important. On the other\nhand, the definition of positional encoding in graphs is not a trivial problem because the order or\ncoordinates of graph nodes are underdefined.\nAccording to these two challenges, Transformer-based methods for graph representation learning\ncan be classified into two major categories, one considering graph structure during the attention\nprocess, and the other encoding the topological information of the graph into initial node features.\nWe name the first one as Attention Modification and the second one as Encoding Enhancement. A\nsummarization is provided in Table 5. In the following discussion, if both methods are used in\none paper, we will list them in different subsections, and we will ignore the multi-head trick in\nattention operation.\n6.3 Attention Modification\nThis group of works attempts to modify the full attention operation to capture structure information.\nThe most prevalent approach is changing the score function, which is denoted as s(\u00b7, \u00b7) in Eq. 57.\nGGT [102] constrains each node feature can only attend to neighbors and enables the model to\nrepresent edge feature information by rewrite s(\u00b7, \u00b7) as:\ns\u02dc1 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\ud835\udc44x\ud835\udc56)\nT\n(W\ud835\udc3ex\ud835\udc57 \u2299 W\ud835\udc38e\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\u2212 \u221e, otherwise\n,\ns1 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc1 (x\ud835\udc56, x\ud835\udc58 )),\n(59)\nwhere \u2299 is Hadamard product and W\ud835\udc44,\ud835\udc3e,\ud835\udc38 represents trainable parameter matrix. This approach\nis not efficient yet to model long-distance dependencies since only 1st-neighbors are considered.\nThough it adopts Laplacian eigenvectors to gather global information (see Section 6.4), but only long\u0002distance structure information is remedied while the node and edge features are not. GTSA [241]\nimproves this approach by combining the original graph and the full graph. Specifically, it extends\ns1 (\u00b7, \u00b7) to:\ns\u02dc2 (x\ud835\udc56, x\ud835\udc57) =\n(\n(W\n\ud835\udc44\n1\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n1\nx\ud835\udc57 \u2299 W\ud835\udc38\n1\ne\ud835\udc57\ud835\udc56), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n(W\n\ud835\udc44\n0\nx\ud835\udc56)\nT\n(W\ud835\udc3e\n0\nx\ud835\udc57 \u2299 W\ud835\udc38\n0\ne\ud835\udc57\ud835\udc56), otherwise\n,\ns2 (x\ud835\udc56, x\ud835\udc57) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\n\uf8f3\n1\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2208\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), \u27e8\ud835\udc57,\ud835\udc56\u27e9 \u2208 \ud835\udc38\n\ud835\udf06\n1 + \ud835\udf06\nSoftMax\ud835\udc57 ( \u2225\n\u27e8\ud835\udc58,\ud835\udc56\u27e9\u2209\ud835\udc38\ns\u02dc2 (x\ud835\udc56, x\ud835\udc58 )), otherwise\n,\n(60)\nwhere \ud835\udf06 is a hyperparameter representing the strength of the full connection.\nSome works try to reduce information-mixing problems [55] in heterogeneous graphs. HGT [183]\ndisentangles the attention of different node types and edge types by adopting additional attention\nheads. It defines W\ud835\udf0f (\ud835\udc63)\n\ud835\udc44,\ud835\udc3e,\ud835\udc49 for each node type \ud835\udf0f (\ud835\udc63) and W\n\ud835\udf19 (\ud835\udc52 )\n\ud835\udc38\nfor each edge type \ud835\udf19 (\ud835\udc52), \ud835\udf0f (\u00b7) and\n\ud835\udf19 (\u00b7) are type indicating function. G2SHGT [536] defines four types of subgraphs, fully-connected,\nconnected, default and reverse, to capture global, undirected, forward and backward information\nrespectively. Each subgraph is homogeneous, so it can reduce interactions between different classes.\nPath features between nodes are always treated as inductive bias added to the original score\nfunction. Let SP\ud835\udc56\ud835\udc57 = (\ud835\udc521, \ud835\udc522, . . . , \ud835\udc52\ud835\udc41 ) denote the shortest path between node pair (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57). GRUGT [41]\nuses GRU [74] to encode forward and backward features as: r\ud835\udc56\ud835\udc57 = GRU(SP\ud835\udc56\ud835\udc57), r\ud835\udc57\ud835\udc56 = GRU(SP\ud835\udc57\ud835\udc56).\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n26 W. Ju, et al.\nThen, the final attention score is calculated by adding up four components:\ns\u02dc3 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nx\ud835\udc57 + (W\ud835\udc44r\ud835\udc56\ud835\udc57)\nTW\ud835\udc3e\nr\ud835\udc57\ud835\udc56, (61)\nfrom front to back, which represent content-based score, source-dependent bias, target-dependent\nbias and universal bias respectively. Graphormer [540] uses both path length and path embedding\nto introduce structural bias as:\ns\u02dc4 (x\ud835\udc56, x\ud835\udc57) = (W\ud835\udc44x\ud835\udc56)\nTW\ud835\udc3e\nx\ud835\udc57 /\n\u221a\n\ud835\udc51 + \ud835\udc4f\ud835\udc41 + \ud835\udc50\ud835\udc56\ud835\udc57,\n\ud835\udc50\ud835\udc56\ud835\udc57 =\n1\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc58=1\n(e\ud835\udc58 )\nTw\ud835\udc38\n\ud835\udc58\n,\ns4 (x\ud835\udc56, x\ud835\udc57) = SoftMax\ud835\udc57 ( \u2225\nx\ud835\udc58 \u2208X\ns\u02dc4 (x\ud835\udc56, x\ud835\udc58 )),\n(62)\nwhere \ud835\udc4f\ud835\udc41 is a trainable scalar indexed by \ud835\udc41, the length of SP\ud835\udc56\ud835\udc57 . e\ud835\udc58 is the embedding of the the\nedge \ud835\udc52\ud835\udc58 , and w\ud835\udc38\n\ud835\udc58\n\u2208 R\n\ud835\udc51\nis the \ud835\udc58-th edge parameter. If SP\ud835\udc56\ud835\udc57 does not exist, then \ud835\udc4f\ud835\udc41 and \ud835\udc50\ud835\udc56\ud835\udc57 are set to\nbe special values. GRIT [324] utilizes relative random walk probabilities as an inductive bias to\nencode relative path information. Graphormer-GD [560] also incorporates relative distance as bias,\nand rigorously proves that this bias is crucial for determining the biconnectivity of a graph.\n6.4 Encoding Enhancement\nThis kind of method intends to enhance initial node representations to enable the Transformer to\nencode structure information. They can be further divided into two categories, position-analogy\nmethods and structure-aware methods.\n6.4.1 Position-analogy methods In Euclidean space, the Laplacian operator corresponds to the\ndivergence of the gradient, whose eigenfunctions are sine/cosine functions. For the graph, the\nLaplacian operator is the Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.\nHence, inspired by Eq. 58, position-analogy methods utilize Laplacian eigenvectors to simulate\npositional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as they are the equivalents of sine/cosine functions.\nLaplacian eigenvectors can be calculated via the eigendecomposition of normalized graph Lapla\u0002cian matrix L\u02dc\n:\nL\u02dc \u225c I \u2212 D\n\u22121/2AD\u22121/2 = U\u039bUT\n, (63)\nwhere A is the adjacency matrix, D is the degree matrix, U = [u1, u2, . . . , u\ud835\udc5b\u22121] are eigenvectors\nand \u039b = \ud835\udc51\ud835\udc56\ud835\udc4e\ud835\udc54(\ud835\udf060, \ud835\udf061, . . . , \ud835\udf06\ud835\udc5b\u22121) are eigenvalues. With U and \u039b, GGT [102] uses eigenvectors of the\nk smallest non-trivial eigenvalues to denote the intermediate embedding X\n\ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\n, and maps it\nto d-dimensional space and gets the position encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\n. This process can be formalized\nas:\n\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 = argmin\ud835\udc58({\ud835\udf06\ud835\udc56|0 \u2264 \ud835\udc56  0}),\nX\n\ud835\udc5a\ud835\udc56\ud835\udc51 = [u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc650\n, u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc651, . . . , u\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\ud835\udc58\u22121]\nT\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = X\ud835\udc5a\ud835\udc56\ud835\udc51W\ud835\udc58\u00d7\ud835\udc51\n,\n(64)\nwhere \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65 is the subscript of the selected eigenvectors. GTSA [241] puts eigenvector u\ud835\udc56 on\nthe frequency axis at \ud835\udf06\ud835\udc56 and uses sequence modeling methods to generate positional encoding.\nSpecifically, it extends X\n\ud835\udc5a\ud835\udc56\ud835\udc51 in Eq. 64 to X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51 \u2208 R\ud835\udc5b\u00d7\ud835\udc58\u00d72 by concatenating each value in eigenvectors\nwith corresponding eigenvalue, and then positional encoding X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 \u2208 R\ud835\udc5b\u00d7\ud835\udc51\nare generated as:\nX\n\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 = X\u02dc \ud835\udc5a\ud835\udc56\ud835\udc51W2\u00d7\ud835\udc51\n,\nX\n\ud835\udc5d\ud835\udc5c\ud835\udc60 = SumPooling(Transformer(X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61), dim = 1).\n(65)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 27\nHere, X\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 \u2208 R\n\ud835\udc5b\u00d7\ud835\udc58\u00d7\ud835\udc51\nis equivalent to the input matrix in sequence modeling with shape\n(\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e_\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52,\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e, \ud835\udc51\ud835\udc56\ud835\udc5a), and can be naturally processed by Transformer. Since the Laplacian\neigenvectors can be complex-valued for directed graph, GSGT [191] proposes to utilize SVD of\nadjacency matrix A, which is denoted as A = U\u03a3VT, and uses the largest \ud835\udc58 singular values \u03a3\ud835\udc58 and\nassociated left and right singular vectors U\ud835\udc58 and V\nT\n\ud835\udc58\nto output X\n\ud835\udc5d\ud835\udc5c\ud835\udc60 as X\ud835\udc5d\ud835\udc5c\ud835\udc60 = [U\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n\u2225V\ud835\udc58\u03a3\n1/2\n\ud835\udc58\n],\nwhere \u2225 is the concatenation operation. In addition to SVD, TMDG [145] processes directed graphs\nby utilizing the Magnetic Laplacian. All these methods above randomly flip the signs of eigenvectors\nor singular vectors during the training phase to promote the invariance of the models to the sign\nambiguity.\n6.4.2 Structure-aware methods In contrast to position-analogy methods, structure-aware methods\ndo not attempt to mathematically rigorously simulate sequence positional encoding. They use some\nadditional mechanisms to directly calculate structure-related encoding.\nSome approaches compute extra encoding X\n\ud835\udc4e\ud835\udc51\ud835\udc51 and add it to the initial node representation.\nGraphormer [540] proposes to leverage node centrality as an additional signal to address the\nimportance of each node. Concretely, x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nis determined by the in-degree deg\u2212\n\ud835\udc56\nand outdegree deg+\n\ud835\udc56\n:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56 = P\n\u2212\n(deg\u2212\n\ud835\udc56\n) + P+(deg+\n\ud835\udc56\n), (66)\nwhere P\n\u2212\nand P\n+\nare learnable embedding function. Graph-BERT [567] employs Weisfeiler-Lehman\nalgorithm to label node \ud835\udc63\ud835\udc56 to a number WL(\ud835\udc63\ud835\udc56) \u2208 N and defines x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56\nas:\nx\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57 = sin(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n), x\n\ud835\udc4e\ud835\udc51\ud835\udc51\n\ud835\udc56,2\ud835\udc57+1 = cos(WL(\ud835\udc63\ud835\udc56)/100002\ud835\udc57/\ud835\udc51\n). (67)\nThe other approaches try to leverage GNNs to initialize inputs to the Transformer. LRGT [498]\napplies GNN to get intermediate vectors as X\n\u2032 = GNN(X), and passes the concatenation of X\u2032\nand\na special vector xCLS to Transformer layer as: X\u02c6 = Transformer( [X\n\u2032\n\u2225xCLS]). Then x\u02c6CLS can be used\nas the representation of the entire graph for downstream tasks. This method cannot break the 1-WL\nbottleneck because it uses GCN [230] and GIN [518] as graph encoders in the first step, which\nare intrinsically limited by 1-WL test. SAT [56] improves this deficiency by using subgraph-GNN\nNGNN [569] for initialization, and achieves outstanding performance.\n6.5 Summary\nThis section introduces Transformer-based approaches for graph representation learning and we\nprovide the summary as follows:\n\u2022 Techniques. Graph Transformer methods modify two fundamental techniques in Trans\u0002former, attention operation and positional encoding, to enhance its ability to encode graph\ndata. Typically, they introduce fully connected attention to model long-distance relationships,\nutilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck, and separate\npoints and edges belonging to different classes to avoid over-mixing problems.\n\u2022 Challenges and Limitations. Though Graph Transformers achieve encouraging perfor\u0002mance, they still face two major challenges. The first challenge is the computational cost of\nthe quadratic attention mechanism and shortest path calculation. These operations require\nsignificant computing resources and can be a bottleneck, particularly for large graphs. The\nsecond is the reliance of Transformer-based models on large amounts of data for stable perfor\u0002mance. It poses a challenge when dealing with problems that lack sufficient data, especially\nfor few-shot and zero-shot settings.\n\u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further\nexplored. Additionally, there are some works using pre-training and fine-tuning frameworks\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n28 W. Ju, et al.\nto balance performance and complexity in downstream tasks [540], this may be a promising\nsolution to address the aforementioned two challenges.\n7 Semi-supervised Learning on Graphs\nWe have investigated various architectures of graph neural networks in which the parameters should\nbe tuned by a learning objective. The most prevalent optimization approach is supervised learning on\ngraph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention\nin the data mining community. In detail, these methods attempt to combine graph representation\nlearning with current semi-supervised techniques including pseudo-labeling, consistency learning,\nknowledge distillation and active learning. These works can be further subdivided into node-level\nrepresentation learning and graph-level representation learning. We would introduce both parts in\ndetail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6.\n7.1 Node Representation Learning\nTypically, node representation learning follows the concept of transductive learning, which has\naccess to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised\nloss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is\nusually adopted for optimization. In formulation,\nL\ud835\udc41 \ud835\udc46\ud835\udc3f = \u2212\n1\n|Y\ud835\udc3f |\n\u2211\ufe01\n\ud835\udc56\u2208Y\ud835\udc3f\ny\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (68)\nwhere Y\ud835\udc3f denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that\ncan be used to offer semantic information. To fully utilize these nodes, a range of methods attempt\nto combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a\nfundamental semi-supervised technique that uses the classifier to produce the label distribution of\nunlabeled examples and then adds appropriately labeled examples to the training set [265, 604].\nAnother line of semi-supervised learning is consistency regularization [247] that requires two\nexamples to have identical predictions under perturbation. This regularization is based on the\nassumption that each instance has a distinct label that is resistant to random perturbations [118, 357].\nThen, we show several representative works in detail.\nCooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-label\u0002based GNN approach for semi-supervised node classification. It employs two GNN classifiers to\njointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows:\n\ud835\udc36\ud835\udc49 (p\ud835\udc56) = p\n\ud835\udc47\n\ud835\udc56\nlog p\ud835\udc56, (69)\nwhere p\ud835\udc56 denotes the output label distribution. Then it selects the pseudo-labels with high confidence\ngenerated from one model to supervise the optimization of the other model. In particular, the\nobjective for unlabeled nodes is written as follows:\nL\ud835\udc36\ud835\udc5c\ud835\udc3a\ud835\udc41 \ud835\udc52\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc56\u2208V\ud835\udc48\n1\ud835\udc36\ud835\udc49 (p\ud835\udc56 )>\ud835\udf0fy\u02c6\n\ud835\udc47\n\ud835\udc56\n\ud835\udc59\ud835\udc5c\ud835\udc54q\ud835\udc56, (70)\nwhere y\u02c6\ud835\udc56 denotes the one-hot formulation of the pseudo-label \ud835\udc66\u02c6\ud835\udc56 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65p\ud835\udc56 and q\ud835\udc56 denotes\nthe label distribution predicted by the other classifier. \ud835\udf0f is a pre-defined temperature coefficient.\nThis cross supervision has been demonstrated effective in [64, 312] to prevent the provision of\nbiased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information\nfrom a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still\ngenerate the same prediction. In this way, CoGNet can illustrate the entire optimization process to\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 29\nTable 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered\nas a specific kind of consistency learning.\nApproach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning\nNode-level\nCoGNet [265] \u2713\nDSGCN [604] \u2713\nGRAND [118] \u2713\nAugGCR [357] \u2713\nHCPL [309] \u2713\nGraph-level\nSEAL [264] \u2713 \u2713\nInfoGraph [431] \u2713 \u2713\nDSGC [527] \u2713\nASGN [166] \u2713 \u2713\nTGNN [218] \u2713\nKGNN [221] \u2713\nHGMI [262] \u2713 \u2713\nASGNN [508] \u2713 \u2713\nDualGraph [310] \u2713 \u2713\nGLA [556] \u2713\nSS [507] \u2713\nenhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in\nsemi-supervised node classification, which can generate dynamics thresholds for reliable nodes.\nDynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive\nmanner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller\nweights to samples with lower confidence with the additional consideration of class balance. The\nweight is formulated as:\n\ud835\udf14\ud835\udc56 =\n1\n\ud835\udc5b\ud835\udc50\n\ud835\udc56\nmax (RELU (p\ud835\udc56 \u2212 \ud835\udefd \u00b7 1)) , (71)\nwhere \ud835\udc5b\ud835\udc50\n\ud835\udc56 denotes the number of unlabeled samples assigned to the class \ud835\udc50\n\ud835\udc56\n. This technique will\ndecrease the impact of wrong pseudo-labels during iterative training.\nGraph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learning\u0002based method. It first adds a variety of perturbations to the input graph to generate a list of\ngraph views. Each graph view \ud835\udc3a\n\ud835\udc5f\nis sent to a GNN classifier to produce a prediction matrix\nP\n\ud835\udc5f = [p\ud835\udc5f\n1\n, \u00b7 \u00b7 \u00b7 , p\n\ud835\udc5f\n\ud835\udc41\n]. Then it summarizes these matrices as:\nP =\n1\n\ud835\udc45\nP\n\ud835\udc5f\n. (72)\nTo provide more discriminative information and ensure that the matrix is row-normalized,\nGRAND sharpens the summarized label matrix into P\n\ud835\udc46\ud835\udc34 as:\nP\n\ud835\udc46\ud835\udc34\n\ud835\udc56\ud835\udc57 =\nP\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\n\u00cd\n\ud835\udc57\n\u2032=0 P\n1/\ud835\udc47\n\ud835\udc56\ud835\udc57\u2032\n, (73)\nwhere \ud835\udc47 is a given temperature parameter. Finally, consistency learning is performed by comparing\nthe sharpened summarized matrix with the matrix of each graph view. Formally, the objective is:\nL\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 =\n1\n\ud835\udc45\n\u2211\ufe01\n\ud835\udc45\n\ud835\udc5f=1\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n||P\n\ud835\udc46\ud835\udc34\n\ud835\udc56 \u2212 P\ud835\udc56\n||, (74)\nhere L\ud835\udc3a\ud835\udc45\ud835\udc34\ud835\udc41 \ud835\udc37 serves as a regularization which is combined with the standard supervised loss.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n30 W. Ju, et al.\nAugmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with\nthe generation of augmented graphs by random dropout and mixup of different order features. To\nenhance the model generalization, it borrows the idea of meta-learning to partition the training data,\nwhich improves the quality of graph augmentation. In addition, it utilizes consistency regularization\nto enhance the semi-supervised node classification.\n7.2 Graph Representation Learning\nThe objective of graph classification is to predict the property of the whole graph example.\nAssuming that the training set comprises \ud835\udc41\n\ud835\udc59\nand \ud835\udc41\n\ud835\udc62 graph samples G\ud835\udc59 = {\ud835\udc3a1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59\n} and\nG\n\ud835\udc62 = {\ud835\udc3a\ud835\udc41\n\ud835\udc59 +1\n, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc41\n\ud835\udc59 +\ud835\udc41\ud835\udc62\n}, the graph-level supervised loss for labeled data can be expressed as\nfollows:\nL\ud835\udc3a\ud835\udc46\ud835\udc3f = \u2212\n1\n|G\ud835\udc62 |\n\u2211\ufe01\n\ud835\udc3a\ud835\udc57 \u2208 G\ud835\udc3f\ny\n\ud835\udc57\ud835\udc47\n\ud835\udc59\ud835\udc5c\ud835\udc54p\n\ud835\udc57\n, (75)\nwhere y\n\ud835\udc57 denotes the one-hot label vector for the \ud835\udc57-th sample while p\ud835\udc57 denotes the predicted\ndistribution of \ud835\udc3a\n\ud835\udc57\n. When \ud835\udc41\n\ud835\udc62 = 0, this objective can be utilized to optimize supervised methods.\nHowever, due to the shortage of labels in graph data, supervised methods cannot reach exceptional\nperformance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph\nclassification has been developed extensively. These approaches can be categorized into pseudo\u0002labeling-based methods, knowledge distillation-based methods and contrastive learning-based\nmethods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph\nexamples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually\nutilize a teacher-student architecture, where the teacher model conducts graph representation\nlearning without label information to extract generalized knowledge while the student model\nfocuses on the downstream task. Due to the restricted number of labeled instances, the student\nmodel transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of\nthis topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning.\nTypically, these methods extract topological information from two perspectives (i.e., different\nperturbation strategies and graph encoders), and maximize the similarity of their representations\ncompared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique\nto improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166,\n508]. Then, we review these methods in detail.\nSEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node\nin a hierarchical graph. It builds two graph classifiers which generate graph representations and\nconduct semi-supervised graph classification respectively. SEAL employs a self-attention module\nto encode each graph into a graph-level representation, and then conducts message passing from a\ngraph level for final classification. SEAL can also be combined with cautious iteration and active\niteration. The former merely utilizes partial graph samples to optimize the parameters in the first\nclassifier due to the potential erroneous pseudo-labels. The second combines active learning with\nthe model, which increases the annotation efficiency in semi-supervised scenarios.\nInfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the\nsimilarity between summarized graph representations and their node representations. In particular,\nit generates node representations using the message passing mechanism and summarizes these\nnode representations into a graph representation. Let \u03a6(\u00b7, \u00b7) denote a discriminator to distinguish\nwhether a node belongs to the graph, and we have:\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 31\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc5d\u210e =\n| G\ud835\udc59|+| G\ud835\udc62 \u2211\ufe01|\n\ud835\udc57=1\n\u2211\ufe01\n\ud835\udc56\u2208 G\ud835\udc57\nh\n\u2212 sp \u0010\u2212\u03a6\n\u0010\nh\n\ud835\udc57\n\ud835\udc56\n, z\n\ud835\udc57\n\u0011 \u0011 i \u2212\n1\n|\ud835\udc41\n\ud835\udc57\n\ud835\udc56\n|\n\u2211\ufe01\n\ud835\udc56\n\u2032\ud835\udc57\n\u2032\n\u2208\ud835\udc41\n\ud835\udc57\n\ud835\udc56\nh\nsp \u0010\u03a6\n\u0010\nh\n\ud835\udc57\n\u2032\n\ud835\udc56\n\u2032\n, z\n\ud835\udc57\n\u0011 \u0011 i , (76)\nwhere sp(\u00b7) denotes the softplus function. \ud835\udc41\n\ud835\udc57\n\ud835\udc56\ndenotes the negative node set where nodes are not\nin \ud835\udc3a\n\ud835\udc57\n. This mutual information maximization formulation is originally developed for unsupervised\nlearning and it can be simply extended for semi-supervised graph classification. In particular,\nInfoGraph utilizes a teacher-student architecture that compares the representation across the\nteacher and student networks. The contrastive learning objective serves as a regularization by\ncombining with supervised loss.\nDual Space Graph Contrastive Learning (DSGC) [527]. DSGC is a representative contrastive\nlearning-based method. It utilizes two graph encoders. The first is a standard GNN encoder in the\nEuclidean space and the second is the hyperbolic GNN encoder. The hyperbolic GNN encoder first\nconverts graph embeddings into hyperbolic space and then measures the distance based on the\nlength of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic\nspace. Assuming the two GNNs are named as \ud835\udc531 (\u00b7) and \ud835\udc532 (\u00b7), the positive pair is denoted as:\nz\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n= exp\ud835\udc50\no\n(\ud835\udc531 (\ud835\udc3a\n\ud835\udc57\n)),\nz\n\ud835\udc57\n\ud835\udc3b\n= exp\ud835\udc50\no\n\ud835\udc532 (\ud835\udc3a\n\ud835\udc57\n)\n\u0001\n.\n(77)\nThen it selects one labeled sample and \ud835\udc41\ud835\udc35 unlabeled sample \ud835\udc3a\n\ud835\udc57\nfor graph contrastive learning in\nthe hyperbolic space. In formulation,\nL\ud835\udc37\ud835\udc46\ud835\udc3a\ud835\udc36 = \u2212 log e\n\ud835\udc51\n\ud835\udc3b\n(h\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f\ne\n\ud835\udc51\ud835\udc3b (z\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b )/\ud835\udf0f +\n\u00cd\ud835\udc41\n\ud835\udc56=1\ne\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc3b\n\u0011\n/\ud835\udf0f\n\u2212\n\ud835\udf06\ud835\udc62\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56=1\nlog e\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\ne\n\ud835\udc51\n\ud835\udc62\nD\n\u0010\nz\n\ud835\udc57\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n+ e\n\ud835\udc51D\n\u0010\nz\n\ud835\udc56\n\ud835\udc3b\n,z\n\ud835\udc57\n\ud835\udc38\u2192\ud835\udc3b\n\u0011\n/\ud835\udf0f\n,\n(78)\nwhere z\n\ud835\udc56\n\ud835\udc38\u2192\ud835\udc3b\nand z\n\ud835\udc56\n\ud835\udc3b\ndenote the embeddings for labeled graph sample\ud835\udc3a\n\ud835\udc56\nand \ud835\udc51\n\ud835\udc3b (\u00b7) denotes a distance\nmetric in the hyperbolic space. This contrastive learning objective maximizes the similarity between\nembeddings learned from two encoders compared with other samples. Finally, the contrastive\nlearning objective can be combined with the supervised loss to achieve effective semi-supervised\ncontrastive learning.\nActive Semi-supervised Graph Neural Network (ASGN) [166]. ASGN utilizes a teacher-student\narchitecture with the teacher model focusing on representation learning and the student model\ntargeting at molecular property prediction. In the teacher model, ASGN first employs a message\npassing neural network to learn node representations under the reconstruction task and then\nborrows the idea of balanced clustering to learn graph-level representations in a self-supervised\nfashion. In the student model, ASGN utilizes label information to monitor the model training based\non the weights of the teacher model. In addition, active learning is also used to minimize the\nannotation cost while maintaining sufficient performance. Typically, the teacher model seeks to\nprovide discriminative graph-level representations without labels, which transfer knowledge to the\nstudent model to overcome the potential overfitting in the presence of label scarcity.\nTwin Graph Neural Networks (TGNN) [218]. TGNN also uses two graph neural networks to\ngive different perspectives to learn graph representations. Differently, it adopts a graph kernel\nneural network to learn graph-level representations in virtue of random walk kernels. Rather than\ndirectly enforcing representation from two modules to be similar, TGNN exchanges information\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n32 W. Ju, et al.\nby contrasting the similarity structure of the two modules. In particular, it constructs a list of\nanchor graphs, \ud835\udc3a\n\ud835\udc4e1\n,\ud835\udc3a\ud835\udc4e2, \u00b7 \u00b7 \u00b7 ,\ud835\udc3a\ud835\udc4e\ud835\udc40 , and utilizes two graph encoders to produce their embeddings,\ni.e., {\ud835\udc67\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n, {\ud835\udc64\n\ud835\udc4e\ud835\udc5a }\n\ud835\udc40\n\ud835\udc5a=1\n. Then it calculates the similarity distribution between each unlabeled\ngraph and anchor graphs for two modules. Formally,\n\ud835\udc5d\n\ud835\udc57\n\ud835\udc5a =\nexp cos \ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (\ud835\udc67\n\ud835\udc57\n, \ud835\udc67\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n, (79)\n\ud835\udc5e\n\ud835\udc57\n\ud835\udc5a =\nexp cos w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\n\u0001\n/\ud835\udf0f\n\u0001\n\u00cd\ud835\udc40\n\ud835\udc5a\u2032=1\nexp (cos (w\ud835\udc57, w\ud835\udc4e\ud835\udc5a\u2032\n) /\ud835\udf0f)\n. (80)\nThen, TGNN minimizes the distance between distributions from different modules as follows:\nL\ud835\udc47\ud835\udc3a\ud835\udc41 \ud835\udc41 =\n1\nG\ud835\udc48\n\u2211\ufe01\n\ud835\udc3a \ud835\udc57 \u2208 G\ud835\udc62\n1\n2\n\ud835\udc37KL\np\n\ud835\udc57\n\u2225q\n\ud835\udc57\n\u0001\n+ \ud835\udc37KL\nq\n\ud835\udc57\n\u2225p\n\ud835\udc57\n\u0001\u0001 , (81)\nwhich serves as a regularization term to combine with the supervised loss.\n7.3 Summary\nThis section introduces semi-supervised learning for graph representation learning and we provide\nthe summary as follows:\n\u2022 Techniques. Classic node classification aims to conduct transductive learning on graphs\nwith access to unlabeled data, which is a natural semi-supervised problem. Semi-supervised\ngraph classification aims to relieve the requirement of abundant labeled graphs. Here, a\nvariety of semi-supervised methods have been put forward to achieve better performance\nunder the label scarcity. Typically, they try to integrate semi-supervised techniques such as\nactive learning, pseudo-labeling, consistency learning, and consistency learning with graph\nrepresentation learning.\n\u2022 Challenges and Limitations. Despite their great success, the performance of these methods\nis still unsatisfactory, especially in graph-level representation learning. For example, DSGC\ncan only achieve an accuracy of 57% in a binary classification dataset REDDIT-BINARY. Even\nworse, label scarcity is often accompanied by unbalanced datasets and potential domain\nshifts, which provides more challenges from real-world applications.\n\u2022 Future Works. In the future, we expect that these methods can be applied to different\nproblems such as molecular property predictions. There are also works to extend graph\nrepresentation learning in more realistic scenarios like few-shot learning [51, 326]. A higher\naccuracy is always anticipated for more advanced and effective semi-supervised techniques.\n8 Graph Self-supervised Learning\nBesides supervised or semi-supervised methods, self-supervised learning (SSL) also has shown its\npowerful capability in data mining and representation embedding in recent years. In this section,\nwe investigated Graph Neural Networks based on SSL and provided a detailed introduction to a\nfew typical models. Graph SSL methods usually have a unified pipeline, which includes pretext\ntasks and downstream tasks. Pretext tasks help the model encoder to learn better representation,\nas a premise of better performance in downstream tasks. So a delicate design of pretext task is\ncrucial for Graph SSL. We would firstly introduce the overall framework of Graph SSL in Section 8.1,\nthen introduce the two kinds of pretext task design, generation-based methods and contrast-based\nmethods respectively in Section 8.2 and 8.3. A summarisation is provided in Table 7.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 33\nTable 7. Summary of methods for self-supervised Learning on Graphs. \"PT\", \"CT\" and \"UFE\" mean \"Pre\u0002training\", \"Collaborative Train\" and \"Unsupervised Feature Extracting\" respectively.\nApproach Augmentation Scheme Training Scheme Generation Target Objective Function\nGeneration-based\nGraph Completion [544] Feature Mask PT/CT Node Feature -\nAttributeMask [208] Feature Mask PT/CT PCA Node Feature -\nAttrMasking [181] Feature Mask PT Node/Edge Feature -\nMGAE [459] No Augmentation CT Node Feature -\nGAE [231] Feature Noise UFE Adjacency Matrix -\nContrast-based\nDeepWalk [362] Random Walk UFE - SkipGram\nLINE [443] Random Walk UFE - Jensen-Shannon\nGCC [375] Random Walk PT/URL - InfoNCE\nSimGCL [547] Embedding Noise UFE - InfoNCE\nSimGRACE [503] Model Noise UFE - InfoNCE\nGCA [612]\nFeature Masking &\nStructure Adjustment URL - InfoNCE\nBGRL [152]\nFeature Masking &\nStructure Adjustment URL - BYOL\n8.1 Overall framework\nConsider a featured graph G, we denote a graph encoder \ud835\udc53 to learn the representation of the\ngraph, and a pretext decoder \ud835\udc54 with specific architecture in different pretext tasks. Then the pretext\nself-supervised learning loss can be formulated as:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 = \ud835\udc38G\u223cD [L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G)], (82)\nwhere D denotes the distribution of featured graph G. By minimizing L\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc59 , we can learn encoder\n\ud835\udc53 with capacity to produce high-quality embedding. As for downstream tasks, we denote a graph\ndecoder \ud835\udc51 which transforms the output of graph encoder \ud835\udc53 into model prediction. The loss of\ndownstream tasks can be formulated as:\nL\ud835\udc60\ud835\udc62\ud835\udc5d = L\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66), (83)\nwhere \ud835\udc66 is the ground truth in downstream tasks. We can obverse that L\ud835\udc60\ud835\udc62\ud835\udc5d is a typical supervised\nloss. To ensure the model achieves wise graph representation extraction and optimistic prediction\nperformance, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d have to be minimized simultaneously. We introduce 3 different ways\nto minimize the two loss functions:\nPre-training. This strategy has two steps. In pre-training step, the L\ud835\udc60\ud835\udc60\ud835\udc59 is minimized to get \ud835\udc54\n\u2217\nand \ud835\udc53\n\u2217\n:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D). (84)\nThen the parameter of \ud835\udc53\n\u2217\nis kept to continue training in pretext supervised learning progress.\nThe supervised loss is minimized to get the final parameters of \ud835\udc53 and \ud835\udc51.\nmin\n\ud835\udc51,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc51, \ud835\udc53 |\ud835\udc530=\ud835\udc53\n\u2217 , G;\ud835\udc66). (85)\nCollaborative Train. In this strategy, L\ud835\udc60\ud835\udc60\ud835\udc59 and L\ud835\udc60\ud835\udc62\ud835\udc5d are optimized simultaneously. A hyper\u0002parameter \ud835\udefc is used to balance the contribution of pretext task loss and downstream task loss.\nThe overall minimization strategy is like the traditional supervised strategy with a pretext task\nregularization:\nmin\n\ud835\udc54,\ud835\udc53 ,\ud835\udc51\n[L\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , G) + \ud835\udefcL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 , G;\ud835\udc66)]. (86)\nUnsupervised Feature Extracting. This strategy is similar to the Pre-training and Fine-tuning\nstrategy in the first step to minimize pretext task loss L\ud835\udc60\ud835\udc60\ud835\udc59 and get \ud835\udc53\n\u2217\n. However, when minimizing\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n34 W. Ju, et al.\ndownstream loss L\ud835\udc60\ud835\udc62\ud835\udc5d , the encoder \ud835\udc53\n\u2217\nis fixed. Also, the training graph data are on the same dataset,\nwhich differs from the Pre-training and Fine-tuning strategy. The formulation is defined as:\n\ud835\udc54\n\u2217\n, \ud835\udc53 \u2217 = arg min\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54, \ud835\udc53 , D), (87)\nmin\n\ud835\udc51\nL\ud835\udc60\ud835\udc62\ud835\udc5d (\ud835\udc51, \ud835\udc53 \u2217, G;\ud835\udc66). (88)\n8.2 Generation-based pretext task design\nIf a model with an encoder-decoder structure can reproduce certain graph features from an in\u0002complete or perturbed graph, it indicates the encoder has the ability to extract useful graph\nrepresentation. This motivation is derived from Autoencoder [174] which originally learns on\nimage dataset. In such a case, Eq. 84 can be rewritten as:\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54(\ud835\udc53 (G)) \u02c6 , G), (89)\nwhere \ud835\udc53 (\u00b7) and \ud835\udc54(\u00b7) stand for the representation encoder and rebuilding decoder. However, feature\ninformation and structure information are both important compositions suitable to be rebuilt for\ngraph datasets. So generation-based pretext can be divided into two categories: feature rebuilding\nand structure rebuilding. We introduce several outstanding models in the following part.\nGraph Completion [544] is one of the representative methods of feature rebuilding. They mask\nsome node features to generate an incomplete graph. Then the pretext task is set as predicting the\nremoved node features. As shown in Eq. 90, this method can be formulated as a special case of\nEq. 90, letting G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and replacing G \u2212\u2192 \ud835\udc4b. The loss function is often Mean Squared Error or\nCross Entropy, depending on whether the feature is continuous or binary.\nmin\n\ud835\udc54,\ud835\udc53\nMSE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , X). (90)\nOther works make some changes to the feature settings. For example, AttrMasking [181] aims\nto rebuild both node representation and edge representation, AttributeMask [208] preprocess \ud835\udc4b\nfirstly by PCA to reduce the complexity of rebuilding features.\nOn the other hand, MGAE [459] modifies the original graph by adding noise in node representa\u0002tion, motivated by denoising autoencoder [454]. As shown in Eq. 90, we can also consider MGAE as\nan implement of Eq. 84 where G\u02c6 = (\ud835\udc34,\ud835\udc4b\u02c6) and G \u2212\u2192 \ud835\udc4b. \ud835\udc4b\u02c6 stands for perturbed node representation.\nSince the noise is independent and random, the encoder is more robust to feature input.\nmin\n\ud835\udc54,\ud835\udc53\nBCE(\ud835\udc54(\ud835\udc53 (G)) \u02c6 , A). (91)\nAs for structure rebuilding methods, GAE [231] is the simplest instance, which can be regarded as\nan implement of Eq. 84 where G\u02c6 = G and G \u2212\u2192 \ud835\udc34. \ud835\udc34 is the adjacency matrix of the graph. Similar to\nfeature rebuilding methods, GAE compresses raw node representation vectors into low-dimensional\nembedding with its encoder. Then the adjacency matrix is rebuilt by computing node embedding\nsimilarity. The loss function is set to the error between the ground-truth adjacency matrix and\nthe recovered one, to help the model rebuild the correct graph structure. Other feature rebuilding\nmethods [558] and structure rebuilding methods [440, 487] are also increasingly being developed\nacross numerous related publications.\n8.3 Contrast-Based pretext task design\nThe mutual information maximization principle, which implements self-supervising by predicting\nthe similarity between the two augmented views, forms the foundation of contrast-based approaches.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 35\nSince mutual information represents the degree of correlation between two samples, we can\nmaximize it in augmented pairs and minimize it in random-selected pairs.\nThe contrast-based graph SSL taxonomy can be formulated as Eq. 92. The discriminator that\ncalculates the similarity of sample pairs is indicated by pretext decoder \ud835\udc54. G\n(1)\nand G\n(2)\nare two\nvariants of \ud835\udc3a that have been augmented. Since graph contrastive learning methods differ from each\nother in 1) view generation, 2) MI estimation method we introduce this methodology in these two\nperspectives.\nmin\n\ud835\udc54,\ud835\udc53\nL\ud835\udc60\ud835\udc60\ud835\udc59 (\ud835\udc54[\ud835\udc53 (G\u02c6(1)), \ud835\udc53 (G\u02c6(2))]). (92)\nThe domain of contrastive-based graph SSL is witnessing an expanding body of work in a\ngrowing number of methods [176, 197, 531, 587] and applications [114, 136, 504].\n8.3.1 View generation. The traditional pipeline of contrastive learning-based models first involves\naugmenting the graph using well-crafted empirical methods, and then maximizing the consistency\nbetween different augmentations. Drawing from methods in the computer vision domain and\nconsidering the non-Euclidean structure of graph data, typical graph augmentation methods aim\nto modify the graph topologically or representationally.\nGiven graph G = (\ud835\udc34, \ud835\udc4b), the topologically augmentation methods usually modify the adjacency\nmatrix \ud835\udc34, which can be formulated as:\n\ud835\udc34\u02c6 = \ud835\udcaf\ud835\udc34 (\ud835\udc34), (93)\nwhere \ud835\udcaf\ud835\udc34 (\u00b7) is the transform function of adjacency matrix. Topology augmentation methods\nhave many variants, in which the most popular one is edge modification, given by \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\ud835\udc43 \u25e6 \ud835\udc34 + \ud835\udc44 \u25e6 (1 \u2212 \ud835\udc34). \ud835\udc43 and \ud835\udc44 are two matrices representing edge dropping and adding respectively.\nAnother method, graph diffusion, connect nodes with their k-hop neighbors with specific weight,\ndefined as: \ud835\udcaf\ud835\udc34 (\ud835\udc34) =\n\u00cd\u221e\n\ud835\udc58=0\n\ud835\udefc\ud835\udc58\ud835\udc47\n\ud835\udc58\n. where \ud835\udefc and\ud835\udc47 are coefficient and transition matrix. Graph diffusion\nmethod can integrate broad topological information with local structure.\nOn the other hand, the representative augmentation modifies the node representation directly,\nwhich can be formulated as:\n\ud835\udc4b\u02c6 = \ud835\udcaf\ud835\udc4b (\ud835\udc4b), (94)\nusually \ud835\udcaf\ud835\udc4b (\u00b7) can be a simple masking operater, a.k.a. \ud835\udcaf\ud835\udc4b (\ud835\udc4b) = \ud835\udc40 \u25e6 \ud835\udc4b and \ud835\udc40 \u2208 {0, 1}\n\ud835\udc41 \u00d7\ud835\udc37 . Based\non such mask strategy, some methods propose ways to improve performance. GCA [612] preserves\ncritical nodes while giving less significant nodes a larger masking probability, where significance is\ndetermined by node centrality.\nAs introduced before, the paradigm of augmentation has been proven to be effective in contrastive\nlearning view generation. However, given the variety of graph data, it is challenging to maintain\nsemantics properly during augmentations. To preserve the valuable nature of specific graph datasets,\nThere are currently three mainly used methods: picking by trial-and-errors, trying laborious search,\nor seeking domain-specific information as guidance [214, 308, 311]. Such complicated augmentation\nmethods constrain the effectiveness and widespread application of graph contrastive learning. So\nmany newest works question the necessity of augmentation and seek other contrastive view\ngeneration methods.\nSimGCL [547] is one of the outstanding works challenging the effectiveness of graph augmenta\u0002tion. The author finds that noise can be a substitution to augmentation to produce graph views\nin specific tasks such as recommendation. After doing an ablation study about augmentation and\nInfoNCE [510], they find that the InfoNCE loss, not the augmentation of the graph, is what makes\nthe difference. It can be further explained by the importance of distribution uniformity. Contrastive\nlearning enhances model representation ability by intensifying two characteristics: The alignment\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n36 W. Ju, et al.\nof features from positive samples and the uniformity of the normalized feature distribution. SimGCL\ndirectly adds random noises to node embeddings as augmentation, to control the uniformity of the\nrepresentation distribution more effectively:\ne\n(1)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(1)\n\u2217 \ud835\udf0f\n(1)\n\ud835\udc56\n, e\n(2)\n\ud835\udc56\n= e\ud835\udc56 + \ud835\udf16\n(2)\n\u2217 \ud835\udf0f\n(2)\n\ud835\udc56\n,\n\ud835\udf16 \u223c N (0, \ud835\udf0e2),\n(95)\nwhere e\ud835\udc56is a node representation in embedding space, \ud835\udf0f\n(1)\n\ud835\udc56\nand \ud835\udf0f\n(2)\n\ud835\udc56\nare two random sampled unit\nvector. The experiment results indicate that SimGCL performs better than its graph augmentation\u0002based competitors, while training time is significantly decreased.\nSimGRACE [503] is another graph contrastive learning framework without data augmentation.\nMotivated by the observation that despite encoder disruption, graph data can effectively maintain\ntheir semantics, SimGRACE takes GNN with its modified version as an encoder to produce two\ncontrastive embedding views by the same graph input. For GNN encoder \ud835\udc53 (\u00b7; \ud835\udf03), the two contrastive\nembedding views e, e\n\u2032\ncan be computed by:\ne\n(1) = \ud835\udc53 (G; \ud835\udf03), e(2) = \ud835\udc53 (G; \ud835\udf03 + \ud835\udf16 \u00b7 \u0394\ud835\udf03),\n\u0394\ud835\udf03\ud835\udc59 \u223c N (0, \ud835\udf0e2\n\ud835\udc59\n),\n(96)\nwhere \u0394\ud835\udf03\ud835\udc59 represents GNN parameter perturbation \u0394\ud835\udf03 in the \ud835\udc59th layer. SimGRACE can improve\nalignment and uniformity simultaneously, proving its capacity to produce high-quality embedding.\n8.3.2 MI estimation method. The mutual information \ud835\udc3c(\ud835\udc65, \ud835\udc66) measures the information that x and y\nshare, given a pair of random variables (\ud835\udc65, \ud835\udc66). As discussed before, mutual information is a significant\ncomponent of the contrast-based method by formulating the loss function. Mathematically rigorous\nMI is defined on the probability space, we can formulate mutual information between a pair of\ninstances (\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) as:\n\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \ud835\udc37\ud835\udc3e\ud835\udc3f (\ud835\udc5d(\ud835\udc65, \ud835\udc66)||\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66))\n= \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc5d(\ud835\udc65, \ud835\udc66)\n\ud835\udc5d(\ud835\udc65)\ud835\udc5d(\ud835\udc66)\n].\n(97)\nHowever, directly computing Eq. 97 is quite difficult, so we introduce several different types of\nestimation for MI:\nInfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimator. Given a\npositive sample \ud835\udc66 and several negative sample \ud835\udc66\n\u2032\n\ud835\udc56\n, a noise-contrastive estimator can be formulated\nas [611][375]:\nL = \u2212\ud835\udc3c(\ud835\udc65, \ud835\udc66) = \u2212\ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [log \ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66)\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66) +\n\u00cd\n\ud835\udc56\n\ud835\udc52\n\ud835\udc54(\ud835\udc65,\ud835\udc66\u2032\n\ud835\udc56\n)\n], (98)\nusually the kernal function \ud835\udc54(\u00b7) can be cosine similarity or dot product.\nTriplet Loss. Intuitively, we can aim to create a distinct separation in the degree of similarity,\nensuring that positive samples are closer together and negative samples are further apart by a\ncertain distance. So we can define the loss function in the following manner [204]:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [max(\ud835\udc54(\ud835\udc65, \ud835\udc66) \u2212 \ud835\udc54(\ud835\udc65, \ud835\udc66\u2032) + \ud835\udf16, 0)], (99)\nwhere \ud835\udf16 is a hyperparameter. This function is straightforward to compute.\nBYOL Loss. Estimation without negative samples is investigated by BYOL [152]. The estimator\nis Asymmetrically structured:\nL = \ud835\udc38\ud835\udc5d (\ud835\udc65,\ud835\udc66) [2 \u2212 2\n\ud835\udc54(\ud835\udc65) \u00b7 \ud835\udc66\n\u2225\ud835\udc54(\ud835\udc65) \u2225 \u2225\ud835\udc66\u2225\n], (100)\nnote that encoder \ud835\udc54 should keep the dimension of input and output the same.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 37\n8.4 Summary\nThis section introduces graph self-supervised learning and we provide the summary as follows:\n\u2022 Techniques. Differing from classic supervised and semi-supervised learning, self-supervised\nlearning increases a model\u2019s generalization ability and robustness while decreasing reliance\non labels. Graph SSL utilizes pretext tasks to extract inherent information from representation\ndistributions. Typical Graph SSL methods can be divided into generation-based and contrast\u0002based approaches. Generation-based methods learn an encoder with the ability to reconstruct\na graph as precisely as possible, motivated by the principles of Autoencoder. Contrast-based\nmethods, which have recently attracted significant interest, involve learning an encoder to\nminimize mutual information between relevant instances and maximize mutual information\nbetween unrelated instances.\n\u2022 Challenges and Limitations. Although graph SSL has achieved superior performance in\nmany tasks, its theoretical basis is not so solid. Many well-known methods are validated only\nthrough experiments, without providing theoretical explanations or mathematical proofs. It\nis imperative to establish a strong theoretical foundation for graph SSL.\n\u2022 Future Works. In the future we expect more graph ssl methods designed essentially by\ntheoretical proof, without dedicated designed augment process or pretext tasks by intuition.\nThis will bring us more definite mathematical properties and a less ambiguous empirical\nsense. Also, graphs are a prevalent form of data representation across diverse domains, yet\nobtaining manual labels can be prohibitively expensive. Expanding the applications of graph\nSSL to broader fields is a promising avenue for future research.\n9 Graph Structure Learning\nGraph structure determines how node features propagate and affect each other, playing a crucial\nrole in graph representation learning. In some scenarios the provided graph is incomplete, noisy, or\neven has no structure information at all. Recent research also finds that graph adversarial attacks\n(i.e., modifying a small number of node features or edges), can degrade learned representations\nsignificantly. These issues motivate graph structure learning (GSL), which aims to learn a new\ngraph structure to produce optimal graph representations. According to how edge connectivity is\nmodeled, there are three different approaches in GSL, namely metric-based approaches, model-based\napproaches, and direct approaches. Besides edge modeling, regularization is also a common trick to\nmake the learned graph satisfy some desired properties. We first present the basic framework and\nregularization methods for GSL in Sec. 9.1 and Sec. 9.2, respectively, and then introduce different\ncategories of GSL in Sec. 9.3, 9.4 and 9.5. We summarize GSL approaches in Table 8.\n9.1 Overall Framework\nWe denote a graph by G = (A, X), where A \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is the adjacency matrix and X \u2208 R\ud835\udc41 \u00d7\ud835\udc40 is\nthe node feature matrix with \ud835\udc40 being the dimension of each node feature. A graph encoder \ud835\udc53\ud835\udf03\nlearns to represent the graph based on node features and graph structure for task-specific objective\nL\ud835\udc61 (\ud835\udc53\ud835\udf03 (A, X)). In the GSL setting, there is also a graph structure learner which aims to build a\nnew graph adjacency matrix A\n\u2217\nto optimize the learned representation. Besides the task-specific\nobjective, a regularization term can be added to constrain the learned structure. So the overall\nobjective function of GSL can be formulated as\nmin\n\ud835\udf03,A\u2217\nL = L\ud835\udc61 (\ud835\udc53\ud835\udf03 (A\n\u2217\n, X)) + \ud835\udf06L\ud835\udc5f (A\n\u2217\n, A, X), (101)\nwhere L\ud835\udc61is the task-specific objective, L\ud835\udc5fis the regularization term and \ud835\udf06 is a hyperparameter for\nthe weight of regularization.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n38 W. Ju, et al.\nTable 8. Summary of graph structure learning methods.\nMethod Structure Learning Regularization\nSparsity Low-rank Smoothness\nMetric-based\nAGCN [267] Mahalanobis distance\nGRCN [546] Inner product \u2713\nCAGCN [613] Inner product \u2713\nGNNGUARD [571] Cosine similarity\nIDGL [65] Cosine similarity \u2713 \u2713 \u2713\nHGSL [586] Cosine similarity \u2713\nGDC [139] Graph diffusion \u2713\nModel-based\nGLN [364] Recurrent blocks\nGLCN [199] One-layer neural network \u2713 \u2713\nNeuralSparse [595] Multi-layer neural network \u2713\nGAT [452] Self-attention\nGaAN [566] Gated attention\nhGAO [132] Hard attention \u2713\nVIB-GSL [433] Dot-product attention \u2713\nMAGNA [461] Graph attention diffusion\nDirect\nGLNN [135] MAP estimation \u2713 \u2713\nPro-GNN [210] Direct optimization \u2713 \u2713 \u2713\nGSML [458] Bilevel optimization \u2713\nLSD-GNN [124] Bilevel optimization\nBGCNN [573] Bayesion optimization\nVGCN [104] Stochastic variational inference\n9.2 Regularization\nThe goal of regularization is to constrain the learned graph to satisfy some properties by adding\nsome penalties to the learned structure. The most common properties used in GSL are sparsity, low\nlank, and smoothness.\n9.2.1 Sparsity Noise or adversarial attacks will introduce redundant edges into graphs and degrade\nthe quality of graph representation. An effective technique to remove unnecessary edges is sparsity\nregularization, i.e., adding a penalty on the number of nonzero entries of the adjacency matrix\n(\u21130-norm) [458, 546, 586, 595]:\nL\ud835\udc60\ud835\udc5d = \u2225A\u22250, (102)\nhowever, \u21130-norm is not differentiable so optimizing it is difficult, and in many cases \u21131-norm\nis used instead as a convex relaxation. Other methods to impose sparsity include pruning and\ndiscretization [124, 613]. These processes are also called postprocessing since they usually happen\nafter the adjacency matrix is learned. Pruning removes part of the edges according to some crite\u0002ria [613]. For example, edges with weights lower than a threshold, or those not in the top-K edges\nof nodes or graphs. Discretization is applied to generate graph structure by sampling from some\ndistribution [124]. Compared to directly learning edge weights, sampling enjoys the advantage\nof controlling the generated graph, but has issues during optimizing since sampling itself is dis\u0002crete and hard to optimize. Reparameterization and Gumbel-softmax are two useful techniques to\novercome such issues, and are widely adopted in GSL.\n9.2.2 Low Rank In real-world graphs, similar nodes are likely to group together and form commu\u0002nities, which should lead to a low-rank adjacency matrix. Recent work also finds that adversarial\nattacks tend to increase the rank of the adjacency matrix quickly [65, 210]. Therefore, low-rank\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 39\nregularization is also a useful tool to make graph representation learning more robust:\nL\ud835\udc59\ud835\udc5f = \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc58 (A). (103)\nIt is hard to minimize matrix rank directly. A common technique is to optimize the nuclear norm,\nwhich is a convex envelope of the matrix rank:\nL\ud835\udc5b\ud835\udc50 = \u2225A\u2225\u2217 =\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56\n\ud835\udf0e\ud835\udc56, (104)\nwhere \ud835\udf0e\ud835\udc56 are singular values of A. Entezari et al. replaces the learned adjacency matrix with rank-r\napproximation by singular value decomposition (SVD) to achieve robust graph learning against\nadversarial attacks.\n9.2.3 Smoothness A common assumption is that connected nodes share similar features, or in other\nwords, the graph is \u201csmooth\u201d as the difference between local neighbors is small [65, 135, 199, 210].\nThe following metric is a natural way to measure graph smoothness:\nL\ud835\udc60\ud835\udc5a =\n1\n2\n\u2211\ufe01\n\ud835\udc41\n\ud835\udc56,\ud835\udc57=1\n\ud835\udc34\ud835\udc56\ud835\udc57 (\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n2 = \ud835\udc61\ud835\udc5f(X\u22a4\n(D \u2212 A)X) = \ud835\udc61\ud835\udc5f(X\n\u22a4\nLX), (105)\nwhere D is the degree matrix of A and L = D \u2212 A is called graph Laplacian. A variant is to use the\nnormalized graph Laplacian bL = D\n\u2212\n1\n2 LD\u2212\n1\n2 .\n9.3 Metric-based Methods\nMetric-based methods measure the similarity between nodes as the edge weights. They follow\nthe basic assumption that similar nodes tend to have connections with each other. We show some\nrepresentative works\nAdaptive Graph Convolutional Neural Networks (AGCN) [267]. AGCN learns a task-driven adaptive\ngraph during training to enable a more generalized and flexible graph representation model. After\nparameterizing the distance metric between nodes, AGCN is able to adapt graph topology to the\ngiven task. It proposes a generalized Mahalanobis distance between two nodes with the following\nformula:\nD(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57) =\n\u221a\ufe03\n(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57)\n\u22a4\ud835\udc40(\ud835\udc65\ud835\udc56 \u2212 \ud835\udc65\ud835\udc57), (106)\nwhere \ud835\udc40 = \ud835\udc4a\ud835\udc51\ud835\udc4a \u22a4\n\ud835\udc51\nand \ud835\udc4a\ud835\udc51 is the trainable weights to minimize task-specific objective. Then the\nGaussian kernel is used to obtain the adjacency matrix:\nG\ud835\udc56\ud835\udc57 = exp(\u2212D(\ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc57)/(2\ud835\udf0e\n2\n)), (107)\n\ud835\udc34\u02c6 = \ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc67\ud835\udc52 (G). (108)\nGraph-Revised Convolutional Network (GRCN) [546]. GRCN uses a graph revision module to\npredict missing edges and revise edge weights through joint optimization on downstream tasks. It\nfirst learns the node embedding with GCN and then calculates pair-wise node similarity with the\ndot product as the kernel function.\n\ud835\udc4d = \ud835\udc3a\ud835\udc36\ud835\udc41\ud835\udc54 (\ud835\udc34, \ud835\udc4b), (109)\n\ud835\udc46\ud835\udc56\ud835\udc57 =\n\ud835\udc67\ud835\udc56, \ud835\udc67\ud835\udc57\n. (110)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n40 W. Ju, et al.\nThe revised adjacency matrix is the residual summation of the original adjacency matrix\ud835\udc34\u02c6 = \ud835\udc34+\ud835\udc46.\nGRCN also applies a sparsification technique on the similarity matrix \ud835\udc46 to reduce computation cost:\n\ud835\udc46\n(\ud835\udc3e)\n\ud835\udc56\ud835\udc57 =\n\u001a\n\ud835\udc46\ud835\udc56\ud835\udc57, \ud835\udc46\ud835\udc56\ud835\udc57 \u2208 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n0, \ud835\udc46\ud835\udc56\ud835\udc57 \u2209 \ud835\udc61\ud835\udc5c\ud835\udc5d\ud835\udc3e(\ud835\udc46\ud835\udc56)\n. (111)\nThreshold pruning is also a common strategy for sparsification. For example, CAGCN [613] uses\ndot product to measure node similarity, and refines the graph structure by removing edges between\nnodes whose similarity is less than a threshold \ud835\udf0f\ud835\udc5f and adding edges between nodes whose similarity\nis greater than another threshold \ud835\udf0f\ud835\udc4e.\nDefending Graph Neural Networks against Adversarial Attacks (GNNGuard) [571]. GNNGuard\nmeasures similarity between a node \ud835\udc62 and its neighbor \ud835\udc63 in the \ud835\udc58-th layer by cosine similarity and\nnormalizes node similarity at the node level within the neighborhood as follows:\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\u210e\n\ud835\udc58\n\ud835\udc62 \u2299 \u210e\n\ud835\udc58\n\ud835\udc63\n\u2225\u210e\n\ud835\udc58\n\ud835\udc62 \u22252 \u2225\u210e\n\ud835\udc58\n\ud835\udc63 \u22252\n, (112)\n\ud835\udefc\n\ud835\udc58\n\ud835\udc62\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63/\n\u2211\ufe01\n\ud835\udc63\u2208N\ud835\udc62\n\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u00d7 \ud835\udc41\u02c6 \ud835\udc58\ud835\udc62\n/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 \u2260 \ud835\udc63\n1/(\ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 + 1), \ud835\udc56 \ud835\udc53 \ud835\udc62 = \ud835\udc63\n, (113)\nwhere N\ud835\udc62 denotes the neighborhood of node \ud835\udc62 and \ud835\udc41\u02c6 \ud835\udc58\n\ud835\udc62 =\n\u00cd\n\ud835\udc63\u2208N\ud835\udc62\n\u2225\ud835\udc60\n\ud835\udc58\n\ud835\udc62\ud835\udc63 \u22250. To stabilize GNN training,\nit also proposes a layer-wise graph memory by keeping part of the information from the previous\nlayer in the current layer. Similar to GNNGuard, IDGL [65] uses multi-head cosine similarity and\nmask edges with node similarity smaller than a non-negative threshold, and HGSL [586] generalizes\nthis idea to heterogeneous graphs.\nGraph Diffusion Convolution (GDC) [139]. GDC replaces the original adjacency matrix with\ngeneralized graph diffusion matrix S:\nS =\n\u2211\ufe01\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58T\n\ud835\udc58\n, (114)\nwhere \ud835\udf03\ud835\udc58 is the weighting coefficient and T is the generalized transition matrix. To ensure conver\u0002gence, GDC further requires that \u00cd\u221e\n\ud835\udc58=0\n\ud835\udf03\ud835\udc58 = 1 and the eigenvalues of T lie in [0, 1]. The random\nwalk transition matrix T\ud835\udc5f \ud835\udc64 = AD\u22121and the symmetric transition matrix T\ud835\udc60\ud835\udc66\ud835\udc5a = D\n\u22121/2AD\u22121/2\nare\ntwo examples. This new graph structure allows graph convolution to aggregate information from a\nlarger neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying noise.\nHowever, in most cases graph diffusion will result in a dense adjacency matrix \ud835\udc46, so sparsification\ntechnology like top-k filtering and threshold filtering will be applied to graph diffusion. Following\nGDC, there are some other graph diffusion proposed. For example, AdaCAD [281] proposes Class\u0002Attentive Diffusion, which further considers node features and aggregates nodes probably of the\nsame class among K-hop neighbors. Adaptive diffusion convolution (ADC) [585] learns the optimal\nneighborhood size via optimizing a bi-level problem.\n9.4 Model-based Methods\nModel-based methods parameterize edge weights with more complex models like deep neural\nnetworks. Compared to metric-based methods, model-based methods offer greater flexibility and\nexpressive power.\nGraph Learning Network (GLN) [364]. GLN proposes a recurrent block to first produce interme\u0002diate node embeddings and then merge them with adjacency information as the output of this\nlayer to predict the adjacency matrix for the next layer. Specifically, it uses convolutional graph\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 41\noperations to extract node features, and creates a local-context embedding based on node features\nand the current adjacency matrix:\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61 =\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc56=1\n\ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\n), (115)\n\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59 = \ud835\udf0e\ud835\udc59 (\ud835\udf0f (\ud835\udc34\n(\ud835\udc59)\n)\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc48\n(\ud835\udc59)\n), (116)\nwhere \ud835\udc4a\n(\ud835\udc59)\n\ud835\udc56\nand \ud835\udc48\n(\ud835\udc59)\nare the learnable weights. GLN then predicts the next adjacency matrix as\nfollows:\n\ud835\udc34\n(\ud835\udc59+1) = \ud835\udf0e\ud835\udc59 (\ud835\udc40(\ud835\udc59)\n\ud835\udefc\ud835\udc59 (\ud835\udc3b\n(\ud835\udc59)\n\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc59)\ud835\udc40\n(\ud835\udc59) \u22a4\n). (117)\nSimilarly, GLCN [199] models graph structure with a softmax layer over the inner product\nbetween the difference of node features and a learnable vector. NeuralSparse [595] uses a multi\u0002layer neural network to generate a learnable distribution from which a sparse graph structure is\nsampled. PTDNet [305] prunes graph edges with a multi-layer neural network and penalizes the\nnumber of non-zero elements to encourage sparsity.\nGraph Attention Networks (GAT) [452]. Besides constructing a new graph to guide the message\npassing and aggregation process of GNNs, many recent researchers also leverage the attention\nmechanism to adaptively model the relationship between nodes. GAT is the first work to introduce\nthe self-attention strategy into graph learning. In each attention layer, the attention weight between\ntwo nodes is calculated as the Softmax output on the combination of linear and non-linear transform\nof node features:\n\ud835\udc52\ud835\udc56\ud835\udc57 = \ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57), (118)\n\ud835\udefc\ud835\udc56\ud835\udc57 =\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc57)\n\u00cd\n\ud835\udc58 \u2208N\ud835\udc56\n\ud835\udc52\ud835\udc65\ud835\udc5d(\ud835\udc52\ud835\udc56\ud835\udc58 )\n, (119)\nwhere N\ud835\udc56 denotes the neighborhood of node \ud835\udc56,W is learnable linear transform and \ud835\udc4e is pre-defined\nattention function. In the original implementation of GAT, \ud835\udc4e is a single-layer neural network with\nLeakyReLU:\n\ud835\udc4e(W\u00ae\u210e\ud835\udc56, W\u00ae\u210e\ud835\udc57) = LeakyReLU(\u00aea\n\u22a4\n[W\u00ae\u210e\ud835\udc56||W\u00ae\u210e\ud835\udc57]). (120)\nThe attention weights are then used to guide the message-passing phase of GNNs:\n\u00ae\u210e\n\u2032\n\ud835\udc56 = \ud835\udf0e(\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n\ud835\udefc\ud835\udc56\ud835\udc57W\u00ae\u210e\ud835\udc57), (121)\nwhere \ud835\udf0e is a nonlinear function. It is beneficial to concatenate multiple heads of attention to\u0002gether to get a more stable and generalizable model, so-called multi-head attention. The attention\nmechanism serves as a soft graph structure learner which captures important connections within\nnode neighborhoods. Following GAT, many recent works propose more effective and efficient\ngraph attention operators to improve performance. GaAN [566] adds a soft gate at each attention\nhead to adjust its importance. MAGNA [461] proposes a novel graph attention diffusion layer to\nincorporate multi-hop information. One drawback of graph attention is that the time and space\ncomplexities are both \ud835\udc42(\ud835\udc41\n3\n). hGAO [132] performs hard graph attention by limiting node attention\nto its neighborhood. VIB-GSL [433] adopts the information bottleneck principle to guide feature\nmasking in order to drop task-irrelevant information and preserve actionable information for the\ndownstream task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n42 W. Ju, et al.\n9.5 Direct Methods\nDirect methods treat edge weights as free learnable parameters. These methods enjoy more flexibility\nbut are also more difficult to train. The optimization is usually carried out in an alternating way,\ni.e., iteratively updating the adjacency matrix A and the GNN encoder parameters \ud835\udf03.\nGLNN [135]. GLNN uses MAP estimation to learn an optimal adjacency matrix for a joint\nobjective function including sparsity and smoothness. Specifically, it targets at finding the most\nprobable adjacency matrix \ud835\udc34\u02c6 given graph node features \ud835\udc65:\n\ud835\udc34\u02dc\ud835\udc40\ud835\udc34\ud835\udc43 (\ud835\udc65) = argmax\n\ud835\udc34\u02c6\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6)\ud835\udc54(\ud835\udc34\u02c6), (122)\nwhere \ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) measures the likelihood of observing \ud835\udc65 given \ud835\udc34\u02c6, and \ud835\udc54(\ud835\udc34\u02c6) is the prior distribution of\n\ud835\udc34\u02c6. GLNN uses sparsity and property constraint as prior, and defines the likelihood function \ud835\udc53 as:\n\ud835\udc53 (\ud835\udc65 |\ud835\udc34\u02c6) = \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n\ud835\udc3f\ud835\udc65) (123)\n= \ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udf060\ud835\udc65\n\u22a4\n(\ud835\udc3c \u2212 \ud835\udc34\u02c6)\ud835\udc65), (124)\nwhere \ud835\udf060 is a parameter. This likelihood imposed a smoothness assumption on the learned graph\nstructure. Some other works also model the adjacency matrix in a probabilistic manner. Bayesian\nGCNN [573] adopts a Bayesian framework and treats the observed graph as a realization from a\nfamily of random graphs. Then it estimates the posterior probablity of labels given the observed\ngraph adjacency matrix and features with Monte Carlo approximation. VGCN [104] follows a\nsimilar formulation and estimates the graph posterior through stochastic variational inference.\nPro-GNN [210] learns a clean graph structure from perturbed data and optimizes parameters for a\nrobust GNN, leveraging properties like sparsity, low rank, and feature smoothness.\nGraph Sparsification via Meta-Learning (GSML) [458]. GSML formulates GSL as a meta-learning\nproblem and uses bi-level optimization to find the optimal graph structure. The goal is to find a\nsparse graph structure that leads to high node classification accuracy at the same time given labeled\nand unlabeled nodes. To achieve this, GSML makes the inner optimization as training on the node\nclassification task, and targets the outer optimization at the sparsity of the graph structure, which\nformulates the following bi-level optimization problem:\n\ud835\udc3a\u02c6\n\u2217 = min\n\ud835\udc3a\u02c6 \u2208\u03a6(\ud835\udc3a)\n\ud835\udc3f\ud835\udc60\ud835\udc5d\ud835\udc60 (\ud835\udc53\ud835\udf03\n\u2217 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc48 ), (125)\n\ud835\udc60.\ud835\udc61 . \ud835\udf03 \u2217 = argmin\n\ud835\udf03\n\ud835\udc3f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (\ud835\udc53\ud835\udf03 (\ud835\udc3a\u02c6), \ud835\udc4c\ud835\udc3f). (126)\nIn this bi-level optimization problem, \ud835\udc3a\u02c6 \u2208 \u03a6(\ud835\udc3a) are the meta-parameters and optimized directly\nwithout parameterization. Similarly, LSD-GNN [124] also uses bi-level optimization. It models graph\nstructure with a probability distribution over the graph and reformulates the bi-level program in\nterms of the continuous distribution parameters.\n9.6 Summary\nIn this section, we provide the summary as follows:\n\u2022 Techniques. GSL aims to learn an optimized graph structure for better graph representations.\nIt is also used for more robust graph representation against adversarial attacks. According\nto the way of edge modeling, we categorize GSL into three groups: metric-based methods,\nmodel-based methods, and direct methods. Regularization is also a commonly used principle\nto make the learned graph structure satisfy specific properties including sparsity, low-rank\nand smoothness.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 43\n\u2022 Challenges and Limitations. Since there is no way to access the ground truth or optimal\ngraph structure as training data, the learning objective of GSL is either indirect (e.g., perfor\u0002mance on downstream tasks) or manually designed (e.g., sparsity and smoothness). Therefore,\nthe optimization of GSL is difficult and the performance is not satisfying. In addition, many\nGSL methods are based on homophily assumption, i.e., similar nodes are more likely to\nconnect with each other. However, many other types of connection exist in the real world\nwhich impose great challenges for GSL.\n\u2022 Future Works. In the future we expect more efficient and generalizable GSL methods to\nbe applied to large-scale and heterogeneous graphs. Most existing GSL methods focus on\npair-wise node similarities and thus struggle to scale to large graphs. Besides, they often\nlearn homogeneous graph structure, but in many scenarios graphs are heterogeneous.\n10 Social Analysis\nIn the real world, there usually exist complex relations and interactions between people and multiple\nentities. Taking people, concrete things, and abstract concepts in society as nodes and taking the\ndiverse, changeable, and large-scale connections between data as links, we can form massive and\ncomplex social information as social networks [43, 436]. Compared with traditional data structures\nsuch as texts and forms, modeling social data as graphs has many benefits. Especially with the arrival\nof the \"big data\" era, more and more heterogeneous information is interconnected and integrated,\nand it is difficult and uneconomical to model this information with a traditional data structure. The\ngraph is an effective implementation for information integration, as it can naturally incorporate\ndifferent types of objects and their interactions from heterogeneous data sources [349, 411]. A\nsummarization of social analysis applications is provided in Table 9.\n10.1 Concepts of Social Networks\nA social network is usually composed of multiple types of nodes, link relationships, and node\nattributes, which inherently include rich structural and semantic information. Specifically, a social\nnetwork can be homogeneous or heterogeneous and directed or undirected in different scenarios.\nWithout loss of generality, we define the social network as a directed heterogeneous graph \ud835\udc3a =\n{\ud835\udc49 , \ud835\udc38, T, R}, where \ud835\udc49 = {\ud835\udc5b\ud835\udc56 }\n|\ud835\udc49 |\n\ud835\udc56=1\nis the node set, \ud835\udc38 = {\ud835\udc52\ud835\udc56 }\n|\ud835\udc38|\n\ud835\udc56=1\nis the edge set, T = {\ud835\udc61\ud835\udc56 }\n| T |\n\ud835\udc56=1\nis the node\ntype set, and R = {\ud835\udc5f\ud835\udc56 }\n| R |\n\ud835\udc56=1\nis the edge type set. Each node \ud835\udc5b\ud835\udc56 \u2208 \ud835\udc49 is associated with a node type\nmapping: \ud835\udf19\ud835\udc5b (\ud835\udc5b\ud835\udc56) = \ud835\udc61\ud835\udc57: \ud835\udc49 \u2212\u2192 T and each edge \ud835\udc52\ud835\udc56 \u2208 \ud835\udc38 is associated with a node type mapping:\n\ud835\udf19\ud835\udc52 (\ud835\udc52\ud835\udc56) = \ud835\udc5f\ud835\udc57: \ud835\udc38 \u2212\u2192 R. A node \ud835\udc5b\ud835\udc56 may have a feature set, where the feature space is specific for the\nnode type. An edge \ud835\udc52\ud835\udc56is also represented by node pairs (\ud835\udc5b\ud835\udc57, \ud835\udc5b\ud835\udc58 ) at both ends and can be directed\nor undirected with relation-type-specific attributes. If |T | = 1 and |R| = 1, the social network is a\nhomogeneous graph; otherwise, it is a heterogeneous graph.\nAlmost any data produced by social activities can be modeled as social networks, for example,\nthe academic social network produced by academic activities such as collaboration and citation,\nthe online social network produced by user following and followed on social media, and the\nlocation-based social network produced by human activities on different locations. Based on\nconstructing social networks, researchers have new paths to data mining, knowledge discovery,\nand multiple application tasks on social data. Exploring social networks also brings new challenges.\nOne of the critical challenges is how to succinctly represent the network from the massive and\nheterogeneous raw graph data, that is, how to learn continuous and low-dimensional social network\nrepresentations, so as to researchers can efficiently perform advanced machine learning techniques\non the social network data for multiple application tasks, such as analysis, clustering, prediction,\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n44 W. Ju, et al.\nTable 9. A summarization of social analysis applications\nSocial networks Node type Edge type Applications References\nAcademic\nSocial\nNetwork\nAuthor,\nPublication,\nVenue,\nOrganization,\nKeyword\nAuthorship,\nCo-Author,\nAdvisor\u0002advisee,\nCiting, Cited,\nCo-Citing,\nPublishing\nClassification/\nClustering\nPaper/author classification [92, 370, 471,\n563], name disambiguation [52, 319, 368,\n575]\nRelationship\nprediction\nCo-authorship [69, 72, 610], citation rela\u0002tionship [203, 464, 550], advisor-advisee re\u0002lationship [286, 317, 594]\nRecommen\u0002dation\nCollaborator recommendation [235, 236,\n296], paper recommendation [20, 81, 429],\nvenue recommendation [337, 549]\nSocial\nMedia\nNetwork\nUser, Blog,\nArticle, Image,\nVideo\nFollowing,\nLike, Unlike,\nClicked,\nViewed,\nCommented,\nReposted\nAnomaly\ndetection\nMalicious attacks [294, 395, 434], emer\u0002gency detection [28, 79, 257], and robot dis\u0002covery [117, 304]\nSentiment\nanalysis\nCustomer feedback [389, 449, 572], public\nevents [33, 332, 450]\nInfluence\nanalysis\nImportant node finding [91, 386], informa\u0002tion diffusion modeling [226, 246, 356, 562]\nLocation-based\nSocial\nNetwork\nRestaurant,\nCinema, Mall,\nParking\nFriendship,\nCheck-in\nPOI recom\u0002mendation\nSpatial/temporal influence [416, 484, 589],\nsocial relationship [297, 513], textual infor\u0002mation [469, 483, 515]\nUrban\ncomputing\nTraffic congestion prediction [202, 511], ur\u0002ban mobility analysis [45, 539], event de\u0002tection [420, 548]\nand knowledge discovery. Thus, graph representation learning on the social network becomes the\nfoundational technique for social analysis.\n10.2 Academic Social Network\nAcademic collaboration is a common and important behavior in academic society, and also a major\nway for scientists and researchers to innovate and breakthrough scientific research, which leads to\nsocial relationship between scholars. The academic data generated by academic collaboration usually\ncontains a large number of interconnected entities with complex relationships [237, 602]. Normally,\nin an academic social network, the node type set consists of Author, Publication, Venue, Organization,\nKeyword, etc., and the relation set consists of Authorship, Co-Author, Advisor-advisee, Citing, Cited,\nCo-Citing, Publishing, Co-Word, etc. Note that in most social networks, each relation type always\nconnects two fixed node types with a fixed direction. For example, the relation Authorship points\nfrom the node type Author to Publication, and the Co-Author is an undirected relation between two\nnodes with type Author. Based on the node and relation types in an academic social network, one\ncan divide it into multiple categories. For example, the co-author network with nodes of Author and\nrelations of Co-Author, the citation network with nodes of Publication and relation of Citing, and the\nacademic heterogeneous information graph with multiple academic node and relation types. Many\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 45\nresearch institutes and academic search engines, such as Aminer1, DBLP2, Microsoft Academic\nGraph (MAG)3, have provided open academic social network datasets for research purposes.\nThere are multiple applications of graph representation learning on the academic social net\u0002work. Roughly, they can be divided into three categories\u2013academic entity classification/clustering,\nacademic relationship prediction, and academic resource recommendation.\n\u2022 Academic entities usually belong to different classes of research areas. Research of academic\nentity classification and clustering aims to categorize these entities, such as papers and\nauthors, into different classes [92, 213, 370, 471, 537, 563]. In literature, academic networks\nsuch as Cora, citepSeer, and Pubmed [406] have become the most widely used benchmark\ndatasets for examining the performance of graph representation learning models on paper\nclassification. Also, the author name disambiguation problem [52, 319, 368, 575] is also\nessentially a node clustering task on co-author networks and is usually solved by the graph\nrepresentation learning technique.\n\u2022 Academic relationship prediction represents the link prediction task on various academic\nrelations. Typical applications are co-authorship prediction [69, 72, 610] and citation rela\u0002tionship prediction [203, 464, 550]. Existing methods learn representations of authors and\npapers and use the similarity between two nodes to predict the link probability. Besides, some\nwork [286, 317, 594] studies the problem of advisor-advisee relationship prediction in the\ncollaboration network.\n\u2022 Various academic recommendation systems have been introduced to retrieve academic re\u0002sources for users from large amounts of academic data in recent years. For example, collabo\u0002rator recommendation [235, 236, 296] benefit researchers by finding suitable collaborators\nunder particular topics; paper recommendation [20, 81, 429] help researchers find relevant pa\u0002pers on given topics; venue recommendation [337, 549] help researchers choose appropriate\nvenues when they submit papers.\n10.3 Social Media Network\nWith the development of the Internet in decades, various online social media have emerged in large\nnumbers and greatly changed people\u2019s traditional social models. People can establish friendships\nwith others beyond the distance limit and share interests, hobbies, status, activities, and other\ninformation among friends. These abundant interactions on the Internet form large-scale complex\nsocial media networks, also named online social networks. Usually, in an academic social network,\nthe node type set consists of User, Blog, Article, Image, Video, etc., and the relation type set consists\nof Following, Like, Unlike, Clicked, Viewed, Commented, Reposted, etc. The main property of a social\nmedia network is that it usually contains multi-mode information on the nodes, such as video,\nimage, and text. Also, the relations are more complex and multiplex, including the explicit relations\nsuch as Like and Unlike and the implicit relations such as Clicked. The social media network can\nbe categorized into multiple types based on their media categories. For example, the friendship\nnetwork, the movie review network, and the music interacting network are extracted from different\nsocial media platforms. In a broad sense, the user-item networks in online shopping system can also\nbe viewed as social media networks as they also exist on the Internet and contains rich interactions\nby people. There are many widely used data sources for social media network analysis, such as\nTwitter, Facebook, Weibo, YouTube, and Instagram.\n1https://www.aminer.cn/\n2https://dblp.uni-trier.de/\n3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n46 W. Ju, et al.\nThe mainstream application research on social media networks via graph representation learning\ntechniques mainly includes anomaly detection, sentiment analysis, and influence analysis.\n\u2022 Anomaly detection aims to find strange or unusual patterns in social networks, which has\na wide range of application scenarios, such as malicious attacks [294, 395, 434], emergency\ndetection [28, 79], and robot discovery [117, 304] in social networks. Unsupervised anomaly\ndetection usually learns a reconstructed graph to detect those nodes with higher reconstructed\nerror as the anomaly nodes [5, 588]; Supervised methods model the problem as a binary\nclassification task on the learned graph representations [340, 596].\n\u2022 Sentiment analysis, also named as opinion mining, is to mine the sentiment, opinions, and\nattitudes, which can help enterprises understand customer feedback on products [389, 449,\n572] and help the government analyze the public emotion and make rapid response to public\nevents [33, 332, 450]. The graph representation learning model is usually combined with\nRNN-based [58, 561] or Transformer-based [7, 441] text encoders to incorporate both the\nuser relationship and textual semantic information.\n\u2022 Influence analysis usually aims to find several nodes in a social network to initially spread\ninformation such as advertisements, so as to maximize the final spread of information [91, 386].\nThe core challenge is to model the information diffusion process in the social network. Deep\nlearning methods [226, 246, 356, 562] usually leverage graph neural networks to learn node\nembeddings and diffusion probabilities between nodes.\n10.4 Location-based Social Network\nLocations are the fundamental information of human social activities. With the wide availability of\nmobile Internet and GPS positioning technology, people can easily acquire their precise locations\nand socialize with their friends by sharing their historical check-ins on the Internet. This opens up\na new avenue of research on location-based social network analysis, which gathered significant\nattention from the user, business, and government perspectives. Usually, in a location-based social\nnetwork, the node type set consists of User, and Location, also named Point of Interest(POI) in the\nrecommendation scenario containing multiple categories such as Restaurant, Cinema, Mall, Parking,\netc. The relation type set consists of Friendship, Check-in. Also, those node and relation types that\nexist in traditional social media networks can be included in a location-based social network. The\ndifference with other social networks, the main location-based social networks are spatial and\ntemporal, making the graph representation learning more challenging. For example, in a typical\nsocial network constructed for the POI recommendation, the user nodes are connected with each\nother by their friendship. The location nodes are connected by user nodes with the relations feature\nof timestamps. The location nodes also have a spatial relationship with each other and own have\ncomplex features, including categories, tags, check-in counts, number of users check-in, etc. There\nare many location-based social network datasets, such as Foursquare4, Gowalla5, and Waze6. Also,\nmany social media such as Twitter, Instagram, and Facebook can provide location information.\nThe research of graph representation learning on location-based social networks can be divided\ninto two categories: POI recommendation for business benefits and urban computing for public\nmanagement.\n\u2022 POI recommendation is one of the research hotspots in the field of location-based social\nnetworks and recommendation systems in recent years [195, 219, 489], which aim to uti\u0002lize historical check-ins of users and auxiliary information to recommend potential favor\n4https://foursquare.com/\n5https://www.gowalla.com/\n6https://www.waze.com/live-map/\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 47\nplaces for users from a large of location points. Existing researches mainly integrate four\nessential characteristics, including spatial influence, temporal influence [416, 484, 589], social\nrelationship [297, 513], and textual information [469, 483, 515].\n\u2022 Urban computing is defined as a process of analysis of the large-scale connected urban data\ncreated from city activities of vehicles, human beings, and sensors [358, 359, 417]. Besides the\nlocal-based social network, the urban data also includes physical sensors, city infrastructure,\ntraffic roads, and so on. Urban computing aims to improve the quality of public management\nand life quality of people living in city environments. Typical applications including traffic\ncongestion prediction [202, 511], urban mobility analysis [45, 539], event detection [420, 548].\n10.5 Summary\nThis section introduces social analysis by graph representation learning and we provide the\nsummary as follows:\n\u2022 Techniques. Social networks, generated by human social activities, such as communication,\ncollaboration, and social interactions, typically involve massive and heterogeneous data, with\ndifferent types of attributes and properties that can change over time. Thus, social network\nanalysis is a field of study that explores the techniques to understand and analyze the complex\nattributes, heterogeneous structures, and dynamic information of social networks. Social\nnetwork analysis typically learns low-dimensional graph representations that capture the\nessential properties and patterns of the social network data, which can be used for various\ndownstream tasks, such as classification, clustering, link prediction, and recommendation.\n\u2022 Chanllenges and Limitations. Despite the structural heterogeneity in social networks\n(nodes and relations have different types), with the technological advances in social media,\nthe node attributes have become more heterogeneous now, containing text, video, and images.\nAlso, the large-scale problem is a pending issue in social network analysis. The data in\nthe social network has increased exponentially in past decades, containing a high density\nof topological links and a large amount of node attribute information, which brings new\nchallenges to the efficiency and effectiveness of traditional network representation learning\non the social network. Lastly, social networks are often dynamic, which means the network\ninformation usually changes over time, and this temporal information plays a significant\nrole in many downstream tasks, such as recommendations. This brings new challenges to\nrepresentation learning on social networks in incorporating temporal information.\n\u2022 Future Works. Recently, multi-modal big pre-training models that can fuse information\nfrom different modalities have gained increasing attention [369, 379]. These models can\nobtain valuable information from a large amount of unlabeled data and transfer it to various\ndownstream analysis tasks. Moreover, Transformer-based models have demonstrated better\neffectiveness than RNNs in capturing temporal information. In the future, there is potential\nfor introducing multi-modal big pre-training models in social network analysis. Also, it is\nimportant to make the models more efficient for network information extraction and use\nlightweight techniques like knowledge distillation to further enhance the applicability of the\nmodels. These advancements can lead to more effective social network analysis and enable\nthe development of more sophisticated applications in various domains.\n11 Molecular Property Prediction\nMolecular Property Prediction is an essential task in computational drug discovery and cheminfor\u0002matics. Traditional quantitative structure property/activity relationship (QSPR/QSAR) approaches\nare based on either SMILES or fingerprints [344, 522, 570], largely overlooking the topological\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n48 W. Ju, et al.\nfeatures of the molecules. To address this problem, graph representation learning has been widely\napplied to molecular property prediction. A molecule can be represented as a graph where nodes\nstand for atoms and edges stand for atom-bonds (ABs). Graph-level molecular representations\nare learned via the message passing mechanism to incorporate the topological information. The\nrepresentations are then utilized for the molecular property prediction tasks.\nSpecifically, a molecule is denoted as a topological graph G = (V, E), where V = {\ud835\udc63\ud835\udc56|\ud835\udc56 =\n1, . . . , |G|} is the set of nodes representing atoms. A feature vector x\ud835\udc56is associated with each node \ud835\udc63\ud835\udc56\nindicating its type such as Carbon, Nitrogen. E = {\ud835\udc52\ud835\udc56\ud835\udc57 |\ud835\udc56, \ud835\udc57 = 1, . . . , |G|} is the set of edges connecting\ntwo nodes (atoms) \ud835\udc63\ud835\udc56 and \ud835\udc63\ud835\udc57 representing atom bonds. Graph representation learning methods\nare used to obtain the molecular representation hG. Then downstream classification or regression\nlayers \ud835\udc53 (\u00b7) are applied to predict the probability of target property of each molecule \ud835\udc66 = \ud835\udc53 (hG).\nIn Section 11.1, we introduce 4 types of molecular properties graph representation learning can\nbe treated and their corresponding datasets. Section 11.2 reviews the graph representation learning\nbackbones applied to molecular property prediction. Strategies for training the molecular property\nprediction methods are listed in Section 11.3.\n11.1 Molecular Property Categorization\nPlenty of molecular properties can be predicted by graph-based methods. We follow Wieder et al.\n[490] to categorize them into 4 types: quantum chemistry, physicochemical properties, biophysics,\nand biological effect.\nQuantum chemistry is a branch of physical chemistry focused on the application of quantum\nmechanics to chemical systems, including conformation, partial charges and energies. QM7, QM8,\nQM9 [501], COD [391] and CSD [154] are datasets for quantum chemistry prediction.\nPhysicochemical properties are the intrinsic physical and chemical characteristics of a substance,\nsuch as bioavailability, octanol solubility, aqueous solubility and hydrophobicity. ESOL, Lipophilicity\nand Freesolv [501] are datasets for physicochemical properties prediction.\nBiophysics properties are about the physical underpinnings of biomolecular phenomena, such\nas affinity, efficacy and activity. PDBbind [466], MUV, and HIV [501] are biophysics property\nprediction datasets.\nBiological effect properties are generally defined as the response of an organism, a population,\nor a community to changes in its environment, such as side effects, toxicity and ADMET. Tox21,\ntoxcast [501] and PTC [448] are biological effect prediction datasets.\nMoleculenet [501] is a widely-used benchmark dataset for molecule property prediction. It\ncontains over 700,000 compounds tested on different properties. For each dataset, they provide\na metric and a splitting pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv,\nLipophilicity and PDBbind are regression tasks, using MAE or RMSE as the evaluation metrics.\nOther tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation metric.\n11.2 Molecular Graph Representation Learning Backbones\nSince node attributes and edge attributes are crucial to molecular representation, most works use\nGNN instead of traditional graph representation learning methods as backbones, since many GNN\nmethods consider edge information. Existing GNNs designed for the general domain can be applied\nto molecular graphs. Table 10 summarizes the GNNs used for molecular property prediction and\nthe types of properties they can be applied to predict.\nFurthermore, many works customize their GNN structure by considering the chemical domain\nknowledge.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 49\nTable 10. Summary of GNNs in molecular property prediction.\nType Spatial/Specrtal Method Application\nReccurent GNN - R-GNN Biological effect [400]\nReccurent GNN - GGNN Quantum chemistry [333],\nBiological effect [9, 115, 492]\nReccurent GNN - IterRefLSTM Biophysics [9], Biological effect [9]\nConvolutional GNN Spatial/Specrtal GCN\nQuantum chemistry [280, 492, 529],\npysicochemical properties [75, 101, 393],\nBiophysics [34, 101, 529]\nBiological effect [261, 501]\nConvolutional GNN Specrtal LanczosNet Quantum chemistry [280]\nConvolutional GNN Specrtal ChebNet Physicochemical properties,\nBiophysics, Biological effect [267]\nConvolutional GNN Spatial GraphSAGE\nPhysicochemical properties [181],\nBiophysics [68, 108, 279],\nBiological effect [181, 328]\nConvolutional GNN Spatial GAT\nPhysicochemical properties [3, 181],\nBiophysics [34, 68],\nBiological effect [181]\nConvolutional GNN Spatial DGCNN Biophysics [63], Biological effect [568]\nConvolutional GNN Spatial GIN\nPhysicochemical properties [34, 181],\nBiophysics [180, 181],\nBiological effect [181]\nConvolutional GNN Spatial MPNN Physicochemical [320]\nTransformer - MAT Physicochemical, Biophysics [616]\n\u2022 First, the chemical bonds and molecule interaction are taken into consideration carefully. For\nexample, Ma et al. [320] use an additional edge GNN to model the chemical bonds separately.\nSpecifically, given an edge (\ud835\udc63,\ud835\udc64), they formulate an Edge-based GNN as:\nm\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = AGGedge ({h\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(\ud835\udc58\u22121)\n\ud835\udc62\ud835\udc63 , x\ud835\udc62 |\ud835\udc62 \u2208 N\ud835\udc63 \\ \ud835\udc64}), h\n(\ud835\udc58)\n\ud835\udc63\ud835\udc64 = MLPedge ({m\n(\ud835\udc58\u22121)\n\ud835\udc63\ud835\udc64 , h\n(0)\n\ud835\udc63\ud835\udc64 }), (127)\nwhere h\n(0)\n\ud835\udc63\ud835\udc64 = \ud835\udf0e(Weine\ud835\udc63\ud835\udc64) is the input state of the Edge-based GNN, Wein \u2208 R\n\ud835\udc51hid\u00d7\ud835\udc51\ud835\udc52\nis the\ninput weight matrix. PotentialNet [115] further uses different message passing operations\nfor different edge types. DGNN-DDI [325] leverage dual graph neural networks to model the\ninteraction between two molecules.\n\u2022 Second, motifs in molecular graphs play an important role in molecular property prediction.\nGSN [34] leverage substructure encoding to construct a topologically-aware message-passing\nmethod. Each node \ud835\udc63 updates its state h\n\ud835\udc61\n\ud835\udc63\nby combining its previous state with the aggregated\nmessages:\nh\n\ud835\udc61+1\n\ud835\udc63 = UP\ud835\udc61+1\nh\n\ud835\udc61\n\ud835\udc63\n, m\n\ud835\udc61+1\n\ud835\udc63\n\u0001\n, (128)\nm\n\ud835\udc61+1\n\ud835\udc63 =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\n\uf8f3\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-v)\nor\n\ud835\udc40\ud835\udc61+1( [h\n\ud835\udc61\n\ud835\udc63\n, h\n\ud835\udc61\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 ]\ud835\udc62\u2208N (\ud835\udc63)) (GSN-e)\n, (129)\nwhere x\n\ud835\udc49\n\ud835\udc63\n, x\n\ud835\udc49\n\ud835\udc62\n, x\n\ud835\udc38\n\ud835\udc62,\ud835\udc63, e\ud835\udc62,\ud835\udc63 contains the substructure information associated with nodes and\nedges, [] denotes a multiset. Yu et al. [551] constructs a heterogeneous graph using motifs\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n50 W. Ju, et al.\nand molecules. Motifs and molecules are both treated as nodes and the edges model the\nrelationship between motifs and graphs, for example, if a graph contains a motif, there will\nbe an edge between them. MGSSL [580] leverages a retrosynthesis-based algorithm BRICS\nand additional rules to find the motifs and combines motif layers with atom layers. It is a\nhierarchical framework jointly modeling atom-level information and motif-level information.\nAouichaoui et al. [12] introduce group-contribution-based attention to highlight the most\nsubstructures in molecules.\n\u2022 Third, different feature modalities have been used to improve molecular graph embedding.\nLin et al. [283] combine SMILES modality and graph modality with contrastive learning.\nZhu et al. [608] encode 2D molecular graph and 3D molecular conformation with a unified\nTransformer. It uses a unified model to learn 3D conformation generation given 2D graph\nand 2D graph generation given 3D conformation. Cremer et al.[78] use a Equivariant Graph\nNeural Networks to represent the 3D information of molecules. Liu et al. [293] consider\nmolecular chirality and design a chirality-aware molecular convolution module.\n\u2022 Finally, knowledge graph and literature can provide additional knowledge for molecular\nproperty prediction. Fang et al. [110] introduce a chemical element knowledge graph to\nsummarize microscopic associations between elements and augment the molecular graph\nbased on the knowledge graph, and a knowledge-aware message-passing network is used to\nencode the augmented graph. MuMo [428] introduces biomedical literature to guide molecular\nproperty prediction. It pretrains a GNN and a language model on paired data of molecules\nand literature mentions via contrastive learning:\n\u2113\n(z\n\ud835\udc3a\n\ud835\udc56\n,z\n\ud835\udc47\n\ud835\udc56\n)\n\ud835\udc56\n= \u2212 log\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\n)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc57=1\nexp (sim(z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc57\n)/\ud835\udf0f)\n, (130)\nwhere z\n\ud835\udc3a\n\ud835\udc56\n, z\n\ud835\udc47\n\ud835\udc56\nare the representation of molecule and its corresponding literature. Zhao et al.\n[583] propose a unified Transformer architecture to jointly model molecule graph and the\ncorresponding bioassay description.\n11.3 Training strategies\nDespite the encouraging performance achieved by GNNs, the traditional supervised training scheme\nof GNNs faces a severe limitation: The scarcity of available molecules with desired properties.\nAlthough there are a large number of molecular graphs in public databases such as PubChem,\nlabeled molecules are hard to acquire due to the high cost of wet-lab experiments and quantum\nchemistry calculations. Directly training GNNs on such limited molecules in a supervised way\nis prone to over-fitting and lack of generalization. To address this issue, few-shot learning and\nself-supervised learning are widely used in molecular property prediction.\nFew-shot learning. Few-shot learning aims at generalizing to a task with a small labeled data\nset. The prediction of each property is treated as a single task. Metric-based and optimization-based\nfew-shot learning have been adopted for molecular property prediction. Metric-based few-shot\nlearning is similar to nearest neighbors and kernel density estimation, which learns a metric or\ndistance function over objects. IterRefLSTM [9] leverages matching network [455] as the few\u0002shot learning framework, calculating the similarity between support samples and query samples.\nOptimization-based few-shot learning optimizes a meta-learner for parameter initialization which\ncan be fast adapted to new tasks. Meta-MGNN [161] adopts MAML [120] to train a parameter\ninitialization to adapt to different tasks and use self-attentive task weights for each task. PAR [474]\nalso uses MAML framework and learns an adaptive relation graph among molecules for each task.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 51\nSelf-supervised learning. Self-supervised learning can pre-train a GNN model with plenty of\nunlabeled molecular graphs and transfer it to specific molecular property prediction tasks. Self\u0002supervised learning contains generative methods and predictive methods. Predictive methods design\nprediction tasks to capture the intrinsic data features. Pre-GNN [181] exploits both node-level and\ngraph-level prediction tasks including context prediction, attribute masking, graph-level property\nprediction and structural similarity prediction. MGSSL [580] provides a motif-based generative\npre-training framework making topology prediction and motif generation iteratively. Contrastive\nmethods learn graph representations by pulling views from the same graph close and pushing\nviews from different graphs apart. Different views of the same graph are constructed by graph\naugmentation or leveraging the 1D SMILES and 3D structure. MolCLR [478] augments molecular\ngraphs by atom masking, bond deletion and subgraph removal and maximizes the agreement\nbetween the original molecular graph and augmented graphs. Fang et al. [110] uses a chemical\nknowledge graph to guide the graph augmentation. SMICLR [366] uses contrastive learning across\nSMILES and 2D molecular graphs. GeomGCL [268] leverages graph contrastive learning to capture\nthe geometry of the molecule across 2D and 3D views. Jiang et al. [201] and Fang et al. [111] integrate\nmolecule graphs with chemical knowledge graph and fuse the two modalities with contrastive\nlearning. Self-supervised learning can also be combined with few-shot learning to fully leverage\nthe hierarchical information in the training set [215].\n11.4 Summary\nThis section introduces graph representation learning in molecular property prediction and we\nprovide the summary as follows:\n\u2022 Techniques. For molecular property prediction, a molecule is represented as a graph whose\nnodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN, GAT, and GraphSAGE\nare adopted to learn the graph-level representation. The representations are then fed into a\nclassification or regression head for the molecular property prediction tasks. Many works\nguide the model structure design with medical domain knowledge including chemical bond\nfeatures, motif features, different modalities of molecular representation, chemical knowledge\ngraph and literature. Due to the scarcity of available molecules with desired properties,\nfew-shot learning and contrastive learning are used to train molecular property prediction\nmodels, so that the model can leverage the information in large unlabeled dataset and can be\nadapted to new tasks with a few examples.\n\u2022 Challenges and Limitations. Despite the great success of graph representation learning\nin molecular property prediction, the methods still have limitations: 1) Few-shot molecular\nproperty prediction are not fully explored. 2) Most methods depend on training with labeled\ndata, but neglect the chemical domain knowledge.\n\u2022 Future Works. In the future, we expect that: 1) More few-shot learning and zero-shot learning\nmethods are studied for molecular property prediction to solve the data scarcity problem. 2)\nHeterogeneous data can be fused for molecular property prediction. There are a large amount\nof heterogeneous data about molecules such as knowledge graphs, molecule descriptions\nand property descriptions. They can be considered to assist molecular property prediction. 3)\nChemical domain knowledge can be leveraged for the prediction model. For example, when\nwe perform affinity prediction, we can consider molecular dynamics knowledge.\n12 Molecular Generation\nMolecular generation is pivotal to drug discovery, where it serves a fundamental role in downstream\ntasks like molecular docking [341] and virtual screening [457]. The goal of molecular generation\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n52 W. Ju, et al.\nis to produce chemical structures that satisfy a specific molecular profile, e.g., novelty, binding\naffinity, and SA scores. Traditional methods have relied on 1D string formats like SMILES [148] and\nSELFIES [240]. With the recent advances in graph representation learning, numerous graph-based\nmethods have also emerged, where molecular graph G can naturally embody both 2D topology and\n3D geometry. While recent literature reviews [99, 342] have covered the general topics of molecular\ndesign, this chapter is dedicated to the applications of graph representation learning in the molecular\ngeneration task. Molecular generation is intrinsically a de novo task, where molecular structures\nare generated from scratch to navigate and sample from the vast chemical space. Therefore, this\nchapter does not discuss tasks that restrict chemical structures a priori, such as docking [131, 427]\nand conformation generation [412, 607].\n12.1 Taxonomy for molecular featurization methods\nThis section categorizes the different methods to feature molecules. The taxonomy presented here\nis unique to the task of molecular generation, owing to the various modalities of molecular entities,\ncomplex interactions with other bio-molecular systems and formal knowledge from the laws of\nchemistry and physics.\n2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one thing, a\nmolecule can be unambiguously represented by its 2D topological graph G2D, where atoms are\nnodes and bonds are edges. G2D can be encoded by canonical MPNN models like GCN [230],\nGAT [452], and R-GCN [401], in ways similar to tasks like social networks and knowledge graphs. A\ntypical example of this line of work is GCPN [543], a graph convolutional policy network generating\nmolecules with desired properties such as synthetic accessibility and drug-likeness.\nFor another, the 3D conformation of a molecule can be accurately depicted by its 3D geometric\ngraph G3D, which incorporates 3D atom coordinates. In 3D-GNNs like SchNet [405] and Orb\u0002Net [371], G3D is organized into a \ud835\udc58-NN graph or a radius graph according to the Euclidean distance\nbetween atoms. It is justifiable to approximate G3D as a 3D extension to G2D, since covalent atoms\nare closest to each other in most cases. However, G3D can also find a more long-standing origin\nin the realm of computational chemistry [126], where both covalent and non-covalent atomistic\ninteractions are considered to optimize the potential surface and simulate molecular dynamics.\nTherefore, G3D more realistically represents the molecular geometry, which makes a good fit for\nprotein pocket binding and 3D-QSAR optimization [453].\nMolecules can rotate and translate, affecting their position in the 3D space. Therefore, it is ideal to\nencode these molecules with GNNs equivariant/invariant to roto-translations, which can be \u223c 103\ntimes more efficient than data augmentation [144]. Equivariant GNNs can be based on irreducible\nrepresentation [10, 24, 37, 130, 446], regular representation [121, 192], or scalarization [190, 212,\n232\u2013234, 292, 398, 404, 405, 445], which are explained in more detail in [165]. Recent works like\nGraphVF [430] and MolCode [579] have been incorporating G2D and G3D to accurately capture the\nrelationship between structure and properties in molecular design in a unified way.\nUnbounded vs. binding-based. Earlier works have aimed to generate unbounded molecules in\neither 2D or 3D space, striving to learn good molecular representations through this task. In the 2D\nscenario, GraphNVP [329] first introduces a flow-based model to learn an invertible transformation\nbetween the 2D chemical space and the latent space. GraphAF [413] further adopts an autoregressive\ngeneration scheme to check the valence of the generated atoms and bonds. In the 3D scenario,\nG-SchNet [142] first proposes to utilize G3D (instead of 3D density grids) as the generation backbone.\nIt encodes G3D via SchNet, and uses an auxiliary token to generate atoms on the discretized 3D\nspace autoregressively. G-SphereNet [316] uses symmetry-invariant representations in a spherical\ncoordinate system (SCS) to generate atoms in the continuous 3D space and preserve equivariance.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 53\nUnbounded models adopt certain techniques to optimize specific properties of the generated\nmolecules. GCPN and GraphAF use scores like logP, QED, and chemical validity to tune the model\nvia reinforcement learning. EDM [178] can generate 3D molecules with property \ud835\udc50 by re-training\nthe diffusion model with \ud835\udc50\u2019s feature vector concatenated to the E(n) equivariant dynamics function\n\ud835\udf50\u02c6\ud835\udc61 = \ud835\udf19 (\ud835\udc9b\ud835\udc61, [\ud835\udc61, \ud835\udc50]). cG-SchNet [143] adopts a conditioning network architecture to jointly target\nmultiple electronic properties during conditional generation without the need to re-train the model.\nRetMol [481] uses a retrieval-based model for controllable generation.\nOn the other hand, binding-based methods generate drug-like molecules (aka. ligands) according\nto the binding site (aka. binding pocket) of a protein receptor. Drawing inspirations from the lock\u0002and-key model for enzyme action [122], works like LiGAN [380] and DESERT [302] uses 3D\ndensity grids to fit the density surface between the ligand and the receptor, encoded by 3D-CNNs.\nMeanwhile, a growing amount of literature has adopted G3D for representing ligand and receptor\nmolecules, because G3D more accurately depicts molecular structures and atomistic interactions\nboth within and between the ligand and the receptor. Representative works include 3D-SBDD [306],\nGraphBP [288], Pocket2Mol [361], and DiffSBDD [403]. GraphBP shares a similar workflow with G\u0002SphereNet, except that the receptor atoms are also incorporated into G3D to depict the 3D geometry\nat the binding pocket.\nAtom-based vs.fragment-based. Molecules are inherently hierarchical structures. At the atom\u0002istic level, molecules are represented by encoding atoms and bonds. At a coarser level, molecules\ncan also be represented as molecular fragments like functional groups or chemical sub-structures.\nBoth the composition and the geometry are fixed within a given fragment, e.g., the planar peptide\u0002bond (\u2013CO\u2013NH\u2013) structure. Fragment-based generation effectively reduces the degree of freedom\n(DOF) of chemical structures, and injects well-established knowledge about molecular patterns\nand reactivity. JT-VAE [207] decomposes 2D molecular graph G2D into a junction-tree structure\nT, which is further encoded via tree message-passing. DeepScaffold [270] expands the provided\nmolecular scaffold into 3D molecules. L-Net [272] adopts a graph U-Net architecture and devises\na custom three-level node clustering scheme for pooling and unpooling operations in molecular\ngraphs. A number of works have also emerged lately for fragment-based generation in the binding\u0002based setting, including FLAG [581] and FragDiff [360]. FLAG uses a regression-based approach to\nsequentially decide the type and torsion angle of the next fragment to be placed at the binding site,\nand finally optimizes the molecule conformation via a pseudo-force field. FragDiff also adopts a\nsequential generation process but uses a diffusion model to determine the type and pose of each\nfragment in one go.\n12.2 Generative methods for molecular graphs\nFor a molecular graph generation process, the model first learns a latent distribution \ud835\udc43 (\ud835\udc4d |G)\ncharacterizing the input molecular graphs. A new molecular graph G\u02c6 is then generated by sam\u0002pling and decoding from this learned distribution. Various models have been adopted to generate\nmolecular graphs, including generative adversarial network (GAN), variational auto-encoder (VAE),\nnormalizing flow (NF), diffusion model (DM), and autoregressive model (AR).\nGenerative adversarial network (GAN). GAN [149] is trained to discriminate real data \ud835\udc99\nfrom generated generated data \ud835\udc9b, with the training object formalized as\nmin\n\ud835\udc3a\nmax\n\ud835\udc37\nL (\ud835\udc37,\ud835\udc3a) = E\ud835\udc99\u223c\ud835\udc5ddata [log\ud835\udc37(\ud835\udc99)] + E\ud835\udc9b\u223c\ud835\udc5d (\ud835\udc9b) [log(1 \u2212 \ud835\udc37(\ud835\udc3a(\ud835\udc9b)))], (131)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n54 W. Ju, et al.\nwhere \ud835\udc3a(\u00b7) is the generator function and \ud835\udc37(\u00b7) is the discriminator function. For example, Mol\u0002GAN [82] encodes G2D with R-GCN, trains \ud835\udc37 and \ud835\udc3a with improved W-GAN [13], and uses rein\u0002forcement learning to generate attributed molecules, where the score function is assigned from\nRDKit [249] and chemical validity.\nVaraitional auto-encoder (VAE). In VAE [228], the decoder parameterizes the conditional\nlikelihood distribution \ud835\udc5d\ud835\udf03 (\ud835\udc99|\ud835\udc9b), and the encoder parameterizes an approximate posterior distribution\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99) \u2248 \ud835\udc5d\ud835\udf03 (\ud835\udc9b|\ud835\udc99). The model is optimized by the evidence lower bound (ELBO), consisting of the\nreconstruction loss term and the distance loss term:\nmax\n\ud835\udf03,\ud835\udf19\nL\ud835\udf03,\ud835\udf19 (\ud835\udc99) := E\ud835\udc9b\u223c\ud835\udc5e\ud835\udf19 (\u00b7 |\ud835\udc99)\n\u0014\nln \ud835\udc5d\ud835\udf03 (\ud835\udc99, \ud835\udc9b)\n\ud835\udc5e\ud835\udf19 (\ud835\udc9b|\ud835\udc99)\n\u0015\n= ln \ud835\udc5d\ud835\udf03 (\ud835\udc99) \u2212 \ud835\udc37KL \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc99) \u2225\ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc99)\n\u0001\n. (132)\nMaximizing ELBO is equivalent to simultaneously maximizing the log-likelihood of the observed\ndata, and minimizing the divergence of the approximate posterior \ud835\udc5e\ud835\udf19 (\u00b7|\ud835\udc65) from the exact poste\u0002rior \ud835\udc5d\ud835\udf03 (\u00b7|\ud835\udc65). Representative works along this thread include JT-VAE [207], GraphVAE [419], and\nCGVAE [290] for the 2D generation task, and 3DMolNet [351] for the 3D generation task.\nAutoregressive model (AR). Autoregressive model is an umbrella definition for any model\nthat sequentially generates the components (atoms or fragments) of a molecule. ARs better capture\nthe interdependency within the molecular structure and allows for explicit valency check. For each\nstep in AR, the new component can be produced using different techniques:\n\u2022 Regression/classification, such is the case with 3D-SBDD [306], Pocket2Mol [361], etc.\n\u2022 Reinforcement learning, such is the case with L-Net [272], DeepLigBuilder [273], etc.\n\u2022 Probabilistic models like normalizing flow and diffusion.\nNormalizing flow (NF). Based on the change-of-variable theorem, NF [385] constructs an\ninvertible mapping \ud835\udc53 between a complex data distribution \ud835\udc99 \u223c \ud835\udc4b: and a normalized latent distribu\u0002tion \ud835\udc9b \u223c \ud835\udc4d. Unlike VAE, which has juxtaposed parameters for encoder and decoder, the flow model\nuses the same set of parameter for encoding and encoding: reverse flow \ud835\udc53\n\u22121\nfor generation, and\nforward flow \ud835\udc53 for training:\nmax\n\ud835\udc53\nlog \ud835\udc5d(\ud835\udc99) = log \ud835\udc5d\ud835\udc3e (\ud835\udc9b\ud835\udc3e) (133)\n= log \ud835\udc5d\ud835\udc3e\u22121 (\ud835\udc9b\ud835\udc3e\u22121) \u2212 log\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc3e (\ud835\udc9b\ud835\udc3e\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc3e\u22121\n\u0013\n(134)\n= . . . (135)\n= log \ud835\udc5d0 (\ud835\udc9b0) \u2212\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc56=1\nlog\ndet \u0012\n\ud835\udc51 \ud835\udc53\ud835\udc56 (\ud835\udc9b\ud835\udc56\u22121)\n\ud835\udc51\ud835\udc9b\ud835\udc56\u22121\n\u0013\n, (136)\nwhere \ud835\udc53 = \ud835\udc53\ud835\udc3e \u25e6 \ud835\udc53\ud835\udc3e\u22121 \u25e6 ... \u25e6 \ud835\udc531 is a composite of \ud835\udc3e blocks of transformation. While GraphNVP [329]\ngenerates the molecular graph with NF in one go, following works tend to use the autoregressive\nflow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439].\nDiffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion\nsteps to slowly add random noise to data \ud835\udc990 \u223c \ud835\udc5e(\ud835\udc99):\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121) = N (\ud835\udc99\ud835\udc61;\n\u221a\ufe01\n1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc99\ud835\udc61\u22121, \ud835\udefd\ud835\udc61 \ud835\udc70), (137)\n\ud835\udc5e(\ud835\udc991:\ud835\udc47 |\ud835\udc990) =\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc99\ud835\udc61|\ud835\udc99\ud835\udc61\u22121). (138)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 55\nTable 11. Summary of molecular generation models.\nModel 2D/3D Binding\u0002based\nFragment\u0002basedGNN\nBackbone\nGenerative\nModel\nGCPN [543] 2D GCN [230] GAN\nMolGAN [82] 2D R-GCN [401] GAN\nDEFactor [16] 2D GCN GAN\nGraphVAE [419] 2D ECC [418] VAE\nMDVAE [100] 2D GGNN [274] VAE\nJT-VAE [207] 2D \u2713 MPNN [147] VAE\nCGVAE [290] 2D GGNN VAE\nDeepScaffold [270] 2D \u2713 GCN VAE\nGraphNVP [329] 2D R-GCN NF\nMoFlow [557] 2D R-GCN NF\nGraphAF [413] 2D R-GCN NF + AR\nGraphDF [318] 2D R-GCN NF + AR\nL-Net [272] 3D \u2713 g-U-Net [133] AR\nG-SchNet [142] 3D SchNet [405] AR\nGEN3D [388] 3D EGNN [398] AR\nG-SphereNet [316] 3D SphereNet [292] NF + AR\nEDM [178] 3D EGNN DM\nGCDM [347] 3D GCPNet [346] DM\n3D-SBDD [306] 3D \u2713 SchNet AR\nPocket2Mol [361] 3D \u2713 GVP [211] AR\nFLAG [581] 3D \u2713 \u2713 SchNet AR\nGraphBP [288] 3D \u2713 SchNet NF + AR\nSiamFlow [439] 3D \u2713 R-GCN NF\nDiffBP [282] 3D \u2713 EGNN DM\nDiffSBDD [403] 3D \u2713 EGNN DM\nTargetDiff [156] 3D \u2713 EGNN DM\nFragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR\nGraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR\nMolCode [579] 2D + 3D \u2713 EGNN NF + AR\nThey then learn to reverse the diffusion process to construct desired data samples from the noise:\n\ud835\udc5d\ud835\udf03 (\ud835\udc990:\ud835\udc47 ) = \ud835\udc5d(\ud835\udc99\ud835\udc47 )\n\u00d6\n\ud835\udc47\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61), (139)\n\ud835\udc5d\ud835\udf03 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61) = N (\ud835\udc99\ud835\udc61\u22121; \ud835\udf41\ud835\udf03(\ud835\udc99\ud835\udc61, \ud835\udc61), \ud835\udeba\ud835\udf03 (\ud835\udc99\ud835\udc61, \ud835\udc61)), (140)\nwhile the models are trained using a variational lower bound. Diffusion models have been applied\nto generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands\nin DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate\nmolecular fragments in autoregressive models, as is the case with FragDiff [360].\n12.3 Summary and prospects\nWe wrap up this chapter with Table 11, which profiles existing molecular generation models\naccording to their taxonomy for molecular featurization, the GNN backbone, and the generative\nmethod. This chapter covers the critical topics of molecular generation, which also elicit valuable\ninsights into the promising directions for future research. We summarize these important aspects\nas follows.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n56 W. Ju, et al.\nTechniques. Graph neural networks can be flexibly leveraged to encode molecular features\non different representation levels and across different problem settings. Canonical GNNs like\nGCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs,\nwhile 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular,\nthis 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the\nbinding protein receptor is considered alongside the ligand geometry per se. Fragment-based models\nlike JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure.\nVarious generative methods have also been effectively incorporated into the molecular setting,\nincluding generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive\nmodel (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to\ngenerate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating\nthe search for drug candidates.\nChallenges and Limitations. While there has been an abundant supply of unlabelled molecular\nstructural and geometric data [125, 193, 426], the number of labeled molecular data over certain\ncritical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the\nother hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of\nthe generated molecules, such as QED and Vina [103], rather than actual wet lab experiments.\nFuture Works. Besides the structural and geometric attributes described in this chapter, an\neven more extensive array of data can be applied to aid molecular generation, including chemical\nreactions and medical ontology. These data can be organized into a heterogeneous knowledge\ngraph to aid the extraction of high-quality molecular representations. Furthermore, high through\u0002put experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and\ndruggability of the generated molecules in the wet lab. Concrete case studies, such as the design of\npotential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights\ninto leveraging these molecular generative models to facilitate the design and fabrication of potent\nand applicable drug molecules in the pharmaceutical industry.\nIntegrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations\noffers a promising new direction in molecular generation. Recent studies like those by [196] and\n[160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLM\u0002based approaches in this domain, including those by [338] and [18], predominantly utilize textual\nSMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The\nemerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image,\nand text, which could more comprehensively capture the intricacies of molecular structures. This\napproach marks a significant shift towards utilizing graph-based information alongside traditional\ntext, enhancing the capability of LLMs in molecular generation. Such advances suggest that future\nresearch should focus more on exploiting the synergy between graph-based molecular representa\u0002tions and the evolving landscape of LLMs to address complex challenges in chemistry and material\nsciences.\n13 Recommender Systems\nThe use of graph representation learning in recommender systems has been drawing increasing\nattention as one of the key strategies for addressing the issue of information overload. With their\nstrong ability to capture high-order connectivity between graph nodes, deep graph representation\nlearning has been shown to be beneficial in enhancing recommendation performance across a\nvariety of recommendation scenarios.\nTypical recommender systems take the observed interactions between users and items and\ntheir fixed features as input, and are intended for making proper predictions on which items a\nspecific user is probably interested in. To formulate, given an user set U, an item set I and the\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 57\nTable 12. Summary of graph models for recommender systems.\nModel Recommendation Task Graph Structure Graph Encoder Representation\nGC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer\nNGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation\nMMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer\nLightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling\nDGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling\nCAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling\nSR-GNN [496] Session-based Transition Graph GGNN Soft-Attention\nGC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention\nFGNN [377] Session-based Session Graph GAT Last-Layer\nGAG [378] Session-based Session Graph GCN Self-Attention\nGCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling\nHyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention\nDHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer\nMBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention\nHCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer\nH3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer\nSTHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling\ninteraction matrix between users and items \ud835\udc4b \u2208 {0, 1}\n|U|\u00d7|I|\n, where \ud835\udc4b\ud835\udc62,\ud835\udc63 indicates there is an\nobserved interaction between user \ud835\udc62 and item \ud835\udc56. The target of GNNs on recommender systems is to\nlearn representations \u210e\ud835\udc62, \u210e\ud835\udc56 \u2208 R\n\ud835\udc51\nfor given \ud835\udc62 and \ud835\udc56. The preference score can further be calculated\nby a similarity function:\n\ud835\udc65\u02c6\ud835\udc62,\ud835\udc56 = \ud835\udc53 (\u210e\ud835\udc62, \u210e\ud835\udc56), (141)\nwhere \ud835\udc53 (\u00b7, \u00b7) is the similarity function, e.g. inner product, cosine similarity, multi-layer perceptrons\nthat takes the representation of \ud835\udc62 and \ud835\udc56 and calculate the preference score \ud835\udc65\u02c6\ud835\udc62,\ud835\udc56.\nWhen it comes to adapting graph representation learning in recommender systems, a key step is\nto construct graph-structured data from the interaction set \ud835\udc4b. Generally, a graph is represented\nas G = {V, E} where V, E denotes the set of vertices and edges respectively. According to the\nconstruction of G, we can categorize the existing works as follows into three parts which are\nintroduced in the following subsections. A summary is provided in Table 12.\n13.1 User-Item Bipartite Graph\n13.1.1 Graph Construction A undirected bipartite graph where the vertex set V = U \u222a I and the\nundirected edge set E = {(\ud835\udc62,\ud835\udc56)|\ud835\udc62 \u2208 U \u2227\ud835\udc56 \u2208 I}. Under this case the graph adjacency can be directly\nobtained from the interaction matrix, thus the optimization target on the user-item bipartite graph\nis equivalent to collaborative filtering tasks such as MF [239] and SVD++ [238].\nThere have been plenty of previous works that applied GNNs on the constructed user-item bipar\u0002tite graphs. GC-MC [27] firstly applies graph convolution networks to user-item recommendation\nand optimizes a graph autoencoder (GAE) to reconstruct interactions between users and items.\nNGCF [470] introduces the concept of Collaborative Filtering (CF) into graph-based recommen\u0002dations by modeling the affinity between neighboring nodes on the interaction graph. MMGCN\n[485] extends the graph-based recommendation to multi-modal scenarios by constructing different\nsubgraphs for each modal. LightGCN [169] improves NGCF by removing the non-linear activation\nfunctions and simplifying the message function. With the development of disentangled representa\u0002tion learning, there are works like DGCF [473] that introduce disentangled graph representation\nlearning to represent users and items from multiple disentangled perspectives. Additionally, having\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n58 W. Ju, et al.\nrealized the limitation of the existing message-passing scheme in capturing collaborative signals,\nCAGCN [480] proposes Common Interacted Ratio (CIR) as a recommendation-oriented topological\nmetric for GNN-based recommender models.\n13.1.2 Graph Propagation Scheme A common practice is to follow the traditional message-passing\nnetworks (MPNNs) and design the graph propagation method accordingly. GC-MC adopts vanilla\nGCNs to encode the user-item bipartite graph. NGCF enhances GCNs by considering the affinity\nbetween users and items. The message function of NGCF from node \ud835\udc57 to \ud835\udc56 is formulated as:\n(\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc57 = \u221a 1\n|N\ud835\udc56| |N\ud835\udc57|\n(\ud835\udc4a1\ud835\udc52\ud835\udc57 +\ud835\udc4a2 (\ud835\udc52\ud835\udc56 \u2299 \ud835\udc52\ud835\udc57))\n\ud835\udc5a\ud835\udc56\u2190\ud835\udc56 = \ud835\udc4a1\ud835\udc52\ud835\udc56\n, (142)\nwhere \ud835\udc4a1,\ud835\udc4a2 are trainable parameters, \ud835\udc52\ud835\udc56 represents \ud835\udc56\u2019s representation from previous layer. The\nmatrix form can be further provided by:\n\ud835\udc38\n(\ud835\udc59) = LeakyReLU( (L + \ud835\udc3c)\ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n1\n+ L\ud835\udc38\n(\ud835\udc59\u22121) \u2299 \ud835\udc38(\ud835\udc59\u22121)\ud835\udc4a\n(\ud835\udc59)\n2\n), (143)\nwhere L represents the Laplacian matrix of the user-item graph. The element-wise product in Eq.\n143 represents the affinity between connected nodes, containing the collaborative signals from\ninteractions.\nHowever, the notable heaviness and burdensome calculation of NGCF\u2019s architecture hinder\nthe model from making faster recommendations on larger graphs. LightGCN solves this issue by\nproposing Light Graph Convolution (LGC), which simplifies the convolution operation with:\n\ud835\udc52\n(\ud835\udc59+1)\n\ud835\udc56\n=\n\u2211\ufe01\n\ud835\udc57 \u2208N\ud835\udc56\n1\n\u221a\ufe01\n|N\ud835\udc56||N\ud835\udc57|\n\ud835\udc52\n(\ud835\udc59)\n\ud835\udc57\n. (144)\nWhen an interaction takes place, e.g. a user clicks a particular item, there could be multiple\nintentions behind the observed interaction. Thus it is necessary to consider the various disentangled\nintentions among users and items. DGCF proposes the cross-intent embedding propagation scheme\non the graph, inspired by the dynamic routing algorithm of capsule networks [394]. To formulate,\nthe propagation process maintains a set of routing logits \u02dc\ud835\udc46\ud835\udc58 (\ud835\udc62,\ud835\udc56) for each user \ud835\udc62. The weighted sum\naggregator to get the representation of \ud835\udc62 can be defined as:\n\ud835\udc62\n\ud835\udc61\n\ud835\udc58\n=\n\u2211\ufe01\n\ud835\udc56\u2208N\ud835\udc62\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) \u00b7 \ud835\udc56\n0\n\ud835\udc58\n(145)\nfor \ud835\udc61-th iteration, where L\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) denotes the Laplacian matrix of \ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56), formulated as:\nL\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56) =\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n\u221a\ufe03\n[\n\u00cd\n\ud835\udc56\n\u2032\u2208N\ud835\udc62\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62,\ud835\udc56\u2032)] \u00b7 [\u00cd\n\ud835\udc62\n\u2032\u2208N\ud835\udc56\n\ud835\udc46\n\ud835\udc61\n\ud835\udc58\n(\ud835\udc62\n\u2032\n,\ud835\udc56)]\n. (146)\n13.1.3 Node Representations After the graph propagation module outputs node-level representa\u0002tions, there are multiple methods to leverage node representations for recommendation tasks. A\nplain solution is to apply a readout function on layer outputs like the concatenation operation used\nby NGCF:\n\ud835\udc52\n\u2217 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc52(0)\n, ..., \ud835\udc52 (\ud835\udc3f)) = \ud835\udc52\n(0)\n\u2225...\u2225\ud835\udc52\n(\ud835\udc3f)\n. (147)\nHowever, the readout function among layers would neglect the relationship between the target\nitem and the current user. A general solution is to use the attention mechanism [451] to reweight\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 59\nand aggregate the node representations. SR-GNN adapts soft-attention mechanism to model the\nitem-item relationship:\n\ud835\udefc\ud835\udc56 = q\n\ud835\udc47\n\ud835\udf0e(\ud835\udc4a1\ud835\udc52\ud835\udc61 +\ud835\udc4a2\ud835\udc52\ud835\udc56 + \ud835\udc50),\n\ud835\udc60\ud835\udc54 =\n\ud835\udc5b\u2211\ufe01\u22121\n\ud835\udc56=1\n\ud835\udefc\ud835\udc56\ud835\udc52\ud835\udc56,\n(148)\nwhere q, \ud835\udc4a1, \ud835\udc4a2 are trainable matrices.\nSome methods focus on exploiting information from multiple graph structures. A common\npractice is contrastive learning, which maximizes the mutual information between hidden repre\u0002sentations from several views. HCCF leverage InfoNCE loss as the estimator of mutual information,\ngiven a pair of representation \ud835\udc67\ud835\udc56, \u0393\ud835\udc56 for node \ud835\udc56, controlled by temperature parameter \ud835\udf0f:\nL\ud835\udc3c\ud835\udc5b \ud835\udc53 \ud835\udc5c\ud835\udc41\ud835\udc36\ud835\udc38 (\ud835\udc56) = \u2212 log exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56))/\ud835\udf0f\n\u00cd\n\ud835\udc56\n\u2032\u2260\ud835\udc56 exp(\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52 (\ud835\udc67\ud835\udc56\n, \u0393\ud835\udc56\n\u2032 ))/\ud835\udf0f\n. (149)\nBesides InfoNCE, there exist several other ways to combine node representations from different\nviews. For instance, MBHT applies an attention mechanism to fuse multiple semantics, DisenPOI\nadapts bayesian personalized ranking loss (BPR) [384] as a soft estimator for contrastive learning,\nand KBGNN applies pair-wise similarities to ensure the consistency from two views.\n13.2 Transition Graph\n13.2.1 Transition Graph Construction Since sequence-based recommendation (SR) is one of the\nfundamental problems in recommender systems, some researches focus on modeling the sequential\ninformation with GNNs. A commonly applied way is to construct transition graphs based on each\ngiven sequence according to the clicking sequence by a user. To formulate, given a user \ud835\udc62\u2019s clicking\nsequence \ud835\udc60\ud835\udc62 = [\ud835\udc56\ud835\udc62,1,\ud835\udc56\ud835\udc62,2, ...,\ud835\udc56\ud835\udc62,\ud835\udc5b] containing \ud835\udc5b items, noting that there could be duplicated items, the\nsequential graph is constructed via G\ud835\udc60 = {SET(\ud835\udc60\ud835\udc62), E}, where \u2200\n\ud835\udc56\ud835\udc57,\ud835\udc56\ud835\udc58\n\u2208 E indicates there exists\na successive transition from \ud835\udc56\ud835\udc57 to \ud835\udc56\ud835\udc58 . Since G\ud835\udc60 are directed graphs, a widely used way to depict\ngraph connectivity is by building the connection matrix \ud835\udc34\ud835\udc60 \u2208 R\n\ud835\udc5b\u00d72\ud835\udc5b\n. \ud835\udc34\ud835\udc60is the combination of two\nadjacency matrices \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n], which denotes the normalized node degrees of incoming\nand outgoing edges in the session graph respectively.\nThe proposed transition graphs that obtain user behavior patterns have been demonstrated\nimportant to session-based recommendations [263, 291]. SR-GNN and GC-SAN [496, 516] propose\nto leverage transition graphs and apply attention-based GNNs to capture the sequential information\nfor session-based recommendation. FGNN [377] formulates the recommendation within a session\nas a graph classification problem to predict the next item for an anonymous user. GAG [378] and\nGCE-GNN [482] further extend the model to capture global embeddings among multiple session\ngraphs.\n13.2.2 Session Graph Propagation Since the session graphs are directed item graphs, there have\nbeen multiple session graph propagation methods to obtain node representations on session graphs.\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n60 W. Ju, et al.\nSR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information\nfrom a given session graph adjacency \ud835\udc34\ud835\udc60 = [\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n;\ud835\udc34\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n] and item embedding set {\ud835\udc52\ud835\udc56 }:\n\ud835\udc4e\ud835\udc61 = \ud835\udc34\ud835\udc60 [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121]\n\ud835\udc47\ud835\udc3b + \ud835\udc4f, (150)\n\ud835\udc67\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc67\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc67\ud835\udc52\ud835\udc61\u22121), (151)\n\ud835\udc5f\ud835\udc61 = \ud835\udf0e(\ud835\udc4a\ud835\udc5f\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5f\ud835\udc52\ud835\udc61\u22121), (152)\n\ud835\udc52\u02dc\ud835\udc61 = tanh(\ud835\udc4a\ud835\udc5c\ud835\udc4e\ud835\udc61 + \ud835\udc48\ud835\udc5c (\ud835\udc5f\ud835\udc61 \u2299 \ud835\udc52\ud835\udc61\u22121)), (153)\n\ud835\udc52\ud835\udc61 = (1 \u2212 \ud835\udc67\ud835\udc61) \u2299 \ud835\udc52\ud835\udc61\u22121 + \ud835\udc67\ud835\udc61\ud835\udc52\u02dc\ud835\udc61, (154)\nwhere \ud835\udc4a s and \ud835\udc48 s are trainable parameters. GC-SAN extend GGNN by calculating initial state \ud835\udc4e\ud835\udc61\nseparately to better exploit transition information:\n\ud835\udc4e\ud835\udc61 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc34\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc56\ud835\udc5b)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc56\ud835\udc5b)\n), \ud835\udc34(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc60\n( [\ud835\udc521, ..., \ud835\udc52\ud835\udc61\u22121\ud835\udc4a\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n\ud835\udc4e ] + \ud835\udc4f\n(\ud835\udc5c\ud835\udc62\ud835\udc61)\n)). (155)\n13.3 HyperGraph\n13.3.1 Hypergraph Topology Construction Motivated by the idea of modeling hyper-structures\nand high-order correlation among nodes, hypergraphs [119] are proposed as extensions of the\ncommonly used graph structures. For graph-based recommender systems, a common practice is\nto construct hyper structures among the original user-item bipartite graphs. To be specific, an\nincidence matrix of a graph with vertex set V is presented as a binary matrix \ud835\udc3b \u2208 {0, 1}\n|V |\u00d7 | E |\n,\nwhere E represents the set of hyperedges. Each entry \u210e(\ud835\udc63, \ud835\udc52) of \ud835\udc3b depicts the connectivity between\nvertex \ud835\udc63 and hyperedge \ud835\udc52:\n\u210e(\ud835\udc63, \ud835\udc52) =\n(\n1 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2208 \ud835\udc52\n0 \ud835\udc56 \ud835\udc53 \ud835\udc63 \u2209 \ud835\udc52\n. (156)\nGiven the formulation of hypergraphs, the degrees of vertices and hyperedges of \ud835\udc3b can then be\ndefined with two diagonal matrices \ud835\udc37\ud835\udc63 \u2208 N\n|V |\u00d7 |V | and \ud835\udc37\ud835\udc52 \u2208 N| E |\u00d7 | E |, where\n\ud835\udc37\ud835\udc63 (\ud835\udc56;\ud835\udc56) =\n\u2211\ufe01\n\ud835\udc52\u2208 E\n\u210e(\ud835\udc63\ud835\udc56, \ud835\udc52), \ud835\udc37\ud835\udc52 (\ud835\udc57; \ud835\udc57) =\n\u2211\ufe01\n\ud835\udc63\u2208V\n\u210e(\ud835\udc63, \ud835\udc52\ud835\udc57). (157)\nThe development of Hypergraph Neural Networks (HGNNs) [119, 188, 598] have shown to\nbe capable of capturing the high-order connectivity between nodes. HyperRec [463] firstly at\u0002tempts to leverage hypergraph structures for sequential recommendation by connecting items\nwith hyperedges according to the interactions with users during different time periods. DHCF\n[198] proposes to construct hypergraphs for users and items respectively based on certain rules, to\nexplicitly capture the collaborative similarities via HGNNs. MBHT [532] combines hypergraphs\nwith a low-rank self-attention mechanism to capture the dynamic heterogeneous relationships\nbetween users and items. HCCF [505] uses the contrastive information between hypergraph and\ninteraction graph to enhance the recommendation performance. To extend the model\u2019s ability to\nmulti-domain categories of items, H3Trans [523] incorporates two hyperedge-based modules and\nleverages hierarchical hypergraph propagation to transfer from domains. STHGCN [524] formulates\na spatio-temporal hypergraph structure for POI recommendation.\n13.3.2 Hyper Graph Message Passing With the development of HGNNs, previous works have\nproposed different variants of HGNN to better exploit hypergraph structures. A classic high-order\nhyper convolution process on a fixed hypergraph G = {V, E} with hyper adjacency \ud835\udc3b is given by:\n\ud835\udc54 \u2605\ud835\udc4b = \ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc3b\ud835\udc37\u22121\n\ud835\udc52 \ud835\udc3b\n\ud835\udc47\ud835\udc37\n\u22121/2\n\ud835\udc63 \ud835\udc4b\u0398, (158)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 61\nwhere \ud835\udc37\ud835\udc63, \ud835\udc37\ud835\udc52 are degree matrices of nodes and hyperedges, \u0398 denotes the convolution kernel. For\nhyper adjacency matrix \ud835\udc3b, DHCF refers to a rule-based hyperstructure via k-order reachable rule,\nwhere nodes in the same hyperedge group are k-order reachable to each other:\n\ud835\udc34\n\ud835\udc58\n\ud835\udc62 = min(1, power(\ud835\udc34 \u00b7 \ud835\udc34\n\ud835\udc47\n, \ud835\udc58)), (159)\nwhere \ud835\udc34 denotes the graph adjacency matrix. By considering the situations where \ud835\udc58 = 1, 2, the\nmatrix formulation of the hyper connectivity of users and items is calculated with:\n(\n\ud835\udc3b\ud835\udc62 = \ud835\udc34\u2225 (\ud835\udc34(\ud835\udc34\n\ud835\udc47\ud835\udc34))\n\ud835\udc3b\ud835\udc56 = \ud835\udc34\n\ud835\udc47\n\u2225 (\ud835\udc34\n\ud835\udc47\n(\ud835\udc34\ud835\udc34\ud835\udc47))\n, (160)\nwhich depicts the dual hypergraphs for users and items.\nHCCF proposes to construct a learnable hypergraph to depict the global dependencies between\nnodes on the interaction graph. To be specific, the hyperstructure is factorized with two low-rank\nembedding matrices to achieve model efficiency:\n\ud835\udc3b\ud835\udc62 = \ud835\udc38\ud835\udc62 \u00b7\ud835\udc4a\ud835\udc62, \ud835\udc3b\ud835\udc63 = \ud835\udc38\ud835\udc63 \u00b7\ud835\udc4a\ud835\udc63 . (161)\n13.4 Other Graphs\nSince there are a variety of recommendation scenarios, several tailored designed graph structures\nhave been proposed accordingly, to better exploit the domain information from different scenarios.\nFor instance, CKE [564] and MKR [462] introduce Knowledge graphs to enhance graph recommen\u0002dation. GSTN [484], KBGNN [219], DisenPOI [373] and Diff-POI [374] propose to build geographical\ngraphs based on the distance between Point-of-Interests (POIs) to better model the locality of users\u2019\nvisiting patterns. TGSRec [109] and DisenCTR [475] empower the user-item interaction graphs with\ntemporal sampling between layers to obtain sequential information from static bipartite graphs.\n13.5 Summary\nThis section introduces the application of different kinds of graph neural networks in recommender\nsystems and can be summarized as follows:\n\u2022 Graph Constructions. There are multiple options for constructing graph-structured data\nfor a variety of recommendation tasks. For instance, the user-item bipartite graphs reveal\nthe high-order collaborative similarity between users and items, and the transition graph\nis suitable for encoding sequential information in clicking history. These diversified graph\nstructures provide different views for node representation learning on users and items, and\ncan be further used for downstream ranking tasks.\n\u2022 Challenges and Limitations. Though the superiority of graph-structured data and GNNs\nagainst traditional methods has been widely illustrated, there are still challenges unsolved.\nFor example, the computational cost of graph methods is normally expensive and thus\nunacceptable in real-world applications. The data sparsity and cold-started issue in graph\nrecommendation remains to be explored as well.\n\u2022 Future Works. In the future, an efficient solution for applying GNNs in recommendation\ntasks is expected. There are also some attempts [109, 372, 475] on incorporating temporal\ninformation in graph representation learning for sequential recommendation tasks.\n14 Traffic Analysis\nIntelligent Transportation Systems (ITS) are essential for safe, reliable, and efficient transportation\nin smart cities, serving the daily commuting and traveling needs of millions of people. To support\nITS, advanced modeling and analysis techniques are necessary, and Graph Neural Networks (GNNs)\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n62 W. Ju, et al.\nare a promising tool for traffic analysis. GNNs can effectively model spatial correlations, making\nthem well-suited for analyzing complex transportation networks. As such, GNNs have garnered\nsignificant interest in the traffic domain for their ability to provide insights into traffic patterns and\nbehaviors [260].\nIn this section, we first conclude the main GNN research directions in the traffic domain, and\nthen we summarize the typical graph construction processes in different traffic scenes and datasets.\nFinally, we list the classical GNN workflows for dealing with tasks in traffic networks. A summary\nis provided in Table 13.\n14.1 Research Directions in Traffic Domain\nWe summarize main GNN research directions in the traffic domain as follows,\n\u2022 Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role in ITS [90, 381],\nwhich involves leveraging spatial-temporal data collected by various sensors to gain insights\ninto future traffic patterns and behaviors. Classic methods, like autoregressive integrated\nmoving average (ARIMA) [36], support vector machine (SVM) [171] and recurrent neural\nnetworks (RNN) [76] can only model time series separately without considering their spatial\nconnections. To address this issue, graph neural networks (GNNs) have emerged as a powerful\napproach for traffic forecasting due to their strong ability of modeling complex graph\u0002structured correlations [40, 202, 277, 353, 383, 506, 592].\n\u2022 Trajectory Prediction. Trajectory prediction is a crucial task in various applications, such\nas autonomous driving and traffic surveillance, which aims to forecast future positions of\nagents in the traffic scene. However, accurately predicting trajectories can be challenging, as\nthe behavior of an agent is influenced not only by its own motion but also by interactions\nwith surrounding objects. To address this challenge, Graph Neural Networks (GNNs) have\nemerged as a promising tool for modeling complex interactions in trajectory prediction\n[44, 345, 432, 600]. By representing the scene as a graph, where each node corresponds to an\nagent and the edges capture interactions between them, GNNs can effectively capture spatial\ndependencies and interactions between agents. This makes GNNs well-suited for predicting\ntrajectories that accurately capture the behavior of agents in complex traffic scenes.\n\u2022 Traffic Anomaly Detection. Anomaly detection is an essential support for ITS. There are\nlots of traffic anomalies in daily transportation systems, for example, traffic accidents, extreme\nweather and unexpected situations. Handling these traffic anomalies timely can improve the\nservice quality of public transportation. The main trouble of traffic anomaly detection is the\nhighly twisted spatial-temporal characteristics of traffic data. The criteria and influence of\ntraffic anomaly vary among locations and times. GNNs have been introduced and achieved\nsuccess in this domain [66, 85, 86, 565].\n\u2022 Others. Traffic demand prediction targets at estimating the future number of traveling at\nsome location. It is of vital and practical significance in the resource scheduling for ITS. By\nusing GNNs, the spatial dependencies of demands can be revealed [530, 535]. What is more,\nurban vehicle emission analysis is also considered in recent work, which is closely related to\nenvironmental protection and gains increasing researcher attention [521].\n14.2 Traffic Graph Construction\n14.2.1 Traffic Graph The traffic network is represented as a graph G = (\ud835\udc49 , \ud835\udc38, \ud835\udc34), where \ud835\udc49 is the\nset of \ud835\udc41 traffic nodes, \ud835\udc38 is the set of edges, and \ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc41 is an adjacency matrix representing the\nconnectivity of \ud835\udc41 nodes. In the traffic domain, \ud835\udc49 usually represents a set of physical nodes, like\ntraffic stations or traffic sensors. The features of nodes typically depend on the specific task. Take\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 63\nTable 13. Summary of graph models for traffic analysis.\nModels Tasks Adjcency matrices GNN types Temporal modules\nSTGCN[545] Traffic Flow Forecasting Fixed Matrix GCN TCN\nDCRNN[275] Traffic Flow Forecasting Fixed Matrix ChebNet RNN\nAGCRN [19] Traffic Flow Forecasting Dynamic Matrix GCN GRU\nASTGCN [159] Traffic Flow Forecasting Fixed Matrix GAT Attention&TCN\nGraphWaveNet [500] Traffic Flow Forecasting Dynamic Matrix GCN Gated-TCN\nSTSGCN [422] Traffic Flow Forecasting Dynamic Matrix GCN Cropping\nLSGCN [187] Traffic Flow Forecasting Fixed Matrix GAT GLU\nGAC-Net [424] Traffic Flow Forecasting Fixed Matrix GAT Gated-TCN\nSTGODE [112] Traffic Flow Forecasting Fixed Matrix Graph ODE TCN\nSTG-NCDE [70] Traffic Flow Forecasting Dynamic Matrix GCN NCDE\nDDGCRN [488] Traffic Flow Forecasting Dynamic Matrix GAT RNN\nMS-ASTN [467] OD Flow Forecasting OD Matrix GCN LSTM\nSocial-STGCNN [345] Trajectory Prediction Fixed Matrix GCN TXP-CNN\nRSBG [432] Trajectory Prediction Dynamic Matrix GCN LSTM\nATG [555] Trajectory Prediction Fixed Matrix GODE NODE\nSTGAN [86] Anomaly Detection Fixed Matrix GCN GRU\nDMVST-VGNN [206] Traffic Demand Prediction Fixed Matrix GAT GLU\nDST-GNN [185] Traffic Demand Prediction Dynamic Matrix GCN Transformer\nTC-SGC [355] Traffic Speed Prediction Fixed Matrix GCN GRU\ntraffic flow forecasting as an example. The features are the traffic flows, i.e., the historical time\nseries of nodes. The traffic flow can be represented as a flow matrix \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc47\n, where \ud835\udc41 is the\nnumber of traffic nodes and \ud835\udc47 is the length of historical series, and \ud835\udc4b\ud835\udc5b\ud835\udc61 denotes the traffic flow of\nnode \ud835\udc5b at time \ud835\udc61. The goal of traffic flow forecasting is to learn a mapping function \ud835\udc53 to predict the\ntraffic flow during future \ud835\udc47\n\u2032\nsteps given the historical \ud835\udc47 step information, which can be formulated\nas follows:\n\u0002\n\ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +1, \ud835\udc4b:,\ud835\udc61\u2212\ud835\udc47 +2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61; G\n\u0003 \ud835\udc53\n\u2212\u2192 \u0002\ud835\udc4b:,\ud835\udc61+1, \ud835\udc4b:,\ud835\udc61+2, \u00b7 \u00b7 \u00b7 , \ud835\udc4b:,\ud835\udc61+\ud835\udc47\n\u2032\n\u0003\n. (162)\n14.2.2 Graph Construction Constructing a graph to describe the interactions among traffic nodes,\ni.e., the design of the adjacency matrix \ud835\udc34, is the key part of traffic analysis. The mainstream designs\ncan be divided into two categories, fixed matrix and dynamic matrix.\nFixed matrix. Lots of works assume that the correlations among traffic nodes are fixed and\nconstant over time, and they design a fixed and pre-defined adjacency matrix to capture the spatial\ncorrelation. Here we list several common choices of fixed adjacency matrix.\nThe connectivity matrix is the most natural construction way. It relies on the support of\nroad map data. The element of the connectivity matrix is defined as 1 if two nodes are physically\nconnected and 0 otherwise. This binary format is convenient to construct and easy to interpret.\nThe distance-based matrix is also a common choice, which shows the connection between two\nnodes more precisely. The elements of the matrix are defined as the function of distance between\ntwo nodes (driving distance or geographical distance). A typical way is to use the threshold Gaussian\nfunction as follows,\n\ud835\udc34\ud835\udc56\ud835\udc57 =\n(\nexp(\u2212\ud835\udc51\n2\n\ud835\udc56 \ud835\udc57\n\ud835\udf0e\n2 ), \ud835\udc51\ud835\udc56\ud835\udc57  \ud835\udf16\n, (163)\nwhere \ud835\udc51\ud835\udc56\ud835\udc57 is the distance between node \ud835\udc56 and \ud835\udc57, and \ud835\udf0e and \ud835\udf16 are two hyperparameters to control the\ndistribution and the sparsity of the matrix.\nAnother kind of fixed adjacency matrix is the similarity-based matrix. In fact, a similarity\nmatrix is not an adjacency matrix to some extent. It is constructed according to the similarity of\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\n64 W. Ju, et al.\ntwo nodes, which means the neighbors in the similarity graph may be far away in the real world.\nThere are various similarity metrics. For example, many works measure the similarity of two nodes\nby their functionality, e.g., the distribution of surrounding points of interest (POIs). The assumption\nbehind this is that nodes that share similar functionality may share similar traffic patterns. We\ncan also define the similarity through the historical flow patterns. To compute the similarity of\ntwo-time series, a common practice is to use Dynamic Time Wrapping (DTW) algorithm [350],\nwhich is superior to other metrics due to its sensitivity to shape similarity rather than point-wise\nsimilarity. Specifically, given two time series \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc5b) and \ud835\udc4c = (\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc5b), DTW is\na dynamic programming algorithm defined as\n\ud835\udc37(\ud835\udc56, \ud835\udc57) = \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) + min (\ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57), \ud835\udc37(\ud835\udc56, \ud835\udc57 \u2212 1), \ud835\udc37(\ud835\udc56 \u2212 1, \ud835\udc57 \u2212 1)) , (164)\nwhere \ud835\udc37(\ud835\udc56, \ud835\udc57) represents the shortest distance between subseries \ud835\udc4b = (\ud835\udc651, \ud835\udc652, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc56) and \ud835\udc4c =\n(\ud835\udc661, \ud835\udc662, \u00b7 \u00b7 \u00b7 , \ud835\udc66\ud835\udc57), and\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc57) is some distance metric like absolute distance. As a result,\ud835\udc37\ud835\udc47\ud835\udc4a (\ud835\udc4b, \ud835\udc4c) =\n\ud835\udc37(\ud835\udc5b, \ud835\udc5b) is set as the final distance between \ud835\udc4b and \ud835\udc4c, which better reflects the similarity of the\ntwo-time series compared to the Euclidean distance.\nDynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect complete\ninformation of spatial correlations. The dynamic adaptive matrix is proposed to solve the issue.\nThe dynamic matrix is learned from input data automatically. To achieve the best prediction\nperformance, the dynamic matrix will manage to infer the hidden correlations among nodes, more\nthan those physical connections.\nA typical practice is learning adjacency matrix from node embeddings [19]. Let \ud835\udc38\ud835\udc34 \u2208 R\n\ud835\udc41 \u00d7\ud835\udc51 be a\nlearnable node embedding dictionary, where each row of \ud835\udc38\ud835\udc34 represents the embedding of a node,\n\ud835\udc41 and \ud835\udc51 denote the number of nodes and the dimension of embeddings respectively. The graph\nadjacency matrix is defined as the similarities among node embeddings,\n\ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 = \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \u0010\n\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 (\ud835\udc38\ud835\udc34 \u00b7 \ud835\udc38\n\ud835\udc47\n\ud835\udc34\n)\n\u0011\n, (165)\nwhere \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 function is to perform row-normalization, and \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 is the Laplacian matrix.\n14.3 Typical GNN Frameworks in Traffic Domain\nSpatial Temporal Graph Convolution Network (STGCN) [545]. STGCN is a pioneering work in the\nspatial-temporal GNN domain. It utilizes graph convolution to capture spatial features, and deploys\na gated causal convolution to extract temporal patterns. Specifically, the graph convolution and\ntemporal convolution are defined as follows,\n\u0398 \u2217G \ud835\udc65 = \ud835\udf03 (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc65 = \ud835\udf03 (\ud835\udc37\u02dc \u2212\n1\n2\ud835\udc34\u02dc\ud835\udc37\u02dc \u2212\n1\n2 )\ud835\udc65, (166)\n\u0393 \u2217T \ud835\udc66 = \ud835\udc43 \u2299 \ud835\udf0e(\ud835\udc44), (167)\nwhere \u0398 is the parameter of graph convolution, \ud835\udc43 and \ud835\udc44 are the outputs of a 1-d convolution\nalong the temporal dimension. The sigmoid gate \ud835\udf0e(\ud835\udc44) controls how the states of \ud835\udc43 are relevant\nfor discovering hidden temporal patterns. In order to fuse features from both spatial and temporal\ndimension, the spatial convolution layer and the temporal convolution layer are combined to\nconstruct a spatial temporal block to jointly deal with graph-structured time series, and more blocks\ncan be stacked to achieve a more scalable and complex model.\nDiffusion Convolutional Recurrent Neural Network (DCRNN) [275]. DCRNN is a representative\nsolution combining graph convolution networks with recurrent neural networks. It captures spatial\ndependencies by bidirectional random walks on the graph. The diffusion convolution operation on\nJ. ACM, Vol. 1, No. 1, Article . Publication date: February 2024.\nA Comprehensive Survey on Deep Graph Representation Learning 65\na graph is defined as:\n\ud835\udc4b \u2217G \ud835\udc53\ud835\udf03 =\n\u2211\ufe01\n\ud835\udc3e\n\ud835\udc58=0\n\u0010\n\ud835\udf03\ud835\udc58,1 (\ud835\udc37\n\u22121\n\ud835\udc42 \ud835\udc34)\n\ud835\udc58 + \ud835\udf03\ud835\udc58,2 (\ud835\udc37\u22121\n\ud835\udc3c \ud835\udc34)\n\ud835\udc58\n\u0011\n\ud835\udc4b, (168)\nwhere \ud835\udf03 are parameters for the convolution filter, and \ud835\udc37\n\u22121\n\ud835\udc42\n\ud835\udc34, \ud835\udc37\u22121\n\ud835\udc3c\n\ud835\udc34 represent the bidirectional\ndiffusion processes respectively. In term of temporal dependency, DCRNN utilizes Gated Recurrent\nUnits (GRU), and replace the linear transformation in the GRU with the diffusion convolution as\nfollows,\n\ud835\udc5f\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc5f \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc5f), (169)\n\ud835\udc62\n(\ud835\udc61) = \ud835\udf0e(\u0398\ud835\udc62 \u2217G [\ud835\udc4b(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61\u22121)] + \ud835\udc4f\ud835\udc62), (170)\n\ud835\udc36\n(\ud835\udc61) = tanh(\u0398\ud835\udc36 \u2217G [\ud835\udc4b(\ud835\udc61)\n, (\ud835\udc5f\n(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121)\n] + \ud835\udc4f\ud835\udc50 ), (171)\n\ud835\udc3b\n(\ud835\udc61) = \ud835\udc62(\ud835\udc61) \u2299 \ud835\udc3b(\ud835\udc61\u22121) + (1 \u2212 \ud835\udc62(\ud835\udc61)\n) \u2299 \ud835\udc36\n(\ud835\udc61)\n, (172)\nwhere \ud835\udc4b\n(\ud835\udc61)\n, \ud835\udc3b(\ud835\udc61) denote the input and output at time \ud835\udc61, \ud835\udc5f\n(\ud835\udc61)\n, \ud835\udc62(\ud835\udc61)are the reset and update gates\nrespectively, and \u0398\ud835\udc5f, \u0398\ud835\udc62, \u0398\ud835\udc36 are parameters of convolution filters. Moreover, DCRNN employs a\nsequence-to-sequence architecture to predict future series. Both the encoder and the decoder are\nconstructed with diffusion convolutional recurrent layers. The historical time series are fed into\nthe encoder and the predictions are generated by the decoder. The scheduled sampling technique is\nutilized to solve the discrepancy problem between training and test distribution.\nAdaptive Graph Convolutional Recurrent Network (AGCRN) [19]. The focuses of AGCRN are\ntwo-fold. On the one hand, it argues that the temporal patterns are diversified and thus parameter\u0002sharing for each node is inferior; on the other hand, it proposes that the pre-defined graph may be\nintuitive and incomplete for the specific prediction task. To mitigate the two issues, it designs a\nNode Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic\nseries, and a Data Adaptive Graph Generation (DAGG) module to infer the hidden correlations\namong nodes from data and to generate the graph during training. Specifically, the NAPL module\nis defined as follows,\n\ud835\udc4d = (\ud835\udc3c\ud835\udc5b + \ud835\udc37\n\u2212\n1\n2\ud835\udc34\ud835\udc37\u2212\n1\n2 )\ud835\udc4b \ud835\udc38G\ud835\udc4aG + \ud835\udc38G\ud835\udc4f G, (173)\nwhere \ud835\udc4b \u2208 R\n\ud835\udc41 \u00d7\ud835\udc36 is the input feature, \ud835\udc38G \u2208 R\ud835\udc41 \u00d7\ud835\udc51\nis a node embedding dictionary, \ud835\udc51 is the\nembedding dimension (\ud835\udc51",
    "openalex_id": "https://openalex.org/W4392203343",
    "title": "A Comprehensive Survey on Deep Graph Representation Learning",
    "publication_date": "2024-02-27",
    "cited_by_count": 55,
    "topics": "Graph Neural Network Models and Applications, Statistical Mechanics of Complex Networks, Recommender System Technologies",
    "keywords": "Feature learning, Representation Learning, Knowledge Graph Embedding, Signal Processing on Graphs, Network Embedding, Deep Learning, Graph embedding",
    "concepts": "Computer science, Deep learning, Feature learning, Artificial intelligence, Graph, Theoretical computer science, Categorization, Embedding, Graph embedding, Machine learning",
    "pdf_urls_by_priority": [
      "https://arxiv.org/pdf/2304.05055"
    ],
    "text_type": "full_text",
    "successful_pdf_url": "https://arxiv.org/pdf/2304.05055",
    "referenced_works": [
      "https://openalex.org/W103666676",
      "https://openalex.org/W1510073064",
      "https://openalex.org/W1520469672",
      "https://openalex.org/W1614298861",
      "https://openalex.org/W1662382123",
      "https://openalex.org/W1816257748",
      "https://openalex.org/W1888005072",
      "https://openalex.org/W1924770834",
      "https://openalex.org/W1954735160",
      "https://openalex.org/W1959608418",
      "https://openalex.org/W1991252559",
      "https://openalex.org/W1992787746",
      "https://openalex.org/W1993046136",
      "https://openalex.org/W1994389483",
      "https://openalex.org/W1996058270",
      "https://openalex.org/W2006698588",
      "https://openalex.org/W2008056655",
      "https://openalex.org/W2009233867",
      "https://openalex.org/W2022638422",
      "https://openalex.org/W2027482274",
      "https://openalex.org/W2042123098",
      "https://openalex.org/W2053186076",
      "https://openalex.org/W2054141820",
      "https://openalex.org/W2056609785",
      "https://openalex.org/W2062340319",
      "https://openalex.org/W2076498053",
      "https://openalex.org/W2090891622",
      "https://openalex.org/W2095932468",
      "https://openalex.org/W2100495367",
      "https://openalex.org/W2101491865",
      "https://openalex.org/W2110242546",
      "https://openalex.org/W2113052721",
      "https://openalex.org/W2114704115",
      "https://openalex.org/W2115627867",
      "https://openalex.org/W2116341502",
      "https://openalex.org/W2121406124",
      "https://openalex.org/W2128332575",
      "https://openalex.org/W2140310134",
      "https://openalex.org/W2142498761",
      "https://openalex.org/W2142535891",
      "https://openalex.org/W2145658888",
      "https://openalex.org/W2148950790",
      "https://openalex.org/W2152184085",
      "https://openalex.org/W2152630148",
      "https://openalex.org/W2152825437",
      "https://openalex.org/W2153959628",
      "https://openalex.org/W2156718197",
      "https://openalex.org/W2158787690",
      "https://openalex.org/W2170057991",
      "https://openalex.org/W2184148260",
      "https://openalex.org/W2262123273",
      "https://openalex.org/W2319902168",
      "https://openalex.org/W2338678442",
      "https://openalex.org/W2387462954",
      "https://openalex.org/W2393319904",
      "https://openalex.org/W2415243320",
      "https://openalex.org/W2461620095",
      "https://openalex.org/W2481151430",
      "https://openalex.org/W2488133945",
      "https://openalex.org/W2493343568",
      "https://openalex.org/W2509893387",
      "https://openalex.org/W2520633135",
      "https://openalex.org/W2529996553",
      "https://openalex.org/W2531327146",
      "https://openalex.org/W2550925836",
      "https://openalex.org/W2551706664",
      "https://openalex.org/W2565684601",
      "https://openalex.org/W2594183968",
      "https://openalex.org/W2594899909",
      "https://openalex.org/W2602753196",
      "https://openalex.org/W2604738573",
      "https://openalex.org/W2606202972",
      "https://openalex.org/W2612872092",
      "https://openalex.org/W2618530766",
      "https://openalex.org/W2624407581",
      "https://openalex.org/W2700550412",
      "https://openalex.org/W2735246657",
      "https://openalex.org/W2743104969",
      "https://openalex.org/W2743930630",
      "https://openalex.org/W2749279690",
      "https://openalex.org/W2754490690",
      "https://openalex.org/W2767094836",
      "https://openalex.org/W2767404761",
      "https://openalex.org/W2772486182",
      "https://openalex.org/W2773515559",
      "https://openalex.org/W2788134583",
      "https://openalex.org/W2788775653",
      "https://openalex.org/W2788919350",
      "https://openalex.org/W2793544332",
      "https://openalex.org/W2798621783",
      "https://openalex.org/W2800415562",
      "https://openalex.org/W2803526748",
      "https://openalex.org/W2809279178",
      "https://openalex.org/W2809307135",
      "https://openalex.org/W2809435178",
      "https://openalex.org/W2884209963",
      "https://openalex.org/W2888164077",
      "https://openalex.org/W2888192920",
      "https://openalex.org/W2889337896",
      "https://openalex.org/W2892880750",
      "https://openalex.org/W2895744665",
      "https://openalex.org/W2895884529",
      "https://openalex.org/W2896202861",
      "https://openalex.org/W2896457183",
      "https://openalex.org/W2897862648",
      "https://openalex.org/W2897978524",
      "https://openalex.org/W2901454299",
      "https://openalex.org/W2903871660",
      "https://openalex.org/W2905432015",
      "https://openalex.org/W2907230994",
      "https://openalex.org/W2907492528",
      "https://openalex.org/W2911286998",
      "https://openalex.org/W2912323206",
      "https://openalex.org/W2912351665",
      "https://openalex.org/W2913015533",
      "https://openalex.org/W2913350752",
      "https://openalex.org/W2913825337",
      "https://openalex.org/W2914989158",
      "https://openalex.org/W2916446912",
      "https://openalex.org/W2944538680",
      "https://openalex.org/W2948684689",
      "https://openalex.org/W2948729509",
      "https://openalex.org/W2949103145",
      "https://openalex.org/W2949208225",
      "https://openalex.org/W2949243165",
      "https://openalex.org/W2949865801",
      "https://openalex.org/W2950898568",
      "https://openalex.org/W2951659295",
      "https://openalex.org/W2951970475",
      "https://openalex.org/W2959300817",
      "https://openalex.org/W2962756421",
      "https://openalex.org/W2962810718",
      "https://openalex.org/W2963017945",
      "https://openalex.org/W2963066159",
      "https://openalex.org/W2963084622",
      "https://openalex.org/W2963224980",
      "https://openalex.org/W2963241951",
      "https://openalex.org/W2963341924",
      "https://openalex.org/W2963358464",
      "https://openalex.org/W2963410212",
      "https://openalex.org/W2963456618",
      "https://openalex.org/W2963521729",
      "https://openalex.org/W2963639956",
      "https://openalex.org/W2963664410",
      "https://openalex.org/W2963703618",
      "https://openalex.org/W2963726920",
      "https://openalex.org/W2963919031",
      "https://openalex.org/W2964015378",
      "https://openalex.org/W2964044287",
      "https://openalex.org/W2964051675",
      "https://openalex.org/W2964113829",
      "https://openalex.org/W2964321699",
      "https://openalex.org/W2964568038",
      "https://openalex.org/W2964583308",
      "https://openalex.org/W2964926209",
      "https://openalex.org/W2965341826",
      "https://openalex.org/W2965857891",
      "https://openalex.org/W2966357564",
      "https://openalex.org/W2966683369",
      "https://openalex.org/W2966841471",
      "https://openalex.org/W2971126534",
      "https://openalex.org/W2971220558",
      "https://openalex.org/W2979750740",
      "https://openalex.org/W2979845147",
      "https://openalex.org/W2981536126",
      "https://openalex.org/W2981790137",
      "https://openalex.org/W2982108874",
      "https://openalex.org/W2982880755",
      "https://openalex.org/W2986423110",
      "https://openalex.org/W2986466936",
      "https://openalex.org/W2986515219",
      "https://openalex.org/W2988115728",
      "https://openalex.org/W2989285747",
      "https://openalex.org/W2992586577",
      "https://openalex.org/W2992613109",
      "https://openalex.org/W2994860160",
      "https://openalex.org/W2996635575",
      "https://openalex.org/W2996847713",
      "https://openalex.org/W2996910665",
      "https://openalex.org/W2997128522",
      "https://openalex.org/W2997574889",
      "https://openalex.org/W2997785591",
      "https://openalex.org/W2997997679",
      "https://openalex.org/W2998004401",
      "https://openalex.org/W2998122931",
      "https://openalex.org/W2998496395",
      "https://openalex.org/W2998604091",
      "https://openalex.org/W2998702685",
      "https://openalex.org/W3000301417",
      "https://openalex.org/W3000478925",
      "https://openalex.org/W3000577518",
      "https://openalex.org/W3000716014",
      "https://openalex.org/W3005552578",
      "https://openalex.org/W3007488165",
      "https://openalex.org/W3008194092",
      "https://openalex.org/W3011358689",
      "https://openalex.org/W3011667710",
      "https://openalex.org/W3012123536",
      "https://openalex.org/W3012871709",
      "https://openalex.org/W3013107657",
      "https://openalex.org/W3013888836",
      "https://openalex.org/W3016124664",
      "https://openalex.org/W3016427665",
      "https://openalex.org/W3025863369",
      "https://openalex.org/W3026887460",
      "https://openalex.org/W3031353169",
      "https://openalex.org/W3033039844",
      "https://openalex.org/W3033706928",
      "https://openalex.org/W3034231628",
      "https://openalex.org/W3034329572",
      "https://openalex.org/W3035060554",
      "https://openalex.org/W3035096461",
      "https://openalex.org/W3035237749",
      "https://openalex.org/W3035285524",
      "https://openalex.org/W3035523484",
      "https://openalex.org/W3035580605",
      "https://openalex.org/W3035649237",
      "https://openalex.org/W3035664258",
      "https://openalex.org/W3035666843",
      "https://openalex.org/W3035702572",
      "https://openalex.org/W3035740499",
      "https://openalex.org/W3036106327",
      "https://openalex.org/W3036167779",
      "https://openalex.org/W3036974265",
      "https://openalex.org/W3038719422",
      "https://openalex.org/W3038981236",
      "https://openalex.org/W3042918615",
      "https://openalex.org/W3044189835",
      "https://openalex.org/W3045200674",
      "https://openalex.org/W3045662942",
      "https://openalex.org/W3045928028",
      "https://openalex.org/W3046470859",
      "https://openalex.org/W3048817558",
      "https://openalex.org/W3080566854",
      "https://openalex.org/W3080997787",
      "https://openalex.org/W3081203761",
      "https://openalex.org/W3081325717",
      "https://openalex.org/W3081836708",
      "https://openalex.org/W3082154031",
      "https://openalex.org/W3082411326",
      "https://openalex.org/W3087318471",
      "https://openalex.org/W3092339997",
      "https://openalex.org/W3092462694",
      "https://openalex.org/W3093218977",
      "https://openalex.org/W3093687066",
      "https://openalex.org/W3094231942",
      "https://openalex.org/W3094500523",
      "https://openalex.org/W3095448863",
      "https://openalex.org/W3096831136",
      "https://openalex.org/W3098465726",
      "https://openalex.org/W3098797593",
      "https://openalex.org/W3099152386",
      "https://openalex.org/W3099414221",
      "https://openalex.org/W3100078588",
      "https://openalex.org/W3100278010",
      "https://openalex.org/W3100324210",
      "https://openalex.org/W3101707147",
      "https://openalex.org/W3102554291",
      "https://openalex.org/W3103523530",
      "https://openalex.org/W3103720336",
      "https://openalex.org/W3103736477",
      "https://openalex.org/W3104097132",
      "https://openalex.org/W3104644561",
      "https://openalex.org/W3104667978",
      "https://openalex.org/W3105259638",
      "https://openalex.org/W3105423481",
      "https://openalex.org/W3108202858",
      "https://openalex.org/W3108433857",
      "https://openalex.org/W3110901318",
      "https://openalex.org/W3111430045",
      "https://openalex.org/W3113177135",
      "https://openalex.org/W3114613321",
      "https://openalex.org/W3116239416",
      "https://openalex.org/W3117178429",
      "https://openalex.org/W3120567415",
      "https://openalex.org/W3122934853",
      "https://openalex.org/W3123909522",
      "https://openalex.org/W3124962940",
      "https://openalex.org/W3128443161",
      "https://openalex.org/W3129850062",
      "https://openalex.org/W3133780103",
      "https://openalex.org/W3134509497",
      "https://openalex.org/W3135205495",
      "https://openalex.org/W3135389928",
      "https://openalex.org/W3136999308",
      "https://openalex.org/W3137385578",
      "https://openalex.org/W3137928916",
      "https://openalex.org/W3138516171",
      "https://openalex.org/W3148711710",
      "https://openalex.org/W3151900735",
      "https://openalex.org/W3152893301",
      "https://openalex.org/W3154503084",
      "https://openalex.org/W3155056342",
      "https://openalex.org/W3155322940",
      "https://openalex.org/W3155577228",
      "https://openalex.org/W3156642753",
      "https://openalex.org/W3157039246",
      "https://openalex.org/W3157999218",
      "https://openalex.org/W3158827677",
      "https://openalex.org/W3160021293",
      "https://openalex.org/W3163426640",
      "https://openalex.org/W3164446335",
      "https://openalex.org/W3165171933",
      "https://openalex.org/W3165369424",
      "https://openalex.org/W3165924303",
      "https://openalex.org/W3167334189",
      "https://openalex.org/W3168436232",
      "https://openalex.org/W3169168872",
      "https://openalex.org/W3169450514",
      "https://openalex.org/W3169933688",
      "https://openalex.org/W3171581326",
      "https://openalex.org/W3171764584",
      "https://openalex.org/W3174163042",
      "https://openalex.org/W3174174150",
      "https://openalex.org/W3174823757",
      "https://openalex.org/W3175925542",
      "https://openalex.org/W3175971420",
      "https://openalex.org/W3176393519",
      "https://openalex.org/W3176806965",
      "https://openalex.org/W3176890989",
      "https://openalex.org/W3181414820",
      "https://openalex.org/W3184127157",
      "https://openalex.org/W3187985423",
      "https://openalex.org/W3190664711",
      "https://openalex.org/W3191962800",
      "https://openalex.org/W3192448376",
      "https://openalex.org/W3193553875",
      "https://openalex.org/W3194668998",
      "https://openalex.org/W3200806939",
      "https://openalex.org/W3201058350",
      "https://openalex.org/W3201249640",
      "https://openalex.org/W3204651332",
      "https://openalex.org/W3205227354",
      "https://openalex.org/W3206171352",
      "https://openalex.org/W3208638341",
      "https://openalex.org/W3209048663",
      "https://openalex.org/W3209056694",
      "https://openalex.org/W3209451568",
      "https://openalex.org/W3209764902",
      "https://openalex.org/W3210482950",
      "https://openalex.org/W3210611486",
      "https://openalex.org/W3210987203",
      "https://openalex.org/W3211394146",
      "https://openalex.org/W3211477647",
      "https://openalex.org/W3211973371",
      "https://openalex.org/W3213940558",
      "https://openalex.org/W3214642103",
      "https://openalex.org/W3214674106",
      "https://openalex.org/W3214872094",
      "https://openalex.org/W3215452784",
      "https://openalex.org/W4200635484",
      "https://openalex.org/W4205247958",
      "https://openalex.org/W4206174637",
      "https://openalex.org/W4206357214",
      "https://openalex.org/W4206445139",
      "https://openalex.org/W4206776774",
      "https://openalex.org/W4212805305",
      "https://openalex.org/W4213052788",
      "https://openalex.org/W4213147383",
      "https://openalex.org/W4213457653",
      "https://openalex.org/W4214868967",
      "https://openalex.org/W4220742022",
      "https://openalex.org/W4220933119",
      "https://openalex.org/W4221023051",
      "https://openalex.org/W4221138292",
      "https://openalex.org/W4221149947",
      "https://openalex.org/W4221155201",
      "https://openalex.org/W4221157965",
      "https://openalex.org/W4224309748",
      "https://openalex.org/W4224311348",
      "https://openalex.org/W4224311800",
      "https://openalex.org/W4224983022",
      "https://openalex.org/W4225090121",
      "https://openalex.org/W4225338086",
      "https://openalex.org/W4225405705",
      "https://openalex.org/W4225512856",
      "https://openalex.org/W4225596872",
      "https://openalex.org/W4225977739",
      "https://openalex.org/W4226058932",
      "https://openalex.org/W4226060238",
      "https://openalex.org/W4226208698",
      "https://openalex.org/W4229053887",
      "https://openalex.org/W4234842379",
      "https://openalex.org/W4239789016",
      "https://openalex.org/W4240185200",
      "https://openalex.org/W4240592325",
      "https://openalex.org/W4243799827",
      "https://openalex.org/W4246587917",
      "https://openalex.org/W4255866863",
      "https://openalex.org/W4280535976",
      "https://openalex.org/W4281387042",
      "https://openalex.org/W4281563651",
      "https://openalex.org/W4282913028",
      "https://openalex.org/W4282943426",
      "https://openalex.org/W4283121576",
      "https://openalex.org/W4283218438",
      "https://openalex.org/W4283462727",
      "https://openalex.org/W4283798273",
      "https://openalex.org/W4283810298",
      "https://openalex.org/W4283817628",
      "https://openalex.org/W4284666445",
      "https://openalex.org/W4284698122",
      "https://openalex.org/W4285428788",
      "https://openalex.org/W4286588524",
      "https://openalex.org/W4286795917",
      "https://openalex.org/W4286893581",
      "https://openalex.org/W4287123803",
      "https://openalex.org/W4287325738",
      "https://openalex.org/W4287780403",
      "https://openalex.org/W4287863694",
      "https://openalex.org/W4287991183",
      "https://openalex.org/W4287998109",
      "https://openalex.org/W4288052590",
      "https://openalex.org/W4288088467",
      "https://openalex.org/W4288346884",
      "https://openalex.org/W4289389616",
      "https://openalex.org/W4289533979",
      "https://openalex.org/W4289537189",
      "https://openalex.org/W4290648792",
      "https://openalex.org/W4290875097",
      "https://openalex.org/W4290877962",
      "https://openalex.org/W4293112739",
      "https://openalex.org/W4293370878",
      "https://openalex.org/W4293821372",
      "https://openalex.org/W4294170691",
      "https://openalex.org/W4294435970",
      "https://openalex.org/W4294558607",
      "https://openalex.org/W4295097398",
      "https://openalex.org/W4295728955",
      "https://openalex.org/W4295846611",
      "https://openalex.org/W4296047560",
      "https://openalex.org/W4296143727",
      "https://openalex.org/W4296185888",
      "https://openalex.org/W4297510052",
      "https://openalex.org/W4297733535",
      "https://openalex.org/W4297791874",
      "https://openalex.org/W4297946153",
      "https://openalex.org/W4297951436",
      "https://openalex.org/W4297999768",
      "https://openalex.org/W4298052734",
      "https://openalex.org/W4298312696",
      "https://openalex.org/W4301329292",
      "https://openalex.org/W4304097971",
      "https://openalex.org/W4306887124",
      "https://openalex.org/W4307416138",
      "https://openalex.org/W4308505492",
      "https://openalex.org/W4309635196",
      "https://openalex.org/W4309801650",
      "https://openalex.org/W4310012576",
      "https://openalex.org/W4311216457",
      "https://openalex.org/W4312126067",
      "https://openalex.org/W4312689497",
      "https://openalex.org/W4313201684",
      "https://openalex.org/W4315708854",
      "https://openalex.org/W4316495377",
      "https://openalex.org/W4317951161",
      "https://openalex.org/W4318150241",
      "https://openalex.org/W4318347779",
      "https://openalex.org/W4318540750",
      "https://openalex.org/W4318812521",
      "https://openalex.org/W4320814985",
      "https://openalex.org/W4321227311",
      "https://openalex.org/W4321367323",
      "https://openalex.org/W4321479940",
      "https://openalex.org/W4321480027",
      "https://openalex.org/W4321480031",
      "https://openalex.org/W4322614756",
      "https://openalex.org/W4322824839",
      "https://openalex.org/W4323650483",
      "https://openalex.org/W4327525152",
      "https://openalex.org/W4361247736",
      "https://openalex.org/W4362682267",
      "https://openalex.org/W4362714312",
      "https://openalex.org/W4366001157",
      "https://openalex.org/W4366198975",
      "https://openalex.org/W4366975604",
      "https://openalex.org/W4366986925",
      "https://openalex.org/W4367047082",
      "https://openalex.org/W4367047244",
      "https://openalex.org/W4367047306",
      "https://openalex.org/W4367060955",
      "https://openalex.org/W4367595602",
      "https://openalex.org/W4376121360",
      "https://openalex.org/W4378696994",
      "https://openalex.org/W4378766933",
      "https://openalex.org/W4378909195",
      "https://openalex.org/W4378976562",
      "https://openalex.org/W4379185762",
      "https://openalex.org/W4379506768",
      "https://openalex.org/W4380091056",
      "https://openalex.org/W4381573056",
      "https://openalex.org/W4381679608",
      "https://openalex.org/W4382202967",
      "https://openalex.org/W4382239955",
      "https://openalex.org/W4382240004",
      "https://openalex.org/W4382317956",
      "https://openalex.org/W4382468395",
      "https://openalex.org/W4383604692",
      "https://openalex.org/W4384895066",
      "https://openalex.org/W4384915287",
      "https://openalex.org/W4385245566",
      "https://openalex.org/W4385270450",
      "https://openalex.org/W4385568380",
      "https://openalex.org/W4386445678",
      "https://openalex.org/W4386494629",
      "https://openalex.org/W4386587901",
      "https://openalex.org/W4386620158",
      "https://openalex.org/W4386711666",
      "https://openalex.org/W4386746103",
      "https://openalex.org/W4387092606",
      "https://openalex.org/W4387969028",
      "https://openalex.org/W4388185880",
      "https://openalex.org/W4388537645",
      "https://openalex.org/W4388684696",
      "https://openalex.org/W4390534655",
      "https://openalex.org/W4390572965",
      "https://openalex.org/W4391345293",
      "https://openalex.org/W4391561379",
      "https://openalex.org/W637153065"
    ],
    "openalex_rank": 2,
    "num_tokens": 42218,
    "url": "https://arxiv.org/pdf/2304.05055",
    "best_oa_location_pdf_url": "https://arxiv.org/pdf/2304.05055",
    "question_generated": "What are the current methodologies in deep graph representation learning, what challenges do researchers face in optimizing these approaches, and what future directions could enhance the effectiveness of graph-based data analysis?",
    "pipeline_answer": "### Current Methodologies in Deep Graph Representation Learning\n- **Graph Neural Networks (GNNs)** are the primary methodology used in deep graph representation learning. They effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is crucial for various applications in machine learning and data mining [S7901557279].\n\n### Challenges in Optimizing Approaches\n- **Data Requirements**: A significant challenge is the reliance on large amounts of data for stable performance. This creates difficulties in few-shot and zero-shot learning scenarios, where labeled data is scarce [S8137271153].\n- **Computational Costs**: The computational expense associated with Transformer-based models also poses a challenge, making optimization difficult [S8137271153].\n\n### Future Directions for Enhancing Effectiveness\n- **Efficiency Improvements**: Future research may focus on improving the efficiency of Graph Transformers, which could enhance performance while managing complexity in downstream tasks [S6170967708].\n- **Integration of Learning Techniques**: The integration of semi-supervised learning techniques, such as pseudo-labeling and consistency learning, is increasingly being explored to address label scarcity in graph data, thereby improving model performance [S4312425460].\n- **Self-Supervised Learning**: There is a growing interest in self-supervised learning methods, which reduce reliance on labeled data and enhance generalization by leveraging pretext tasks to extract inherent information from graph representations [S9388852359].\n\n### Conclusion\nDeep graph representation learning methodologies, primarily based on GNNs, face challenges related to data requirements and computational costs. Future research directions include enhancing efficiency, integrating semi-supervised learning techniques, and adopting self-supervised learning methods to improve the effectiveness of graph-based data analysis.",
    "pipeline_references": {
      "S6170967708": {
        "id": "S6170967708",
        "text": "Future directions for enhancing graph-based data analysis may involve the exploration of efficiency improvements for Graph Transformers and the integration of pre-training and fine-tuning frameworks to balance performance and complexity in downstream tasks.",
        "children": [
          {
            "id": "E2946614232",
            "text": "..computing resources and can be a bottleneck, particularly for large graphs. The second is the reliance of Transformer-based models on large amounts of data for stable performance. It poses a challenge when dealing with problems that lack sufficient data, especially for few-shot and zero-shot settings. \u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further explored. Additionally, there are some works using pre-training and fine-tuning frameworks J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 28 W. Ju, et al. to balance performance and complexity in downstream tasks [540], this may be a promising solution to address the aforementioned two challenges. 7 Semi-supervised Learning on Graphs We have investigated various architectures of graph neural networks in which the parameters should be tuned by a learning objective. The most prevalent optimization approach is supervised learning on graph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention in the data mining community. In detail, these methods attempt to combine graph representation learning with current semi-supervised techniques including pseudo-labeling, consistency learning, knowledge distillation and active learning. These works can be further subdivided into node-level representation learning and graph-level representation learning. We would introduce both parts in detail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6. 7.1 Node Representation Learning Typically, node representation learning follows the concept of transductive learning, which has access to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised loss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is usually adopted for optimization. In formulation, LN SL = \u2212 1 |YL | \u2211 i\u2208YL y T i log pi, (68) where YL denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that can be used to offer semantic information. To fully utilize these nodes, a range of methods attempt to combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a fundamental semi-supervised technique that uses the classifier to produce the label distribution of unlabeled examples and then adds appropriately labeled examples to the training set [265, 604]. Another line of semi-supervised learning is consistency regularization [247] that requires two examples to have identical predictions under perturbation. This regularization is based on the assumption that each instance has a distinct label that is resistant to random perturbations [118, 357]. Then, we show several representative works in detail. Cooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-labelbased GNN approach for semi-supervised node classification. It employs two GNN classifiers to jointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows: CV (pi) = p T i log pi, (69) where pi denotes the output label distribution. Then it selects the pseudo-labels with high confidence generated from one model to supervise the optimization of the other model. In particular, the objective for unlabeled nodes is written as follows: LCoGN et = \u2211 i\u2208VU 1CV (pi )>\u03c4y\u02c6 T i logqi, (70) where y\u02c6i denotes the one-hot formulation of the pseudo-label y\u02c6i = argmaxpi and qi denotes the label distribution predicted by the other classifier. \u03c4 is a pre-defined temperature coefficient. This cross supervision has been demonstrated effective in [64, 312] to prevent the provision of biased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information from a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still generate the same prediction. In this way, CoGNet can illustrate the entire optimization process to J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 29 Table 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered as a specific kind of consistency learning. Approach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning Node-level CoGNet [265] \u2713 DSGCN [604] \u2713 GRAND [118] \u2713 AugGCR [357] \u2713 HCPL [309] \u2713 Graph-level SEAL [264] \u2713 \u2713 InfoGraph [431] \u2713 \u2713 DSGC [527] \u2713 ASGN [166] \u2713 \u2713 TGNN [218] \u2713 KGNN [221] \u2713 HGMI [262] \u2713 \u2713 ASGNN [508] \u2713 \u2713 DualGraph [310] \u2713 \u2713 GLA [556] \u2713 SS [507] \u2713 enhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in semi-supervised node classification, which can generate dynamics thresholds for reliable nodes. Dynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive manner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller weights to samples with lower confidence with the additional consideration of class balance. The weight is formulated as: \u03c9i = 1 nc i max (RELU (pi \u2212 \u03b2 \u00b7 1)) , (71) where nc i denotes the number of unlabeled samples assigned to the class c i . This technique will decrease the impact of wrong pseudo-labels during iterative training. Graph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learningbased method. It first adds a variety of perturbations to the input graph to generate a list of graph views. Each graph view G r is sent to a GNN classifier to produce a prediction matrix P r = [pr 1 , \u00b7 \u00b7 \u00b7 , p r N ]. Then it summarizes these matrices as: P = 1 R P r . (72) To provide more discriminative information and ensure that the matrix is row-normalized, GRAND sharpens the summarized label matrix into P SA as: P SA ij = P 1/T ij \u00cd j \u2032=0 P 1/T ij\u2032 , (73) where T is a given temperature parameter. Finally, consistency learning is performed by comparing the sharpened summarized matrix with the matrix of each graph view. Formally, the objective is: LGRAN D = 1 R \u2211 R r=1 \u2211 i\u2208V ||P SA i \u2212 Pi ||, (74) here LGRAN D serves as a regularization which is combined with the standard supervised loss. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 30 W. Ju, et al. Augmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with the generation of augmented graphs by random dropout and mixup of different order features. To enhance the model generalization, it borrows the idea of meta-learning to partition the training data, which improves the quality of graph augmentation. In addition, it utilizes consistency regularization to enhance the semi-supervised node classification. 7.2 Graph Representation Learning The objective of graph classification is to predict the property of the whole graph example. Assuming that the training set comprises N l and N u graph samples Gl = {G1 , \u00b7 \u00b7 \u00b7 ,GN l } and G u = {GN l +1 , \u00b7 \u00b7 \u00b7 ,GN l +Nu }, the graph-level supervised loss for labeled data can be expressed as follows: LGSL = \u2212 1 |Gu | \u2211 Gj \u2208 GL y jT logp j , (75) where y j denotes the one-hot label vector for the j-th sample while pj denotes the predicted distribution of G j . When N u = 0, this objective can be utilized to optimize supervised methods. However, due to the shortage of labels in graph data, supervised methods cannot reach exceptional performance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph classification has been developed extensively. These approaches can be categorized into pseudolabeling-based methods, knowledge distillation-based methods and contrastive learning-based methods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph examples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually utilize a teacher-student architecture, where the teacher model conducts graph representation learning without label information to extract generalized knowledge while the student model focuses on the downstream task. Due to the restricted number of labeled instances, the student model transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of this topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning. Typically, these methods extract topological information from two perspectives (i.e., different perturbation strategies and graph encoders), and maximize the similarity of their representations compared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique to improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166, 508]. Then, we review these methods in detail. SEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node in a hierarchical graph. It builds two graph classifiers which generate graph representations and conduct semi-supervised graph classification respectively. SEAL employs a self-attention module to encode each graph into a graph-level representation, and then conducts message passing from a graph level for final classification. SEAL can also be combined with cautious iteration and active iteration. The former merely utilizes partial graph samples to optimize the parameters in the first classifier due to the potential erroneous pseudo-labels. The second combines active learning with the model, which increases the annotation efficiency in semi-supervised scenarios. InfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the similarity between summarized graph representations and their node representations...",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E5615187824",
            "text": "..works tend to use the autoregressive flow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439]. Diffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion steps to slowly add random noise to data x0 \u223c q(x): q(xt|xt\u22121) = N (xt; \u221a 1 \u2212 \u03b2txt\u22121, \u03b2t I), (137) q(x1:T |x0) = \u00d6 T t=1 q(xt|xt\u22121). (138) J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 55 Table 11. Summary of molecular generation models. Model 2D/3D Bindingbased FragmentbasedGNN Backbone Generative Model GCPN [543] 2D GCN [230] GAN MolGAN [82] 2D R-GCN [401] GAN DEFactor [16] 2D GCN GAN GraphVAE [419] 2D ECC [418] VAE MDVAE [100] 2D GGNN [274] VAE JT-VAE [207] 2D \u2713 MPNN [147] VAE CGVAE [290] 2D GGNN VAE DeepScaffold [270] 2D \u2713 GCN VAE GraphNVP [329] 2D R-GCN NF MoFlow [557] 2D R-GCN NF GraphAF [413] 2D R-GCN NF + AR GraphDF [318] 2D R-GCN NF + AR L-Net [272] 3D \u2713 g-U-Net [133] AR G-SchNet [142] 3D SchNet [405] AR GEN3D [388] 3D EGNN [398] AR G-SphereNet [316] 3D SphereNet [292] NF + AR EDM [178] 3D EGNN DM GCDM [347] 3D GCPNet [346] DM 3D-SBDD [306] 3D \u2713 SchNet AR Pocket2Mol [361] 3D \u2713 GVP [211] AR FLAG [581] 3D \u2713 \u2713 SchNet AR GraphBP [288] 3D \u2713 SchNet NF + AR SiamFlow [439] 3D \u2713 R-GCN NF DiffBP [282] 3D \u2713 EGNN DM DiffSBDD [403] 3D \u2713 EGNN DM TargetDiff [156] 3D \u2713 EGNN DM FragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR GraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR MolCode [579] 2D + 3D \u2713 EGNN NF + AR They then learn to reverse the diffusion process to construct desired data samples from the noise: p\u03b8 (x0:T ) = p(xT ) \u00d6 T t=1 p\u03b8 (xt\u22121 |xt), (139) p\u03b8 (xt\u22121 |xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3\u03b8 (xt, t)), (140) while the models are trained using a variational lower bound. Diffusion models have been applied to generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands in DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate molecular fragments in autoregressive models, as is the case with FragDiff [360]. 12.3 Summary and prospects We wrap up this chapter with Table 11, which profiles existing molecular generation models according to their taxonomy for molecular featurization, the GNN backbone, and the generative method. This chapter covers the critical topics of molecular generation, which also elicit valuable insights into the promising directions for future research. We summarize these important aspects as follows. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 56 W. Ju, et al. Techniques. Graph neural networks can be flexibly leveraged to encode molecular features on different representation levels and across different problem settings. Canonical GNNs like GCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular, this 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the binding protein receptor is considered alongside the ligand geometry per se. Fragment-based models like JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure. Various generative methods have also been effectively incorporated into the molecular setting, including generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to generate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating the search for drug candidates. Challenges and Limitations. While there has been an abundant supply of unlabelled molecular structural and geometric data [125, 193, 426], the number of labeled molecular data over certain critical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the other hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of the generated molecules, such as QED and Vina [103], rather than actual wet lab experiments. Future Works. Besides the structural and geometric attributes described in this chapter, an even more extensive array of data can be applied to aid molecular generation, including chemical reactions and medical ontology. These data can be organized into a heterogeneous knowledge graph to aid the extraction of high-quality molecular representations. Furthermore, high throughput experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and druggability of the generated molecules in the wet lab. Concrete case studies, such as the design of potential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights into leveraging these molecular generative models to facilitate the design and fabrication of potent and applicable drug molecules in the pharmaceutical industry. Integrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations offers a promising new direction in molecular generation. Recent studies like those by [196] and [160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLMbased approaches in this domain, including those by [338] and [18], predominantly utilize textual SMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The emerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image, and text, which could more comprehensively capture the intricacies of molecular structures. This approach marks a significant shift towards utilizing graph-based information alongside traditional text, enhancing the capability of LLMs in molecular generation. Such advances suggest that future research should focus more on exploiting the synergy between graph-based molecular representations and the evolving landscape of LLMs to address complex challenges in chemistry and material sciences. 13 Recommender Systems The use of graph representation learning in recommender systems has been drawing increasing attention as one of the key strategies for addressing the issue of information overload. With their strong ability to capture high-order connectivity between graph nodes, deep graph representation learning has been shown to be beneficial in enhancing recommendation performance across a variety of recommendation scenarios. Typical recommender systems take the observed interactions between users and items and their fixed features as input, and are intended for making proper predictions on which items a specific user is probably interested in. To formulate, given an user set U, an item set I and the J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 57 Table 12. Summary of graph models for recommender systems. Model Recommendation Task Graph Structure Graph Encoder Representation GC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer NGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation MMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer LightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling DGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling CAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling SR-GNN [496] Session-based Transition Graph GGNN Soft-Attention GC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention FGNN [377] Session-based Session Graph GAT Last-Layer GAG [378] Session-based Session Graph GCN Self-Attention GCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling HyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention DHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer MBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention HCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer H3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer STHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling interaction matrix between users and items X \u2208 {0, 1} |U|\u00d7|I| , where Xu,v indicates there is an observed interaction between user u and item i. The target of GNNs on recommender systems is to learn representations hu, hi \u2208 R d for given u and i. The preference score can..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      },
      "S7901557279": {
        "id": "S7901557279",
        "text": "Graph representation learning methodologies primarily utilize graph neural networks (GNNs) to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is essential for various applications in machine learning and data mining.",
        "children": [
          {
            "id": "E5304604864",
            "text": "A Comprehensive Survey on Deep Graph Representation Learning WEI JU, ZHENG FANG, YIYANG GU, ZEQUN LIU, and QINGQING LONG, Peking University, China ZIYUE QIAO, The Hong Kong University of Science and Technology, China YIFANG QIN and JIANHAO SHEN, Peking University, China FANG SUN and ZHIPING XIAO, University of California, Los Angeles, USA JUNWEI YANG, JINGYANG YUAN, and YUSHENG ZHAO, Peking University, China YIFAN WANG, University of International Business and Economics, China XIAO LUO\u2217, University of California, Los Angeles, USA MING ZHANG\u2217, Peking University, China Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future. CCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; Learning latent representations. \u2217Corresponding authors. Authors\u2019 addresses: Wei Ju, juwei@pku.edu.cn; Zheng Fang, fang_z@pku.edu.cn; Yiyang Gu, yiyanggu@pku.edu.cn; Zequn Liu, zequnliu@pku.edu.cn; Qingqing Long, qingqinglong@pku.edu.cn, Peking University, Beijing, China, 100871; Ziyue Qiao, ziyuejoe@gmail.com, The Hong Kong University of Science and Technology, Guangzhou, China, 511453; Yifang Qin, qinyifang@pku.edu.cn; Jianhao Shen, jhshen@pku.edu.cn, Peking University, Beijing, China, 100871; Fang Sun, fts@cs.ucla.edu; Zhiping Xiao, patricia.xiao@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Junwei Yang, yjwtheonly@pku.edu.cn; Jingyang Yuan, yuanjy@pku.edu.cn; Yusheng Zhao, yusheng.zhao@stu.pku.edu.cn, Peking University, Beijing, China, 100871; Yifan Wang, yifanwang@uibe.edu.cn, University of International Business and Economics, Beijing, China, 100029; Xiao Luo, xiaoluo@cs.ucla.edu, University of California, Los Angeles, USA, 90095; Ming Zhang, mzhang_cs@pku.edu.cn, Peking University, Beijing, China, 100871. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2024 Association for Computing Machinery. 0004-5411/2024/2-ART $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. arXiv:2304.05055v3 [cs.LG] 28 Feb 2024 2 W. Ju, et al. Additional Key Words and Phrases: Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network, Survey ACM Reference Format: Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024. A Comprehensive Survey on Deep Graph Representation Learning. J. ACM 1, 1 (February 2024), 100 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction Graphs have recently emerged as a powerful tool for representing a variety of structured and complex data, including social networks, traffic networks, information systems, knowledge graphs, protein-protein interaction networks, and physical interaction networks. As a kind of general form of data organization, graph structures are capable of naturally expressing the intrinsic relationship of these data, and thus can characterize plenty of non-Euclidean structures that are crucial in a variety of disciplines and domains due to their flexible adaptability. For example, to encode a social network as a graph, nodes on the graph are used to represent individual users, and edges are used to represent the relationship between two individuals, such as friends. In the field of biology, nodes can be used to represent proteins, and edges can be used to represent biological interactions between various proteins, such as the dynamic interactions between proteins. Thus, by analyzing and mining the graph-structured data, we can understand the deep meaning hidden behind the data, and further discover valuable knowledge, so as to benefit society and human beings. In the last decade years, a wide range of machine learning algorithms have been developed for graph-structured data learning. Among them, traditional graph kernel methods [137, 225, 408, 410] usually break down graphs into different atomic substructures and then use kernel functions to measure the similarity between all pairs of them. Although graph kernels could provide a perspective on modeling graph topology, these approaches often generate substructures or feature representations based on given hand-crafted criteria. These rules are rather heuristic, prone to suffer from high computational complexity, and therefore have weak scalability and subpar performance. In the past few years, graph embedding algorithms [4, 155, 362, 442, 443, 460] have everincreasing emerged, which attempt to encode the structural information of the graph (usually a high-dimensional sparse matrix) and map it into a low-dimensional dense vector embedding to preserve the topology information and attribute information in the embedding space as much as possible, so that the learned graph embeddings can be naturally integrated into traditional machine learning algorithms. Compared to previous works which use feature engineering in the pre-processing phase to extract graph structural features, current graph embedding algorithms are conducted in a data-driven way leveraging machine learning algorithms (such as neural networks) to encode the structural information of the graph. Specifically, existing graph embedding methods can be categorized into the following main groups: (i) matrix factorization based methods [4, 46, 354] that factorize the matrix to learn node embedding which preserves the graph property; (ii) deep learning based methods [155, 362, 443, 460] that apply deep learning techniques specifically designed for graph-structured data; (iii) edge reconstruction based methods [287, 331, 442] that either maximizes edge reconstruction probability or minimizes edge reconstruction loss. Generally, these methods typically depend on shallow architectures, and fail to exploit the potential and capacity of deep neural networks, resulting in sub-optimal representation quality and learning performance. Inspired by the recent remarkable success of deep neural networks, a range of deep learning algorithms has been developed for graph-structured data learning. The core of these methods is to generate effective node and graph representations using graph neural networks (GNNs), followed by a goal-oriented learning paradigm. In this way, the derived representations can be adaptively J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 3 coupled with a variety of downstream tasks and applications. Following this line of thought, in this paper, we propose a new taxonomy to classify the existing graph representation learning algorithms, i.e., graph neural network architectures, learning paradigms, and various promising applications, as shown in Fig. 1. Specifically, for the architectures of GNNs, we investigate the studies on graph convolutions, graph kernel neural networks, graph pooling, and graph transformer. For the learning paradigms, we explore three advanced types namely supervised/semi-supervised learning on graphs, graph self-supervised learning, and graph structure learning. To demonstrate the effectiveness of the learned graph representations, we provide several promising applications to build tight connections between representation learning and downstream tasks, such as social analysis, molecular property prediction and generation, recommender systems, and traffic analysis. Last but not least, we present some perspectives for thought and suggest challenging directions that deserve further study in the future. Differences between this survey and existing ones. Up to now, there exist some other overview papers focusing on different perspectives of graph representation learning[17, 50, 53..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E5615187824",
            "text": "..works tend to use the autoregressive flow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439]. Diffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion steps to slowly add random noise to data x0 \u223c q(x): q(xt|xt\u22121) = N (xt; \u221a 1 \u2212 \u03b2txt\u22121, \u03b2t I), (137) q(x1:T |x0) = \u00d6 T t=1 q(xt|xt\u22121). (138) J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 55 Table 11. Summary of molecular generation models. Model 2D/3D Bindingbased FragmentbasedGNN Backbone Generative Model GCPN [543] 2D GCN [230] GAN MolGAN [82] 2D R-GCN [401] GAN DEFactor [16] 2D GCN GAN GraphVAE [419] 2D ECC [418] VAE MDVAE [100] 2D GGNN [274] VAE JT-VAE [207] 2D \u2713 MPNN [147] VAE CGVAE [290] 2D GGNN VAE DeepScaffold [270] 2D \u2713 GCN VAE GraphNVP [329] 2D R-GCN NF MoFlow [557] 2D R-GCN NF GraphAF [413] 2D R-GCN NF + AR GraphDF [318] 2D R-GCN NF + AR L-Net [272] 3D \u2713 g-U-Net [133] AR G-SchNet [142] 3D SchNet [405] AR GEN3D [388] 3D EGNN [398] AR G-SphereNet [316] 3D SphereNet [292] NF + AR EDM [178] 3D EGNN DM GCDM [347] 3D GCPNet [346] DM 3D-SBDD [306] 3D \u2713 SchNet AR Pocket2Mol [361] 3D \u2713 GVP [211] AR FLAG [581] 3D \u2713 \u2713 SchNet AR GraphBP [288] 3D \u2713 SchNet NF + AR SiamFlow [439] 3D \u2713 R-GCN NF DiffBP [282] 3D \u2713 EGNN DM DiffSBDD [403] 3D \u2713 EGNN DM TargetDiff [156] 3D \u2713 EGNN DM FragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR GraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR MolCode [579] 2D + 3D \u2713 EGNN NF + AR They then learn to reverse the diffusion process to construct desired data samples from the noise: p\u03b8 (x0:T ) = p(xT ) \u00d6 T t=1 p\u03b8 (xt\u22121 |xt), (139) p\u03b8 (xt\u22121 |xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3\u03b8 (xt, t)), (140) while the models are trained using a variational lower bound. Diffusion models have been applied to generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands in DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate molecular fragments in autoregressive models, as is the case with FragDiff [360]. 12.3 Summary and prospects We wrap up this chapter with Table 11, which profiles existing molecular generation models according to their taxonomy for molecular featurization, the GNN backbone, and the generative method. This chapter covers the critical topics of molecular generation, which also elicit valuable insights into the promising directions for future research. We summarize these important aspects as follows. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 56 W. Ju, et al. Techniques. Graph neural networks can be flexibly leveraged to encode molecular features on different representation levels and across different problem settings. Canonical GNNs like GCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular, this 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the binding protein receptor is considered alongside the ligand geometry per se. Fragment-based models like JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure. Various generative methods have also been effectively incorporated into the molecular setting, including generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to generate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating the search for drug candidates. Challenges and Limitations. While there has been an abundant supply of unlabelled molecular structural and geometric data [125, 193, 426], the number of labeled molecular data over certain critical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the other hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of the generated molecules, such as QED and Vina [103], rather than actual wet lab experiments. Future Works. Besides the structural and geometric attributes described in this chapter, an even more extensive array of data can be applied to aid molecular generation, including chemical reactions and medical ontology. These data can be organized into a heterogeneous knowledge graph to aid the extraction of high-quality molecular representations. Furthermore, high throughput experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and druggability of the generated molecules in the wet lab. Concrete case studies, such as the design of potential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights into leveraging these molecular generative models to facilitate the design and fabrication of potent and applicable drug molecules in the pharmaceutical industry. Integrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations offers a promising new direction in molecular generation. Recent studies like those by [196] and [160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLMbased approaches in this domain, including those by [338] and [18], predominantly utilize textual SMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The emerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image, and text, which could more comprehensively capture the intricacies of molecular structures. This approach marks a significant shift towards utilizing graph-based information alongside traditional text, enhancing the capability of LLMs in molecular generation. Such advances suggest that future research should focus more on exploiting the synergy between graph-based molecular representations and the evolving landscape of LLMs to address complex challenges in chemistry and material sciences. 13 Recommender Systems The use of graph representation learning in recommender systems has been drawing increasing attention as one of the key strategies for addressing the issue of information overload. With their strong ability to capture high-order connectivity between graph nodes, deep graph representation learning has been shown to be beneficial in enhancing recommendation performance across a variety of recommendation scenarios. Typical recommender systems take the observed interactions between users and items and their fixed features as input, and are intended for making proper predictions on which items a specific user is probably interested in. To formulate, given an user set U, an item set I and the J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 57 Table 12. Summary of graph models for recommender systems. Model Recommendation Task Graph Structure Graph Encoder Representation GC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer NGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation MMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer LightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling DGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling CAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling SR-GNN [496] Session-based Transition Graph GGNN Soft-Attention GC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention FGNN [377] Session-based Session Graph GAT Last-Layer GAG [378] Session-based Session Graph GCN Self-Attention GCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling HyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention DHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer MBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention HCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer H3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer STHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling interaction matrix between users and items X \u2208 {0, 1} |U|\u00d7|I| , where Xu,v indicates there is an observed interaction between user u and item i. The target of GNNs on recommender systems is to learn representations hu, hi \u2208 R d for given u and i. The preference score can..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      },
      "S9388852359": {
        "id": "S9388852359",
        "text": "Self-supervised learning methods are gaining traction in graph representation learning, as they reduce reliance on labeled data and enhance the model's generalization ability by leveraging pretext tasks to extract inherent information from graph representations.",
        "children": [
          {
            "id": "E2946614232",
            "text": "..computing resources and can be a bottleneck, particularly for large graphs. The second is the reliance of Transformer-based models on large amounts of data for stable performance. It poses a challenge when dealing with problems that lack sufficient data, especially for few-shot and zero-shot settings. \u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further explored. Additionally, there are some works using pre-training and fine-tuning frameworks J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 28 W. Ju, et al. to balance performance and complexity in downstream tasks [540], this may be a promising solution to address the aforementioned two challenges. 7 Semi-supervised Learning on Graphs We have investigated various architectures of graph neural networks in which the parameters should be tuned by a learning objective. The most prevalent optimization approach is supervised learning on graph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention in the data mining community. In detail, these methods attempt to combine graph representation learning with current semi-supervised techniques including pseudo-labeling, consistency learning, knowledge distillation and active learning. These works can be further subdivided into node-level representation learning and graph-level representation learning. We would introduce both parts in detail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6. 7.1 Node Representation Learning Typically, node representation learning follows the concept of transductive learning, which has access to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised loss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is usually adopted for optimization. In formulation, LN SL = \u2212 1 |YL | \u2211 i\u2208YL y T i log pi, (68) where YL denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that can be used to offer semantic information. To fully utilize these nodes, a range of methods attempt to combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a fundamental semi-supervised technique that uses the classifier to produce the label distribution of unlabeled examples and then adds appropriately labeled examples to the training set [265, 604]. Another line of semi-supervised learning is consistency regularization [247] that requires two examples to have identical predictions under perturbation. This regularization is based on the assumption that each instance has a distinct label that is resistant to random perturbations [118, 357]. Then, we show several representative works in detail. Cooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-labelbased GNN approach for semi-supervised node classification. It employs two GNN classifiers to jointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows: CV (pi) = p T i log pi, (69) where pi denotes the output label distribution. Then it selects the pseudo-labels with high confidence generated from one model to supervise the optimization of the other model. In particular, the objective for unlabeled nodes is written as follows: LCoGN et = \u2211 i\u2208VU 1CV (pi )>\u03c4y\u02c6 T i logqi, (70) where y\u02c6i denotes the one-hot formulation of the pseudo-label y\u02c6i = argmaxpi and qi denotes the label distribution predicted by the other classifier. \u03c4 is a pre-defined temperature coefficient. This cross supervision has been demonstrated effective in [64, 312] to prevent the provision of biased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information from a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still generate the same prediction. In this way, CoGNet can illustrate the entire optimization process to J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 29 Table 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered as a specific kind of consistency learning. Approach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning Node-level CoGNet [265] \u2713 DSGCN [604] \u2713 GRAND [118] \u2713 AugGCR [357] \u2713 HCPL [309] \u2713 Graph-level SEAL [264] \u2713 \u2713 InfoGraph [431] \u2713 \u2713 DSGC [527] \u2713 ASGN [166] \u2713 \u2713 TGNN [218] \u2713 KGNN [221] \u2713 HGMI [262] \u2713 \u2713 ASGNN [508] \u2713 \u2713 DualGraph [310] \u2713 \u2713 GLA [556] \u2713 SS [507] \u2713 enhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in semi-supervised node classification, which can generate dynamics thresholds for reliable nodes. Dynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive manner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller weights to samples with lower confidence with the additional consideration of class balance. The weight is formulated as: \u03c9i = 1 nc i max (RELU (pi \u2212 \u03b2 \u00b7 1)) , (71) where nc i denotes the number of unlabeled samples assigned to the class c i . This technique will decrease the impact of wrong pseudo-labels during iterative training. Graph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learningbased method. It first adds a variety of perturbations to the input graph to generate a list of graph views. Each graph view G r is sent to a GNN classifier to produce a prediction matrix P r = [pr 1 , \u00b7 \u00b7 \u00b7 , p r N ]. Then it summarizes these matrices as: P = 1 R P r . (72) To provide more discriminative information and ensure that the matrix is row-normalized, GRAND sharpens the summarized label matrix into P SA as: P SA ij = P 1/T ij \u00cd j \u2032=0 P 1/T ij\u2032 , (73) where T is a given temperature parameter. Finally, consistency learning is performed by comparing the sharpened summarized matrix with the matrix of each graph view. Formally, the objective is: LGRAN D = 1 R \u2211 R r=1 \u2211 i\u2208V ||P SA i \u2212 Pi ||, (74) here LGRAN D serves as a regularization which is combined with the standard supervised loss. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 30 W. Ju, et al. Augmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with the generation of augmented graphs by random dropout and mixup of different order features. To enhance the model generalization, it borrows the idea of meta-learning to partition the training data, which improves the quality of graph augmentation. In addition, it utilizes consistency regularization to enhance the semi-supervised node classification. 7.2 Graph Representation Learning The objective of graph classification is to predict the property of the whole graph example. Assuming that the training set comprises N l and N u graph samples Gl = {G1 , \u00b7 \u00b7 \u00b7 ,GN l } and G u = {GN l +1 , \u00b7 \u00b7 \u00b7 ,GN l +Nu }, the graph-level supervised loss for labeled data can be expressed as follows: LGSL = \u2212 1 |Gu | \u2211 Gj \u2208 GL y jT logp j , (75) where y j denotes the one-hot label vector for the j-th sample while pj denotes the predicted distribution of G j . When N u = 0, this objective can be utilized to optimize supervised methods. However, due to the shortage of labels in graph data, supervised methods cannot reach exceptional performance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph classification has been developed extensively. These approaches can be categorized into pseudolabeling-based methods, knowledge distillation-based methods and contrastive learning-based methods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph examples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually utilize a teacher-student architecture, where the teacher model conducts graph representation learning without label information to extract generalized knowledge while the student model focuses on the downstream task. Due to the restricted number of labeled instances, the student model transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of this topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning. Typically, these methods extract topological information from two perspectives (i.e., different perturbation strategies and graph encoders), and maximize the similarity of their representations compared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique to improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166, 508]. Then, we review these methods in detail. SEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node in a hierarchical graph. It builds two graph classifiers which generate graph representations and conduct semi-supervised graph classification respectively. SEAL employs a self-attention module to encode each graph into a graph-level representation, and then conducts message passing from a graph level for final classification. SEAL can also be combined with cautious iteration and active iteration. The former merely utilizes partial graph samples to optimize the parameters in the first classifier due to the potential erroneous pseudo-labels. The second combines active learning with the model, which increases the annotation efficiency in semi-supervised scenarios. InfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the similarity between summarized graph representations and their node representations...",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E5615187824",
            "text": "..works tend to use the autoregressive flow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439]. Diffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion steps to slowly add random noise to data x0 \u223c q(x): q(xt|xt\u22121) = N (xt; \u221a 1 \u2212 \u03b2txt\u22121, \u03b2t I), (137) q(x1:T |x0) = \u00d6 T t=1 q(xt|xt\u22121). (138) J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 55 Table 11. Summary of molecular generation models. Model 2D/3D Bindingbased FragmentbasedGNN Backbone Generative Model GCPN [543] 2D GCN [230] GAN MolGAN [82] 2D R-GCN [401] GAN DEFactor [16] 2D GCN GAN GraphVAE [419] 2D ECC [418] VAE MDVAE [100] 2D GGNN [274] VAE JT-VAE [207] 2D \u2713 MPNN [147] VAE CGVAE [290] 2D GGNN VAE DeepScaffold [270] 2D \u2713 GCN VAE GraphNVP [329] 2D R-GCN NF MoFlow [557] 2D R-GCN NF GraphAF [413] 2D R-GCN NF + AR GraphDF [318] 2D R-GCN NF + AR L-Net [272] 3D \u2713 g-U-Net [133] AR G-SchNet [142] 3D SchNet [405] AR GEN3D [388] 3D EGNN [398] AR G-SphereNet [316] 3D SphereNet [292] NF + AR EDM [178] 3D EGNN DM GCDM [347] 3D GCPNet [346] DM 3D-SBDD [306] 3D \u2713 SchNet AR Pocket2Mol [361] 3D \u2713 GVP [211] AR FLAG [581] 3D \u2713 \u2713 SchNet AR GraphBP [288] 3D \u2713 SchNet NF + AR SiamFlow [439] 3D \u2713 R-GCN NF DiffBP [282] 3D \u2713 EGNN DM DiffSBDD [403] 3D \u2713 EGNN DM TargetDiff [156] 3D \u2713 EGNN DM FragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR GraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR MolCode [579] 2D + 3D \u2713 EGNN NF + AR They then learn to reverse the diffusion process to construct desired data samples from the noise: p\u03b8 (x0:T ) = p(xT ) \u00d6 T t=1 p\u03b8 (xt\u22121 |xt), (139) p\u03b8 (xt\u22121 |xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3\u03b8 (xt, t)), (140) while the models are trained using a variational lower bound. Diffusion models have been applied to generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands in DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate molecular fragments in autoregressive models, as is the case with FragDiff [360]. 12.3 Summary and prospects We wrap up this chapter with Table 11, which profiles existing molecular generation models according to their taxonomy for molecular featurization, the GNN backbone, and the generative method. This chapter covers the critical topics of molecular generation, which also elicit valuable insights into the promising directions for future research. We summarize these important aspects as follows. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 56 W. Ju, et al. Techniques. Graph neural networks can be flexibly leveraged to encode molecular features on different representation levels and across different problem settings. Canonical GNNs like GCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular, this 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the binding protein receptor is considered alongside the ligand geometry per se. Fragment-based models like JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure. Various generative methods have also been effectively incorporated into the molecular setting, including generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to generate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating the search for drug candidates. Challenges and Limitations. While there has been an abundant supply of unlabelled molecular structural and geometric data [125, 193, 426], the number of labeled molecular data over certain critical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the other hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of the generated molecules, such as QED and Vina [103], rather than actual wet lab experiments. Future Works. Besides the structural and geometric attributes described in this chapter, an even more extensive array of data can be applied to aid molecular generation, including chemical reactions and medical ontology. These data can be organized into a heterogeneous knowledge graph to aid the extraction of high-quality molecular representations. Furthermore, high throughput experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and druggability of the generated molecules in the wet lab. Concrete case studies, such as the design of potential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights into leveraging these molecular generative models to facilitate the design and fabrication of potent and applicable drug molecules in the pharmaceutical industry. Integrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations offers a promising new direction in molecular generation. Recent studies like those by [196] and [160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLMbased approaches in this domain, including those by [338] and [18], predominantly utilize textual SMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The emerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image, and text, which could more comprehensively capture the intricacies of molecular structures. This approach marks a significant shift towards utilizing graph-based information alongside traditional text, enhancing the capability of LLMs in molecular generation. Such advances suggest that future research should focus more on exploiting the synergy between graph-based molecular representations and the evolving landscape of LLMs to address complex challenges in chemistry and material sciences. 13 Recommender Systems The use of graph representation learning in recommender systems has been drawing increasing attention as one of the key strategies for addressing the issue of information overload. With their strong ability to capture high-order connectivity between graph nodes, deep graph representation learning has been shown to be beneficial in enhancing recommendation performance across a variety of recommendation scenarios. Typical recommender systems take the observed interactions between users and items and their fixed features as input, and are intended for making proper predictions on which items a specific user is probably interested in. To formulate, given an user set U, an item set I and the J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 57 Table 12. Summary of graph models for recommender systems. Model Recommendation Task Graph Structure Graph Encoder Representation GC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer NGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation MMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer LightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling DGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling CAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling SR-GNN [496] Session-based Transition Graph GGNN Soft-Attention GC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention FGNN [377] Session-based Session Graph GAT Last-Layer GAG [378] Session-based Session Graph GCN Self-Attention GCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling HyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention DHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer MBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention HCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer H3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer STHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling interaction matrix between users and items X \u2208 {0, 1} |U|\u00d7|I| , where Xu,v indicates there is an observed interaction between user u and item i. The target of GNNs on recommender systems is to learn representations hu, hi \u2208 R d for given u and i. The preference score can..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      },
      "S8137271153": {
        "id": "S8137271153",
        "text": "Challenges in optimizing deep graph representation learning approaches include the reliance on large amounts of data for stable performance, which poses difficulties in few-shot and zero-shot learning scenarios, as well as the computational cost associated with Transformer-based models.",
        "children": [
          {
            "id": "E2946614232",
            "text": "..computing resources and can be a bottleneck, particularly for large graphs. The second is the reliance of Transformer-based models on large amounts of data for stable performance. It poses a challenge when dealing with problems that lack sufficient data, especially for few-shot and zero-shot settings. \u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further explored. Additionally, there are some works using pre-training and fine-tuning frameworks J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 28 W. Ju, et al. to balance performance and complexity in downstream tasks [540], this may be a promising solution to address the aforementioned two challenges. 7 Semi-supervised Learning on Graphs We have investigated various architectures of graph neural networks in which the parameters should be tuned by a learning objective. The most prevalent optimization approach is supervised learning on graph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention in the data mining community. In detail, these methods attempt to combine graph representation learning with current semi-supervised techniques including pseudo-labeling, consistency learning, knowledge distillation and active learning. These works can be further subdivided into node-level representation learning and graph-level representation learning. We would introduce both parts in detail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6. 7.1 Node Representation Learning Typically, node representation learning follows the concept of transductive learning, which has access to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised loss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is usually adopted for optimization. In formulation, LN SL = \u2212 1 |YL | \u2211 i\u2208YL y T i log pi, (68) where YL denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that can be used to offer semantic information. To fully utilize these nodes, a range of methods attempt to combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a fundamental semi-supervised technique that uses the classifier to produce the label distribution of unlabeled examples and then adds appropriately labeled examples to the training set [265, 604]. Another line of semi-supervised learning is consistency regularization [247] that requires two examples to have identical predictions under perturbation. This regularization is based on the assumption that each instance has a distinct label that is resistant to random perturbations [118, 357]. Then, we show several representative works in detail. Cooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-labelbased GNN approach for semi-supervised node classification. It employs two GNN classifiers to jointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows: CV (pi) = p T i log pi, (69) where pi denotes the output label distribution. Then it selects the pseudo-labels with high confidence generated from one model to supervise the optimization of the other model. In particular, the objective for unlabeled nodes is written as follows: LCoGN et = \u2211 i\u2208VU 1CV (pi )>\u03c4y\u02c6 T i logqi, (70) where y\u02c6i denotes the one-hot formulation of the pseudo-label y\u02c6i = argmaxpi and qi denotes the label distribution predicted by the other classifier. \u03c4 is a pre-defined temperature coefficient. This cross supervision has been demonstrated effective in [64, 312] to prevent the provision of biased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information from a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still generate the same prediction. In this way, CoGNet can illustrate the entire optimization process to J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 29 Table 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered as a specific kind of consistency learning. Approach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning Node-level CoGNet [265] \u2713 DSGCN [604] \u2713 GRAND [118] \u2713 AugGCR [357] \u2713 HCPL [309] \u2713 Graph-level SEAL [264] \u2713 \u2713 InfoGraph [431] \u2713 \u2713 DSGC [527] \u2713 ASGN [166] \u2713 \u2713 TGNN [218] \u2713 KGNN [221] \u2713 HGMI [262] \u2713 \u2713 ASGNN [508] \u2713 \u2713 DualGraph [310] \u2713 \u2713 GLA [556] \u2713 SS [507] \u2713 enhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in semi-supervised node classification, which can generate dynamics thresholds for reliable nodes. Dynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive manner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller weights to samples with lower confidence with the additional consideration of class balance. The weight is formulated as: \u03c9i = 1 nc i max (RELU (pi \u2212 \u03b2 \u00b7 1)) , (71) where nc i denotes the number of unlabeled samples assigned to the class c i . This technique will decrease the impact of wrong pseudo-labels during iterative training. Graph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learningbased method. It first adds a variety of perturbations to the input graph to generate a list of graph views. Each graph view G r is sent to a GNN classifier to produce a prediction matrix P r = [pr 1 , \u00b7 \u00b7 \u00b7 , p r N ]. Then it summarizes these matrices as: P = 1 R P r . (72) To provide more discriminative information and ensure that the matrix is row-normalized, GRAND sharpens the summarized label matrix into P SA as: P SA ij = P 1/T ij \u00cd j \u2032=0 P 1/T ij\u2032 , (73) where T is a given temperature parameter. Finally, consistency learning is performed by comparing the sharpened summarized matrix with the matrix of each graph view. Formally, the objective is: LGRAN D = 1 R \u2211 R r=1 \u2211 i\u2208V ||P SA i \u2212 Pi ||, (74) here LGRAN D serves as a regularization which is combined with the standard supervised loss. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 30 W. Ju, et al. Augmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with the generation of augmented graphs by random dropout and mixup of different order features. To enhance the model generalization, it borrows the idea of meta-learning to partition the training data, which improves the quality of graph augmentation. In addition, it utilizes consistency regularization to enhance the semi-supervised node classification. 7.2 Graph Representation Learning The objective of graph classification is to predict the property of the whole graph example. Assuming that the training set comprises N l and N u graph samples Gl = {G1 , \u00b7 \u00b7 \u00b7 ,GN l } and G u = {GN l +1 , \u00b7 \u00b7 \u00b7 ,GN l +Nu }, the graph-level supervised loss for labeled data can be expressed as follows: LGSL = \u2212 1 |Gu | \u2211 Gj \u2208 GL y jT logp j , (75) where y j denotes the one-hot label vector for the j-th sample while pj denotes the predicted distribution of G j . When N u = 0, this objective can be utilized to optimize supervised methods. However, due to the shortage of labels in graph data, supervised methods cannot reach exceptional performance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph classification has been developed extensively. These approaches can be categorized into pseudolabeling-based methods, knowledge distillation-based methods and contrastive learning-based methods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph examples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually utilize a teacher-student architecture, where the teacher model conducts graph representation learning without label information to extract generalized knowledge while the student model focuses on the downstream task. Due to the restricted number of labeled instances, the student model transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of this topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning. Typically, these methods extract topological information from two perspectives (i.e., different perturbation strategies and graph encoders), and maximize the similarity of their representations compared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique to improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166, 508]. Then, we review these methods in detail. SEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node in a hierarchical graph. It builds two graph classifiers which generate graph representations and conduct semi-supervised graph classification respectively. SEAL employs a self-attention module to encode each graph into a graph-level representation, and then conducts message passing from a graph level for final classification. SEAL can also be combined with cautious iteration and active iteration. The former merely utilizes partial graph samples to optimize the parameters in the first classifier due to the potential erroneous pseudo-labels. The second combines active learning with the model, which increases the annotation efficiency in semi-supervised scenarios. InfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the similarity between summarized graph representations and their node representations...",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E5615187824",
            "text": "..works tend to use the autoregressive flow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439]. Diffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion steps to slowly add random noise to data x0 \u223c q(x): q(xt|xt\u22121) = N (xt; \u221a 1 \u2212 \u03b2txt\u22121, \u03b2t I), (137) q(x1:T |x0) = \u00d6 T t=1 q(xt|xt\u22121). (138) J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 55 Table 11. Summary of molecular generation models. Model 2D/3D Bindingbased FragmentbasedGNN Backbone Generative Model GCPN [543] 2D GCN [230] GAN MolGAN [82] 2D R-GCN [401] GAN DEFactor [16] 2D GCN GAN GraphVAE [419] 2D ECC [418] VAE MDVAE [100] 2D GGNN [274] VAE JT-VAE [207] 2D \u2713 MPNN [147] VAE CGVAE [290] 2D GGNN VAE DeepScaffold [270] 2D \u2713 GCN VAE GraphNVP [329] 2D R-GCN NF MoFlow [557] 2D R-GCN NF GraphAF [413] 2D R-GCN NF + AR GraphDF [318] 2D R-GCN NF + AR L-Net [272] 3D \u2713 g-U-Net [133] AR G-SchNet [142] 3D SchNet [405] AR GEN3D [388] 3D EGNN [398] AR G-SphereNet [316] 3D SphereNet [292] NF + AR EDM [178] 3D EGNN DM GCDM [347] 3D GCPNet [346] DM 3D-SBDD [306] 3D \u2713 SchNet AR Pocket2Mol [361] 3D \u2713 GVP [211] AR FLAG [581] 3D \u2713 \u2713 SchNet AR GraphBP [288] 3D \u2713 SchNet NF + AR SiamFlow [439] 3D \u2713 R-GCN NF DiffBP [282] 3D \u2713 EGNN DM DiffSBDD [403] 3D \u2713 EGNN DM TargetDiff [156] 3D \u2713 EGNN DM FragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR GraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR MolCode [579] 2D + 3D \u2713 EGNN NF + AR They then learn to reverse the diffusion process to construct desired data samples from the noise: p\u03b8 (x0:T ) = p(xT ) \u00d6 T t=1 p\u03b8 (xt\u22121 |xt), (139) p\u03b8 (xt\u22121 |xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3\u03b8 (xt, t)), (140) while the models are trained using a variational lower bound. Diffusion models have been applied to generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands in DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate molecular fragments in autoregressive models, as is the case with FragDiff [360]. 12.3 Summary and prospects We wrap up this chapter with Table 11, which profiles existing molecular generation models according to their taxonomy for molecular featurization, the GNN backbone, and the generative method. This chapter covers the critical topics of molecular generation, which also elicit valuable insights into the promising directions for future research. We summarize these important aspects as follows. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 56 W. Ju, et al. Techniques. Graph neural networks can be flexibly leveraged to encode molecular features on different representation levels and across different problem settings. Canonical GNNs like GCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular, this 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the binding protein receptor is considered alongside the ligand geometry per se. Fragment-based models like JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure. Various generative methods have also been effectively incorporated into the molecular setting, including generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to generate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating the search for drug candidates. Challenges and Limitations. While there has been an abundant supply of unlabelled molecular structural and geometric data [125, 193, 426], the number of labeled molecular data over certain critical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the other hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of the generated molecules, such as QED and Vina [103], rather than actual wet lab experiments. Future Works. Besides the structural and geometric attributes described in this chapter, an even more extensive array of data can be applied to aid molecular generation, including chemical reactions and medical ontology. These data can be organized into a heterogeneous knowledge graph to aid the extraction of high-quality molecular representations. Furthermore, high throughput experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and druggability of the generated molecules in the wet lab. Concrete case studies, such as the design of potential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights into leveraging these molecular generative models to facilitate the design and fabrication of potent and applicable drug molecules in the pharmaceutical industry. Integrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations offers a promising new direction in molecular generation. Recent studies like those by [196] and [160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLMbased approaches in this domain, including those by [338] and [18], predominantly utilize textual SMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The emerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image, and text, which could more comprehensively capture the intricacies of molecular structures. This approach marks a significant shift towards utilizing graph-based information alongside traditional text, enhancing the capability of LLMs in molecular generation. Such advances suggest that future research should focus more on exploiting the synergy between graph-based molecular representations and the evolving landscape of LLMs to address complex challenges in chemistry and material sciences. 13 Recommender Systems The use of graph representation learning in recommender systems has been drawing increasing attention as one of the key strategies for addressing the issue of information overload. With their strong ability to capture high-order connectivity between graph nodes, deep graph representation learning has been shown to be beneficial in enhancing recommendation performance across a variety of recommendation scenarios. Typical recommender systems take the observed interactions between users and items and their fixed features as input, and are intended for making proper predictions on which items a specific user is probably interested in. To formulate, given an user set U, an item set I and the J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 57 Table 12. Summary of graph models for recommender systems. Model Recommendation Task Graph Structure Graph Encoder Representation GC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer NGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation MMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer LightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling DGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling CAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling SR-GNN [496] Session-based Transition Graph GGNN Soft-Attention GC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention FGNN [377] Session-based Session Graph GAT Last-Layer GAG [378] Session-based Session Graph GCN Self-Attention GCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling HyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention DHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer MBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention HCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer H3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer STHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling interaction matrix between users and items X \u2208 {0, 1} |U|\u00d7|I| , where Xu,v indicates there is an observed interaction between user u and item i. The target of GNNs on recommender systems is to learn representations hu, hi \u2208 R d for given u and i. The preference score can..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      },
      "S4312425460": {
        "id": "S4312425460",
        "text": "Semi-supervised learning techniques, such as pseudo-labeling and consistency learning, are increasingly being integrated with graph representation learning to address the challenge of label scarcity in graph data, thereby improving model performance.",
        "children": [
          {
            "id": "E2946614232",
            "text": "..computing resources and can be a bottleneck, particularly for large graphs. The second is the reliance of Transformer-based models on large amounts of data for stable performance. It poses a challenge when dealing with problems that lack sufficient data, especially for few-shot and zero-shot settings. \u2022 Future Works. We expect efficiency improvement for Graph Transformer should be further explored. Additionally, there are some works using pre-training and fine-tuning frameworks J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 28 W. Ju, et al. to balance performance and complexity in downstream tasks [540], this may be a promising solution to address the aforementioned two challenges. 7 Semi-supervised Learning on Graphs We have investigated various architectures of graph neural networks in which the parameters should be tuned by a learning objective. The most prevalent optimization approach is supervised learning on graph data. Due to the label deficiency, semi-supervised learning has attracted increasing attention in the data mining community. In detail, these methods attempt to combine graph representation learning with current semi-supervised techniques including pseudo-labeling, consistency learning, knowledge distillation and active learning. These works can be further subdivided into node-level representation learning and graph-level representation learning. We would introduce both parts in detail as in Sec. 7.1 and Sec. 7.2, respectively. A summarization is provided in Table 6. 7.1 Node Representation Learning Typically, node representation learning follows the concept of transductive learning, which has access to test unlabeled data. We first review the simplest loss objective, i.e., node-level supervised loss. This loss exploits the ground truth of labeled nodes on graphs. The standard cross-entropy is usually adopted for optimization. In formulation, LN SL = \u2212 1 |YL | \u2211 i\u2208YL y T i log pi, (68) where YL denotes the set of labeled nodes. Additionally, there are a variety of unlabeled nodes that can be used to offer semantic information. To fully utilize these nodes, a range of methods attempt to combine semi-supervised approaches with graph neural networks. Pseudo-labeling [251] is a fundamental semi-supervised technique that uses the classifier to produce the label distribution of unlabeled examples and then adds appropriately labeled examples to the training set [265, 604]. Another line of semi-supervised learning is consistency regularization [247] that requires two examples to have identical predictions under perturbation. This regularization is based on the assumption that each instance has a distinct label that is resistant to random perturbations [118, 357]. Then, we show several representative works in detail. Cooperative Graph Neural Networks (CoGNet) [265]. CoGNet is a representative pseudo-labelbased GNN approach for semi-supervised node classification. It employs two GNN classifiers to jointly annotate unlabeled nodes. In particular, it calculates the confidence of each node as follows: CV (pi) = p T i log pi, (69) where pi denotes the output label distribution. Then it selects the pseudo-labels with high confidence generated from one model to supervise the optimization of the other model. In particular, the objective for unlabeled nodes is written as follows: LCoGN et = \u2211 i\u2208VU 1CV (pi )>\u03c4y\u02c6 T i logqi, (70) where y\u02c6i denotes the one-hot formulation of the pseudo-label y\u02c6i = argmaxpi and qi denotes the label distribution predicted by the other classifier. \u03c4 is a pre-defined temperature coefficient. This cross supervision has been demonstrated effective in [64, 312] to prevent the provision of biased pseudo-labels. Moreover, it employs GNNExplainer [541] to provide additional information from a dual perspective. Here it measures the minimal subgraphs where GNN classifiers can still generate the same prediction. In this way, CoGNet can illustrate the entire optimization process to J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 29 Table 6. Summary of methods for semi-supervised Learning on Graphs. Contrastive learning can be considered as a specific kind of consistency learning. Approach Pseudo-labeling Consistency Learning Knowledge Distillation Active Learning Node-level CoGNet [265] \u2713 DSGCN [604] \u2713 GRAND [118] \u2713 AugGCR [357] \u2713 HCPL [309] \u2713 Graph-level SEAL [264] \u2713 \u2713 InfoGraph [431] \u2713 \u2713 DSGC [527] \u2713 ASGN [166] \u2713 \u2713 TGNN [218] \u2713 KGNN [221] \u2713 HGMI [262] \u2713 \u2713 ASGNN [508] \u2713 \u2713 DualGraph [310] \u2713 \u2713 GLA [556] \u2713 SS [507] \u2713 enhance our understanding. HCPL [309] incorporates curriculum learning into pseudo-labeling in semi-supervised node classification, which can generate dynamics thresholds for reliable nodes. Dynamic Self-training Graph Neural Network (DSGCN) [604]. DSGCN develops an adaptive manner to utilize reliable pseudo-labels for unlabeled nodes. In particular, it allocates smaller weights to samples with lower confidence with the additional consideration of class balance. The weight is formulated as: \u03c9i = 1 nc i max (RELU (pi \u2212 \u03b2 \u00b7 1)) , (71) where nc i denotes the number of unlabeled samples assigned to the class c i . This technique will decrease the impact of wrong pseudo-labels during iterative training. Graph Random Neural Networks (GRAND) [118]. GRAND is a representative consistency learningbased method. It first adds a variety of perturbations to the input graph to generate a list of graph views. Each graph view G r is sent to a GNN classifier to produce a prediction matrix P r = [pr 1 , \u00b7 \u00b7 \u00b7 , p r N ]. Then it summarizes these matrices as: P = 1 R P r . (72) To provide more discriminative information and ensure that the matrix is row-normalized, GRAND sharpens the summarized label matrix into P SA as: P SA ij = P 1/T ij \u00cd j \u2032=0 P 1/T ij\u2032 , (73) where T is a given temperature parameter. Finally, consistency learning is performed by comparing the sharpened summarized matrix with the matrix of each graph view. Formally, the objective is: LGRAN D = 1 R \u2211 R r=1 \u2211 i\u2208V ||P SA i \u2212 Pi ||, (74) here LGRAN D serves as a regularization which is combined with the standard supervised loss. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 30 W. Ju, et al. Augmentation for GNNs with the Consistency Regularization (AugGCR) [357]. AugGCR begins with the generation of augmented graphs by random dropout and mixup of different order features. To enhance the model generalization, it borrows the idea of meta-learning to partition the training data, which improves the quality of graph augmentation. In addition, it utilizes consistency regularization to enhance the semi-supervised node classification. 7.2 Graph Representation Learning The objective of graph classification is to predict the property of the whole graph example. Assuming that the training set comprises N l and N u graph samples Gl = {G1 , \u00b7 \u00b7 \u00b7 ,GN l } and G u = {GN l +1 , \u00b7 \u00b7 \u00b7 ,GN l +Nu }, the graph-level supervised loss for labeled data can be expressed as follows: LGSL = \u2212 1 |Gu | \u2211 Gj \u2208 GL y jT logp j , (75) where y j denotes the one-hot label vector for the j-th sample while pj denotes the predicted distribution of G j . When N u = 0, this objective can be utilized to optimize supervised methods. However, due to the shortage of labels in graph data, supervised methods cannot reach exceptional performance in real-world applications [166, 285, 336, 538]. To tackle this, semi-supervised graph classification has been developed extensively. These approaches can be categorized into pseudolabeling-based methods, knowledge distillation-based methods and contrastive learning-based methods. Pseudo-labeling methods annotate graph instances and utilize well-classified graph examples to update the training set [217, 262, 264]. Knowledge distillation-based methods usually utilize a teacher-student architecture, where the teacher model conducts graph representation learning without label information to extract generalized knowledge while the student model focuses on the downstream task. Due to the restricted number of labeled instances, the student model transfers knowledge from the teacher model to prevent overfitting [166, 431]. Another line of this topic is to utilize graph contrastive learning, which is frequently used in unsupervised learning. Typically, these methods extract topological information from two perspectives (i.e., different perturbation strategies and graph encoders), and maximize the similarity of their representations compared with those from other examples [216, 218, 310]. Active learning, as a prevalent technique to improve the efficiency of data annotation, has also been utilized for semi-supervised methods [166, 508]. Then, we review these methods in detail. SEmi-supervised grAph cLassification (SEAL) [264]. SEAL treats each graph example as a node in a hierarchical graph. It builds two graph classifiers which generate graph representations and conduct semi-supervised graph classification respectively. SEAL employs a self-attention module to encode each graph into a graph-level representation, and then conducts message passing from a graph level for final classification. SEAL can also be combined with cautious iteration and active iteration. The former merely utilizes partial graph samples to optimize the parameters in the first classifier due to the potential erroneous pseudo-labels. The second combines active learning with the model, which increases the annotation efficiency in semi-supervised scenarios. InfoGraph [431]. Infograph is the first contrastive learning-based method. It maximizes the similarity between summarized graph representations and their node representations...",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          },
          {
            "id": "E5615187824",
            "text": "..works tend to use the autoregressive flow model, including GraphAF [413], GraphDF [318], GraphBP [288] and SiamFlow [439]. Diffusion model (DM). Diffusion models [175, 421, 425] define a Markov chain of diffusion steps to slowly add random noise to data x0 \u223c q(x): q(xt|xt\u22121) = N (xt; \u221a 1 \u2212 \u03b2txt\u22121, \u03b2t I), (137) q(x1:T |x0) = \u00d6 T t=1 q(xt|xt\u22121). (138) J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 55 Table 11. Summary of molecular generation models. Model 2D/3D Bindingbased FragmentbasedGNN Backbone Generative Model GCPN [543] 2D GCN [230] GAN MolGAN [82] 2D R-GCN [401] GAN DEFactor [16] 2D GCN GAN GraphVAE [419] 2D ECC [418] VAE MDVAE [100] 2D GGNN [274] VAE JT-VAE [207] 2D \u2713 MPNN [147] VAE CGVAE [290] 2D GGNN VAE DeepScaffold [270] 2D \u2713 GCN VAE GraphNVP [329] 2D R-GCN NF MoFlow [557] 2D R-GCN NF GraphAF [413] 2D R-GCN NF + AR GraphDF [318] 2D R-GCN NF + AR L-Net [272] 3D \u2713 g-U-Net [133] AR G-SchNet [142] 3D SchNet [405] AR GEN3D [388] 3D EGNN [398] AR G-SphereNet [316] 3D SphereNet [292] NF + AR EDM [178] 3D EGNN DM GCDM [347] 3D GCPNet [346] DM 3D-SBDD [306] 3D \u2713 SchNet AR Pocket2Mol [361] 3D \u2713 GVP [211] AR FLAG [581] 3D \u2713 \u2713 SchNet AR GraphBP [288] 3D \u2713 SchNet NF + AR SiamFlow [439] 3D \u2713 R-GCN NF DiffBP [282] 3D \u2713 EGNN DM DiffSBDD [403] 3D \u2713 EGNN DM TargetDiff [156] 3D \u2713 EGNN DM FragDiff [360] 2D + 3D \u2713 \u2713 MPNN DM + AR GraphVF [430] 2D + 3D \u2713 \u2713 SchNet NF + AR MolCode [579] 2D + 3D \u2713 EGNN NF + AR They then learn to reverse the diffusion process to construct desired data samples from the noise: p\u03b8 (x0:T ) = p(xT ) \u00d6 T t=1 p\u03b8 (xt\u22121 |xt), (139) p\u03b8 (xt\u22121 |xt) = N (xt\u22121; \u03bc\u03b8(xt, t), \u03a3\u03b8 (xt, t)), (140) while the models are trained using a variational lower bound. Diffusion models have been applied to generate unbounded 3D molecules in EDM [178] and GCDM [347], and binding-specific ligands in DiffSBDD [403], DiffBP [282] and TargetDiff [156]. Diffusion can also be applied to generate molecular fragments in autoregressive models, as is the case with FragDiff [360]. 12.3 Summary and prospects We wrap up this chapter with Table 11, which profiles existing molecular generation models according to their taxonomy for molecular featurization, the GNN backbone, and the generative method. This chapter covers the critical topics of molecular generation, which also elicit valuable insights into the promising directions for future research. We summarize these important aspects as follows. J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. 56 W. Ju, et al. Techniques. Graph neural networks can be flexibly leveraged to encode molecular features on different representation levels and across different problem settings. Canonical GNNs like GCN [230], GAT [452], and R-GCN [401] have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs have also been effective in modeling 3D molecular graphs. In particular, this 3D approach can be readily extended to binding-based scenarios, where the 3D geometry of the binding protein receptor is considered alongside the ligand geometry per se. Fragment-based models like JT-VAE [207] and L-Net [272] can also effectively capture the hierarchical molecular structure. Various generative methods have also been effectively incorporated into the molecular setting, including generative adversarial network (GAN), variational auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion model (DM). These models have been able to generate valid 2D molecular topologies and realistic 3D molecular geometries, greatly accelerating the search for drug candidates. Challenges and Limitations. While there has been an abundant supply of unlabelled molecular structural and geometric data [125, 193, 426], the number of labeled molecular data over certain critical biochemical properties like toxicity [141] and solubility [84] remain very limited. On the other hand, existing models have heavily relied on expert-crafted metrics to evaluate the quality of the generated molecules, such as QED and Vina [103], rather than actual wet lab experiments. Future Works. Besides the structural and geometric attributes described in this chapter, an even more extensive array of data can be applied to aid molecular generation, including chemical reactions and medical ontology. These data can be organized into a heterogeneous knowledge graph to aid the extraction of high-quality molecular representations. Furthermore, high throughput experimentation (HTE) should be adopted to realistically evaluate the synthesizablity and druggability of the generated molecules in the wet lab. Concrete case studies, such as the design of potential inhibitors to SARS-CoV-2 [273], would be even more encouraging, bringing new insights into leveraging these molecular generative models to facilitate the design and fabrication of potent and applicable drug molecules in the pharmaceutical industry. Integrating Large Language Models (LLMs) like GPT-4 [352] with graph-based representations offers a promising new direction in molecular generation. Recent studies like those by [196] and [160] highlight LLMs\u2019 potential in chemistry, especially in low-data scenarios. While current LLMbased approaches in this domain, including those by [338] and [18], predominantly utilize textual SMILES strings, their potential is somewhat constrained by the limits of text-only inputs. The emerging trend, exemplified by [289], is to leverage multi-modal data, integrating graph, image, and text, which could more comprehensively capture the intricacies of molecular structures. This approach marks a significant shift towards utilizing graph-based information alongside traditional text, enhancing the capability of LLMs in molecular generation. Such advances suggest that future research should focus more on exploiting the synergy between graph-based molecular representations and the evolving landscape of LLMs to address complex challenges in chemistry and material sciences. 13 Recommender Systems The use of graph representation learning in recommender systems has been drawing increasing attention as one of the key strategies for addressing the issue of information overload. With their strong ability to capture high-order connectivity between graph nodes, deep graph representation learning has been shown to be beneficial in enhancing recommendation performance across a variety of recommendation scenarios. Typical recommender systems take the observed interactions between users and items and their fixed features as input, and are intended for making proper predictions on which items a specific user is probably interested in. To formulate, given an user set U, an item set I and the J. ACM, Vol. 1, No. 1, Article . Publication date: February 2024. A Comprehensive Survey on Deep Graph Representation Learning 57 Table 12. Summary of graph models for recommender systems. Model Recommendation Task Graph Structure Graph Encoder Representation GC-MC [27] Matrix Completion User-Item Graph GCN Last-Layer NGCF [470] Collaborative Filtering User-Item Graph GCN+Affinity Concatenation MMGCN [485] Micro-Video Multi-Modal Graph GCN Last-Layer LightGCN [169] Collaborative Filtering User-Item Graph LGC Mean-Pooling DGCF [473] Collaborative Filtering User-Item Graph Dynamic Routing Mean-Pooling CAGCN [480] Collaborative Filtering User-Item Graph GCN+CIR Mean-Pooling SR-GNN [496] Session-based Transition Graph GGNN Soft-Attention GC-SAN [496, 516] Session-based Session Graph GGNN Self-Attention FGNN [377] Session-based Session Graph GAT Last-Layer GAG [378] Session-based Session Graph GCN Self-Attention GCE-GNN [482] Session-based Transition+Global GAT Sum-Pooling HyperRec [463] Sequence-based Sequential HyperGraph HGCN Self-Attention DHCF [198] Collaborative Filtering Dual HyperGraph JHConv Last-Layer MBHT [532] Sequence-based Learnable HyperGraph Transformer Cross-View Attention HCCF [505] Collaborative Filtering Learnable HyperGraph HGCN Last-Layer H3Trans [523] Sequence-based Hierarchical HyperGraph Message-passing Last-Layer STHGCN [524] POI Recommendation Spatio-temporal HyperGraph HGCN Mean-Pooling interaction matrix between users and items X \u2208 {0, 1} |U|\u00d7|I| , where Xu,v indicates there is an observed interaction between user u and item i. The target of GNNs on recommender systems is to learn representations hu, hi \u2208 R d for given u and i. The preference score can..",
            "url": "https://arxiv.org/pdf/2304.05055",
            "openalex_id": ""
          }
        ]
      }
    }
  },
  {
    "id": "https://openalex.org/W4388017359",
    "text": "1\nEnd-to-End Speech Recognition: A Survey\nRohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE,\nRalf Schluter, \u00a8 Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE\nAbstract\u2014In the last decade of automatic speech recognition\n(ASR) research, the introduction of deep learning has brought\nconsiderable reductions in word error rate of more than 50%\nrelative, compared to modeling without deep learning. In the\nwake of this transition, a number of all-neural ASR architectures\nhave been introduced. These so-called end-to-end (E2E) models\nprovide highly integrated, completely neural ASR models, which\nrely strongly on general machine learning knowledge, learn more\nconsistently from data, with lower dependence on ASR domain\u0002specific experience. The success and enthusiastic adoption of deep\nlearning, accompanied by more generic model architectures has\nled to E2E models now becoming the prominent ASR approach.\nThe goal of this survey is to provide a taxonomy of E2E ASR\nmodels and corresponding improvements, and to discuss their\nproperties and their relationship to classical hidden Markov\nmodel (HMM) based ASR architectures. All relevant aspects\nof E2E ASR are covered in this work: modeling, training,\ndecoding, and external language model integration, discussions of\nperformance and deployment opportunities, as well as an outlook\ninto potential future developments.\nIndex Terms\u2014end-to-end, automatic speech recognition.\nI. INTRODUCTION\nThe classical1statistical architecture decomposes an auto\u0002matic speech recognition (ASR) system into four main compo\u0002nents: acoustic feature extraction from speech audio signals,\nacoustic modeling, language modeling and search based on\nBayes\u2019 decision rule [1], [2], [3]. Classical acoustic modeling\nis based on hidden Markov models (HMMs) to account for\nspeaking rate variation. Within the classical approach, deep\nlearning has been introduced into acoustic and language mod\u0002eling. In acoustic modeling, deep learning has replaced Gaus\u0002sian mixture distributions (hybrid HMM [4], [5]) or augmented\nthe acoustic feature set (e.g., non-linear discriminant/tandem\napproach [6], [7]). In language modeling, deep learning has re\u0002placed count-based approaches [8], [9], [10]. However, in these\nearly attempts at introducing deep learning, the classical ASR\narchitecture was unmodified. Classical state-of-the-art ASR\nsystems today are composed of many separate components and\nknowledge sources: especially speech signal preprocessing;\nmethods for robustness with respect to recording conditions;\nphoneme inventories and pronunciation lexica; phonetic clus\u0002tering; handling of out-of-vocabulary words; various methods\nfor adaptation/normalization; elaborate training schedules with\ndifferent objectives including sequence discriminative training,\netc. The potential of deep learning, on the other hand, initiated\nsuccessful approaches to integrate formerly separate modeling\nsteps, e.g., by integrating speech signal pre-processing and\nfeature extraction into acoustic modeling [11], [12].\n1 The term \u201cclassical\u201d here refers to the former, long-term, state-of-the-art\nASR architecture based on the decomposition into acoustic and language\nmodel, and with acoustic modeling based on hidden Markov models.\nMore consequently, the introduction of deep learning to\nASR also initiated research to replace classical ASR archi\u0002tectures based on hidden Markov models (HMM) with more\nintegrated joint neural network model structures [13], [14],\n[15], [16]. These ventures might be seen as trading specific\nspeech processing models for more generic machine learning\napproaches to sequence-to-sequence processing \u2013 akin to how\nstatistical approaches to natural language processing have\ncome to replace more linguistically oriented models. For these\nall-neural approaches recently the term end-to-end (E2E) [14],\n[17], [18], [19] has been established. Therefore, first of all\nan attempt to define the term end-to-end in the context of\nASR is due in this survey. According to the Cambridge\nDictionary, the adjective \u201cend-to-end\u201d is defined as: \u201cinclud\u0002ing all the stages of a process\u201d [20]. We therefore propose\nthe following definition of end-to-end ASR: an integrated\nASR model that enables joint training from scratch; avoids\nseparately obtained knowledge sources; and, provides single\u0002pass recognition consistent with the objective to optimize the\ntask-specific evaluation measure, i.e., usually label (word,\ncharacter, subword, etc.) error rate. While this definition\nsuffices for the present discussion, we note that such an\nidealized definition hides many nuances involved in the term\nE2E and lacks distinctiveness; we elaborate on some of these\nnuances in Sec. II to discuss the various connotations of the\nterm E2E in the context of ASR.\nWhat are potential benefits of E2E approaches to ASR?\nThe primary objective when developing an ASR systems is to\nminimize the expected word error rate; secondary objectives\nare to reduce time and memory complexity of the resulting\ndecoder, and \u2013 assuming a constrained development budget \u2013\ngenericity, and ease of modeling. First of all, an integrated\nASR system, defined in terms of a single neural network\nstructure supports genericity of modeling and may allow for\nfaster development cycles when building ASR systems for\nnew languages or domains. Similarly, ASR models defined\nby a single neural network structure may become more \u2018lean\u2019\ncompared to classical modeling, with a simpler decoding\nprocess, obviating the need to integrate separate models. The\nresulting reduction in memory footprint and power consump\u0002tion supports embedded ASR applications [21], [22]. Further\u0002more, end-to-end joint training may help to avoid spurious\noptima from intermediate training stages. Avoiding secondary\nknowledge sources like pronunciation lexica may be helpful\nfor languages/domains where such resources are not easily\navailable. Also, secondary knowledge sources may themselves\nbe erroneous; avoiding these may improve models trained\ndirectly from data, provided that sufficient amounts of task\u0002specific training data are available.\nWith the current surge of interest in E2E ASR models and an\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nincreasing diversity of corresponding work, the authors of this\nreview think it is time to provide an overview of this rapidly\nevolving domain of research. The goal of this survey is to\nprovide an in-depth overview of the current state of research\non E2E ASR systems, covering all relevant aspects of E2E\nASR, with a contrastive discussion of the different E2E and\nclassical ASR architectures.\nThis survey of E2E speech recognition is structured as fol\u0002lows. Sec. II discusses the nuances in the term E2E as it applies\nto ASR. Sec. III describes the historical evolution of E2E\nspeech recognition, with specific focus on the input-output\nalignment and an overview of prominent E2E ASR models.\nSec. IV discusses improvements of the basic E2E models,\nincluding E2E model combination, training loss functions,\ncontext, encoder/decoder structures and endpointing. Sec. V\nprovides an overview of E2E ASR model training. Decoding\nalgorithms for the different E2E approaches are discussed\nin Sec. VI. Sec. VII discusses the role and integration of\n(separate) language models in E2E ASR. Sec. VIII reviews\nexperimental comparisons of the different E2E as well as\nclassical ASR approaches. Sec. IX provides an overview of\napplications of E2E ASR. Sec. X investigates future directions\nof E2E research in ASR, before concluding in Sec. XI. Finally,\nwe note that this survey paper also includes comparative\ndiscussions between novel E2E models and classical HMM\u0002based ASR approaches in terms of various aspects; most\nsections end with a summarization of the relationship between\nE2E models and HMM-based ASR approaches in relation to\nthe topics covered within the respective sections.\nII. DISTINCTIVENESS OF THE TERM E2E\nAs noted in Sec. I the term E2E provides an idealized\ndefinition of ASR systems, and can benefit from a more\ndetailed discussion based on the following perspectives.\na) Joint Modeling: In terms of ASR, the E2E property\ncan be interpreted as considering all components of an ASR\nsystem jointly as a single computational graph. Even more so,\nthe common understanding of E2E in ASR is that of a single\njoint modeling approach that does not necessarily distinguish\nseparate components, which may also mean dropping the\nclassical separation of ASR into an acoustic model and a\nlanguage model. However, in practice E2E ASR systems are\noften combined with external language models trained on text\u0002only data, which weakens the end-to-end nature of the system\nto some extent.\nb) Joint Training: In terms of model training, E2E can\nbe interpreted as estimating all parameters, of all components\nof a model jointly using a single objective function that is\nconsistent with the task at hand, which in case of ASR means\nminimizing the expected word error rate2. However, the term\nlacks distinctiveness here, as classical and/or modular ASR\nmodel architectures also support joint training with a single\nobjective.\n2 Note that this does not necessarily require Bayes Risk training, as standard\ntraining criteria like cross entropy, maximum mutual information and max\u0002imum likelihood in case of classical ASR models asymptotically guarantee\noptimal performance in the sense of Bayes decision rule, also [23], [24].\nc) Training from Scratch: The E2E property can also be\ninterpreted with respect to the training process itself, by re\u0002quiring training from scratch, avoiding external knowledge like\nprior alignments or initial models pre-trained using different\ncriteria or knowledge sources. However, note that pre-training\nand fine-tuning strategies are also relevant, if the model has\nexplicit modularity, including self-supervised learning [25] or\njoint training of front-end and speech recognition models [26].\nEspecially in case of limited amounts of target task training\ndata, utilizing large pretrained models is important to obtain\nperformant E2E ASR systems.\nd) Avoiding Secondary Knowledge Sources: For ASR,\nstandard secondary knowledge sources are pronunciation lex\u0002ica and phoneme sets, as well as phonetic clustering, which\nin classical state-of-the-art ASR systems usually is based on\nclassification and regression trees (CART) [27]. Secondary\nknowledge sources and separately trained components may\nintroduce errors, might be inconsistent with the overall training\nobjective and/or may generate additional cost. Therefore, in\nan E2E approach, these would be avoided. Standard joint\ntraining of an E2E model requires using a single kind of\ntraining data, which in case of ASR would be transcribed\nspeech audio data. However, in ASR often even larger amounts\nof text-only data, as well as optional untranscribed speech\naudio are available. One of the challenges of E2E modeling\ntherefore is how to take advantage of text-only and audio-only\ndata jointly without introducing secondary (pretrained) models\nand/or training objectives [28], [29].\ne) Direct Vocabulary Modeling: Avoiding pronunciation\nlexica and corresponding subword units leave E2E recognition\nvocabularies to be derived from whole word or character\nrepresentations. Whole word models [30], according to Zipf\u2019s\nlaw [31], would require unrealistically high amounts of tran\u0002scribed training data for large vocabularies, which might not\nbe attainable for many tasks. On the other hand, methods\nto generate subword vocabularies based on characters, like\nthe currently popular byte pair encoding (BPE) approach\n[32], might be seen as secondary approaches outside the E2E\nobjective, even more so if acoustic data is considered for\nsubword derivation [33], [34], [35], [36].\nf) Generic Modeling: Finally, E2E modeling also re\u0002quires genericity of the underlying modeling: task-specific\nconstraints are learned completely from data, in contrast to\ntask-specific knowledge which influences the modeling of\nthe system architecture in the first place. For example, the\nmonotonicity constraint in ASR may be learned completely\nfrom data in an end-to-end fashion (e.g., in attention-based\napproaches [16]), or it may directly be implemented, as in\nclassical HMM structures. However, model constraints may\nbe considered by way of regularization in E2E ASR model\ntraining, and can thus provide an alternative way to introduce\ntask-specific knowledge.\ng) Single-Pass Search: In terms of the recognition/search\nproblem, the E2E property can be interpreted as integrating all\ncomponents (models, knowledge sources) of an ASR system\nbefore coming to a decision. This is in line with Bayes\u2019\ndecision rule, which exactly requires a single global decision\nintegrating all available knowledge sources, which is supported\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nby both classical ASR models as well as E2E models. On\nthe other hand, multipass search is not only exploited by\nclassical ASR models, but also by E2E ASR models, the\nmost prominent case here being (external) language model\nrescoring.\nAll in all, we need to conclude that a) \u201cE2E\u201d does not\nprovide a clear distinction between classical and novel, so\u0002called E2E models, and b) the E2E property often is weakened\nin practice, leaving the term as a more general, idealized\nperspective on ASR modeling.\nIII. A TAXONOMY OF E2E MODELS IN ASR\nBefore we derive a taxonomy of E2E ASR modeling\napproaches, we first introduce our notation. We denote the\ninput speech utterance as X, which we assume has been pa\u0002rameterized into D-dimensional acoustic frames (e.g., log-mel\nfeatures) of length T\n\u2032\n: X = (x1, \u00b7 \u00b7 \u00b7 , xT\u2032 ), where xt \u2208 R\nD.\nWe denote the corresponding word sequences as C, which can\nbe decomposed into a suitable sequence of labels of length L:\nC = (c1, \u00b7 \u00b7 \u00b7 , cL), where each label cj \u2208 C. Our description is\nagnostic to the specific representation used for decomposing\nthe word sequence into labels; popular choices include char\u0002acters, words, or sub-word sequences (e.g., BPE [32], word\u0002pieces [37]).\nASR may be viewed as a sequence classification problem\nwhich maps a variable length input, X, into an output,\nC, of unknown length. Following Bayes\u2019 decision rule, any\nstatistical approach to ASR must determine how to model the\nword sequence posterior probability, P(C|X). Thus, a natural\ntaxonomy of E2E ASR modeling can be based on the various\nstrategies for modeling this word sequence posterior: i.e., how\nthe alignment problem between input and output sequence is\nhandled; and, how sequence modeling is decomposed to the\nlevel of individual input vectors xt\n\u2032 and/or output labels cl\n.\nWe find that it is useful to distinguish implicit and explicit\nmodeling approaches, based on the modeling of the sequence\u0002to-sequence alignment:\na) Explicit Alignment Modeling: does not necessarily\nrefer to the determination of a single unique alignment, but\ninstead introduces an explicit alignment modeled as a latent\nvariable, A:\nP(C|X) = X\nA\nP(C, A|X)\nb) Implicit Alignment Modeling: does not introduce a\nlatent alignment variable, but models the label sequence pos\u0002terior P(C|X) directly.\nExplicit alignment modeling approaches can mainly be\ndistinguished by their choice of latent variable; these can be\nencoded in terms of valid emission paths in corresponding\nfinite state automata (FSA) [38] which relate the input and\noutput sequences \u2013 the approach taken in our article. Typically,\nlatent variables in explicit alignment modeling in transducer\nE2E models introduce extensions to the output label set\nwith different forms of continuation labels (including, but not\nlimited to so-called blank labels).3\n3 For example, these extensions may also include explicit duration variables,\nleading to segmental models [39]. Such models can be rewritten into equiv\u0002alent transducer models [40], and vice-versa.\nA. Encoder and Decoder Modules\nIrrespective of the alignment modeling approach, following\nthe notation introduced in [41], it is useful to view all E2E\nASR models as being composed of an encoder module and\na decoder module. The encoder module, denoted H(X),\nmaps an input acoustic frame sequence, X, of length T\n\u2032\ninto a higher-level representation, H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ) of\nlength T (typically T \u2264 T\n\u2032\n). Note that the encoder output is\nindependent of the hypothesized label sequence. The decoder\nmodule models the label sequence posterior on top of the\nencoder output:\nP(C|X) = P\nC\nH(X)\n\u0001\nThus, we may distinguish different approaches based upon\nhow the output label sequence distribution (including potential\nlatent variables resulting from the alignment modeling) are de\u0002composed into individual label (and alignment) contributions;\nthese may occur per output label position, per encoder frame\nposition, or combinations thereof:\nP\nC[, A]\nH(X)\n\u0001\n=\nY\nL\ni=1\nP\nci[, ai]\nc\ni\u22121\n1\n[, ai\u22121\n1\n], vi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X))\u0001\nwhere the notation mi\u22121\n1\ncorresponds to the sequence\nof i \u2212 1 previous instances of the variables m; and,\nvi(c\ni\u22121\n1\n[, ai\u22121\n1\n], H(X)) denotes a context-vector that provides\nthe connection between encoder output, H(X), and the la\u0002bel output position, i. In general the context vector may\ndepend on the label context (and possibly the latent vari\u0002able context, for explicit alignment modeling approaches).\nApart from the underlying alignment model and corresponding\noutput label decomposition, decoder modules differ in terms\nof the assumptions on their label context c\ni\u22121\n1\n(and their\nlatent variable context a\ni\u22121\n1\n), which correspond to different\nconditional independence assumptions, and by their access to\nthe encoder output. For example, the local posterior may only\ndepend on a single encoder frame output (i.e., with the context\nvector being reduced to a single encoder frame\u2019s output):\nvi\nc\ni\u22121\n1\n, H(X)\n\u0001\n= hti(X). As we shall see in detail in the\nfollowing sections, the simplest case of an encoder frame\u0002level decomposition (with L = T, and ti = i) corresponds to\nCTC [13]; AED models [16] and their variants maintain the\nfull dependency of the context vector.\nFinally, different E2E models can also be distinguished by\nthe specific modeling choices that are involved in the design\nof the neural network used to implement the encoder and the\ndecoder. These might involve feed-forward neural networks,\nconvolutional neural networks, recurrent neural networks (ei\u0002ther uni-directional or bi-directional) [42], attention [43],\nand various combinations thereof (e.g., transformers [44] or\nconformers [45]). These modeling choices and corresponding\ntraining methods can be applied across E2E ASR models and\ntherefore do not enter the taxonomy of E2E ASR models\ndiscussed here. However, specific choices will be discussed as\npart of the exemplary E2E ASR models presented in Sec. VIII\nand Sec. IX and.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nB. Explicit Alignment Modeling Approaches\nEarly E2E modeling approaches modeled alignments explic\u0002itly through a latent variable, which is marginalized out (pos\u0002sibly, approximately) during training and inference. Examples\nof this family of approaches include connectionist temporal\nclassification (CTC) [13], the recurrent neural network trans\u0002ducer (RNN-T) [14], the recurrent neural aligner (RNA) [46],\nand the hybrid auto-regressive transducer [47] (HAT). As\nwill be discussed in subsequent sections, the latter modeling\napproaches in this family represent increasingly sophisticated\nmodeling of alignments, with fewer independence assumptions\nand are thus increasingly powerful. A common feature of all\nexplicit alignment models discussed in this section is that\nthey introduce an additional blank symbol, denoted \u27e8b\u27e9, and\ndefine an output probability distribution over symbols in the\nset Cb = C \u222a {\u27e8b\u27e9}. The interpretation of the \u27e8b\u27e9 symbol\nvaries slightly between each of these models, as we discuss in\ngreater details below. For now, it suffices to say that given\na specific training example, (X, C), each of these models\ndefines a set of valid alignments, denoted by A(T ,C), and\ndefine the conditional distribution P(C|X) by marginalizing\nover all valid alignment sequences:\nP(C|X) = X\nA\nP(C|A, H(X))P(A|H(X))\n=\nX\nA\u2208A(T =|H(X)|,C)\nP(A|H(X)) (1)\nwhere, by definition P(C|A, H(X)) = 1 if and only if A \u2208\nA(T ,C) and 0 otherwise.4 We discuss the specific formulations\nof each of these models in the subsequent sections.\n1) Connectionist Temporal Classification (CTC): Connec\u0002tionist Temporal Classification (CTC) was proposed by Graves\net al. [13] as a technique for mapping a sequence of input\ntokens to a corresponding sequence of output tokens. CTC ex\u0002plicitly models alignments between the encoder output, H(X),\nand the label sequence, C, by introducing a special \u201cblank\u201d la\u0002bel, denoted by \u27e8b\u27e9: Cb = C \u222a {\u27e8b\u27e9}. An alignment, A \u2208 C\u2217\nb\n, is\nthus a sequence of labels in C or \u27e8b\u27e9.\n5 Given a specific training\nexample, (X, C), we denote the set of all valid alignments,\nACTC\n(X,C) = {A = (a1, a2, . . . , aT )}, such that each at \u2208 Cb\nwith the additional constraint that A is identical to C after first\ncollapsing consecutive identical labels, and then removing all\nblank symbols. For example, if T = 10, and C = (s, e, e),\nthen A = (s,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ACTC\n(X,C)\n, as\nillustrated in Figure 1. As can be seen in this example, repeated\nlabels in the output can be represented by intervening blanks.\nFollowing Eq. (1), CTC defines the posterior probability\nof the label sequence C conditioned on the input, X, by\n4 This is equivalent to the assumption that the mapping from an alignment\nA to a label sequence C is unique, by definition. 5 S\n\u2217 denotes a Kleene\nclosure: the set of all possible sequences composed of tokens in the set S.\nTime\ns\ne\ne\n \ns\ne\ne\ns\ne\ne\ne\nFig. 1. Example alignment sequence for a CTC model with the target\nsequence C = (s, e, e) (right), alongside a (non-deterministic) finite state\nautomaton (FSA) [38] (left) representing the set of all valid alignment paths.\nEncoder H(X)\nSoftmax\nFig. 2. A representation of the CTC model consisting of an encoder which\nmaps the input speech into a higher-level representation, and a softmax layer\nwhich predicts frame-level probabilities over the set of output labels and blank.\nmarginalizing over all possible CTC alignments as:\nPCTC(C|X) = X\nA\u2208ACTC\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, \u00b7 \u00b7 \u00b7 , a1, H(X))\n=\nX\nA\u2208ACTC\n(X,C)\nY\nT\nt=1\nP(at|ht) (2)\nCritically, as can be seen in Eq. (2), CTC makes a strong\nindependence assumption that the model\u2019s output at time t is\nconditionally independent of the outputs at other timesteps,\ngiven the local encoder output at time t.\nThus, a CTC model consists of a neural network that\nmodels the distribution P(at|X), at each step as shown in\nFigure 2. The encoder is connected to a softmax layer with\n|Cb| targets representing the individual probabilities in Eq. (2):\nP(at = c|X) = P(at = c|H(X)), which comprises the\ndecoder module for CTC. Thus, at each step, t, the model\nconsumes a single encoded frame ht and outputs a distribution\nover the labels; in other words, the model \u201coutputs\u201d a single\nlabel either blank, \u27e8b\u27e9, or one of the targets in C.\n2) Recurrent Neural Network Transducer (RNN-T): The\nRecurrent Neural Network Transducer (RNN-T) [14], [48] was\nproposed by Graves as an improvement over the basic CTC\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 3. An RNN-T Model [14], [48] consists of an encoder which transforms\nthe input speech frames into a high-level representation, and a prediction\u0002network which models the sequence of non-blank labels that have been\noutput previously. The prediction network output, pit\n, represents the output\nafter producing the previous non-blank label sequence c1, . . . , cit\n. The\njoint network produces a probability distribution over the output symbols\n(augmented with blank) given the prediction network state and a specific\nencoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 4. Example alignment sequence (right) for an RNN-T model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs. The FSA (left) represents the set of all valid RNN-T\nalignment paths.\nmodel [13], by removing some of the conditional indepen\u0002dence assumptions that we discussed previously. The RNN\u0002T model, which is depicted in Figure 3, is best understood\nby contrasting it against the CTC model. As with CTC, the\nRNN-T model augments the output symbols with the blank\nsymbol, and thus defines a distribution over label sequences\nin Cb. Similarly, as with CTC, the model consists of an encoder\nwhich processes the input acoustic frames X to generate the\nencoded representation H(X) = (h1, \u00b7 \u00b7 \u00b7 , hT ).\nUnlike CTC, however, the blank symbol in RNN-T has a\nslightly different interpretation; for each input encoder frame,\nht, the RNN-T model outputs a sequence of zero or more\nsymbols in C which are terminated by a single blank symbol.\nThus, we may define the set of all valid alignment se\u0002quences in RNN-T as: ARNNT\n(X,C) = {A = (a1, a2, \u00b7 \u00b7 \u00b7 , aT +L)},\nthe set of all sequences of T + L symbols in C\n\u2217\nb\n, which\nare identical to C after removing all blanks. Finally, for a\ngiven output position \u03c4 , let i\u03c4 denote the number of non\u0002blank labels in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121). Thus, the\nnumber of blanks in the partial sequence (a1, \u00b7 \u00b7 \u00b7 , a\u03c4\u22121) is\n\u03c4 \u2212 i\u03c4 \u2212 1. For example, if T = 7, and C = (s, e, e),\nthen A = (\u27e8b\u27e9, s,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9, e, e,\u27e8b\u27e9,\u27e8b\u27e9,\u27e8b\u27e9) \u2208 ARNNT\n(X,C)\n.\nNote that, unlike the CTC model, repeated labels in the output\nrequire no special treatment as illustrated in Figure 4, where,\ni1 = i2 = 0;i3 = i4 = 1;i10 = 3; etc.\nWe may then define the posterior probability P(C|X) as\nbefore:\nPRNNT(C|X) = X\nA\u2208ARNNT\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |a\u03c4\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |ci\u03c4\n, ci\u03c4 \u22121, . . . , c0, h\u03c4\u2212i\u03c4\n)\n(3)\n=\nX\nA\u2208ARNNT\n(X,C)\nT\nY\n+L\n\u03c4=1\nP(a\u03c4 |pi\u03c4, h\u03c4\u2212i\u03c4)\nwhere, P = (p1, \u00b7 \u00b7 \u00b7 , pL) represents the output of the predic\u0002tion network depicted in Figure 3 which summarizes the se\u0002quence of previously predicted non-blank labels, implemented\nas another neural network: pj = NN(\u00b7|c0, . . . , cj\u22121), where\nc0 is a special start-of-sentence label, \u27e8sos\u27e9. Thus, as can be\nseen in Eq. (2), RNN-T reduces some of the independence\nassumptions in CTC since the output at time t is conditionally\ndependent on the sequence of previous non-blank predictions,\nbut is independent of the specific choice of alignment (i.e.,\nthe choice of the frames at which the non-blank tokens were\nemitted).\nOur presentation of RNN-T alignments considers the\n\u201ccanonical\u201d case. In principle, however, the model can encode\nthe same set of conditional independence assumptions in\nRNN-T (i.e., the model structure), while considering alter\u0002native alignment structures as in the work of [49]. In their\nwork, Moritz et al., represent valid frame-level alignments as\nan arbitrary graph. This formulation, for example, allows for\nthe use of \u201cCTC-like\u201d alignments in the RNN-T model (i.e.,\noutputting a single label \u2013 blank, or non-blank \u2013 at each frame)\nwhile conditioning on the set of previous non-blank symbols\nas in the RNN-T model.\n3) Recurrent Neural Aligner (RNA): The recurrent neural\naligner (RNA) was proposed by Sak et al. [46]. The RNA\nmodel generalizes the RNN-T model by removing one of its\nconditional independence assumptions. The model, depicted\nin Figure 5, is best understood by considering how it differs\nfrom the RNN-T model. As with CTC and RNN-T, the RNA\nmodel defines a probability distribution over blank augmented\nlabels in the set Cb, where \u27e8b\u27e9 has the same semantics\nas in the CTC model: at each frame the model can only\noutput a single label \u2013 either blank, or non-blank \u2013 before\nadvancing to the next frame; unlike CTC (but as in RNN\u0002T) the model only outputs a single instance of each non\u0002blank label. More specifically, the set of valid alignments,\nARNA\n(X,C) = (a1, \u00b7 \u00b7 \u00b7 , aT ), in the RNA model consist of length T\nsequences in C\n\u2217\nb with exactly T \u2212L blank symbols, and which\nare identical to C after removing all blanks. Thus, the blank\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nEncoder H(X)\nSoftmax\nJoint Network\nPrediction\nNetwork\nFig. 5. An RNA Model [46] resembles the RNN-T model [14], [48] in\nterms of the model structure. However, this model is only permitted to output\na single label \u2013 either blank, or non-blank \u2013 in a single frame. Unlike\nRNN-T, the prediction network state in the RNA model, qt\u22121, depends on\nthe entire alignment sequence at\u22121, . . . , a1. The joint network produces a\nprobability distribution over the output symbols (augmented with blank) given\nthe prediction network state and a specific encoded frame.\nTime\ns\ne\ne\ns\ne\ne\nFig. 6. Example alignment sequence (right) for an RNA model with the\ntarget sequence C = (s, e, e). Horizontal transitions in the image correspond\nto blank outputs; diagonal transitions correspond to outputting a non-blank\nsymbol. The FSA (left) represents the set of valid alignments for the RNA\nmodel. Although the FSA is identical to the corresponding FSA for RNN-T\nin Figure 4, the semantics of the \u27e8b\u27e9 label are different in the two cases.\nsymbol has a different interpretation in RNA and the RNN\u0002T models: in RNN-T, outputting a blank symbol advances\nthe model to the next frame; in RNA, however, the model\nadvances to the next frame after outputting a single blank or\nnon-blank label. Restricting the model to output a single non\u0002blank label at each frame improves computational efficiency\nand simplifies the decoding process, by limiting the number\nof model expansions at each frame (in constrast to RNN-T\ndecoding). For example, if T = 8, and C = (s, e, e), then\nA = (\u27e8b\u27e9, s,\u27e8b\u27e9, e,\u27e8b\u27e9,\u27e8b\u27e9, e,\u27e8b\u27e9) \u2208 ARNA\n(X,C)\nas illustrated\nin Figure 6.\nThe RNA posterior probability, P(C|X), is defined as:\nPRNA(C|X) = X\nA\u2208ARNA\n(X,C)\nP(A|H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|at\u22121, . . . , a1, H(X))\n=\nX\nA\u2208ARNA\n(X,C)\nY\nT\nt=1\nP(at|qt\u22121, ht) (4)\nwhere, as before it denotes the number of non-blank sym\u0002bols in the partial alignment sequence (a1, . . . , at\u22121), and\nqt\u22121 = NN(\u00b7|at\u22121, \u00b7 \u00b7 \u00b7 , a1) represents the output of a neu\u0002ral network which summarizes the entire partial alignment\nsequence, where NN(\u00b7) represents a suitable neural network\n(an LSTM in [46]). Thus, RNA removes the one remaining\nconditional independence assumption of the RNN-T model,\nby conditioning on the sequence of previous non-blank labels\nas well as the alignment that generated them. However, this\ncomes at a cost: the exact computation of the log-likelihood in\nEq. (3) (and corresponding gradients) is intractable. Instead,\nRNA makes two simplifying assumption to ensure tractable\ntraining: by assuming that the model can only output a single\nlabel at each frame; and utilizing a straight-through estimator\nfor the alignment [50]. The latter constraint \u2013 allowing only a\nsingle label (blank or non-blank) at each frame \u2013 has also been\nexplored in the context of the monotonic RNN-T model [51].\nFinally, we note that the work in [52] further generalizes\nthe RNA model by employing two RNNs when defining the\nstate: a slow RNN (which corresponds to the sequence of\npreviously predicted non-blank labels), and a fast RNN (which\nalso conditions on the frames at which the non-blank labels\nwere output).\nC. Implicit Alignment Modeling Approaches\nOne of the main benefits of the explicit alignment ap\u0002proaches such as CTC, RNN-T, or RNA is that they result in\nASR models that are easily amenable to frame-synchronous\ndecoding6In this section, we discuss the attention-based\nencoder-decoder (AED) models (also known as, listen-attend\u0002and-spell (LAS)) [15], [16], [53], which employs the attention\nmechanism [43] to implicitly identify and model the portions\nof the input acoustics which are relevant to each output\nunit. These models were first popularized in the context of\nmachine translation [54]. Unlike explicit alignment modeling\napproaches, attention-based encoder-decoder models use an\nattention mechanism [43] to learn a correspondence between\nthe entire acoustic sequence and the individual labels. Such\nmodels support label-synchronous decoding, meaning that\nduring inference, each hypothesis in the beam is expanded\nby 1 label.\nIn the explicit alignment approaches presented in Sec\u0002tion III-B, during inference, the model continues to output\nsymbols until it has processed the final frame at which point\nthe decoding process is complete; similarly, during training,\nthe forward-backward algorithm aligns over all possible align\u0002ment sequences. Since an AED model processes the entire\nacoustic sequence at once, the model needs a mechanism\nby which it can indicate that it is done emitting all output\nsymbols. This is achieved by augmenting the set of outputs\nwith an end-of-sentence symbol, \u27e8eos\u27e9, so that the output\nvocabulary consists of the set Ceos = C \u222a {\u27e8eos\u27e9}. Thus,\nthe AED model, depicted in Figure 7, consists of an en\u0002coder network \u2013 which encodes the input acoustic frame\n6 By frame-synchronous decoding, we refer to the ability of the model to\nproduce output label for each input frame of speech. Models such as CTC,\nRNN-T, or RNA, support frame-synchronous decoding.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nSoftmax\nDecoder\nAttention\nEncoder H(X)\nFig. 7. An attention-based encoder decoder (AED) model [15], [16], [53].\nThe output distribution is conditioned on the decoder state, si (which\nsummarizes the previously decoded symbols), and the context vector, vi\n(which summarizes the encoder output based on the decoder state). In the\nseminal work of Chan et al., [16], for example, this is accomplished by\nconcatenating the two vectors, as denoted by the L symbol in the figure.\nsequence, X = (x1, . . . , xT\u2032 ), into a higher-level representa\u0002tion H(X) = (h1, . . . , hT ) \u2013 and an attention-based decoder\nwhich defines the probability distribution over the set of output\nsymbols, Ceos. Thus, given a paired training example, (X, C),\nwe denote by Ce = (c1, . . . , cL,\u27e8eos\u27e9), the ground-truth\nsymbol sequence of length (L + 1) augmented with the \u27e8eos\u27e9\nsymbol. AED models compute the conditional probability of\nthe output sequence augmented with the \u27e8eos\u27e9 symbol as:\nP(Ce|X) = P(Ce|H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, H(X))\n=\nL\nY\n+1\ni=1\nP(ci|ci\u22121, . . . , c0 = \u27e8sos\u27e9, vi)\n=\nL\nY\n+1\ni=1\nP(ci|si, vi) (5)\nwhere, vi corresponds to a context vector, which summarizes\nthe relevant portions of the encoder output, H(X), given\nthe sequence of previous predictions ci\u22121, . . . , c0; and, si\ncorresponds to the corresponding decoder state after outputting\nthe sequence of previous symbols, which is produced by\nupdating the decoder state based on the previous context vector\nand output label:\nsi = Decoder(vi\u22121, si\u22121, ci\u22121)\nThe symbol c0 = \u27e8sos\u27e9 is a special start-of-sentence symbol\nwhich serves as the first input to the attention-based decoder\nbefore it has produced any outputs. As can be seen in Eq. (5),\nan important benefit of AED models over models such as\nCTC or RNN-T is that they do not make any independence\nassumptions between model outputs and the input acoustics,\nTime\ns\ne\ne\nFig. 8. Unlike models such as RNN-T or CTC, AED models do not have\nexplicit alignment. However, it is possible to interpret the attention weights\n\u03b1t,i for a particular output symbol ci as an alignment weight which is\nrepresented above for the target sequence C = (s, e, e, \u27e8eos\u27e9). In this\nrepresentation, the size of the circle and the darkness level are proportional\nto the corresponding attention weights; thus the total probability mass is the\nsame for each row. As illustrated above, the first few frames correspond to\nthe first symbol c1 = s, while the latter frames correspond to the second \u2018e\u2019:\nc3 = e.\nand are thus more general than the implicit alignment models,\nwhile being considerably easier to train and implement since\nwe do not have to explicitly marginalize over all possible\nalignment sequences. However, this comes at a cost: previ\u0002ously generated context vectors (which are analogous to the\ndecoded partial alignment in explicit alignment models) are\nnot revised as the decoding proceeds. Stated another way,\nwhile the encoder processing H(X) might be bi-directional,\nthe decoding process in AED models reveals a left-right\nasymmetry [55].\n1) Computing the Context Vector in AED Models: As\nwe mentioned before, the context vector, vi, is computed\nby employing the attention mechanism [43]. The central\nidea behind these approaches is to define a state vector si\nwhich corresponds to the state of the model after outputting\nc1, . . . , ci\u22121. The attention function, atten(ht, si) \u2208 R, then\ndefines a score between the model state after outputting\ni \u2212 1 previous symbols, and each of the encoded frames in\nH(X). These scores can then be normalized using the softmax\nfunction to define a set of weights corresponding to each ht\nas:\n\u03b1t,i =\nexp {atten(ht, si)}\nPT\nt\n\u2032=1 exp {atten(ht\n\u2032 , si)}\nIntuitively, the weight \u03b1t,i represents the relevance of a\nparticular encoded frame ht when outputting the next symbol\nci, after the model has already output the symbols c1, . . . , ci\u22121,\nas illustrated in Figure 8. The context vector summarizes the\nencoder output based on the computed attention weights:\nvi =\nX\nt\n\u03b1t,iht\nA number of possible attention mechanisms have been\nexplored in the literature: the most common forms are called\n\u2018content-based attention\u2019, which include dot-product atten\u0002tion [16] and additive attention [43]. The content-based atten\u0002tion computes the attention score atten(ht, si) based on the\nrelevance between ht and si. However, the score does not\nconsider location information, i.e., it is determined by only the\ncontent, independent of the position. This can lead to incorrect\nattention weights with a large discrepancy against the previous\nsteps. Thus, location-based attention atten(si,fi,j ) has been\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nproposed [15], where fi,j is a convolutional feature vector\nextracted from \u03b1i\u22121, the attention weights in the previous step.\nThe hybrid attention, i.e., a combination of the content- and\nlocation-based attentions, has also been investigated in [15],\nshowing a higher accuracy than the separate ones. Besides,\nother location-based methods use a Gaussian (mixture) model\nestimated with sito obtain attention weights [56], [57].\nTransformer model [44] uses only content-based dot-product\nattention, but also takes location information into account\nthrough positional encoding. Apart from the specific choice\nof the attention mechanism, a common technique to improve\nperformance involves the use of multiple independent attention\nheads \u2013 v\n1\ni\n, . . . , v\nK\ni\n\u2013 which are then concatenated together\nto obtain the final context vector vi =\n\u0002\nv\n1\ni\n; . . . ; v\nK\ni\n\u0003\n, in\nthe so-called multi-head attention approach [44], or indeed\nby stacking together multiple attention-based layers in the\ntransformer decoder presented by Vaswani et al. [44].\nD. From Implicit to Explicit Alignment Modeling\nAED models, which make no conditional independence\nassumptions, are extremely powerful, often outperforming\nexplicit alignment E2E approaches such as CTC, or RNN\u0002T [41]. However, these models also have some significant\ndisadvantages, most notably that the models are typically non\u0002streaming: i.e., the models must process all acoustic frames\nbefore they can generate any output hypotheses. A somewhat\nrelated issue is that the models are extremely sensitive to\nthe length of the acoustic sequences, which requires special\nprocessing to be able to decode long-form audio [58]. There\nis a body of work that lies in between these two extremes:\nmodels such as the neural transducer [59], or those based on\nmonotonic alignments [60] and its variants (e.g., monotonic\nchunkwise alignments (MoChA) [61], monotonic infinite look\u0002back (MILK) [62] etc.) use an explicit alignment model, while\nalso utilizing an attention mechanism that allows the model\nto examine local acoustics in order to refine predictions. In\nother words, this corresponds to a class of streaming AED\nmodels. Generally speaking, these models are motivated by\nthe observation that speech (unlike tasks such as machine\ntranslation) exhibits a \u2018local\u2019 relationship between the encoded\nframes (assuming that the encoder is uni-directional) and\nthe output units; thus, unlike the general AED model which\ncomputes the context vector, vi, as a sum over all input\nframes ht, the various proposed models constrain this sum\nto be computed over a subset of frames to allow for streaming\ndecoding. In the context of our presentation, it is easiest to\nthink of these models as consisting of an underlying alignment\n(whether known or unknown) which can be used to perform\nstreaming inference.\nThe Neural Transducer (NT) [59] explicitly partitions the in\u0002put encoder frames into T W non-overlapping chunks of length\nW: HW\n1 = [h1, . . . , hW ]; \u00b7 \u00b7 \u00b7 ; HW\nT W = [hT W +1, . . . , hT W W ],\nwhere T W =\n\u0006\nT\nW\n\u0007\n, and ht = 0 if t > T. Unlike the AED\nmodel which examines all encoded frames when computing\nthe context vector, the NT model is restricted to process\na single chunk at a time; the model only advances to the\nnext chunk when it outputs a special end-of-chunk symbol\n(analogous to \u27e8eos\u27e9 in the AED model); inference in the model\nterminates when the model has output the end-of-chunk sym\u0002bol in the final chunk HW\nT W . If the alignments of the ground\u0002truth output sequence, C, with respect to the W-length chunks\nare unknown, then it is possible to train the system by using a\nrough initial alignment where symbols are distributed equally\namong the T W chunks, followed by iterative refinement by\ncomputing the most likely output alignments given the current\nmodel parameters [59] similar to forced-alignments in HMM\u0002based systems. An alternate approach [63] consists of using a\nseparate system (e.g., a classical hybrid system) to get initial\nalignments (e.g., word-level alignments), which can be used\nto assign sub-word units to the individual chunks.\nAn alternative approach, proposed by Raffel et al. [60],\nmodifies the vanilla AED model by explicitly introducing an\nalignment module which scans the encoder frames, H(X),\nfrom left-to-right to identify whether the current frame should\nbe used to emit any outputs (modeled as a Bernoulli random\nvariable). If a frame, \u03c4 , is selected, then the model produces\nan output based on the local encoder frame, h\u03c4 . The process\nis then repeated starting from the currently selected frame,\nthus allowing multiple outputs to be generated at the same\nframe. This results in a model with hard monotonic alignments\nbetween the input speech and the output labels since the\nmodels are constrained to generate outputs in a streaming fash\u0002ion. A Monotonic Chunkwise Attention (MoChA) model [61]\nimproves upon the work of Raffel et al., by allowing the model\nto generate the next output using a context vector computed\nusing attention over a local window of frames to the left of the\nselected frame \u03c4 : h\u03c4\u2212W+1, . . . , h\u03c4 . Thus, the MoChA model\nconsists of a two-level process \u2013 identifying frames where\noutput should be produced following [60], followed by an\nAED model over frames to the left of the selected frame. A\nrefinement to the MoChA model, proposed by Arivazhagan et\nal. [62] \u2013 the monotonic infinite lookback (MILK) attention\nmodel \u2013 computes the context vector over all frames to the\nleft of the selected frame \u03c4 (i.e., h1, . . . , h\u03c4 ) at each step.\nAnother two-fold approach to enable streaming operation is\npresented in [64] under the term of triggered attention, where\na CTC-network is used to trigger, i.e. control the activation\nof an AED model with a limited decoder delay. We also\ndirect interested readers to studies of various attention variants:\nMerboldt et al. [65] compare a number of local monotonic\nattention variants; Zeyer et al. [66] discuss segmental attention\nvariants; Zeyer et al. [67] study the related decoding and the\nrelevance of segment length modeling, leading to improved\ngeneralization towards long sequences. Segmental attention\nmodels are related to transducer models [68]. However, seg\u0002mental E2E ASR models are not limited to be realized based\non the attention mechanism and may not only be related to a\ndirect HMM [39], but have also been shown to be equivalent\nto neural transducer modeling [40], thus even providing a clear\nrelation between duration modeling and blank probabilities.\nRelationship to Classical ASR\nIn classical ASR models, these frame-level alignments can\nbe modeled with HMMs while using generative GMMs or\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nneural networks to model the output distribution of acous\u0002tic frames; frame-level alignments to train neural network\nacoustic models may be obtained by force-alignment from a\nbase GMM-HMM systems, but direct sequence training not\nrequiring initial alignments is also possible [69].\nE2E models introduce alternative alignment modeling ap\u0002proaches to ASR. While the attention mechanism provides\na qualitatively novel approach to map acoustic observation\nsequences to label sequences, transducer approaches [13], [14],\n[46], [70] handle the alignment problem in a way that can\nbe interpreted to be similar to HMMs with a specific model\ntopology, including marginalization over alignments [71], [72],\n[73]. CTC models can also be employed in an HMM-like fash\u0002ion during decoding [74]. Moreover, transducer approaches are\nequivalent to segmental models/direct HMM [40].\nAnother prominent feature of E2E systems besides the\nalignment property is their direct character-level modeling\navoiding phoneme-based modeling and pronunciation lexica\n[19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82],\nwith some even heading for whole-word modeling [76], [30].\nHowever, character-level modeling also is viable with classical\nhybrid HMM architectures [83] and has been shown to work\neven with standard HMM models w/o neural networks [84].\nIV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E\nMODELS\nIn this section, we describe various algorithmic changes\nto vanilla E2E models which are critical in order to obtain\nimproved performance over classical ASR systems. First, we\ndescribe various ways of combining different complementary\nE2E models to improve performance. Next, we introduce\nways to incorporate context into these models to improve\nperformance on rare proper noun entities. We then describe\nimproved encoder and decoder architectures that take better\nadvantage of the many cores on specialized architectures such\nas tensor processing units (TPUs) [85]. Finally, we discuss how\nto improve the latency of the model through an integrated E2E\nendpointer.\nA. Combinations of Models\nDifferent end-to-end models are complementary, and there\nhave been numerous attempts at combining these methods.\nFor example, Watanabe et al. [86] find that attention-based\nmodels perform poorly on long or noisy utterances, mainly\nbecause the model has too much flexibility in predicting\nalignments when presented with the entire input utterance.\nIn contrast, models such as CTC \u2013 which have left-to-right\nconstraints during decoding \u2013 perform much better in these\ncases. They propose to employ a multi-task learning strategy\nwith both CTC and attention-based losses, which provides a\n5\u201314% relative improvement in word error rate over attention\u0002based models on Wall Street Journal (WSJ) and Chime tasks.\nPang et al. [87] explore combining the benefits of RNN-T\nand AED. Specifically, RNN-T decodes utterances in a left\u0002to-right fashion, which works well for long utterances. On\nthe other hand, since AED sees the entire utterance, it often\nshows improvements for utterances where surrounding context\nis needed to predict the current word, e.g., \"one dollar\nand fifty cents\" \u2192 $1.50. To combine RNN-T and\nAED, the authors propose to produce a first-pass result with\nRNN-T, that is then rescored with AED in the second-pass.\nTo reduce computation, the authors share the encoder between\nRNN-T and AED. The authors find that RNN-T + AED\nprovides a 17\u201322% relative improvement in word error rate\nover RNN-T alone on a voice search task. Other flavors\nof streaming 1st-pass following by attention-based 2nd-pass\nrescoring, such as deliberation [88], have also been explored.\nOne of the issues with such rescoring approaches is that any\npotential improvements are limited to the lattice produced by\nthe 1st-pass system. To address this, methods which run a\n2nd-pass beam search have also been explored, particularly in\nthe context of streaming ASR \u2013 e.g. cascaded encoder [89],\nY-architecture [90] and Universal ASR [91].\nB. Incorporating Context\nContextual biasing to a specific domain, including a user\u2019s\nsong names, app names and contact names, is an impor\u0002tant component of any production-level automatic speech\nrecognition (ASR) system. Contextual biasing is particularly\nchallenging in E2E models because these models typically\nretain only a small list of candidates during beam search, and\ntend to perform poorly when recognizing words that are seen\ninfrequently during training (typically named entities), which\nis the main source of biasing phrases. There have been a few\napproaches in the literature to incorporate context.\nOne approach, known as shallow-fusion contextual bias\u0002ing [92], constructs a stand-alone weighted finite state trans\u0002ducer (FST) representing the biasing phrases. The scores from\nthe biasing FST are interpolated with the scores of the E2E\nmodel during beam search, with special care taken to ensure\nwe do not over- or under-bias phrases. An alternate approach\nproposes to inject biasing phrases into the model in an all\u0002neural fashion. For example, Pundak et al. [93] represent a\nset of biasing phrases by embedding vectors. These vectors\nare fed as additional input to an attention-based model, which\ncan then choose to attend to the phrases and hence boost the\nchances of predicting the phrases. Kim and Metze [94] propose\nto bias towards dialog context. In addition, Bruguier et al. [95]\nextend [93], by leveraging phonemic pronunciations for the\nbiasing phrases when constructing phrase embeddings. Finally,\nDelcroix et al. [96] use an utterance-wise context vector like an\ni-vector computed by a pooling across frame-by-frame hidden\nstate vectors obtained from a sub network (this sub-network\nis called a sequence-summary network).\nC. Encoder and Decoder Structure\nThere have been improvements to encoder architectures\nof E2E models over time. The first end-to-end models used\nlong short-term memory recurrent neural networks (LSTMs),\nfor both the encoder and decoder. The main drawback of\nthese sequential models is that each frame depends on the\ncomputation from the previous frame, and therefore multiple\nframes cannot be batched in parallel.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nWith the improvement of hardware, specifically on-device\nEdge Tensor Processing Units (TPUs), with thousands of\ncores, architectures that can better take advantage of the\nhardware, have been explored. Such architectures include\nconvolution-based architectures, such as ContextNet [97]. The\nuse of self-attention to replace the sequential recurrence\nin LSTMs was explored in Transformers for ASR [98],\n[99], [100]. Finally, combining self-attention with convolution,\nknown as Conformer [45], or multi-layer perceptron [101],\nwas also explored. Both Transformer and Conformer have\nshown competitive performance to LSTMs on many tasks\n[102], [103].\nOn the decoder side, research for transducer models has\nshown that a large LSTM decoder can be replaced with a sim\u0002ple embedding lookup table, that attends to only a few previous\ntokens from the model [47], [104], [105], [106], [107]. This\ndemonstrates that most of the power of the E2E model is in\nthe encoder, which has been a consistent theme of both E2E\nas well as classical hybrid HMM models. However, improved\ndecoder modeling may also be effective depending on the\nspecific downstream task. Research has shown that extended\ndecoder architectures enable pre-training and adaptation of the\ndecoder using extensive text-only data, leading to accuracy\ngains [108], [109]. For example, one approach separates RNN\u0002T\u2019s prediction network into separate blank and vocabulary\nprediction (LM) components, where the LM component can\nbe trained with text data [108]. In addition, in line with the\ngrowing interest in large language models in recent years,\nresearch has also begun on solving multiple tasks, including\nspeech recognition, using only an auto-regressive, GPT-style\ndecoder [110], [111].\nD. Integrated Endpointing\nAn important characteristic of streaming speech recognition\nsystems is that they must endpoint quickly, so that the ASR\nresult can be finalized and sent to the server for the appro\u0002priate action to be performed. Endpointing is typically done\nwith an external voice-activity detector. Since endpointing is\nboth an acoustic and language model decision, recent works\nin streaming RNN-T models [112], [113] have investigated\npredicting a microphone closing token \u27e8eos\u27e9 at the end of the\nutterance \u2013 e.g., \u201cWhat\u2019s the weather \u27e8eos\u27e9\u201d. Following the\nnotation from Section III, this is done by including an \u27e8eos\u27e9\ntoken as part of the set of class labels C and encouraging\nthe model to predict this token to terminate decoding. These\nmodels have shown improved latency and WER trade-off\nby having the endpointing decision predicted as part of the\nmodel. Furthermore, [114], [115] explored using the CTC\nblank symbol for endpoint detection.\nV. TRAINING E2E MODELS\nIn general, training of E2E models follows deep learn\u0002ing schemes [116], [117], with specific consideration of the\nsequential structure and the latent alignment problem to be\nhandled in ASR. E2E ASR models may be trained end-to\u0002end, notwithstanding potential elaborate training schedules\nand extensive data augmentation. Part of the appeal of end\u0002to-end models is that they do not assume conditional in\u0002dependence between the input frames. Given a training set\nT = {(Xn, Cn)}\nN\nn=1, the training criterion L to be minimized\ncan be written as: L = \u2212\nPN\nn=1 log P(Cn|Xn) (which is\nequivalent to maximizing the total conditional log-likelihood).\nA. Alignment in Training\nE2E models such as RNN-T and CTC introduce an addi\u0002tional blank token \u27e8b\u27e9 for alignment. Therefore optimization\nimplies marginalizing across all alignments, as follows:\nLex = \u2212\nX\nN\nn=1\nX\nAn\nlog P(Cn, An|Xn)\nThis requires the forward-backward algorithm [118], [119] for\nefficient computation of the training criterion and its gradient,\nwith minor modifications for CTC, RNN-T, and RNA models,\nas well as classical (full-sum) hybrid ANN/HMMs correspond\u0002ing to the differences in alignments defined in each of these\nmodels. In comparison, AED models are based on implicit\nalignment modeling approaches, and the training criterion does\nnot have a latent variable A for explicit alignment as:\nLim = \u2212\nX\nN\nn=1\nlog P(Cn|Xn)\nWe refer the interested reader to the individual papers for\nfurther details on the training algorithms [13], [14], [15], [16],\n[46], [48], [53], [71], [120]. As shown in Section III-A, in both\nexplicit and implicit alignment cases, P(C|X) is factorized\nwith respect to input time t and output position i, respectively,\nand the factorized distribution is conditioned on the label\ncontext c\ni\u22121\n1\n, except for CTC. For example, in the AED case:\nlog P(C|X) = PL\ni=1 log P(ci\n|X, ci\u22121\n1\n). During training, we\nuse a teacher-forcing technique where the ground truth history\nis used as a label context.\nAs part of the training procedure, all E2E as well as classical\nhidden Markov models for ASR provide mechanisms to solve\nthe underlying sequence alignment problem - either explicitly\nvia corresponding latent variables, as in CTC, RNN-T or RNA,\nand also hybrid ANN/HMM, or implicitly, as in AED models.\nAlso, the distinction between speech and silence needs to be\nconsidered, which may be handled explicitly by introducing\nsilence as a latent label (hybrid ANN/HMM), or implicitly\nby not labeling silence at all, as currently is the standard in\nvirtually all E2E models.\nE2E models also may take advantage of hierarchical training\nschedules. These schedules may comprise several separate\ntraining passes and explicit, initially generated alignments that\nare kept fixed for some Viterbi-style [121], [122], [123] train\u0002ing epochs before re-enabling E2E-style full-sum training that\nmarginalizes over all possible alignments. Such an alternative\napproach is employed by Zeyer et al. [52], where an initial\nfull-sum RNN-T model is used to generate an alignment and\ncontinue with framewise cross-entropy training. This greatly\nsimplifies the training process by replacing the summation\nover all possible alignments in Eq. (4) by a single term cor\u0002responding to the alignment sequence generated. Recently, a\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nsimilar procedure has been introduced in [124] also employing\nE2E models, only. In this work, CTC is used to initialize\nthe training and to generate an initial alignment, followed by\nintermediate Viterbi-style RNN-T training and final full-sum\nfine tuning, which improved convergence compared to full\u0002sum-only training approaches.\nIt is interesting to note that in contrast to the RNN-T and\nRNA label-topologies, CTC does not require alignments with\nsingle label emissions per label position. However, training\nCTC models eventually does lead to single label emissions\nper hypothesized label. An analysis of this property of CTC\ntraining which is usually called peaky behavior can be found\nin [125] and references therein. Laptev et al. [126] even\nintroduces a CTC variant without non-blank loop transitions.\nB. Training with External Language Models\nE2E ASR models generally are normalized on sequence\nlevel. Therefore, sequence training with the maximum mutual\ninformation criterion [127] is the same as standard cross en\u0002tropy/conditional likelihood training. However, once external\nlanguage models are included in the training phase, sequence\nnormalization needs to be included explicitly, leading to MMI\nsequence discriminative training. This has been exploited as\na further approach to combine E2E models with external\nlanguage models trained on text-only data during the training\nphase itself [128], [129], [130].\nC. Minimum Word Error Rate Training\nSince the objective of speech recognition is to minimize\nword error rate (WER), there has been a growing number of\nresearch studies that incorporate this into the objective function\nby minimizing the model-based expectation of the number of\nword errors, as follows:\nLmwer =\nX\nN\nn=1\nX\nC\u2032\nn\nW(Cn, C\u2032\nn\n)P(C\n\u2032\nn\n|Xn)\nwhere W(Cn, C\u2032\nn\n) is the word error count in a hypothesis\nC\n\u2032\nn given a reference Cn, and n is an index which iterates over\nthe entire training set. These methods, known as sequence\nor discriminative training, have shown great improvements\nfor classical ASR [131], [132], [133], [134], [135], and have\nsince been explored in E2E models. Typically these losses\nare constructed by running in \u2018beam-search\u2019 mode rather than\nteacher-forcing mode, and construct a loss from the errors\nmade from the candidate hypotheses in the beam. Thus, this\ntype of training first requires training the model to optimize\nP(C|X) in order to initialize the model with a good set\nof parameters to run a beam search. However, also direct\napproaches have been introduced that avoid this separation\nto train discriminatively from scratch [69], [136].\nPapers that explore penalizing word errors include, Mini\u0002mum Word Error Rate (MWER) training [137], where the loss\nfunction is constructed such that the expected number of word\nerrors are minimized. Further work includes MWER for RNN\u0002T and self-attention-T [138], as well as MWER using prefix\nsearch instead of n-best [139]. Also, there have been studies\nthat consider MWER in terms of reinforcement learning [140],\n[141]. Optimal Completion Distillation (OCD) [81] proposes\nto minimize the total edit distance using an efficient dynamic\nprogramming algorithm. Finally, another body of research with\nsequence training introduce a separate external language model\nat training time [142], which can also be done efficiently via\napproximate lattice recombination [129] and also lattice-free\napproaches [130].\nD. Pretraining\nAll E2E models as well as classical hidden Markov models\nfor ASR provide holistic models that in principle enable train\u0002ing from scratch. However, many strategies exist to initialize\nand guide the training process to reach optimal performance\nand/or to obtain efficient convergence by applying pretrain\u0002ing and model growing [143], [144]. Supervised layer-wise\npretraining has been successfully applied for classical [5],\n[145], as well as attention-based ASR models [146], which can\nbe combined with intermediate sub-sampling schemes [147],\nand model growing [148]. Pretraining approaches utilizing\nuntranscribed audio, large-scale semi-supervised data and/or\nmultilingual data [149], [150], [151], [152], [153], [154],\n[155], [156], [157], [158], [159], [160] would deserve a\nself-contained survey and they are applicable for hybrid\nDNN/HMM and E2E approaches likewise \u2013 they will not be\nfurther discussed here.\nE. Training Schedules and Curricula\nDedicated training schedules have been developed to guide\nthe optimization process and as part of that reach proper\nalignment behavior explicitly or implicitly [52], [124], [147].\nMany approaches exist for learning rate control [161], [162]:\nNewBob [163], [164] and enhancements [162]; global ver\u0002sus parameter-wise learning rate control (exponential decay,\npower decay, etc.) [165]; learning rate warm-up [44]; warm\nrestarts/cosine annealing [166]; weight decay versus gradually\ndecreasing batch size [167]; fine-tuning [168] or population\u0002based training [169]; etc. For a survey of meta learning\ncf. [170].\nSequence learning approaches also consider curriculum\nlearning [171], [172], e.g., by considering short sequences\nfirst [173], [174]; interim increase of sub-sampling [147]\ninitially more sub-sampling; or, for multi-speaker ASR training\nsort mixed speech by SNR and start with speakers of balanced\nenergy and mixed gender [175].\nF. Optimization and Regularization\nOptimization usually is based on stochastic gradient descent\n[176], with momentum [177], [178], and a number of corre\u0002sponding adaptive approaches, most prominently Adam [179]\nand variants thereof [145], [179], [180].\nInvesting more training epochs seems to provide improve\u0002ments [52, Table 8], and also averaging over epochs has been\nreported to help [102]. For a discussion of the double descent\neffect and its relation to the amount of training data, label\nnoise and early stopping cf. [181].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nRegularization strongly contributes to training performance:\ne.g., L2 and weight decay [182], [166]; weight noise [183];\nadaptive mean L1/L2 [184]; gradient noise [185]; dropout\n[186], [187], [188], layer dropout [189], [190], [191]; dropcon\u0002nect [192]; zoneout [193]; smoothing of attention scores [15];\nlabel smoothing [194]; scheduled sampling [195]; auxiliary\nloss [194], [196]; variable backpropagation through time [197],\n[198]; mixup [199]; increased frame rate [180]; or, batch\nnormalization [200].\nG. Data Augmentation\nTraining of E2E ASR models also benefit from data aug\u0002mentation methods, which might also be viewed as regu\u0002larization methods. However, their diversity and impact on\nperformance justifies a separate overview.\nMost data augmentation methods perform data perturbation\nby exploiting certain dimensions of speech signal variation:\nspeed perturbation [201], [202], vocal tract length perturbation\n[201], [203], frequency axis distortion [201], sequence noise\ninjection [204], SpecAugment [205], or semantic mask [206].\nAlso, text-only data may be used to generate data using text\u0002to-speech (TTS) on feature [207] or signal level [208]. In a\ncomparison of the effect of TTS-based data augmentation on\ndifferent E2E ASR architectures in [208], AED seemed to be\nthe only architecture that appeared to benefit significantly from\nthe TTS data.\nIn a recent study [174] and corresponding follow-up work\n[180], many of the regularization and data augmentation\nmethods listed here have been exploited jointly leading to\nstate-of-the-art performance on the Switchboard task for a\nsingle-headed AED model.\nRelationship to Classical ASR\nE2E systems attempt to define ASR models that integrate\nall knowledge sources into a single global joint model that\ndoes not utilize secondary knowledge sources and avoids the\nclassical separation into acoustic and language models. These\nglobal joint models are completely trained from scratch using\na single global training criterion based on a single kind of\n(transcribed) training data and thus require less ASR domain\u0002specific knowledge provided sufficient amounts of training\ndata are available.\nWhile standard hybrid ANN/HMM training for ASR using\nframe-wise cross entropy already is discriminative, it is not\nyet sequence discriminative, requires prior alignments and\nalso lacks consideration of an (external) language model\nduring training. However, these potential shortcomings may\nbe remedied by using sequence discriminative training criteria\n[127] and lattice-free training approaches [69].\nIn contrast to strict E2E systems, the classical ASR ar\u0002chitecture includes the use of secondary knowledge sources\nbeyond the primary training data, i.e. (transcribed) speech\naudio for acoustic model training, and textual data for language\nmodel training. Most prominently, this includes the use of a\npronunciation lexicon and the definition of a phoneme set.\nSecondary resources like pronunciation lexica may be helpful\nin low-resource scenarios. However, their generation often is\ncostly and may even introduce errors, like pronunciations from\na lexicon not reflecting the actual pronunciations observed.\nTherefore, for large enough training resources, secondary\nknowledge sources might become obsolete [209], or even\nharmful, in case of erroneous information introduced [210],\n[211].\nClassical ASR models usually are trained successively, with\nknowledge derived from models trained earlier injected into\nlater training stages, e.g. in the form of HMM state alignments.\nHowever, such approaches from classical ASR might also\nbe interpreted as specific training schedules. Initializing deep\nlearning models using HMM alignments obtained from acous\u0002tic models based on mixtures of Gaussians may be interpreted\nin this way, with the Gaussian mixtures serving as an initial\nshallow model. In classical ASR, also approaches training\ndeep neural networks from scratch while avoiding interme\u0002diate training of Gaussians has been proposed [212], [213],\n[214], also in combination with character-level modeling [83].\nAnother step towards more integrated training of classical\nsystems has been to apply discriminative training criteria\navoiding intermediate (usually lattice-based) representations of\ncompeting word sequences [215], [69], [216], [217], [136].\nThe training of classical ASR systems usually applies sec\u0002ondary objectives to solve subtasks like phonetic clustering.\nThe classification and regression trees (CART) approach is\nused to cluster triphone HMM states [27], [218]. More re\u0002cent approaches proposed clustering within a neural network\nmodeling framework, while still retaining secondary clustering\nobjectives [219], [213]. However, also in E2E approaches\nsecondary objectives are used, most prominently for subword\ngeneration, e.g. via byte-pair encoding [32]. Also, available\npronunciation lexica can be utilized indirectly for assisting\nsubword generation for E2E systems [35], [36], which are\nshown to outperform byte-pair encoding. Within classical ASR\nsystems, phonetic clustering also can be avoided completely\nby modeling phonemes in context directly [220].\nIt is interesting to observe that specifically attention-based\nencoder-decoder models require larger numbers of training\nepochs to reach high performance, e.g. for a comparison\nof systems trained on Switchboard 300h cf. Table 5 in\n[221]. Also, attention-based encoder-decoder models have\nbeen shown to suffer from low training resources [222], [223],\nwhich can be improved by a number of approaches, including\nregularization techniques [174] as well as data augmentation\nusing SpecAugment [224] and text-to-speech (TTS) [29].\nSpecAugment also is shown to improve classical hybrid HMM\nmodels [225]. TTS on the other hand considerably improved\nattention-based encoder-decoder models trained on limited\nresources, but did not reach the performance of other E2E\napproaches or hybrid HMM models, which in turn were not\nconsiderably improved by TTS [208]. Multilingual approaches\nalso help improve ASR development for low resource tasks,\nagain both for classical [226], as well as for E2E systems\n[227], [228].\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\nVI. DECODING E2E MODELS\nThis section describes several decoding algorithms for end\u0002to-end speech recognition. The basic decoding algorithm of\nend-to-end ASR tries to estimate the most likely sequence C\u02c6\namong all possible sequences, as follows:\nC\u02c6 = arg max\nC\u2208U\u2217\nP(C|X)\nThe following section describes how to obtain the recognition\nresult C\u02c6.\nA. Greedy Search\nThe Greedy search algorithm is mainly used in CTC, which\nignores the dependency of the output labels as follows:\nA\u02c6 =\nY\nT\nt=1\n\u0012\narg max\nat\nP(at|X)\n\u0013\nwhere at is an alignment token introduced in Section III-B1.\nThe original character sequence is obtained by converting\nalignment token sequence A\u02c6 to the corresponding token se\u0002quence C\u02c6. The argmax operation can be performed in parallel\nover input frame t, yielding fast decoding [13], [229], although\nthe lack of the output dependency causes relatively poor\nperformance than the attention and RNN-T based methods in\ngeneral.\nCTC\u2019s fast decoding is further boosted with transformer\n[44], [98], [102] and its variants [45], [103] since their entire\ncomputation across the frames is parallelized [190], [230].\nFor example, the non-autoregressive models, including Im\u0002puter [231], Mask-CTC [230], Insertion-based modeling [232],\nContinuous integrate-and-fire (CIF) [233] and other variants\n[234], [235] have been actively studied as an alternative non\u0002autoregressive model to CTC. [235] shows that CTC greedy\nsearch and its variants achieve 0.06 real-time factor (RTF)7\nby using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz. The\npaper also shows that the degradation of the non-autoregressive\nmodels from the attention/RNN-T methods with beam search\nis not extremely large (19.7% with self-conditioned CTC [234]\nversus 18.5 and 18.9% with AED and RNN-T, respectively).\nThe greedy search algorithm is also used as approximate\ndecoding for both implicit and explicit alignment modeling\napproaches, including AED, RNA, CTC, and RNN-T, as\nfollows:\nc\u02c6i = arg max\nci\nP(ci|C\u02c6\n1:i\u22121, X) for i = 1, . . . , N\na\u02c6t = arg max\nat\nP(at|A\u02c6\n1:t\u22121, X) for t = 1, . . . , T\nThe greedy search algorithm does not consider alternate\nhypotheses in a sequence compared with the beam search\nalgorithm described below. However, it is known that the\ndegradation of the greedy search algorithm is not very large\n[16], [46], especially when the model is well trained in\nmatched conditions8.\n7 The ratio of the actual decoding time to the duration of the input speech.\n8 On the other hand, in the AED models, increasing the search space does\nnot consistently improve the speech recognition performance [77], [236] \u2013 a\nfact also observed in neural machine translation [237].\nB. Beam Search\nThe beam search algorithm is introduced to approximately\nconsider a subset of possible hypotheses C\u02dc among all possible\nhypotheses U\n\u2217 during decoding, i.e., C \u2282 U \u02dc \u2217\n. A predicted\noutput sequence C\u02c6 is selected among a hypothesis subset C\u02dc\ninstead of all possible hypotheses U\n\u2217\n, i.e.,\nC\u02c6 = arg max\nC\u2208C\u02dc\nP(C|X) (6)\nThe beam search algorithm is to find a set of possible hy\u0002potheses C\u02dc, which can include promising hypotheses efficiently\nby avoiding the combinatorial explosion encountered with all\npossible hypotheses U\n\u2217\n.\nThere are two major beam search categories: 1) frame\nsynchronous beam search and 2) label synchronous beam\nsearch. The major difference between them is whether it\nperforms hypothesis pruning for every input frame t or every\noutput token i. The following sections describe these two\nalgorithms in more detail.\nC. Label Synchronous Beam Search\nSuppose we have a set of partial hypotheses up to (i \u2212 1)th\ntoken C\u02dc\n1:i\u22121. A set of all possible partial hypotheses up to ith\ntoken C1:iis expanded from C\u02dc\n1:i\u22121 as follows:\nC1:i = {(C\u02dc\n1:i\u22121, ci = c)}c\u2208U (7)\nThe number of hypotheses |C1:i| would be |C\u02dc\n1:i\u22121| \u00d7 |U|, at\nmost. The beam search algorithm prunes the low probability\nscore hypotheses from C1:i and only keeps a certain number\n(beam size \u2206) of hypotheses at i among C1:i. This pruning\nstep is represented as follows:\nC\u02dc\n1:i = NBESTC1:i\u2208C1:i P(C1:i\n|X), where |C\u02dc\n1:i\n| = \u2206 (8)\nNote that NBEST(\u00b7) is an operation to extract top \u2206 hypothe\u0002ses in terms of the probability score P(C1:i\n|X) computed from\nan end-to-end neural network, or a fusion of multiple scores\ndescribed in Section VII-B.\nIn the label synchronous beam search, the length of the out\u0002put sequence (N) is unknown. Therefore, during this pruning\nprocess, we also add the hypothesis that reaches the end of an\nutterance (i.e., predict the end of sentence symbol \u27e8eos\u27e9) to a\nset of hypotheses C\u02dc in Eq. (6) as a promising hypothesis.\nThe label synchronous beam search does not explicitly\ndepend on the alignment information; thus, it is often used\nin implicit alignment modeling approaches, including AED.\nDue to this nature, sequence hypotheses of the same length\nmight cover a completely different number of encoder frames,\nunlike the frame synchronous beam search, as pointed out by\n[40]. As a result, we observe that the scores of very short and\nlong segment hypotheses often become the same range, and\nthe beam search wrongly selects such hypotheses. [86] shows\nan example of such extreme cases, resulting in large deletion\nand insertion errors for short and long-segment hypotheses,\nrespectively. Thus, the label synchronous beam search requires\nheuristics to limit the output sequence length to avoid ex\u0002tremely long/short output sequences. Usually, the minimum\nand maximum length thresholds are determined proportionally\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nto the input frame length |X| with tunable parameters \u03c1min and\n\u03c1max as Lmin = \u230a\u03c1min|X|\u230b, Lmax = \u230a\u03c1max|X|\u230b. Although these\nare quite intuitive ways to control the length of a hypothesis,\nthe minimum and maximum output lengths depend on the\ntoken unit or type of script in each language. Another heuristic\nis to provide an additional score related to the output length\nor attention weights \u2013 e.g., a length penalty, and a coverage\nterm [77], [238]. The end-point detection [239] is also used to\nestimate the hypothesis length automatically. [236] redefines\nthe implicit length model of the attention decoder to take into\naccount beam search, resulting in consistent behavior without\ndegradation for increasing beam sizes.\nNote that there are several studies on applying label syn\u0002chronous beam search to explicit alignment modeling ap\u0002proaches. For example, label synchronous beam search al\u0002gorithms for CTC are realized by marginalizing all possible\nalignments for each label hypothesis [13]. [240] extends\nCIF [233] to produce label-level encoder representation and\nrealizes label synchronous beam search in RNN-T.\nD. Frame Synchronous Beam Search\nIn contrast to the label synchronous case in Eq. (8), the\nframe synchronous beam search algorithm performs pruning\nat every input frame t, as follows:\nC\u02dc\n1:i(t) = NBESTC1;i(t) P(C1;i(t)\n|X), where |C\u02dc\n1:i(t)\n| = \u2206\nwhere C1;i(t)is an i(t)-length label sequence obtained from\nthe alignment A1:t, which is introduced in Sec. III-B.\nP(C1;i(t)|X) is obtained by summing up all possible align\u0002ments A1:t \u2208 A(X,C1;i(t))\n. Unlike the label synchronous beam\nsearch, frame synchronous beam search depends on explicit\nalignment A; thus, it is often used for explicit alignment\nmodeling approaches, including CTC, RNN-T, and RNA.\nC1:i(t)is an expanded partial hypotheses up to input frame\nt, similar to Eq. (7).\nCompared with the label synchronous algorithm, the frame\nsynchronous algorithm needs to handle additional output to\u0002ken transitions inside the beam search algorithm. The frame\nsynchronous algorithm can be easily extended in online and/or\nstreaming decoding, thanks to the explicit alignment informa\u0002tion with input frame and output token.\nClassical approaches to beam search for HMM, but also\nCTC and RNN-T variants, are based on weighted finite state\ntransducers (WFST) [38], [74], [241] or lexical prefix trees\n[106], [242], [243]. They are categorized as frame synchronous\nbeam search. These methods are often combined with an N\u0002gram language model or a full-context neural language model\n[244], [245]. RNN-T [14], [246] and CTC prefix search [247]\ncan deal with a neural language model by incorporating the\nlanguage model score in the label transition state. Interestingly,\ntriggered attention approaches [248], [249] allow us to use\nimplicit alignment modeling approaches, including AED, in\nframe-synchronous beam search together with CTC and neural\nLM, which applies on-the-fly rescoring to the hypotheses given\nby CTC prefix search using the AED and LM scores.\nE. Block-wise Decoding\nAnother beam search implementation uses a fixed-length\nblock unit for the input feature. In this block processing, we\ncan use the future context inside the block by using the non\u0002causal encoder network based on the BLSTM, output-delayed\nunidirectional LSTM, or transformer (and its variants). This\nfuture context information avoids the degradation of the fully\ncausal network. In this setup, the chunk size becomes the\ntrade-off of controlling latency and accuracy. This technique is\nused in both RNN-T [100], [250], [251] and AED [61], [252],\n[253], [254]. Block-wise processing is especially important for\nimplicit alignment modeling approaches, including AED, since\nit can provide block-wise monotonic alignment constraint\nbetween the input feature and output label, and realize block\u0002wise streaming decoding.\nF. Model Fusion during Decoding\nSimilar to the classical HMM-based beam search, we com\u0002bine various scores obtained from different modules, including\nthe main end-to-end ASR and LM scores.\n1) Synchronous Score Fusion: The most simple score fu\u0002sion is performed when the scores of multiple modules are\nsynchronized. In this case, we can simply add the multiple\nscores at each frame t or label i. The most well-known score\ncombination is LM shallow fusion.\nLM shallow fusion: As discussed in Sec. VII, various neural\nLMs can be integrated with end-to-end ASR. The most simple\nintegration is based on LM shallow fusion [255][256][257], as\ndiscussed in Sec. VII-B1, which (log-) linearly adds the LM\nscore Plm(C1:i) to E2E ASR scores P(C1:i|X) during beam\nsearch in Eq. (8) as follows:\nlog P(C1:i|X) \u2192 log P(C1:i|X) + \u03b3 log Plm(C1:i)\nwhere \u03b3 is a language model weight. Of course, we can\ncombine other scores, such as the length penalty and coverage\nterms, as discussed in Sec. VI-C.u\n2) Asynchronous Score Fusion: If we combine the frame\u0002dependent score functions, P(at|\u00b7), used in explicit alignment\nmodeling approaches, e.g., CTC, RNN-T, and label-dependent\nscore functions, P(ci|\u00b7), used implicit alignment modeling\napproaches, e.g., AED, language model, we have to deal with\nthe mismatch between the frame and label time indices t and\ni, respectively.\nIn the time-synchronous beam search, this fusion is per\u0002formed by incorporating the language model score in the\nlabel transition state [70], [22], [258]. [247] also combines\na word-based language model and token-based CTC model\nby incorporating the language model score triggered by the\nword delimiter (space) symbol.\nIn the label-synchronous beam search, we first compute\nthe label-dependent scores from the frame-dependent score\nfunction by marginalizing all possible alignments given a\nhypothesis label sequence. CTC/attention joint decoding [86]\nis a typical example, where the CTC score is computed\nby marginalizing all possible alignments based on the CTC\nforward algorithm [229]. This approach eliminates the wrong\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\nalignment issues and difficulties of finding the correct end of\nsentences in the label-synchronous beam search [86].\nNote that the model fusion method during beam search can\nrealize simple one-pass decoding, while it limits the time unit\nof the models to be the same or it requires additional dynamic\nprogramming to adjust the different time units, especially for\nthe label-synchronous beam search. This dynamic program\u0002ming computation becomes significantly large when the length\nof the utterance becomes larger and requires some heuristics\nto reduce the computational cost [259].\nG. Lexical Constraint during Score Fusion\nClassically, we use a word-based language model to cap\u0002ture the contextual information with the word unit, and also\nconsider the word-based lexical constraint for ASR. However,\nend-to-end ASR often uses a letter or token unit and it causes\nfurther unit mismatch during beam search. As described in\nprevious sections, the classical approach of incorporating the\nlexical constraint from the token unit to the word unit is\nbased on a WFST. This method first makes a TLG transducer\ncomposed of the token (T), word lexicon (L), and word-based\nlanguage transducers (G) [74]. This TLG transducer has been\nused for both CTC [74] and attention-based [53] models.\nAnother approach used in the time synchronous beam search\nis to insert the word-based language model score triggered by\nthe word delimiter (space) symbol [75]. To synchronize the\nword-based language model with a character-based end-to-end\nASR, [260] combines the word and character-based LMs with\nthe prefix tree representation, while [239], [261] uses look\u0002ahead word probabilities to predict next characters instead of\nusing the character-based LM. The prefix tree representation\nis also used for the sub-word token unit case [262], [263].\nH. Multi-pass Fusion\nThe previous fusion methods are performed during the\nbeam search, which enables a one-pass algorithm. The popular\nalternative methods are based on multi-pass algorithms where\nwe do not care about the synchronization and perform n-best or\nlattice scoring by considering the entire context within an ut\u0002terance. [16] uses the N-best rescoring techniques to integrate\na word-based language model. [55] combines forward and\nbackward searches within a multi-pass decoding framework to\ncombine bidirectional LSTM decoder networks. Recently two\u0002pass algorithms of switching different end-to-end ASR systems\nhave been investigated, including RNN-T \u2192 AED [264]; CTC\n\u2192 AED [265], [266]. This aims to provide streamed output in\nthe first pass and re-scoring with AED in the second pass to\nrefine the previous output, thus satisfying a real-time interface\nrequirement while providing high recognition performance.\nIn addition to the N-best output in the above discussion,\nthere is a strong demand for generating a lattice output\nfor better multi-pass decoding thanks to richer hypothesis\ninformation in a lattice. The lattice output can also be used\nfor spoken term detection, spoken language understanding,\nand word posteriors. However, due to the lack of Markov\nassumptions, RNN-T and AED cannot merge the hypothesis\nand cannot generate a lattice straightforwardly, unlike the\nHMM-based or CTC systems. To tackle this issue, there are\nseveral studies of modifying these models by limiting the\noutput dependencies in the fixed length (i.e., finite-history)\n[47], [267], or keeping the original RNN-T structure but\nmerging the similar hypotheses during beam search [107].\nI. Vectorization across both Hypotheses and Utterances\nWe can accelerate the decoding process by vectorizing\nmultiple hypotheses during the beam search, where we replace\nthe score accumulation steps for each hypothesis with vector\u0002matrix operations for the vectorized hypotheses. This has been\nstudied in RNN-T [22], [258], [268] and attention-based [259]\nmodels. This modification leverages the parallel computing\ncapabilities of multi-core CPUs, GPUs and TPUs, resulting in\nsignificant speedups, while enabling multiple utterances to be\nprocessed simultaneously in a batch. Major deep neural net\u0002work and end-to-end ASR toolkits support this vectorization.\nFor example, Tensorflow9[269], and FAIRESEQ10 [270] pro\u0002vide a vectorized beam search interface for a generic sequence\nto sequence task, and it can be used for attention-based end-to\u0002end ASR. End-to-end ASR toolkits including ESPnet11 [259],\nESPRESSO12[261], LINGVO [271], and, RETURNN13 [272]\nalso support the vectorized beam search algorithm.\nRelationship to Classical ASR\nOne of the most prominent properties shared between E2E\nand classical statistical ASR systems is the use of a single\u0002pass decoding strategy, which integrates all knowledge sources\ninvolved (models, components), before coming to a final\ndecision [123]. This includes the use of full label context\ndependency both for E2E systems [229], [51], [77], [273],\n[174], [262], [274], [275], as well as classical systems via full\u0002context language models [276], [244], [245], [277]. In classical\nASR systems, even HMM alignment path summation may be\nretained in search [278]. Both E2E as well as classical ASR\nsystems employ beam search in decoding. However, compared\nto classical search approaches, E2E beam search usually is\nhighly simplified with very small beam sizes around 1 to\n100 [15], [16], [77], [147]. Very small beam sizes also partly\nmask a length bias exhibited by E2E attention-based encoder\u0002decoder models [279], [280], thus trading model errors against\nsearch errors [281]. An overview of approaches to handle the\nlength bias beyond using small beam sizes in ASR is presented\nin [236].\nMany classical ASR search paradigms are based on mul\u0002tipass approaches that successively generate search space\nrepresentations applying increasingly complex acoustic and/or\nlanguage models [282], [283], [243]. However, multipass\nstrategies also are employed using E2E models, which how\u0002ever softens the E2E concept. Decoder model combination is\npursued in a two-pass approach, while even retaining latency\nconstraints as in [87]. Further multipass approaches include\nE2E adaptation approaches [284], [285], [286], [287].\n9 https://www.tensorflow.org/api docs/python/tf/contrib/seq2seq/BeamSearchDecoder\n10 https://github.com/pytorch/fairseq/blob/master/fairseq/sequence\ngenerator.py 11 https://github.com/espnet/espnet\n12 https://github.com/freewym/espresso\n13 https://github.com/rwth-i6/returnn\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\nVII. LM INTEGRATION\nThis section discusses language models (LMs) used for E2E\nASR. Hybrid ASR systems have long been using a pretrained\nLM [2], whereas most end-to-end (E2E) ASR systems employ\na single E2E model that includes a network component acting\nas an LM.14 For example, the prediction network of RNN\u0002T and the decoder network of AED models take on the role\nof a LM covering label back-histories. Therefore, E2E ASR\ndoes not seem to require external LMs. Nevertheless, many\nstudies have demonstrated that external LMs help improve the\nrecognition accuracy in E2E ASR.\nThere are presumably three reasons that E2E ASR still\nrequires an external LM:\na) Compensation for poor generalization: E2E models\nneed to learn a more complicated mapping function than\nclassical modular-based models such as acoustic models. Con\u0002sequently, E2E models tend to face overfitting problems if\nthe amount of training data is not sufficient. Pretrained LMs\npotentially compensate for the less generalized predictions\nmade by E2E models.\nb) Use of external text data: E2E models need to be\ntrained using paired speech and text data, while LMs can\nbe trained with only text data. Generally, text data can be\ncollected more easily than the paired data. The training speed\nof an LM is also faster than that of E2E models for the same\nnumber of sentences. Accordingly, the LM can be improved\nmore effectively with external text data, providing additional\nperformance gain to the ASR system.\nc) Domain adaptation: Domain adaptation helps im\u0002prove recognition accuracy when the E2E model is applied\nto a specific domain. However, domain adaptation of the E2E\nmodel requires a certain amount of paired data in the target\ndomain. Also, when multiple domains are assumed, it may be\ncostly to maintain multiple E2E models for the domains the\nsystem supports. If a pretrained LM for the target domain is\navailable, it may more easily improve recognition accuracy for\ndomain-specific words and speaking styles without updating\nthe E2E model.\nThis section reviews various types of LMs used for E2E\nASR and fusion techniques to integrate LMs into E2E models.\nA. Language Models\nThe LMs provide a prior probability distribution, P(C). If\nthe sentence, C, can be decomposed into a sequence of tokens\nsuch as characters, subwords, and single words, the probability\ndistribution can be computed based on the chain rule as:\nP(C) =\nL\nY\n+1\ni=1\nP(ci|c0:i\u22121)\nwhere ci denotes the i-th token of C, and c0:i\u22121 represents\ntoken sequence c0, c1, . . . , ci\u22121, assuming c0 = \u27e8sos\u27e9 and\ncL+1 = \u27e8eos\u27e9.\nMost LMs are designed to provide the conditional probabil\u0002ity P(ci\n|c0:i\u22121), i.e., they are modeled to predict the next token\n14 In the simplest case of a CTC model as in Fig. 2, the included LM\ncomponent however is limited to a label prior without label context.\ngiven a sequence of the preceding tokens. We briefly review\nsuch LMs focusing on the different techniques to represent\neach token, ci, and back-history, c0:i\u22121.\n1) N-gram LM: N-gram LMs have long been used for\nASR [2]. Early E2E systems in [53], [74], [77] also employed\nan N-gram LM. The N-gram models rely on the Markov\nassumption that the probability distribution of the next token\ndepends only on the previous N\u22121 tokens, i.e., P(ci|c0:i\u22121) \u2248\nP(ci|ci\u2212N+1:i\u22121), where N is typically 3 to 5 for word-based\nmodels and higher for sub-word and character-based models.\nThe maximum likelihood estimates of N-gram probabilities\nare determined based on the counts of N sequential tokens in\nthe training data set as:\nP(ci|ci\u2212N+1:i\u22121) = K(ci\u2212N+1, . . . , ci)\nP\nci\nK(ci\u2212N+1, . . . , ci)\nwhere, K(\u00b7) denotes the count of each token sequence. Since\nthe data size is finite, it is important to apply a smoothing\ntechnique to avoid estimating the probabilities based on zero or\nvery small counts for rare token sequences. Those techniques\ncompensate the N-gram probabilities with lower order models,\ne.g., (N \u2212 1)-gram models, according to the magnitude of\nthe count [288]. However, since the N-gram probabilities\nstill rely on the discrete representation of each token and the\nhistory, they suffer from data sparsity problems, leading to\npoor generalization.\nThe advantage of the N-gram models is their simplicity,\nalthough they underperform state-of-the-art neural LMs. In the\ntraining, the main step is to just count the N tuples in the data\nset, which is required only once. During decoding, the LM\nprobabilities can be obtained very quickly by table lookup or\ncan be attached to a decoding graph, e.g., WFST, in advance.\n2) FNN-LM: The feed-forward neural network (FNN) LM\nwas proposed in [9], which estimates N-gram probabilities\nusing a neural network. The network accepts N \u2212 1 tokens,\nand predicts the next token as:\nP(ci|ci\u2212N+1:i\u22121) = softmax(Wohi + bo)\nhi = tanh(Whei + bh)\nei = concat(E(ci\u2212N+1), . . . , E(ci\u22121))\nwhere Wo and Wh are weight matrices, and bo and bh are\nbias vectors. E(y) provides an embedding vector of c, and\nconcat(\u00b7) operation concatenates given vectors 15. This model\nfirst maps each input token to an embedding space, and then\nobtains hidden vector, hi, as a context vector representing the\nprevious N \u22121 tokens. Finally, it outputs the probability distri\u0002bution of the next token through the softmax layer. Although\nthis LM still relies on the Markov assumption, it outperforms\nclassical N-gram LMs described in the previous section.\nThe superior performance of FNN-LM is primarily due to\nthe distributed representation of each token and the history.\nThe LM learns to represent token/context vectors such that\nsemantically similar tokens/histories are placed close to each\nother in the embedding space. Since this representation has a\nbetter smoothing effect than the count-based one used for N\u0002gram LMs, FNN-LM can provide a better generalization than\n15 We omit the optional direct connection from the embedding layer to the\nsoftmax layer in [9] for simplicity.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\nN-gram LMs for predicting the next token. Neural network\u0002based LMs basically utilize this type of representation.\n3) RNN-LM: A recurrent neural network (RNN) LM was\nintroduced to exploit longer contextual information over N \u2212\n1 previous tokens using recurrent connections [289]. Unlike\nFNN-LM, the hidden vector is computed as:\nhi = recurrence(ei, hi\u22121)\nei = E(ci\u22121)\nwhere, recurrence(ei, hi\u22121) represents a recursive function,\nwhich accepts previous hidden vector hi\u22121 with input ei, and\noutputs next hidden vector hi. In the case of simple (Elman\u0002type) RNN, the function can be computed as\nrecurrence(e, h) = tanh(Whe + Wrh + bh)\nwhere, Wr is a weight matrix for the recurrent connection,\nwhich is applied to the previous hidden vector h. This recurrent\nloop makes it possible to hold the history information in the\nhidden vector without limiting the history to N \u2212 1 tokens.\nHowever, the history information decays exponentially as\ntokens are processed with this recursion. Therefore, currently\nstacked LSTM layers are more widely used for the recurrent\nnetwork, which have separate internal memory cells and gating\nmechanisms to keep long-range history information [290].\nWith this mechanism, RNN-LMs outperform other N-gram\u0002based models in many tasks.\n4) ConvLM: Convolutional neural networks (ConvLM)\nhave also been applied to LMs [291], [292], [293]. ConvLM\n[292] replace the recurrent connections used in RNN-LMs\nwith gated temporal convolutions. The hidden vector is com\u0002puted as\nhi =h\n\u2032\ni \u2297 \u03c3(gi)\nh\n\u2032\ni =ei\u2212k+1:i \u2217 W + b\ngi =ei\u2212k+1:i \u2217 V + c\nwhere \u2297 is element-wise multiplication, \u2217 is a temporal\nconvolution operation, and k is the patch size. \u03c3(gi) represents\na gating function of convoluted activation h\n\u2032\ni\n, and is modeled\nas a sigmoid function. W and V are matrices for convolution\nand b and c are bias vectors. The convolution and gating blocks\nare typically stacked multiple times with residual connections.\nIn [293], a ConvLM with 14 blocks has been applied for E2E\nASR. Similar to FNN-LM, ConvLM allow us to use only a\nfixed history size, but they are more parameter efficient and\neasier to utilize longer histories than the FNN-LM by stacking\nthe layers. Thus, they achieve competitive performance to that\nof RNN-LMs [292], even with the finite history consisting\nof short tokens such as characters [294]. Moreover, they are\nhighly parallelizable and thus suitable for training the model\nwith a large training data set.\n5) Transformer LM: Transformer architecture [44] has been\napplied to LMs [295] and used for ASR [102], [296], where\nthe LMs are designed as a Transformer decoder without any\ninputs from other modules such as encoders. The hidden vector\nis computed as:\nhi = FFN(h\n\u2032\ni\n) + h\n\u2032\ni\nh\n\u2032\ni = MHA(ei\n, e1:i, e1:i) + ei\nwhere FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network\nand a multi-head attention module, respectively. The multi\u0002head attention and feed-forward blocks are typically stacked\nmultiple times, e.g., 6 times [102], to obtain the final hidden\nvector. The advantage of Transformer LMs is that they can\ntake all tokens in the history into account through the self\u0002attention mechanism without summarizing them into a fixed\u0002size memory like RNN-LMs. Thus, the long history can be\nfully considered with attention to predict the next token,\nachieving better performance than RNN-LMs. However, the\ncomputational complexity increases quadratically as the length\nof the sequence. Therefore, the history length is typically\nlimited to a fixed size or within every single sentence. To\novercome this limitation, Transformer-XL [297] reuses already\ncomputed activations, which includes information on farther\nprevious tokens, and the model is trained with a truncated\nback-propagation through time (BPTT) algorithm [298]. Com\u0002pressive Transformer [299] extends this approach to utilize\neven longer contextual information by incorporating a com\u0002pression step to keep older, but important, information in a\nfixed-size memory network.\nB. Fusion Approaches\nThere are several ways to incorporate an external LM into\nE2E ASR, called LM fusion. Their purpose is to improve the\nrecognition accuracy of E2E ASR by leveraging the benefits\nof the external LM described in the first part of this section.\nHowever, there can be a mismatch in the prediction between\nthe E2E model and the LM when trained on different data\nsets, and therefore the LM may not collaborate well with\nthe E2E model. Researchers have investigated various LM\nfusion approaches to reduce the mismatch between models\nin different situations.\n1) Shallow Fusion: Shallow fusion is the most popular\napproach to combine the pretrained E2E model and LM in\nthe inference time. As we described in Sec. VI-F, shallow\nfusion simply combines the E2E and LM scores by a log\u0002linear combination as\nScore(C|X) = log P(C|X) + \u03b3 log P(C) (9)\nwhere \u03b3 is a scaling factor for the LM [255][256][257]. The\nadvantage of this approach is that it is easy and effective when\nthere are no major mismatches between the source and target\ndomains.\n2) Deep Fusion: Deep fusion [300] is an approach to\ncombine an LM with an E2E model using a joint network.\nGiven a pretrained E2E model and an LM, all the network\nparameters are fine-tuned jointly so that the models collaborate\nbetter to improve the recognition accuracy, where the joint\nnetwork is used to combine the E2E and LM states through\na gating mechanism that controls the contribution of the LM\naccording to the current state.\n3) Cold fusion: Cold fusion [301] is another approach to\ncombine a pretrained LM like deep fusion, but the E2E model\nis learned while freezing the LM parameters. Since the E2E\nmodel is aware of the LM throughout training, it learns to use\nthe LM to reduce language specific information and capture\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\nonly the relevant information to map the source to the target\nsequence. This mechanism reduces the role of LM in the E2E\nmodel and alleviates the language bias of the training data.\nAccordingly, the E2E model becomes more robust to domain\nmismatches between the training data and the target domain.\nUnlike deep fusion, cold fusion makes it possible to combine\nthe E2E model with a pretrained LM for the target domain,\nimproving the recognition accuracy. Component fusion [302]\nextends cold fusion to use a pretrained LM with transcriptions\nof the training data for the E2E model, more focusing on\nreducing the bias of the training data.\n4) Internal LM Estimation: There is another approach to\nreduce language bias in training data through shallow fusion.\nThe language bias is a problem when a big domain mismatch\nexists between the source domain (training data) and the target\ndomain (test data) because the E2E model scores are strongly\ndependent on the language priors in the source domain. To\nremove such a bias from the score, we can explicitly estimate\nthe LM that represents the language priors, called Internal LM,\nand subtract the LM score from the ASR score of Eq. (9):\nScore(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C)\nwhere subscripts \u03c6 and \u03c4 indicate the source and target\ndomains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub\u0002tracting the internal LM score corresponds to approximating\nacoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d\nP\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR\nscore can be seen as a classical hybrid ASR system. Ac\u0002cordingly, the subtracted E2E model score plays a role of\nacoustic model and makes it more domain independent in\nterms of language, achieving a higher recognition accuracy\nin combination with the external LM P\u03c4 (C).\nThe density ratio method [303] trains an internal LM using\nthe transcript of the training data. Hybrid autoregressive trans\u0002ducer (HAT) [47] extends RNN-T so that the model becomes\nthe internal LM when the encoder output is eliminated, i.e., set\nto zero. This approach simplifies the framework by utilizing\nthe prediction network as the internal LM, which avoids\ntraining an additional LM and using it in the inference time.\nIn the work of [304], an approach similar to HAT has been\nproposed where the internal LM is formulated on top of\nstandard RNN-T and attention-based encoder-decoder models,\nrespectively. In [128], several techniques to estimate internal\nLMs have been proposed for AED models, where an estimated\nbias vector is fed to the LM instead of a zero vector. The bias\nvector can be estimated by averaging encoder states or context\nvectors, or by a small LSTM predicting the context vector\nbased on the decoder label context, only. These techniques to\nestimate the internal LM were also evaluated for RNN-T in\n[305].\nC. Use of Large-scale Pretrained LMs\nIn recent years, LMs trained with large-scale text data are\navailable for different NLP tasks. BERT [306] and GPT-2\n[307] are representative models based on Transformer LMs.\nSuch LMs have also been applied to E2E ASR systems in\ndifferent ways, e.g., N-best rescoring [308] and dialog context\nembedding [309].\nRelationship to Classical ASR\nThe architecture of classical ASR systems provides a sepa\u0002ration between the acoustic model and the language model.\nIn contrast to this, E2E models avoid this separation and\ndefine a joint model. While this allows for training with a\nsingle objective, it limits training of the (implicit) prior to\nthe transcriptions of the audio training data. To exploit further\ntext-only training data, usually a separate LM is combined with\nE2E models, nonetheless. However, due to the implicit prior\nof E2E models, i.e. the internal language model, combination\nwith separate language models is not straightforward and\nrequires corresponding internal language model estimation\nand compensation approaches, e.g. [303], [47], [304], [128],\n[310]. At least from the recognition accuracy perspective, it\nremains unclear, if the clear separation of acoustic modeling\nand language modeling in the classical ASR architecture is a\ndisadvantage because of separate training objectives, or rather\nan advantage, since text-only training data may be used easily.\nAlso, the language model training objective, i.e. language\nmodel perplexity, is observed to correlate well with word error\nrate [311], [312], [313], [314]. Furthermore, discriminative\napproaches to language modeling [315] may be viewed as a\nstep towards joint modeling.\nVIII. OVERALL PERFORMANCE TRENDS OF E2E\nAPPROACHES IN COMMON BENCHMARKS\nThis section summarizes various techniques with the com\u0002mon ASR benchmarks based on switchboard (SWBD) [316]\nin Figure 9 and Librispeech [317] in Figure 10 to see the\ntrajectory of the techniques developed in end-to-end ASR. We\nchoose these two databases because they are widely used in\nspeech and machine learning communities and cover sponta\u0002neous (SWBD) and read speech (Librispeech) speaking styles.\nFigures 9 and 10 show that the performance improvement\nrelative to the initial works [147], [79] based on the E2E\nmodels is significant, and the error rates of all tasks become\nless than half of the original error rates!16\nAlthough the overall trends show that the ASR performance\nhas steadily improved over time, there are several remarkable\ngains. One significant gain observed in both benchmarks in the\nmiddle of 2019 comes from the data augmentation method\nrepresented by SpecAugment [205], [206], as discussed in\nSection V-G. The subsequent gains mostly come from the\nexploration of the new neural network architectures, including\ntransformer [102], [318], conformer [45], [103], and contextnet\n[97] on top of SpecAugment, as discussed in Section IV-C.\nSuch an exploration is also performed in language modeling\nto improve the ASR performance [296], [102]. The final gain\nobserved in the Librispeech benchmark in 2021 is based\non self-supervised learning [25], [319] and semi-supervised\nlearning [320], [321]. These techniques utilize a considerable\n16 For readers who want to know the latest update of these\nbenchmarks can also check https://github.com/syhw/wer are we and\nhttps://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\nAEDAEDAEDAED\nHMM\nAEDAEDAEDAED AEDAED\nWER (%)\n0\n10\n20\n30\n40\n50\n1/1/2017 1/1/2018 1/1/2019 1/1/2020 1/1/2021\nswb chm\nFig. 9. E2E ASR performance improvement in the switchboard task. AED AED\nAEDAEDAED\ntransducer ContextNet Transducer\nCTC\nHMM\nAED transducer\nWER (%)\ntransformer\n0\n5\n10\n15\n1/1/2019 7/1/2019 1/1/2020 7/1/2020 1/1/2021 7/1/2021\ntest_clean test_other\nFig. 10. E2E ASR performance improvement in the Librispeech task.\namount of unlabeled in-domain speech data (e.g., Libri-light\n60K hours [322]).\nRelationship to Classical ASR\nSpeech recognition research has always been pushed by\ninternational evaluation campaigns (e.g. as lead by NIST)\nand corresponding benchmark tasks. The competition between\nclassical and E2E approaches is nicely reflected in the widely\nused Librispeech [317] and Switchboard [316] tasks, showing\nthat E2E models gain momentum. As shown in Figure 10,\non Librispeech, the current best-published classical hybrid\nsystems range around 2.3% (test-clean) and 4.9% (test-other)\nword error rate [323], [222], while there already are a number\nof E2E systems providing similar performance [224], [205],\n[320], [206], with some E2E systems clearly outperforming\nformer state-of-the-art results with word error rates down to\n1.8% (test-clean) and 3.7% (test-other) [324] with similar\nresults reported in [45], [97]. Merging insights from classical\nHMM-based and monotonic RNN-T provided similarly well\nresults with a limited training budget [124]. Finally, when\ntrained on Switchboard 300h, the current best result, obtained\nwith an E2E system [180] is 5.4% compared to 6.6% word\nerror rate for the best hybrid system result [325] on the\nHUB5\u201900 Switchboard test set, in Figure 9\nIX. DEPLOYMENT OF E2E MODELS\nMany of the ideas discussed in this paper have been\nexplored by various industry research labs [326], [327], [328],\n[329], [330], [331], [265], inter alia. In this section, we\nreview the development of on-device production-level systems\nat Google as a typical case study for deployment.\nThe first streaming E2E model, deployed to production,\nwas launched in 2019 for the Pixel 4 smartphone [22], [332].\nThis model used a streaming RNN-T first-pass system, while\nre-scoring first-pass hypotheses with an AED system in the\nsecond pass. In addition, FST-based contextual biasing [92]\nwas employed in the model, which was critical to obtain\naccurate results for diverse queries. This model ran on CPU\nand was much faster than real time.\nIn 2020, for the Pixel 5 smartphone [333], the system was\nimproved further to reduce user-perceived latency (i.e., the\ntime between when the user speaks, and when words appear\non the device). This included advancements such as end-to-end\nendpointing [113] to encourage faster microphone closing; as\nwell as FastEmit [91] to encourage the model to emit tokens\nearlier.\nFinally, in 2021 the model was further improved for the\nPixel 6 smartphone [334], to take advantage of the tensor\nprocessing unit (TPU) [85] on the device. Improvements\ninclude the use of conformer layers for the encoder [45]; a\nsmall embedding prediction network for the decoder [104]; a\n2-pass cascaded encoder to run a 2nd-pass beam search [89];\nand, a neural LM re-scorer to help improve accuracy long-tail\nnamed entities. This model is the best ASR system that Google\nhas released to date, both in terms of quality and latency.\nX. AREAS FOR FUTURE WORK\nCurrently, E2E models dominate the academic debate on\nASR. However, at least partly, this is not (yet?) reflected\nin the corresponding commercial deployment of E2E ASR\narchitectures. E2E models are not yet the perfect match for\nall ASR conditions and further research is needed to take full\nadvantage of the benefits of E2E modeling.\nE2E models seem to perform really well when training data\nis abundant, while not scaling well to low-resource conditions.\nSimilarly, domain change requires a flexible exchange of lan\u0002guage models, which is natural for classical ASR models based\non a separation of acoustic and language models. Ongoing\nresearch on the use of external language models in E2E models\nand internal language model estimation already is promising,\nbut can be expected to see further improvements.\nTop E2E ASR systems usually require orders of magnitude\nmore training epochs than comparable classical ASR systems,\nand further research into efficient and robust optimization and\ntraining schedules is needed.\nThe high level of integration of E2E models also involves a\nloss in modularity, which might support the explainability and\nreusability of models. Also, more efficient training schedules\nmight take advantage of modularity. One assumed advantage\nof E2E models is that everything is trained from data and\nsecondary knowledge sources (e.g. pronunciation lexica and\nphoneme sets) are avoided. However, rare events, like rare\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\nwords in ASR still provide a challenge, which needs further\nresearch.\nWith the missing separation of acoustic and language mod\u0002els, the question arises of how to exploit text-only resources in\nE2E model training - do we foresee solutions beyond training\ndata generation using TTS? We note that a number of recent\nworks have explored approaches to combine speech and text\nmodalities by attempting to implicitly or explicitly map them\ninto a shared space [159], [335], [336], [337], [338], [339],\n[340], [341]. Furthermore, high-performance E2E solutions\nexist for both discriminative problems like ASR, as well as\ngenerative problems like TTS, how can both be exploited\njointly to support semi-supervised training based on text-only\nand/or audio-only data on top of transcribed speech audio [28],\n[342]?\nFor AED architectures, we observe a length bias, which\ncomplicates the decoding process. Although many heuristics\nare known to tackle length bias in AED, we are still missing\na well-founded explanation for it, as well as a corresponding\nremedy of the original model.\nOther open research problems include speaker adaptation\nand robustness to recording conditions, especially in mismatch\nsituations. The E2E principle also provides a promising candi\u0002date to solve multichannel ASR by providing an E2E solution\njointly tackling the source separation, speaker diarization and\nspeech recognition problem [343], [26].\nFinally, we need to investigate, if E2E is a suitable guiding\nprinciple, and how different E2E ASR models relate to each\nother as well as to classical ASR approaches. The most\nimportant guiding principle of ASR research and development\nhas been performance, and ASR has been boosted strongly\nby widely used benchmark tasks and international evaluation\ncampaigns. With the current diversity of classical and E2E\nmodels, we also need to resolve the question of what con\u0002stitutes state-of-the-art in ASR today, and can we expect a\ncommon state-of-the-art ASR architecture in the future?\nXI. CONCLUSIONS\nIn this work, we presented a detailed overview of end-to\u0002end approaches to ASR. Such models, which have grown in\npopularity over the last few years, propose to use highly inte\u0002grated neural network components which allow input speech\nto be converted directly into output text sequences through\ncharacter-based output units. Thus, such models eschew the\nclassical modular ASR architecture consisting of an acoustic\nmodel, a pronunciation model, and a language model, in\nfavor of a single compact structure, and rely on the data to\nlearn effectively. These design choices enable the deployment\nof highly accurate on-device speech recognition models (see\nSection IX), but also come with a number of downsides which\nare still areas of active research (see Section X).\nFinally, we direct interested readers to Li\u2019s excellent\ncontemporaneous overview article on end-to-end ASR [344],\nwhich offers a complementary perspective to our own. In\nparticular, readers of [344] may find a more detailed exposition\non the choice of encoder structure, and the applications of\nE2E approaches to allied ASR areas (e.g., multi-speaker\nrecognition; multilingual ASR; adaptation to new application\ndomains, and speakers; etc.), which we do not cover due to\nspace limitations.\nACKNOWLEDGMENT\nThe authors would like to thank Julian Dierkes, Yifan Peng,\nZoltan T \u00b4 uske, Albert Zeyer, and Wei Zhou for their help on \u00a8\nrefining our manuscript.\nREFERENCES\n[1] T. Bayes, \u201cAn Essay Towards Solving a Problem in the Doctrine of\nChances,\u201d Philosophical Transactions of the Royal Society of London,\nvol. 53, pp. 370\u2013418, 1763.\n[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,\nMA: MIT Press, 1997.\n[3] L. R. Rabiner, \u201cA Tutorial on Hidden Markov Models and Selected\nApplications in Speech Recognition,\u201d Proc. of the IEEE, vol. 77, no. 2,\npp. 257\u2013286, Feb. 1989.\n[4] H. A. Bourlard and N. Morgan, Connectionist Speech Recognition: a\nHybrid Approach. Norwell, MA: Kluwer Academic Publishers, 1993.\n[5] F. Seide, G. Li, and D. Yu, \u201cConversational Speech Transcription Using\nContext-Dependent Deep Neural Networks,\u201d in Proc. Interspeech,\nFlorence, Italy, Aug. 2011, pp. 437\u2013440.\n[6] V. Fontaine, C. Ris, and H. Leich, \u201cNonlinear Discriminant Analysis for\nImproved Speech Recognition,\u201d in Proc. Eurospeech, Rhodes, Greece,\nSep. 1997, pp. 1\u20134.\n[7] H. Hermansky, D. Ellis, and S. Sharma, \u201cTandem connectionist Feature\nExtraction for Conventional HMM Systems,\u201d in Proc. IEEE ICASSP,\nvol. 3, Istanbul, Turkey, Jun. 2000, pp. 1635\u20131638.\n[8] M. Nakamura and K. Shikano, \u201cA Study of English Word Category\nPrediction Based on Neural Networks,\u201d in Proc. IEEE ICASSP, Glas\u0002glow, UK, May 1989, pp. 731\u2013734.\n[9] Y. Bengio, R. Ducharme, and P. Vincent, \u201cA Neural Probabilistic\nLanguage Model,\u201d in Proc. NIPS, vol. 13, Denver, CO, Nov. 2000,\npp. 932\u2013938.\n[10] H. Schwenk and J.-L. Gauvain, \u201cConnectionist Language Modeling\nfor Large Vocabulary Continuous Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 765\u2013768.\n[11] Z. Tuske, P. Golik, R. Schl \u00a8 uter, and H. Ney, \u201cAcoustic Modeling with \u00a8\nDeep Neural Networks Using Raw Time Signal for LVCSR,\u201d in Proc.\nInterspeech, Singapore, Sep. 2014, pp. 890\u2013894.\n[12] T. N. Sainath, R. J. Weiss, K. W. Wilson, A. Narayanan, M. Bacchiani,\nand A. Senior, \u201cSpeaker Location and Microphone Spacing Invariant\nAcoustic Modeling from Raw Multichannel Waveforms,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 30\u201336.\n[13] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \u201cConnection- \u00b4\nist temporal classification: labelling unsegmented sequence data with\nrecurrent neural networks,\u201d in Proc. ICML, Pittsburgh, PA, Jun. 2006,\npp. 369\u2013376.\n[14] A. Graves, \u201cSequence Transduction with Recurrent Neural Networks,\u201d\nin Proc. ICML, Edinburgh, Scotland, Jun. 2012, Workshop on Repre\u0002sentation Learning, arXiv:1211.3711.\n[15] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\n\u201cAttention-Based Models for Speech Recognition,\u201d in Proc. NIPS,\nvol. 28, Laval, Queebec, Canada, Dec. 2015, pp. 577\u2013585. `\n[16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n4960\u20134964.\n[17] P. Liang, A. Bouchard-Cot\u02c6 e, D. Klein, and B. Taskar, \u201cAn End-to- \u00b4\nEnd Discriminative Approach to Machine Translation,\u201d in Proc. ACL,\nSydney, Australia, Jul. 2006, p. 761\u2013768.\n[18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, \u201cNatural Language Processing (Almost) from Scratch,\u201d\nJournal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.\n[19] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recognition\nwith Recurrent Neural Networks,\u201d in Proc. ICML, Beijing, China, Jun.\n2014, pp. 1764\u20131772.\n[20] \u201cCambridge Dictionary,\u201d https://dictionary.cambridge.org/dictionary/\nenglish/end-to-end, accessed: 2020-02-21.\n[21] R. Pang, T. N. Sainath, R. Prabhavalkar, S. Gupta, Y. Wu, S. Zhang, and\nC.-c. Chiu, \u201cCompression of End-to-End Models,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 27\u201331.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n21\n[22] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,\nD. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shang\u0002guan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-y. Chang, K. Rao, and\nA. Gruenstein, \u201cStreaming End-to-End Speech Recognition for Mobile\nDevices,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6381\u2013\n6385.\n[23] R. Schluter and H. Ney, \u201cModel-based MCE Bound to the True Bayes\u2019 \u00a8\nError,\u201d IEEE Signal Processing Letters, vol. 8, no. 5, pp. 131\u2013133, May\n2001.\n[24] H. Ney, \u201cOn the Relationship between Classification Error Bounds\nand Training Criteria in Statistical Pattern Recognition,\u201d in Iberian\nConference on Pattern Recognition and Image Analysis (IbPRIA),\nPuerto de Andratx, Spain, Jun. 2003, pp. 636\u2013645.\n[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A\nFramework for Self-Supervised Learning of Speech Representations,\u201d\nin Proc. NeurIPS, Vancouver, BC, Canada, Dec. 2020, pp. 12 449\u2013\n12 460.\n[26] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, \u201cMIMO\u0002Speech: End-to-End Multi-Channel Multi-Speaker Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU. Sentosa, Singapore: IEEE, Dec. 2019,\npp. 237\u2013244.\n[27] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and\nRegression Trees. Belmont, CA: Taylor & Francis, 1984.\n[28] A. Tjandra, S. Sakti, and S. Nakamura, \u201cListening While Speaking:\nSpeech Chain by Deep Learning,\u201d in Proc. IEEE ASRU. Okinawa,\nJapan: IEEE, Dec. 2017, pp. 301\u2013308.\n[29] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and\nJ. Cernock \u02c7 y, \u201cSemi-Supervised Sequence-to-Sequence ASR Using \u00b4\nUnpaired Speech and Text,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3790\u20133794, arXiv:1905.01152.\n[30] H. Soltau, H. Liao, and H. Sak, \u201cNeural Speech Recognizer: Acoustic\u0002to-Word LSTM Model for Large Vocabulary Speech Recognition,\u201d in\nProc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1610.09975.\n[31] G. K. Zipf, Human Behavior and the Principle of Least Effort. Boston,\nMA: Addison-Wesley Press, 1949.\n[32] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural Machine Translation\nof Rare Words with Subword Units,\u201d in Proc. ACL, Berlin, Germany,\nAug. 2015, pp. 1715\u20131725.\n[33] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, \u201cLatent Sequence Decompo\u0002sitions,\u201d in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1610.03035.\n[34] H. Liu, Z. Zhu, X. Li, and S. Satheesh, \u201cGram-CTC: Automatic unit\nselection and target decomposition for sequence labelling,\u201d in Proc.\nICML, ser. Proceedings of Machine Learning Research, D. Precup\nand Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 2188\u20132197,\narXiv:1703.00096.\n[35] H. Xu, S. Ding, and S. Watanabe, \u201cImproving End-to-End Speech\nRecognition with Pronunciation-Assisted Sub-Word Modeling,\u201d in\nProc. IEEE ICASSP, Brighton, UK, Sep. 2019, pp. 7110\u20137114.\n[36] W. Zhou, M. Zeineldeen, Z. Zheng, R. Schluter, and H. Ney, \u201cAcoustic \u00a8\nData-Driven Subword Modeling for End-to-End Speech Recognition,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2886\u20132890.\n[37] M. Schuster and K. Nakajima, \u201cJapanese and Korean Voice Search,\u201d\nin Proc. IEEE ICASSP, Kyoto, Japan, Mar. 2012, pp. 5149\u20135152.\n[38] M. Mohri, F. Pereira, and M. Riley, \u201cWeighted Finite-State Transducers\nin Speech Recognition,\u201d Computer Speech & Language, vol. 16, no. 1,\npp. 69\u201388, 2002.\n[39] E. Beck, M. Hannemann, P. Doetsch, R. Schluter, and H. Ney, \u00a8\n\u201cSegmental Encoder-Decoder Models for Large Vocabulary Automatic\nSpeech Recognition,\u201d in Proc. Interspeech, Hyderabad, India, Sep.\n2018.\n[40] W. Zhou, A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cEquivalence \u00a8\nof Segmental and Neural Transducer Modeling: A Proof of Concept,\u201d\nin Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2891\u20132895.\n[41] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, \u201cA Comparison of Sequence-to-Sequence Models for Speech\nRecognition,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp.\n939\u2013943.\n[42] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural\nComputation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[43] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by\nJointly Learning to Align and Translate,\u201d in Proc. ICLR, San Diego,\nCA, May 2015, arXiv:1409.0473.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is All You Need,\u201d\nin Proc. NIPS, Los Angeles, CA, Dec. 2017, pp. 5998\u20136008.\n[45] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution\u0002Augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 5036\u20135040.\n[46] H. Sak, M. Shannon, K. Rao, and F. Beaufays, \u201cRecurrent Neural\nAligner: An Encoder-Decoder Neural Network Model for Sequence to\nSequence Mapping,\u201d in Proc. Interspeech, vol. 8, Stockhol, Sweden,\nAug. 2017, pp. 1298\u20131302.\n[47] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid Autore\u0002gressive Transducer (HAT),\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6139\u20136143.\n[48] A. Graves, A.-r. Mohamed, and G. Hinton, \u201cSpeech Recognition with\nDeep Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP, Vancouver,\nBC, Canada, May 2013, pp. 6645\u20136649.\n[49] N. Moritz, T. Hori, S. Watanabe, and J. Le Roux, \u201cSequence Trans\u0002duction with Graph-Based Supervision,\u201d in Proc. IEEE ICASSP, Sin\u0002gapore, May 2022, pp. 7212\u20137216.\n[50] Y. Bengio, N. Leonard, and A. Courville, \u201cEstimating or Propagating \u00b4\nGradients through Stochastic Neurons for Conditional Computation,\u201d\nAug. 2013, arXiv:1308.3432.\n[51] A. Tripathi, H. Lu, H. Sak, and H. Soltau, \u201cMonotonic Recurrent\nNeural Network Transducer and Decoding Strategies,\u201d in Proc. IEEE\nASRU, Sentosa, Singapore, Dec. 2019, pp. 944\u2013948.\n[52] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA New Training \u00a8\nPipeline for an Improved Neural Transducer,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 2812\u20132816.\n[53] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-Based Large Vocabulary Speech Recognition,\u201d\nin Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4945\u20134949.\n[54] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, \u0141. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa,\nK. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean,\n\u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap Be\u0002tween Human and Machine Translation,\u201d Oct. 2016, arXiv:1609.08144.\n[55] M. Mimura, S. Sakai, and T. Kawahara, \u201cForward-Backward Attention\nDecoder,\u201d in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2232\u2013\n2236.\n[56] A. Graves, \u201cGenerating Sequences with Recurrent Neural Networks,\u201d\nAug. 2013, arXiv:1308.0850.\n[57] J. Hou, S. Zhang, and L.-R. Dai, \u201cGaussian Prediction Based At\u0002tention for Online End-to-End Speech Recognition,\u201d in Proc. In\u0002terspeech, Stockholm, Sweden, Aug. 2017, pp. 3692\u20133696, DOI:\n10.21437/Interspeech.2017-751.\n[58] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen,\nA. Narayanan, H. Liao, S. Zhang, A. Kannan, R. Prabhavalkar, Z. Chen,\nT. Sainath, and Y. Wu, \u201cA Comparison of End-to-End Models for Long\u0002Form Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 889\u2013896.\n[59] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,\n\u201cAn Online Sequence-to-Sequence Model Using Partial Conditioning,\u201d\nin Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 5067\u20135075.\n[60] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, \u201cOnline and\nLinear-Time Attention by Enforcing Monotonic Alignments,\u201d in Proc.\nICML, Sydney, Australia, Aug. 2017, pp. 2837\u20132846.\n[61] C.-C. Chiu and C. Raffel, \u201cMonotonic Chunkwise Attention,\u201d in Proc.\nICLR, Vancouver, Canada, Apr. 2018, arXiv:1712.05382.\n[62] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz,\nR. Pang, W. Li, and C. Raffel, \u201cMonotonic Infinite Lookback Attention\nfor Simultaneous Machine Translation,\u201d in Proc. ACL, Florence, Italy,\nJun. 2019, pp. 1313\u20131323.\n[63] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu,\nP. Nguyen, and Z. Chen, \u201cImproving the Performance of Online Neural\nTransducer Models,\u201d in Proc. IEEE ICASSP, Calgary, Alberta, Canada,\nApr. 2018, pp. 5864\u20135868.\n[64] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Brighton, England,\nMay 2019, pp. 5666\u20135670.\n[65] A. Merboldt, A. Zeyer, R. Schluter, and H. Ney, \u201cAn Analysis of Local \u00a8\nMonotonic Attention Variants,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 1398\u20131402.\n[66] A. Zeyer, R. Schluter, and H. Ney, \u201cA Study of Latent Monotonic \u00a8\nAttention Variants,\u201d Mar. 2021, arXiv:2103.16710.\n[67] A. Zeyer, R. Schmitt, W. Zhou, R. Schluter, and H. Ney, \u201cMonotonic \u00a8\nSegmental Attention for Automatic Speech Recognition,\u201d in Proc. IEEE\nSLT, Doha, Qatar, Jan. 2023, arXiv:2210.14742.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n22\n[68] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, \u201cSynchronous\nTransformers for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, arXiv:1912.02958.\n[69] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely Sequence-Trained Neural\nNetworks for ASR Based on Lattice-Free MMI,\u201d in Proc. Inter\u0002speech. San Francisco, CA: ISCA, Sep. 2016, pp. 2751\u20132755, DOI:\n10.21437/Interspeech.2016-595.\n[70] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: An End\u0002to-End Convnet-Based Speech Recognition System,\u201d Sep. 2016,\narXiv:1609.03193.\n[71] P. Haffner, \u201cConnectionist Speech Recognition with a Global MMI\nAlgorithm,\u201d in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp.\n1929\u20131932.\n[72] A. Zeyer, E. Beck, R. Schluter, and H. Ney, \u201cCTC in the Context of \u00a8\nGeneralized Full-Sum HMM Training,\u201d in Proc. Interspeech, Stock\u0002holm, Sweden, Aug. 2017, pp. 944\u2013948.\n[73] T. Raissi, W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cHMM vs. \u00a8\nCTC for Automatic Speech Recognition: Comparison Based on Full\u0002Sum Training from Scratch,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan.\n2023, arXiv:2210.09951.\n[74] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-End Speech\nRecognition Using Deep RNN Models and WFST-Based Decoding,\u201d\nin Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 167\u2013174.\n[75] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng,\n\u201cDeep Speech: Scaling up End-to-End Speech Recognition,\u201d Dec.\n2014, arXiv:1412.5567.\n[76] L. Lu, X. Zhang, and S. Renals, \u201cOn Training the Recurrent Neural\nNetwork Encoder-Decoder for Large Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp.\n5060\u20135064.\n[77] J. Chorowski and N. Jaitly, \u201cTowards Better Decoding and Language\nModel Integration in Sequence to Sequence Models,\u201d in Proc. Inter\u0002speech, Stockhol, Sweden, Aug. 2017, pp. 523\u2013527.\n[78] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery Deep Convolutional Networks\nfor End-to-End Speech Recognition,\u201d in Proc. IEEE ICASSP, New\nOrleans, LA, Mar. 2017, pp. 4845\u20134849.\n[79] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learning\nwith Low-Level Auxiliary Tasks for Encoder-Decoder based Speech\nRecognition,\u201d in Proc. Interspeech, Stockholm, Sweden, Aug. 2017,\narXiv:1704.01631.\n[80] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti\u0002Modal Data Augmentation for End-to-End ASR,\u201d in Proc. Interspeech,\nHyderabad, India, Mar. 2018, pp. 2394\u20132398.\n[81] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Distillation\nfor Sequence Learning,\u201d in Proc. ICLR, New Orleans, LA, May 2019,\narXiv:1810.01398.\n[82] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu,\n\u201cImproving Attention Based Sequence-to-Sequence Models for End-to\u0002End English Conversational Speech Recognition,\u201d in Proc. Interspeech,\nHyderabad, India, Sep. 2018, pp. 761\u2013765.\n[83] D. Le, X. Zhang, W. Zheng, C. Fugen, G. Zweig, and M. L. Seltzer, \u00a8\n\u201cFrom Senones to Chenones: Tied Context-Dependent Graphemes for\nHybrid Speech Recognition,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 457\u2013464.\n[84] S. Kanthak and H. Ney, \u201cContext-Dependent Acoustic Modeling Using\nGraphemes for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Orlando, FL, May 2002, pp. 845\u2013848.\n[85] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al., \u201cIn-Datacenter\nPerformance Analysis of a Tensor Processing Unit,\u201d in Proc. of\nthe 44th Annual International Symposium on Computer Architecture,\nToronto, Ontario, Canada, Jun. 2017, pp. 1\u201312.\n[86] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, \u201cHybrid\nCTC Attention Architecture for End-to-End Speech Recognition,\u201d\nIEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8,\npp. 1240\u20131253, 2017.\n[87] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C. Chung\u0002Cheng, \u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech, Brighton, UK, May 2019, pp. 2773\u20132777.\n[88] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, \u201cDeliberation\nModel Based Two-Pass End-to-End Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 7799\u20137803.\n[89] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab\u0002havalkar, E. Variani, and T. Strohman, \u201cCascaded Encoders for Uni\u0002fying Streaming and Non-Streaming ASR,\u201d in Proc. IEEE ICASSP,\nToronto, Ontario, Canada, Jun. 2021, pp. 5629\u20135633.\n[90] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, \u201cTransformer Trans\u0002ducer: One Model Unifying Streaming and Non-Streaming Speech\nRecognition,\u201d Oct. 2020, arXiv:2010.03192.\n[91] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu, and\nR. Pang, \u201cUniversal ASR: Unify and Improve Streaming ASR with\nFull-Context Modeling,\u201d Oct. 2020, arXiv:2010.06030.\n[92] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, and\nR. Pang, \u201cShallow-Fusion End-to-End Contextual Biasing,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 1418\u20131422.\n[93] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao,\n\u201cDeep Context: End-to-end Contextual Speech Recognition,\u201d in Proc.\nIEEE SLT, Athens, Greece, Dec. 2018, pp. 418\u2013425.\n[94] S. Kim and F. Metze, \u201cDialog-Context Aware End-to-End Speech\nRecognition,\u201d in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 434\u2013\n440.\n[95] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, \u201cPhoebe:\nPronunciation-Aware Contextualization for End-to-End Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6171\u2013\n6175.\n[96] M. Delcroix, S. Watanabe, A. Ogawa, S. Karita, and T. Nakatani,\n\u201cAuxiliary Feature Based Adaptation of End-to-End ASR Systems,\u201d\nin Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2444\u20132448.\n[97] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextNet: Improving Convolutional Neural\nNetworks for Automatic Speech Recognition with Global Context,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 3610\u20133614.\n[98] L. Dong, S. Xu, and B. Xu, \u201cSpeech-Transformer: A No-Recurrence\nSequence-to-Sequence Model for Speech Recognition,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5884\u20135888.\n[99] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer Transducer: A Streamable Speech Recognition\nModel with Transformer Encoders and RNN-T Loss,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7829\u20137833.\n[100] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain,\nK. Schubert, C. Fuegen, and M. L. Seltzer, \u201cTransformer-Transducer:\nEnd-to-Snd Speech Recognition with Self-Attention,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 7829\u20137833.\n[101] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, \u201cBranchformer: Parallel\nMLP-Attention Architectures to Capture Local and Global Context for\nSpeech Recognition and Understanding,\u201d in Proc. ICML. Baltimore,\nMD: PMLR, Jul. 2022, pp. 17 627\u201317 643.\n[102] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang, \u201cA Comparative Study on Transformer\nvs RNN in Speech Applications,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 449\u2013456.\n[103] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,\nN. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei,\nW. Zhang, and Y. Zhang, \u201cRecent Developments on ESPNET Toolkit\nBoosted by Conformer,\u201d in Proc. IEEE ICASSP. Toronto, Ontario,\nCanada: IEEE, Jun. 2021, pp. 5874\u20135878.\n[104] R. Botros, T. Sainath, R. David, E. Guzman, W. Li, and Y. He, \u201cTied &\nReduced RNN-T Decoder,\u201d in Proc. Interspeech, Brno, Czechia, Sep.\n2021, pp. 4563\u20134567.\n[105] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, \u201cRNN\u0002Transducer with Stateless Prediction Network,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7049\u20137053.\n[106] W. Zhou, S. Berger, R. Schluter, and H. Ney, \u201cPhoneme Based Neural \u00a8\nTransducer for Large Vocabulary Speech Recognition,\u201d in Proc. IEEE\nICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5644\u20135648.\n[107] R. Prabhavalkar, Y. He, D. Rybach, S. Campbell, A. Narayanan,\nT. Strohman, and T. N. Sainath, \u201cLess is More: Improved RNN-T\nDecoding Using Limited Label Context and Path Merging,\u201d in Proc.\nIEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5659\u20135663.\n[108] X. Chen, Z. Meng, S. Parthasarathy, and J. Li, \u201cFactorized Neural\nTransducer for Efficient Language Model Adaptation,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8132\u20138136, arXiv:2110.01500.\n[109] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi,\nJ. Emond, T. Strohman, B. Ramabhadran, W. R. Huang et al., \u201cModular\nHybrid Autoregressive Transducer,\u201d in Proc. IEEE SLT, Doha, Qatar,\nJan. 2023, pp. 197\u2013204, https://arXiv:2210.17049.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n23\n[110] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and\nF. Wei, \u201cVioLA: Unified Codec Language Models for Speech Recog\u0002nition, Synthesis, and Translation,\u201d May 2023, arXiv:2305.16107.\n[111] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor\u0002sos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov\net al., \u201cAudioPaLM: A Large Language Model That Can Speak and\nListen,\u201d Jun. 2023, arXiv:2306.12925.\n[112] S.-Y. Chang, B. Li, and G. Simko, \u201cA Unified Endpointer Using\nMultitask and Multidomain Training,\u201d in Proc. IEEE ASRU, Sentosa,\nSingapore, Dec. 2019, pp. 100\u2013106.\n[113] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and\nY. Wu, \u201cTowards Fast and Accurate Streaming End-To-End ASR,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6069\u20136073.\n[114] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, \u201cEnd-To\u0002End Automatic Speech Recognition Integrated with CTC-Based Voice\nActivity Detection,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6999\u20137003.\n[115] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, \u201cToward Stream\u0002ing ASR with Non-Autoregressive Insertion-Based Model,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3740\u20133744.\n[116] Y. Bengio, \u201cPractical Recommendations for Gradient-Based Training\nof Deep Architectures,\u201d Jun. 2012, arXiv:1206.5533.\n[117] J. Schmidhuber, \u201cDeep Learning in Neural Networks: An Overview,\u201d\nNeural Networks, vol. 61, pp. 85\u2013117, Jan. 2015, arXiv:1404.7828.\n[118] L. Baum, \u201cAn Inequality and Associated Maximization Technique in\nStatistical Estimation for Probabilistic Functions of Markov Processes,\u201d\nInequalities, vol. 3, pp. 1\u20138, 1972.\n[119] L. Rabiner and B.-H. Juang, \u201cAn Introduction to Hidden Markov Mod\u0002els,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing,\nvol. 3, no. 1, pp. 4\u201316, 1986.\n[120] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, \u201cNeural Network\u0002Gaussian Mixture Hybrid for Speech Recognition or Density Estima\u0002tion,\u201d in Proc. NIPS, vol. 4, Colorado, Dec. 1991, pp. 175\u2013182.\n[121] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton\nUniversity Press, 1957.\n[122] A. Viterbi, \u201cError Bounds for Convolutional Codes and an Asymptoti\u0002cally Optimal Decoding Algorithm,\u201d IEEE Transactions on Information\nTheory, vol. 13, pp. 260\u2013269, 1967.\n[123] H. Ney, \u201cThe Use of a One-Stage Dynamic Programming Algorithm\nfor Connected Word Recognition,\u201d IEEE Transactions on Acoustics,\nSpeech, and Signal Processing, vol. 32, no. 2, pp. 263\u2013271, 1984.\n[124] W. Zhou, W. Michel, R. Schluter, and H. Ney, \u201cEfficient Training \u00a8\nof Neural Transducer for Speech Recognition,\u201d in Proc. Interspeech,\nIncheon, Korea, Sep. 2022, arXiv:2204.10586.\n[125] A. Zeyer, R. Schluter, and H. Ney, \u201cWhy does CTC Result in Peaky \u00a8\nBehavior?\u201d May 2021, arXiv:2105.14849.\n[126] A. Laptev, S. Majumdar, and B. Ginsburg, \u201cCTC Variations Through\nNew WFST Topologies,\u201d in Proc. Interspeech, Incheon, Korea, sep\n2022, DOI: 10.21437/interspeech.2022-10854.\n[127] X. He, L. Deng, and W. Chou, \u201cDiscriminative Learning in Sequential\nPattern Recognition \u2013 A Unifying Review for Optimization-Oriented\nSpeech Recognition,\u201d IEEE Signal Processing Magazine, vol. 25, no. 5,\npp. 14\u201336, 2008.\n[128] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schluter, and \u00a8\nH. Ney, \u201cInvestigating Methods to Improve Language Model Inte\u0002gration for Attention-Based Encoder-Decoder ASR Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Aug. 2021, pp. 2856\u20132860.\n[129] N.-P. Wynands, W. Michel, J. Rosendahl, R. Schluter, and H. Ney, \u00a8\n\u201cEfficient Sequence Training of Attention Models using Approxima\u0002tive Recombination,\u201d in Proc. IEEE ICASSP, Singapore, May 2022,\narXiv:2110.09245.\n[130] Z. Yang, W. Zhou, R. Schluter, and H. Ney, \u201cLattice-Free Sequence \u00a8\nDiscriminative Training for Phoneme-based Neural Transducers,\u201d in\nProc. IEEE ICASSP, Rhodes, Greece, Jun. 2023, arXiv:2212.04325.\n[131] V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young, \u201cMMIE\nTraining of Large Vocabulary Recognition Systems,\u201d Speech Commu\u0002nication, vol. 22, no. 4, pp. 303\u2013314, 1997.\n[132] D. Povey and P. Woodland, \u201cImproved Discriminative Training Tech\u0002niques for Large Vocabulary Continuous Speech Recognition,\u201d in Proc.\nIEEE ICASSP, Salt Lake City, UT, May 2001, pp. 45\u201348.\n[133] R. Schluter, W. Macherey, B. M \u00a8 uller, and H. Ney, \u201cComparison of \u00a8\nDiscriminative Training Criteria and Optimization Methods for Speech\nRecognition,\u201d Speech Communication, vol. 34, no. 3, pp. 287\u2013310, May\n2001, EURASIP Best Paper Award.\n[134] B. Kingsbury, \u201cLattice-Based Optimization of Sequence Classifica\u0002tion Criteria for Neural-Network Acoustic Modeling,\u201d in Proc. IEEE\nICASSP, Taipei, Taiwan, Apr. 2009, pp. 3761\u20133764.\n[135] G. Heigold, R. Schluter, H. Ney, and S. Wiesler, \u201cDiscriminative \u00a8\nTraining for Automatic Speech Recognition: Modeling, Criteria, Opti\u0002mization, Implementation, and Performance,\u201d IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 58\u201369, Nov. 2012.\n[136] W. Michel, R. Schluter, and H. Ney, \u201cComparison of Lattice-Free and \u00a8\nLattice-Based Sequence Discriminative Training Criteria for LVCSR,\u201d\nin Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1601\u20131605,\narXiv:1907.01409.\n[137] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu,\nand A. Kannan, \u201cMinimum Word Error Rate Training for Attention\u0002Based Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4839\u20134843.\n[138] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, \u201cMinimum Bayes Risk\nTraining of RNN-Transducer for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Shanghai, China, Oct. 2020, pp. 966\u2013970, DOI:\n10.21437/Interspeech.2020-1221.\n[139] M. K. Baskar, L. Burget, S. Watanabe, M. Karafiat, T. Hori, and \u00b4\nJ. H. Cernock \u02c7 y, \u201cPromising Accurate Prefix Boosting for Sequence- `\nto-Sequence ASR,\u201d in Proc. IEEE ICASSP. Brighton, UK: IEEE,\nMay 2019, pp. 5646\u20135650.\n[140] A. Tjandra, S. Sakti, and S. Nakamura, \u201cSequence-to-Sequence ASR\nOptimization via Reinforcement Learning,\u201d in Proc. IEEE ICASSP.\nCalgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5829\u20135833.\n[141] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, \u201cSequence Training\nof Encoder-Decoder Model Using Policy Gradient for End-to-End\nSpeech Recognition,\u201d in Proc. IEEE ICASSP. Calgary, Alberta,\nCanada: IEEE, Apr. 2018, pp. 5839\u20135843.\n[142] W. Michel, R. Schluter, and H. Ney, \u201cEarly Stage LM Integration Using \u00a8\nLocal and Global Log-Linear Combination,\u201d in Proc. Interspeech,\nShanghai, China, Oct. 2020, pp. 3605\u20133609, arXiv:2005.10049.\n[143] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA Fast Learning Algorithm\nfor Deep Belief Nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u2013\n1554, Jul. 2006.\n[144] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy Layer\u0002Wise Training of Deep Networks,\u201d in Proc. NIPS, Barcelona, Spain,\nDec. 2006, pp. 153\u2013160.\n[145] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schluter, and H. Ney, \u00a8\n\u201cA Comprehensive Study of Deep Bidirectional LSTM RNNs for\nAcoustic Modeling in Speech Recognition,\u201d in Proc. IEEE ICASSP,\nNew Orleans, LA, Mar. 2017, pp. 2462\u20132466.\n[146] A. Zeyer, T. Alkhouli, and H. Ney, \u201cRETURNN as a Generic Flexible\nNeural Toolkit with Application to Translation and Speech Recogni\u0002tion,\u201d in Proc. ACL, Melbourne, Australia, Jul. 2018, pp. 128\u2013133.\n[147] A. Zeyer, K. Irie, R. Schluter, and H. Ney, \u201cImproved Training \u00a8\nof End-to-End Attention Models for Speech Recognition,\u201d in Proc.\nInterspeech, Hyderabad, India, Sep. 2018, pp. 7\u201311.\n[148] A. Zeyer, A. Merboldt, R. Schluter, and H. Ney, \u201cA Comprehensive \u00a8\nAnalysis on Attention Models,\u201d in Proc. NIPS, Montreal, Canada, Dec.\n2018.\n[149] Y. Chung, C. Wu, C. Shen, H. Lee, and L. Lee, \u201cAudio Word2Vec:\nUnsupervised Learning of Audio Segment Representations using\nSequence-to-sequence Autoencoder,\u201d in Proc. Interspeech, San Fran\u0002cisco, CA, Sep. 2016, arXiv:1603.00982.\n[150] Y.-C. Chen, S.-F. Huang, H.-y. Lee, Y.-H. Wang, and C.-H. Shen, \u201cAu\u0002dio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised\nLearning of Audio Segmentation and Representation,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 27,\nno. 9, pp. 1481\u20131493, 2019, DOI: 10.1109/TASLP.2019.2922832.\n[151] S. Scanzio, P. Laface, L. Fissore, R. Gemello, and F. Mana, \u201cOn the\nUse of a Multilingual Neural Network Front-End,\u201d in Proc. Interspeech,\nBrisbane, Australia, Sep. 2008, pp. 2711\u20132714.\n[152] Z. Tuske, J. Pinto, D. Willett, and R. Schl \u00a8 uter, \u201cInvestigation on \u00a8\nCross- and Multilingual MLP features under matched and mismatched\nacoustical conditions,\u201d in IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, Vancouver, Canada, May 2013, pp.\n7349\u20137353.\n[153] S. Zhou, S. Xu, and B. Xu, \u201cMultilingual End-to-End Speech Recog\u0002nition with a Single Transformer on Low-Resource Languages,\u201d Jun.\n2018, arXiv:1806.05059.\n[154] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL-HLT,\nMinneapolis, MN, Jun. 2019, arXiv:1904.02210.\n[155] W. Hou, Y. Dong, B. Zhuang, L. Yang, J. Shi, and T. Shinozaki,\n\u201cLarge-scale end-to-end multilingual speech recognition and language\nidentification with multi-task learning,\u201d in Proc. Interspeech, Shanghai,\nChina, Oct. 2020, pp. 1037\u20131041.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n24\n[156] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Syn\u0002naeve, and R. Collobert, \u201cMassively Multilingual ASR: 50 Languages,\n1 Model, 1 Billion Parameters,\u201d in Proc. Interspeech, Shanghai, China,\nOct. 2020, arXiv:2007.03001.\n[157] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani,\nW. R. Huang, M. Ma, and J. Bai, \u201cScaling End-to-End Models for\nLarge-Scale Multilingual ASR,\u201d in Proc. IEEE ASRU, 2021, pp. 1011\u2013\n1018.\n[158] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen,\nY. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li, M. Ma, W. Chan,\nJ. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath,\nF. Beaufays, Z. Chen, Q. V. Le, C.-C. Chiu, R. Pang, and Y. Wu,\n\u201cBigSSL: Exploring the frontier of large-scale semi-supervised learning\nfor automatic speech recognition,\u201d IEEE Journal of Selected Topics\nin Signal Processing, vol. 16, no. 6, pp. 1519\u20131532, oct 2022,\narXiv:2109.13226.\n[159] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Repre\u0002sentations through Modality Matching,\u201d in Proc. Interspeech, Incheon,\nSouth Korea, Sep. 2022, arXiv:2204.03409.\n[160] A. Radford, J. W. Kim, C. McLeavey, P. Mishkin, T. Xu, G. Brockman,\nand I. Sutskever, \u201cIntroducing Whisper - Robust Speech Recognition\nvia Large-Scale Weak Supervision,\u201d Sep. 2022. [Online]. Available:\nhttps://openai.com/blog/whisper/\n[161] T. P. Vogl, J. Mangis, A. Rigler, W. Zink, and D. Alkon, \u201cAcceler\u0002ating the Convergence of the Back-Propagation Method,\u201d Biological\nCybernetics, vol. 59, no. 4, pp. 257\u2013263, 1988.\n[162] N. S. Keskar and G. Saon, \u201cA Nonmonotone Learning Rate Strategy\nfor SGD Training of Deep Neural Networks,\u201d in Proc. IEEE ICASSP.\nQueensland, Australia: IEEE, Apr. 2015, pp. 4974\u20134978.\n[163] S. Renals, N. Morgan, H. Bourlard, C. Wooters, and P. Kohn, \u201cConnec\u0002tionist Speech Recognition: Status and Prospects,\u201d ICSI, 1991, Tech.\nRep. TR-OI-070.\n[164] D. Johnson, D. Ellis, C. Oei, C. Wooters, and P. Faerber, \u201cQuickNet,\u201d\nICSI, Berkeley, 2004. [Online]. Available: http://www.icsi.berkeley.\nedu/Speech/qn.html\n[165] A. Senior, G. Heigold, M. Ranzato, and K. Yang, \u201cAn Empirical Study\nof Learning Rates in Deep Neural Networks for Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Vancouver, BC, Canada: IEEE, May 2013,\npp. 6724\u20136728.\n[166] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d\nin Proc. ICLR, New Orleans, LA, May 2019, arXiv:1711.05101.\n[167] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, \u201cDon\u2019t Decay the\nLearning Rate, Increase the Batch Size,\u201d in Proc. ICLR, New Orleans,\nLA, May 2018, arXiv:1711.00489.\n[168] J. Howard and S. Ruder, \u201cUniversal Language Model Fine-Tuning for\nText Classification,\u201d in Proc. ACL, Melbourne, Australia, Jun. 2018,\npp. 328\u2013339.\n[169] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fer\u0002nando, and K. Kavukcuoglu, \u201cPopulation Based Training of Neural\nNetworks,\u201d Nov. 2017, arXiv:1711.09846.\n[170] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta\u0002Learning in Neural Networks: A Survey,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. PP, pp. 1\u201320, 2021.\n[171] J. L. Elman, \u201cLearning and Development in Neural Networks: The\nImportance of Starting Small,\u201d Cognition, vol. 48, no. 1, pp. 71\u201399,\n1993, DOI: 10.1016/0010-0277(93)90058-4.\n[172] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum\nLearning,\u201d in Proc. ICML, Montreal, Quebec, Canada, Jun. 2009, p.\n41\u201348.\n[173] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan\u0002zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen,\nJ. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley,\nL. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh,\nD. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao,\nD. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep Speech 2: End-to-End Speech\nRecognition in English and Mandarin,\u201d in Proc. ICML, New York City,\nNY, Jun. 2016, pp. 173\u2013182.\n[174] Z. Tuske, G. Saon, K. Audhkhasi, and B. Kingsbury, \u201cSingle Headed \u00a8\nAttention Based Sequence-to-Sequence Model for State-of-the-Art\nResults on Switchboard,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 551\u2013555.\n[175] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, \u201cImproving End\u0002to-End Single-Channel Multi-Talker Speech Recognition,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 28, pp. 1385\u2013\n1394, 2020.\n[176] B. Polyak, \u201cSome Methods of Speeding up the Convergence of\nIteration Methods,\u201d USSR Computational Mathematics and Mathe\u0002matical Physics, vol. 4, no. 5, pp. 1\u201317, 1964, DOI: 10.1016/0041-\n5553(64)90137-5.\n[177] Y. Nesterov, \u201cA method of solving a convex programming problem\nwith convergence rate O( 1\nk2\n),\u201d Soviet Mathematics Doklady, vol. 27,\npp. 372\u2013376, 1983.\n[178] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, \u201cOn the Importance\nof Initialization and Momentum in Deep Learning,\u201d in Proc. ICML,\nAtlanta, GA, Jun. 2013, pp. 1139\u20131147.\n[179] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimiza\u0002tion,\u201d in Proc. ICLR, San Diego, CA, May 2015, arXiv:1412.6980.\n[180] Z. Tuske, G. Saon, and B. Kingsbury, \u201cOn the Limit of English Con- \u00a8\nversational Speech Recognition,\u201d in Proc. Interspeech, Brno, Czechia,\nSep. 2021, pp. 2062\u20132066.\n[181] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever,\n\u201cDeep Double Descent: Where Bigger Models and More Data Hurt,\u201d\nin Proc. ICLR, virtual, Apr. 2020, arXiv:1912.02292.\n[182] A. Krogh and J. Hertz, \u201cA Simple Weight Decay Can Improve\nGeneralization,\u201d in Neural Information Processing Systems (NIPS),\nDenver, CO, Dec. 1991, pp. 950\u2013957.\n[183] A. F. Murray and P. J. Edwards, \u201cEnhanced MLP Performance and\nFault Tolerance Resulting from Synaptic Weight Noise during Train\u0002ing,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 5, pp. 792\u2013\n802, Sep. 1994.\n[184] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nAdvances in Neural Information Processing Systems, vol. 24, 2011.\n[185] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach,\nand J. Martens, \u201cAdding Gradient Noise Improves Learning for Very\nDeep Networks,\u201d Nov. 2015, arXiv:1511.06807.\n[186] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, \u201cImproving Neural Networks by Preventing Co\u0002Adaptation of Feature Detectors,\u201d Jul. 2012, arXiv:1207.0580.\n[187] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet Classification\nwith Deep Convolutional Neural Networks,\u201d in Advances in Neural\nInformation Processing Systems (NIPS), vol. 25, Lake Tahoe, NV, Dec.\n2012.\n[188] Y. Gal and Z. Ghahramani, \u201cDropout as a Bayesian Approximation:\nRepresenting Model Uncertainty in Deep Learning,\u201d in Proc. ICML,\nNew York City, NY, Jun. 2016, pp. 1050\u20131059.\n[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep Net\u0002works with Stochastic Depth,\u201d in European Conference on Computer\nVision, Amsterdam, Netherlands, Oct. 2016, pp. 646\u2013661.\n[190] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. Muller, and A. Waibel, \u00a8\n\u201cVery Deep Self-Attention Networks for End-to-End Speech Recogni\u0002tion,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 66\u201370.\n[191] J. Lee and S. Watanabe, \u201cIntermediate Loss Regularization for CTC\u0002Based Speech Recognition,\u201d in Proc. IEEE ICASSP, Toronto, Ontario,\nCanada, Jun. 2021, pp. 6224\u20136228.\n[192] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, \u201cRegularization\nof Neural Networks using DropConnect,\u201d in Proc. ICML, 2013, pp.\n1058\u20131066.\n[193] D. Krueger, T. Maharaj, J. Kramar, M. Pezeshki, N. Ballas, N. R. Ke, \u00b4\nA. Goyal, Y. Bengio, A. Courville, and C. Pal, \u201cZoneout: Regularizing\nRNNs by Randomly Preserving Hidden Activations,\u201d in Proc. ICLR,\nToulon, France, Apr. 2017, arXiv:1606.01305.\n[194] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink\u0002ing the Inception Architecture for Computer Vision,\u201d in IEEE Conf. on\nComputer Vision and Pattern Recognition, Las Vegas, NV, Jun. 2016,\npp. 2818\u20132826.\n[195] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled Sampling\nfor Sequence Prediction with Recurrent Neural Networks,\u201d Proc. NIPS,\nvol. 28, Dec. 2015.\n[196] T. Trinh, A. Dai, T. Luong, and Q. Le, \u201cLearning Longer-Term Depen\u0002dencies in RNNs with Auxiliary Losses,\u201d in Proc. ICML, Stockholm,\nSweden, Jul. 2018, pp. 4965\u20134974.\n[197] R. J. Williams and J. Peng, \u201cAn Efficient Gradient-Based Algorithm\nfor On-Line Training of Recurrent Network Trajectories,\u201d IEEE Neural\nComputation, vol. 2, no. 4, pp. 490\u2013501, 1990.\n[198] S. Merity, N. S. Keskar, and R. Socher, \u201cAn Analysis of Neural\nLanguage Modeling at Multiple Scales,\u201d Mar. 2018, arXiv:1803.08240.\n[199] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, \u201cMixSpeech:\nData Augmentation for Low-resource Automatic Speech Recognition,\u201d\nin Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021,\npp. 7008\u20137012.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n25\n[200] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep\nNetwork Training by Reducing Internal Covariate Shift,\u201d in Proc.\nICML, Lille, France, Jul. 2015, pp. 448\u2013456.\n[201] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic Spectral Distortion for\nLow Resource Speech Recognition with Deep Neural Networks,\u201d in\nProc. IEEE ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 309\u2013\n314.\n[202] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmentation\nfor Speech Recognition,\u201d in Proc. Interspeech, Dresden, Germany, Sep.\n2015.\n[203] N. Jaitly and G. E. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nImproves Speech Recognition,\u201d in Proc. ICML, vol. 117, Jun. 2013,\np. 21.\n[204] G. Saon, Z. Tuske, K. Audhkhasi, and B. Kingsbury, \u201cSequence Noise \u00a8\nInjected Training for End-to-End Speech Recognition,\u201d in Proc. IEEE\nICASSP, Brighton, England, May 2019, pp. 6261\u20136265.\n[205] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le,\nand Y. Wu, \u201cSpecAugment on Large Scale Datasets,\u201d in Proc. IEEE\nICASSP, Brighton, UK, May 2019, pp. 6879\u20136883.\n[206] C. Wang, Y. Wu, Y. Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao, and\nM. Zhou, \u201cSemantic Mask for Transformer Based End-to-End Speech\nRecognition,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n971\u2013975.\n[207] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo,\nand K. Takeda, \u201cBack-Translation-Style Data Augmentation for End\u0002to-End ASR,\u201d in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018,\npp. 426\u2013433.\n[208] N. Rossenbach, M. Zeineldeen, B. Hilmes, R. Schluter, and H. Ney, \u00a8\n\u201cComparing the Benefit of Synthetic Training Data for Various Au\u0002tomatic Speech Recognition Architectures,\u201d in Proc. IEEE ASRU,\nCartagena, Colombia, Dec. 2021, arXiv:2104.05379.\n[209] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen, and\nC.-C. Chiu, \u201cNo Need for a Lexicon? Evaluating the Value of\nthe Pronunciation Lexica in End-to-End Models,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5859\u20135863, DOI:\n10.1109/ICASSP.2018.8462380.\n[210] C. Wooters and A. Stolcke, \u201cMultiple-Pronunciation Lexical Modeling\nin a Speaker Independent Speech Understanding System,\u201d in Proc.\nICSLP, Yokohama, Japan, Sep. 1994, pp. 1363\u20131366.\n[211] I. McGraw, I. Badr, and J. R. Glass, \u201cLearning Lexicons From Speech\nUsing a Pronunciation Mixture Model,\u201d IEEE/ACM Trans. Audio,\nSpeech, and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2012.\n[212] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGMM-Free DNN\nAcoustic Model Training,\u201d in Proc. IEEE ICASSP, Florence, Italy, May\n2014, pp. 5602\u20135606, DOI: 10.1109/ICASSP.2014.6854675.\n[213] G. Gosztolya, T. Grosz, and L. T \u00b4 oth, \u201cGMM-Free Flat Start Sequence- \u00b4\nDiscriminative DNN Training,\u201d in Proc. Interspeech, N. Morgan,\nEd. San Francisco, CA: ISCA, Sep. 2016, pp. 3409\u20133413, DOI:\n10.21437/Interspeech.2016-391.\n[214] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cFlat-Start\nSingle-Stage Discriminatively Trained HMM-Based Models for ASR,\u201d\nIEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 26,\nno. 11, pp. 1949\u20131961, 2018.\n[215] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig,\n\u201cThe IBM 2004 Conversational Telephony System for Rich Transcrip\u0002tion,\u201d in Proc. IEEE ICASSP, Philadelphia, PA, Mar. 2005, pp. 205\u2013\n208.\n[216] H. Hadian, D. Povey, H. Sameti, J. Trmal, and S. Khudanpur,\n\u201cImproving LF-MMI Using Unconstrained Supervisions for ASR,\u201d\nin Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 43\u201347, DOI:\n10.1109/SLT.2018.8639684.\n[217] N. Kanda, Y. Fujita, and K. Nagamatsu, \u201cLattice-Free State-Level Min\u0002imum Bayes Risk Training of Acoustic Models,\u201d in Proc. Interspeech,\nB. Yegnanarayana, Ed. Hyderabad, India: ISCA, Sep. 2018, pp. 2923\u2013\n2927, DOI: 10.21437/Interspeech.2018-79.\n[218] S. J. Young and P. C. Woodland, \u201cThe Use of State Tying in Continuous\nSpeech Recognition,\u201d in Proc. Eurospeech, Berlin, Germany, Dec.\n1993, pp. 2203\u20132206.\n[219] S. Wiesler, G. Heigold, M. Nu\u00dfbaum-Thom, R. Schluter, and H. Ney, \u00a8\n\u201cA Discriminative Splitting Criterion for Phonetic Decision Trees,\u201d\nin Proc. Interspeech, Makuhari, Japan, Sep. 2010, pp. 54\u201357, one of\nshortlist for Best Student Paper Award.\n[220] T. Raissi, E. Beck, R. Schluter, and H. Ney, \u201cTowards Consistent \u00a8\nHybrid HMM Acoustic Modeling,\u201d Apr. 2021, arXiv:2104.02387.\n[221] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schluter, and H. Ney, \u00a8\n\u201cA Systematic Comparison of Grapheme-Based vs. Phoneme-Based\nLabel Units for Encoder-Decoder-Attention Models,\u201d Nov. 2020,\narXiv:2005.09336.\n[222] C. Luscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, \u00a8\nR. Schluter, and H. Ney, \u201cRWTH ASR Systems for LibriSpeech: \u00a8\nHybrid vs Attention,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 231\u2013235.\n[223] D. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. Le,\n\u201cImproved Noisy Student Training for Automatic Speech Recognition,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2817\u20132821.\n[224] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and\nQ. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for\nAutomatic Speech Recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 2613\u20132617.\n[225] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schluter, and H. Ney, \u201cThe \u00a8\nRWTH ASR System for TED-LIUM Release 2: Improving Hybrid\nHMM with SpecAugment,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 7839\u20137843.\n[226] J. Cui, B. Kingsbury, B. Ramabhadran, A. Sethy, K. Audhkhasi, X. Cui,\nE. Kislal, L. Mangu, M. Nussbaum-Thom, M. Picheny, Z. Tuske, \u00a8\nP. Golik, R. Schluter, H. Ney, M. J. F. Gales, K. M. Knill, A. Ragni, \u00a8\nH. Wang, and P. Woodland, \u201cMultilingual Representations for Low\nResource Speech Recognition and Keyword Search,\u201d in Proc. IEEE\nASRU, Scottsdale, AZ, Dec. 2015, pp. 259\u2013266.\n[227] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, \u201cMassively\nMultilingual Adversarial Speech Recognition,\u201d in Proc. NAACL, Min\u0002neapolis, MN, Jun. 2019, pp. 96\u2013108.\n[228] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. Ramabhadran,\nY. Wu, A. Bapna, Z. Chen, and S. Lee, \u201cLarge-Scale Multilingual\nSpeech Recognition with a Streaming End-to-End Model,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2130\u20132134.\n[229] A. Graves, \u201cConnectionist Temporal Classification,\u201d in Supervised\nSequence Labelling with Recurrent Neural Networks. Heidelberg,\nGermany: Springer, 2012, ch. Connectionist Temporal Classification,\npp. 61\u201393.\n[230] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi,\n\u201cMask CTC: Non-Autoregressive End-to-End ASR with CTC and\nMask Predict,\u201d in Proc. Interspeech, Shanghai, China, Oct. 2020, pp.\n3655\u20133659.\n[231] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, \u201cImputer:\nSequence Modelling via Imputation and Dynamic Programming,\u201d in\nProc. ICML. PMLR, Jul. 2020, pp. 1403\u20131413.\n[232] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, \u201cInsertion-Based\nModeling for End-to-End Automatic Speech Recognition,\u201d in Proc.\nInterspeech, Shanghai, China, Oct. 2020, pp. 3660\u20133664.\n[233] L. Dong and B. Xu, \u201cCif: Continuous Integrate-and-Fire for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain,\nMay 2020, pp. 6079\u20136083.\n[234] J. Nozaki and T. Komatsu, \u201cRelaxing the Conditional Independence\nAssumption of CTC-Based ASR by Conditioning on Intermediate\nPredictions,\u201d in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3735\u2013\n3739.\n[235] Y. Higuchi, N. Chen, Y. Fujita, H. Inaguma, T. Komatsu, J. Lee,\nJ. Nozaki, T. Wang, and S. Watanabe, \u201cA Comparative Study on Non\u0002Autoregressive Modelings for Speech-to-Text Generation,\u201d in Proc.\nIEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2110.05249.\n[236] W. Zhou, R. Schluter, and H. Ney, \u201cRobust Beam Search for Encoder- \u00a8\nDecoder Attention Based Speech Recognition without Length Bias,\u201d\nin Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1768\u20131772.\n[237] P. Koehn and R. Knowles, \u201cSix Challenges for Neural Machine Transla\u0002tion,\u201d in First Workshop on Neural Machine Translation. Vancouver,\nBC, Canada: Association for Computational Linguistics, Aug. 2017,\npp. 28\u201339.\n[238] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, \u201cModeling Coverage for Neural\nMachine Translation,\u201d in Proc. ACL, Berlin, Germany, May 2016, pp.\n76\u201385.\n[239] T. Hori, J. Cho, and S. Watanabe, \u201cEnd-to-End Speech Recogni\u0002tion with Word-Based RNN Language Models,\u201d in Proc. IEEE SLT.\nAthens, Greece: IEEE, Dec. 2018, pp. 389\u2013396.\n[240] K. Deng and P. C. Woodland, \u201cLabel-Synchronous Neural Transducer\nfor End-to-End ASR,\u201d Jul. 2023, arXiv:2307.03088.\n[241] T. Hori and A. Nakamura, Speech Recognition Algorithms Using\nWeighted Finite-State Transducers. San Rafael, CA: Morgan &\nClaypool Publishers, 2013.\n[242] R. Haeb-Umbach and H. Ney, \u201cImprovements in Beam Search for\n10000-Word Continuous-Speech Recognition,\u201d IEEE Transactions on\nSpeech and Audio Processing, vol. 2, no. 2, pp. 353\u2013356, 1994.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n26\n[243] H. Ney and S. Ortmanns, \u201cProgress in Dynamic Programming Search\nfor LVCSR,\u201d Proc. of the IEEE, vol. 88, no. 8, pp. 1224\u20131240, Aug.\n2000, http://dx.doi.org/10.1109/5.880081.\n[244] T. Hori, Y. Kubo, and A. Nakamura, \u201cReal-Time One-Pass Decoding\nwith Recurrent Neural Network Language Model for Speech Recog\u0002nition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6364\u2013\n6368.\n[245] E. Beck, W. Zhou, R. Schluter, and H. Ney, \u201cLSTM Language Models \u00a8\nfor LVCSR in First-Pass Decoding and Lattice-Rescoring,\u201d Jul. 2019,\narXiv:1907.01030.\n[246] G. Saon, Z. Tuske, and K. Audhkhasi, \u201cAlignment-Length Syn- \u00a8\nchronous Decoding for RNN Transducer,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, May 2020, pp. 7804\u20137808.\n[247] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, \u201cFirst\u0002Pass Large Vocabulary Continuous Speech Recognition Using Bi\u0002Directional Recurrent DNNs,\u201d Dec. 2014, arXiv:1408.2873.\n[248] N. Moritz, T. Hori, and J. Le Roux, \u201cTriggered Attention for End-to\u0002End Speech Recognition,\u201d in Proc. IEEE ICASSP. Brighton, UK:\nIEEE, May 2019, pp. 5666\u20135670.\n[249] N. Moritz, T. Hori, and J. Le, \u201cStreaming Automatic Speech Recogni\u0002tion with the Transformer Model,\u201d in Proc. IEEE ICASSP. Barcelona,\nSpain: IEEE, May 2020, pp. 6074\u20136078.\n[250] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kalgaonkar, A. Sri\u0002ram, C. Fuegen, and M. L. Seltzer, \u201cRNN-T for Latency Controlled\nASR with Improved Beam Search,\u201d Nov. 2019, arXiv:1911.01629.\n[251] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring Transformers for Large\u0002Scale Speech Recognition,\u201d in Proc. Interspeech, Shanghai, China, Oct.\n2020, pp. 5041\u20135045.\n[252] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, \u201cStreaming End-to\u0002End ASR Based on Blockwise Non-Autoregressive Models,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 3755\u20133759.\n[253] H. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, \u201cOnline Hybrid\nCTC/Attention Architecture for End-to-End Speech Recognition,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 2623\u20132627, DOI:\n10.21437/Interspeech.2019-2018.\n[254] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, \u201cStreaming Transformer\nASR with Blockwise Synchronous Beam Search,\u201d in Proc. IEEE SLT.\nShenzhen, China: IEEE, Jun. 2021, pp. 22\u201329.\n[255] K. Hwang and W. Sung, \u201cCharacter-level language modeling with\nhierarchical recurrent neural networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5720\u20135724.\n[256] T. Hori, S. Watanabe, Y. Zhang, and W. Chan, \u201cAdvances in Joint CTC\u0002Attention Based End-to-End Speech Recognition with a Deep CNN\nEncoder and RNN-LM,\u201d in Proc. Interspeech, Stockhol, Sweden, Aug.\n2017, pp. 949\u2013953.\n[257] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, \u201cAn Analysis of Incorporating an External Lan\u0002guage Model into a Sequence-to-Sequence Model,\u201d in Proc. IEEE\nICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5824\u20135828, DOI:\n10.1109/ICASSP.2018.8462682.\n[258] G. Saon, Z. Tuske, D. Bolanos, and B. Kingsbury, \u201cAdvancing \u00a8\nRNN Transducer Technology for Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5654\u20135658,\narXiv:2103.09935.\n[259] H. Seki, T. Hori, S. Watanabe, N. Moritz, and J. Le Roux, \u201cVectorized\nBeam Search for CTC-Attention-Based Speech Recognition,\u201d in Proc.\nInterspeech, Brighton, UK, May 2019, pp. 3825\u20133829.\n[260] T. Hori, S. Watanabe, and J. R. Hershey, \u201cMulti-Level Language\nModeling and Decoding for Open Vocabulary End-to-End Speech\nRecognition,\u201d in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec.\n2017, pp. 287\u2013293.\n[261] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie,\nS. Watanabe, and S. Khudanpur, \u201cEspresso: A Fast End-to-End Neural\nSpeech Recognition Toolkit,\u201d in Proc. IEEE ASRU, Sentosa, Singapore,\nDec. 2019, pp. 136\u2013143.\n[262] Z. Tuske, K. Audhkhasi, and G. Saon, \u201cAdvancing Sequence-to- \u00a8\nSequence Based Speech Recognition,\u201d in Proc. Interspeech, Graz,\nAustria, Sep. 2019, pp. 3780\u20133784.\n[263] J. Drexler and J. Glass, \u201cSubword Regularization and Beam Search\nDecoding for End-to-End Automatic Speech Recognition,\u201d in Proc.\nIEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 6266\u20136270.\n[264] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li,\nM. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu,\n\u201cTwo-Pass End-to-End Speech Recognition,\u201d in Proc. Interspeech,\nGraz, Austria, Sep. 2019, pp. 2773\u20132777.\n[265] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen,\nL. Xie, and X. Lei, \u201cWeNet: Production Oriented Streaming and Non\u0002Streaming End-to-End Speech Recognition Toolkit,\u201d Brno, Czechia, pp.\n4054\u20134058, Sep. 2021.\n[266] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei,\n\u201cU2++: Unified Two-Pass Bidirectional End-to-End Model for Speech\nRecognition,\u201d Dec. 2021, arXiv:2106.05642.\n[267] M. Zapotoczny, P. Pietrzak, A. Lancucki, and J. Chorowski, \u201cLattice\nGeneration in Attention-Based Speech Recognition Models,\u201d in Proc.\nInterspeech, Graz, Austria, Sep. 2019, pp. 2225\u20132229.\n[268] J. Kim, Y. Lee, and E. Kim, \u201cAccelerating RNN Transducer Inference\nvia Adaptive Expansion Search,\u201d IEEE Signal Processing Letters,\nvol. 27, pp. 2019\u20132023, 2020.\n[269] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, and X. Zheng, \u201cTensorFlow: A system for Large\u0002Scale Machine Learning,\u201d in Proc. OSDI, Savannah, GA, Nov. 2016,\npp. 265\u2013283.\n[270] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,\nand M. Auli, \u201cFAIRSEQ: A Fast, Extensible Toolkit for Sequence\nModeling,\u201d in Proc. NAACL, Minneapolis, MN, Jun. 2019, pp. 48\u201353.\n[271] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan,\nT. Sainath, Y. Cao, C.-C. Chiu et al., \u201cLingvo: a Modular and\nScalable Framework for Sequence-to-Sequence Modeling,\u201d Feb. 2019,\narXiv:1902.08295.\n[272] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schluter, and \u00a8\nH. Ney, \u201cRETURNN: The RWTH Extensible Training Framework for\nUniversal Recurrent Neural Networks,\u201d in Proc. IEEE ICASSP. New\nOrleans, LA: IEEE, Mar. 2017, pp. 5345\u20135349.\n[273] A. Hannun, A. Lee, Q. Xu, and R. Collobert, \u201cSequence-to-Sequence\nSpeech Recognition with Time-Depth Separable Convolutions,\u201d in\nProc. Interspeech, Graz, Austria, Sep. 2019, pp. 3785\u20133789.\n[274] M. Li, M. Liu, and H. Masanori, \u201cEnd-to-End Speech Recognition with\nAdaptive Computation Steps,\u201d in Proc. IEEE ICASSP, Brighton, UK,\nMay 2019, pp. 6246\u20136250.\n[275] P. Bahar, N. Makarov, A. Zeyer, R. Schuter, and H. Ney, \u201cExploring \u00a8\na Zero-Order Direct HMM Based on Latent Attention for Automatic\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 7854\u20137858.\n[276] Z. Huang, G. Zweig, and B. Dumoulin, \u201cCache Based Recurrent\nNeural Network Language Model Inference for First Pass Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp.\n6354\u20136358.\n[277] J. Jorge, A. Gimenez, J. Iranzo-S \u00b4 anchez, J. Civera, A. Sanchis, and \u00b4\nA. Juan, \u201cReal-Time One-Pass Decoder for Speech Recognition Using\nLSTM Language Models,\u201d in Proc. Interspeech, Graz, Austria, Sep.\n2019, pp. 3820\u20133824.\n[278] W. Zhou, R. Schluter, and H. Ney, \u201cFull-Sum Decoding for Hybrid \u00a8\nHMM Based Speech Recognition Using LSTM Language Model,\u201d in\nProc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7834\u20137838.\n[279] P. Sountsov and S. Sarawagi, \u201cLength Bias in Encoder Decoder Models\nand a Case for Global Conditioning,\u201d in Proc. EMNLP, Austin, TX,\nNov. 2016, pp. 1516\u20131525.\n[280] K. Murray and D. Chiang, \u201cCorrecting Length Bias in Neural Machine\nTranslation,\u201d in Proc. WMT, Brussels, Belgium, Oct. 2018, pp. 212\u2013\n223.\n[281] F. Stahlberg and B. Byrne, \u201cOn NMT Search Errors and Model Errors:\nCat Got Your Tongue?\u201d in Proc. EMNLP. Hong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 3354\u20133360.\n[282] N. Deshmukh, A. Ganapathiraju, and J. Picone, \u201cHierarchical Search\nfor Large-Vocabulary Conversational Speech Recognition: Working\nToward a Solution to the Decoding Problem,\u201d IEEE Signal Processing\nMagazine, vol. 16, no. 5, pp. 84\u2013107, 1999.\n[283] L. Nguyen and R. Schwartz, \u201cSingle-Tree Method for Grammar\u0002Directed Search,\u201d in Proc. IEEE ICASSP, vol. 2, Phoenix, AZ, Mar.\n1999, pp. 613\u2013616.\n[284] L. Sar\u0131, N. Moritz, T. Hori, and J. Le Roux, \u201cUnsupervised Speaker\nAdaptation using Attention-based Speaker Memory for End-to-End\nASR,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 2\u2013\n6.\n[285] F. Weninger, J. Andres-Ferrer, X. Li, and P. Zhan, \u201cListen, Attend, \u00b4\nSpell and Adapt: Speaker Adapted Sequence-to-Sequence ASR,\u201d in\nProc. Interspeech. Graz, Austria: ISCA, Sep. 2019, pp. 3805\u20133809.\n[286] Z. Meng, Y. Gaur, J. Li, and Y. Gong, \u201cSpeaker Adaptation for\nAttention-Based End-to-End Speech Recognition,\u201d in Proc. Inter\u0002speech. Graz, Austria: ISCA, Sep. 2019, pp. 241\u2013245.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n27\n[287] N. Tomashenko and Y. Esteve, \u201cEvaluation of Feature-Space Speaker `\nAdaptation for End-to-End Acoustic Models,\u201d in Proc. LREC.\nMiyazaki, Japan: ELRA, May 2018, pp. 3163\u20133170.\n[288] S. F. Chen and J. Goodman, \u201cAn Empirical Study of Smoothing\nTechniques for Language Modeling,\u201d in Proc. ACL, Santa Cruz, CA,\nJun. 1996, pp. 310\u2013318.\n[289] T. Mikolov, M. Karafiat, L. Burget, J. \u00b4 Cernock \u02c7 y, and S. Khudanpur, `\n\u201cRecurrent Neural Network Based Language Model,\u201d in Proc. Inter\u0002speech, Makuhari, Japan, Sep. 2010, pp. 1045\u20131048.\n[290] M. Sundermeyer, R. Schluter, and H. Ney, \u201cLSTM Neural Networks for \u00a8\nLanguage Modeling,\u201d in Proc. Interspeech, Portland, OR, Sep. 2012,\npp. 194\u2013197.\n[291] N.-Q. Pham, G. Kruszewski, and G. Boleda, \u201cConvolutional Neural\nNetwork Language Models,\u201d in Proc. EMNLP, Austin, TX, Nov. 2016,\npp. 1153\u20131162.\n[292] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage Modeling\nwith Gated Convolutional Networks,\u201d in Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. JMLR.\norg, 2017, pp. 933\u2013941.\n[293] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and\nR. Collobert, \u201cFully Convolutional Speech Recognition,\u201d Feb. 2018,\narXiv:1812.06864.\n[294] T. Likhomanenko, G. Synnaeve, and R. Collobert, \u201cWho needs words?\nlexicon-free speech recognition,\u201d in Proc. Interspeech, Graz, Austria,\nSep. 2019, pp. 3915\u20133919, arXiv:1904.04479.\n[295] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, \u201cCharacter\u0002Level Language Modeling with Deeper Self-Attention,\u201d in Proc. AIII,\nvol. 33, Honolulu, Hawaii, Feb. 2019, pp. 3159\u20133166.\n[296] K. Irie, A. Zeyer, R. Schluter, and H. Ney, \u201cLanguage Modeling with \u00a8\nDeep Transformers,\u201d in Proc. Interspeech, Graz, Austria, Sep. 2019,\npp. 3905\u20133909.\n[297] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov,\n\u201cTransformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 2978\u20132988.\n[298] P. Werbos, \u201cBackpropagation Through Time: What It Does and How\nto Do It,\u201d Proc. of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990,\nDOI: 10.1109/5.58337.\n[299] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap,\n\u201cCompressive Transformers for Long-Range Sequence Modelling,\u201d\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6154\u2013\n6158, 2020.\n[300] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn Using Monolingual\nCorpora in Neural Machine Translation,\u201d Jun. 2015, arXiv:1503.03535.\n[301] A. Sriram, H. Jun, S. Satheesh, and A. Coates, \u201cCold Fusion: Training\nSeq2Seq Models Together with Language Models,\u201d in Proc. Inter\u0002speech, Hyderabad, India, Sep. 2018, pp. 387\u2013391.\n[302] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n\u201cComponent Fusion: Learning Replaceable Language Model Com\u0002ponent for End-to-End Speech Recognition System,\u201d in Proc. IEEE\nICASSP. Brighton, UK: IEEE, May 2019, pp. 5361\u20135635.\n[303] E. McDermott, H. Sak, and E. Variani, \u201cA Density Ratio Approach to\nLanguage Model Fusion in End-To-End Automatic Speech Recogni\u0002tion,\u201d in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 434\u2013\n441.\n[304] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen,\nR. Zhao, J. Li, and Y. Gong, \u201cInternal Language Model Estimation\nfor Domain-Adaptive End-to-End Speech Recognition,\u201d in Proc. IEEE\nSLT, Shenzhen , China, Dec. 2020, pp. 243\u2013250.\n[305] W. Zhou, Z. Zheng, R. Schluter, and H. Ney, \u201cOn Language Model Inte- \u00a8\ngration for RNN Transducer based Speech Recognition,\u201d in Proc. IEEE\nICASSP, Singapore, May 2022, pp. 8407\u20138411, arXiv:2110.06841.\n[306] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre\u0002Training of Deep Bidirectional Transformers for Language Understand\u0002ing,\u201d in Proc. ACL, Florence, Italy, Jul. 2019, pp. 4171\u20134186.\n[307] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, \u201cLanguage Models are Unsuper\u0002vised Multitask Learners,\u201d 2019, openAI blog. [Online].\nAvailable: https://cdn.openai.com/better-language-models/language\nmodels are unsupervised multitask learners.pdf\n[308] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, \u201cMasked\nLanguage Model Scoring,\u201d in Proc. ACL, Jul. 2020, pp. 2699\u20132712.\n[309] S. Kim, S. Dalmia, and F. Metze, \u201cGated Embeddings in End-to-End\nSpeech Recognition for Conversational-Context Fusion,\u201d in Proc. ACL,\nFlorence, Italy, Jul. 2019, pp. 1131\u20131141.\n[310] A. Zeyer, A. Merboldt, W. Michel, R. Schluter, and H. Ney, \u201cLib- \u00a8\nrispeech Transducer Model with Internal Language Model Prior Cor\u0002rection,\u201d in Proc. Interspeech, Brno, Czech Republic, Apr. 2021, pp.\n2052\u20132056.\n[311] L. R. Bahl, F. Jelinek, and R. L. Mercer, \u201cA Maximum Likelihood\nApproach to Continuous Speech Recognition,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 5, no. 2, pp. 179\u2013190,\nMar. 1983.\n[312] J. Makhoul and R. Schwartz, \u201cState of the Art in Continuous Speech\nRecognition,\u201d Proc. NAS, vol. 92, no. 22, pp. 9956\u20139963, Oct. 1995.\n[313] D. Klakow and J. Peters, \u201cTesting the Correlation of Word Error Rate\nand Perplexity,\u201d Speech Communication, vol. 38, no. 1, pp. 19\u201328,\n2002.\n[314] M. Sundermeyer, H. Ney, and R. Schluter, \u201cFrom Feedforward to Re- \u00a8\ncurrent LSTM Neural Networks for Language Modeling,\u201d IEEE/ACM\nTrans. Audio, Speech, and Language Processing, vol. 23, no. 3, pp.\n517\u2013529, Mar. 2015.\n[315] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, \u201cMinimum Word\nError Training of Long Short-Term Memory Recurrent Neural Network\nLanguage Models for Speech Recognition,\u201d in Proc. IEEE ICASSP,\nShanghai, China, Mar. 2016, pp. 5990\u20135994.\n[316] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Tele\u0002phone Speech Corpus for Research and Development,\u201d in Proc. IEEE\nICASSP, vol. 1, San Francisco, CA, Mar. 1992, pp. 517\u2013520 vol.1.\n[317] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an\nASR Corpus Based on Public Domain Audio Books,\u201d in Proc. IEEE\nICASSP, Queensland, Australia, Apr. 2015, pp. 5206\u20135210.\n[318] A. Zeyer, P. Bahar, K. Irie, R. Schluter, and H. Ney, \u201cA Comparison of \u00a8\nTransformer and LSTM Encoder Decoder Models for ASR,\u201d in Proc.\nIEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 8\u201315.\n[319] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,\nand A. Mohamed, \u201cHuBERT: Self-Supervised Speech Representation\nLearning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans.\nAudio, Speech, and Language Processing, vol. 19, pp. 3451\u20133460,\n2021.\n[320] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap,\nA. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-End ASR: from\nSupervised to Semi-Supervised Learning with Modern Architectures,\u201d\nin Proc. ICML, Jul. 2020, arXiv:1911.08460.\n[321] E. G. Ng, C.-C. Chiu, Y. Zhang, and W. Chan, \u201cPushing the Limits of\nNon-Autoregressive Speech Recognition,\u201d in Proc. Interspeech, Brno,\nCzechia, Sep. 2021, pp. 3725\u20132729.\n[322] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazare,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko,\nG. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-Light: A\nBenchmark for ASR with Limited or no Supervision,\u201d in Proc. IEEE\nICASSP, Barcelona, Spain, May 2020, pp. 7669\u20137673.\n[323] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig,\nand M. L. Seltzer, \u201cTransformer-Based Acoustic Modeling for Hybrid\nSpeech Recognition,\u201d in Proc. IEEE ICASSP, Barcelona, Spain, May\n2020, pp. 6874\u20136878.\n[324] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and\nS. Watanabe, \u201cE-branchformer: Branchformer with enhanced merging\nfor speech recognition,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.00077.\n[325] M. Kitza, P. Golik, R. Schluter, and H. Ney, \u201cCumulative Adaptation for \u00a8\nBLSTM Acoustic Models,\u201d in Interspeech, Graz, Austria, Sep. 2019,\npp. 754\u2013758.\n[326] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. Li,\nJ. Chorowski, and M. Bacchiani, \u201cState-of-the-Art Speech Recognition\nwith Sequence-to-Sequence Models,\u201d in Proc. IEEE ICASSP, Calgary,\nAlberta, Canada, Apr. 2018, pp. 4774\u20134778.\n[327] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo,\nD. Kim, S. Jung, J. Lee, M. Han, and C. Kim, \u201cAttention Based On\u0002Device Streaming Speech Recognition with Large Speech Corpus,\u201d in\nProc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 956\u2013963.\n[328] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov,\nZ. Wang, L. He, S. Zhao et al., \u201cDeveloping RNN-T Models Surpassing\nHigh-Performance Hybrid Models with Customization Capability,\u201d in\nProc. Interspeech, Shanghai, China (virtual), Oct. 2020, pp. 3590\u2013\n3594, arXiv:2007.15188.\n[329] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, \u201cOnline\nAutomatic Speech Recognition with Listen, Attend and Spell Model,\u201d\nIEEE Signal Processing Letters, vol. 27, pp. 1889\u20131893, 2020.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n28\n[330] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and\nM. Seltzer, \u201cEmformer: Efficient Memory Transformer based Acoustic\nModel for Low Latency Streaming Speech Recognition,\u201d in Proc. IEEE\nICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 6783\u20136787.\n[331] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping Real-Time\nStreaming Transformer Transducer for Speech Recognition on Large\u0002Scale Dataset,\u201d in Proc. IEEE ICASSP. Toronto, Ontario, Canada:\nIEEE, Jun. 2021, pp. 5904\u20135908.\n[332] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, C.-C. Chiu, D. Garcia,\nA. Gruenstein, K. Hu, M. Jin, A. Kannan, Q. Liang, I. McGraw,\nC. Peyser, R. Prabhavalkar, G. Pundak, D. Rybach, Y. Shangguan,\nY. Sheth, T. Strohman, M. Visontai, Y. Wu, Y. Zhang, and D. Zhao,\n\u201cA Streaming On-Device End-To-End Model Surpassing Server-Side\nConventional Model Quality and Latency,\u201d in Proc. IEEE ICASSP,\nBarcelona, Spain, may 2020, pp. 6059\u20136063.\n[333] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan,\nS.-Y. Chang, R. Pang, Y. He, J. Qin, W. Han, Q. Liang, Y. Zhang,\nT. Strohman, and Y. Wu, \u201cA Better and Faster End-to-End Model for\nStreaming ASR,\u201d in Proc. IEEE ICASSP, Toronto, Ontario, Canada,\nJun. 2021, pp. 5634\u20135638.\n[334] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach,\nC. Allauzen, E. Variani, J. Qin, Q.-N. Le-The, S.-Y. Chang, B. Li,\nA. Gulati, J. Yu, C.-C. Chiu, D. Caseiro, W. Li, Q. Liang, and\nP. Rondon, \u201cAn Efficient Streaming Non-Recurrent On-Device End\u0002to-End Model with Improvements to Rare-Word Modeling,\u201d in Proc.\nInterspeech, Brno, Czechia, Sep. 2021, pp. 1777\u20131781.\n[335] A. Bapna, Y.-A. Chung, N. Wu, , A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSLAM: A Unified\nEncoder for Speech and Language Modeling via Speech-Text Joint\nPre-Training,\u201d Oct. 2021, arXiv:2110.10329.\n[336] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng,\nS. Khanuja, J. Riesa, and A. Conneau, \u201cmSLAM: Massively Mul\u0002tilingual Joint Pre-Training for Speech and Text,\u201d Feb. 2022,\narXiv:2202.01374.\n[337] Y. Tang, H. Gong, N. Dong, C. Wag, W. Hsu, J. Gu, A. Baevski, X. Li,\nA. Mohamed, M. Auli, and J. Pino, \u201cUnified Speech-Text Pre-training\nfor Speech Translation and Recognition,\u201d in Proc. ACL, Dublin, Ireland,\nMay 2022, pp. 1488\u20131499, arXiv:2204.05409.\n[338] Y.-A. Chung, C. Zhu, and M. Zeng, \u201cSPLAT: Speech-Language Joint\nPre-Training for Spoken Language Understanding,\u201d in Proc. NAACL,\nJun. 2021, pp. 1897\u20131907, arXiv:2010.02295.\n[339] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko,\nQ. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, \u201cSpeechT5:\nUnified-Modal Encoder-Decoder Pre-Training for Spoken Language\nProcessing,\u201d in Proc. ACL, Dublin, Ireland, May 2022, pp. 5723\u20135738,\narXiv:2110.07205.\n[340] S. Thomas, H. J. Kuo, B. Kingsbury, and G. Saon, \u201cTowards Reducing\nthe Need for Speech Training Data to Build Spoken Language Under\u0002standing Systems,\u201d in Proc. IEEE ICASSP, Singapore, May 2022, pp.\n7932\u20137936, arXiv:2203.00006.\n[341] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen,\nB. Li, W. Wang, and T. Strohman, \u201cJOIST: A joint speech and text\nstreaming model for ASR,\u201d in Proc. IEEE SLT, Doha, Qatar, Jan. 2023,\narXiv:2210.07353.\n[342] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and\nJ. Le Roux, \u201cCycle-Consistency Training for End-to-End Speech\nRecognition,\u201d in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp.\n6271\u20136275.\n[343] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, \u201cMultichannel\nEnd-to-End Speech Recognition,\u201d in Proc. ICML. Sydney, Australia:\nPMLR, Aug. 2017, pp. 2632\u20132641.\n[344] J. Li, \u201cRecent Advances in End-to-End Automatic Speech Recogni\u0002tion,\u201d APSIPA Trans. on Signal and Information Processing, vol. 11,\nno. 1, Nov. 2021, DOI: 10.1561/116.00000050, arXiv:2111.01690.\nPLACE\nPHOTO\nHERE\nRohit Prabhavalkar Rohit Prabhavalkar received\nhis PhD in Computer Science and Engineering from\nThe Ohio State University, USA, in 2013. Follow\u0002ing his PhD, Rohit joined the Speech Technologies\ngroup at Google where he is currently a Staff Re\u0002search Scientist. At Google, his research has focused\nprimarily on developing compact acoustic models\nwhich can run efficiently on mobile devices, and on\ndeveloping improved end-to-end automatic speech\nrecognition systems. Rohit has co-authored over 50\nrefereed papers, which have received two best paper\nawards (ASRU 2017; ICASSP 2018). He currently serves as a member of the\nIEEE Speech and Language Processing Technical Committee (2018\u20132024),\nand as an Associate Editor of the IEEE/ACM Transactions on Audio, Speech,\nand Language Processing.\nPLACE\nPHOTO\nHERE\nTakaaki Hori received his PhD degree in system\nand information engineering from Yamagata Uni\u0002versity, Yonezawa, Japan, in 1999. From 1999 to\n2015, he had been engaged in researches on speech\nrecognition and spoken language processing at Cy\u0002ber Space Laboratories and Communication Science\nLaboratories in Nippon Telegraph and Telephone\n(NTT) Corporation, Japan. From 2015 to 2021,\nhe was a Senior Principal Research Scientist at\nMitsubishi Electric Research Laboratories (MERL),\nUSA. He is currently a Machine Learning Re\u0002searcher at Apple. His research interests include automatic speech recognition,\nspoken language understanding, and language modeling. He served as a\nmember of the IEEE Speech and Language Processing Technical Committee\n(2020\u20132022).\nPLACE\nPHOTO\nHERE\nTara Sainath received her PhD in Electrical Engi\u0002neering and Computer Science from MIT in 2009.\nThe main focus of her PhD work was in acoustic\nmodeling for noise robust speech recognition. After\nher PhD, she spent 5 years at the Speech and\nLanguage Algorithms group at IBM T.J. Watson Re\u0002search Center, before joining Google Research. She\nhas served as a Program Chair for ICLR in 2017 and\n2018. Also, she has co-organized numerous special\nsessions and workshops, including Interspeech 2010,\nICML 2013, Interspeech 2016 and ICML 2017. In\naddition, she is a member of the IEEE Speech and Language Processing\nTechnical Committee (SLTC) as well as the Associate Editor for IEEE/ACM\nTransactions on Audio, Speech, and Language Processing.\nPLACE\nPHOTO\nHERE\nRalf Schluter \u00a8 Ralf Schluter received his Dr.rer.nat. \u00a8\ndegree in Computer Science in 2000 and habilitated\nin Computer Science in 2019, both at RWTH Aachen\nUniversity. In May 1996, Ralf Schluter joined the \u00a8\nComputer Science Department at RWTH Aachen\nUniversity, where he currently is Lecturer and\nAcademic Director, leading the Automatic Speech\nRecognition Group at the Chair Computer Science\n6 \u2013 Machine Learning and Human Language Tech\u0002nology. In 2019, Ralf also joined AppTek GmbH\nAachen as Senior Researcher. His research interests\ncover sequence classification, specifically all aspects of automatic speech\nrecognition, decision theory, stochastic modeling, and signal analysis. Ralf\nserved as Subject Editor for Speech Communication (2013-2019).\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n29\nPLACE\nPHOTO\nHERE\nShinji Watanabe is an Associate Professor at\nCarnegie Mellon University, Pittsburgh, PA. He re\u0002ceived his B.S., M.S., and Ph.D. (Dr. Eng.) degrees\nfrom Waseda University, Tokyo, Japan. He was a\nresearch scientist at NTT Communication Science\nLaboratories, Kyoto, Japan, from 2001 to 2011, a\nvisiting scholar at Georgia institute of technology,\nAtlanta, GA, in 2009, and a senior principal research\nscientist at Mitsubishi Electric Research Laborato\u0002ries (MERL), Cambridge, MA USA from 2012 to\n2017. Before Carnegie Mellon University, he was\nan associate research professor at Johns Hopkins University, Baltimore,\nMD, USA, from 2017 to 2020. His research interests include automatic\nspeech recognition, speech enhancement, spoken language understanding, and\nmachine learning for speech and language processing. He is an IEEE and\nISCA Fellow.\nThis article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and\ncontent may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
    "openalex_id": "https://openalex.org/W4388017359",
    "title": "End-to-End Speech Recognition: A Survey",
    "publication_date": "2024-01-01",
    "cited_by_count": 33,
    "topics": "Speech Recognition Technology, Statistical Machine Translation and Natural Language Processing, Audio Signal Classification and Analysis",
    "keywords": "End-to-End Speech Recognition, Automatic Speech Recognition, Acoustic Modeling, Environmental Sound Recognition, End-to-end principle, Deep Learning, Word error rate, Deep neural networks",
    "concepts": "Computer science, Hidden Markov model, Deep learning, Artificial neural network, Language model, Software deployment, Artificial intelligence, End-to-end principle, Speech recognition, Word error rate, Domain (mathematical analysis), Deep neural networks, Natural language processing, Machine learning, Mathematical analysis, Mathematics, Operating system",
    "pdf_urls_by_priority": [
      "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
      "https://arxiv.org/pdf/2303.03329"
    ],
    "text_type": "full_text",
    "successful_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "referenced_works": [
      "https://openalex.org/W108866686",
      "https://openalex.org/W1494198834",
      "https://openalex.org/W1501286448",
      "https://openalex.org/W1508165687",
      "https://openalex.org/W1553004968",
      "https://openalex.org/W1583239513",
      "https://openalex.org/W1587755118",
      "https://openalex.org/W1588735863",
      "https://openalex.org/W1710082047",
      "https://openalex.org/W179875071",
      "https://openalex.org/W1806891645",
      "https://openalex.org/W1904365287",
      "https://openalex.org/W1915251500",
      "https://openalex.org/W1922655562",
      "https://openalex.org/W1966812932",
      "https://openalex.org/W1975550806",
      "https://openalex.org/W1979136262",
      "https://openalex.org/W1985258458",
      "https://openalex.org/W1986184096",
      "https://openalex.org/W1988720110",
      "https://openalex.org/W1989674786",
      "https://openalex.org/W1991133427",
      "https://openalex.org/W2000200144",
      "https://openalex.org/W2001679125",
      "https://openalex.org/W2008554732",
      "https://openalex.org/W2014151772",
      "https://openalex.org/W2024539680",
      "https://openalex.org/W2033245860",
      "https://openalex.org/W2033565080",
      "https://openalex.org/W2046932483",
      "https://openalex.org/W2050526637",
      "https://openalex.org/W2056590938",
      "https://openalex.org/W2057653135",
      "https://openalex.org/W2064675550",
      "https://openalex.org/W206545267",
      "https://openalex.org/W2066378046",
      "https://openalex.org/W2078354939",
      "https://openalex.org/W2080213370",
      "https://openalex.org/W2091981305",
      "https://openalex.org/W2097927681",
      "https://openalex.org/W2100180150",
      "https://openalex.org/W2105482032",
      "https://openalex.org/W2105594594",
      "https://openalex.org/W2110798204",
      "https://openalex.org/W2114016253",
      "https://openalex.org/W2121879602",
      "https://openalex.org/W2125838338",
      "https://openalex.org/W2127095586",
      "https://openalex.org/W2127141656",
      "https://openalex.org/W2129545859",
      "https://openalex.org/W2131968858",
      "https://openalex.org/W2136617108",
      "https://openalex.org/W2136922672",
      "https://openalex.org/W2143564602",
      "https://openalex.org/W2143612262",
      "https://openalex.org/W2145249131",
      "https://openalex.org/W2150355110",
      "https://openalex.org/W2151058131",
      "https://openalex.org/W2151834591",
      "https://openalex.org/W2155368638",
      "https://openalex.org/W2157749010",
      "https://openalex.org/W2165712214",
      "https://openalex.org/W2166637769",
      "https://openalex.org/W2183341477",
      "https://openalex.org/W2242818861",
      "https://openalex.org/W2288217446",
      "https://openalex.org/W2291975472",
      "https://openalex.org/W2296073425",
      "https://openalex.org/W2327501763",
      "https://openalex.org/W2331143823",
      "https://openalex.org/W2394932179",
      "https://openalex.org/W2396464458",
      "https://openalex.org/W2402268235",
      "https://openalex.org/W2407080277",
      "https://openalex.org/W2408093180",
      "https://openalex.org/W2411921399",
      "https://openalex.org/W2471933213",
      "https://openalex.org/W2514741789",
      "https://openalex.org/W2516457973",
      "https://openalex.org/W2520160253",
      "https://openalex.org/W2525778437",
      "https://openalex.org/W2530876040",
      "https://openalex.org/W2545177271",
      "https://openalex.org/W2566563465",
      "https://openalex.org/W2577366047",
      "https://openalex.org/W2606722458",
      "https://openalex.org/W2608712415",
      "https://openalex.org/W2618530766",
      "https://openalex.org/W2627092829",
      "https://openalex.org/W2745439869",
      "https://openalex.org/W2746192915",
      "https://openalex.org/W2748816379",
      "https://openalex.org/W2750499125",
      "https://openalex.org/W2766219058",
      "https://openalex.org/W2787663903",
      "https://openalex.org/W2792376130",
      "https://openalex.org/W2799800213",
      "https://openalex.org/W2808640845",
      "https://openalex.org/W2808939837",
      "https://openalex.org/W2883586237",
      "https://openalex.org/W2886025712",
      "https://openalex.org/W2886180730",
      "https://openalex.org/W2886319145",
      "https://openalex.org/W2888779557",
      "https://openalex.org/W2888909726",
      "https://openalex.org/W2889129739",
      "https://openalex.org/W2889163603",
      "https://openalex.org/W2889187401",
      "https://openalex.org/W2889374926",
      "https://openalex.org/W2889504751",
      "https://openalex.org/W2892009249",
      "https://openalex.org/W2892124901",
      "https://openalex.org/W2899879954",
      "https://openalex.org/W2900209846",
      "https://openalex.org/W2904818793",
      "https://openalex.org/W2914018192",
      "https://openalex.org/W2915977493",
      "https://openalex.org/W2928941594",
      "https://openalex.org/W2933138175",
      "https://openalex.org/W2936123380",
      "https://openalex.org/W2936774411",
      "https://openalex.org/W2937402758",
      "https://openalex.org/W2937780860",
      "https://openalex.org/W2938348542",
      "https://openalex.org/W2939111082",
      "https://openalex.org/W2940180244",
      "https://openalex.org/W2943845043",
      "https://openalex.org/W2949975180",
      "https://openalex.org/W2951974815",
      "https://openalex.org/W2952992734",
      "https://openalex.org/W2953561564",
      "https://openalex.org/W2962699523",
      "https://openalex.org/W2962728618",
      "https://openalex.org/W2962742956",
      "https://openalex.org/W2962745521",
      "https://openalex.org/W2962760690",
      "https://openalex.org/W2962784628",
      "https://openalex.org/W2962824709",
      "https://openalex.org/W2962826786",
      "https://openalex.org/W2963022149",
      "https://openalex.org/W2963026768",
      "https://openalex.org/W2963088785",
      "https://openalex.org/W2963144852",
      "https://openalex.org/W2963211739",
      "https://openalex.org/W2963240019",
      "https://openalex.org/W2963260202",
      "https://openalex.org/W2963303028",
      "https://openalex.org/W2963382396",
      "https://openalex.org/W2963431393",
      "https://openalex.org/W2963506925",
      "https://openalex.org/W2963571336",
      "https://openalex.org/W2963739817",
      "https://openalex.org/W2963747784",
      "https://openalex.org/W2964012862",
      "https://openalex.org/W2964103964",
      "https://openalex.org/W2964107261",
      "https://openalex.org/W2964110616",
      "https://openalex.org/W2970692082",
      "https://openalex.org/W2971840980",
      "https://openalex.org/W2972451902",
      "https://openalex.org/W2972528057",
      "https://openalex.org/W2972621414",
      "https://openalex.org/W2972625221",
      "https://openalex.org/W2972630480",
      "https://openalex.org/W2972692349",
      "https://openalex.org/W2972780808",
      "https://openalex.org/W2972799770",
      "https://openalex.org/W2972837679",
      "https://openalex.org/W2972889948",
      "https://openalex.org/W2972953886",
      "https://openalex.org/W2972977747",
      "https://openalex.org/W2972995428",
      "https://openalex.org/W2973122799",
      "https://openalex.org/W2981857663",
      "https://openalex.org/W2987019345",
      "https://openalex.org/W2995181338",
      "https://openalex.org/W2997617958",
      "https://openalex.org/W3005302685",
      "https://openalex.org/W3007328579",
      "https://openalex.org/W3007528493",
      "https://openalex.org/W3008037978",
      "https://openalex.org/W3008174054",
      "https://openalex.org/W3008191852",
      "https://openalex.org/W3008284571",
      "https://openalex.org/W3008525923",
      "https://openalex.org/W3008762051",
      "https://openalex.org/W3008898571",
      "https://openalex.org/W3008912312",
      "https://openalex.org/W3011339933",
      "https://openalex.org/W3015190365",
      "https://openalex.org/W3015194534",
      "https://openalex.org/W3015369343",
      "https://openalex.org/W3015383801",
      "https://openalex.org/W3015501067",
      "https://openalex.org/W3015671919",
      "https://openalex.org/W3015686596",
      "https://openalex.org/W3015726069",
      "https://openalex.org/W3015927303",
      "https://openalex.org/W3015974384",
      "https://openalex.org/W3015995734",
      "https://openalex.org/W3016010032",
      "https://openalex.org/W3016053754",
      "https://openalex.org/W3016167541",
      "https://openalex.org/W3016234571",
      "https://openalex.org/W3017474798",
      "https://openalex.org/W3026041220",
      "https://openalex.org/W3028545098",
      "https://openalex.org/W3034775979",
      "https://openalex.org/W3092122846",
      "https://openalex.org/W3094667432",
      "https://openalex.org/W3094713728",
      "https://openalex.org/W3094957294",
      "https://openalex.org/W3095173472",
      "https://openalex.org/W3095189764",
      "https://openalex.org/W3095376166",
      "https://openalex.org/W3095697114",
      "https://openalex.org/W3096032230",
      "https://openalex.org/W3096160024",
      "https://openalex.org/W3096215352",
      "https://openalex.org/W3097747488",
      "https://openalex.org/W3097777922",
      "https://openalex.org/W3097882114",
      "https://openalex.org/W3097973766",
      "https://openalex.org/W3100910367",
      "https://openalex.org/W3103005696",
      "https://openalex.org/W3105532142",
      "https://openalex.org/W3147187328",
      "https://openalex.org/W3147414526",
      "https://openalex.org/W3148001440",
      "https://openalex.org/W3148654612",
      "https://openalex.org/W3151269043",
      "https://openalex.org/W3152221657",
      "https://openalex.org/W3160551958",
      "https://openalex.org/W3160766462",
      "https://openalex.org/W3161375121",
      "https://openalex.org/W3161873870",
      "https://openalex.org/W3162249256",
      "https://openalex.org/W3162665866",
      "https://openalex.org/W3163203022",
      "https://openalex.org/W3163300396",
      "https://openalex.org/W3163793923",
      "https://openalex.org/W3163839574",
      "https://openalex.org/W3163842339",
      "https://openalex.org/W3167895882",
      "https://openalex.org/W3170405627",
      "https://openalex.org/W3197140813",
      "https://openalex.org/W3197304116",
      "https://openalex.org/W3197478142",
      "https://openalex.org/W3197507772",
      "https://openalex.org/W3197976839",
      "https://openalex.org/W3197991202",
      "https://openalex.org/W3198116002",
      "https://openalex.org/W3198439131",
      "https://openalex.org/W3198442913",
      "https://openalex.org/W3198455051",
      "https://openalex.org/W3198654230",
      "https://openalex.org/W3202184514",
      "https://openalex.org/W3202419788",
      "https://openalex.org/W3204696009",
      "https://openalex.org/W3205201903",
      "https://openalex.org/W3205644108",
      "https://openalex.org/W3206573929",
      "https://openalex.org/W3206876927",
      "https://openalex.org/W3207222250",
      "https://openalex.org/W3209059054",
      "https://openalex.org/W3211040052",
      "https://openalex.org/W3211278025",
      "https://openalex.org/W4206410067",
      "https://openalex.org/W4210463634",
      "https://openalex.org/W4221155340",
      "https://openalex.org/W4223622550",
      "https://openalex.org/W4224518768",
      "https://openalex.org/W4225319488",
      "https://openalex.org/W4225334634",
      "https://openalex.org/W4226120743",
      "https://openalex.org/W4240908132",
      "https://openalex.org/W4288290348",
      "https://openalex.org/W4297781872",
      "https://openalex.org/W4299649720",
      "https://openalex.org/W4319862255",
      "https://openalex.org/W4319862408",
      "https://openalex.org/W4319862418",
      "https://openalex.org/W4319862474",
      "https://openalex.org/W4319862683",
      "https://openalex.org/W4372259859",
      "https://openalex.org/W4378501656",
      "https://openalex.org/W4381827575",
      "https://openalex.org/W4383605108",
      "https://openalex.org/W4385245566",
      "https://openalex.org/W4394662461",
      "https://openalex.org/W4473315",
      "https://openalex.org/W66978610",
      "https://openalex.org/W98857008"
    ],
    "openalex_rank": 5,
    "num_tokens": 51849,
    "url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "best_oa_location_pdf_url": "https://ieeexplore.ieee.org/ielx7/6570655/6633080/10301513.pdf",
    "question_generated": "What are the key advancements in end-to-end speech recognition systems, what challenges do researchers face in optimizing these models, and what future directions could enhance their performance and applicability?",
    "pipeline_answer": "**Key Advancements in End-to-End Speech Recognition Systems:**  \n- **Self-Supervised Learning:** Recent advancements include the successful application of self-supervised learning techniques, which utilize large amounts of unlabeled data to enhance model performance [S3403243270].  \n- **RNN-T Model:** The RNN-Transducer (RNN-T) model has become a competitive alternative to traditional hybrid models, particularly in streaming applications, demonstrating that end-to-end architectures can achieve comparable accuracy with appropriate initialization and training techniques [S3842804358].  \n- **Transformer Architectures:** Transformer-based models have been shown to outperform recurrent neural network (RNN)-based models in both streaming and non-streaming modes, indicating a trend towards more complex model structures in end-to-end speech recognition [S0310792189].  \n\n**Challenges in Optimizing Models:**  \n- **Data Requirements:** A significant challenge in optimizing end-to-end speech recognition models is the need for extensive labeled datasets, which can be difficult to obtain [S9867329058].  \n- **Robustness Issues:** Achieving robustness across diverse acoustic environments remains a substantial barrier to the widespread adoption of these systems [S9867329058].  \n\n**Future Directions for Improvement:**  \n- **Multi-Task Learning:** Future enhancements could involve exploring multi-task learning approaches, which may improve the model's ability to generalize across different tasks [S8629680064].  \n- **Data Augmentation Techniques:** The integration of advanced data augmentation techniques is also suggested to enhance model generalization and performance [S8629680064].  \n\nIn summary, while significant advancements have been made in end-to-end speech recognition systems, challenges related to data requirements and robustness must be addressed. Future research directions focusing on multi-task learning and data augmentation hold promise for further enhancing these models.",
    "pipeline_references": {
      "S3403243270": {
        "id": "S3403243270",
        "text": "Recent advancements in end-to-end speech recognition systems include the successful application of self-supervised learning techniques, which leverage large amounts of unlabeled data to improve model performance.",
        "children": [
          {
            "id": "E3603107642",
            "text": "Advancements, Opportunities, and Impacts of Automatic Speech Recognition Technology in Various Domains    Photo by Andrew DesLauriers on Unsplash   TL;DR:  This post focuses on the advancements in Automatic Speech Recognition (ASR) technology and its impact on various domains. ASR has become prevalent in multiple industries, with improved accuracy driven by scaling model size and constructing larger labeled and unlabelled training datasets.   Looking ahead, ASR technology is expected to continue improving with the scaling of the acoustic model size and the enhancement of the internal language model. Additionally, self-supervised and multi-task training techniques will enable low-resource languages to benefit from ASR technology, while multilingual training will boost performance even further, allowing for basic usage such as voice commands in many low-resource languages.   ASR will also play a significant role in Generative AI, as interaction with avatars will be via an audio/text interface. With the emergence of textless NLP, some end-tasks, such as speech-2-speech translation, may be solved without using any explicit ASR model. Multimodal models that can be prompted using text, audio, or both will be released and generate text or synthesize audio as an output.   Furthermore, open-ended dialogue systems with voice-based human-machine interfaces will improve robustness to transcription errors and differences between written and spoken forms. This will provide robustness to challenging accents and children\u2019s speech, enabling ASR technology to become an essential tool for many applications.   An end-to-end speech enhancement-ASR-diarization system is set to be released, enabling the personalization of ASR models and improving performance on overlapped speech and challenging acoustic scenarios. This is a significant step towards solving ASR technology\u2019s challenges in real-world scenarios.   Lastly, A wave of speech APIs is expected. And still, there are opportunities for small startups to outperform big tech companies in domains with more legal or regulatory restrictions on the use of technology/data acquisition and in populations with low technology adoption rates.   2022 In A Review  Automatic Speech Recognition (ASR) technology is gaining momentum across various industries such as education, podcasts, social media, telemedicine, call centers, and more. A great example is the growing prevalence of voice-based human-machine interface (HMI) in consumer products, such as smart cars, smart homes, smart assistive technology [1], smartphones, and even artificial intelligence (AI) assistants in hotels [2]. In order to meet the increasing demand for fast and accurate responses, low-latency ASR models have been deployed for tasks like keyword spotting [3], endpointing [4], and transcription [5]. Speaker-attributed ASR models [6\u20137] are also gaining attention as they enable product personalization, providing greater value to end-users.  Prevalence of Data. Streaming audio and video platforms such as social media and YouTube have led to the easy acquisition of unlabeled audio data [8]. New self-supervised techniques have been introduced to utilize this audio without needing ground truth [9\u201310]. These techniques improve the performance of ASR systems in the target domain, even without fine-tuning on labeled data for that domain [11]. Another approach gaining attention due to its ability to utilize this unlabeled data is self-training using pseudo-labeling [12\u201313]. The main concept is to automatically transcribe unlabeled audio data using an automatic speech recognition (ASR) system and then use the generated transcription as ground truth for training a different ASR system in a supervised fashion. OpenAI took a different approach, assuming they can find human-generated transcripts at scale online. They generated a high-quality and large-scale (640K hours) training dataset by crawling publicly available audio data with human-generated subtitles. Using this dataset, they trained an ASR model (a.k.a Whisper) in a fully supervised manner, achieving state-of-the-art (SoTA) results on several benchmarks in zero-shot settings [14].  Losses. Despite End-2-end (E2E) losses dominating SoTA ASR models [15\u201317], new losses are still being published. A new technique called hybrid autoregressive transducer (HAT) [18] has been introduced, enabling to measure the quality of the internal language model (ILM) by separating the blank and label posteriors. Later work [19] used this factorization to effectively adapt the ILM using only textual data, which improved the overall performance of ASR systems, particularly the transcription of named entities, slang terms, and nouns, which are major pain points for ASR systems. New metrics have also been developed to better align with human perception and overcome word error rate (WER) semantic issues [20].  Architecture Choice. Regarding the acoustic model\u2019s architectural choices, Conformer [21] remained preferred for streaming models, while Transformers [22] is the default architecture for non-streaming models. As for the latter, encoder-only (wav2vec2 based [23\u201324]) and encoder-decoder (Whisper [14]) multi-lingual models were introduced and improved over the SoTA results across several benchmarks in zero-shot settings. These models outperform their streaming counterparts due to model size, training data size, and their larger context.  Multilingual AI Developments from Tech Giants. Google has announced its \u201c1,000 Languages Initiative\u201d to build an AI model that supports the 1,000 most spoken languages [25], while Meta AI has announced its long-term effort to build language and machine translation (MT) tools that include most of the world\u2019s languages [26].  Spoken Language Breakthrough. Multi-modal (speech/text) and multi-task pre-trained seq-2-seq (encoder-decoder) models such as SpeechT5 [27] were released, showing great success on a wide variety of spoken language processing tasks, including ASR, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. These advancements in ASR technology are expected to drive further innovation and impact a wide range of industries in the years to come.  A Look Ahead  Despite its challenges, the field of Automatic Speech Recognition (ASR) is expected to make significant advancements in various domains, ranging from acoustic and semantic modeling to conversational and generative AI, and even speaker-attributed ASR. This section provides detailed insights into these areas and shares my predictions for the future of ASR technology.  Photo by Nik on Unsplash    General Improvements:  The improvement of ASR systems is expected on both the acoustic and semantic parts. On the acoustic model side, larger model and training data sizes are anticipated to enhance the overall performance of ASR systems, similar to the progress observed in LLMs. Although scaling Transformer encoders, such as Wav2Vec or Conformer, poses a challenge, a breakthrough is expected to enable their scaling or see a shift towards encoder-decoder architectures as in Whisper. However, encoder-decoder architectures have drawbacks that need to be addressed, such as hallucinations. Optimizations such as faster-whisper [28] and NVIDIA-wav2vec2 [29] will reduce training and inference time, lowering the barrier to deploying large ASR models. On the semantic side, researchers will focus on improving ASR models by incorporating larger acoustic or textual contexts. Injecting large-scale unpaired text into the ILM during E2E training, as in JEIT [30], will also be explored. These efforts will help to overcome key challenges such as accurately transcribing named entities, slang terms, and nouns. Although Whisper and Google\u2019s universal speech model (USM) [31] have improved ASR system performances over several benchmarks, some benchmarks still need to be solved as the word error rate (WER) remains around 20% [32]. Using speech foundation models, adding more diverse training data, and applying multi-task learning will significantly improve performance in such scenarios, opening up new business opportunities. Moreover, new metrics and benchmarks are expected to emerge to better align new end-tasks and domains, such as non-lexical conversational sounds [33] in the medical domain and filler word detection and classification [34] in media editing and educational domains. Task-specific fine-tuned models may be developed for this purpose. Finally, with the growth of multi-modality, more models, training datasets, and new benchmarks for several tasks are also expected to be released [35\u201336]. As progress continues, a wave of speech APIs is expected, similar to natural language processing (NLP). Google\u2019s USM, OpenAI\u2019s Whisper, and Assembly\u2019s Conformer-1 [37] are some of the early examples. Although it sounds silly, force alignment is still challenging for many companies. An open-source code for that may help many achieve accurate alignment between audio segments and their corresponding transcript.  Low Resources Languages:  Advancements in self-supervised lear",
            "url": "https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?gi=6e8673898c2a",
            "openalex_id": ""
          },
          {
            "id": "E0960993452",
            "text": "Member, IEEERohit Prabhavalkar\nSenior Member, IEEETakaaki Hori\nFellow, IEEETara N Sainath\nSenior Member, IEEERalf Schl\u00fcter\nFellow, IEEEShinji Watanabe\nEnd-to-End Speech Recognition: A Survey\n1Index Terms-end-to-end, automatic speech recognition\nIn the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domainspecific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.\nI. INTRODUCTION\nThe classical 1 statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule [1], [2]. Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation. Within the classical approach, deep learning has been introduced to acoustic and language modeling. In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM [3], [4]) or augmented the acoustic feature set (nonlinear disciminant/tandem approach [5], [6]). In language modeling, deep learning replaced count-based approaches [7], [8], [9]. However, when introducing deep learning, the classical ASR architecture was not yet touched. Classical stateof-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t. recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl. sequence discriminative training, etc. The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g. integrating speech signal preprocessing and feature extraction into acoustic modeling [10], [11].\nMore consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [12], [13], [14], [15]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [16], [17], [13], [18] has been established. However, it lacks distinction in many ways. Therefore, first of all an attempt to defining the term end-to-end in the context of ASR is due in this survey.\nAccording to the Cambridge Dictionary, the adjective \"endto-end\" is defined by: \"including all the stages of a process\" [19]. This can be regarded from a number of perspectives: a) Joint Modeling: In terms of ASR, the E2E property might be understood as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which also may mean dropping the classical separation of ASR into an acoustic model and a language model. b) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes' decision rule, which exactly requires a single global decision integrating all available knowledge sources. c) Joint Training: In terms of model training, E2E suggests estimating all parameters of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate. d) Training Data: Joint training of an integrated model implies using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data [20], [21]. e) Training from Scratch: The E2E property can also be interpreted for the training process itself, by requiring training from scratch avoiding external knowledge like prior alignments or initial models pre-trained using different criteria and/or knowledge sources. Note that pre-training and finetuning strategies are also important in some scenarios, if the model has explicit modularity, including self-supervised learn-ing [22] or joint training of front-end and speech recognition models [23].\nf) Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [24]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. g) Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units would limit E2E recognition vocabularies to be based on whole word or character models. Whole word models [25], according to Zipf's law [26], would require unrealisticly high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [27], might be seen as secondary approaches outside the E2E objective.\nh) Generic vs. Informed Modeling: Finally, E2E may also be seen in terms of the genericity of the underlying modeling: are task-specific constraints learned from data completely, or does task-specific knowledge enter modeling the system architecture in the first place? For example, the monotonicity constraint in ASR may be learned completely from data like in attention-based E2E approaches [15], or it may directly be implemented, as in classical HMM structures.\nOverall, end-to-end ASR therefore can be defined as an integrated ASR model that enables joint training and recognition consistently minimizing expected word error rate, avoiding separately obtained knowledge sources.\nHowever, what are potential benefits of E2E approaches to ASR? The primary objective when developing ASR systems is to minimize the expected word error rate. However, secondary objectives are to reduce time and memory complexity of the resulting decoder, and -assuming a constrained development budget -genericity and ease of modeling.\nFirst of all, defining an ASR system E2E in terms of a single neural network structure supports modeling genericity and may allow for faster development cycles when building ASR systems for new languages/domains. ASR models defined by a single neural network structure may become more lean compared to classical modeling, and the decoding process becomes simpler as it does not need to integrate separate models. The resulting reduction in memory footprint and power consumption supports embedded ASR applications [28].\nFurthermore, joint training E2E may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may itself be erroneous. Avoiding these supports improved models trained directly from data, provided that sufficient amounts of taskspecific training data are available.\nWith the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research w.r.t. E2E ASR systems, covering all relevant aspects of E2E ASR and including a contrastive discussion of the different E2E and classical ASR architectures.\nThis survey of E2E speech recognition is structured as follows. Sec. II describes the historical evolution of E2E speech recognition, with specific focus on the input/output alignment and an overview of currently prominent basic E2E ASR models. Sec. III discusses improvements of the basic E2E models, incl. E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. IV provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. V. Sec. VI discusses the role and integ",
            "url": "https://arxiv.org/pdf/2303.03329.pdf",
            "openalex_id": ""
          }
        ]
      },
      "S9867329058": {
        "id": "S9867329058",
        "text": "Challenges in optimizing end-to-end speech recognition models include the need for extensive labeled datasets and the difficulties in achieving robustness across diverse acoustic environments, which remain significant barriers to widespread adoption.",
        "children": [
          {
            "id": "E0960993452",
            "text": "Member, IEEERohit Prabhavalkar\nSenior Member, IEEETakaaki Hori\nFellow, IEEETara N Sainath\nSenior Member, IEEERalf Schl\u00fcter\nFellow, IEEEShinji Watanabe\nEnd-to-End Speech Recognition: A Survey\n1Index Terms-end-to-end, automatic speech recognition\nIn the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domainspecific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.\nI. INTRODUCTION\nThe classical 1 statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule [1], [2]. Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation. Within the classical approach, deep learning has been introduced to acoustic and language modeling. In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM [3], [4]) or augmented the acoustic feature set (nonlinear disciminant/tandem approach [5], [6]). In language modeling, deep learning replaced count-based approaches [7], [8], [9]. However, when introducing deep learning, the classical ASR architecture was not yet touched. Classical stateof-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t. recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl. sequence discriminative training, etc. The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g. integrating speech signal preprocessing and feature extraction into acoustic modeling [10], [11].\nMore consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [12], [13], [14], [15]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [16], [17], [13], [18] has been established. However, it lacks distinction in many ways. Therefore, first of all an attempt to defining the term end-to-end in the context of ASR is due in this survey.\nAccording to the Cambridge Dictionary, the adjective \"endto-end\" is defined by: \"including all the stages of a process\" [19]. This can be regarded from a number of perspectives: a) Joint Modeling: In terms of ASR, the E2E property might be understood as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which also may mean dropping the classical separation of ASR into an acoustic model and a language model. b) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes' decision rule, which exactly requires a single global decision integrating all available knowledge sources. c) Joint Training: In terms of model training, E2E suggests estimating all parameters of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate. d) Training Data: Joint training of an integrated model implies using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data [20], [21]. e) Training from Scratch: The E2E property can also be interpreted for the training process itself, by requiring training from scratch avoiding external knowledge like prior alignments or initial models pre-trained using different criteria and/or knowledge sources. Note that pre-training and finetuning strategies are also important in some scenarios, if the model has explicit modularity, including self-supervised learn-ing [22] or joint training of front-end and speech recognition models [23].\nf) Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [24]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. g) Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units would limit E2E recognition vocabularies to be based on whole word or character models. Whole word models [25], according to Zipf's law [26], would require unrealisticly high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [27], might be seen as secondary approaches outside the E2E objective.\nh) Generic vs. Informed Modeling: Finally, E2E may also be seen in terms of the genericity of the underlying modeling: are task-specific constraints learned from data completely, or does task-specific knowledge enter modeling the system architecture in the first place? For example, the monotonicity constraint in ASR may be learned completely from data like in attention-based E2E approaches [15], or it may directly be implemented, as in classical HMM structures.\nOverall, end-to-end ASR therefore can be defined as an integrated ASR model that enables joint training and recognition consistently minimizing expected word error rate, avoiding separately obtained knowledge sources.\nHowever, what are potential benefits of E2E approaches to ASR? The primary objective when developing ASR systems is to minimize the expected word error rate. However, secondary objectives are to reduce time and memory complexity of the resulting decoder, and -assuming a constrained development budget -genericity and ease of modeling.\nFirst of all, defining an ASR system E2E in terms of a single neural network structure supports modeling genericity and may allow for faster development cycles when building ASR systems for new languages/domains. ASR models defined by a single neural network structure may become more lean compared to classical modeling, and the decoding process becomes simpler as it does not need to integrate separate models. The resulting reduction in memory footprint and power consumption supports embedded ASR applications [28].\nFurthermore, joint training E2E may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may itself be erroneous. Avoiding these supports improved models trained directly from data, provided that sufficient amounts of taskspecific training data are available.\nWith the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research w.r.t. E2E ASR systems, covering all relevant aspects of E2E ASR and including a contrastive discussion of the different E2E and classical ASR architectures.\nThis survey of E2E speech recognition is structured as follows. Sec. II describes the historical evolution of E2E speech recognition, with specific focus on the input/output alignment and an overview of currently prominent basic E2E ASR models. Sec. III discusses improvements of the basic E2E models, incl. E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. IV provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. V. Sec. VI discusses the role and integ",
            "url": "https://arxiv.org/pdf/2303.03329.pdf",
            "openalex_id": ""
          },
          {
            "id": "E2700463692",
            "text": "Electrical Engineering and Systems Science &gt; Audio and Speech Processing\n \n arXiv:2111.01690 (eess)\n[Submitted on 2 Nov 2021 (v1), last revised 2 Feb 2022 (this version, v2)]\n View PDF  \nAbstract:Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.\nSubmission history  From: Jinyu Li [view email]   [v1] \nTue, 2 Nov 2021 15:49:20 UTC (6,878 KB)\n[v2]\nWed, 2 Feb 2022 23:38:10 UTC (6,908 KB)\n \n \nCurrent browse context:  eess.AS\nexport BibTeX citation\n  Bookmark    \n  \n \n \nBibliographic and Citation Tools\n \nCode, Data and Media Associated with this Article\n \nDemos\n \nRecommenders and Search Tools\n \narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?  Learn more about arXivLabs .",
            "url": "https://arxiv.org/abs/2111.01690",
            "openalex_id": ""
          }
        ]
      },
      "S0310792189": {
        "id": "S0310792189",
        "text": "Transformer-based architectures have been shown to outperform RNN-based models in both streaming and non-streaming modes, indicating a shift towards more complex model structures in end-to-end speech recognition.",
        "children": [
          {
            "id": "E8060983784",
            "text": "Authors:\nJinyu Li, Microsoft Research Asia, jinyli@microsoft.com\nYu Wu, Microsoft Research Asia, wu.yu@microsoft.com\nYashesh Gaur, Microsoft Research Asia, yagaur@microsoft.com\nChengyi Wang, Microsoft Research Asia\nRui Zhao, Microsoft Research Asia, ruzhao@microsoft.com\nShujie Liu, Microsoft Research Asia, shujiliu@microsoft.com\nMicrosoft Speech, Microsoft Research Asia\nLanguage Group, Microsoft Research Asia\nAbstract\nRecently, there has been a strong push to transition from hybrid models to end-to-end (E2E) models for automatic speech recognition. Currently, there are three promising E2E methods: recurrent neural network transducer (RNN-T), RNN attentionbased encoder-decoder (AED), and Transformer-AED. In this study, we conduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models, in both non-streaming and streaming modes. We use 65 thousand hours of Microsoft anonymized training data to train these models. As E2E models are more data hungry, it is better to compare their effectiveness with large amount of training data. To the best of our knowledge, no such comprehensive study has been conducted yet. We show that although AED models are stronger than RNN-T in the non-streaming mode, RNN-T is very competitive in streaming mode if its encoder can be properly initialized. Among all three E2E models, transformer-AED achieved the best accuracy in both streaming and non-streaming mode. We show that both streaming RNN-T and transformer-AED models can obtain better accuracy than a highly-optimized hybrid model.\n1. Introduction\nRecently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling [1] to end-to-end (E2E) modeling [2, 3, 4, 5, 6, 7, 8, 9, 10] for automatic speech recognition (ASR). While hybrid models require disjoint optimization of separate constituent models such as acoustic and language model, E2E ASR systems directly translate an input speech sequence into an output token (subwords, or even words) sequence using a single network. Some widely used contemporary E2E approaches for sequence-to-sequence transduction are: (a) Connectionist Temporal Classification (CTC) [11, 12] , (b) recurrent neural network Transducer (RNN-T) [13] , and (c) Attention-based Encoder-Decoder (AED) [14, 15, 3] . Among these three approaches, CTC was the earliest and can map the input speech signal to target labels without requiring any external alignments. However, it also suffers from the conditional frameindependence assumption. RNN-T extends CTC modeling by changing the objective function and the model architecture to remove the frame-independence assumption. Because of its streaming nature, RNN-T has received a lot of attention for industrial applications and has also managed to replace traditional hybrid models for some cases [9, 16, 17, 18] .\nAED is a general family of models that was initially proposed for machine translation [19] but has shown success in other domains (including ASR [14, 15, 3] ) as well. These models are not streaming in nature by default but there are several studies towards that direction, such as monotonic chunkwise attention [20] and triggered attention [21] . The early AED models used RNNs as a building block for its the encoder and decoder modules. We refer to them as RNN-AED in this study. More recently, the transformer architecture with self attention [22] has also become prevalent and is being used as a fundamental building block for encoder and decoder modules [23, 24, 25] . We refer to such a model as Transformer-AED in this paper.\nGiven the fast evolving landscape of E2E technology, it is timely to compare the most popular and promising E2E technologies for ASR in the field, shaping the future research direction. This paper focuses on the comparison of current most promising E2E technologies, namely RNN-T, RNN-AED, and Transformer-AED, in both non-streaming and streaming modes. All models are trained with 65 thousand hours of Microsoft anonymized training data. As E2E models are data hungry, it is better to compare its power with such a large amount of training data. To our best knowledge, there is no such a detailed comparison. In a recent work [16] , the streaming RNN-T model was compared with the non-streaming RNN-AED. In [26] , streaming RNN-AED is compared with streaming RNN-T for long-form speech recognition. In [25] , RNN-AED and Transformer-AED are compared in a non-streaming mode, with training data up to 960 hours. As the industrial applications usually requires the ASR service in a streaming mode, we further put more efforts on how to develop these E2E models in a streaming mode. While it has been shown in [27] that combining RNN-T and RNN-AED in a two-pass decoding configuration can surpass an industry-grade state-of-the-art hybrid model, this study shows that a single streaming E2E model, either RNN-T or Transformer-AED, can also surpass a state-ofthe-art hybrid model [28, 29] .\nIn addition to performing a detailed comparison of these promising E2E models for the first time, other contributions of this paper are 1) We propose a multi-layer context modeling scheme to explore future context with significant gains; 2) The cross entropy (CE) initialization is shown to be much more effective than CTC initialization to boost RNN-T models; 3) For streaming Transformer-AED, we show chunk-based future context integration is more effective than the lookahead method; 4) We release our Transformer related code with reproducible results on Librispeech at [30] to facilitate future research.\n2. Popular End-to-End Models\nIn this section, we give a brief introduction of current popular E2E models: RNN-T, RNN-AED, and Transformer-AED. These models have an acoustic encoder that generates high level representation for speech and a decoder, which autoregressively generates output tokens in the linguistic domain. While the acoustic encoders can be same, the decoders of RNN-T and AED are different. In RNN-T, the generation of next label is only conditioned on the label outputs at previous steps while the decoder of AED conditions the next output on acoustics as well. More importantly, RNN-T works in a frame-synchronized way while AED works in a label-synchronized fashion.\n2.1. RNN transducer\nThe encoder network converts the acoustic feature x1:T into a high-level representation h enc 1:T . The decoder, called prediction network, produces a high-level representation h pre u by consuming previous non-blank target yu-1. Here u denotes output label index. The joint network is a feed-forward network that combines the encoder network output h enc t and the prediction network output h pre u to generate the joint matrix ht,u, which is used to calculate softmax output. Here t denotes time index.\nThe encoder and prediction networks are usually realized using RNN with LSTM [31] units. When the encoder is a unidirectional LSTM-RNN as Eq. ( 1), RNN-T works in streaming mode by default.\nh enc t = LST M (xt, h enc t-1 )However, when the underlying LSTM-RNN encoder is a bidirectional model as Eq. ( 2), it is a non-streaming E2E model.\nh enc t = [LST M (xt, h enc t-1 ), LST M (xt, h enc t+1 )]When implemented with LSTM-RNN, the prediction network formulation is\nh pre u = LST M (yu-1, h pre u-1 ).With the advantage of Transformer models, there is a recent work to replace the LSTM-RNN in the encoder with the Transformer model to construct Transformer transducer [32] and Conformer transducer [33] .\n2.2. Attention-based Encoder-Decoder\nWhile RNN-T has received more attention from the industry due to its streaming nature, the Attention-based Encoder-Decoder (AED) models attracts more research from academia because of its powerful attention structure. RNN-AED and Transformer-AED differ at the realization of encoder and decoder by using LSTM-RNN and Transformer, respectively.\n2.2.1. RNN-AED\nThe encoder of RNN-AED can have the same structure as RNN-T like Eq. ( 1) and Eq. ( 2). However, the attention-enhanced decoder operates differently as below:\nh dec u = LST M (cu, yu-1, h dec u-1 ). ()\nhere cu is the context vector obtained by weighted combination of the encoder output. cu is supposed to contain the acoustic information necessary to emit the next token. It is calculated using the help of the attention mechanism [14, 34] .\n2.2.2. Transformer-AED\nEven though RNNs can capture long term dependencies, Transformer [22] based models can do it more effectively given the attention mechanism sees all context directly. Specifically, the encoder is composed of a stack of Transformer blocks, where each block has a multi-head self-attention layer and a feedforward layer. Suppose that the input of a Transformer block can be linearly transformed to Q, K, and V. Then, the output of a multi-head self-attention layer is\nMultihead(Q, K, V) = [H1 . . . H d head ]W headwhere\nH i = softmax( Q i K T i \u221a d k )V i , Q i = QW Q i , K i = KW K i , V i = VW V i .\nHere d head is the number of attention heads and d k is the dimension of the feature vector for each head. This output is fed to the feed-forward layer. Residual connections [35] and layer normalization (LN) [36] are indispensable when we connect different layers and blocks. In addition to the two layers in an encoder block, the Transformer decoder also has an additional third layer that performs multi-head attention over the output of the encoder. This is similar to the attention mechanism in RNN-AED.\n3.1. Model building block\nThe encoder and decoder of E2E models are constructed as the stack of multiple building blocks described in this section. For the models using LSTM-RNN, we explore two structures. The first one, LSTM cuDNN, directly calls Nvidia cuDNN library [37] for the LSTM implementation. We build every block by concatenating a cuDNN LSTM layer, a linear projection layer to reduce model size, and then followed by LN. Calling Nvidia cuDNN implementation enables us for fast ex",
            "url": "https://arxiv.org/pdf/2005.14327.pdf",
            "openalex_id": ""
          },
          {
            "id": "E3603107642",
            "text": "Advancements, Opportunities, and Impacts of Automatic Speech Recognition Technology in Various Domains    Photo by Andrew DesLauriers on Unsplash   TL;DR:  This post focuses on the advancements in Automatic Speech Recognition (ASR) technology and its impact on various domains. ASR has become prevalent in multiple industries, with improved accuracy driven by scaling model size and constructing larger labeled and unlabelled training datasets.   Looking ahead, ASR technology is expected to continue improving with the scaling of the acoustic model size and the enhancement of the internal language model. Additionally, self-supervised and multi-task training techniques will enable low-resource languages to benefit from ASR technology, while multilingual training will boost performance even further, allowing for basic usage such as voice commands in many low-resource languages.   ASR will also play a significant role in Generative AI, as interaction with avatars will be via an audio/text interface. With the emergence of textless NLP, some end-tasks, such as speech-2-speech translation, may be solved without using any explicit ASR model. Multimodal models that can be prompted using text, audio, or both will be released and generate text or synthesize audio as an output.   Furthermore, open-ended dialogue systems with voice-based human-machine interfaces will improve robustness to transcription errors and differences between written and spoken forms. This will provide robustness to challenging accents and children\u2019s speech, enabling ASR technology to become an essential tool for many applications.   An end-to-end speech enhancement-ASR-diarization system is set to be released, enabling the personalization of ASR models and improving performance on overlapped speech and challenging acoustic scenarios. This is a significant step towards solving ASR technology\u2019s challenges in real-world scenarios.   Lastly, A wave of speech APIs is expected. And still, there are opportunities for small startups to outperform big tech companies in domains with more legal or regulatory restrictions on the use of technology/data acquisition and in populations with low technology adoption rates.   2022 In A Review  Automatic Speech Recognition (ASR) technology is gaining momentum across various industries such as education, podcasts, social media, telemedicine, call centers, and more. A great example is the growing prevalence of voice-based human-machine interface (HMI) in consumer products, such as smart cars, smart homes, smart assistive technology [1], smartphones, and even artificial intelligence (AI) assistants in hotels [2]. In order to meet the increasing demand for fast and accurate responses, low-latency ASR models have been deployed for tasks like keyword spotting [3], endpointing [4], and transcription [5]. Speaker-attributed ASR models [6\u20137] are also gaining attention as they enable product personalization, providing greater value to end-users.  Prevalence of Data. Streaming audio and video platforms such as social media and YouTube have led to the easy acquisition of unlabeled audio data [8]. New self-supervised techniques have been introduced to utilize this audio without needing ground truth [9\u201310]. These techniques improve the performance of ASR systems in the target domain, even without fine-tuning on labeled data for that domain [11]. Another approach gaining attention due to its ability to utilize this unlabeled data is self-training using pseudo-labeling [12\u201313]. The main concept is to automatically transcribe unlabeled audio data using an automatic speech recognition (ASR) system and then use the generated transcription as ground truth for training a different ASR system in a supervised fashion. OpenAI took a different approach, assuming they can find human-generated transcripts at scale online. They generated a high-quality and large-scale (640K hours) training dataset by crawling publicly available audio data with human-generated subtitles. Using this dataset, they trained an ASR model (a.k.a Whisper) in a fully supervised manner, achieving state-of-the-art (SoTA) results on several benchmarks in zero-shot settings [14].  Losses. Despite End-2-end (E2E) losses dominating SoTA ASR models [15\u201317], new losses are still being published. A new technique called hybrid autoregressive transducer (HAT) [18] has been introduced, enabling to measure the quality of the internal language model (ILM) by separating the blank and label posteriors. Later work [19] used this factorization to effectively adapt the ILM using only textual data, which improved the overall performance of ASR systems, particularly the transcription of named entities, slang terms, and nouns, which are major pain points for ASR systems. New metrics have also been developed to better align with human perception and overcome word error rate (WER) semantic issues [20].  Architecture Choice. Regarding the acoustic model\u2019s architectural choices, Conformer [21] remained preferred for streaming models, while Transformers [22] is the default architecture for non-streaming models. As for the latter, encoder-only (wav2vec2 based [23\u201324]) and encoder-decoder (Whisper [14]) multi-lingual models were introduced and improved over the SoTA results across several benchmarks in zero-shot settings. These models outperform their streaming counterparts due to model size, training data size, and their larger context.  Multilingual AI Developments from Tech Giants. Google has announced its \u201c1,000 Languages Initiative\u201d to build an AI model that supports the 1,000 most spoken languages [25], while Meta AI has announced its long-term effort to build language and machine translation (MT) tools that include most of the world\u2019s languages [26].  Spoken Language Breakthrough. Multi-modal (speech/text) and multi-task pre-trained seq-2-seq (encoder-decoder) models such as SpeechT5 [27] were released, showing great success on a wide variety of spoken language processing tasks, including ASR, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. These advancements in ASR technology are expected to drive further innovation and impact a wide range of industries in the years to come.  A Look Ahead  Despite its challenges, the field of Automatic Speech Recognition (ASR) is expected to make significant advancements in various domains, ranging from acoustic and semantic modeling to conversational and generative AI, and even speaker-attributed ASR. This section provides detailed insights into these areas and shares my predictions for the future of ASR technology.  Photo by Nik on Unsplash    General Improvements:  The improvement of ASR systems is expected on both the acoustic and semantic parts. On the acoustic model side, larger model and training data sizes are anticipated to enhance the overall performance of ASR systems, similar to the progress observed in LLMs. Although scaling Transformer encoders, such as Wav2Vec or Conformer, poses a challenge, a breakthrough is expected to enable their scaling or see a shift towards encoder-decoder architectures as in Whisper. However, encoder-decoder architectures have drawbacks that need to be addressed, such as hallucinations. Optimizations such as faster-whisper [28] and NVIDIA-wav2vec2 [29] will reduce training and inference time, lowering the barrier to deploying large ASR models. On the semantic side, researchers will focus on improving ASR models by incorporating larger acoustic or textual contexts. Injecting large-scale unpaired text into the ILM during E2E training, as in JEIT [30], will also be explored. These efforts will help to overcome key challenges such as accurately transcribing named entities, slang terms, and nouns. Although Whisper and Google\u2019s universal speech model (USM) [31] have improved ASR system performances over several benchmarks, some benchmarks still need to be solved as the word error rate (WER) remains around 20% [32]. Using speech foundation models, adding more diverse training data, and applying multi-task learning will significantly improve performance in such scenarios, opening up new business opportunities. Moreover, new metrics and benchmarks are expected to emerge to better align new end-tasks and domains, such as non-lexical conversational sounds [33] in the medical domain and filler word detection and classification [34] in media editing and educational domains. Task-specific fine-tuned models may be developed for this purpose. Finally, with the growth of multi-modality, more models, training datasets, and new benchmarks for several tasks are also expected to be released [35\u201336]. As progress continues, a wave of speech APIs is expected, similar to natural language processing (NLP). Google\u2019s USM, OpenAI\u2019s Whisper, and Assembly\u2019s Conformer-1 [37] are some of the early examples. Although it sounds silly, force alignment is still challenging for many companies. An open-source code for that may help many achieve accurate alignment between audio segments and their corresponding transcript.  Low Resources Languages:  Advancements in self-supervised lear",
            "url": "https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?gi=6e8673898c2a",
            "openalex_id": ""
          }
        ]
      },
      "S3842804358": {
        "id": "S3842804358",
        "text": "The RNN-T model has emerged as a competitive alternative to traditional hybrid models, particularly in streaming applications, demonstrating that E2E architectures can achieve comparable accuracy with proper initialization and training techniques.",
        "children": [
          {
            "id": "E8060983784",
            "text": "Authors:\nJinyu Li, Microsoft Research Asia, jinyli@microsoft.com\nYu Wu, Microsoft Research Asia, wu.yu@microsoft.com\nYashesh Gaur, Microsoft Research Asia, yagaur@microsoft.com\nChengyi Wang, Microsoft Research Asia\nRui Zhao, Microsoft Research Asia, ruzhao@microsoft.com\nShujie Liu, Microsoft Research Asia, shujiliu@microsoft.com\nMicrosoft Speech, Microsoft Research Asia\nLanguage Group, Microsoft Research Asia\nAbstract\nRecently, there has been a strong push to transition from hybrid models to end-to-end (E2E) models for automatic speech recognition. Currently, there are three promising E2E methods: recurrent neural network transducer (RNN-T), RNN attentionbased encoder-decoder (AED), and Transformer-AED. In this study, we conduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models, in both non-streaming and streaming modes. We use 65 thousand hours of Microsoft anonymized training data to train these models. As E2E models are more data hungry, it is better to compare their effectiveness with large amount of training data. To the best of our knowledge, no such comprehensive study has been conducted yet. We show that although AED models are stronger than RNN-T in the non-streaming mode, RNN-T is very competitive in streaming mode if its encoder can be properly initialized. Among all three E2E models, transformer-AED achieved the best accuracy in both streaming and non-streaming mode. We show that both streaming RNN-T and transformer-AED models can obtain better accuracy than a highly-optimized hybrid model.\n1. Introduction\nRecently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling [1] to end-to-end (E2E) modeling [2, 3, 4, 5, 6, 7, 8, 9, 10] for automatic speech recognition (ASR). While hybrid models require disjoint optimization of separate constituent models such as acoustic and language model, E2E ASR systems directly translate an input speech sequence into an output token (subwords, or even words) sequence using a single network. Some widely used contemporary E2E approaches for sequence-to-sequence transduction are: (a) Connectionist Temporal Classification (CTC) [11, 12] , (b) recurrent neural network Transducer (RNN-T) [13] , and (c) Attention-based Encoder-Decoder (AED) [14, 15, 3] . Among these three approaches, CTC was the earliest and can map the input speech signal to target labels without requiring any external alignments. However, it also suffers from the conditional frameindependence assumption. RNN-T extends CTC modeling by changing the objective function and the model architecture to remove the frame-independence assumption. Because of its streaming nature, RNN-T has received a lot of attention for industrial applications and has also managed to replace traditional hybrid models for some cases [9, 16, 17, 18] .\nAED is a general family of models that was initially proposed for machine translation [19] but has shown success in other domains (including ASR [14, 15, 3] ) as well. These models are not streaming in nature by default but there are several studies towards that direction, such as monotonic chunkwise attention [20] and triggered attention [21] . The early AED models used RNNs as a building block for its the encoder and decoder modules. We refer to them as RNN-AED in this study. More recently, the transformer architecture with self attention [22] has also become prevalent and is being used as a fundamental building block for encoder and decoder modules [23, 24, 25] . We refer to such a model as Transformer-AED in this paper.\nGiven the fast evolving landscape of E2E technology, it is timely to compare the most popular and promising E2E technologies for ASR in the field, shaping the future research direction. This paper focuses on the comparison of current most promising E2E technologies, namely RNN-T, RNN-AED, and Transformer-AED, in both non-streaming and streaming modes. All models are trained with 65 thousand hours of Microsoft anonymized training data. As E2E models are data hungry, it is better to compare its power with such a large amount of training data. To our best knowledge, there is no such a detailed comparison. In a recent work [16] , the streaming RNN-T model was compared with the non-streaming RNN-AED. In [26] , streaming RNN-AED is compared with streaming RNN-T for long-form speech recognition. In [25] , RNN-AED and Transformer-AED are compared in a non-streaming mode, with training data up to 960 hours. As the industrial applications usually requires the ASR service in a streaming mode, we further put more efforts on how to develop these E2E models in a streaming mode. While it has been shown in [27] that combining RNN-T and RNN-AED in a two-pass decoding configuration can surpass an industry-grade state-of-the-art hybrid model, this study shows that a single streaming E2E model, either RNN-T or Transformer-AED, can also surpass a state-ofthe-art hybrid model [28, 29] .\nIn addition to performing a detailed comparison of these promising E2E models for the first time, other contributions of this paper are 1) We propose a multi-layer context modeling scheme to explore future context with significant gains; 2) The cross entropy (CE) initialization is shown to be much more effective than CTC initialization to boost RNN-T models; 3) For streaming Transformer-AED, we show chunk-based future context integration is more effective than the lookahead method; 4) We release our Transformer related code with reproducible results on Librispeech at [30] to facilitate future research.\n2. Popular End-to-End Models\nIn this section, we give a brief introduction of current popular E2E models: RNN-T, RNN-AED, and Transformer-AED. These models have an acoustic encoder that generates high level representation for speech and a decoder, which autoregressively generates output tokens in the linguistic domain. While the acoustic encoders can be same, the decoders of RNN-T and AED are different. In RNN-T, the generation of next label is only conditioned on the label outputs at previous steps while the decoder of AED conditions the next output on acoustics as well. More importantly, RNN-T works in a frame-synchronized way while AED works in a label-synchronized fashion.\n2.1. RNN transducer\nThe encoder network converts the acoustic feature x1:T into a high-level representation h enc 1:T . The decoder, called prediction network, produces a high-level representation h pre u by consuming previous non-blank target yu-1. Here u denotes output label index. The joint network is a feed-forward network that combines the encoder network output h enc t and the prediction network output h pre u to generate the joint matrix ht,u, which is used to calculate softmax output. Here t denotes time index.\nThe encoder and prediction networks are usually realized using RNN with LSTM [31] units. When the encoder is a unidirectional LSTM-RNN as Eq. ( 1), RNN-T works in streaming mode by default.\nh enc t = LST M (xt, h enc t-1 )However, when the underlying LSTM-RNN encoder is a bidirectional model as Eq. ( 2), it is a non-streaming E2E model.\nh enc t = [LST M (xt, h enc t-1 ), LST M (xt, h enc t+1 )]When implemented with LSTM-RNN, the prediction network formulation is\nh pre u = LST M (yu-1, h pre u-1 ).With the advantage of Transformer models, there is a recent work to replace the LSTM-RNN in the encoder with the Transformer model to construct Transformer transducer [32] and Conformer transducer [33] .\n2.2. Attention-based Encoder-Decoder\nWhile RNN-T has received more attention from the industry due to its streaming nature, the Attention-based Encoder-Decoder (AED) models attracts more research from academia because of its powerful attention structure. RNN-AED and Transformer-AED differ at the realization of encoder and decoder by using LSTM-RNN and Transformer, respectively.\n2.2.1. RNN-AED\nThe encoder of RNN-AED can have the same structure as RNN-T like Eq. ( 1) and Eq. ( 2). However, the attention-enhanced decoder operates differently as below:\nh dec u = LST M (cu, yu-1, h dec u-1 ). ()\nhere cu is the context vector obtained by weighted combination of the encoder output. cu is supposed to contain the acoustic information necessary to emit the next token. It is calculated using the help of the attention mechanism [14, 34] .\n2.2.2. Transformer-AED\nEven though RNNs can capture long term dependencies, Transformer [22] based models can do it more effectively given the attention mechanism sees all context directly. Specifically, the encoder is composed of a stack of Transformer blocks, where each block has a multi-head self-attention layer and a feedforward layer. Suppose that the input of a Transformer block can be linearly transformed to Q, K, and V. Then, the output of a multi-head self-attention layer is\nMultihead(Q, K, V) = [H1 . . . H d head ]W headwhere\nH i = softmax( Q i K T i \u221a d k )V i , Q i = QW Q i , K i = KW K i , V i = VW V i .\nHere d head is the number of attention heads and d k is the dimension of the feature vector for each head. This output is fed to the feed-forward layer. Residual connections [35] and layer normalization (LN) [36] are indispensable when we connect different layers and blocks. In addition to the two layers in an encoder block, the Transformer decoder also has an additional third layer that performs multi-head attention over the output of the encoder. This is similar to the attention mechanism in RNN-AED.\n3.1. Model building block\nThe encoder and decoder of E2E models are constructed as the stack of multiple building blocks described in this section. For the models using LSTM-RNN, we explore two structures. The first one, LSTM cuDNN, directly calls Nvidia cuDNN library [37] for the LSTM implementation. We build every block by concatenating a cuDNN LSTM layer, a linear projection layer to reduce model size, and then followed by LN. Calling Nvidia cuDNN implementation enables us for fast ex",
            "url": "https://arxiv.org/pdf/2005.14327.pdf",
            "openalex_id": ""
          },
          {
            "id": "E2700463692",
            "text": "Electrical Engineering and Systems Science &gt; Audio and Speech Processing\n \n arXiv:2111.01690 (eess)\n[Submitted on 2 Nov 2021 (v1), last revised 2 Feb 2022 (this version, v2)]\n View PDF  \nAbstract:Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry's perspective.\nSubmission history  From: Jinyu Li [view email]   [v1] \nTue, 2 Nov 2021 15:49:20 UTC (6,878 KB)\n[v2]\nWed, 2 Feb 2022 23:38:10 UTC (6,908 KB)\n \n \nCurrent browse context:  eess.AS\nexport BibTeX citation\n  Bookmark    \n  \n \n \nBibliographic and Citation Tools\n \nCode, Data and Media Associated with this Article\n \nDemos\n \nRecommenders and Search Tools\n \narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?  Learn more about arXivLabs .",
            "url": "https://arxiv.org/abs/2111.01690",
            "openalex_id": ""
          }
        ]
      },
      "S8629680064": {
        "id": "S8629680064",
        "text": "Future directions for enhancing the performance of end-to-end speech recognition systems include the exploration of multi-task learning and the integration of advanced data augmentation techniques to improve model generalization.",
        "children": [
          {
            "id": "E3603107642",
            "text": "Advancements, Opportunities, and Impacts of Automatic Speech Recognition Technology in Various Domains    Photo by Andrew DesLauriers on Unsplash   TL;DR:  This post focuses on the advancements in Automatic Speech Recognition (ASR) technology and its impact on various domains. ASR has become prevalent in multiple industries, with improved accuracy driven by scaling model size and constructing larger labeled and unlabelled training datasets.   Looking ahead, ASR technology is expected to continue improving with the scaling of the acoustic model size and the enhancement of the internal language model. Additionally, self-supervised and multi-task training techniques will enable low-resource languages to benefit from ASR technology, while multilingual training will boost performance even further, allowing for basic usage such as voice commands in many low-resource languages.   ASR will also play a significant role in Generative AI, as interaction with avatars will be via an audio/text interface. With the emergence of textless NLP, some end-tasks, such as speech-2-speech translation, may be solved without using any explicit ASR model. Multimodal models that can be prompted using text, audio, or both will be released and generate text or synthesize audio as an output.   Furthermore, open-ended dialogue systems with voice-based human-machine interfaces will improve robustness to transcription errors and differences between written and spoken forms. This will provide robustness to challenging accents and children\u2019s speech, enabling ASR technology to become an essential tool for many applications.   An end-to-end speech enhancement-ASR-diarization system is set to be released, enabling the personalization of ASR models and improving performance on overlapped speech and challenging acoustic scenarios. This is a significant step towards solving ASR technology\u2019s challenges in real-world scenarios.   Lastly, A wave of speech APIs is expected. And still, there are opportunities for small startups to outperform big tech companies in domains with more legal or regulatory restrictions on the use of technology/data acquisition and in populations with low technology adoption rates.   2022 In A Review  Automatic Speech Recognition (ASR) technology is gaining momentum across various industries such as education, podcasts, social media, telemedicine, call centers, and more. A great example is the growing prevalence of voice-based human-machine interface (HMI) in consumer products, such as smart cars, smart homes, smart assistive technology [1], smartphones, and even artificial intelligence (AI) assistants in hotels [2]. In order to meet the increasing demand for fast and accurate responses, low-latency ASR models have been deployed for tasks like keyword spotting [3], endpointing [4], and transcription [5]. Speaker-attributed ASR models [6\u20137] are also gaining attention as they enable product personalization, providing greater value to end-users.  Prevalence of Data. Streaming audio and video platforms such as social media and YouTube have led to the easy acquisition of unlabeled audio data [8]. New self-supervised techniques have been introduced to utilize this audio without needing ground truth [9\u201310]. These techniques improve the performance of ASR systems in the target domain, even without fine-tuning on labeled data for that domain [11]. Another approach gaining attention due to its ability to utilize this unlabeled data is self-training using pseudo-labeling [12\u201313]. The main concept is to automatically transcribe unlabeled audio data using an automatic speech recognition (ASR) system and then use the generated transcription as ground truth for training a different ASR system in a supervised fashion. OpenAI took a different approach, assuming they can find human-generated transcripts at scale online. They generated a high-quality and large-scale (640K hours) training dataset by crawling publicly available audio data with human-generated subtitles. Using this dataset, they trained an ASR model (a.k.a Whisper) in a fully supervised manner, achieving state-of-the-art (SoTA) results on several benchmarks in zero-shot settings [14].  Losses. Despite End-2-end (E2E) losses dominating SoTA ASR models [15\u201317], new losses are still being published. A new technique called hybrid autoregressive transducer (HAT) [18] has been introduced, enabling to measure the quality of the internal language model (ILM) by separating the blank and label posteriors. Later work [19] used this factorization to effectively adapt the ILM using only textual data, which improved the overall performance of ASR systems, particularly the transcription of named entities, slang terms, and nouns, which are major pain points for ASR systems. New metrics have also been developed to better align with human perception and overcome word error rate (WER) semantic issues [20].  Architecture Choice. Regarding the acoustic model\u2019s architectural choices, Conformer [21] remained preferred for streaming models, while Transformers [22] is the default architecture for non-streaming models. As for the latter, encoder-only (wav2vec2 based [23\u201324]) and encoder-decoder (Whisper [14]) multi-lingual models were introduced and improved over the SoTA results across several benchmarks in zero-shot settings. These models outperform their streaming counterparts due to model size, training data size, and their larger context.  Multilingual AI Developments from Tech Giants. Google has announced its \u201c1,000 Languages Initiative\u201d to build an AI model that supports the 1,000 most spoken languages [25], while Meta AI has announced its long-term effort to build language and machine translation (MT) tools that include most of the world\u2019s languages [26].  Spoken Language Breakthrough. Multi-modal (speech/text) and multi-task pre-trained seq-2-seq (encoder-decoder) models such as SpeechT5 [27] were released, showing great success on a wide variety of spoken language processing tasks, including ASR, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. These advancements in ASR technology are expected to drive further innovation and impact a wide range of industries in the years to come.  A Look Ahead  Despite its challenges, the field of Automatic Speech Recognition (ASR) is expected to make significant advancements in various domains, ranging from acoustic and semantic modeling to conversational and generative AI, and even speaker-attributed ASR. This section provides detailed insights into these areas and shares my predictions for the future of ASR technology.  Photo by Nik on Unsplash    General Improvements:  The improvement of ASR systems is expected on both the acoustic and semantic parts. On the acoustic model side, larger model and training data sizes are anticipated to enhance the overall performance of ASR systems, similar to the progress observed in LLMs. Although scaling Transformer encoders, such as Wav2Vec or Conformer, poses a challenge, a breakthrough is expected to enable their scaling or see a shift towards encoder-decoder architectures as in Whisper. However, encoder-decoder architectures have drawbacks that need to be addressed, such as hallucinations. Optimizations such as faster-whisper [28] and NVIDIA-wav2vec2 [29] will reduce training and inference time, lowering the barrier to deploying large ASR models. On the semantic side, researchers will focus on improving ASR models by incorporating larger acoustic or textual contexts. Injecting large-scale unpaired text into the ILM during E2E training, as in JEIT [30], will also be explored. These efforts will help to overcome key challenges such as accurately transcribing named entities, slang terms, and nouns. Although Whisper and Google\u2019s universal speech model (USM) [31] have improved ASR system performances over several benchmarks, some benchmarks still need to be solved as the word error rate (WER) remains around 20% [32]. Using speech foundation models, adding more diverse training data, and applying multi-task learning will significantly improve performance in such scenarios, opening up new business opportunities. Moreover, new metrics and benchmarks are expected to emerge to better align new end-tasks and domains, such as non-lexical conversational sounds [33] in the medical domain and filler word detection and classification [34] in media editing and educational domains. Task-specific fine-tuned models may be developed for this purpose. Finally, with the growth of multi-modality, more models, training datasets, and new benchmarks for several tasks are also expected to be released [35\u201336]. As progress continues, a wave of speech APIs is expected, similar to natural language processing (NLP). Google\u2019s USM, OpenAI\u2019s Whisper, and Assembly\u2019s Conformer-1 [37] are some of the early examples. Although it sounds silly, force alignment is still challenging for many companies. An open-source code for that may help many achieve accurate alignment between audio segments and their corresponding transcript.  Low Resources Languages:  Advancements in self-supervised lear",
            "url": "https://towardsdatascience.com/overcoming-automatic-speech-recognition-challenges-the-next-frontier-e26c31d643cc?gi=6e8673898c2a",
            "openalex_id": ""
          },
          {
            "id": "E0960993452",
            "text": "Member, IEEERohit Prabhavalkar\nSenior Member, IEEETakaaki Hori\nFellow, IEEETara N Sainath\nSenior Member, IEEERalf Schl\u00fcter\nFellow, IEEEShinji Watanabe\nEnd-to-End Speech Recognition: A Survey\n1Index Terms-end-to-end, automatic speech recognition\nIn the last decade of automatic speech recognition (ASR) research, the introduction of deep learning brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures were introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, while depending less on ASR domainspecific experience. The success and enthusiastic adoption of deep learning accompanied by more generic model architectures lead to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relation to the classical hidden Markov model (HMM) based ASR architecture. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, accompanied by discussions of performance and deployment opportunities, as well as an outlook into potential future developments.\nI. INTRODUCTION\nThe classical 1 statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule [1], [2]. Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation. Within the classical approach, deep learning has been introduced to acoustic and language modeling. In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM [3], [4]) or augmented the acoustic feature set (nonlinear disciminant/tandem approach [5], [6]). In language modeling, deep learning replaced count-based approaches [7], [8], [9]. However, when introducing deep learning, the classical ASR architecture was not yet touched. Classical stateof-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t. recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl. sequence discriminative training, etc. The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g. integrating speech signal preprocessing and feature extraction into acoustic modeling [10], [11].\nMore consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [12], [13], [14], [15]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [16], [17], [13], [18] has been established. However, it lacks distinction in many ways. Therefore, first of all an attempt to defining the term end-to-end in the context of ASR is due in this survey.\nAccording to the Cambridge Dictionary, the adjective \"endto-end\" is defined by: \"including all the stages of a process\" [19]. This can be regarded from a number of perspectives: a) Joint Modeling: In terms of ASR, the E2E property might be understood as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which also may mean dropping the classical separation of ASR into an acoustic model and a language model. b) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes' decision rule, which exactly requires a single global decision integrating all available knowledge sources. c) Joint Training: In terms of model training, E2E suggests estimating all parameters of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate. d) Training Data: Joint training of an integrated model implies using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data [20], [21]. e) Training from Scratch: The E2E property can also be interpreted for the training process itself, by requiring training from scratch avoiding external knowledge like prior alignments or initial models pre-trained using different criteria and/or knowledge sources. Note that pre-training and finetuning strategies are also important in some scenarios, if the model has explicit modularity, including self-supervised learn-ing [22] or joint training of front-end and speech recognition models [23].\nf) Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [24]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. g) Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units would limit E2E recognition vocabularies to be based on whole word or character models. Whole word models [25], according to Zipf's law [26], would require unrealisticly high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [27], might be seen as secondary approaches outside the E2E objective.\nh) Generic vs. Informed Modeling: Finally, E2E may also be seen in terms of the genericity of the underlying modeling: are task-specific constraints learned from data completely, or does task-specific knowledge enter modeling the system architecture in the first place? For example, the monotonicity constraint in ASR may be learned completely from data like in attention-based E2E approaches [15], or it may directly be implemented, as in classical HMM structures.\nOverall, end-to-end ASR therefore can be defined as an integrated ASR model that enables joint training and recognition consistently minimizing expected word error rate, avoiding separately obtained knowledge sources.\nHowever, what are potential benefits of E2E approaches to ASR? The primary objective when developing ASR systems is to minimize the expected word error rate. However, secondary objectives are to reduce time and memory complexity of the resulting decoder, and -assuming a constrained development budget -genericity and ease of modeling.\nFirst of all, defining an ASR system E2E in terms of a single neural network structure supports modeling genericity and may allow for faster development cycles when building ASR systems for new languages/domains. ASR models defined by a single neural network structure may become more lean compared to classical modeling, and the decoding process becomes simpler as it does not need to integrate separate models. The resulting reduction in memory footprint and power consumption supports embedded ASR applications [28].\nFurthermore, joint training E2E may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may itself be erroneous. Avoiding these supports improved models trained directly from data, provided that sufficient amounts of taskspecific training data are available.\nWith the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research w.r.t. E2E ASR systems, covering all relevant aspects of E2E ASR and including a contrastive discussion of the different E2E and classical ASR architectures.\nThis survey of E2E speech recognition is structured as follows. Sec. II describes the historical evolution of E2E speech recognition, with specific focus on the input/output alignment and an overview of currently prominent basic E2E ASR models. Sec. III discusses improvements of the basic E2E models, incl. E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. IV provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. V. Sec. VI discusses the role and integ",
            "url": "https://arxiv.org/pdf/2303.03329.pdf",
            "openalex_id": ""
          }
        ]
      }
    }
  }
]