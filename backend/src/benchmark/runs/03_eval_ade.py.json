[
  {
    "id": "E7830178856",
    "meta": {
      "id": "https://openalex.org/W4391109864",
      "title": "Segment anything in medical images",
      "publication_date": "2024-01-22",
      "cited_by_count": 206,
      "topics": "Radiomics in Medical Imaging Analysis, Deep Learning in Medical Image Analysis, Applications of Deep Learning in Medical Imaging",
      "keywords": "Medical Imaging, Medical Image Analysis, Modalities, Robustness (evolution), Modality (human\u2013computer interaction), Cancer Imaging, Whole Slide Imaging, Image-Based Diagnosis",
      "concepts": "Generalizability theory, Computer science, Segmentation, Modalities, Robustness (evolution), Artificial intelligence, Modality (human\u2013computer interaction), Image segmentation, Medical imaging, Personalization, Computer vision, Bridging (networking), Machine learning, Computer network, Mathematics, Social science, Biochemistry, Gene, Statistics, World Wide Web, Sociology, Chemistry",
      "best_oa_location_pdf_url": "https://www.nature.com/articles/s41467-024-44824-z.pdf",
      "pdf_urls_by_priority": [
        "https://www.nature.com/articles/s41467-024-44824-z.pdf"
      ],
      "text_type": "full_text",
      "openalex_rank": 7,
      "num_tokens": 10761
    },
    "text": "Article https://doi.org/10.1038/s41467-024-44824-z\nSegment anything in medical images\nJun Ma1,2,3, Yuting He4, Feifei Li 1, Lin Han5, Chenyu You 6 &\nBo Wang 1,2,3,7,8\nMedical image segmentation is a critical component in clinical practice, facil\u0002itating accurate diagnosis, treatment planning, and disease monitoring.\nHowever, existing methods, often tailored to specific modalities or disease\ntypes, lack generalizability across the diverse spectrum of medical image\nsegmentation tasks. Here we present MedSAM, a foundation model designed\nfor bridging this gap by enabling universal medical image segmentation. The\nmodel is developed on a large-scale medical image dataset with 1,570,263\nimage-mask pairs, covering 10 imaging modalities and over 30 cancer types.\nWe conduct a comprehensive evaluation on 86 internal validation tasks and 60\nexternal validation tasks, demonstrating better accuracy and robustness than\nmodality-wise specialist models. By delivering accurate and efficient seg\u0002mentation across a wide spectrum of tasks, MedSAM holds significant\npotential to expedite the evolution of diagnostic tools and the personalization\nof treatment plans.\nSegmentation is a fundamental task in medical imaging analysis, which\ninvolves identifying and delineating regions of interest (ROI) in various\nmedical images, such as organs, lesions, and tissues1\n. Accurate seg\u0002mentation is essential for many clinical applications, including disease\ndiagnosis, treatment planning, and monitoring of disease\nprogression2,3\n. Manual segmentation has long been the gold standard\nfor delineating anatomical structures and pathological regions, but\nthis process is time-consuming, labor-intensive, and often requires a\nhigh degree of expertise. Semi- or fully automatic segmentation\nmethods can significantly reduce the time and labor required, increase\nconsistency, and enable the analysis of large-scale datasets4\n.\nDeep learning-based models have shown great promise in medical\nimage segmentation due to their ability to learn intricate image fea\u0002tures and deliver accurate segmentation results across a diverse range\nof tasks, from segmenting specific anatomical structures to identifying\npathological regions5\n. However, a significant limitation of many cur\u0002rent medical image segmentation models is their task-specific nature.\nThese models are typically designed and trained for a specific seg\u0002mentation task, and their performance can degrade significantly when\napplied to new tasks or different types of imaging data6\n. This lack of\ngenerality poses a substantial obstacle to the wider application of\nthese models in clinical practice. In contrast, recent advances in the\nfield of natural image segmentation have witnessed the emergence of\nsegmentation foundation models, such as segment anything model\n(SAM)7 and Segment Everything Everywhere with Multi-modal\nprompts all at once8\n, showcasing remarkable versatility and perfor\u0002mance across various segmentation tasks.\nThere is a growing demand for universal models in medical image\nsegmentation: models that can be trained once and then applied to a\nwide range of segmentation tasks. Such models would not only exhibit\nheightened versatility in terms of model capacity but also potentially\nlead to more consistent results across different tasks. However, the\napplicability of the segmentation foundation models (e.g., SAM7\n) to\nmedical image segmentation remains limited due to the significant\ndifferences between natural images and medical images. Essentially,\nSAM is a promptable segmentation method that requires points or\nbounding boxes to specify the segmentation targets. This resembles\nconventional interactive segmentation methods4,9\u201311 but SAM has bet\u0002ter generalization ability, while existing deep learning-based inter\u0002active segmentation methods focus mainly on limited tasks and image\nmodalities.\nMany studies have applied the out-of-the-box SAM models to\ntypical medical image segmentation tasks12\u201317 and other challenging\nscenarios18\u201321. For example, the concurrent studies22,23 conducted a\nReceived: 24 October 2023\nAccepted: 5 January 2024\nCheck for updates\n1\nPeter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada. 2Department of Laboratory Medicine and Pathobiology, University of Toronto,\nToronto, ON, Canada. 3\nVector Institute, Toronto, ON, Canada. 4Department of Computer Science, Western University, London, ON, Canada. 5Tandon School\nof Engineering, New York University, New York, NY, USA. 6\nDepartment of Electrical Engineering, Yale University, New Haven, CT, USA. 7Department of\nComputer Science, University of Toronto, Toronto, ON, Canada. 8\nUHN AI Hub, Toronto, ON, Canada. e-mail: bowang@vectorinstitute.ai\nNature Communications | (2024) 15:654 1\n1234567890():,;1234567890():,;\ncomprehensive assessment of SAM across a diverse array of medical\nimages, underscoring that SAM achieved satisfactory segmentation\noutcomes primarily on targets characterized by distinct boundaries.\nHowever, the model exhibited substantial limitations in segmenting\ntypical medical targets with weak boundaries or low contrast. In con\u0002gruence with these observations, we further introduce MedSAM, a\nrefined foundation model that significantly enhances the segmenta\u0002tion performance of SAM on medical images. MedSAM accomplishes\nthis by fine-tuning SAM on an unprecedented dataset with more than\none million medical image-mask pairs.\nWe thoroughly evaluate MedSAM through comprehensive\nexperiments on 86 internal validation tasks and 60 external validation\ntasks, spanning a variety of anatomical structures, pathological con\u0002ditions, and medical imaging modalities. Experimental results\ndemonstrate that MedSAM consistently outperforms the state-of-the\u0002art (SOTA) segmentation foundation model7\n, while achieving perfor\u0002mance on par with, or even surpassing specialist models1,24 that were\ntrained on the images from the same modality. These results highlight\nthe potential of MedSAM as a new paradigm for versatile medical\nimage segmentation.\nResults\nMedSAM: a foundation model for promptable medical image\nsegmentation\nMedSAM aims to fulfill the role of a foundation model for universal\nmedical image segmentation. A crucial aspect of constructing such a\nmodel is the capacity to accommodate a wide range of variations in\nimaging conditions, anatomical structures, and pathological condi\u0002tions. To address this challenge, we curated a diverse and large-scale\nmedical image segmentation dataset with 1,570,263 medical image\u0002mask pairs, covering 10 imaging modalities, over 30 cancer types, and\na multitude of imaging protocols (Fig. 1 and Supplementary\nTables 1\u20134). This large-scale dataset allows MedSAM to learn a rich\nrepresentation of medical images, capturing a broad spectrum of\nanatomies and lesions across different modalities. Figure 2a provides\nan overview of the distribution of images across different medical\nimaging modalities in the dataset, ranked by their total numbers. It is\nevident that computed tomography (CT), magnetic resonance ima\u0002ging (MRI), and endoscopy are the dominant modalities, reflecting\ntheir ubiquity in clinical practice. CT and MRI images provide detailed\ncross-sectional views of 3D body structures, making them indis\u0002pensable for non-invasive diagnostic imaging. Endoscopy, albeit more\ninvasive, enables direct visual inspection of organ interiors, proving\ninvaluable for diagnosing gastrointestinal and urological conditions.\nDespite the prevalence of these modalities, others such as ultrasound,\npathology, fundus, dermoscopy, mammography, and optical coher\u0002ence tomography (OCT) also hold significant roles in clinical practice.\nThe diversity of these modalities and their corresponding segmenta\u0002tion targets underscores the necessity for universal and effective\nsegmentation models capable of handling the unique characteristics\nassociated with each modality.\nAnother critical consideration is the selection of the appropriate\nsegmentation prompt and network architecture. While the concept of\nfully automatic segmentation foundation models is enticing, it is\nfraught with challenges that make it impractical. One of the primary\nchallenges is the variability inherent in segmentation tasks. For\nexample, given a liver cancer CT image, the segmentation task can vary\ndepending on the specific clinical scenario. One clinician might be\nFig. 1 | MedSAM is trained on a large-scale dataset that can handle diverse segmentation tasks. The dataset covers a variety of anatomical structures, pathological\nconditions, and medical imaging modalities. The magenta contours and mask overlays denote the expert annotations and MedSAM segmentation results, respectively.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 2\ninterested in segmenting the liver tumor, while another might need to\nsegment the entire liver and surrounding organs. Additionally, the\nvariability in imaging modalities presents another challenge. Mod\u0002alities such as CT and MR generate 3D images, whereas others like\nX-ray and ultrasound yield 2D images. These variabilities in task defi\u0002nition and imaging modalities complicate the design of a fully auto\u0002matic model capable of accurately anticipating and addressing the\ndiverse requirements of different users.\nConsidering these challenges, we argue that a more practical\napproach is to develop a promptable 2D segmentation model. The\nmodel can be easily adapted to specific tasks based on user-provided\nprompts, offering enhanced flexibility and adaptability. It is also able\nto handle both 2D and 3D images by processing 3D images as a series\nof 2D slices. Typical user prompts include points and bounding boxes\nand we show some segmentation examples with the different prompts\nin Supplementary Fig. 1. It can be found that bounding boxes provide a\nmore unambiguous spatial context for the region of interest, enabling\nthe algorithm to more precisely discern the target area. This stands in\ncontrast to point-based prompts, which can introduce ambiguity,\nparticularly when proximate structures resemble each other. More\u0002over, drawing a bounding box is efficient, especially in scenarios\ninvolving multi-object segmentation. We follow the network archi\u0002tecture in SAM7\n, including an image encoder, a prompt encoder, and a\nmask decoder (Fig. 2b). The image encoder25 maps the input image\ninto a high-dimensional image embedding space. The prompt encoder\ntransforms the user-drawn bounding boxes into feature representa\u0002tions via positional encoding26. Finally, the mask decoder fuses the\nimage embedding and prompt features using cross-attention27\n(Methods).\nQuantitative and qualitative analysis\nWe evaluated MedSAM through both internal validation and external\nvalidation. Specifically, we compared it to the SOTA segmentation\nfoundation model SAM7 as well as modality-wise specialist U-Net1 and\nDeepLabV3+24 models. Each specialized model was trained on images\nfrom the corresponding modality, resulting in 10 dedicated specialist\nmodels for each method. During inference, these specialist models\nwere used to segment the images from corresponding modalities,\nwhile SAM and MedSAM were employed for segmenting images across\nall modalities (Methods). The internal validation contained 86 seg\u0002mentation tasks (Supplementary Tables 5\u20138 and Fig. 2), and Fig. 3a\nshows the median dice similarity coefficient (DSC) score of these tasks\nfor the four methods. Overall, SAM obtained the lowest performance\non most segmentation tasks although it performed promisingly on\nsome RGB image segmentation tasks, such as polyp (DSC: 91.3%,\ninterquartile range (IQR): 81.2\u201395.1%) segmentation in endoscopy\nimages. This could be attributed to SAM\u2019s training on a variety of RGB\nimages, and the fact that many targets in these images are relatively\nstraightforward to segment due to their distinct appearances. The\nother three models outperformed SAM by a large margin and MedSAM\nhas a narrower distribution of DSC scores of the 86 interval validation\ntasks than the two groups of specialist models, reflecting the robust\u0002ness of MedSAM across different tasks. We further connected the DSC\nscores corresponding to the same task of the four models with the\npodium plot Fig. 3b, which is complementary to the box plot. In the\nupper part, each colored dot denotes the median DSC achieved with\nthe respective method on one task. Dots corresponding to identical\ntest cases are connected by a line. In the lower part, the frequency of\nachieved ranks for each method is presented with bar charts. It can be\nfound that MedSAM ranked in first place on most tasks, surpassing the\nperformance of the U-Net and DeepLabV3+ specialist models that have\na high frequency of ranks with second and third places, respectively, In\ncontrast, SAM ranked last place in almost all tasks. Figure 3c (and\nSupplementary Fig. 9) visualizes some randomly selected segmenta\u0002tion examples where MedSAM obtained a median DSC score, including\nliver tumor in CT images, brain tumor in MR images, breast tumor in\nultrasound images, and polyp in endoscopy images. SAM struggles\nwith targets of weak boundaries, which is prone to under or over\u0002segmentation errors. In contrast, MedSAM can accurately segment a\nwide range of targets across various imaging conditions, which\nachieves comparable of even better than the specialist U-Net and\nDeepLabV3+ models.\nThe external validation included 60 segmentation tasks, all of\nwhich either were from new datasets or involved unseen segmen\u0002tation targets (Supplementary Tables 9\u201311 and Figs. 10\u201312). Fig\u0002ure 4a, b show the task-wise median DSC score distribution and their\ncorrespondence of the 60 tasks, respectively. Although SAM con\u0002tinued exhibiting lower performance on most CT and MR segmen\u0002tation tasks, the specialist models no longer consistently\noutperformed SAM (e.g., right kidney segmentation in MR T1-\nweighted images: 90.1%, 85.3%, 86.4% for SAM, U-Net, and Dee\u0002pLabV3+, respectively). This indicates the limited generalization\nability of such specialist models on unseen targets. In contrast,\nMedSAM consistently delivers superior performance. For example,\nMedSAM obtained median DSC scores of 87.8% (IQR: 85.0-91.4%) on\nthe nasopharynx cancer segmentation task, demonstrating 52.3%,\n15.5%, and 22.7 improvements over SAM, the specialist U-Net, and\nDeepLabV3+, respectively. Significantly, MedSAM also achieved\nbetter performance in some unseen modalities (e.g., abdomen T1\nInphase and Outphase), surpassing SAM and the specialist models\nwith improvements by up to 10%. Figure 4c presents four randomly\nselected segmentation examples for qualitative evaluation, reveal\u0002ing that while all the methods have the ability to handle simple\nsegmentation targets, MedSAM performs better at segmenting\nchallenging targets with indistinguishable boundaries, such as cer\u0002vical cancer in MR images (more examples are presented in Sup\u0002plementary Fig. 13). Furthermore, we evaluated MedSAM on the\nmultiple myeloma plasma cell dataset, which represents a distinct\nmodality and task in contrast to all previously leveraged validation\ntasks. Although this task had never been seen during training,\na b\nImage\nencoder\nBounding box prompts\nMask decoder\nPrompt encoder\nInput Image Segmentation\nImage\nembedding\nFig. 2 | Overview of the modality distribution in the dataset and the network architecture. a The number of medical image-mask pairs in each modality. b MedSAM is a\npromptable segmentation method where users can use bounding boxes to specify the segmentation targets. Source data are provided as a Source Data file.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 3\nc\na b\nSAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM\nFig. 4 | Quantitative and qualitative evaluation results on the external\nvalidation set. a Performance distribution of 60 external validation tasks in terms\nof median dice similarity coefficient (DSC) score. The center line within the box\nrepresents the median value, with the bottom and top bounds of the box deli\u0002neating the 25th and 75th percentiles, respectively. Whiskers are chosen to show\nthe 1.5 of the interquartile range. Up-triangles denote the minima and down\u0002triangles denote the maxima. b Podium plots for visualizing the performance\ncorrespondence of 60 external validation tasks. Upper part: each colored dot\ndenotes the median DSC achieved with the respective method on one task. Dots\ncorresponding to identical tasks are connected by a line. Lower part: bar charts\nrepresent the frequency of achieved ranks for each method. MedSAM ranks in the\nfirst place on most tasks. c Visualized segmentation examples on the external\nvalidation set. The four examples are the lymph node, cervical cancer, fetal head,\nand polyp in CT, MR, ultrasound, and endoscopy images, respectively. Source data\nare provided as a Source Data file.\nSAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM\na\nc\nb\nFig. 3 | Quantitative and qualitative evaluation results on the internal\nvalidation set. a Performance distribution of 86 internal validation tasks in terms\nof median dice similarity coefficient (DSC) score. The center line within the box\nrepresents the median value, with the bottom and top bounds of the box deli\u0002neating the 25th and 75th percentiles, respectively. Whiskers are chosen to show\nthe 1.5 of the interquartile range. Up-triangles denote the minima and down\u0002triangles denote the maxima. b Podium plots for visualizing the performance\ncorrespondence of 86 internal validation tasks. Upper part: each colored dot\ndenotes the median DSC achieved with the respective method on one task. Dots\ncorresponding to identical tasks are connected by a line. Lower part: bar charts\nrepresent the frequency of achieved ranks for each method. MedSAM ranks in the\nfirst place on most tasks. c Visualized segmentation examples on the internal\nvalidation set. The four examples are liver cancer, brain cancer, breast cancer, and\npolyp in computed tomography (CT), (Magnetic Resonance Imaging) MRI, ultra\u0002sound, and endoscopy images, respectively. Blue: bounding box prompts; Yellow:\nsegmentation results. Magenta: expert annotations. Source data are provided as a\nSource Data file.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 4\nMedSAM still exhibited superior performance compared to the SAM\n(Supplementary Fig. 14), highlighting its remarkable generalization\nability.\nThe effect of training dataset size\nWe also investigated the effect of varying dataset sizes on MedSAM\u2019s\nperformance because the training dataset size has been proven to be\npivotal in model performance28. We additionally trained MedSAM on\ntwo different dataset sizes: 10,000 (10K) and 100,000 (100K) images\nand their performances were compared with the default MedSAM\nmodel. The 10K and 100K training images were uniformly sampled\nfrom the whole training set, to maintain data diversity. As shown in\n(Fig. 5a) (Supplementary Tables 12\u201314), the performance adhered to\nthe scaling rule, where increasing the number of training images sig\u0002nificantly improved the performance in both internal and external\nvalidation sets.\nMedSAM can improve the annotation efficiency\nFurthermore, we conducted a human annotation study to assess the\ntime cost of two pipelines (Methods). For the first pipeline, two human\nexperts manually annotate 3D adrenal tumors in a slice-by-slice way. For\nthe second pipeline, the experts first drew the long and short tumor axes\nwith the linear marker (initial marker) every 3-10 slices, which is a com\u0002mon practice in tumor response evaluation. Then, MedSAM was used to\nsegment the tumors based on these sparse linear annotations. Finally,\nthe expert manually revised the segmentation results until they were\nsatisfied. We quantitatively compared the annotation time cost between\nthe two pipelines (Fig. 5b). The results demonstrate that with the assis\u0002tance of MedSAM, the annotation time is substantially reduced by\n82.37% and 82.95% for the two experts, respectively.\nDiscussion\nWe introduce MedSAM, a deep learning-powered foundation model\ndesigned for the segmentation of a wide array of anatomical structures\nand lesions across diverse medical imaging modalities. MedSAM is\ntrained on a meticulously assembled large-scale dataset comprised of\nover one million medical image-mask pairs. Its promptable config\u0002uration strikes an optimal balance between automation and customi\u0002zation, rendering MedSAM a versatile tool for universal medical image\nsegmentation.\nThrough comprehensive evaluations encompassing both internal\nand external validation, MedSAM has demonstrated substantial cap\u0002abilities in segmenting a diverse array of targets and robust general\u0002ization abilities to manage new data and tasks. Its performance not\nonly significantly exceeds that of existing the state-of-the-art seg\u0002mentation foundation model, but also rivals or even surpasses spe\u0002cialist models. By providing precise delineation of anatomical\nstructures and pathological regions, MedSAM facilitates the compu\u0002tation of various quantitative measures that serve as biomarkers. For\ninstance, in the field of oncology, MedSAM could play a crucial role in\naccelerating the 3D tumor annotation process, enabling subsequent\ncalculations of tumor volume, which is a critical biomarker29 for\nassessing disease progression and response to treatment. Additionally,\nMedSAM provides a successful paradigm for adapting natural image\nfoundation models to new domains, which can be further extended to\nbiological image segmentation30, such as cell segmentation in light\nmicroscopy images31 and organelle segmentation in electron micro\u0002scopy images32.\nWhile MedSAM boasts strong capabilities, it does present certain\nlimitations. One such limitation is the modality imbalance in the\ntraining set, with CT, MRI, and endoscopy images dominating the\ndataset. This could potentially impact the model\u2019s performance on\nless-represented modalities, such as mammography. Another limita\u0002tion is its difficulty in the segmentation of vessel-like branching\nstructures because the bounding box prompt can be ambiguous in this\nsetting. For example, arteries and veins share the same bounding box\nin eye fundus images. However, these limitations do not diminish\nMedSAM\u2019s utility. Since MedSAM has learned rich and representative\nmedical image features from the large-scale training set, it can be fine\u0002tuned to effectively segment new tasks from less-represented mod\u0002alities or intricate structures like vessels.\nIn conclusion, this study highlights the feasibility of constructing a\nsingle foundation model capable of managing a multitude of seg\u0002mentation tasks, thereby eliminating the need for task-specific models.\nMedSAM, as the inaugural foundation model in medical image seg\u0002mentation, holds great potential to accelerate the advancement of new\ndiagnostic and therapeutic tools, and ultimately contribute to\nimproved patient care33.\nMethods\nDataset curation and pre-processing\nWe curated a comprehensive dataset by collating images from publicly\navailable medical image segmentation datasets, which were obtained\nfrom various sources across the internet, including the Cancer Imaging\nArchive (TCIA)34, Kaggle, Grand-Challenge, Scientific Data, CodaLab,\nand segmentation challenges in the Medical Image Computing and\nComputer Assisted Intervention Society (MICCAI). All the datasets\nprovided segmentation annotations by human experts, which have\nbeen widely used in existing literature (Supplementary Table 1\u20134). We\nincorporated these annotations directly for both model development\nand validation.\nThe original 3D datasets consisted of computed tomography (CT)\nand magnetic resonance (MR) images in DICOM, nrrd, or mhd formats.\nTo ensure uniformity and compatibility with developing medical\nimage deep learning models, we converted the images to the widely\nused NifTI format. Additionally, grayscale images (such as X-Ray and\nUltrasound) as well as RGB images (including endoscopy, dermoscopy,\nfundus, and pathology images), were converted to the png format.\nFig. 5 | The effect of training dataset size and a user study of tumor annotation\nefficiency. a Scaling up the training image size to one million can significantly\nimprove the model performance on both internal and external validation sets.\nb MedSAM can be used to substantially reduce the annotation time cost. Source\ndata are provided as a Source Data file.\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 5\nSeveral exclusive criteria are applied to improve the dataset quality\nand consistency, including incomplete images and segmentation tar\u0002gets with branching structures, inaccurate annotations, and tiny\nvolumes. Notably, image intensities varied significantly across differ\u0002ent modalities. For instance, CT images had intensity values ranging\nfrom -2000 to 2000, while MR images exhibited a range of 0 to 3000.\nIn endoscopy and ultrasound images, intensity values typically span\u0002ned from 0 to 255. To facilitate stable training, we performed intensity\nnormalization across all images, ensuring they shared the same\nintensity range.\nFor CT images, we initially normalized the Hounsfield units using\ntypical window width and level values. The employed window width\nand level values for soft tissues, lung, and brain are (W:400, L:40),\n(W:1500, L:-160), and (W:80, L:40), respectively. Subsequently, the\nintensity values were rescaled to the range of [0, 255]. For MR, X-ray,\nultrasound, mammography, and optical coherence tomography (OCT)\nimages, we clipped the intensity values to the range between the 0.5th\nand 99.5th percentiles before rescaling them to the range of [0, 255].\nRegarding RGB images (e.g., endoscopy, dermoscopy, fundus, and\npathology images), if they were already within the expected intensity\nrange of [0, 255], their intensities remained unchanged. However, if\nthey fell outside this range, we utilized max-min normalization to\nrescale the intensity values to [0, 255]. Finally, to meet the model\u2019s\ninput requirements, all images were resized to a uniform size of\n1024 \u00d7 1024 \u00d7 3. In the case of whole-slide pathology images, patches\nwere extracted using a sliding window approach without overlaps. The\npatches located on boundaries were padded to this size with 0. As for\n3D CT and MR images, each 2D slice was resized to 1024 \u00d7 1024, and\nthe channel was repeated three times to maintain consistency. The\nremaining 2D images were directly resized to 1024 \u00d7 1024 \u00d7 3. Bi-cubic\ninterpolation was used for resizing images, while nearest-neighbor\ninterpolation was applied for resizing masks to preserve their precise\nboundaries and avoid introducing unwanted artifacts. These standar\u0002dization procedures ensured uniformity and compatibility across all\nimages and facilitated seamless integration into the subsequent stages\nof the model training and evaluation pipeline.\nNetwork architecture\nThe network utilized in this study was built on transformer\narchitecture27, which has demonstrated remarkable effectiveness in\nvarious domains such as natural language processing and image\nrecognition tasks25. Specifically, the network incorporated a vision\ntransformer (ViT)-based image encoder responsible for extracting\nimage features, a prompt encoder for integrating user interactions\n(bounding boxes), and a mask decoder that generated segmentation\nresults and confidence scores using the image embedding, prompt\nembedding, and output token.\nTo strike a balance between segmentation performance and com\u0002putational efficiency, we employed the base ViT model as the image\nencoder since extensive evaluation indicated that larger ViT models,\nsuch as ViT Large and ViT Huge, offered only marginal improvements in\naccuracy7 while significantly increasing computational demands. Speci\u0002fically, the base ViT model consists of 12 transformer layers27, with each\nblock comprising a multi-head self-attention block and a Multilayer\nPerceptron (MLP) block incorporating layer normalization35. Pre-training\nwas performed using masked auto-encoder modeling36, followed by\nfully supervised training on the SAM dataset7\n. The input image\n(1024 \u00d7 1024 \u00d7 3) was reshaped into a sequence of flattened 2D patches\nwith the size 16 \u00d7 16 \u00d7 3, yielding a feature size in image embedding of\n64 \u00d7 64 after passing through the image encoder, which is 16 \u00d7 down\u0002scaled. The prompt encoders mapped the corner point of the bounding\nbox prompt to 256-dimensional vectorial embeddings26. In particular,\neach bounding box was represented by an embedding pair of the top\u0002left corner point and the bottom-right corner point. To facilitate real\u0002time user interactions once the image embedding had been computed, a\nlightweight mask decoder architecture was employed. It consists of two\ntransformer layers27 for fusing the image embedding and prompt\nencoding, and two transposed convolutional layers to enhance the\nembedding resolution to 256 \u00d7 256. Subsequently, the embedding\nunderwent sigmoid activation, followed by bi-linear interpolations to\nmatch the input size.\nTraining protocol and experimental setting\nDuring data pre-processing, we obtained 1,570,263 medical image\u0002mask pairs for model development and validation. For internal vali\u0002dation, we randomly split the dataset into 80%, 10%, and 10% as\ntraining, tuning, and validation, respectively. Specifically, for mod\u0002alities where within-scan continuity exists, such as CT and MRI, and\nmodalities where continuity exists between consecutive frames, we\nperformed the data splitting at the 3D scan and the video level\nrespectively, by which any potential data leak was prevented. For\npathology images, recognizing the significance of slide-level cohe\u0002siveness, we first separated the whole-slide images into distinct slide\u0002based sets. Then, each slide was divided into small patches with a fixed\nsize of 1024 \u00d7 1024. This setup allowed us to monitor the model\u2019s\nperformance on the tuning set and adjust its parameters during\ntraining to prevent overfitting. For the external validation, all datasets\nwere held out and did not appear during model training. These data\u0002sets provide a stringent test of the model\u2019s generalization ability, as\nthey represent new patients, imaging conditions, and potentially new\nsegmentation tasks that the model has not encountered before. By\nevaluating the performance of MedSAM on these unseen datasets, we\ncan gain a realistic understanding of how MedSAM is likely to perform\nin real-world clinical settings, where it will need to handle a wide range\nof variability and unpredictability in the data. The training and vali\u0002dation are independent.\nThe model was initialized with the pre-trained SAM model with\nthe ViT-Base model. We fixed the prompt encoder since it can already\nencode the bounding box prompt. All the trainable parameters in the\nimage encoder and mask decoder were updated during training.\nSpecifically, the number of trainable parameters for the image encoder\nand mask decoder are 89,670,912 and 4,058,340, respectively. The\nbounding box prompt was simulated from the expert annotations with\na random perturbation of 0-20 pixels. The loss function is the\nunweighted sum between dice loss and cross-entropy loss, which has\nbeen proven to be robust in various segmentation tasks1\n. The network\nwas optimized by AdamW37 optimizer (\u03b21 = 0.9, \u03b22 = 0.999) with an\ninitial learning rate of 1e-4 and a weight decay of 0.01. The global batch\nsize was 160 and data augmentation was not used. The model was\ntrained on 20 A100 (80G) GPUs with 150 epochs and the last check\u0002point was selected as the final model.\nFurthermore, to thoroughly evaluate the performance of Med\u0002SAM, we conducted comparative analyses against both the state-of\u0002the-art segmentation foundation model SAM7 and specialist models\n(i.e., U-Net1 and DeepLabV3+24). The training images contained 10\nmodalities: CT, MR, chest X-ray (CXR), dermoscopy, endoscopy,\nultrasound, mammography, OCT, and pathology, and we trained the\nU-Net and DeepLabV3+ specialist models for each modality. There\nwere 20 specialist models in total and the number of corresponding\ntraining images was presented in Supplementary Table 5. We\nemployed the nnU-Net to conduct all U-Net experiments, which can\nautomatically configure the network architecture based on the dataset\nproperties. In order to incorporate the bounding box prompt into the\nmodel, we transformed the bounding box into a binary mask and\nconcatenated it with the image as the model input. This function was\noriginally supported by nnU-Net in the cascaded pipeline, which has\ndemonstrated increased performance in many segmentation tasks by\nusing the binary mask as an additional channel to specify the target\nlocation. The training settings followed the default configurations of\n2D nnU-Net. Each model was trained on one A100 GPU with 1000\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 6\nepochs and the last checkpoint was used as the final model. The\nDeepLabV3+ specialist models used ResNet5038 as the encoder. Similar\nto ref. 3, the input images were resized to 224 \u00d7 224 \u00d7 3. The bounding\nbox was transformed into a binary mask as an additional input channel\nto provide the object location prompt. Segmentation Models Pytorch\n(0.3.3)39 was used to perform training and inference for all the\nmodality-wise specialist DeepLabV3 + models. Each modality-wise\nmodel was trained on one A100 GPU with 500 epochs and the last\ncheckpoint was used as the final model. During the inference phase,\nSAM and MedSAM were used to perform segmentation across all\nmodalities with a single model. In contrast, the U-Net and DeepLabV3+\nspecialist models were used to individually segment the respective\ncorresponding modalities.\nA task-specific segmentation model might outperform a modality\u0002based one for certain applications. Since U-Net obtained better per\u0002formance than DeepLabV3+ on most tasks, we further conducted a\ncomparison study by training task-specific U-Net models on four\nrepresentative tasks, including liver cancer segmentation in CT scans,\nabdominal organ segmentation in MR scans, nerve cancer segmenta\u0002tion in ultrasound, and polyp segmentation in endoscopy images. The\nexperiments included both internal validation and external validation.\nFor internal validation, we adhered to the default data splits, using\nthem to train the task-specific U-Net models and then evaluate their\nperformance on the corresponding validation set. For external vali\u0002dation, the trained U-Net models were evaluated on new datasets from\nthe same modality or segmentation targets. In all these experiments,\nMedSAM was directly applied to the validation sets without additional\nfine-tuning. As shown in Supplementary Fig. 15, while task-specific U\u0002Net models often achieved great results on internal validation sets,\ntheir performance diminished significantly for external sets. In con\u0002trast, MedSAM maintained consistent performance across both inter\u0002nal and external validation sets. This underscores MedSAM\u2019s superior\ngeneralization ability, making it a versatile tool in a variety of medical\nimage segmentation tasks.\nLoss function\nWe used the unweighted sum between cross-entropy loss and dice\nloss40 as the final loss function since it has been proven to be robust\nacross different medical image segmentation tasks41. Specifically, let\nS, G denote the segmentation result and ground truth, respectively.\nsi, gi denotes the predicted segmentation and ground truth of voxel i,\nrespectively. N is the number of voxels in the image I. Binary cross\u0002entropy loss is defined by\nLBCE = \u0001 1\nN\nXN\ni = 1\ngi log si + \u00f01 \u0001 gi\u00de log\u00f01 \u0001 si\u00de \u0002 \u0003, \u00f01\u00de\nand dice loss is defined by\nLDice = 1 \u0001 2\nPN\ni = 1 gisi PN\ni = 1 \u00f0gi\u00de\n2 + PN\ni = 1 \u00f0si\u00de\n2 : \u00f02\u00de\nThe final loss L is defined by\nL = LBCE + LDice: \u00f03\u00de\nHuman annotation study\nThe objective of the human annotation study was to quantitatively\nevaluate how MedSAM can reduce the annotation time cost. Specifi\u0002cally, we used the recent adrenocortical carcinoma CT dataset34,42,43,\nwhere the segmentation target, adrenal tumor, was neither part of the\ntraining nor of the existing validation sets. We randomly sampled 10\ncases, comprising a total of 733 tumor slices requiring annotations.\nTwo human experts participated in this study, both of whom are\nexperienced radiologists with 8 and 6 years of clinical practice in\nabdominal diseases, respectively. Each expert generated two groups of\nannotations, one with the assistance of MedSAM and one without.\nIn the first group, the experts manually annotated the 3D adrenal\ntumor in a slice-by-slice manner. Annotations by the two experts were\nconducted independently, with no collaborative discussions, and the\ntime taken for each case was recorded. In the second group, annota\u0002tions were generated after one week of cooling period. The experts\nindependently drew the long and short tumor axes as initial markers,\nwhich is a common practice in tumor response evaluation. This pro\u0002cess was executed every 3-10 slices from the top slice to the bottom\nslice of the tumor. Then, we applied MedSAM to segment the tumors\nbased on these sparse linear annotations, including three steps.\n\u2022 Step 1. For each annotated slice, a rectangle binary mask was\ngenerated based on the linear label that can completely cover\nthe linear label. \u2022 Step 2. For the unlabeled slices, the rectangle binary masks were\ncreated through interpolation of the surrounding labeled slices. \u2022 Step 3. We transformed the binary masks into bounding boxes\nand then fed them along with the images into MedSAM to gen\u0002erate segmentation results.\nAll these steps were conducted in an automatic way and the model\nrunning time was recorded for each case. Finally, human experts\nmanually refined the segmentation results until they met their satis\u0002faction. To summarize, the time cost of the second group of annota\u0002tions contained three parts: initial markers, MedSAM inference, and\nrefinement. All the manual annotation processes were based on ITK\u0002SNAP44, an open-source software designed for medical image visuali\u0002zation and annotation.\nEvaluation metrics\nWe followed the recommendations in Metrics Reloaded45 and used the\ndice similarity coefficient and normalized surface distance (NSD) to\nquantitatively evaluate the segmentation results. DSC is a region-based\nsegmentation metric, aiming to evaluate the region overlap between\nexpert annotation masks and segmentation results, which is defined by\nDSC\u00f0G, S\u00de = 2jG \\ Sj\njGj + jSj\n,\nNSD46 is a boundary-based metric, aiming to evaluate the boundary\nconsensus between expert annotation masks and segmentation results\nat a given tolerance, which is defined by\nNSD\u00f0G, S\u00de = j\u2202G \\ B\u00f0\u03c4\u00de\n\u2202S j + j\u2202S \\ B\u00f0\u03c4\u00de\u2202Gj\nj\u2202Gj + j\u2202Sj ,\nwhere B\u00f0\u03c4\u00de\n\u2202G = fx 2 R3 j 9x~ 2 \u2202G, jjx \u0001 x~jj \u2264 \u03c4g, B\u00f0\u03c4\u00de\u2202S = fx 2 R3 j 9x~ 2 \u2202S, jjx \u0001\nx~jj \u2264 \u03c4g denote the border region of the expert annotation mask and\nthe segmentation surface at tolerance \u03c4, respectively. In this paper, we\nset the tolerance \u03c4 as 2.\nStatistical analysis\nTo statistically analyze and compare the performance of the afore\u0002mentioned four methods (MedSAM, SAM, U-Net, and DeepLabV3+\nspecialist models), we employed the Wilcoxon signed-rank test. This\nnon-parametric test is well-suited for comparing paired samples and is\nparticularly useful when the data does not meet the assumptions of\nnormal distribution. This analysis allowed us to determine if any\nmethod demonstrated statistically superior segmentation perfor\u0002mance compared to the others, providing valuable insights into the\ncomparative effectiveness of the evaluated methods. The Wilcoxon\nsigned-rank test results are marked on the DSC and NSD score tables\n(Supplementary Table 6\u201311).\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 7\nSoftware utilized\nAll code was implemented in Python (3.10) using Pytorch (2.0) as the\nbase deep learning framework. We also used several Python packages\nfor data analysis and results visualization, including connected\u0002components-3d (3.10.3), SimpleITK (2.2.1), nibabel (5.1.0), torchvision\n(0.15.2), numpy (1.24.3), scikit-image (0.20.0), scipy (1.10.1), and pan\u0002das (2.0.2), matplotlib (3.7.1), opencv-python (4.8.0), ChallengeR\n(1.0.5), and plotly (5.15.0). Biorender was used to create Fig. 1.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe training and validating datasets used in this study are available in\nthe public domain and can be downloaded via the links provided in\nSupplementary Tables 16 and 17. Source data are provided with this\npaper in the Source Data file. We confirmed that All the image datasets\nin this study are publicly accessible and permitted for research pur\u0002poses. Source data are provided in this paper.\nCode availability\nThe training script, inference script, and trained model have been\npublicly available at https://github.com/bowang-lab/MedSAM. A per\u0002manent version is released on Zenodo47.\nReferences\n1. Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. & Maier-Hein, K. H.\nnnU-Net: a self-configuring method for deep learning-based bio\u0002medical image segmentation. Nat. Method. 18, 203\u2013211 (2021).\n2. De Fauw, J. Clinically applicable deep learning for diagnosis and\nreferral in retinal disease. Nat. Med. 24, 1342\u20131350 (2018).\n3. Ouyang, D. Video-based AI for beat-to-beat assessment of cardiac\nfunction. Nature 580, 252\u2013256 (2020).\n4. Wang, G. Deepigeos: a deep interactive geodesic framework for\nmedical image segmentation. In IEEE Transactions on Pattern Ana\u0002lysis and Machine Intelligence 41, 1559\u20131572 (IEEE, 2018).\n5. Antonelli, M. The medical segmentation decathlon. Nat. Commun.\n13, 4128 (2022).\n6. Minaee, S. Image segmentation using deep learning: A survey. In\nIEEE Transactions on Pattern Analysis and Machine Intelligence 44,\n3523\u20133542 (IEEE, 2021).\n7. Kirillov, A. et al. Segment anything. In IEEE International Conference\non Computer Vision. 4015\u20134026 (IEEE, 2023).\n8. Zou, X. et al. Segment everything everywhere all at once. In\nAdvances in Neural Information Processing Systems (MIT\nPress, 2023).\n9. Wang, G. Interactive medical image segmentation using deep\nlearning with image-specific fine tuning. In IEEE Transactions on\nMedical Imaging 37, 1562\u20131573 (IEEE, 2018).\n10. Zhou, T. Volumetric memory network for interactive medical image\nsegmentation. Med. Image Anal. 83, 102599 (2023).\n11. Luo, X. Mideepseg: Minimally interactive segmentation of unseen\nobjects from medical images using deep learning. Med. Image Anal.\n72, 102102 (2021).\n12. Deng, R. et al. Segment anything model (SAM) for digital pathology:\nassess zero-shot segmentation on whole slide imaging. Preprint at\nhttps://arxiv.org/abs/2304.04155 (2023).\n13. Hu, C., Li, X. When SAM meets medical images: an investigation of\nsegment anything model (SAM) on multi-phase liver tumor seg\u0002mentation. Preprint at https://arxiv.org/abs/2304.08506\n(2023).\n14. He, S., Bao, R., Li, J., Grant, P.E., Ou, Y. Accuracy of segment\u0002anything model (SAM) in medical image segmentation tasks. Pre\u0002print at https://doi.org/10.48550/arXiv.2304.09324 (2023).\n15. Roy, S. et al. SAM.MD: zero-shot medical image segmentation\ncapabilities of the segment anything model. Preprint at https://\narxiv.org/abs/2304.05396 (2023).\n16. Zhou, T., Zhang, Y., Zhou, Y., Wu, Y. & Gong, C. Can SAM segment\npolyps? Preprint at https://arxiv.org/abs/2304.07583 (2023).\n17. Mohapatra, S., Gosai, A., Schlaug, G. Sam vs bet: a comparative\nstudy for brain extraction and segmentation of magnetic resonance\nimages using deep learning. Preprint at https://arxiv.org/abs/2304.\n04738 (2023).\n18. Chen, J., Bai, X. Learning to\" segment anything\" in thermal infrared\nimages through knowledge distillation with a large scale dataset\nSATIR. Preprint at https://arxiv.org/abs/2304.07969 (2023).\n19. Tang, L., Xiao, H., Li, B. Can SAM segment anything? when SAM\nmeets camouflaged object detection. Preprint at https://arxiv.org/\nabs/2304.04709 (2023).\n20. Ji, G.-P. et al. SAM struggles in concealed scenes\u2013empirical study\non\u201d segment anything\u201d. Science China Information Sciences. 66,\n226101 (2023).\n21. Ji, W., Li, J., Bi, Q., Li, W., Cheng, L. Segment anything is not always\nperfect: an investigation of SAM on different real-world applica\u0002tions. Preprint at https://arxiv.org/abs/2304.05750 (2023).\n22. Mazurowski, M. A. Segment anything model for medical image\nanalysis: an experimental study. Med. Image Anal. 89,\n102918 (2023).\n23. Huang, Y. et al. Segment anything model for medical images? Med.\nImage Anal. 92, 103061 (2024).\n24. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H. Encoder\u0002decoder with atrous separable convolution for semantic image\nsegmentation. In Proc. European Conference on Computer Vision.\n801\u2013818 (IEEE, 2018).\n25. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers\nfor image recognition at scale. In: International Conference on\nLearning Representations (OpenReview.net, 2020).\n26. Tancik, M. Fourier features let networks learn high frequency\nfunctions in low-dimensional domains. In Advances in Neural\nInformation Processing Systems 33, 7537\u20137547 (Curran Associates,\nInc., 2020).\n27. Vaswani, A. et al. Attention is all you need. In Advances in Neural\nInformation Processing Systems, Vol. 30 (Curran Associates,\nInc., 2017).\n28. He, B. Blinded, randomized trial of sonographer versus AI cardiac\nfunction assessment. Nature 616, 520\u2013524 (2023).\n29. Eisenhauer, E. A. New response evaluation criteria in solid tumours:\nrevised recist guideline (version 1.1). Eur. J. Cancer 45,\n228\u2013247 (2009).\n30. Ma, J. & Wang, B. Towards foundation models of biological image\nsegmentation. Nat. Method. 20, 953\u2013955 (2023).\n31. Ma, J. et al. The multi-modality cell segmentation challenge:\ntowards universal solutions. Preprint at https://arxiv.org/abs/2308.\n05864 (2023).\n32. Xie, R., Pang, K., Bader, G.D., Wang, B. Maester: masked auto\u0002encoder guided segmentation at pixel resolution for accurate, self\u0002supervised subcellular structure recognition. In IEEE Conference on\nComputer Vision and Pattern Recognition. 3292\u20133301 (IEEE, 2023).\n33. Bera, K., Braman, N., Gupta, A., Velcheti, V. & Madabhushi, A. Pre\u0002dicting cancer outcomes with radiomics and artificial intelligence in\nradiology. Nat. Rev. Clin. Oncol. 19, 132\u2013146 (2022).\n34. Clark, K. The cancer imaging archive (TCIA): maintaining and\noperating a public information repository. J. Digit. Imaging 26,\n1045\u20131057 (2013).\n35. Ba, J.L., Kiros, J.R., Hinton, G.E. Layer normalization. Preprint at\nhttps://arxiv.org/abs/1607.06450 (2016).\n36. He, K. et al. Masked autoencoders are scalable vision learners. In\nProc. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 16000\u201316009 (IEEE, 2022).\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 8\n37. Loshchilov, I., Hutter, F. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations (Open\u0002Review.net, 2019).\n38. He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image\nrecognition. In Proc. IEEE Conference on Computer Vision and Pat\u0002tern Recognition. 770\u2013778 (IEEE, 2016).\n39. Iakubovskii, P. Segmentation models pytorch. GitHub https://\ngithub.com/qubvel/segmentation_models.pytorch (2019).\n40. Milletari, F., Navab, N., Ahmadi, S.-A. V-net: Fully convolutional\nneural networks for volumetric medical image segmentation. In\nInternational Conference on 3D Vision (3DV). 565\u2013571\n(IEEE, 2016).\n41. Ma, J. Loss odyssey in medical image segmentation. Med. Image\nAnal. 71, 102035 (2021).\n42. Ahmed, A. Radiomic mapping model for prediction of Ki-67\nexpression in adrenocortical carcinoma. Clin. Radiol. 75,\n479\u201317 (2020).\n43. Moawad, A.W. et al. Voxel-level segmentation of pathologically\u0002proven Adrenocortical carcinoma with Ki-67 expression (Adrenal\u0002ACC-Ki67-Seg) [data set]. https://doi.org/10.7937/1FPG\u0002VM46 (2023).\n44. Yushkevich, P.A., Gao, Y., Gerig, G. Itk-snap: an interactive tool for\nsemi-automatic segmentation of multi-modality biomedical ima\u0002ges. In International Conference of the IEEE Engineering in Medicine\nand Biology Society (EMBC). 3342\u20133345 (IEEE, 2016).\n45. Maier-Hein, L. et al. Metrics reloaded: Pitfalls and recommendations\nfor image analysis validation. Preprint at https://arxiv.org/abs/\n2206.01653 (2022).\n46. DeepMind surface-distance. https://github.com/google\u0002deepmind/surface-distance (2018).\n47. Ma, J. bowang-lab/MedSAM: v1.0.0. https://doi.org/10.5281/\nzenodo.10452777 (2023).\nAcknowledgements\nThis work was supported by the Natural Sciences and Engineering\nResearch Council of Canada (NSERC, RGPIN-2020-06189 and DGECR\u00022020-00294) and CIFAR AI Chair programs. The authors of this paper\nhighly appreciate all the data owners for providing public medical\nimages to the community. We also thank Meta AI for making the source\ncode of segment anything publicly available to the community. This\nresearch was enabled in part by computing resources provided by the\nDigital Research Alliance of Canada.\nAuthor contributions\nConceived and designed the experiments: J.M. Y.H., C.Y., B.W. Per\u0002formed the experiments: J.M. Y.H., F.L., L.H., C.Y. Analyzed the data: J.M.\nY.H., F.L., L.H., C.Y., B.W. Wrote the paper: J.M. Y.H., F.L., L.H., C.Y., B.W.\nAll authors have read and agreed to the published version of the\nmanuscript.\nCompeting interests\nThe authors declare no competing interests\nAdditional information\nSupplementary information The online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-44824-z.\nCorrespondence and requests for materials should be addressed to Bo\nWang.\nPeer review information Nature Communications thanks David Ouyang,\nand the other, anonymous, reviewer(s) for their contribution to the peer\nreview of this work. A peer review file is available.\nReprints and permissions information is available at\nhttp://www.nature.com/reprints\nPublisher\u2019s note Springer Nature remains neutral with regard to jur\u0002isdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article\u2019s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article\u2019s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\n\u00a9 The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-44824-z\nNature Communications | (2024) 15:654 9",
    "snippets": {
      "s1": "Article https://doi.org/10.1038/s41467-024-44824-z Segment anything in medical images Jun Ma1,2,3, Yuting He4, Feifei Li 1, Lin Han5, Chenyu You 6 & Bo Wang 1,2,3,7,8 Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans. Segmentation is a fundamental task in medical imaging analysis, which involves identifying and delineating regions of interest (ROI) in various medical images,..",
      "s2": "..such as organs, lesions, and tissues1 . Accurate segmentation is essential for many clinical applications, including disease diagnosis, treatment planning, and monitoring of disease progression2,3 . Manual segmentation has long been the gold standard for delineating anatomical structures and pathological regions, but this process is time-consuming, labor-intensive, and often requires a high degree of expertise. Semi- or fully automatic segmentation methods can significantly reduce the time and labor required, increase consistency, and enable the analysis of large-scale datasets4 . Deep learning-based models have shown great promise in medical image segmentation due to their ability to learn intricate image features and deliver accurate segmentation results across a diverse range of tasks, from segmenting specific anatomical structures to identifying pathological regions5 . However, a significant limitation of many current medical image segmentation models is their task-specific nature. These models are typically designed and trained for a specific segmentation task, and their performance can degrade significantly when applied to new tasks or different types of imaging data6 . This lack of generality poses a substantial obstacle to the wider application of these models in clinical practice. In contrast, recent advances in the field of natural image segmentation have witnessed the emergence of segmentation foundation models, such as segment anything model (SAM)7 and Segment Everything Everywhere with Multi-modal..",
      "s3": "..prompts all at once8 , showcasing remarkable versatility and performance across various segmentation tasks. There is a growing demand for universal models in medical image segmentation: models that can be trained once and then applied to a wide range of segmentation tasks. Such models would not only exhibit heightened versatility in terms of model capacity but also potentially lead to more consistent results across different tasks. However, the applicability of the segmentation foundation models (e.g., SAM7 ) to medical image segmentation remains limited due to the significant differences between natural images and medical images. Essentially, SAM is a promptable segmentation method that requires points or bounding boxes to specify the segmentation targets. This resembles conventional interactive segmentation methods4,9\u201311 but SAM has better generalization ability, while existing deep learning-based interactive segmentation methods focus mainly on limited tasks and image modalities. Many studies have applied the out-of-the-box SAM models to typical medical image segmentation tasks12\u201317 and other challenging scenarios18\u201321. For example, the concurrent studies22,23 conducted a Received: 24 October 2023 Accepted: 5 January 2024 Check for updates 1 Peter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada. 2Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, ON, Canada. 3..",
      "s4": "..Vector Institute, Toronto, ON, Canada. 4Department of Computer Science, Western University, London, ON, Canada. 5Tandon School of Engineering, New York University, New York, NY, USA. 6 Department of Electrical Engineering, Yale University, New Haven, CT, USA. 7Department of Computer Science, University of Toronto, Toronto, ON, Canada. 8 UHN AI Hub, Toronto, ON, Canada. e-mail: bowang@vectorinstitute.ai Nature Communications | (2024) 15:654 1 1234567890():,;1234567890():,; comprehensive assessment of SAM across a diverse array of medical images, underscoring that SAM achieved satisfactory segmentation outcomes primarily on targets characterized by distinct boundaries. However, the model exhibited substantial limitations in segmenting typical medical targets with weak boundaries or low contrast. In congruence with these observations, we further introduce MedSAM, a refined foundation model that significantly enhances the segmentation performance of SAM on medical images. MedSAM accomplishes this by fine-tuning SAM on an unprecedented dataset with more than one million medical image-mask pairs. We thoroughly evaluate MedSAM through comprehensive experiments on 86 internal validation tasks and 60 external validation tasks, spanning a variety of anatomical structures, pathological conditions, and medical imaging modalities..",
      "s5": "... Experimental results demonstrate that MedSAM consistently outperforms the state-of-theart (SOTA) segmentation foundation model7 , while achieving performance on par with, or even surpassing specialist models1,24 that were trained on the images from the same modality. These results highlight the potential of MedSAM as a new paradigm for versatile medical image segmentation. Results MedSAM: a foundation model for promptable medical image segmentation MedSAM aims to fulfill the role of a foundation model for universal medical image segmentation. A crucial aspect of constructing such a model is the capacity to accommodate a wide range of variations in imaging conditions, anatomical structures, and pathological conditions. To address this challenge, we curated a diverse and large-scale medical image segmentation dataset with 1,570,263 medical imagemask pairs, covering 10 imaging modalities, over 30 cancer types, and a multitude of imaging protocols (Fig. 1 and Supplementary Tables 1\u20134). This large-scale dataset allows MedSAM to learn a rich representation of medical images, capturing a broad spectrum of anatomies and lesions across different modalities. Figure 2a provides an overview of the distribution of images across different medical imaging modalities in the dataset, ranked by their total numbers. It is evident that computed tomography (CT), magnetic resonance imaging (MRI..",
      "s6": "..), and endoscopy are the dominant modalities, reflecting their ubiquity in clinical practice. CT and MRI images provide detailed cross-sectional views of 3D body structures, making them indispensable for non-invasive diagnostic imaging. Endoscopy, albeit more invasive, enables direct visual inspection of organ interiors, proving invaluable for diagnosing gastrointestinal and urological conditions. Despite the prevalence of these modalities, others such as ultrasound, pathology, fundus, dermoscopy, mammography, and optical coherence tomography (OCT) also hold significant roles in clinical practice. The diversity of these modalities and their corresponding segmentation targets underscores the necessity for universal and effective segmentation models capable of handling the unique characteristics associated with each modality. Another critical consideration is the selection of the appropriate segmentation prompt and network architecture. While the concept of fully automatic segmentation foundation models is enticing, it is fraught with challenges that make it impractical. One of the primary challenges is the variability inherent in segmentation tasks. For example, given a liver cancer CT image, the segmentation task can vary depending on the specific clinical scenario. One clinician might be Fig. 1 | MedSAM is trained on a large-scale dataset that can handle diverse segmentation tasks. The dataset covers a variety of anatomical structures, pathological conditions, and medical imaging modalities. The magenta contours and mask overlays denote the expert..",
      "s7": "..annotations and MedSAM segmentation results, respectively. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 2 interested in segmenting the liver tumor, while another might need to segment the entire liver and surrounding organs. Additionally, the variability in imaging modalities presents another challenge. Modalities such as CT and MR generate 3D images, whereas others like X-ray and ultrasound yield 2D images. These variabilities in task definition and imaging modalities complicate the design of a fully automatic model capable of accurately anticipating and addressing the diverse requirements of different users. Considering these challenges, we argue that a more practical approach is to develop a promptable 2D segmentation model. The model can be easily adapted to specific tasks based on user-provided prompts, offering enhanced flexibility and adaptability. It is also able to handle both 2D and 3D images by processing 3D images as a series of 2D slices. Typical user prompts include points and bounding boxes and we show some segmentation examples with the different prompts in Supplementary Fig. 1. It can be found that bounding boxes provide a more unambiguous spatial context for the region of interest, enabling the algorithm to more precisely discern the target area. This stands in..",
      "s8": "..contrast to point-based prompts, which can introduce ambiguity, particularly when proximate structures resemble each other. Moreover, drawing a bounding box is efficient, especially in scenarios involving multi-object segmentation. We follow the network architecture in SAM7 , including an image encoder, a prompt encoder, and a mask decoder (Fig. 2b). The image encoder25 maps the input image into a high-dimensional image embedding space. The prompt encoder transforms the user-drawn bounding boxes into feature representations via positional encoding26. Finally, the mask decoder fuses the image embedding and prompt features using cross-attention27 (Methods). Quantitative and qualitative analysis We evaluated MedSAM through both internal validation and external validation. Specifically, we compared it to the SOTA segmentation foundation model SAM7 as well as modality-wise specialist U-Net1 and DeepLabV3+24 models. Each specialized model was trained on images from the corresponding modality, resulting in 10 dedicated specialist models for each method. During inference, these specialist models were used to segment the images from corresponding modalities, while SAM and MedSAM were employed for segmenting images across all modalities (Methods). The internal validation contained 86 segmentation tasks (Supplementary Tables 5\u20138 and Fig. 2), and Fig. 3a shows..",
      "s9": "..the median dice similarity coefficient (DSC) score of these tasks for the four methods. Overall, SAM obtained the lowest performance on most segmentation tasks although it performed promisingly on some RGB image segmentation tasks, such as polyp (DSC: 91.3%, interquartile range (IQR): 81.2\u201395.1%) segmentation in endoscopy images. This could be attributed to SAM\u2019s training on a variety of RGB images, and the fact that many targets in these images are relatively straightforward to segment due to their distinct appearances. The other three models outperformed SAM by a large margin and MedSAM has a narrower distribution of DSC scores of the 86 interval validation tasks than the two groups of specialist models, reflecting the robustness of MedSAM across different tasks. We further connected the DSC scores corresponding to the same task of the four models with the podium plot Fig. 3b, which is complementary to the box plot. In the upper part, each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical test cases are connected by a line. In the lower part, the frequency of achieved ranks for each method is presented with bar charts. It can be found that MedSAM ranked in first place on most tasks, surpassing the..",
      "s10": "..performance of the U-Net and DeepLabV3+ specialist models that have a high frequency of ranks with second and third places, respectively, In contrast, SAM ranked last place in almost all tasks. Figure 3c (and Supplementary Fig. 9) visualizes some randomly selected segmentation examples where MedSAM obtained a median DSC score, including liver tumor in CT images, brain tumor in MR images, breast tumor in ultrasound images, and polyp in endoscopy images. SAM struggles with targets of weak boundaries, which is prone to under or oversegmentation errors. In contrast, MedSAM can accurately segment a wide range of targets across various imaging conditions, which achieves comparable of even better than the specialist U-Net and DeepLabV3+ models. The external validation included 60 segmentation tasks, all of which either were from new datasets or involved unseen segmentation targets (Supplementary Tables 9\u201311 and Figs. 10\u201312). Figure 4a, b show the task-wise median DSC score distribution and their correspondence of the 60 tasks, respectively. Although SAM continued exhibiting lower performance on most CT and MR segmentation tasks, the specialist models no longer consistently outperformed SAM (e.g., right kidney segmentation in MR T1- weighted images: 90.1%, 85.3..",
      "s11": "..%, 86.4% for SAM, U-Net, and DeepLabV3+, respectively). This indicates the limited generalization ability of such specialist models on unseen targets. In contrast, MedSAM consistently delivers superior performance. For example, MedSAM obtained median DSC scores of 87.8% (IQR: 85.0-91.4%) on the nasopharynx cancer segmentation task, demonstrating 52.3%, 15.5%, and 22.7 improvements over SAM, the specialist U-Net, and DeepLabV3+, respectively. Significantly, MedSAM also achieved better performance in some unseen modalities (e.g., abdomen T1 Inphase and Outphase), surpassing SAM and the specialist models with improvements by up to 10%. Figure 4c presents four randomly selected segmentation examples for qualitative evaluation, revealing that while all the methods have the ability to handle simple segmentation targets, MedSAM performs better at segmenting challenging targets with indistinguishable boundaries, such as cervical cancer in MR images (more examples are presented in Supplementary Fig. 13). Furthermore, we evaluated MedSAM on the multiple myeloma plasma cell dataset, which represents a distinct modality and task in contrast to all previously leveraged validation tasks. Although this task had never been..",
      "s12": "..seen during training, a b Image encoder Bounding box prompts Mask decoder Prompt encoder Input Image Segmentation Image embedding Fig. 2 | Overview of the modality distribution in the dataset and the network architecture. a The number of medical image-mask pairs in each modality. b MedSAM is a promptable segmentation method where users can use bounding boxes to specify the segmentation targets. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 3 c a b SAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM Fig. 4 | Quantitative and qualitative evaluation results on the external validation set. a Performance distribution of 60 external validation tasks in terms of median dice similarity coefficient (DSC) score. The center line within the box represents the median value, with the bottom and top bounds of the box delineating the 25th and 75th percentiles, respectively. Whiskers are chosen to show the 1.5 of the interquartile range. Up-triangles denote the minima and downtriangles denote the maxima. b Podium plots for visualizing the performance correspondence of 60 external validation tasks. Upper part:..",
      "s13": "..each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical tasks are connected by a line. Lower part: bar charts represent the frequency of achieved ranks for each method. MedSAM ranks in the first place on most tasks. c Visualized segmentation examples on the external validation set. The four examples are the lymph node, cervical cancer, fetal head, and polyp in CT, MR, ultrasound, and endoscopy images, respectively. Source data are provided as a Source Data file. SAM U-Net DeepLabV3+ MedSAM SAM U-Net DeepLabV3+ MedSAM a c b Fig. 3 | Quantitative and qualitative evaluation results on the internal validation set. a Performance distribution of 86 internal validation tasks in terms of median dice similarity coefficient (DSC) score. The center line within the box represents the median value, with the bottom and top bounds of the box delineating the 25th and 75th percentiles, respectively. Whiskers are chosen to show the 1.5 of the interquartile range. Up-triangles denote the minima and downtriangles denote the maxima. b Podium plots for visualizing the performance correspondence of 86 internal validation tasks. Upper part: each colored dot denotes the median DSC achieved with the..",
      "s14": "..respective method on one task. Dots corresponding to identical tasks are connected by a line. Lower part: bar charts represent the frequency of achieved ranks for each method. MedSAM ranks in the first place on most tasks. c Visualized segmentation examples on the internal validation set. The four examples are liver cancer, brain cancer, breast cancer, and polyp in computed tomography (CT), (Magnetic Resonance Imaging) MRI, ultrasound, and endoscopy images, respectively. Blue: bounding box prompts; Yellow: segmentation results. Magenta: expert annotations. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 4 MedSAM still exhibited superior performance compared to the SAM (Supplementary Fig. 14), highlighting its remarkable generalization ability. The effect of training dataset size We also investigated the effect of varying dataset sizes on MedSAM\u2019s performance because the training dataset size has been proven to be pivotal in model performance28. We additionally trained MedSAM on two different dataset sizes: 10,000 (10K) and 100,000 (100K) images and their performances were compared with the default MedSAM model. The 10K and 100K training..",
      "s15": "..images were uniformly sampled from the whole training set, to maintain data diversity. As shown in (Fig. 5a) (Supplementary Tables 12\u201314), the performance adhered to the scaling rule, where increasing the number of training images significantly improved the performance in both internal and external validation sets. MedSAM can improve the annotation efficiency Furthermore, we conducted a human annotation study to assess the time cost of two pipelines (Methods). For the first pipeline, two human experts manually annotate 3D adrenal tumors in a slice-by-slice way. For the second pipeline, the experts first drew the long and short tumor axes with the linear marker (initial marker) every 3-10 slices, which is a common practice in tumor response evaluation. Then, MedSAM was used to segment the tumors based on these sparse linear annotations. Finally, the expert manually revised the segmentation results until they were satisfied. We quantitatively compared the annotation time cost between the two pipelines (Fig. 5b). The results demonstrate that with the assistance of MedSAM, the annotation time is substantially reduced by 82.37% and 82.95% for the two experts, respectively. Discussion We introduce MedSAM, a deep learning-powered foundation model designed for the segmentation of a wide array of anatomical structures..",
      "s16": "..and lesions across diverse medical imaging modalities. MedSAM is trained on a meticulously assembled large-scale dataset comprised of over one million medical image-mask pairs. Its promptable configuration strikes an optimal balance between automation and customization, rendering MedSAM a versatile tool for universal medical image segmentation. Through comprehensive evaluations encompassing both internal and external validation, MedSAM has demonstrated substantial capabilities in segmenting a diverse array of targets and robust generalization abilities to manage new data and tasks. Its performance not only significantly exceeds that of existing the state-of-the-art segmentation foundation model, but also rivals or even surpasses specialist models. By providing precise delineation of anatomical structures and pathological regions, MedSAM facilitates the computation of various quantitative measures that serve as biomarkers. For instance, in the field of oncology, MedSAM could play a crucial role in accelerating the 3D tumor annotation process, enabling subsequent calculations of tumor volume, which is a critical biomarker29 for assessing disease progression and response to treatment. Additionally, MedSAM provides a successful paradigm for adapting natural image foundation models to new domains, which can be further extended to biological image segmentation30, such as cell segmentation in light microscopy images31 and organelle segmentation in electron microscopy images32. While MedSAM boasts strong capabilities, it does present certain limitations. One such..",
      "s17": "..limitation is the modality imbalance in the training set, with CT, MRI, and endoscopy images dominating the dataset. This could potentially impact the model\u2019s performance on less-represented modalities, such as mammography. Another limitation is its difficulty in the segmentation of vessel-like branching structures because the bounding box prompt can be ambiguous in this setting. For example, arteries and veins share the same bounding box in eye fundus images. However, these limitations do not diminish MedSAM\u2019s utility. Since MedSAM has learned rich and representative medical image features from the large-scale training set, it can be finetuned to effectively segment new tasks from less-represented modalities or intricate structures like vessels. In conclusion, this study highlights the feasibility of constructing a single foundation model capable of managing a multitude of segmentation tasks, thereby eliminating the need for task-specific models. MedSAM, as the inaugural foundation model in medical image segmentation, holds great potential to accelerate the advancement of new diagnostic and therapeutic tools, and ultimately contribute to improved patient care33. Methods Dataset curation and pre-processing We curated a comprehensive dataset by collating images from publicly available medical image segmentation datasets, which were obtained from various sources across the internet, including the Cancer Imaging Archive (TCIA)34, Kaggle, Grand-Challenge..",
      "s18": ".., Scientific Data, CodaLab, and segmentation challenges in the Medical Image Computing and Computer Assisted Intervention Society (MICCAI). All the datasets provided segmentation annotations by human experts, which have been widely used in existing literature (Supplementary Table 1\u20134). We incorporated these annotations directly for both model development and validation. The original 3D datasets consisted of computed tomography (CT) and magnetic resonance (MR) images in DICOM, nrrd, or mhd formats. To ensure uniformity and compatibility with developing medical image deep learning models, we converted the images to the widely used NifTI format. Additionally, grayscale images (such as X-Ray and Ultrasound) as well as RGB images (including endoscopy, dermoscopy, fundus, and pathology images), were converted to the png format. Fig. 5 | The effect of training dataset size and a user study of tumor annotation efficiency. a Scaling up the training image size to one million can significantly improve the model performance on both internal and external validation sets. b MedSAM can be used to substantially reduce the annotation time cost. Source data are provided as a Source Data file. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024..",
      "s19": "..) 15:654 5 Several exclusive criteria are applied to improve the dataset quality and consistency, including incomplete images and segmentation targets with branching structures, inaccurate annotations, and tiny volumes. Notably, image intensities varied significantly across different modalities. For instance, CT images had intensity values ranging from -2000 to 2000, while MR images exhibited a range of 0 to 3000. In endoscopy and ultrasound images, intensity values typically spanned from 0 to 255. To facilitate stable training, we performed intensity normalization across all images, ensuring they shared the same intensity range. For CT images, we initially normalized the Hounsfield units using typical window width and level values. The employed window width and level values for soft tissues, lung, and brain are (W:400, L:40), (W:1500, L:-160), and (W:80, L:40), respectively. Subsequently, the intensity values were rescaled to the range of [0, 255]. For MR, X-ray, ultrasound, mammography, and optical coherence tomography (OCT) images, we clipped the intensity values to the range between the 0.5th and 99.5th percentiles before rescaling them to the range of [0, 255]. Regarding RGB images (e...",
      "s20": "..g., endoscopy, dermoscopy, fundus, and pathology images), if they were already within the expected intensity range of [0, 255], their intensities remained unchanged. However, if they fell outside this range, we utilized max-min normalization to rescale the intensity values to [0, 255]. Finally, to meet the model\u2019s input requirements, all images were resized to a uniform size of 1024 \u00d7 1024 \u00d7 3. In the case of whole-slide pathology images, patches were extracted using a sliding window approach without overlaps. The patches located on boundaries were padded to this size with 0. As for 3D CT and MR images, each 2D slice was resized to 1024 \u00d7 1024, and the channel was repeated three times to maintain consistency. The remaining 2D images were directly resized to 1024 \u00d7 1024 \u00d7 3. Bi-cubic interpolation was used for resizing images, while nearest-neighbor interpolation was applied for resizing masks to preserve their precise boundaries and avoid introducing unwanted artifacts. These standardization procedures ensured uniformity and compatibility across all images and facilitated seamless integration into the subsequent stages of the model training and evaluation pipeline. Network architecture The network utilized in this study was built on transformer architecture27, which has demonstrated remarkable effectiveness in various domains such as natural language..",
      "s21": "..processing and image recognition tasks25. Specifically, the network incorporated a vision transformer (ViT)-based image encoder responsible for extracting image features, a prompt encoder for integrating user interactions (bounding boxes), and a mask decoder that generated segmentation results and confidence scores using the image embedding, prompt embedding, and output token. To strike a balance between segmentation performance and computational efficiency, we employed the base ViT model as the image encoder since extensive evaluation indicated that larger ViT models, such as ViT Large and ViT Huge, offered only marginal improvements in accuracy7 while significantly increasing computational demands. Specifically, the base ViT model consists of 12 transformer layers27, with each block comprising a multi-head self-attention block and a Multilayer Perceptron (MLP) block incorporating layer normalization35. Pre-training was performed using masked auto-encoder modeling36, followed by fully supervised training on the SAM dataset7 . The input image (1024 \u00d7 1024 \u00d7 3) was reshaped into a sequence of flattened 2D patches with the size 16 \u00d7 16 \u00d7 3, yielding a feature size in image embedding of 64 \u00d7 64 after passing through the image encoder, which is 16 \u00d7 downscaled. The prompt encoders mapped the corner point of the bounding box prompt to..",
      "s22": "..256-dimensional vectorial embeddings26. In particular, each bounding box was represented by an embedding pair of the topleft corner point and the bottom-right corner point. To facilitate realtime user interactions once the image embedding had been computed, a lightweight mask decoder architecture was employed. It consists of two transformer layers27 for fusing the image embedding and prompt encoding, and two transposed convolutional layers to enhance the embedding resolution to 256 \u00d7 256. Subsequently, the embedding underwent sigmoid activation, followed by bi-linear interpolations to match the input size. Training protocol and experimental setting During data pre-processing, we obtained 1,570,263 medical imagemask pairs for model development and validation. For internal validation, we randomly split the dataset into 80%, 10%, and 10% as training, tuning, and validation, respectively. Specifically, for modalities where within-scan continuity exists, such as CT and MRI, and modalities where continuity exists between consecutive frames, we performed the data splitting at the 3D scan and the video level respectively, by which any potential data leak was prevented. For pathology images, recognizing the significance of slide-level cohesiveness, we first separated the whole-slide images into distinct slidebased sets. Then, each slide was divided into small patches with a fixed size..",
      "s23": "..of 1024 \u00d7 1024. This setup allowed us to monitor the model\u2019s performance on the tuning set and adjust its parameters during training to prevent overfitting. For the external validation, all datasets were held out and did not appear during model training. These datasets provide a stringent test of the model\u2019s generalization ability, as they represent new patients, imaging conditions, and potentially new segmentation tasks that the model has not encountered before. By evaluating the performance of MedSAM on these unseen datasets, we can gain a realistic understanding of how MedSAM is likely to perform in real-world clinical settings, where it will need to handle a wide range of variability and unpredictability in the data. The training and validation are independent. The model was initialized with the pre-trained SAM model with the ViT-Base model. We fixed the prompt encoder since it can already encode the bounding box prompt. All the trainable parameters in the image encoder and mask decoder were updated during training. Specifically, the number of trainable parameters for the image encoder and mask decoder are 89,670,912 and 4,058,340, respectively. The bounding box prompt was simulated from the expert annotations with a random perturbation of 0-20 pixels. The loss function is the unweighted sum between dice loss and cross..",
      "s24": "..-entropy loss, which has been proven to be robust in various segmentation tasks1 . The network was optimized by AdamW37 optimizer (\u03b21 = 0.9, \u03b22 = 0.999) with an initial learning rate of 1e-4 and a weight decay of 0.01. The global batch size was 160 and data augmentation was not used. The model was trained on 20 A100 (80G) GPUs with 150 epochs and the last checkpoint was selected as the final model. Furthermore, to thoroughly evaluate the performance of MedSAM, we conducted comparative analyses against both the state-ofthe-art segmentation foundation model SAM7 and specialist models (i.e., U-Net1 and DeepLabV3+24). The training images contained 10 modalities: CT, MR, chest X-ray (CXR), dermoscopy, endoscopy, ultrasound, mammography, OCT, and pathology, and we trained the U-Net and DeepLabV3+ specialist models for each modality. There were 20 specialist models in total and the number of corresponding training images was presented in Supplementary Table 5. We employed the nnU-Net to conduct all U-Net experiments, which can automatically configure the network architecture based on the dataset..",
      "s25": "..properties. In order to incorporate the bounding box prompt into the model, we transformed the bounding box into a binary mask and concatenated it with the image as the model input. This function was originally supported by nnU-Net in the cascaded pipeline, which has demonstrated increased performance in many segmentation tasks by using the binary mask as an additional channel to specify the target location. The training settings followed the default configurations of 2D nnU-Net. Each model was trained on one A100 GPU with 1000 Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 6 epochs and the last checkpoint was used as the final model. The DeepLabV3+ specialist models used ResNet5038 as the encoder. Similar to ref. 3, the input images were resized to 224 \u00d7 224 \u00d7 3. The bounding box was transformed into a binary mask as an additional input channel to provide the object location prompt. Segmentation Models Pytorch (0.3.3)39 was used to perform training and inference for all the modality-wise specialist DeepLabV3 + models. Each modality-wise model was trained on one A100 GPU with 500 epochs and the last checkpoint..",
      "s26": "..was used as the final model. During the inference phase, SAM and MedSAM were used to perform segmentation across all modalities with a single model. In contrast, the U-Net and DeepLabV3+ specialist models were used to individually segment the respective corresponding modalities. A task-specific segmentation model might outperform a modalitybased one for certain applications. Since U-Net obtained better performance than DeepLabV3+ on most tasks, we further conducted a comparison study by training task-specific U-Net models on four representative tasks, including liver cancer segmentation in CT scans, abdominal organ segmentation in MR scans, nerve cancer segmentation in ultrasound, and polyp segmentation in endoscopy images. The experiments included both internal validation and external validation. For internal validation, we adhered to the default data splits, using them to train the task-specific U-Net models and then evaluate their performance on the corresponding validation set. For external validation, the trained U-Net models were evaluated on new datasets from the same modality or segmentation targets. In all these experiments, MedSAM was directly applied to the validation sets without additional fine-tuning. As shown in Supplementary Fig. 15, while task-specific UNet models often achieved great results on internal validation sets, their performance diminished significantly for external sets. In..",
      "s27": "..contrast, MedSAM maintained consistent performance across both internal and external validation sets. This underscores MedSAM\u2019s superior generalization ability, making it a versatile tool in a variety of medical image segmentation tasks. Loss function We used the unweighted sum between cross-entropy loss and dice loss40 as the final loss function since it has been proven to be robust across different medical image segmentation tasks41. Specifically, let S, G denote the segmentation result and ground truth, respectively. si, gi denotes the predicted segmentation and ground truth of voxel i, respectively. N is the number of voxels in the image I. Binary crossentropy loss is defined by LBCE = 1 N XN i = 1 gi log si + \u00f01 gi\u00de log\u00f01 si\u00de , \u00f01\u00de and dice loss is defined by LDice = 1 2 PN i = 1 gisi PN i = 1 \u00f0gi\u00de 2 + PN i = 1 \u00f0si\u00de 2 : \u00f02\u00de The final loss L is defined by L = LBCE + LDice: \u00f03\u00de Human annotation study The objective of the human annotation study was to quantitatively evaluate how MedSAM can reduce the annotation time cost. Specifically, we used the recent adrenocortical carcinoma CT dataset34,42,..",
      "s28": "..43, where the segmentation target, adrenal tumor, was neither part of the training nor of the existing validation sets. We randomly sampled 10 cases, comprising a total of 733 tumor slices requiring annotations. Two human experts participated in this study, both of whom are experienced radiologists with 8 and 6 years of clinical practice in abdominal diseases, respectively. Each expert generated two groups of annotations, one with the assistance of MedSAM and one without. In the first group, the experts manually annotated the 3D adrenal tumor in a slice-by-slice manner. Annotations by the two experts were conducted independently, with no collaborative discussions, and the time taken for each case was recorded. In the second group, annotations were generated after one week of cooling period. The experts independently drew the long and short tumor axes as initial markers, which is a common practice in tumor response evaluation. This process was executed every 3-10 slices from the top slice to the bottom slice of the tumor. Then, we applied MedSAM to segment the tumors based on these sparse linear annotations, including three steps. \u2022 Step 1. For each annotated slice, a rectangle binary mask was generated based on the linear label that can completely cover the linear label. \u2022 Step 2. For the unlabeled slices, the rectangle binary masks were..",
      "s29": "..created through interpolation of the surrounding labeled slices. \u2022 Step 3. We transformed the binary masks into bounding boxes and then fed them along with the images into MedSAM to generate segmentation results. All these steps were conducted in an automatic way and the model running time was recorded for each case. Finally, human experts manually refined the segmentation results until they met their satisfaction. To summarize, the time cost of the second group of annotations contained three parts: initial markers, MedSAM inference, and refinement. All the manual annotation processes were based on ITKSNAP44, an open-source software designed for medical image visualization and annotation. Evaluation metrics We followed the recommendations in Metrics Reloaded45 and used the dice similarity coefficient and normalized surface distance (NSD) to quantitatively evaluate the segmentation results. DSC is a region-based segmentation metric, aiming to evaluate the region overlap between expert annotation masks and segmentation results, which is defined by DSC\u00f0G, S\u00de = 2jG \\ Sj jGj + jSj , NSD46 is a boundary-based metric, aiming to evaluate the boundary consensus between expert annotation masks and segmentation results at a given tolerance, which is defined by NSD\u00f0G, S\u00de = j\u2202G \\ B\u00f0\u03c4\u00de \u2202S j + j..",
      "s30": "..\u2202S \\ B\u00f0\u03c4\u00de\u2202Gj j\u2202Gj + j\u2202Sj , where B\u00f0\u03c4\u00de \u2202G = fx 2 R3 j 9x~ 2 \u2202G, jjx x~jj \u2264 \u03c4g, B\u00f0\u03c4\u00de\u2202S = fx 2 R3 j 9x~ 2 \u2202S, jjx x~jj \u2264 \u03c4g denote the border region of the expert annotation mask and the segmentation surface at tolerance \u03c4, respectively. In this paper, we set the tolerance \u03c4 as 2. Statistical analysis To statistically analyze and compare the performance of the aforementioned four methods (MedSAM, SAM, U-Net, and DeepLabV3+ specialist models), we employed the Wilcoxon signed-rank test. This non-parametric test is well-suited for comparing paired samples and is particularly useful when the data does not meet the assumptions of normal distribution. This analysis allowed us to determine if any method demonstrated statistically superior segmentation performance compared to the others, providing valuable insights into the comparative effectiveness of the evaluated methods. The Wilcoxon signed-rank test results are marked on the DSC and NSD score tables (Supplementary Table 6\u201311). Article https://doi.org/10.1038/s..",
      "s31": "..41467-024-44824-z Nature Communications | (2024) 15:654 7 Software utilized All code was implemented in Python (3.10) using Pytorch (2.0) as the base deep learning framework. We also used several Python packages for data analysis and results visualization, including connectedcomponents-3d (3.10.3), SimpleITK (2.2.1), nibabel (5.1.0), torchvision (0.15.2), numpy (1.24.3), scikit-image (0.20.0), scipy (1.10.1), and pandas (2.0.2), matplotlib (3.7.1), opencv-python (4.8.0), ChallengeR (1.0.5), and plotly (5.15.0). Biorender was used to create Fig. 1. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The training and validating datasets used in this study are available in the public domain and can be downloaded via the links provided in Supplementary Tables 16 and 17. Source data are..",
      "s32": "..provided with this paper in the Source Data file. We confirmed that All the image datasets in this study are publicly accessible and permitted for research purposes. Source data are provided in this paper. Code availability The training script, inference script, and trained model have been publicly available at https://github.com/bowang-lab/MedSAM. A permanent version is released on Zenodo47. References 1. Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. & Maier-Hein, K. H. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat. Method. 18, 203\u2013211 (2021). 2. De Fauw, J. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat. Med. 24, 1342\u20131350 (2018). 3. Ouyang, D. Video-based AI for beat-to-beat assessment of cardiac function. Nature 580, 252\u2013256 (2020). 4. Wang, G. Deepigeos: a deep interactive geodesic framework for medical image segmentation. In IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 1559\u20131572 (..",
      "s33": "..IEEE, 2018). 5. Antonelli, M. The medical segmentation decathlon. Nat. Commun. 13, 4128 (2022). 6. Minaee, S. Image segmentation using deep learning: A survey. In IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 3523\u20133542 (IEEE, 2021). 7. Kirillov, A. et al. Segment anything. In IEEE International Conference on Computer Vision. 4015\u20134026 (IEEE, 2023). 8. Zou, X. et al. Segment everything everywhere all at once. In Advances in Neural Information Processing Systems (MIT Press, 2023). 9. Wang, G. Interactive medical image segmentation using deep learning with image-specific fine tuning. In IEEE Transactions on Medical Imaging 37, 1562\u20131573 (IEEE, 2018). 10. Zhou, T. Volumetric memory network for interactive medical image segmentation. Med. Image Anal. 83, 102599 (2023). 11. Luo, X. Mideepseg: Minimally interactive segmentation of unseen objects from medical images using deep learning. Med. Image Anal. 72, 102102 (2021). 12. Deng, R. et al. Segment anything model (SAM) for digital pathology:..",
      "s34": "..assess zero-shot segmentation on whole slide imaging. Preprint at https://arxiv.org/abs/2304.04155 (2023). 13. Hu, C., Li, X. When SAM meets medical images: an investigation of segment anything model (SAM) on multi-phase liver tumor segmentation. Preprint at https://arxiv.org/abs/2304.08506 (2023). 14. He, S., Bao, R., Li, J., Grant, P.E., Ou, Y. Accuracy of segmentanything model (SAM) in medical image segmentation tasks. Preprint at https://doi.org/10.48550/arXiv.2304.09324 (2023). 15. Roy, S. et al. SAM.MD: zero-shot medical image segmentation capabilities of the segment anything model. Preprint at https:// arxiv.org/abs/2304.05396 (2023). 16. Zhou, T., Zhang, Y., Zhou, Y., Wu, Y. & Gong, C. Can SAM segment polyps? Preprint at https://arxiv.org/abs/..",
      "s35": "..2304.07583 (2023). 17. Mohapatra, S., Gosai, A., Schlaug, G. Sam vs bet: a comparative study for brain extraction and segmentation of magnetic resonance images using deep learning. Preprint at https://arxiv.org/abs/2304. 04738 (2023). 18. Chen, J., Bai, X. Learning to\" segment anything\" in thermal infrared images through knowledge distillation with a large scale dataset SATIR. Preprint at https://arxiv.org/abs/2304.07969 (2023). 19. Tang, L., Xiao, H., Li, B. Can SAM segment anything? when SAM meets camouflaged object detection. Preprint at https://arxiv.org/ abs/2304.04709 (2023). 20. Ji, G.-P. et al. SAM struggles in concealed scenes\u2013empirical study on\u201d segment anything\u201d. Science China Information Sciences. 66, 226101 (2023). 21. Ji, W., Li, J., Bi, Q., Li, W., Cheng, L. Segment anything is not always perfect: an investigation of SAM on different real..",
      "s36": "..-world applications. Preprint at https://arxiv.org/abs/2304.05750 (2023). 22. Mazurowski, M. A. Segment anything model for medical image analysis: an experimental study. Med. Image Anal. 89, 102918 (2023). 23. Huang, Y. et al. Segment anything model for medical images? Med. Image Anal. 92, 103061 (2024). 24. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H. Encoderdecoder with atrous separable convolution for semantic image segmentation. In Proc. European Conference on Computer Vision. 801\u2013818 (IEEE, 2018). 25. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers for image recognition at scale. In: International Conference on Learning Representations (OpenReview.net, 2020). 26. Tancik, M. Fourier features let networks learn high frequency functions in low-dimensional domains. In Advances in Neural Information Processing Systems 33, 7537\u20137547 (Curran Associates, Inc., 2020). 27. Vaswani, A. et al..",
      "s37": "... Attention is all you need. In Advances in Neural Information Processing Systems, Vol. 30 (Curran Associates, Inc., 2017). 28. He, B. Blinded, randomized trial of sonographer versus AI cardiac function assessment. Nature 616, 520\u2013524 (2023). 29. Eisenhauer, E. A. New response evaluation criteria in solid tumours: revised recist guideline (version 1.1). Eur. J. Cancer 45, 228\u2013247 (2009). 30. Ma, J. & Wang, B. Towards foundation models of biological image segmentation. Nat. Method. 20, 953\u2013955 (2023). 31. Ma, J. et al. The multi-modality cell segmentation challenge: towards universal solutions. Preprint at https://arxiv.org/abs/2308. 05864 (2023). 32. Xie, R., Pang, K., Bader, G.D., Wang, B. Maester: masked autoencoder guided segmentation at pixel resolution for accurate, selfsupervised subcellular structure recognition. In IEEE Conference on Computer Vision and Pattern Recognition. 3292\u20133301 (IEEE, 2023). 33. Bera, K., Braman, N...",
      "s38": ".., Gupta, A., Velcheti, V. & Madabhushi, A. Predicting cancer outcomes with radiomics and artificial intelligence in radiology. Nat. Rev. Clin. Oncol. 19, 132\u2013146 (2022). 34. Clark, K. The cancer imaging archive (TCIA): maintaining and operating a public information repository. J. Digit. Imaging 26, 1045\u20131057 (2013). 35. Ba, J.L., Kiros, J.R., Hinton, G.E. Layer normalization. Preprint at https://arxiv.org/abs/1607.06450 (2016). 36. He, K. et al. Masked autoencoders are scalable vision learners. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16000\u201316009 (IEEE, 2022). Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 8 37. Loshchilov, I., Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations (OpenReview.net, 2019). 38. He..",
      "s39": ".., K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition. 770\u2013778 (IEEE, 2016). 39. Iakubovskii, P. Segmentation models pytorch. GitHub https:// github.com/qubvel/segmentation_models.pytorch (2019). 40. Milletari, F., Navab, N., Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In International Conference on 3D Vision (3DV). 565\u2013571 (IEEE, 2016). 41. Ma, J. Loss odyssey in medical image segmentation. Med. Image Anal. 71, 102035 (2021). 42. Ahmed, A. Radiomic mapping model for prediction of Ki-67 expression in adrenocortical carcinoma. Clin. Radiol. 75, 479\u201317 (2020). 43. Moawad, A.W. et al. Voxel-level segmentation of pathologicallyproven Adrenocortical carcinoma with Ki-67 expression (AdrenalACC-Ki67-Seg) [data set]. https://doi..",
      "s40": "...org/10.7937/1FPGVM46 (2023). 44. Yushkevich, P.A., Gao, Y., Gerig, G. Itk-snap: an interactive tool for semi-automatic segmentation of multi-modality biomedical images. In International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 3342\u20133345 (IEEE, 2016). 45. Maier-Hein, L. et al. Metrics reloaded: Pitfalls and recommendations for image analysis validation. Preprint at https://arxiv.org/abs/ 2206.01653 (2022). 46. DeepMind surface-distance. https://github.com/googledeepmind/surface-distance (2018). 47. Ma, J. bowang-lab/MedSAM: v1.0.0. https://doi.org/10.5281/ zenodo.10452777 (2023). Acknowledgements This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC, RGPIN-2020-06189 and DGECR2020-00294) and CIFAR AI Chair programs. The authors of this paper highly appreciate all the..",
      "s41": "..data owners for providing public medical images to the community. We also thank Meta AI for making the source code of segment anything publicly available to the community. This research was enabled in part by computing resources provided by the Digital Research Alliance of Canada. Author contributions Conceived and designed the experiments: J.M. Y.H., C.Y., B.W. Performed the experiments: J.M. Y.H., F.L., L.H., C.Y. Analyzed the data: J.M. Y.H., F.L., L.H., C.Y., B.W. Wrote the paper: J.M. Y.H., F.L., L.H., C.Y., B.W. All authors have read and agreed to the published version of the manuscript. Competing interests The authors declare no competing interests Additional information Supplementary information The online version contains supplementary material available at https://doi.org/10.1038/s41467-024-44824-z. Correspondence and requests for materials should be addressed to Bo Wang. Peer review information Nature Communications thanks David Ouyang, and the other, anonymous, reviewer(s) for..",
      "s42": "..their contribution to the peer review of this work. A peer review file is available. Reprints and permissions information is available at http://www.nature.com/reprints Publisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/ licenses/by/4.0/. \u00a9 The Author(s) 2024 Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications..",
      "s43": "..| (2024) 15:654 9"
    },
    "question_generated": "How can recent advancements in foundation models and promptable segmentation techniques be leveraged to develop a universal medical image segmentation model that addresses the limitations of task-specific approaches and enhances the generalizability and consistency of segmentation across diverse medical imaging tasks?",
    "golden_answer_generated": "# Universal Medical Image Segmentation: A Literature Synthesis\n\n## Introduction\nMedical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. Despite its importance, existing segmentation methods often lack the versatility needed to handle the diverse range of medical imaging tasks\u3010s1\u3011\u3010s2\u3011.\n\n## Background\nCurrent segmentation models are typically designed for specific tasks or modalities, such as CT or MRI scans. This task-specific nature limits their applicability across different imaging conditions and anatomical structures\u3010s2\u3011\u3010s3\u3011. As a result, there is a growing demand for models that can generalize across various tasks, providing consistent and accurate results\u3010s3\u3011.\n\n## Literature Synthesis\n### Limitations of Task-Specific Models\n- **Generalizability Issues:** Task-specific models often fail when applied to new tasks or different types of imaging data\u3010s2\u3011\u3010s3\u3011.\n- **Consistency Challenges:** Variability in imaging modalities and clinical scenarios can lead to inconsistent segmentation results\u3010s6\u3011\u3010s7\u3011.\n\n### Advances in Segmentation Models\n- **Foundation Models:** Recent advances in natural image segmentation have introduced foundation models capable of handling multiple tasks with a single training process\u3010s3\u3011\u3010s4\u3011.\n- **Promptable Segmentation:** Interactive segmentation methods, which use prompts like bounding boxes, have shown promise in improving generalization across tasks\u3010s7\u3011\u3010s8\u3011.\n\n### Need for a Universal Model\n- **Versatility and Efficiency:** A universal model trained on a diverse dataset can potentially deliver accurate segmentation across a wide spectrum of tasks\u3010s5\u3011\u3010s6\u3011.\n- **Potential for Personalization:** Such models could expedite the evolution of diagnostic tools and the personalization of treatment plans\u3010s1\u3011\u3010s5\u3011.\n\n## Conclusion\nBased on the synthesis of existing literature, it is evident that a universal medical image segmentation model could address the limitations of current task-specific approaches. By leveraging advances in foundation models and promptable segmentation, the development of a versatile model is a logical next step. This hypothesis suggests that a single model, trained on a comprehensive dataset, could achieve superior performance across various medical imaging tasks\u3010s5\u3011\u3010s6\u3011.",
    "used_snippets_with_context": {
      "s1": "Article https://doi.org/10.1038/s41467-024-44824-z Segment anything in medical images Jun Ma1,2,3, Yuting He4, Feifei Li 1, Lin Han5, Chenyu You 6 & Bo Wang 1,2,3,7,8 Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans. Segmentation is a fundamental task in medical imaging analysis, which involves identifying and delineating regions of interest (ROI) in various medical images,..",
      "s2": "..such as organs, lesions, and tissues1 . Accurate segmentation is essential for many clinical applications, including disease diagnosis, treatment planning, and monitoring of disease progression2,3 . Manual segmentation has long been the gold standard for delineating anatomical structures and pathological regions, but this process is time-consuming, labor-intensive, and often requires a high degree of expertise. Semi- or fully automatic segmentation methods can significantly reduce the time and labor required, increase consistency, and enable the analysis of large-scale datasets4 . Deep learning-based models have shown great promise in medical image segmentation due to their ability to learn intricate image features and deliver accurate segmentation results across a diverse range of tasks, from segmenting specific anatomical structures to identifying pathological regions5 . However, a significant limitation of many current medical image segmentation models is their task-specific nature. These models are typically designed and trained for a specific segmentation task, and their performance can degrade significantly when applied to new tasks or different types of imaging data6 . This lack of generality poses a substantial obstacle to the wider application of these models in clinical practice. In contrast, recent advances in the field of natural image segmentation have witnessed the emergence of segmentation foundation models, such as segment anything model (SAM)7 and Segment Everything Everywhere with Multi-modal..",
      "s3": "..prompts all at once8 , showcasing remarkable versatility and performance across various segmentation tasks. There is a growing demand for universal models in medical image segmentation: models that can be trained once and then applied to a wide range of segmentation tasks. Such models would not only exhibit heightened versatility in terms of model capacity but also potentially lead to more consistent results across different tasks. However, the applicability of the segmentation foundation models (e.g., SAM7 ) to medical image segmentation remains limited due to the significant differences between natural images and medical images. Essentially, SAM is a promptable segmentation method that requires points or bounding boxes to specify the segmentation targets. This resembles conventional interactive segmentation methods4,9\u201311 but SAM has better generalization ability, while existing deep learning-based interactive segmentation methods focus mainly on limited tasks and image modalities. Many studies have applied the out-of-the-box SAM models to typical medical image segmentation tasks12\u201317 and other challenging scenarios18\u201321. For example, the concurrent studies22,23 conducted a Received: 24 October 2023 Accepted: 5 January 2024 Check for updates 1 Peter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada. 2Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, ON, Canada. 3..",
      "s4": "..Vector Institute, Toronto, ON, Canada. 4Department of Computer Science, Western University, London, ON, Canada. 5Tandon School of Engineering, New York University, New York, NY, USA. 6 Department of Electrical Engineering, Yale University, New Haven, CT, USA. 7Department of Computer Science, University of Toronto, Toronto, ON, Canada. 8 UHN AI Hub, Toronto, ON, Canada. e-mail: bowang@vectorinstitute.ai Nature Communications | (2024) 15:654 1 1234567890():,;1234567890():,; comprehensive assessment of SAM across a diverse array of medical images, underscoring that SAM achieved satisfactory segmentation outcomes primarily on targets characterized by distinct boundaries. However, the model exhibited substantial limitations in segmenting typical medical targets with weak boundaries or low contrast. In congruence with these observations, we further introduce MedSAM, a refined foundation model that significantly enhances the segmentation performance of SAM on medical images. MedSAM accomplishes this by fine-tuning SAM on an unprecedented dataset with more than one million medical image-mask pairs. We thoroughly evaluate MedSAM through comprehensive experiments on 86 internal validation tasks and 60 external validation tasks, spanning a variety of anatomical structures, pathological conditions, and medical imaging modalities..",
      "s5": "... Experimental results demonstrate that MedSAM consistently outperforms the state-of-theart (SOTA) segmentation foundation model7 , while achieving performance on par with, or even surpassing specialist models1,24 that were trained on the images from the same modality. These results highlight the potential of MedSAM as a new paradigm for versatile medical image segmentation. Results MedSAM: a foundation model for promptable medical image segmentation MedSAM aims to fulfill the role of a foundation model for universal medical image segmentation. A crucial aspect of constructing such a model is the capacity to accommodate a wide range of variations in imaging conditions, anatomical structures, and pathological conditions. To address this challenge, we curated a diverse and large-scale medical image segmentation dataset with 1,570,263 medical imagemask pairs, covering 10 imaging modalities, over 30 cancer types, and a multitude of imaging protocols (Fig. 1 and Supplementary Tables 1\u20134). This large-scale dataset allows MedSAM to learn a rich representation of medical images, capturing a broad spectrum of anatomies and lesions across different modalities. Figure 2a provides an overview of the distribution of images across different medical imaging modalities in the dataset, ranked by their total numbers. It is evident that computed tomography (CT), magnetic resonance imaging (MRI..",
      "s6": "..), and endoscopy are the dominant modalities, reflecting their ubiquity in clinical practice. CT and MRI images provide detailed cross-sectional views of 3D body structures, making them indispensable for non-invasive diagnostic imaging. Endoscopy, albeit more invasive, enables direct visual inspection of organ interiors, proving invaluable for diagnosing gastrointestinal and urological conditions. Despite the prevalence of these modalities, others such as ultrasound, pathology, fundus, dermoscopy, mammography, and optical coherence tomography (OCT) also hold significant roles in clinical practice. The diversity of these modalities and their corresponding segmentation targets underscores the necessity for universal and effective segmentation models capable of handling the unique characteristics associated with each modality. Another critical consideration is the selection of the appropriate segmentation prompt and network architecture. While the concept of fully automatic segmentation foundation models is enticing, it is fraught with challenges that make it impractical. One of the primary challenges is the variability inherent in segmentation tasks. For example, given a liver cancer CT image, the segmentation task can vary depending on the specific clinical scenario. One clinician might be Fig. 1 | MedSAM is trained on a large-scale dataset that can handle diverse segmentation tasks. The dataset covers a variety of anatomical structures, pathological conditions, and medical imaging modalities. The magenta contours and mask overlays denote the expert..",
      "s7": "..annotations and MedSAM segmentation results, respectively. Article https://doi.org/10.1038/s41467-024-44824-z Nature Communications | (2024) 15:654 2 interested in segmenting the liver tumor, while another might need to segment the entire liver and surrounding organs. Additionally, the variability in imaging modalities presents another challenge. Modalities such as CT and MR generate 3D images, whereas others like X-ray and ultrasound yield 2D images. These variabilities in task definition and imaging modalities complicate the design of a fully automatic model capable of accurately anticipating and addressing the diverse requirements of different users. Considering these challenges, we argue that a more practical approach is to develop a promptable 2D segmentation model. The model can be easily adapted to specific tasks based on user-provided prompts, offering enhanced flexibility and adaptability. It is also able to handle both 2D and 3D images by processing 3D images as a series of 2D slices. Typical user prompts include points and bounding boxes and we show some segmentation examples with the different prompts in Supplementary Fig. 1. It can be found that bounding boxes provide a more unambiguous spatial context for the region of interest, enabling the algorithm to more precisely discern the target area. This stands in..",
      "s8": "..contrast to point-based prompts, which can introduce ambiguity, particularly when proximate structures resemble each other. Moreover, drawing a bounding box is efficient, especially in scenarios involving multi-object segmentation. We follow the network architecture in SAM7 , including an image encoder, a prompt encoder, and a mask decoder (Fig. 2b). The image encoder25 maps the input image into a high-dimensional image embedding space. The prompt encoder transforms the user-drawn bounding boxes into feature representations via positional encoding26. Finally, the mask decoder fuses the image embedding and prompt features using cross-attention27 (Methods). Quantitative and qualitative analysis We evaluated MedSAM through both internal validation and external validation. Specifically, we compared it to the SOTA segmentation foundation model SAM7 as well as modality-wise specialist U-Net1 and DeepLabV3+24 models. Each specialized model was trained on images from the corresponding modality, resulting in 10 dedicated specialist models for each method. During inference, these specialist models were used to segment the images from corresponding modalities, while SAM and MedSAM were employed for segmenting images across all modalities (Methods). The internal validation contained 86 segmentation tasks (Supplementary Tables 5\u20138 and Fig. 2), and Fig. 3a shows..",
      "s9": "..the median dice similarity coefficient (DSC) score of these tasks for the four methods. Overall, SAM obtained the lowest performance on most segmentation tasks although it performed promisingly on some RGB image segmentation tasks, such as polyp (DSC: 91.3%, interquartile range (IQR): 81.2\u201395.1%) segmentation in endoscopy images. This could be attributed to SAM\u2019s training on a variety of RGB images, and the fact that many targets in these images are relatively straightforward to segment due to their distinct appearances. The other three models outperformed SAM by a large margin and MedSAM has a narrower distribution of DSC scores of the 86 interval validation tasks than the two groups of specialist models, reflecting the robustness of MedSAM across different tasks. We further connected the DSC scores corresponding to the same task of the four models with the podium plot Fig. 3b, which is complementary to the box plot. In the upper part, each colored dot denotes the median DSC achieved with the respective method on one task. Dots corresponding to identical test cases are connected by a line. In the lower part, the frequency of achieved ranks for each method is presented with bar charts. It can be found that MedSAM ranked in first place on most tasks, surpassing the.."
    },
    "eval_answer": "**Leveraging Advancements in Foundation Models and Promptable Segmentation for Universal Medical Image Segmentation**\n\n**Key Strategies:**\n- **Adversarial Training Techniques:** Utilizing methods such as Projected Gradient Descent (PGD) and Misclassification Aware Adversarial Training (MART) can enhance the robustness of medical image classification models against adversarial attacks, which is crucial for developing a universal segmentation model [S7698558203].\n\n- **Feature Enhancement Strategies:** Implementing feature enhancement techniques, like average pooling instead of max pooling, has been shown to improve the resilience of medical image classification models against adversarial attacks, indicating a pathway for enhancing segmentation models [S9082774450].\n\n- **Integration of Adversarial Examples:** Incorporating adversarial examples during the training phase has demonstrated improvements in the resilience of deep learning models in medical imaging, suggesting that similar strategies could enhance the generalizability of universal segmentation models [S3506700528].\n\n**Importance of Robustness:**\n- **Need for Robust Defenses:** Research highlights that adversarial attacks can significantly degrade the performance of medical image classification models, underscoring the necessity for robust defenses in any universal segmentation model to ensure reliable clinical outcomes [S2581826040].\n\n**Conclusion:**\nBy leveraging advancements in adversarial training and feature enhancement strategies, researchers can develop a universal medical image segmentation model that addresses the limitations of task-specific approaches, ultimately enhancing the generalizability and consistency of segmentation across diverse medical imaging tasks.",
    "eval_references": {
      "S3506700528": {
        "id": "S3506700528",
        "text": "The integration of adversarial examples during the training phase has been demonstrated to improve the resilience of deep learning models in medical imaging, suggesting that similar approaches could be applied to enhance the generalizability of universal segmentation models.",
        "children": [
          {
            "id": "E5867154300",
            "text": "..20.4 PGD [45] 2/255 22.8 2.6 8.8 4.6 4/255 12.5 0.8 1.7 0.4 6/255 11.9 0.6 0.2 0.0 8/255 12.8 0.4 0.0 0.0 Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversarial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Mathematics 2023, 11, x FOR PEER REVIEW 25 of Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversa ial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a ran of perturbation rate. 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial imag while training. This improves the model\u2019s capacity for precise judgments in both routin and adversarial cases. This work applies PGD-AT [45] adversarial training approaches Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a range of perturbation rate. Mathematics 2023, 11, 4272 24 of 41 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial images while training. This improves the model\u2019s capacity for precise judgments in both routine and adversarial cases. This work applies PGD-AT [45] adversarial training approaches to the field of biomedical imaging to further these methodologies. In both binary and multi-class contexts, the research illustrates the effectiveness of this strategy in obtaining adversarial resilience for medical classification (see Table 6). The findings show that adversarial-trained models preserve their resilience in the face of different attack configurations. It is important to note that PGD-AT [45] differs from other medical classification models in terms of robustness since it focuses on various methods for creating internal enemies. Table 6. White-box accuracy (%) of adversarial trained (PGD-AT [45]) medical classification models for binary and multi-class classification in various scenarios. Adversarial Attack Type \u03b5 Dermoscopy Fundoscopy Binary (%) Multi-Class (%) Binary(%) Multi-Class (%) Clean Image 0 61.2 52 64.9 60 AA [167] 2/255 55.6 43.7 55.2 41.3 4/255 48.4 35.5 52.3 35 6/255 41.6 26 36.2 31.9 8/255 34.8 20.9 42.1 28.3 CW [41] 2/255 55.6 43.6 56.4 42.3 4/255 48.5 36.2 53.6 34.9 6/255 42.5 29.4 48 31.6 8/255 36.8 23.3 43.5 28.8 FGSM [42] 2/255 55.6 44.5 55.6 44.5 4/255 49.2 38.2 53.6 40 6/255 44.3 32.3 49.3 38.2 8/255 39.5 25.5 45.5 36.4 PGD [45] 2/255 57.9 48 56.4 41.6 4/255 48.2 36.2 53.2 36.4 6/255 42.4 29.7 48.2 33.8 8/255 35.1 22.4 43.6 31.2 We expand our method to adversarial training in the context of medical segmentation problems in addition to demonstrating adversarial resilience for single-label classification models. By analyzing how well these models function under various attack scenarios, as shown in Table 7, we evaluate the efficacy of this adversarial training for medical segmentation. To increase the natural robustness of medical segmentation models, we use the widely used PGD-AT [45] adversarial training technique. Our findings show that compared to their natively trained counterparts, adversarial-trained segmentation models are more resilient to various types of adversarial attacks. Notably, our research shows that attacking the Dice loss still has a greater success rate than attacking the BCE loss. Mathematics 2023, 11, 4272 25 of 41 Table 7. Results of white-box resilience against PGD attack in various attack configurations utilizing u-net for biomedical segmentation. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU (%) Dice (%) mIOU (%) Dice (%) None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.931 0.962 0.773 0.842 4/255 0.890 0.940 0.733 0.810 6/255 0.854 0.912 0.684 0.705 8/255 0.812 0.887 0.601 0.700 Dice 2/255 0.923 0.958 0.767 0.838 4/255 0.880 0.932 0.723 0.803 6/255 0.832 0.894 0.669 0.750 8/255 0.774 0.860 0.594 0.694 5. Discussion Adversarial phenomena have demonstrated substantial prospective applicability, even within the context of cutting-edge deep neural networks (DNNs), irrespective of the extent of access granted to the attacker in relation to the model, as well as their potential to remain imperceptible to the human visual system. In comparison to various other sectors within the field of computer vision, it has been established that medical-oriented DNNs exhibit heightened vulnerability when subjected to adversarial attacks. Pertinent to this notion, it has been shown that adversarial samples with limited disturbance can trick advanced medical systems, which otherwise demonstrate excellent performance when handling clean data. The aforementioned occurrence highlights the vulnerability of medical deep neural networks (DNNs) when exposed to adversarial inputs. A. Dataset and Labeling The availability of labeled datasets for training models in medical imaging is significantly more limited compared to datasets for general computer vision tasks. Typically, datasets for general computer vision encompass a wide range of several hundred thousand to millions of annotated photographs [158]. This scarcity can be attributed to several factors, including concerns over patient privacy and the absence of widely adopted procedures for the exchange of medical data. Another issue is that the process of assigning labels to medical images is a labor-intensive and time-consuming activity, and it is worth noting that the true nature of images in medical databases sometimes presents ambiguity and controversy, even among physicians and radiologists. Hence, this disparity in dataset size has a direct impact on the performance of deep neural networks (DNNs) when used in medical imaging analysis. In light of the scarcity of accessible annotated medical datasets, several scholars have lately put forth several approaches aimed at addressing this issue employing straightforward augmentation techniques. These phenomena have a substantial impact on the generalizability of the network and render it susceptible to adversarial attacks. The effective use of basic augmentation methods, including cropping, rotating, and flipping, has shown to be successful in the creation of unique and unconventional imagery. To address the challenges of vanishing gradient and overfitting, researchers have employed several techniques such as improved activation functions, modified cost function architecture, and drop-out methods [159]. The problem of high computational load has been effectively mitigated by the utilization of highly parallel technology, such as graphics processing units (GPUs), coupled with the implementation of batch normalization techniques. On the other hand, a synergistic methodology involves the amalgamation of convolutional neural networks (CNNs) with transfer learning approaches [160]. The essence of this approach is in the use of parameters obtained by convolutional neural networks (CNNs) in the context of the primary application to enable the training of the modal. The incorporation of transfer learning into convolutional neural network (CNN) frameworks Mathematics 2023, 11, 4272 26 of 41 is a significant direction for future study. This approach shows promise in addressing the challenge of limited labeled medical data. Additionally, a potential approach for increasing the quantity of the dataset involves the utilization of a crowdsourcing technique [103]. The notion of crowdsourcing in the context of health concerns involves the distribution of solutions, facilitated by skilled entities, from a specific research group to a wider population, comprising the general public. This channel offers a compelling direction for future study, facilitating a shift from individual duties to collective endeavors, resulting in societal benefits. B. Computational Resources Deep neural networks (DNNs) have become a fundamental component in the domain of machine learning, significantly transforming several disciplines like computer vision, natural language processing, and medical diagnostics...",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          },
          {
            "id": "E2562140742",
            "text": "...; Pereira, G.; Houssami, N. Artificial intelligence (AI) for breast cancer screening: BreastScreen population-based cohort study of cancer detection. eBioMedicine 2023, 90, 104498. [CrossRef] 24. Family Members Awarded $16.7 Million after Radiologist Missed. Available online: https://www.reliasmedia.com/articles/2163 2-family-members-awarded-16-7-million-after-radiologist-missed-evidence-of-lung-cancer (accessed on 27 September 2023). 25. Zbrzezny, A.M.; Grzybowski, A.E. Deceptive Tricks in Artificial Intelligence: Adversarial Attacks in Ophthalmology. J. Clin. Med. 2023, 12, 3266. [CrossRef] 26. Biggest Healthcare Data Breaches Reported This Year, So Far. Available online: https://healthitsecurity.com/features/biggesthealthcare-data-breaches-reported-this-year-so-far (accessed on 27 September 2023). 27. Kumar, A.; Kumar, D.; Kumar, P.; Dhawan, V. Optimization of Incremental Sheet Forming Process Using Artificial IntelligenceBased Techniques. Nat.-Inspired Optim. Adv. Manuf. Process Syst. 2020, 8, 113\u2013130. [CrossRef] 28. Mukherjee, A.; Sumit; Deepmala; Dhiman, V.K.; Srivastava, P.; Kumar, A. Intellectual Tool to Compute Embodied Energy and Carbon Dioxide Emission for Building Construction Materials. J. Phys. Conf. Ser. 2021, 1950, 012025. [CrossRef] 29. Phogat, M.; Kumar, A.; Nandal, D.; Shokhanda, J. A Novel Automating Irrigation Techniques based on Artificial Neural Network and Fuzzy Logic. J. Phys. Conf. Ser. 2021, 1950, 012088. [CrossRef] 30. Ukwuoma, C.C.; Hossain, M.A.; Jackson, J.K.; Nneji, G.U.; Monday, H.N.; Qin, Z. Multi-Classification of Breast Cancer Lesions in Histopathological Images Using DEEP_Pachi: Multiple Self-Attention Head. Diagnostics 2022, 12, 1152. [CrossRef] 31. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Ejiyi, C.J.; Bamisile, O.; Chikwendu, I.A.; Tienin, B.W.; Hossin, M.A. LCSB-inception: Reliable and effective light-chroma separated branches for Covid-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 150, 106195. [CrossRef] 32. Ukwuoma, C.C.; Qin, Z.; Heyat, M.B.B.; Akhtar, F.; Smahi, A.; Jackson, J.K.; Furqan Qadri, S.; Muaad, A.Y.; Monday, H.N.; Nneji, G.U. Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images. Bioengineering 2022, 9, 709. [CrossRef] [PubMed] 33. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Cobbinah, B.M.; Yussif, S.B.; Abubakar, H.S.; Lemessa, B.D. Dual_Pachi: Attention-based dual path framework with intermediate second order-pooling for COVID-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 151, 106324. [CrossRef] [PubMed] 34. Ritter, F.; Boskamp, T.; Homeyer, A.; Laue, H.; Schwier, M.; Link, F.; Peitgen, H.O. Medical image analysis. IEEE Pulse 2011, 2, 60\u201370. [CrossRef] [PubMed] 35. Phogat, M.; Kumar, D.; Phogat, M.; Kumar, D. Classification of Complex Diseases using an Improved Binary Cuckoo Search and Conditional Mutual Information Maximization. Comput. Sist. 2020, 24, 1121\u20131129. [CrossRef] 36. Ker, J.; Wang, L.; Rao, J.; Lim, T. Deep Learning Applications in Medical Image Analysis. IEEE Access 2017, 6, 9375\u20139379. [CrossRef] 37. Ukwuoma, C.C.; Cai, D.; Gati, E.S.; Agbesi, V.K.; Deribachew, G.; Yobsan Bayisa, L.; Abu, T. Attention-Based End-to-End Hybrid Ensemble Model for Breast Cancer Multi-Classification. Off. Publ. Direct Res. J. Public Health Environ. Technol. 2023, 8, 22\u201339. 38. Anaam, A.; Al-antari, M.A.; Gofuku, A. A deep learning self-attention cross residual network with Info-WGANGP for mitotic cell identification in HEp-2 medical microscopic images. Biomed. Signal Process. Control 2023, 86, 105191. [CrossRef] 39. Fraiwan, M.; Audat, Z.; Fraiwan, L.; Manasreh, T. Using deep transfer learning to detect scoliosis and spondylolisthesis from X-ray images. PLoS ONE 2022, 17, e0267851. [CrossRef] 40. Abdel-Monem, A.; Abouhawwash, M. A Machine Learning Solution for Securing the Internet of Things Infrastructures. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] Mathematics 2023, 11, 4272 35 of 41 41. Carlini, N.; Wagner, D. Towards Evaluating the Robustness of Neural Networks. In Proceedings of the2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 22\u201326 May 2017; pp. 39\u201357. [CrossRef] 42. Goodfellow, I.J.; Shlens, J.; Szegedy, C. Explaining and harnessing adversarial examples. In Proceedings of the 3rd International Conference on Learning Representations ICLR 2015, San Diego, CA, USA, 7\u20139 May 2014; pp. 1\u201311. 43. Pranava Raman, B.M.S.; Anusree, V.; Sreeratcha, B.; Preeti Krishnaveni, R.A.; Dunston, S.D.; Rajam, M.A.V. Analysis of the Effect of Black Box Adversarial Attacks on Medical Image Classification Models. In Proceedings of the Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT), Kannur, India, 11\u201312 August 2022; pp. 528\u2013531. [CrossRef] 44. Tripathi, A.M.; Mishra, A. Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks on Deep COVID-19 Models. arXiv 2020, arXiv:2009.04004. 45. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. In Proceedings of the 6th International Conference on Learning Representations ICLR 2018, Vancouver, BC, Canada, 30 April\u20133 May 2017. Available online: https://arxiv.org/abs/1706.06083v4 (accessed on 18 April 2023). 46. Kansal, K.; Krishna, P.S.; Jain, P.B.; Surya, R.; Honnavalli, P.; Eswaran, S. Defending against adversarial attacks on Covid-19 classifier: A denoiser-based approach. Heliyon 2022, 8, e11209. [CrossRef] [PubMed] 47. Paul, R.; Schabath, M.; Gillies, R.; Hall, L.; Goldgof, D. Mitigating Adversarial Attacks on Medical Image Understanding Systems. In Proceedings of the 2020 IEEE 17th International Symposium on Biomedical Imaging, Iowa City, IA, USA, 3\u20137 April 2020; pp. 1517\u20131521. [CrossRef] 48. Abdelhafeez, A.; Ali, A.M. DeepHAR-Net: A Novel Machine Intelligence Approach for Human Activity Recognition from Inertial Sensors. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 49. Abdelhafeez, A.; Aziz, A.; Khalil, N. Building a Sustainable Social Feedback Loop: A Machine Intelligence Approach for Twitter Opinion Mining. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 50. Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash, A.; Kohno, T.; Song, D. Robust Physical-World Attacks on Deep Learning Visual Classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitio, Seattle, WA, USA, 14\u201319 June 2018; pp. 1625\u20131634. [CrossRef] 51. Ozbulak, U.; Van Messem, A.; De Neve, W. Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation. In Medical Image Computing and Computer Assisted Intervention\u2014MICCAI 2019; Springer: Cham,..",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          }
        ]
      },
      "S2581826040": {
        "id": "S2581826040",
        "text": "Research indicates that adversarial attacks can significantly degrade the performance of medical image classification models, emphasizing the necessity for robust defenses in any universal segmentation model to ensure reliable clinical outcomes.",
        "children": [
          {
            "id": "E5867154300",
            "text": "..20.4 PGD [45] 2/255 22.8 2.6 8.8 4.6 4/255 12.5 0.8 1.7 0.4 6/255 11.9 0.6 0.2 0.0 8/255 12.8 0.4 0.0 0.0 Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversarial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Mathematics 2023, 11, x FOR PEER REVIEW 25 of Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversa ial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a ran of perturbation rate. 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial imag while training. This improves the model\u2019s capacity for precise judgments in both routin and adversarial cases. This work applies PGD-AT [45] adversarial training approaches Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a range of perturbation rate. Mathematics 2023, 11, 4272 24 of 41 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial images while training. This improves the model\u2019s capacity for precise judgments in both routine and adversarial cases. This work applies PGD-AT [45] adversarial training approaches to the field of biomedical imaging to further these methodologies. In both binary and multi-class contexts, the research illustrates the effectiveness of this strategy in obtaining adversarial resilience for medical classification (see Table 6). The findings show that adversarial-trained models preserve their resilience in the face of different attack configurations. It is important to note that PGD-AT [45] differs from other medical classification models in terms of robustness since it focuses on various methods for creating internal enemies. Table 6. White-box accuracy (%) of adversarial trained (PGD-AT [45]) medical classification models for binary and multi-class classification in various scenarios. Adversarial Attack Type \u03b5 Dermoscopy Fundoscopy Binary (%) Multi-Class (%) Binary(%) Multi-Class (%) Clean Image 0 61.2 52 64.9 60 AA [167] 2/255 55.6 43.7 55.2 41.3 4/255 48.4 35.5 52.3 35 6/255 41.6 26 36.2 31.9 8/255 34.8 20.9 42.1 28.3 CW [41] 2/255 55.6 43.6 56.4 42.3 4/255 48.5 36.2 53.6 34.9 6/255 42.5 29.4 48 31.6 8/255 36.8 23.3 43.5 28.8 FGSM [42] 2/255 55.6 44.5 55.6 44.5 4/255 49.2 38.2 53.6 40 6/255 44.3 32.3 49.3 38.2 8/255 39.5 25.5 45.5 36.4 PGD [45] 2/255 57.9 48 56.4 41.6 4/255 48.2 36.2 53.2 36.4 6/255 42.4 29.7 48.2 33.8 8/255 35.1 22.4 43.6 31.2 We expand our method to adversarial training in the context of medical segmentation problems in addition to demonstrating adversarial resilience for single-label classification models. By analyzing how well these models function under various attack scenarios, as shown in Table 7, we evaluate the efficacy of this adversarial training for medical segmentation. To increase the natural robustness of medical segmentation models, we use the widely used PGD-AT [45] adversarial training technique. Our findings show that compared to their natively trained counterparts, adversarial-trained segmentation models are more resilient to various types of adversarial attacks. Notably, our research shows that attacking the Dice loss still has a greater success rate than attacking the BCE loss. Mathematics 2023, 11, 4272 25 of 41 Table 7. Results of white-box resilience against PGD attack in various attack configurations utilizing u-net for biomedical segmentation. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU (%) Dice (%) mIOU (%) Dice (%) None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.931 0.962 0.773 0.842 4/255 0.890 0.940 0.733 0.810 6/255 0.854 0.912 0.684 0.705 8/255 0.812 0.887 0.601 0.700 Dice 2/255 0.923 0.958 0.767 0.838 4/255 0.880 0.932 0.723 0.803 6/255 0.832 0.894 0.669 0.750 8/255 0.774 0.860 0.594 0.694 5. Discussion Adversarial phenomena have demonstrated substantial prospective applicability, even within the context of cutting-edge deep neural networks (DNNs), irrespective of the extent of access granted to the attacker in relation to the model, as well as their potential to remain imperceptible to the human visual system. In comparison to various other sectors within the field of computer vision, it has been established that medical-oriented DNNs exhibit heightened vulnerability when subjected to adversarial attacks. Pertinent to this notion, it has been shown that adversarial samples with limited disturbance can trick advanced medical systems, which otherwise demonstrate excellent performance when handling clean data. The aforementioned occurrence highlights the vulnerability of medical deep neural networks (DNNs) when exposed to adversarial inputs. A. Dataset and Labeling The availability of labeled datasets for training models in medical imaging is significantly more limited compared to datasets for general computer vision tasks. Typically, datasets for general computer vision encompass a wide range of several hundred thousand to millions of annotated photographs [158]. This scarcity can be attributed to several factors, including concerns over patient privacy and the absence of widely adopted procedures for the exchange of medical data. Another issue is that the process of assigning labels to medical images is a labor-intensive and time-consuming activity, and it is worth noting that the true nature of images in medical databases sometimes presents ambiguity and controversy, even among physicians and radiologists. Hence, this disparity in dataset size has a direct impact on the performance of deep neural networks (DNNs) when used in medical imaging analysis. In light of the scarcity of accessible annotated medical datasets, several scholars have lately put forth several approaches aimed at addressing this issue employing straightforward augmentation techniques. These phenomena have a substantial impact on the generalizability of the network and render it susceptible to adversarial attacks. The effective use of basic augmentation methods, including cropping, rotating, and flipping, has shown to be successful in the creation of unique and unconventional imagery. To address the challenges of vanishing gradient and overfitting, researchers have employed several techniques such as improved activation functions, modified cost function architecture, and drop-out methods [159]. The problem of high computational load has been effectively mitigated by the utilization of highly parallel technology, such as graphics processing units (GPUs), coupled with the implementation of batch normalization techniques. On the other hand, a synergistic methodology involves the amalgamation of convolutional neural networks (CNNs) with transfer learning approaches [160]. The essence of this approach is in the use of parameters obtained by convolutional neural networks (CNNs) in the context of the primary application to enable the training of the modal. The incorporation of transfer learning into convolutional neural network (CNN) frameworks Mathematics 2023, 11, 4272 26 of 41 is a significant direction for future study. This approach shows promise in addressing the challenge of limited labeled medical data. Additionally, a potential approach for increasing the quantity of the dataset involves the utilization of a crowdsourcing technique [103]. The notion of crowdsourcing in the context of health concerns involves the distribution of solutions, facilitated by skilled entities, from a specific research group to a wider population, comprising the general public. This channel offers a compelling direction for future study, facilitating a shift from individual duties to collective endeavors, resulting in societal benefits. B. Computational Resources Deep neural networks (DNNs) have become a fundamental component in the domain of machine learning, significantly transforming several disciplines like computer vision, natural language processing, and medical diagnostics...",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          },
          {
            "id": "E2562140742",
            "text": "...; Pereira, G.; Houssami, N. Artificial intelligence (AI) for breast cancer screening: BreastScreen population-based cohort study of cancer detection. eBioMedicine 2023, 90, 104498. [CrossRef] 24. Family Members Awarded $16.7 Million after Radiologist Missed. Available online: https://www.reliasmedia.com/articles/2163 2-family-members-awarded-16-7-million-after-radiologist-missed-evidence-of-lung-cancer (accessed on 27 September 2023). 25. Zbrzezny, A.M.; Grzybowski, A.E. Deceptive Tricks in Artificial Intelligence: Adversarial Attacks in Ophthalmology. J. Clin. Med. 2023, 12, 3266. [CrossRef] 26. Biggest Healthcare Data Breaches Reported This Year, So Far. Available online: https://healthitsecurity.com/features/biggesthealthcare-data-breaches-reported-this-year-so-far (accessed on 27 September 2023). 27. Kumar, A.; Kumar, D.; Kumar, P.; Dhawan, V. Optimization of Incremental Sheet Forming Process Using Artificial IntelligenceBased Techniques. Nat.-Inspired Optim. Adv. Manuf. Process Syst. 2020, 8, 113\u2013130. [CrossRef] 28. Mukherjee, A.; Sumit; Deepmala; Dhiman, V.K.; Srivastava, P.; Kumar, A. Intellectual Tool to Compute Embodied Energy and Carbon Dioxide Emission for Building Construction Materials. J. Phys. Conf. Ser. 2021, 1950, 012025. [CrossRef] 29. Phogat, M.; Kumar, A.; Nandal, D.; Shokhanda, J. A Novel Automating Irrigation Techniques based on Artificial Neural Network and Fuzzy Logic. J. Phys. Conf. Ser. 2021, 1950, 012088. [CrossRef] 30. Ukwuoma, C.C.; Hossain, M.A.; Jackson, J.K.; Nneji, G.U.; Monday, H.N.; Qin, Z. Multi-Classification of Breast Cancer Lesions in Histopathological Images Using DEEP_Pachi: Multiple Self-Attention Head. Diagnostics 2022, 12, 1152. [CrossRef] 31. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Ejiyi, C.J.; Bamisile, O.; Chikwendu, I.A.; Tienin, B.W.; Hossin, M.A. LCSB-inception: Reliable and effective light-chroma separated branches for Covid-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 150, 106195. [CrossRef] 32. Ukwuoma, C.C.; Qin, Z.; Heyat, M.B.B.; Akhtar, F.; Smahi, A.; Jackson, J.K.; Furqan Qadri, S.; Muaad, A.Y.; Monday, H.N.; Nneji, G.U. Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images. Bioengineering 2022, 9, 709. [CrossRef] [PubMed] 33. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Cobbinah, B.M.; Yussif, S.B.; Abubakar, H.S.; Lemessa, B.D. Dual_Pachi: Attention-based dual path framework with intermediate second order-pooling for COVID-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 151, 106324. [CrossRef] [PubMed] 34. Ritter, F.; Boskamp, T.; Homeyer, A.; Laue, H.; Schwier, M.; Link, F.; Peitgen, H.O. Medical image analysis. IEEE Pulse 2011, 2, 60\u201370. [CrossRef] [PubMed] 35. Phogat, M.; Kumar, D.; Phogat, M.; Kumar, D. Classification of Complex Diseases using an Improved Binary Cuckoo Search and Conditional Mutual Information Maximization. Comput. Sist. 2020, 24, 1121\u20131129. [CrossRef] 36. Ker, J.; Wang, L.; Rao, J.; Lim, T. Deep Learning Applications in Medical Image Analysis. IEEE Access 2017, 6, 9375\u20139379. [CrossRef] 37. Ukwuoma, C.C.; Cai, D.; Gati, E.S.; Agbesi, V.K.; Deribachew, G.; Yobsan Bayisa, L.; Abu, T. Attention-Based End-to-End Hybrid Ensemble Model for Breast Cancer Multi-Classification. Off. Publ. Direct Res. J. Public Health Environ. Technol. 2023, 8, 22\u201339. 38. Anaam, A.; Al-antari, M.A.; Gofuku, A. A deep learning self-attention cross residual network with Info-WGANGP for mitotic cell identification in HEp-2 medical microscopic images. Biomed. Signal Process. Control 2023, 86, 105191. [CrossRef] 39. Fraiwan, M.; Audat, Z.; Fraiwan, L.; Manasreh, T. Using deep transfer learning to detect scoliosis and spondylolisthesis from X-ray images. PLoS ONE 2022, 17, e0267851. [CrossRef] 40. Abdel-Monem, A.; Abouhawwash, M. A Machine Learning Solution for Securing the Internet of Things Infrastructures. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] Mathematics 2023, 11, 4272 35 of 41 41. Carlini, N.; Wagner, D. Towards Evaluating the Robustness of Neural Networks. In Proceedings of the2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 22\u201326 May 2017; pp. 39\u201357. [CrossRef] 42. Goodfellow, I.J.; Shlens, J.; Szegedy, C. Explaining and harnessing adversarial examples. In Proceedings of the 3rd International Conference on Learning Representations ICLR 2015, San Diego, CA, USA, 7\u20139 May 2014; pp. 1\u201311. 43. Pranava Raman, B.M.S.; Anusree, V.; Sreeratcha, B.; Preeti Krishnaveni, R.A.; Dunston, S.D.; Rajam, M.A.V. Analysis of the Effect of Black Box Adversarial Attacks on Medical Image Classification Models. In Proceedings of the Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT), Kannur, India, 11\u201312 August 2022; pp. 528\u2013531. [CrossRef] 44. Tripathi, A.M.; Mishra, A. Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks on Deep COVID-19 Models. arXiv 2020, arXiv:2009.04004. 45. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. In Proceedings of the 6th International Conference on Learning Representations ICLR 2018, Vancouver, BC, Canada, 30 April\u20133 May 2017. Available online: https://arxiv.org/abs/1706.06083v4 (accessed on 18 April 2023). 46. Kansal, K.; Krishna, P.S.; Jain, P.B.; Surya, R.; Honnavalli, P.; Eswaran, S. Defending against adversarial attacks on Covid-19 classifier: A denoiser-based approach. Heliyon 2022, 8, e11209. [CrossRef] [PubMed] 47. Paul, R.; Schabath, M.; Gillies, R.; Hall, L.; Goldgof, D. Mitigating Adversarial Attacks on Medical Image Understanding Systems. In Proceedings of the 2020 IEEE 17th International Symposium on Biomedical Imaging, Iowa City, IA, USA, 3\u20137 April 2020; pp. 1517\u20131521. [CrossRef] 48. Abdelhafeez, A.; Ali, A.M. DeepHAR-Net: A Novel Machine Intelligence Approach for Human Activity Recognition from Inertial Sensors. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 49. Abdelhafeez, A.; Aziz, A.; Khalil, N. Building a Sustainable Social Feedback Loop: A Machine Intelligence Approach for Twitter Opinion Mining. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 50. Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash, A.; Kohno, T.; Song, D. Robust Physical-World Attacks on Deep Learning Visual Classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitio, Seattle, WA, USA, 14\u201319 June 2018; pp. 1625\u20131634. [CrossRef] 51. Ozbulak, U.; Van Messem, A.; De Neve, W. Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation. In Medical Image Computing and Computer Assisted Intervention\u2014MICCAI 2019; Springer: Cham,..",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          }
        ]
      },
      "S7698558203": {
        "id": "S7698558203",
        "text": "Adversarial training techniques, such as Projected Gradient Descent (PGD) and Misclassification Aware adversarial Training (MART), have been shown to enhance the robustness of medical image classification models against adversarial attacks, which is essential for developing a universal segmentation model.",
        "children": [
          {
            "id": "E5867154300",
            "text": "..20.4 PGD [45] 2/255 22.8 2.6 8.8 4.6 4/255 12.5 0.8 1.7 0.4 6/255 11.9 0.6 0.2 0.0 8/255 12.8 0.4 0.0 0.0 Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversarial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Mathematics 2023, 11, x FOR PEER REVIEW 25 of Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversa ial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a ran of perturbation rate. 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial imag while training. This improves the model\u2019s capacity for precise judgments in both routin and adversarial cases. This work applies PGD-AT [45] adversarial training approaches Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a range of perturbation rate. Mathematics 2023, 11, 4272 24 of 41 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial images while training. This improves the model\u2019s capacity for precise judgments in both routine and adversarial cases. This work applies PGD-AT [45] adversarial training approaches to the field of biomedical imaging to further these methodologies. In both binary and multi-class contexts, the research illustrates the effectiveness of this strategy in obtaining adversarial resilience for medical classification (see Table 6). The findings show that adversarial-trained models preserve their resilience in the face of different attack configurations. It is important to note that PGD-AT [45] differs from other medical classification models in terms of robustness since it focuses on various methods for creating internal enemies. Table 6. White-box accuracy (%) of adversarial trained (PGD-AT [45]) medical classification models for binary and multi-class classification in various scenarios. Adversarial Attack Type \u03b5 Dermoscopy Fundoscopy Binary (%) Multi-Class (%) Binary(%) Multi-Class (%) Clean Image 0 61.2 52 64.9 60 AA [167] 2/255 55.6 43.7 55.2 41.3 4/255 48.4 35.5 52.3 35 6/255 41.6 26 36.2 31.9 8/255 34.8 20.9 42.1 28.3 CW [41] 2/255 55.6 43.6 56.4 42.3 4/255 48.5 36.2 53.6 34.9 6/255 42.5 29.4 48 31.6 8/255 36.8 23.3 43.5 28.8 FGSM [42] 2/255 55.6 44.5 55.6 44.5 4/255 49.2 38.2 53.6 40 6/255 44.3 32.3 49.3 38.2 8/255 39.5 25.5 45.5 36.4 PGD [45] 2/255 57.9 48 56.4 41.6 4/255 48.2 36.2 53.2 36.4 6/255 42.4 29.7 48.2 33.8 8/255 35.1 22.4 43.6 31.2 We expand our method to adversarial training in the context of medical segmentation problems in addition to demonstrating adversarial resilience for single-label classification models. By analyzing how well these models function under various attack scenarios, as shown in Table 7, we evaluate the efficacy of this adversarial training for medical segmentation. To increase the natural robustness of medical segmentation models, we use the widely used PGD-AT [45] adversarial training technique. Our findings show that compared to their natively trained counterparts, adversarial-trained segmentation models are more resilient to various types of adversarial attacks. Notably, our research shows that attacking the Dice loss still has a greater success rate than attacking the BCE loss. Mathematics 2023, 11, 4272 25 of 41 Table 7. Results of white-box resilience against PGD attack in various attack configurations utilizing u-net for biomedical segmentation. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU (%) Dice (%) mIOU (%) Dice (%) None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.931 0.962 0.773 0.842 4/255 0.890 0.940 0.733 0.810 6/255 0.854 0.912 0.684 0.705 8/255 0.812 0.887 0.601 0.700 Dice 2/255 0.923 0.958 0.767 0.838 4/255 0.880 0.932 0.723 0.803 6/255 0.832 0.894 0.669 0.750 8/255 0.774 0.860 0.594 0.694 5. Discussion Adversarial phenomena have demonstrated substantial prospective applicability, even within the context of cutting-edge deep neural networks (DNNs), irrespective of the extent of access granted to the attacker in relation to the model, as well as their potential to remain imperceptible to the human visual system. In comparison to various other sectors within the field of computer vision, it has been established that medical-oriented DNNs exhibit heightened vulnerability when subjected to adversarial attacks. Pertinent to this notion, it has been shown that adversarial samples with limited disturbance can trick advanced medical systems, which otherwise demonstrate excellent performance when handling clean data. The aforementioned occurrence highlights the vulnerability of medical deep neural networks (DNNs) when exposed to adversarial inputs. A. Dataset and Labeling The availability of labeled datasets for training models in medical imaging is significantly more limited compared to datasets for general computer vision tasks. Typically, datasets for general computer vision encompass a wide range of several hundred thousand to millions of annotated photographs [158]. This scarcity can be attributed to several factors, including concerns over patient privacy and the absence of widely adopted procedures for the exchange of medical data. Another issue is that the process of assigning labels to medical images is a labor-intensive and time-consuming activity, and it is worth noting that the true nature of images in medical databases sometimes presents ambiguity and controversy, even among physicians and radiologists. Hence, this disparity in dataset size has a direct impact on the performance of deep neural networks (DNNs) when used in medical imaging analysis. In light of the scarcity of accessible annotated medical datasets, several scholars have lately put forth several approaches aimed at addressing this issue employing straightforward augmentation techniques. These phenomena have a substantial impact on the generalizability of the network and render it susceptible to adversarial attacks. The effective use of basic augmentation methods, including cropping, rotating, and flipping, has shown to be successful in the creation of unique and unconventional imagery. To address the challenges of vanishing gradient and overfitting, researchers have employed several techniques such as improved activation functions, modified cost function architecture, and drop-out methods [159]. The problem of high computational load has been effectively mitigated by the utilization of highly parallel technology, such as graphics processing units (GPUs), coupled with the implementation of batch normalization techniques. On the other hand, a synergistic methodology involves the amalgamation of convolutional neural networks (CNNs) with transfer learning approaches [160]. The essence of this approach is in the use of parameters obtained by convolutional neural networks (CNNs) in the context of the primary application to enable the training of the modal. The incorporation of transfer learning into convolutional neural network (CNN) frameworks Mathematics 2023, 11, 4272 26 of 41 is a significant direction for future study. This approach shows promise in addressing the challenge of limited labeled medical data. Additionally, a potential approach for increasing the quantity of the dataset involves the utilization of a crowdsourcing technique [103]. The notion of crowdsourcing in the context of health concerns involves the distribution of solutions, facilitated by skilled entities, from a specific research group to a wider population, comprising the general public. This channel offers a compelling direction for future study, facilitating a shift from individual duties to collective endeavors, resulting in societal benefits. B. Computational Resources Deep neural networks (DNNs) have become a fundamental component in the domain of machine learning, significantly transforming several disciplines like computer vision, natural language processing, and medical diagnostics...",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          },
          {
            "id": "E2562140742",
            "text": "...; Pereira, G.; Houssami, N. Artificial intelligence (AI) for breast cancer screening: BreastScreen population-based cohort study of cancer detection. eBioMedicine 2023, 90, 104498. [CrossRef] 24. Family Members Awarded $16.7 Million after Radiologist Missed. Available online: https://www.reliasmedia.com/articles/2163 2-family-members-awarded-16-7-million-after-radiologist-missed-evidence-of-lung-cancer (accessed on 27 September 2023). 25. Zbrzezny, A.M.; Grzybowski, A.E. Deceptive Tricks in Artificial Intelligence: Adversarial Attacks in Ophthalmology. J. Clin. Med. 2023, 12, 3266. [CrossRef] 26. Biggest Healthcare Data Breaches Reported This Year, So Far. Available online: https://healthitsecurity.com/features/biggesthealthcare-data-breaches-reported-this-year-so-far (accessed on 27 September 2023). 27. Kumar, A.; Kumar, D.; Kumar, P.; Dhawan, V. Optimization of Incremental Sheet Forming Process Using Artificial IntelligenceBased Techniques. Nat.-Inspired Optim. Adv. Manuf. Process Syst. 2020, 8, 113\u2013130. [CrossRef] 28. Mukherjee, A.; Sumit; Deepmala; Dhiman, V.K.; Srivastava, P.; Kumar, A. Intellectual Tool to Compute Embodied Energy and Carbon Dioxide Emission for Building Construction Materials. J. Phys. Conf. Ser. 2021, 1950, 012025. [CrossRef] 29. Phogat, M.; Kumar, A.; Nandal, D.; Shokhanda, J. A Novel Automating Irrigation Techniques based on Artificial Neural Network and Fuzzy Logic. J. Phys. Conf. Ser. 2021, 1950, 012088. [CrossRef] 30. Ukwuoma, C.C.; Hossain, M.A.; Jackson, J.K.; Nneji, G.U.; Monday, H.N.; Qin, Z. Multi-Classification of Breast Cancer Lesions in Histopathological Images Using DEEP_Pachi: Multiple Self-Attention Head. Diagnostics 2022, 12, 1152. [CrossRef] 31. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Ejiyi, C.J.; Bamisile, O.; Chikwendu, I.A.; Tienin, B.W.; Hossin, M.A. LCSB-inception: Reliable and effective light-chroma separated branches for Covid-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 150, 106195. [CrossRef] 32. Ukwuoma, C.C.; Qin, Z.; Heyat, M.B.B.; Akhtar, F.; Smahi, A.; Jackson, J.K.; Furqan Qadri, S.; Muaad, A.Y.; Monday, H.N.; Nneji, G.U. Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images. Bioengineering 2022, 9, 709. [CrossRef] [PubMed] 33. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Cobbinah, B.M.; Yussif, S.B.; Abubakar, H.S.; Lemessa, B.D. Dual_Pachi: Attention-based dual path framework with intermediate second order-pooling for COVID-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 151, 106324. [CrossRef] [PubMed] 34. Ritter, F.; Boskamp, T.; Homeyer, A.; Laue, H.; Schwier, M.; Link, F.; Peitgen, H.O. Medical image analysis. IEEE Pulse 2011, 2, 60\u201370. [CrossRef] [PubMed] 35. Phogat, M.; Kumar, D.; Phogat, M.; Kumar, D. Classification of Complex Diseases using an Improved Binary Cuckoo Search and Conditional Mutual Information Maximization. Comput. Sist. 2020, 24, 1121\u20131129. [CrossRef] 36. Ker, J.; Wang, L.; Rao, J.; Lim, T. Deep Learning Applications in Medical Image Analysis. IEEE Access 2017, 6, 9375\u20139379. [CrossRef] 37. Ukwuoma, C.C.; Cai, D.; Gati, E.S.; Agbesi, V.K.; Deribachew, G.; Yobsan Bayisa, L.; Abu, T. Attention-Based End-to-End Hybrid Ensemble Model for Breast Cancer Multi-Classification. Off. Publ. Direct Res. J. Public Health Environ. Technol. 2023, 8, 22\u201339. 38. Anaam, A.; Al-antari, M.A.; Gofuku, A. A deep learning self-attention cross residual network with Info-WGANGP for mitotic cell identification in HEp-2 medical microscopic images. Biomed. Signal Process. Control 2023, 86, 105191. [CrossRef] 39. Fraiwan, M.; Audat, Z.; Fraiwan, L.; Manasreh, T. Using deep transfer learning to detect scoliosis and spondylolisthesis from X-ray images. PLoS ONE 2022, 17, e0267851. [CrossRef] 40. Abdel-Monem, A.; Abouhawwash, M. A Machine Learning Solution for Securing the Internet of Things Infrastructures. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] Mathematics 2023, 11, 4272 35 of 41 41. Carlini, N.; Wagner, D. Towards Evaluating the Robustness of Neural Networks. In Proceedings of the2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 22\u201326 May 2017; pp. 39\u201357. [CrossRef] 42. Goodfellow, I.J.; Shlens, J.; Szegedy, C. Explaining and harnessing adversarial examples. In Proceedings of the 3rd International Conference on Learning Representations ICLR 2015, San Diego, CA, USA, 7\u20139 May 2014; pp. 1\u201311. 43. Pranava Raman, B.M.S.; Anusree, V.; Sreeratcha, B.; Preeti Krishnaveni, R.A.; Dunston, S.D.; Rajam, M.A.V. Analysis of the Effect of Black Box Adversarial Attacks on Medical Image Classification Models. In Proceedings of the Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT), Kannur, India, 11\u201312 August 2022; pp. 528\u2013531. [CrossRef] 44. Tripathi, A.M.; Mishra, A. Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks on Deep COVID-19 Models. arXiv 2020, arXiv:2009.04004. 45. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. In Proceedings of the 6th International Conference on Learning Representations ICLR 2018, Vancouver, BC, Canada, 30 April\u20133 May 2017. Available online: https://arxiv.org/abs/1706.06083v4 (accessed on 18 April 2023). 46. Kansal, K.; Krishna, P.S.; Jain, P.B.; Surya, R.; Honnavalli, P.; Eswaran, S. Defending against adversarial attacks on Covid-19 classifier: A denoiser-based approach. Heliyon 2022, 8, e11209. [CrossRef] [PubMed] 47. Paul, R.; Schabath, M.; Gillies, R.; Hall, L.; Goldgof, D. Mitigating Adversarial Attacks on Medical Image Understanding Systems. In Proceedings of the 2020 IEEE 17th International Symposium on Biomedical Imaging, Iowa City, IA, USA, 3\u20137 April 2020; pp. 1517\u20131521. [CrossRef] 48. Abdelhafeez, A.; Ali, A.M. DeepHAR-Net: A Novel Machine Intelligence Approach for Human Activity Recognition from Inertial Sensors. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 49. Abdelhafeez, A.; Aziz, A.; Khalil, N. Building a Sustainable Social Feedback Loop: A Machine Intelligence Approach for Twitter Opinion Mining. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 50. Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash, A.; Kohno, T.; Song, D. Robust Physical-World Attacks on Deep Learning Visual Classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitio, Seattle, WA, USA, 14\u201319 June 2018; pp. 1625\u20131634. [CrossRef] 51. Ozbulak, U.; Van Messem, A.; De Neve, W. Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation. In Medical Image Computing and Computer Assisted Intervention\u2014MICCAI 2019; Springer: Cham,..",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          }
        ]
      },
      "S9082774450": {
        "id": "S9082774450",
        "text": "Feature enhancement strategies, including the use of average pooling instead of max pooling, have been identified as effective methods to improve the resilience of medical image classification models against adversarial attacks, indicating potential pathways for enhancing segmentation models.",
        "children": [
          {
            "id": "E5867154300",
            "text": "..20.4 PGD [45] 2/255 22.8 2.6 8.8 4.6 4/255 12.5 0.8 1.7 0.4 6/255 11.9 0.6 0.2 0.0 8/255 12.8 0.4 0.0 0.0 Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversarial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Mathematics 2023, 11, x FOR PEER REVIEW 25 of Table 5. Adversarial segmentation result (white box)using the u-net model based on PGD adversa ial attack. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU Dice mIOU Dice None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.559 0.690 0.405 0.517 4/255 0.355 0.492 0.248 0.354 6/255 0.200 0.412 0.198 0.301 8/255 0.230 0.350 0.167 0.255 Dice 2/255 0.473 0.610 0.340 0.450 4/255 0.265 0.391 0.158 0.243 6/255 0.213 0.332 0.102 0.198 8/255 0.152 0.246 0.082 0.140 Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a ran of perturbation rate. 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial imag while training. This improves the model\u2019s capacity for precise judgments in both routin and adversarial cases. This work applies PGD-AT [45] adversarial training approaches Figure 15. Medical adversarial instances (segmentation tasks) along with predictions across a range of perturbation rate. Mathematics 2023, 11, 4272 24 of 41 4.4.2. Adversary Defense Experimental Results Modern adversarial training methods mostly focus on improving adversarial images while training. This improves the model\u2019s capacity for precise judgments in both routine and adversarial cases. This work applies PGD-AT [45] adversarial training approaches to the field of biomedical imaging to further these methodologies. In both binary and multi-class contexts, the research illustrates the effectiveness of this strategy in obtaining adversarial resilience for medical classification (see Table 6). The findings show that adversarial-trained models preserve their resilience in the face of different attack configurations. It is important to note that PGD-AT [45] differs from other medical classification models in terms of robustness since it focuses on various methods for creating internal enemies. Table 6. White-box accuracy (%) of adversarial trained (PGD-AT [45]) medical classification models for binary and multi-class classification in various scenarios. Adversarial Attack Type \u03b5 Dermoscopy Fundoscopy Binary (%) Multi-Class (%) Binary(%) Multi-Class (%) Clean Image 0 61.2 52 64.9 60 AA [167] 2/255 55.6 43.7 55.2 41.3 4/255 48.4 35.5 52.3 35 6/255 41.6 26 36.2 31.9 8/255 34.8 20.9 42.1 28.3 CW [41] 2/255 55.6 43.6 56.4 42.3 4/255 48.5 36.2 53.6 34.9 6/255 42.5 29.4 48 31.6 8/255 36.8 23.3 43.5 28.8 FGSM [42] 2/255 55.6 44.5 55.6 44.5 4/255 49.2 38.2 53.6 40 6/255 44.3 32.3 49.3 38.2 8/255 39.5 25.5 45.5 36.4 PGD [45] 2/255 57.9 48 56.4 41.6 4/255 48.2 36.2 53.2 36.4 6/255 42.4 29.7 48.2 33.8 8/255 35.1 22.4 43.6 31.2 We expand our method to adversarial training in the context of medical segmentation problems in addition to demonstrating adversarial resilience for single-label classification models. By analyzing how well these models function under various attack scenarios, as shown in Table 7, we evaluate the efficacy of this adversarial training for medical segmentation. To increase the natural robustness of medical segmentation models, we use the widely used PGD-AT [45] adversarial training technique. Our findings show that compared to their natively trained counterparts, adversarial-trained segmentation models are more resilient to various types of adversarial attacks. Notably, our research shows that attacking the Dice loss still has a greater success rate than attacking the BCE loss. Mathematics 2023, 11, 4272 25 of 41 Table 7. Results of white-box resilience against PGD attack in various attack configurations utilizing u-net for biomedical segmentation. Adversarial Loss \u03b5 COVID-19 Dermoscopy mIOU (%) Dice (%) mIOU (%) Dice (%) None 0 0.976 0.982 0.801 0.875 BCE 2/255 0.931 0.962 0.773 0.842 4/255 0.890 0.940 0.733 0.810 6/255 0.854 0.912 0.684 0.705 8/255 0.812 0.887 0.601 0.700 Dice 2/255 0.923 0.958 0.767 0.838 4/255 0.880 0.932 0.723 0.803 6/255 0.832 0.894 0.669 0.750 8/255 0.774 0.860 0.594 0.694 5. Discussion Adversarial phenomena have demonstrated substantial prospective applicability, even within the context of cutting-edge deep neural networks (DNNs), irrespective of the extent of access granted to the attacker in relation to the model, as well as their potential to remain imperceptible to the human visual system. In comparison to various other sectors within the field of computer vision, it has been established that medical-oriented DNNs exhibit heightened vulnerability when subjected to adversarial attacks. Pertinent to this notion, it has been shown that adversarial samples with limited disturbance can trick advanced medical systems, which otherwise demonstrate excellent performance when handling clean data. The aforementioned occurrence highlights the vulnerability of medical deep neural networks (DNNs) when exposed to adversarial inputs. A. Dataset and Labeling The availability of labeled datasets for training models in medical imaging is significantly more limited compared to datasets for general computer vision tasks. Typically, datasets for general computer vision encompass a wide range of several hundred thousand to millions of annotated photographs [158]. This scarcity can be attributed to several factors, including concerns over patient privacy and the absence of widely adopted procedures for the exchange of medical data. Another issue is that the process of assigning labels to medical images is a labor-intensive and time-consuming activity, and it is worth noting that the true nature of images in medical databases sometimes presents ambiguity and controversy, even among physicians and radiologists. Hence, this disparity in dataset size has a direct impact on the performance of deep neural networks (DNNs) when used in medical imaging analysis. In light of the scarcity of accessible annotated medical datasets, several scholars have lately put forth several approaches aimed at addressing this issue employing straightforward augmentation techniques. These phenomena have a substantial impact on the generalizability of the network and render it susceptible to adversarial attacks. The effective use of basic augmentation methods, including cropping, rotating, and flipping, has shown to be successful in the creation of unique and unconventional imagery. To address the challenges of vanishing gradient and overfitting, researchers have employed several techniques such as improved activation functions, modified cost function architecture, and drop-out methods [159]. The problem of high computational load has been effectively mitigated by the utilization of highly parallel technology, such as graphics processing units (GPUs), coupled with the implementation of batch normalization techniques. On the other hand, a synergistic methodology involves the amalgamation of convolutional neural networks (CNNs) with transfer learning approaches [160]. The essence of this approach is in the use of parameters obtained by convolutional neural networks (CNNs) in the context of the primary application to enable the training of the modal. The incorporation of transfer learning into convolutional neural network (CNN) frameworks Mathematics 2023, 11, 4272 26 of 41 is a significant direction for future study. This approach shows promise in addressing the challenge of limited labeled medical data. Additionally, a potential approach for increasing the quantity of the dataset involves the utilization of a crowdsourcing technique [103]. The notion of crowdsourcing in the context of health concerns involves the distribution of solutions, facilitated by skilled entities, from a specific research group to a wider population, comprising the general public. This channel offers a compelling direction for future study, facilitating a shift from individual duties to collective endeavors, resulting in societal benefits. B. Computational Resources Deep neural networks (DNNs) have become a fundamental component in the domain of machine learning, significantly transforming several disciplines like computer vision, natural language processing, and medical diagnostics...",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          },
          {
            "id": "E2562140742",
            "text": "...; Pereira, G.; Houssami, N. Artificial intelligence (AI) for breast cancer screening: BreastScreen population-based cohort study of cancer detection. eBioMedicine 2023, 90, 104498. [CrossRef] 24. Family Members Awarded $16.7 Million after Radiologist Missed. Available online: https://www.reliasmedia.com/articles/2163 2-family-members-awarded-16-7-million-after-radiologist-missed-evidence-of-lung-cancer (accessed on 27 September 2023). 25. Zbrzezny, A.M.; Grzybowski, A.E. Deceptive Tricks in Artificial Intelligence: Adversarial Attacks in Ophthalmology. J. Clin. Med. 2023, 12, 3266. [CrossRef] 26. Biggest Healthcare Data Breaches Reported This Year, So Far. Available online: https://healthitsecurity.com/features/biggesthealthcare-data-breaches-reported-this-year-so-far (accessed on 27 September 2023). 27. Kumar, A.; Kumar, D.; Kumar, P.; Dhawan, V. Optimization of Incremental Sheet Forming Process Using Artificial IntelligenceBased Techniques. Nat.-Inspired Optim. Adv. Manuf. Process Syst. 2020, 8, 113\u2013130. [CrossRef] 28. Mukherjee, A.; Sumit; Deepmala; Dhiman, V.K.; Srivastava, P.; Kumar, A. Intellectual Tool to Compute Embodied Energy and Carbon Dioxide Emission for Building Construction Materials. J. Phys. Conf. Ser. 2021, 1950, 012025. [CrossRef] 29. Phogat, M.; Kumar, A.; Nandal, D.; Shokhanda, J. A Novel Automating Irrigation Techniques based on Artificial Neural Network and Fuzzy Logic. J. Phys. Conf. Ser. 2021, 1950, 012088. [CrossRef] 30. Ukwuoma, C.C.; Hossain, M.A.; Jackson, J.K.; Nneji, G.U.; Monday, H.N.; Qin, Z. Multi-Classification of Breast Cancer Lesions in Histopathological Images Using DEEP_Pachi: Multiple Self-Attention Head. Diagnostics 2022, 12, 1152. [CrossRef] 31. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Ejiyi, C.J.; Bamisile, O.; Chikwendu, I.A.; Tienin, B.W.; Hossin, M.A. LCSB-inception: Reliable and effective light-chroma separated branches for Covid-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 150, 106195. [CrossRef] 32. Ukwuoma, C.C.; Qin, Z.; Heyat, M.B.B.; Akhtar, F.; Smahi, A.; Jackson, J.K.; Furqan Qadri, S.; Muaad, A.Y.; Monday, H.N.; Nneji, G.U. Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images. Bioengineering 2022, 9, 709. [CrossRef] [PubMed] 33. Ukwuoma, C.C.; Qin, Z.; Agbesi, V.K.; Cobbinah, B.M.; Yussif, S.B.; Abubakar, H.S.; Lemessa, B.D. Dual_Pachi: Attention-based dual path framework with intermediate second order-pooling for COVID-19 detection from chest X-ray images. Comput. Biol. Med. 2022, 151, 106324. [CrossRef] [PubMed] 34. Ritter, F.; Boskamp, T.; Homeyer, A.; Laue, H.; Schwier, M.; Link, F.; Peitgen, H.O. Medical image analysis. IEEE Pulse 2011, 2, 60\u201370. [CrossRef] [PubMed] 35. Phogat, M.; Kumar, D.; Phogat, M.; Kumar, D. Classification of Complex Diseases using an Improved Binary Cuckoo Search and Conditional Mutual Information Maximization. Comput. Sist. 2020, 24, 1121\u20131129. [CrossRef] 36. Ker, J.; Wang, L.; Rao, J.; Lim, T. Deep Learning Applications in Medical Image Analysis. IEEE Access 2017, 6, 9375\u20139379. [CrossRef] 37. Ukwuoma, C.C.; Cai, D.; Gati, E.S.; Agbesi, V.K.; Deribachew, G.; Yobsan Bayisa, L.; Abu, T. Attention-Based End-to-End Hybrid Ensemble Model for Breast Cancer Multi-Classification. Off. Publ. Direct Res. J. Public Health Environ. Technol. 2023, 8, 22\u201339. 38. Anaam, A.; Al-antari, M.A.; Gofuku, A. A deep learning self-attention cross residual network with Info-WGANGP for mitotic cell identification in HEp-2 medical microscopic images. Biomed. Signal Process. Control 2023, 86, 105191. [CrossRef] 39. Fraiwan, M.; Audat, Z.; Fraiwan, L.; Manasreh, T. Using deep transfer learning to detect scoliosis and spondylolisthesis from X-ray images. PLoS ONE 2022, 17, e0267851. [CrossRef] 40. Abdel-Monem, A.; Abouhawwash, M. A Machine Learning Solution for Securing the Internet of Things Infrastructures. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] Mathematics 2023, 11, 4272 35 of 41 41. Carlini, N.; Wagner, D. Towards Evaluating the Robustness of Neural Networks. In Proceedings of the2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 22\u201326 May 2017; pp. 39\u201357. [CrossRef] 42. Goodfellow, I.J.; Shlens, J.; Szegedy, C. Explaining and harnessing adversarial examples. In Proceedings of the 3rd International Conference on Learning Representations ICLR 2015, San Diego, CA, USA, 7\u20139 May 2014; pp. 1\u201311. 43. Pranava Raman, B.M.S.; Anusree, V.; Sreeratcha, B.; Preeti Krishnaveni, R.A.; Dunston, S.D.; Rajam, M.A.V. Analysis of the Effect of Black Box Adversarial Attacks on Medical Image Classification Models. In Proceedings of the Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT), Kannur, India, 11\u201312 August 2022; pp. 528\u2013531. [CrossRef] 44. Tripathi, A.M.; Mishra, A. Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks on Deep COVID-19 Models. arXiv 2020, arXiv:2009.04004. 45. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. Towards Deep Learning Models Resistant to Adversarial Attacks. In Proceedings of the 6th International Conference on Learning Representations ICLR 2018, Vancouver, BC, Canada, 30 April\u20133 May 2017. Available online: https://arxiv.org/abs/1706.06083v4 (accessed on 18 April 2023). 46. Kansal, K.; Krishna, P.S.; Jain, P.B.; Surya, R.; Honnavalli, P.; Eswaran, S. Defending against adversarial attacks on Covid-19 classifier: A denoiser-based approach. Heliyon 2022, 8, e11209. [CrossRef] [PubMed] 47. Paul, R.; Schabath, M.; Gillies, R.; Hall, L.; Goldgof, D. Mitigating Adversarial Attacks on Medical Image Understanding Systems. In Proceedings of the 2020 IEEE 17th International Symposium on Biomedical Imaging, Iowa City, IA, USA, 3\u20137 April 2020; pp. 1517\u20131521. [CrossRef] 48. Abdelhafeez, A.; Ali, A.M. DeepHAR-Net: A Novel Machine Intelligence Approach for Human Activity Recognition from Inertial Sensors. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 49. Abdelhafeez, A.; Aziz, A.; Khalil, N. Building a Sustainable Social Feedback Loop: A Machine Intelligence Approach for Twitter Opinion Mining. Sustain. Mach. Intell. J. 2022, 1. [CrossRef] 50. Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash, A.; Kohno, T.; Song, D. Robust Physical-World Attacks on Deep Learning Visual Classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitio, Seattle, WA, USA, 14\u201319 June 2018; pp. 1625\u20131634. [CrossRef] 51. Ozbulak, U.; Van Messem, A.; De Neve, W. Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation. In Medical Image Computing and Computer Assisted Intervention\u2014MICCAI 2019; Springer: Cham,..",
            "url": "https://openalex.org/W4387617233",
            "title": "A Comprehensive Review and Analysis of Deep Learning-Based Medical Image Adversarial Attack and Defense",
            "publication_date": "2023-10-13"
          }
        ]
      }
    }
  }
]